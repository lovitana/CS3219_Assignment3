<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001940">
<title confidence="0.92926">
Syntactic Sentence Simplification for French
</title>
<note confidence="0.954224">
Laetitia Brouwers Delphine Bernhard Anne-Laure Ligozat Thomas Franc¸ois
Aspirante FNRS LiLPa LIMSI-CNRS CENTAL, IL&amp;C
CENTAL, IL&amp;C Universit´e de Strasbourg ENSIIE UCLouvain
UCLouvain France France Belgium
Belgium
</note>
<sectionHeader confidence="0.987529" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969944444445">
This paper presents a method for the syn-
tactic simplification of French texts. Syn-
tactic simplification aims at making texts
easier to understand by simplifying com-
plex syntactic structures that hinder read-
ing. Our approach is based on the study
of two parallel corpora (encyclopaedia ar-
ticles and tales). It aims to identify the lin-
guistic phenomena involved in the manual
simplification of French texts and organ-
ise them within a typology. We then pro-
pose a syntactic simplification system that
relies on this typology to generate simpli-
fied sentences. The module starts by gen-
erating all possible variants before select-
ing the best subset. The evaluation shows
that about 80% of the simplified sentences
produced by our system are accurate.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999741">
In most of our daily activities, the ability to read
quickly and effectively is an undeniable asset,
even often a prerequisite (Willms, 2003). How-
ever, a sizeable part of the population is not able
to deal adequately with the texts they face. For
instance, Richard et al. (1993) reported that, in 92
applications for an unemployment allowance filled
by people with a low level of education, about half
of the required information was missing (some of
which was crucial for the processing of the appli-
cation), mainly because of comprehension issues.
These comprehension issues are often related
to the complexity of texts, particularly at the lex-
ical and syntactic levels. These two factors are
known to be important causes of reading difficul-
ties (Chall and Dale, 1995), especially for young
children, learners of a foreign language or people
with language impairments or intellectual disabil-
ities.
In this context, automatic text simplification
(ATS) appears as a means to help various peo-
ple access more easily the contents of the written
documents. ATS is an application domain of Nat-
ural Language Processing (NLP) aiming at mak-
ing texts more accessible for readers, while en-
suring the integrity of their contents and structure.
Among the investigations in this regard are those
of Caroll et al. (1999), Inui et al. (2003) and, more
recently, of Rello et al. (2013), who developed
tools to produce more accessible texts for people
with language disabilities such as aphasia, deaf-
ness or dyslexia. In the FIRST project, Barbu et
al. (2013) and Evans and Or˘asan (2013) imple-
mented a simplification system for patients with
autism, who may also struggle to understand diffi-
cult texts.
However, reading assistance is not only in-
tended for readers with disabilities, but also for
those who learn a new language (as first or sec-
ond language). De Belder and Moens (2010) fo-
cused on ATS for native English schoolchildren,
while Siddharthan (2006), Petersen and Ostendorf
(2007) and Medero and Ostendorf (2011) focused
on learners of a second language. Williams and
Reiter (2008), Aluisio et al. (2008) and Gasperin
et al. (2009) addressed ATS for illiterate adults.
Most of these studies are dealing with the En-
glish language, with the exception of some work
in Japanese (Inui et al., 2003), Spanish (Saggion
et al., 2011; Bott et al., 2012), Portuguese (Aluisio
et al., 2008) and French (Seretan, 2012).
ATS was also used as a preprocessing step to
increase the effectiveness of subsequent NLP op-
erations on texts. Chandrasekar et al. (1996)
first considered that long and complex sentences
were an obstacle for automatic parsing or machine
translation and they showed that a prior simplifi-
cation may result in a better automatic analysis
of sentences. More recently, Heilman and Smith
(2010) showed that adding ATS in the context
</bodyText>
<page confidence="0.993786">
47
</page>
<note confidence="0.9940905">
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47–56,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999931857142857">
of automatic question generation yields better re-
sults. Similarly, Lin and Wilbur (2007) and Jon-
nalagadda et al. (2009) optimized information ex-
traction from biomedical texts using ATS as a pre-
processing step.
In these studies, the simplifications carried out
are generally based on a set of manually defined
transformation rules. However, ATS may also
be solved with methods from machine transla-
tion and machine learning. This lead some re-
searchers (Zhu et al., 2010; Specia, 2010; Wood-
send and Lapata, 2011) to train statistical models
from comparable corpora of original and simpli-
fied texts. The data used in these studies are of-
ten based on the English Wikipedia (for original
texts) and the Simple English Wikipedia, a simpli-
fied version for children and non-native speakers
that currently comprises more than 100,000 arti-
cles. Similar resources exist for French, such as
Vikidia and Wikimini, but texts are far less nu-
merous in these as in their English counterpart.
Moreover, the original and simplified versions of
an article are not strictly parallel, which further
complicates machine learning. This is why, so far,
there was no attempt to adapt this machine learn-
ing methodology to French. The only previous
work on French, to our knowledge, is that of Sere-
tan (2012), which analysed a corpus of newspa-
pers to semi-automatically detect complex struc-
tures that has to be simplified. However, her sys-
tem of rules has not been implemented and evalu-
ated.
In this paper, we aim to further investigate the
issue of syntactic simplification for French. We
assume a midway point between the two main ten-
dencies in the field. We use parallel corpora sim-
ilar to those used in machine learning approaches
and analyse it to manually define a set of simpli-
fication rules. We have also implemented the syn-
tactic part of our typology through a simplification
system. It is based on the technique of overgen-
eration, which consists in generating all possible
simplified variants of a sentence, and then on the
selection of the best subset of variants for a given
text with the optimization technique known as in-
teger linear programming (ILP). ILP allows us to
specify a set of constraints that regulate the selec-
tion of the output by the syntactic simplification
system. This method has already been applied to
ATS in English by Belder and Moens (2010) and
Woodsend and Lapata (2011).
To conclude, the contributions of this paper are:
(1) a first corpus-based study of simplification pro-
cesses in French that relies on a corpus of parallel
sentences, (2) the organization of this study’s re-
sults in what might be the first typology of sim-
plification for French based on a corpus analysis
of original and simplified texts; (3) two new crite-
ria to select the best subset of simplified sentences
among the set of variants, namely the spelling list
of Catach (1985) and the use of keywords, and
finally (4) a syntactic simplification system for
French, a language with little resources as regards
text simplification.
In the next sections, we first present the cor-
pora building process (Section 2.1) and describe
a general typology of simplification derived from
our corpora (Section 2.2). Then, we present the
system based on the syntactic part of the typol-
ogy, which operates in two steps: overgeneration
of all possible simplified sentences (Section 2.3.1)
and selection of the best subset of candidates us-
ing readability criteria (Section 2.3.2) and ILP.
Finally, we evaluate the quality of the syntacti-
cally simplified sentences as regards grammatical-
ity, before performing some error analysis (Sec-
tion 3).
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999476">
2.1 Corpus Description
</subsectionHeader>
<bodyText confidence="0.999800277777778">
We based our typology of simplification rules on
the analysis of two corpora. More specifically,
since our aim is to identify and classify the var-
ious strategies used to transform a complex sen-
tence into a more simple one, the corpora had to
include parallel sentences. The reason why we
analysed two corpora is to determine whether dif-
ferent genres of texts lead to different simplifica-
tion strategies. In this study, we focused on the
analysis of informative and narrative texts. The in-
formative corpus comprises encyclopaedia articles
from Wikipedia 1 and Vikidia 2. For the narrative
texts, we used three classic tales by Perrault, Mau-
passant and Daudet and their simplified versions
for learners of French as a foreign language.
To collect the first of our parallel corpora, we
used the MediaWiki API to retrieve Wikipedia
and Vikidia articles with the same title. The
</bodyText>
<footnote confidence="0.999924">
1http://fr.wikipedia.org
2This site is intended for young people from eight to thirteen years and gathers more
accessible articles than Wikipedia, both in terms of language and content. It is available at
the address http://fr.vikidia.org
</footnote>
<page confidence="0.99952">
48
</page>
<bodyText confidence="0.999990111111111">
WikiExtractor 3 was then applied to the articles
to discard the wiki syntax and only keep the raw
texts. This corpus comprises 13,638 texts (7,460
from Vikidia and only 6,178 from Wikipedia,
since some Vikidia articles had no counterpart in
Wikipedia).
These articles were subsequently processed to
identify parallel sentences (Wikipedia sentence
with a simplified equivalent in Vikidia). The align-
ment has been made partly manually and partly
automatically with the monolingual alignment al-
gorithm described in Nelken and Shieber (2006),
which relies on a cosine similarity between sen-
tence vectors weighted with the tf-idf. This pro-
gram outputs alignments between sentences, along
with a confidence score. Among these files,
twenty articles or excerpts from Wikipedia were
selected along with their equivalent in Vikidia.
This amounts to 72 sentences for the former and
80 sentences for the latter.
The second corpus is composed of 16 narra-
tive texts, and more specifically tales, by Perrault,
Maupassant, and Daudet. We used tales since
their simplified version was closer to the origi-
nal than those of longer novels, which made the
sentence alignment simpler. The simplified ver-
sions of these tales were found in two collections
intended to learners of French as a foreign lan-
guage (FFL): “Hachette - Lire en franc¸ais facile”
and “De Boeck - Lire et s’entrainer”. Their level
of difficulty ranges from A1 (Daudet) to B1 (Mau-
passant) on the CEFR scale (Council of Europe,
2001), with Perrault being A2. The texts were dig-
itized by OCR processing and manually aligned,
by two annotators, with an adjudication phase for
the disagreement cases. In this corpus, we anal-
ysed 83 original sentences and their correspond-
ing 98 simplified versions, which gives us a size
roughly similar to the Wikipedia-Vikidia corpus.
The two corpora created are relevant for a man-
ual analysis, as done in the next section, but they
are too small for automatic processing. We plan
to implement a method to align automatically the
narrative texts in the near future and thus be able
to collect a larger corpus.
</bodyText>
<subsectionHeader confidence="0.999674">
2.2 Simplification Typology
</subsectionHeader>
<bodyText confidence="0.999957666666667">
The observations carried out on these two cor-
pora have made it possible to establish a typol-
ogy organised according to three main linguistic
</bodyText>
<footnote confidence="0.752323">
3http://medialab.di.unipi.it/wiki/Wikipedia\_Extractor
</footnote>
<bodyText confidence="0.999986456521739">
levels of transformation: lexical, discursive and
syntactic, which can be further divided into sub-
categories. It is worth mentioning that in previ-
ous work, simplification is commonly regarded as
pertaining to two categories of phenomena: lexi-
cal and syntactic (Carroll et al., 1999; Inui et al.,
2003; De Belder and Moens, 2010). Little atten-
tion has been paid to discourse in the area of auto-
matic simplification (Siddharthan, 2006).
The typology is summarized in Table 1. As
regards the lexicon, the phenomena we observed
involve four types of substitution. First, dif-
ficult terms can be replaced by a synonym or
an hypernym perceived as simpler. Second,
some anaphoric expressions, considered simpler
or more explicit, are preferred to their counter-
parts in the original texts. For example, in our
three tales, simplified nominal anaphora are regu-
larly used instead of pronominal anaphora. Third,
rather than using synonymy, the authors of the
simplified texts sometimes replace difficult words
with a definition or an explanatory paraphrase. Fi-
nally, in the particular case where the original texts
contain concepts in a foreign language, these non-
French terms are translated.
At the discourse level, the authors of simple
texts pay particular attention to the organization
of the information which has to be clear and con-
cise. To this end, clauses may be interchanged
to ensure a better presentation of the information.
In addition, information of secondary importance
can be removed while explanations or examples
are added for clarity. These two phenomena can
appear to be contradictory (deletion and addition),
but they actually operate in a common goal: make
the main information more comprehensible. Par-
ticular attention is also placed on the coherence
and cohesion of the text: Authors tend to explain
the pronouns and explicit the relations between
sentences. The last observed strategy is that im-
personal structures are often personalized.
Finally, at the syntactic level, five types of
changes are observed: tense modification, dele-
tion, modification, splitting and grouping. The last
two types can be considered together since they
are two opposite phenomena.
</bodyText>
<listItem confidence="0.8730738">
• First, the tenses used in the simplified ver-
sions are more common and less literary than
those used in the original texts. Thus, the
present and present perfect are preferred to
the simple past, imperfect and past perfect.
</listItem>
<page confidence="0.99791">
49
</page>
<table confidence="0.999704833333333">
Lexicon Discourse Syntax
Translation Reorganisation Tense
Anaphoric synonyms Addition Modification
Definition and paraphrase Deletion Grouping
Synonym or hypernym Coherence and cohesion Deletion
Personalisation Splitting
</table>
<tableCaption confidence="0.99846">
Table 1: Typology of simplifications
</tableCaption>
<listItem confidence="0.995094916666667">
• Secondary or redundant information, that is
generally considered removable at the syn-
tactic level, is not included in the simplified
texts. Adverbial clauses, some adverbs and
adjectives and subordinate clauses, among
others, are omitted.
• When some complex structures are not
deleted, then they are often moved or modi-
fied for better clarity. Such structures include
negative sentences, impersonal structures, in-
direct speech and subordinate clauses.
• The authors sometimes choose to divide long
</listItem>
<bodyText confidence="0.927934475">
sentences or conversely merge several sen-
tences into one. The grouping of elements is
much less frequent than the division of sen-
tences. To split a sentence, the authors gen-
erally transform a secondary clause–be it rel-
ative, coordinate, subordinate, participial or
adjectival–into an independent clause.
This classification can be compared with that of
Medero et al. (2011) who propose three categories
– division, deletion and extension – or that of Zhu
et al. (2010), which includes division, deletion, re-
organization, and substitution.
Among those transformations, some are hardly
implementable. This is the case when a change
requires the use of semantics. For example, noun
modifiers may sometimes be removed, but in other
cases, they are necessary. However, there are of-
ten neither typographical nor grammatical marked
differences between the two cases.
Another issue is that other syntactic changes
should be accompanied by lexical transforma-
tions, which are difficult to generalize. For exam-
ple, transforming a negative sentence into its af-
firmative equivalent requires to find a verb whose
affirmative form includes the meaning of the neg-
ative construction to replace.
There are also changes that are very particular
and require a manual rather than an automatic pro-
cessing of the text, in the sense that each case is
different (even if part of a more global rule). In
addition, they usually involve discourse or lexical
information and not just syntactic one.
Finally, the syntactic changes impacting other
parts of the text or concerning elements that de-
pend on another structure require more compre-
hensive changes to the text. Therefore, they are
also difficult to handle automatically. Thus, to
change the tense of a verb in a sentence, we must
ensure that the sequence of tenses agree in the en-
tire text.
</bodyText>
<subsectionHeader confidence="0.996448">
2.3 The Sentence Simplification System
</subsectionHeader>
<bodyText confidence="0.999946142857143">
We used this typology to implement a system of
syntactic simplification for French sentences. The
simplification is performed as a two-step process.
First, for each sentence of the text, we generate the
set of all possible simplifications (overgeneration
step), and then, we select the best subset of sim-
plified sentences using several criteria.
</bodyText>
<subsectionHeader confidence="0.58152">
2.3.1 Generation of the Simplified Sentences
</subsectionHeader>
<bodyText confidence="0.999975739130435">
The sentence overgeneration module is based on a
set of rules (19 rules), which rely both on morpho-
syntactic features of words and on syntactic rela-
tionships within sentences. To obtain this infor-
mation, the texts from our corpus are analyzed
by MELT 4 (Denis and Sagot, 2009) and Bon-
sai 5 (Candito et al., 2010) during a preprocessing
phase. As a result, texts are represented as syn-
tax trees that include the information necessary to
apply our simplification rules. After preprocess-
ing, the set of simplification rules is applied recur-
sively, one sentence at a time, until there is no fur-
ther structure to simplify. All simplified sentences
produced by a given rule are saved and gathered in
a set of variants.
The rules for syntactic simplification included
in our program are of three kinds: deletion rules
(12 rules), modification rules (3 rules) and split-
ting rules (4 rules). With regards to our typology, it
can be noted that two types of rules have not been
implemented: aggregation rules and tense simpli-
fication rules. The merging strategies (in which
several sentences are aggregated into one) were
</bodyText>
<footnote confidence="0.995690333333333">
4https://gforge.inria.fr/projects/lingwb
5http://alpage.inria.fr/statgram/frdep/fr_stat_dep_
parsing.html
</footnote>
<page confidence="0.995161">
50
</page>
<bodyText confidence="0.875154020833333">
not observed consistently in the corpus. Moreover,
aggregation rules could have come into conflict
with the deletion rules, since they have opposite
goals. Concerning tense aspects, some of them
are indeed more likely to be used than others in
Vikidia. However, this strategy has not been im-
plemented, since it implies global changes to the
text. For instance, when a simple past is replaced
by a present form, we must also adapt the verbs in
the surrounding context in accordance with tense
agreement. This requires to consider the whole
text, or at least the paragraph that contains the
modified verbal form, and be able to automatically
model tense agreement. Otherwise, we may alter
the coherence of the text and decrease its readabil-
ity.
This leaves us with 19 simplification rules.6 To
apply them, the candidate structures for simplifi-
cation first need to be detected using regular ex-
pressions, via Tregex 7(Levy and Andrew, 2006)
that allows the retrieval of elements and relation-
ships in a parse tree. In a second step, syntactic
trees in which a structure requires simplification
are modified according a set of operations imple-
mented through Tsurgeon.
The operations to perform depend on the type
of rules:
1. For the deletion cases, simply deleting all
the elements involved is sufficient (via the
delete operation in Tsurgeon). The ele-
ments affected by the deletion rules are ad-
verbial clauses, clauses between brackets,
some of the subordinate clauses, clauses be-
tween commas or introduced by words such
as “comme” (as), “voire” (even), “soit” (ei-
ther), or similar terms, some adverbs and
agent prepositional phrases.
2. For the modification rules, several opera-
tions need to be combined: some terms are
dropped (via Tsurgeon delete), others
are moved (operation Tsurgeon move)
and specific labels are added to the text to sig-
nal a possible later processing. These labels
are useful for rules implying a modification
of tense or mode aspects for a verb. In such
cases, tags are added around the verb to indi-
cate that it needs to be modified. The mod-
ification is performed later, using the conju-
</bodyText>
<footnote confidence="0.987900333333333">
6These 19 rules are available at http://cental.fltr.ucl.ac.be/team/
lbrouwers/rules.pdf
7http://nlp.stanford.edu/software/tregex.shtml
</footnote>
<bodyText confidence="0.90147941025641">
gation system Verbiste.8 For instance, to
change a passive into an active structure, not
only the voice must be changed, but some-
times also the person, so that the verb agrees
well with the agent that has become the new
subject. As regards modification rules, three
changes were implemented: moving adver-
bial clauses at the beginning of the sentence,
transforming passive structures into active
forms, and transforming a cleft to a non-cleft.
3. For the splitting rules, we followed a two-
step process. The subordinate clause is first
deleted, while the main clause is saved as a
new sentence. Resuming from the original
sentence, the main clause is, in turn, removed
to keep only the subordinate clause, which
must then be transformed into an independent
clause. In general, the verbal form of the sub-
ordinate clause needs to be altered in order to
operate as a main verb. Moreover, the pro-
noun governing the subordinated clause must
be substituted with its antecedent and the sub-
ject must be added when missing. In the case
of a relative clause, the relative pronoun thus
needs to be substituted by its antecedent, but
it is also important to consider the function
of the pronoun to find out where to insert this
antecedent. Our splitting rules apply when a
sentence includes either relative or participle
clauses, or clauses introduced by a colon or a
coordinating conjunction.
All these simplification rules are applied recur-
sively to a sentence until all possible alternatives
have been generated. Therefore, it is common to
have more than one simplified variant for a given
sentence. In this case, the next step consists in se-
lecting the most suitable variant to substitute the
original one. The selection process is described in
the next section.
</bodyText>
<subsectionHeader confidence="0.962466">
2.3.2 Selection of the Best Simplifications
</subsectionHeader>
<bodyText confidence="0.999926125">
Given a set of candidate simplified sentences for a
text, our goal is to select the best subset of simpli-
fied sentences, that is to say the subset that max-
imizes some measure of readability. More pre-
cisely, text readability is measured through differ-
ent criteria, which are optimized with an Integer
Linear Programming (ILP) approach (Gillick and
Favre, 2009). These criteria are rather simple in
</bodyText>
<footnote confidence="0.998408333333333">
8This software is available at the address http://sarrazip.com/dev/
verbiste.html under GNU general public license and was developed by Pierre Sar-
razin.
</footnote>
<page confidence="0.999188">
51
</page>
<bodyText confidence="0.99961435">
this approach. They are used to ensure that not
only the syntactic difficulty, but also the lexical
complexity decrease, since syntactic transforma-
tions may cause lexical or discursive alterations in
the text.
We considered four criteria to select the most
suitable sentences among the simplified set: sen-
tence length (in words) (hw), mean word length
(in characters) in the sentence (hs), familiarity of
the vocabulary (ha), and presence of some key-
words (h,). While the first two criteria are pretty
obvious as regards implementation, we measured
word familiarity based on Catach’s list (1985).9 It
contains about 3,000 of the most frequent words
in French, whose spelling should be taught in pri-
ority to schoolchildren. The keywords were in this
study simply defined as any term occurring more
than once in the text.
These four criteria were combined using integer
linear programming as follows: 10
</bodyText>
<equation confidence="0.9969554">
Maximize: hw + hs + ha + h,
hw = wps X Pi si − P
Where : i lwi si
hs = cpw X Pi lwi si − Pi l,i si
ha = aps X P i si − P i lai si
h, = Pj wjcj
Subject to: Pi∈gk si = 1 Vgk
sioccij G cj Vi, j
Pi sioccij &gt; cj Vj
(1)
</equation>
<bodyText confidence="0.91195">
The above variables are defined as follows:
</bodyText>
<listItem confidence="0.997269421052632">
• wps: desired (mean) number of words per sentence
• cpw: desired (mean) number of characters per word
• aps: desired (mean) number of words absent from
Catach’s list for a sentence
• si: binary variable indicating whether the sentence i
should be kept or not, with i varying from 1 to the total
number of simplified sentences
• cj: binary variable indicating whether keyword j is in
the simplification or not, with j varying from 1 to the
total number of keywords
• lwi : length of sentence i in words
• lci: number of characters in sentence i
• lai : number of words absent from Catach’s list in sen-
tence i
• wj: number of occurrences of keyword j
• gk: set of simplified sentences obtained from the same
original sentence k
• occij: binary variable indicating the presence of term j
in sentence i
</listItem>
<footnote confidence="0.9912075">
9This list is available at the site http://www.ia93.ac-creteil.fr/spip/
spip.php?article2900.
10We used an ILP module based on glpk that is available at the address http://
www.gnu.org/software/glpk/
</footnote>
<bodyText confidence="0.999839260869565">
wps, cpw and aps are constant parameters
whose values have been set respectively to 10, 5
and 2 for this study. 5 for cpw corresponds to the
value computed on the Vikidia corpus, while for
wps and aps, lower values than observed were
used to force simplification (respectively 10 in-
stead of 17 and 2 instead of 31).
However, these parameters may vary depending
on the context of use and the target population, as
they determine the level of difficulty of the simpli-
fied sentences obtained.
The constraints specify that (i) for each original
sentence, at most one simplification set should be
chosen, (ii) selecting a sentence means selecting
all the terms it contains and (iii) selecting a key-
word is only possible if it is present in at least one
selected sentence.
We illustrate this process with the Wikipedia ar-
ticle entitled Abel. This article contains 25 sen-
tences, from which 67 simplified sentences have
been generated. For the original sentence (1a) for
example, 5 variants were generated and simplifi-
cation (2) was selected by ILP.
</bodyText>
<listItem confidence="0.9994732">
(1a) Original sentence11 : Ca¨ın, l’afn´e, cultive la
terre et Abel ( ´etymologie : de l’h´ebreu «souffle »,
« vapeur », « existence pr´ecaire ») garde le trou-
peau.
(1b) Possible simplifications :
</listItem>
<bodyText confidence="0.957743142857143">
Simplification 1 : Cain, l’afn´e, cultive la terre et
Abel garde le troupeau.
Simplification 2 : Cain, l’afn´e, cultive la terre.
Abel garde le troupeau.
Simplification 3 : Ca¨ın, l’afn´e, cultive la terre.
Simplification 4 : Abel garde le troupeau.
(...)
</bodyText>
<listItem confidence="0.975929">
(1c) Selected simplification (2) : Cain, l’afn´e,
cultive la terre. Abel garde le troupeau.
</listItem>
<sectionHeader confidence="0.99495" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9995574">
Syntactic simplification involves substantial
changes within the sentence both in terms of
contents and form. It is therefore important to
check that the application of a rule does not cause
errors that would make the sentences produced
unintelligible or ungrammatical. A manual
evaluation of our system’s efficiency to generate
correct simplified sentences was carried out on
our two corpora. In each of them, we selected a
set of texts that had not been previously used for
</bodyText>
<footnote confidence="0.8934745">
11Cain, the eldest brother, farms the land and Abel (etymology : from Hebrew
« breath A, « steam A, «fragile existence A) looks after the flock.
</footnote>
<page confidence="0.99351">
52
</page>
<table confidence="0.999707666666667">
Sentence length Word length Word familiarity Keywords
Expected values 10 5 2 /
Original 19 6.1 11 5
Simplification 1 11 4.3 5 5
Simplification 2 5 4.6 2 5
Simplification 3 6 4.5 3 3
Simplification 4 4 4.7 2 2
Simplification 5 9 6.3 5 5
Simplification 6 12 7.3 8 2
</table>
<tableCaption confidence="0.999312">
Table 2: Values of the criteria in IPL for example (1).
</tableCaption>
<bodyText confidence="0.988200545454545">
the typological analysis, that is to say 9 articles
from Wikipedia (202 sentences) and two tales
from Perrault (176 sentences). In this evaluation,
all simplified sentences are considered, not only
those selected by ILP. The results are displayed in
Table 3 and discussed in Section 3.1. Two types
of errors can be detected: those resulting from
morpho-syntactic preprocessing, and particularly
the syntactic parser, and the simplification errors
per se, that we discuss in larger details in Section
3.2.
</bodyText>
<subsectionHeader confidence="0.999506">
3.1 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999967479166667">
Out of the 202 sentences selected in the informa-
tive corpus for evaluation, 113 (56%) have under-
gone one or more simplifications, which gives us
333 simplified variants. Our manual error analy-
sis revealed that 71 sentences (21%) contain some
errors, among which we can distinguish those due
to the preprocessing from those actually due to the
simplification system itself. It is worth mention-
ing that the first category amounts to 89% of the
errors, while the simplification rule are only re-
sponsible for 11% of those. We further refined the
analysis of the system’s errors distinguishing syn-
tactic from semantic errors.
The scores obtained on the narrative corpus are
slightly less good: out of the 369 simplified vari-
ants produced from the 154 original sentences, 77
(20.9%) contain errors. This value is very simi-
lar to the percentage for the informative corpus.
However, only 50.7% of these errors are due to the
preprocessing, while the remaining 49.3% come
from our rules. It means that our rules yield about
10.3% incorrect simplified variants compared to
2.7% for the informative corpus. Nevertheless,
these errors are caused mostly by 2 or 3 rules:
the deletion of subordinate clauses, of infinitives
or of clauses coordinated with a colon. This loss
in efficiency can be partly explained by the greater
presence of indirect speech in the tales that include
more non-removable subordinate clauses, difficult
to distinguish from removable clauses.
Globally, our results appear to be in line with
those of similar systems developed for English.12
Yet, few studies have a methodology and evalu-
ation close enough to ours to allow comparison
of the results. Siddharthan (2006) assessed his
system output using three judges who found that
about 80% of the simplified sentences were gram-
matical, while 87% preserved the original mean-
ing. These results are very similar to our find-
ings that mixed the syntactic and discourse dimen-
sions. Drndarevi´c et al. (2013) also presented the
output of their system to human judges who esti-
mated that 60% of the sentences were grammatical
and that 70% preserved the initial meaning. These
scores appear lower than ours, but Drndarevi´c et
al. also used lexical rules, which means that their
error rate includes both grammatical and lexical
errors.
</bodyText>
<subsectionHeader confidence="0.99577">
3.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999910157894737">
As regards syntax, the structure of a sentence can
be modified so that it becomes grammatically in-
correct. Three simplification rules are concerned.
Deletion rules may cause this kind of problem, be-
cause they involve removing a part of the sentence,
considered as secondary. However, sometimes the
deleted element is essential, as in the case of the
removal of the referent of a pronoun. This type of
problem arises both with the deletion of a subor-
dinate clause or that of an infinitive clause. Dele-
tion rules are also subject to a different kind of
errors. During the reconstruction of the sentence
resulting from the subordinate clause, some con-
stituents, such as the subject, may not be properly
identified and will be misplaced in the new sen-
tence.
At the semantic level, the information conveyed
by the original sentence may be modified or even
removed. When an agent or an infinitive clause
</bodyText>
<footnote confidence="0.7790375">
12We do not discuss French here, since no simplification system were found for French,
as explained previously.
</footnote>
<page confidence="0.995329">
53
</page>
<table confidence="0.9992192">
Wikipedia-Vikidia corpus
nb. sent. % correct % preproc. errors % simplification errors
333 262 (78.7 %) 63 (18.9%) 8 (2.4 %)
syntax: semantics:
6 (1.8%) 2 (0.6%)
Narrative corpus
nb. sent. % correct % preproc. errors % simplification errors
369 292 (79.1 %) 39 (10.6%) 38 (10.3 %)
syntax: semantics:
20 (5.4%) 18 (4.9%)
</table>
<tableCaption confidence="0.999913">
Table 3: Performance of the simplification system on both corpora
</tableCaption>
<bodyText confidence="0.856257117647059">
are suppressed, the meaning of the sentence may
be disrupted or some of the content lost. For in-
stance, in the following sentence – extracted from
the Wikipedia article abb´e (abbot) – the infinitive
clause explaining the term is dropped:
(2a) C’est aussi depuis le XVIIIe si`ecle
le terme en usage pour d´esigner un clerc
s´eculier ayant au moins rec¸u la ton-
sure.13
(2b) C’est aussi depuis le XVIIIe si`ecle
le terme en usage.
To fix the errors identified above, our rules should
be refined and developed, with the addition of
better tools for sentence regeneration as well as
some exclusion criteria for the incorrect sentences
within the ILP module, as discussed in the next
section.
</bodyText>
<sectionHeader confidence="0.997662" genericHeader="conclusions">
4 Perspectives and Conclusions
</sectionHeader>
<bodyText confidence="0.999972647058823">
This article describes an automatic syntactic sim-
plification system for French intended for children
and language learners. It is based on a set of rules
defined after a corpus study, which also led to the
development of a typology of simplifications in
French. It would be easy to extend our typology to
other target users based on other appropriate cor-
pora, such as people with language disorders.
Our approach also uses the technique of over-
generation, which makes it possible to retain the
best set of simplifications based on readability cri-
teria. Note that among those employed, some had
not been considered previously and produce inter-
esting results. Finally, we showed that the perfor-
mance of our system is good (about 80 % of the
generated sentences are correct) and in line with
previous studies.
</bodyText>
<footnote confidence="0.7412715">
13It is also the term in use since the 18th to refer to a secu-
lar cleric who, at least, received the tonsure.
</footnote>
<bodyText confidence="0.999989810810811">
The evaluation showed that the rules imple-
mented are more suitable for expository texts,
probably because they are more explicit, as style
there is of a minor importance. In addition, the
system set up was first tested on and therefore
adapted to Wikipedia. It was only subsequently
applied to narratives, that revealed new challenges,
especially concerning the deletion rules. The in-
formation provided in secondary clauses or com-
plements indeed seems most essential to under-
standing the story, especially when it comes to di-
rect or indirect speech. In order to comprehend
the differences in terms of efficiency and rules to
be applied between genres, it would be necessary
to extend our study to other texts collected in the
corpora.
We envision multiple perspectives to improve
our system. First, syntactic simplification could be
supplemented by lexical simplification, as is done
in some studies for English (Woodsend and Lap-
ata, 2011). Moreover, our error analysis has high-
lighted the need to add or repeat words when a
sentence is split. It would therefore be useful to
use a tool that manages references in order to im-
prove the quality of simplified text. In addition,
the sentence selection module could include addi-
tional selection criteria, based on the work done in
readability of French (Franc¸ois and Fairon, 2012).
A final perspective of improvement would be to
make the rule system adapt to the target audience
and the genre of the texts. This would require as-
sessing the relevance of various transformations
and selection criteria of the best simplifications.
This perspective would also require assessing the
effectiveness of the rules by means of comprehen-
sion tests both on the original and simplified sen-
tences, which we plan to do.
</bodyText>
<page confidence="0.997727">
54
</page>
<sectionHeader confidence="0.995042" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999583048780488">
S. Alu´ısio, L. Specia, T. Pardo, E. Maziero, and
R. Fortes. 2008. Towards brazilian portuguese auto-
matic text simplification systems. In Proceedings of
the eighth ACM symposium on Document engineer-
ing, pages 240–248.
E. Barbu, P. de Las Lagunillas, M. Martın-Valdivia, and
L. Urena-L´opez. 2013. Open book: a tool for help-
ing asd users’ semantic comprehension. NLP4ITA
2013, pages 11–19.
S. Bott, L. Rello, B. Drndarevic, and H. Saggion. 2012.
Can Spanish Be Simpler? LexSiS: Lexical Simpli-
fication for Spanish. In Proceedings of COLING
2012, pages 357–374.
M. Candito, B. Crabb´e, and P. Denis. 2010. Statisti-
cal french dependency parsing: treebank conversion
and first results. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 1840–1847.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying Text for
Language-Impaired Readers. In Proceedings of
EACL, pages 269–270.
N. Catach. 1985. Les listes orthographiques de base
du franc¸ais. Nathan, Paris.
J.S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall Readability Formula. Brook-
line Books, Cambridge.
R. Chandrasekar, C. Doran, and B. Srinivas. 1996.
Motivations and methods for text simplification. In
Proceedings of the 16th conference on Computa-
tional linguistics, pages 1041–1044.
Council of Europe. 2001. Common European Frame-
work of Reference for Languages: Learning, Teach-
ing, Assessment. Press Syndicate of the University
of Cambridge.
J. De Belder and M.-F. Moens. 2010. Text Simplifica-
tion for Children. In Proceedings of the Workshop
on Accessible Search Systems.
P. Denis and B. Sagot. 2009. Coupling an annotated
corpus and a morphosyntactic lexicon for state-of-
the-art pos tagging with less human effort. In Pro-
ceedings of PACLIC.
B. Drndarevi´c, S. ˇStajner, S. Bott, S. Bautista, and
H. Saggion. 2013. Automatic text simplification in
spanish: a comparative evaluation of complement-
ing modules. In Computational Linguistics and In-
telligent Text Processing, pages 488–500.
R. Evans and C. Or˘asan. 2013. Annotating signs of
syntactic complexity to support sentence simplifica-
tion. In Text, Speech, and Dialogue, pages 92–104.
T. Franc¸ois and C. Fairon. 2012. An ”AI readability”
formula for French as a foreign language. In Pro-
ceedings of EMNLP 2012, pages 466–477.
C. Gasperin, E. Maziero, L. Specia, T. Pardo, and
S. Aluisio. 2009. Natural language processing
for social inclusion: a text simplification architec-
ture for different literacy levels. Proceedings of
SEMISH-XXXVI Semin´ario Integrado de Software e
Hardware, pages 387–401.
D. Gillick and B. Favre. 2009. A scalable global model
for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 10–18.
M. Heilman and N. A. Smith. 2010. Extracting Simpli-
fied Statements for Factual Question Generation. In
Proceedings of the 3rd Workshop on Question Gen-
eration.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second interna-
tional workshop on Paraphrasing, pages 9–16.
S. Jonnalagadda, L. Tari, J. Hakenberg, C. Baral, and
G. Gonzalez. 2009. Towards Effective Sentence
Simplification for Automatic Processing of Biomed-
ical Text. In Proceedings of NAACL-HLT 2009.
R. Levy and G. Andrew. 2006. Tregex and tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proceedings of LREC, pages 2231–2234.
L. Lin and W. J. Wilbur. 2007. Syntactic sentence
compression in the biomedical domain: facilitat-
ing access to related articles. Information Retrieval,
10(4):393–414, October.
J. Medero and M. Ostendorf. 2011. Identifying Tar-
gets for Syntactic Simplification. In Proceedings of
the SLaTE 2011 workshop.
R. Nelken and S.M. Shieber. 2006. Towards robust
context-sensitive sentence alignment for monolin-
gual corpora. In Proceedings of EACL, pages 161–
168.
S. E. Petersen and M. Ostendorf. 2007. Text Simpli-
fication for Language Learners: A Corpus Analysis.
In Proceedings of SLaTE2007, pages 69–72.
L. Rello, C. Bayarri, A. G`orriz, R. Baeza-Yates,
S. Gupta, G. Kanvinde, H. Saggion, S. Bott, R. Car-
lini, and V. Topac. 2013. Dyswebxia 2.0!: more
accessible text for people with dyslexia. In Proceed-
ings of the 10th International Cross-Disciplinary
Conference on Web Accessibility, page 25.
J.F. Richard, J. Barcenilla, B. Brie, E. Charmet,
E. Clement, and P. Reynard. 1993. Le traite-
ment de documents administratifs par des popula-
tions de bas niveau de formation. Le Travail Hu-
main, 56(4):345–367.
H. Saggion, E. Mart´ınez, E. Etayo, A. Anula, and
L. Bourg. 2011. Text simplification in simplext.
making text more accessible. Procesamiento del
lenguaje natural, 47:341–342.
V. Seretan. 2012. Acquisition of syntactic simplifica-
tion rules for french. In LREC, pages 4019–4026.
A. Siddharthan. 2006. Syntactic Simplification and
Text Cohesion. Research on Language &amp; Computa-
tion, 4(1):77–109, jun.
L. Specia. 2010. Translating from Complex to Sim-
plified Sentences. In Proceedings of the 9th Inter-
national Conference on Computational Processing
of the Portuguese Language (Propor-2010)., pages
30–39.
S. Williams and E. Reiter. 2008. Generating basic
skills reports for low-skilled readers. Natural Lan-
guage Engineering, 14(4):495–525.
J.D. Willms. 2003. Literacy proficiency of youth:
Evidence of converging socioeconomic gradients.
International Journal of Educational Research,
</reference>
<page confidence="0.9786">
55
</page>
<reference confidence="0.998856222222222">
39(3):247–252.
K. Woodsend and M. Lapata. 2011. Learning to
Simplify Sentences with Quasi-Synchronous Gram-
mar and Integer Programming. In Proceedings of
EMNLP, pages 409–420.
Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of COLING 2010,
pages 1353–1361.
</reference>
<page confidence="0.998419">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.568250">
<title confidence="0.99851">Syntactic Sentence Simplification for French</title>
<author confidence="0.89665">Laetitia Brouwers Delphine Bernhard Anne-Laure Ligozat</author>
<affiliation confidence="0.903864">Aspirante FNRS LiLPa LIMSI-CNRS CENTAL, IL&amp;C CENTAL, IL&amp;C Universit´e de Strasbourg ENSIIE UCLouvain UCLouvain France France Belgium</affiliation>
<address confidence="0.778493">Belgium</address>
<abstract confidence="0.998336578947368">This paper presents a method for the syntactic simplification of French texts. Syntactic simplification aims at making texts easier to understand by simplifying complex syntactic structures that hinder reading. Our approach is based on the study of two parallel corpora (encyclopaedia articles and tales). It aims to identify the linguistic phenomena involved in the manual simplification of French texts and organise them within a typology. We then propose a syntactic simplification system that relies on this typology to generate simplified sentences. The module starts by generating all possible variants before selecting the best subset. The evaluation shows that about 80% of the simplified sentences produced by our system are accurate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Alu´ısio</author>
<author>L Specia</author>
<author>T Pardo</author>
<author>E Maziero</author>
<author>R Fortes</author>
</authors>
<title>Towards brazilian portuguese automatic text simplification systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the eighth ACM symposium on Document engineering,</booktitle>
<pages>240--248</pages>
<marker>Alu´ısio, Specia, Pardo, Maziero, Fortes, 2008</marker>
<rawString>S. Alu´ısio, L. Specia, T. Pardo, E. Maziero, and R. Fortes. 2008. Towards brazilian portuguese automatic text simplification systems. In Proceedings of the eighth ACM symposium on Document engineering, pages 240–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Barbu</author>
<author>P de Las Lagunillas</author>
<author>M Martın-Valdivia</author>
<author>L Urena-L´opez</author>
</authors>
<title>Open book: a tool for helping asd users’ semantic comprehension. NLP4ITA</title>
<date>2013</date>
<pages>11--19</pages>
<marker>Barbu, Lagunillas, Martın-Valdivia, Urena-L´opez, 2013</marker>
<rawString>E. Barbu, P. de Las Lagunillas, M. Martın-Valdivia, and L. Urena-L´opez. 2013. Open book: a tool for helping asd users’ semantic comprehension. NLP4ITA 2013, pages 11–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bott</author>
<author>L Rello</author>
<author>B Drndarevic</author>
<author>H Saggion</author>
</authors>
<title>Can Spanish Be Simpler? LexSiS: Lexical Simplification for Spanish.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>357--374</pages>
<contexts>
<context position="3363" citStr="Bott et al., 2012" startWordPosition="536" endWordPosition="539">t only intended for readers with disabilities, but also for those who learn a new language (as first or second language). De Belder and Moens (2010) focused on ATS for native English schoolchildren, while Siddharthan (2006), Petersen and Ostendorf (2007) and Medero and Ostendorf (2011) focused on learners of a second language. Williams and Reiter (2008), Aluisio et al. (2008) and Gasperin et al. (2009) addressed ATS for illiterate adults. Most of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) first considered that long and complex sentences were an obstacle for automatic parsing or machine translation and they showed that a prior simplification may result in a better automatic analysis of sentences. More recently, Heilman and Smith (2010) showed that adding ATS in the context 47 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations</context>
</contexts>
<marker>Bott, Rello, Drndarevic, Saggion, 2012</marker>
<rawString>S. Bott, L. Rello, B. Drndarevic, and H. Saggion. 2012. Can Spanish Be Simpler? LexSiS: Lexical Simplification for Spanish. In Proceedings of COLING 2012, pages 357–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabb´e</author>
<author>P Denis</author>
</authors>
<title>Statistical french dependency parsing: treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1840--1847</pages>
<marker>Candito, Crabb´e, Denis, 2010</marker>
<rawString>M. Candito, B. Crabb´e, and P. Denis. 2010. Statistical french dependency parsing: treebank conversion and first results. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC 2010), pages 1840–1847.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>D Pearce</author>
<author>Y Canning</author>
<author>S Devlin</author>
<author>J Tait</author>
</authors>
<title>Simplifying Text for Language-Impaired Readers.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>269--270</pages>
<contexts>
<context position="11435" citStr="Carroll et al., 1999" startWordPosition="1837" endWordPosition="1840"> method to align automatically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 Simplification Typology The observations carried out on these two corpora have made it possible to establish a typology organised according to three main linguistic 3http://medialab.di.unipi.it/wiki/Wikipedia\_Extractor levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronomina</context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlin, and J. Tait. 1999. Simplifying Text for Language-Impaired Readers. In Proceedings of EACL, pages 269–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Catach</author>
</authors>
<title>Les listes orthographiques de base du franc¸ais.</title>
<date>1985</date>
<location>Nathan, Paris.</location>
<contexts>
<context position="6940" citStr="Catach (1985)" startWordPosition="1127" endWordPosition="1128">simplification system. This method has already been applied to ATS in English by Belder and Moens (2010) and Woodsend and Lapata (2011). To conclude, the contributions of this paper are: (1) a first corpus-based study of simplification processes in French that relies on a corpus of parallel sentences, (2) the organization of this study’s results in what might be the first typology of simplification for French based on a corpus analysis of original and simplified texts; (3) two new criteria to select the best subset of simplified sentences among the set of variants, namely the spelling list of Catach (1985) and the use of keywords, and finally (4) a syntactic simplification system for French, a language with little resources as regards text simplification. In the next sections, we first present the corpora building process (Section 2.1) and describe a general typology of simplification derived from our corpora (Section 2.2). Then, we present the system based on the syntactic part of the typology, which operates in two steps: overgeneration of all possible simplified sentences (Section 2.3.1) and selection of the best subset of candidates using readability criteria (Section 2.3.2) and ILP. Finall</context>
</contexts>
<marker>Catach, 1985</marker>
<rawString>N. Catach. 1985. Les listes orthographiques de base du franc¸ais. Nathan, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Chall</author>
<author>E Dale</author>
</authors>
<title>Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books,</title>
<date>1995</date>
<location>Cambridge.</location>
<contexts>
<context position="1795" citStr="Chall and Dale, 1995" startWordPosition="278" endWordPosition="281">er, a sizeable part of the population is not able to deal adequately with the texts they face. For instance, Richard et al. (1993) reported that, in 92 applications for an unemployment allowance filled by people with a low level of education, about half of the required information was missing (some of which was crucial for the processing of the application), mainly because of comprehension issues. These comprehension issues are often related to the complexity of texts, particularly at the lexical and syntactic levels. These two factors are known to be important causes of reading difficulties (Chall and Dale, 1995), especially for young children, learners of a foreign language or people with language impairments or intellectual disabilities. In this context, automatic text simplification (ATS) appears as a means to help various people access more easily the contents of the written documents. ATS is an application domain of Natural Language Processing (NLP) aiming at making texts more accessible for readers, while ensuring the integrity of their contents and structure. Among the investigations in this regard are those of Caroll et al. (1999), Inui et al. (2003) and, more recently, of Rello et al. (2013),</context>
</contexts>
<marker>Chall, Dale, 1995</marker>
<rawString>J.S. Chall and E. Dale. 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>C Doran</author>
<author>B Srinivas</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>1041--1044</pages>
<contexts>
<context position="3564" citStr="Chandrasekar et al. (1996)" startWordPosition="568" endWordPosition="571">dren, while Siddharthan (2006), Petersen and Ostendorf (2007) and Medero and Ostendorf (2011) focused on learners of a second language. Williams and Reiter (2008), Aluisio et al. (2008) and Gasperin et al. (2009) addressed ATS for illiterate adults. Most of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) first considered that long and complex sentences were an obstacle for automatic parsing or machine translation and they showed that a prior simplification may result in a better automatic analysis of sentences. More recently, Heilman and Smith (2010) showed that adding ATS in the context 47 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of automatic question generation yields better results. Similarly, Lin and Wilbu</context>
</contexts>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>R. Chandrasekar, C. Doran, and B. Srinivas. 1996. Motivations and methods for text simplification. In Proceedings of the 16th conference on Computational linguistics, pages 1041–1044.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Council of Europe</author>
</authors>
<date>2001</date>
<booktitle>Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Press Syndicate of the</booktitle>
<institution>University of Cambridge.</institution>
<contexts>
<context position="10321" citStr="Europe, 2001" startWordPosition="1665" endWordPosition="1666">and 80 sentences for the latter. The second corpus is composed of 16 narrative texts, and more specifically tales, by Perrault, Maupassant, and Daudet. We used tales since their simplified version was closer to the original than those of longer novels, which made the sentence alignment simpler. The simplified versions of these tales were found in two collections intended to learners of French as a foreign language (FFL): “Hachette - Lire en franc¸ais facile” and “De Boeck - Lire et s’entrainer”. Their level of difficulty ranges from A1 (Daudet) to B1 (Maupassant) on the CEFR scale (Council of Europe, 2001), with Perrault being A2. The texts were digitized by OCR processing and manually aligned, by two annotators, with an adjudication phase for the disagreement cases. In this corpus, we analysed 83 original sentences and their corresponding 98 simplified versions, which gives us a size roughly similar to the Wikipedia-Vikidia corpus. The two corpora created are relevant for a manual analysis, as done in the next section, but they are too small for automatic processing. We plan to implement a method to align automatically the narrative texts in the near future and thus be able to collect a larger</context>
</contexts>
<marker>Europe, 2001</marker>
<rawString>Council of Europe. 2001. Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Press Syndicate of the University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J De Belder</author>
<author>M-F Moens</author>
</authors>
<title>Text Simplification for Children.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Accessible Search Systems.</booktitle>
<marker>De Belder, Moens, 2010</marker>
<rawString>J. De Belder and M.-F. Moens. 2010. Text Simplification for Children. In Proceedings of the Workshop on Accessible Search Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>B Sagot</author>
</authors>
<title>Coupling an annotated corpus and a morphosyntactic lexicon for state-ofthe-art pos tagging with less human effort.</title>
<date>2009</date>
<booktitle>In Proceedings of PACLIC.</booktitle>
<contexts>
<context position="16841" citStr="Denis and Sagot, 2009" startWordPosition="2681" endWordPosition="2684">f syntactic simplification for French sentences. The simplification is performed as a two-step process. First, for each sentence of the text, we generate the set of all possible simplifications (overgeneration step), and then, we select the best subset of simplified sentences using several criteria. 2.3.1 Generation of the Simplified Sentences The sentence overgeneration module is based on a set of rules (19 rules), which rely both on morphosyntactic features of words and on syntactic relationships within sentences. To obtain this information, the texts from our corpus are analyzed by MELT 4 (Denis and Sagot, 2009) and Bonsai 5 (Candito et al., 2010) during a preprocessing phase. As a result, texts are represented as syntax trees that include the information necessary to apply our simplification rules. After preprocessing, the set of simplification rules is applied recursively, one sentence at a time, until there is no further structure to simplify. All simplified sentences produced by a given rule are saved and gathered in a set of variants. The rules for syntactic simplification included in our program are of three kinds: deletion rules (12 rules), modification rules (3 rules) and splitting rules (4 r</context>
</contexts>
<marker>Denis, Sagot, 2009</marker>
<rawString>P. Denis and B. Sagot. 2009. Coupling an annotated corpus and a morphosyntactic lexicon for state-ofthe-art pos tagging with less human effort. In Proceedings of PACLIC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Drndarevi´c</author>
<author>S ˇStajner</author>
<author>S Bott</author>
<author>S Bautista</author>
<author>H Saggion</author>
</authors>
<title>Automatic text simplification in spanish: a comparative evaluation of complementing modules.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>488--500</pages>
<marker>Drndarevi´c, ˇStajner, Bott, Bautista, Saggion, 2013</marker>
<rawString>B. Drndarevi´c, S. ˇStajner, S. Bott, S. Bautista, and H. Saggion. 2013. Automatic text simplification in spanish: a comparative evaluation of complementing modules. In Computational Linguistics and Intelligent Text Processing, pages 488–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Evans</author>
<author>C Or˘asan</author>
</authors>
<title>Annotating signs of syntactic complexity to support sentence simplification.</title>
<date>2013</date>
<booktitle>In Text, Speech, and Dialogue,</booktitle>
<pages>92--104</pages>
<marker>Evans, Or˘asan, 2013</marker>
<rawString>R. Evans and C. Or˘asan. 2013. Annotating signs of syntactic complexity to support sentence simplification. In Text, Speech, and Dialogue, pages 92–104.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Franc¸ois</author>
<author>C Fairon</author>
</authors>
<title>An ”AI readability” formula for French as a foreign language. In</title>
<date>2012</date>
<booktitle>Proceedings of EMNLP 2012,</booktitle>
<pages>466--477</pages>
<marker>Franc¸ois, Fairon, 2012</marker>
<rawString>T. Franc¸ois and C. Fairon. 2012. An ”AI readability” formula for French as a foreign language. In Proceedings of EMNLP 2012, pages 466–477. C. Gasperin, E. Maziero, L. Specia, T. Pardo, and S. Aluisio. 2009. Natural language processing for social inclusion: a text simplification architecture for different literacy levels. Proceedings of SEMISH-XXXVI Semin´ario Integrado de Software e Hardware, pages 387–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gillick</author>
<author>B Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>10--18</pages>
<contexts>
<context position="22183" citStr="Gillick and Favre, 2009" startWordPosition="3541" endWordPosition="3544">e more than one simplified variant for a given sentence. In this case, the next step consists in selecting the most suitable variant to substitute the original one. The selection process is described in the next section. 2.3.2 Selection of the Best Simplifications Given a set of candidate simplified sentences for a text, our goal is to select the best subset of simplified sentences, that is to say the subset that maximizes some measure of readability. More precisely, text readability is measured through different criteria, which are optimized with an Integer Linear Programming (ILP) approach (Gillick and Favre, 2009). These criteria are rather simple in 8This software is available at the address http://sarrazip.com/dev/ verbiste.html under GNU general public license and was developed by Pierre Sarrazin. 51 this approach. They are used to ensure that not only the syntactic difficulty, but also the lexical complexity decrease, since syntactic transformations may cause lexical or discursive alterations in the text. We considered four criteria to select the most suitable sentences among the simplified set: sentence length (in words) (hw), mean word length (in characters) in the sentence (hs), familiarity of t</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>D. Gillick and B. Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N A Smith</author>
</authors>
<title>Extracting Simplified Statements for Factual Question Generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd Workshop on Question Generation.</booktitle>
<contexts>
<context position="3815" citStr="Heilman and Smith (2010)" startWordPosition="607" endWordPosition="610">t of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) first considered that long and complex sentences were an obstacle for automatic parsing or machine translation and they showed that a prior simplification may result in a better automatic analysis of sentences. More recently, Heilman and Smith (2010) showed that adding ATS in the context 47 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. </context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>M. Heilman and N. A. Smith. 2010. Extracting Simplified Statements for Factual Question Generation. In Proceedings of the 3rd Workshop on Question Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>A Fujita</author>
<author>T Takahashi</author>
<author>R Iida</author>
<author>T Iwakura</author>
</authors>
<title>Text simplification for reading assistance: a project note.</title>
<date>2003</date>
<booktitle>In Proceedings of the second international workshop on Paraphrasing,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2351" citStr="Inui et al. (2003)" startWordPosition="368" endWordPosition="371">mportant causes of reading difficulties (Chall and Dale, 1995), especially for young children, learners of a foreign language or people with language impairments or intellectual disabilities. In this context, automatic text simplification (ATS) appears as a means to help various people access more easily the contents of the written documents. ATS is an application domain of Natural Language Processing (NLP) aiming at making texts more accessible for readers, while ensuring the integrity of their contents and structure. Among the investigations in this regard are those of Caroll et al. (1999), Inui et al. (2003) and, more recently, of Rello et al. (2013), who developed tools to produce more accessible texts for people with language disabilities such as aphasia, deafness or dyslexia. In the FIRST project, Barbu et al. (2013) and Evans and Or˘asan (2013) implemented a simplification system for patients with autism, who may also struggle to understand difficult texts. However, reading assistance is not only intended for readers with disabilities, but also for those who learn a new language (as first or second language). De Belder and Moens (2010) focused on ATS for native English schoolchildren, while S</context>
<context position="11454" citStr="Inui et al., 2003" startWordPosition="1841" endWordPosition="1844">atically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 Simplification Typology The observations carried out on these two corpora have made it possible to establish a typology organised according to three main linguistic 3http://medialab.di.unipi.it/wiki/Wikipedia\_Extractor levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronominal anaphora. Third, </context>
</contexts>
<marker>Inui, Fujita, Takahashi, Iida, Iwakura, 2003</marker>
<rawString>K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura. 2003. Text simplification for reading assistance: a project note. In Proceedings of the second international workshop on Paraphrasing, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jonnalagadda</author>
<author>L Tari</author>
<author>J Hakenberg</author>
<author>C Baral</author>
<author>G Gonzalez</author>
</authors>
<title>Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="4203" citStr="Jonnalagadda et al. (2009)" startWordPosition="664" endWordPosition="668">ered that long and complex sentences were an obstacle for automatic parsing or machine translation and they showed that a prior simplification may result in a better automatic analysis of sentences. More recently, Heilman and Smith (2010) showed that adding ATS in the context 47 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikiped</context>
</contexts>
<marker>Jonnalagadda, Tari, Hakenberg, Baral, Gonzalez, 2009</marker>
<rawString>S. Jonnalagadda, L. Tari, J. Hakenberg, C. Baral, and G. Gonzalez. 2009. Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text. In Proceedings of NAACL-HLT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2231--2234</pages>
<contexts>
<context position="18721" citStr="Levy and Andrew, 2006" startWordPosition="2978" endWordPosition="2981">lies global changes to the text. For instance, when a simple past is replaced by a present form, we must also adapt the verbs in the surrounding context in accordance with tense agreement. This requires to consider the whole text, or at least the paragraph that contains the modified verbal form, and be able to automatically model tense agreement. Otherwise, we may alter the coherence of the text and decrease its readability. This leaves us with 19 simplification rules.6 To apply them, the candidate structures for simplification first need to be detected using regular expressions, via Tregex 7(Levy and Andrew, 2006) that allows the retrieval of elements and relationships in a parse tree. In a second step, syntactic trees in which a structure requires simplification are modified according a set of operations implemented through Tsurgeon. The operations to perform depend on the type of rules: 1. For the deletion cases, simply deleting all the elements involved is sufficient (via the delete operation in Tsurgeon). The elements affected by the deletion rules are adverbial clauses, clauses between brackets, some of the subordinate clauses, clauses between commas or introduced by words such as “comme” (as), “v</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>R. Levy and G. Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of LREC, pages 2231–2234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lin</author>
<author>W J Wilbur</author>
</authors>
<title>Syntactic sentence compression in the biomedical domain: facilitating access to related articles.</title>
<date>2007</date>
<journal>Information Retrieval,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="4172" citStr="Lin and Wilbur (2007)" startWordPosition="659" endWordPosition="662">et al. (1996) first considered that long and complex sentences were an obstacle for automatic parsing or machine translation and they showed that a prior simplification may result in a better automatic analysis of sentences. More recently, Heilman and Smith (2010) showed that adding ATS in the context 47 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts)</context>
</contexts>
<marker>Lin, Wilbur, 2007</marker>
<rawString>L. Lin and W. J. Wilbur. 2007. Syntactic sentence compression in the biomedical domain: facilitating access to related articles. Information Retrieval, 10(4):393–414, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Medero</author>
<author>M Ostendorf</author>
</authors>
<title>Identifying Targets for Syntactic Simplification.</title>
<date>2011</date>
<booktitle>In Proceedings of the SLaTE</booktitle>
<pages>workshop.</pages>
<contexts>
<context position="3031" citStr="Medero and Ostendorf (2011)" startWordPosition="479" endWordPosition="482">loped tools to produce more accessible texts for people with language disabilities such as aphasia, deafness or dyslexia. In the FIRST project, Barbu et al. (2013) and Evans and Or˘asan (2013) implemented a simplification system for patients with autism, who may also struggle to understand difficult texts. However, reading assistance is not only intended for readers with disabilities, but also for those who learn a new language (as first or second language). De Belder and Moens (2010) focused on ATS for native English schoolchildren, while Siddharthan (2006), Petersen and Ostendorf (2007) and Medero and Ostendorf (2011) focused on learners of a second language. Williams and Reiter (2008), Aluisio et al. (2008) and Gasperin et al. (2009) addressed ATS for illiterate adults. Most of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) first considered that long and complex sentences were an obstacle </context>
</contexts>
<marker>Medero, Ostendorf, 2011</marker>
<rawString>J. Medero and M. Ostendorf. 2011. Identifying Targets for Syntactic Simplification. In Proceedings of the SLaTE 2011 workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nelken</author>
<author>S M Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingual corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="9377" citStr="Nelken and Shieber (2006)" startWordPosition="1507" endWordPosition="1510"> terms of language and content. It is available at the address http://fr.vikidia.org 48 WikiExtractor 3 was then applied to the articles to discard the wiki syntax and only keep the raw texts. This corpus comprises 13,638 texts (7,460 from Vikidia and only 6,178 from Wikipedia, since some Vikidia articles had no counterpart in Wikipedia). These articles were subsequently processed to identify parallel sentences (Wikipedia sentence with a simplified equivalent in Vikidia). The alignment has been made partly manually and partly automatically with the monolingual alignment algorithm described in Nelken and Shieber (2006), which relies on a cosine similarity between sentence vectors weighted with the tf-idf. This program outputs alignments between sentences, along with a confidence score. Among these files, twenty articles or excerpts from Wikipedia were selected along with their equivalent in Vikidia. This amounts to 72 sentences for the former and 80 sentences for the latter. The second corpus is composed of 16 narrative texts, and more specifically tales, by Perrault, Maupassant, and Daudet. We used tales since their simplified version was closer to the original than those of longer novels, which made the s</context>
</contexts>
<marker>Nelken, Shieber, 2006</marker>
<rawString>R. Nelken and S.M. Shieber. 2006. Towards robust context-sensitive sentence alignment for monolingual corpora. In Proceedings of EACL, pages 161– 168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Petersen</author>
<author>M Ostendorf</author>
</authors>
<title>Text Simplification for Language Learners: A Corpus Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of SLaTE2007,</booktitle>
<pages>69--72</pages>
<contexts>
<context position="2999" citStr="Petersen and Ostendorf (2007)" startWordPosition="474" endWordPosition="477">, of Rello et al. (2013), who developed tools to produce more accessible texts for people with language disabilities such as aphasia, deafness or dyslexia. In the FIRST project, Barbu et al. (2013) and Evans and Or˘asan (2013) implemented a simplification system for patients with autism, who may also struggle to understand difficult texts. However, reading assistance is not only intended for readers with disabilities, but also for those who learn a new language (as first or second language). De Belder and Moens (2010) focused on ATS for native English schoolchildren, while Siddharthan (2006), Petersen and Ostendorf (2007) and Medero and Ostendorf (2011) focused on learners of a second language. Williams and Reiter (2008), Aluisio et al. (2008) and Gasperin et al. (2009) addressed ATS for illiterate adults. Most of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) first considered that long and com</context>
</contexts>
<marker>Petersen, Ostendorf, 2007</marker>
<rawString>S. E. Petersen and M. Ostendorf. 2007. Text Simplification for Language Learners: A Corpus Analysis. In Proceedings of SLaTE2007, pages 69–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rello</author>
<author>C Bayarri</author>
<author>A G`orriz</author>
<author>R Baeza-Yates</author>
<author>S Gupta</author>
<author>G Kanvinde</author>
<author>H Saggion</author>
<author>S Bott</author>
<author>R Carlini</author>
<author>V Topac</author>
</authors>
<title>Dyswebxia 2.0!: more accessible text for people with dyslexia.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility,</booktitle>
<pages>25</pages>
<marker>Rello, Bayarri, G`orriz, Baeza-Yates, Gupta, Kanvinde, Saggion, Bott, Carlini, Topac, 2013</marker>
<rawString>L. Rello, C. Bayarri, A. G`orriz, R. Baeza-Yates, S. Gupta, G. Kanvinde, H. Saggion, S. Bott, R. Carlini, and V. Topac. 2013. Dyswebxia 2.0!: more accessible text for people with dyslexia. In Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility, page 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Richard</author>
<author>J Barcenilla</author>
<author>B Brie</author>
<author>E Charmet</author>
<author>E Clement</author>
<author>P Reynard</author>
</authors>
<title>Le traitement de documents administratifs par des populations de bas niveau de formation.</title>
<date>1993</date>
<journal>Le Travail Humain,</journal>
<volume>56</volume>
<issue>4</issue>
<contexts>
<context position="1304" citStr="Richard et al. (1993)" startWordPosition="199" endWordPosition="202">e them within a typology. We then propose a syntactic simplification system that relies on this typology to generate simplified sentences. The module starts by generating all possible variants before selecting the best subset. The evaluation shows that about 80% of the simplified sentences produced by our system are accurate. 1 Introduction In most of our daily activities, the ability to read quickly and effectively is an undeniable asset, even often a prerequisite (Willms, 2003). However, a sizeable part of the population is not able to deal adequately with the texts they face. For instance, Richard et al. (1993) reported that, in 92 applications for an unemployment allowance filled by people with a low level of education, about half of the required information was missing (some of which was crucial for the processing of the application), mainly because of comprehension issues. These comprehension issues are often related to the complexity of texts, particularly at the lexical and syntactic levels. These two factors are known to be important causes of reading difficulties (Chall and Dale, 1995), especially for young children, learners of a foreign language or people with language impairments or intell</context>
</contexts>
<marker>Richard, Barcenilla, Brie, Charmet, Clement, Reynard, 1993</marker>
<rawString>J.F. Richard, J. Barcenilla, B. Brie, E. Charmet, E. Clement, and P. Reynard. 1993. Le traitement de documents administratifs par des populations de bas niveau de formation. Le Travail Humain, 56(4):345–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>E Mart´ınez</author>
<author>E Etayo</author>
<author>A Anula</author>
<author>L Bourg</author>
</authors>
<title>Text simplification in simplext. making text more accessible. Procesamiento del lenguaje natural,</title>
<date>2011</date>
<pages>47--341</pages>
<marker>Saggion, Mart´ınez, Etayo, Anula, Bourg, 2011</marker>
<rawString>H. Saggion, E. Mart´ınez, E. Etayo, A. Anula, and L. Bourg. 2011. Text simplification in simplext. making text more accessible. Procesamiento del lenguaje natural, 47:341–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Seretan</author>
</authors>
<title>Acquisition of syntactic simplification rules for french.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>4019--4026</pages>
<contexts>
<context position="3425" citStr="Seretan, 2012" startWordPosition="547" endWordPosition="548">who learn a new language (as first or second language). De Belder and Moens (2010) focused on ATS for native English schoolchildren, while Siddharthan (2006), Petersen and Ostendorf (2007) and Medero and Ostendorf (2011) focused on learners of a second language. Williams and Reiter (2008), Aluisio et al. (2008) and Gasperin et al. (2009) addressed ATS for illiterate adults. Most of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) first considered that long and complex sentences were an obstacle for automatic parsing or machine translation and they showed that a prior simplification may result in a better automatic analysis of sentences. More recently, Heilman and Smith (2010) showed that adding ATS in the context 47 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26</context>
<context position="5363" citStr="Seretan (2012)" startWordPosition="856" endWordPosition="858">ia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. This is why, so far, there was no attempt to adapt this machine learning methodology to French. The only previous work on French, to our knowledge, is that of Seretan (2012), which analysed a corpus of newspapers to semi-automatically detect complex structures that has to be simplified. However, her system of rules has not been implemented and evaluated. In this paper, we aim to further investigate the issue of syntactic simplification for French. We assume a midway point between the two main tendencies in the field. We use parallel corpora similar to those used in machine learning approaches and analyse it to manually define a set of simplification rules. We have also implemented the syntactic part of our typology through a simplification system. It is based on </context>
</contexts>
<marker>Seretan, 2012</marker>
<rawString>V. Seretan. 2012. Acquisition of syntactic simplification rules for french. In LREC, pages 4019–4026.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
</authors>
<title>Syntactic Simplification and Text Cohesion.</title>
<date>2006</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="2968" citStr="Siddharthan (2006)" startWordPosition="472" endWordPosition="473">) and, more recently, of Rello et al. (2013), who developed tools to produce more accessible texts for people with language disabilities such as aphasia, deafness or dyslexia. In the FIRST project, Barbu et al. (2013) and Evans and Or˘asan (2013) implemented a simplification system for patients with autism, who may also struggle to understand difficult texts. However, reading assistance is not only intended for readers with disabilities, but also for those who learn a new language (as first or second language). De Belder and Moens (2010) focused on ATS for native English schoolchildren, while Siddharthan (2006), Petersen and Ostendorf (2007) and Medero and Ostendorf (2011) focused on learners of a second language. Williams and Reiter (2008), Aluisio et al. (2008) and Gasperin et al. (2009) addressed ATS for illiterate adults. Most of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) fir</context>
<context position="11587" citStr="Siddharthan, 2006" startWordPosition="1865" endWordPosition="1866">tions carried out on these two corpora have made it possible to establish a typology organised according to three main linguistic 3http://medialab.di.unipi.it/wiki/Wikipedia\_Extractor levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronominal anaphora. Third, rather than using synonymy, the authors of the simplified texts sometimes replace difficult words with a definition or an explanatory</context>
<context position="29294" citStr="Siddharthan (2006)" startWordPosition="4749" endWordPosition="4750"> 2.7% for the informative corpus. Nevertheless, these errors are caused mostly by 2 or 3 rules: the deletion of subordinate clauses, of infinitives or of clauses coordinated with a colon. This loss in efficiency can be partly explained by the greater presence of indirect speech in the tales that include more non-removable subordinate clauses, difficult to distinguish from removable clauses. Globally, our results appear to be in line with those of similar systems developed for English.12 Yet, few studies have a methodology and evaluation close enough to ours to allow comparison of the results. Siddharthan (2006) assessed his system output using three judges who found that about 80% of the simplified sentences were grammatical, while 87% preserved the original meaning. These results are very similar to our findings that mixed the syntactic and discourse dimensions. Drndarevi´c et al. (2013) also presented the output of their system to human judges who estimated that 60% of the sentences were grammatical and that 70% preserved the initial meaning. These scores appear lower than ours, but Drndarevi´c et al. also used lexical rules, which means that their error rate includes both grammatical and lexical </context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>A. Siddharthan. 2006. Syntactic Simplification and Text Cohesion. Research on Language &amp; Computation, 4(1):77–109, jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
</authors>
<title>Translating from Complex to Simplified Sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Processing of the Portuguese Language (Propor-2010).,</booktitle>
<pages>30--39</pages>
<contexts>
<context position="4565" citStr="Specia, 2010" startWordPosition="725" endWordPosition="726">opulations (PITR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further compl</context>
</contexts>
<marker>Specia, 2010</marker>
<rawString>L. Specia. 2010. Translating from Complex to Simplified Sentences. In Proceedings of the 9th International Conference on Computational Processing of the Portuguese Language (Propor-2010)., pages 30–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>Generating basic skills reports for low-skilled readers.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="3100" citStr="Williams and Reiter (2008)" startWordPosition="490" endWordPosition="493">disabilities such as aphasia, deafness or dyslexia. In the FIRST project, Barbu et al. (2013) and Evans and Or˘asan (2013) implemented a simplification system for patients with autism, who may also struggle to understand difficult texts. However, reading assistance is not only intended for readers with disabilities, but also for those who learn a new language (as first or second language). De Belder and Moens (2010) focused on ATS for native English schoolchildren, while Siddharthan (2006), Petersen and Ostendorf (2007) and Medero and Ostendorf (2011) focused on learners of a second language. Williams and Reiter (2008), Aluisio et al. (2008) and Gasperin et al. (2009) addressed ATS for illiterate adults. Most of these studies are dealing with the English language, with the exception of some work in Japanese (Inui et al., 2003), Spanish (Saggion et al., 2011; Bott et al., 2012), Portuguese (Aluisio et al., 2008) and French (Seretan, 2012). ATS was also used as a preprocessing step to increase the effectiveness of subsequent NLP operations on texts. Chandrasekar et al. (1996) first considered that long and complex sentences were an obstacle for automatic parsing or machine translation and they showed that a p</context>
</contexts>
<marker>Williams, Reiter, 2008</marker>
<rawString>S. Williams and E. Reiter. 2008. Generating basic skills reports for low-skilled readers. Natural Language Engineering, 14(4):495–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Willms</author>
</authors>
<title>Literacy proficiency of youth: Evidence of converging socioeconomic gradients.</title>
<date>2003</date>
<journal>International Journal of Educational Research,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="1167" citStr="Willms, 2003" startWordPosition="176" endWordPosition="177">ticles and tales). It aims to identify the linguistic phenomena involved in the manual simplification of French texts and organise them within a typology. We then propose a syntactic simplification system that relies on this typology to generate simplified sentences. The module starts by generating all possible variants before selecting the best subset. The evaluation shows that about 80% of the simplified sentences produced by our system are accurate. 1 Introduction In most of our daily activities, the ability to read quickly and effectively is an undeniable asset, even often a prerequisite (Willms, 2003). However, a sizeable part of the population is not able to deal adequately with the texts they face. For instance, Richard et al. (1993) reported that, in 92 applications for an unemployment allowance filled by people with a low level of education, about half of the required information was missing (some of which was crucial for the processing of the application), mainly because of comprehension issues. These comprehension issues are often related to the complexity of texts, particularly at the lexical and syntactic levels. These two factors are known to be important causes of reading difficu</context>
</contexts>
<marker>Willms, 2003</marker>
<rawString>J.D. Willms. 2003. Literacy proficiency of youth: Evidence of converging socioeconomic gradients. International Journal of Educational Research, 39(3):247–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>409--420</pages>
<contexts>
<context position="4593" citStr="Woodsend and Lapata, 2011" startWordPosition="727" endWordPosition="731">TR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. Thi</context>
<context position="6462" citStr="Woodsend and Lapata (2011)" startWordPosition="1043" endWordPosition="1046">plification rules. We have also implemented the syntactic part of our typology through a simplification system. It is based on the technique of overgeneration, which consists in generating all possible simplified variants of a sentence, and then on the selection of the best subset of variants for a given text with the optimization technique known as integer linear programming (ILP). ILP allows us to specify a set of constraints that regulate the selection of the output by the syntactic simplification system. This method has already been applied to ATS in English by Belder and Moens (2010) and Woodsend and Lapata (2011). To conclude, the contributions of this paper are: (1) a first corpus-based study of simplification processes in French that relies on a corpus of parallel sentences, (2) the organization of this study’s results in what might be the first typology of simplification for French based on a corpus analysis of original and simplified texts; (3) two new criteria to select the best subset of simplified sentences among the set of variants, namely the spelling list of Catach (1985) and the use of keywords, and finally (4) a syntactic simplification system for French, a language with little resources a</context>
<context position="33895" citStr="Woodsend and Lapata, 2011" startWordPosition="5511" endWordPosition="5515">vealed new challenges, especially concerning the deletion rules. The information provided in secondary clauses or complements indeed seems most essential to understanding the story, especially when it comes to direct or indirect speech. In order to comprehend the differences in terms of efficiency and rules to be applied between genres, it would be necessary to extend our study to other texts collected in the corpora. We envision multiple perspectives to improve our system. First, syntactic simplification could be supplemented by lexical simplification, as is done in some studies for English (Woodsend and Lapata, 2011). Moreover, our error analysis has highlighted the need to add or repeat words when a sentence is split. It would therefore be useful to use a tool that manages references in order to improve the quality of simplified text. In addition, the sentence selection module could include additional selection criteria, based on the work done in readability of French (Franc¸ois and Fairon, 2012). A final perspective of improvement would be to make the rule system adapt to the target audience and the genre of the texts. This would require assessing the relevance of various transformations and selection c</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>K. Woodsend and M. Lapata. 2011. Learning to Simplify Sentences with Quasi-Synchronous Grammar and Integer Programming. In Proceedings of EMNLP, pages 409–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhu</author>
<author>D Bernhard</author>
<author>I Gurevych</author>
</authors>
<title>A Monolingual Tree-based Translation Model for Sentence Simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>1353--1361</pages>
<contexts>
<context position="4551" citStr="Zhu et al., 2010" startWordPosition="721" endWordPosition="724">or Target Reader Populations (PITR) @ EACL 2014, pages 47–56, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which</context>
<context position="14784" citStr="Zhu et al. (2010)" startWordPosition="2352" endWordPosition="2355">structures include negative sentences, impersonal structures, indirect speech and subordinate clauses. • The authors sometimes choose to divide long sentences or conversely merge several sentences into one. The grouping of elements is much less frequent than the division of sentences. To split a sentence, the authors generally transform a secondary clause–be it relative, coordinate, subordinate, participial or adjectival–into an independent clause. This classification can be compared with that of Medero et al. (2011) who propose three categories – division, deletion and extension – or that of Zhu et al. (2010), which includes division, deletion, reorganization, and substitution. Among those transformations, some are hardly implementable. This is the case when a change requires the use of semantics. For example, noun modifiers may sometimes be removed, but in other cases, they are necessary. However, there are often neither typographical nor grammatical marked differences between the two cases. Another issue is that other syntactic changes should be accompanied by lexical transformations, which are difficult to generalize. For example, transforming a negative sentence into its affirmative equivalent</context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A Monolingual Tree-based Translation Model for Sentence Simplification. In Proceedings of COLING 2010, pages 1353–1361.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
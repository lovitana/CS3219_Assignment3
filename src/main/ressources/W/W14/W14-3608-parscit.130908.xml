<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.889865">
Automatic Arabic diacritics restoration based on deep nets
</title>
<note confidence="0.84512875">
Mohsen A. A. Rashwan Ahmad A. Al Sallab Hazem M. Raafat Ahmed Rafea
ELC Dept., Cairo Uni- ELC Dept., Cairo Uni- Computer Science Computer Science
versity, versity Dept., Kuwait Uni- Dept., American
versity University in Cairo
</note>
<address confidence="0.358082">
mohsen_rashwan ahmad.elsallab hazem Rafea
</address>
<email confidence="0.979868">
@rdi-eg.com @gmail.com @cs.ku.edu.kw @aucegypt.edu
</email>
<sectionHeader confidence="0.993217" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982252">
In this paper, Arabic diacritics restoration
problem is tackled under the deep learn-
ing framework presenting Confused Sub-
set Resolution (CSR) method to improve
the classification accuracy, in addition to
Arabic Part-of-Speech (PoS) tagging
framework using deep neural nets. Spe-
cial focus is given to syntactic diacritiza-
tion, which still suffer low accuracy as
indicated by related works. Evaluation is
done versus state-of-the-art systems re-
ported in literature, with quite challeng-
ing datasets, collected from different do-
mains. Standard datasets like LDC Arab-
ic Tree Bank is used in addition to cus-
tom ones available online for results rep-
lication. Results show significant im-
provement of the proposed techniques
over other approaches, reducing the syn-
tactic classification error to 9.9% and
morphological classification error to 3%
compared to 12.7% and 3.8% of the best
reported results in literature, improving
the error by 22% over the best reported
systems
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974509090909">
Arabic is a wide spread language spoken by over
350 million people on the planet. Arabic alphabet
and vocabulary are very rich, with the same word
morphology being a candidate of different mean-
ings and pronunciations. For example the word
y— might bear the meaning of the person name
“Omar” ْ�َ�ُ- or the meaning of “age” ْ�ْ�ُ�. What
distinguish them is the diacritization signs as-
signed to each character of the word.
Diacritics are marks added on the character to
reflect its correct pronunciation, according to
grammatical, syntactical and morphological rules
of the language.
Nowadays, Modern Standard Arabic (MSA)
transcripts are written without diacritics, left to
the ability of the reader to restore them from the
context and knowledge. Diacritics restoration is
not an easy task even for knowledgeable, native
Arabic speakers. On the other hand, there are
many machine learning tasks, like Text-To-
Speech (TTS), translation, spelling correction,
word sense disambiguation,...etc, that require
diacritizing the script as a pre-processing step
before applying the core application technique.
In its basic form, the problem can be reduced
to a pattern classification problem, with seven
diacritics classes being the targets. In addition,
the diacritics classification can be divided into
syntactical diacritization, caring about case-
ending and morphological diacritization, caring
about the rest of the word diacritics. So far, mor-
phological part of the problem is almost solved,
leaving a marginal error of around 3-4%, Rash-
wan et al. (2009, 2011). On the other hand, syn-
tactical diacritization errors are still high, hitting
a ceiling that is claimed to be asymptotic and
cannot be squeezed any further, Rashwan et al.
(2009, 2011). For this reason, we focus our effort
to squeeze this error beyond the least 12.5% er-
ror obtained in Rashwan et al. (2009, 2011).
Recently, a significant advancement in the area
of deep learning has been witnessed, with the
development of a generative model; Deep Belief
Nets (DBN), with a fast algorithm for inference
of the model parameters. Deep Neural Networks
(DNN) shall be the basic machine learning clas-
sifier used in this work, employing the latest re-
sults reached in the deep learning field. An effi-
cient features’ vector is designed under the um-
brella of deep learning to distinguish different
words diacritics. Features that are tested in the
current work are: PoS, morphological quadruple
of lexemes, last character and word identity. In
addition, context features are essential to the dia-
critization problem. Context features include, the
</bodyText>
<page confidence="0.994116">
65
</page>
<bodyText confidence="0.983162046875">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 65–72,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
previous word features, as well as the previous
word diacritic.
Part-of-Speech (PoS) features are critical to
syntactic diacritization, which is the focus of this
work. For some datasets PoS tags are manually
annotated by professional linguistics, while for
the real case and most datasets, they are not
available. For this reason, standalone PoS taggers
are built under the deep learning framework,
which can reused in Arabic PoS tagging systems,
needed for many other applications, not only for
Arabic diacritization.
The deep learning model often hit a perfor-
mance barrier which cannot be crossed. Hence,
error analysis and diagnosis is run on the confu-
sion matrix results, proposing the Confused Sub-
set Resolution (CSR) method to train sub-
classifiers to resolve the identified confusions
and automatically generate a deep network-of-
networks composed of the main classifier and the
sub-classifiers working together to offer im-
proved accuracy system purified of the identified
confusions, offering around 2% error enhance-
ment.
Evaluation of the proposed techniques is done on
two datasets; the first is a custom one collected
from many different sources, which is available
online at (http://www.RDI-
eg.com/RDI/TrainingData is where to download
TRN_DB_II). Manually extracted PoS and mor-
phological quadruples are available for only a
part of this dataset. The PoS tags of this part of
the dataset were used to build the DNN PoS tag-
gers to tag the rest of the dataset. The corres-
ponding test set is available online at
(http://www.RDI-eg.com/RDI/TestData is where
to download TST_DB), which is quite challeng-
ing and collected from different sources than
training ones. The second dataset is the standard
LDC Arabic Tree Bank dataset LDC Arabic Tree
Bank Part 3,
(http://www.ldc.upenn.edu/Catalog/catalogEntry.
jsp?catalogId=LDC2005T20) used to bench
mark the system against state-of-the art systems
in Arabic diacritization area.
The rest of the paper is organized as follows:
first the related works in literature are surveyed,
followed by a formulation of the CSR method.
The next section is dedicated to describing the
features used in the system, and how they are
encoded and represented in the features’ vector
followed by the details of building the DNN PoS
tagger for Arabic. The datasets used for evalua-
tion are then described. The next section de-
scribes the system evaluation experiments. Ex-
perimental results include an error analysis study
of the effect of each feature and method on the
system performance, in addition to benchmark-
ing against state-of-the art systems in literature,
evaluated on standard datasets. Finally, the paper
is concluded with the main results and conclu-
sion.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999973258064516">
There have been many attempts to approach the
Arabic diacritization problem by different tech-
niques. Focus will be around three works strong-
ly related to what is proposed here, and having
the best results in literature. Zitouni et al. (2006)
apply Maximum Entropy classification to the
problem taking the advantage of the MaxEnt
framework to combine different features togeth-
er, like lexical, segment-based, and PoS features.
Segmentation involves getting the prefix, suffix,
and stem of the word. PoS features are also gen-
erated under the MaxEnt framework. Habash
and Rambow (2007) perform Morphological
Analysis and Disambiguation of Arabic (MA-
DA) system, and then apply SVM classification.
Last, Rashwan et al. (2009 and 2011) propose a
hybrid approach composed of two stages: first,
maximum marginal probability via A* lattice
search and n-grams probability estimation. When
full-form words are OOV, the system switches to
the second mode which factorizes each Arabic
word into all its possible morphological constitu-
ents, then uses also the same techniques used by
the first mode to get the most likely sequence of
morphemes, hence the most likely diacritization.
The latter system shall be our baseline, since it
gives the best results in literature so far, and the
dataset used to evaluate it is available at our
hand, and hence fair comparison is possible. Al-
so, comparison to the three systems is made on
the LDC Arabic Tree Bank data set.
</bodyText>
<sectionHeader confidence="0.939844" genericHeader="method">
3 System architecture
</sectionHeader>
<bodyText confidence="0.999961272727273">
In this section the overall system is presented.
The raw text input is fed to the system word by
word. According to the configured context depth,
a number of succeeding and preceding words are
stored in a context memory. In our system the
context is experimentally taken as three preced-
ing words and one succeeding word (N=3, M=1),
which is found to give the best accuracy results
versus other tunings: (N=1, M=1), (N=2, M=2),
(N=3, M=2), (N=1, M=3) and (N=1, M=3). If the
word is the first or last one in a sentence, the pre-
</bodyText>
<page confidence="0.946497">
66
</page>
<bodyText confidence="0.999963760869565">
ceding or succeeding context is zero padded.
Word context serves in case of syntactic diacriti-
zation, while for morphological case, characters
context is also needed, which is directly present
in the character sequence of the single input
word itself.
Features extraction procedure depends on the
feature itself. For PoS tags, a special DNN is
trained for that purpose, which also makes use of
the context of the word. For other features, like
sequence of characters forming the word, the last
character of the word and the morphological
quadruples are directly extracted from the single
word.
The framework in Figure 2 is employed. Three
layers network architecture is used for each fea-
tures extraction subnet or classification network.
For the classification network a 20-20-20 archi-
tecture was used, while for PoS-tagging net-
works a 60-60-60 is used. The network architec-
ture is determined empirically. By experiments it
was found that the best architecture is the sym-
metric one, with the same width for all layers.
The best width is found to be the same as the
average number of ones in the training set fea-
tures vectors.
The neural network training undergoes DBN pre
training as in Hinton et al. (2006) for 20 epochs
per layer, with batch size of 1000 examples each
without mini batches. Momentum is used initial-
ly with 0.5 for the first 5 epochs and then raised
to 0.9 for the next epochs. The discriminative
fine tuning is performed using conjugate gradient
minimization for 30 epochs. For the first 6
epochs, only the upper layer is adjusted, then the
rest of the layers are trained for the next epochs.
Once the features are ready of a certain raw word
it is fed to the DNN classifier. The resulting con-
fusion matrix from the training phase is then fed
to the CSR method to generate the tree structure
that improves the system accuracy. During test-
ing phase, the raw input features are fed to the
DNN classifier to obtain an initial guess of the
target diacritic. This guess is then improved in
the next CSR stage to obtain the final diacritic
decision.
</bodyText>
<figureCaption confidence="0.700925">
Figure 1 Overall Arabic diacritization system
</figureCaption>
<sectionHeader confidence="0.836118" genericHeader="method">
4 Deep learning framework
</sectionHeader>
<bodyText confidence="0.995328">
The Arabic diacritics restoration task can be
formulated as pattern classification problem. The
target classes shall be the diacritics themselves,
described in TABLE I. The input is the raw MSA
transcript. The task is to classify the input based
on well-designed features’ vector and restore the
original diacritics of the raw text. The output
shall be the full diacritized text. All these diacrit-
ics can exist on case-ending character, while
Fathten, Dammeten and Kasreten can never oc-
cur on non-ending character of the word root.
The machine learning classifier tool chosen in
this paper is the Deep Neural Network (DNN),
under the framework of learning deep architec-
ture proposed by Hinton et al. (2006). The raw
text is presented to the classifier, and a group of
sub-nets work to extract the desired features, like
PoS tags. The network architecture is shown in
Figure 2. Each sub-net is trained to extract a cer-
tain kind of features, and the obtained features’
vectors are concatenated together to form the
input that is represented to the classifier network.
In fact the training of features extraction nets is
guided by certain desired features, like PoS tags.
</bodyText>
<table confidence="0.986226142857143">
TABLE I ARABIC DIACRITICS CLASSES
Diacritics form Class name Pronunciation
on Arabic letter
ب
بَ Fatha e_;a /a/
بُ Damma Z-.;- /u/
ِب Kasra ةj­s /i/
ً�� Fathten C,:�;&apos; /an/
بٌ Dammeten /un/
C
ٍب Kasreten ������ /in/
بْ Sukun ن-4- No vowel
بّ Shadda ةّ� Double conso-
nant
</table>
<page confidence="0.997529">
67
</page>
<bodyText confidence="0.9698565">
This enables building a standalone system that
operates on the raw text only.
</bodyText>
<figureCaption confidence="0.940074">
Figure 2 Deep network framework
</figureCaption>
<sectionHeader confidence="0.938504" genericHeader="method">
5 Confused sub-set resolution method
</sectionHeader>
<bodyText confidence="0.999595266666667">
The Confused Sub-Classes Resolution (CSR) is
based on confusion matrix analysis and the me-
thod by Raafat and Rashwan (1993). The output
of this analysis shall be a network architecture
composed of the original classifier operating
with sub-classifiers to resolve confusions that
were identified through confusion matrix analy-
sis.
The method starts with training a global classifi-
er, then evaluating its performance. To enhance
its accuracy, the sources of errors are analyzed
by building the confusion matrix for the training
set. The position of the off diagonal element
identifies the pair of classes that are confused
together.
</bodyText>
<subsectionHeader confidence="0.81432">
5.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.521126333333333">
The flow chart of the CSR method is shown in
Figure 3. The following steps describe the algo-
rithm:
</bodyText>
<listItem confidence="0.99037675">
1. Train a basic global classifier in DNN
framework and obtain the confusion matrix
C on the training set
2. Identify the confusion domains { i }
</listItem>
<equation confidence="0.66425">
D = D
</equation>
<bodyText confidence="0.914287333333333">
that have confusions more than a thresholdS ,
which is a parameter of the algorithm ob-
tained from confusion matrix analysis. It can
be set to the highest confusion figures in the
off diagonal elements of the confusion ma-
trix.
</bodyText>
<listItem confidence="0.9860236">
3. Train sub-classifiers for each confusion do-
main i
D .
4. Determine the architecture of the model hav-
ing nm
</listItem>
<bodyText confidence="0.994461416666667">
N sub-classifiers. The superscript n
denote the index in the layer, while m de-
notes the layer depth in which this domain is
resolved. When a sub classifier is a complete
subset of another one, it is placed in a deeper
layer of the architecture. In this case, the m
superscript is incremented to denote extra
depth in the model.
Figure 3 CSR algorithm flow chart
TABLE II shows the confusion results for DNN
classifier (vertically: true, horizontally: pre-
dicted).
</bodyText>
<listItem confidence="0.930146">
1. Fatha, Damma, Kasra: {4,5,6}
</listItem>
<equation confidence="0.997838">
D 11 = 4
N 11
</equation>
<listItem confidence="0.77462525">
2. Fathten, Dammeten, Kasreten:
D21 = {1,2,3} 4 N21
3. Kasra, Kasreten: D12 = 4 12
{3,6} N
</listItem>
<bodyText confidence="0.96215875">
Each domain has its own nm
N classifier to re-
solve its confusion. The final model shall be as
shown in Figure 4.
</bodyText>
<page confidence="0.998382">
68
</page>
<tableCaption confidence="0.61424">
TABLE II CONFUSION MATRIX RESULTS FOR DNN CLASSIFIER ON SYNTACTIC DIACRITIZATION
</tableCaption>
<table confidence="0.966321555555556">
Fathten Dammeten Kasreten Fatha Damma Kasra Shadda Sukkun
Fathten 4762 2179 2455 336 389 197 0 120
Dammeten 2647 6976 2720 660 1144 408 0 231
Kasreten 4560 3378 32588 801 303 4868 0 951
Fatha 438 475 1458 92755 11671 8340 0 1980
Damma 262 727 579 5858 72994 14995 0 952
Kasra 59 184 3275 2682 3657 220357 0 1970
Shadda 2 78 86 51 75 0 416 4
Sukkun 3 128 271 1150 630 1565 0 73983
</table>
<figureCaption confidence="0.898337">
Figure 4 CSR model for syntactic Arabic diacritization task
</figureCaption>
<sectionHeader confidence="0.99822" genericHeader="method">
6 Features
</sectionHeader>
<bodyText confidence="0.999977773584906">
The input to text processing tasks is a tran-
script or document containing raw text. For
Arabic diacritization task specifically, a set of
features have proved good performance in litera-
ture, such as morphological lexemes, PoS, word
identity,...etc see Rashwan et al. (2009, 2011),
Zitouni et al. (2006) and Habash and Rambow
(2007). In this section the features employed in
our features vector are described.
Last character identity: case-ending diacriti-
zation is about adding diacritics on the last cha-
racter of the word. Arabic language prohibits
some diacritics from being placed over some
characters. For example fatha on”و” is phoneti-
cally forbidden. Also, it favors some diacritics
over some character like fatheten on “ا”. A rule
based system would have set a rule for that,
however, in DNN framework, the system is left
to learn such rules. Hence, the last character
identity is an effective feature for syntactic dia-
critization task.
The raw word identity: is another type of
possible features. The system proposed by
Rasshwan et al. (2009, 2011) uses this feature.
There are two possibilities of encoding such fea-
ture, the first would be to use a raw index
representing the word index from a vocabulary
vector built from training set. However, this
could lead to many out of vocabulary (OOV)
cases, in addition to long vector. On the other
hand, a word can be encoded as sequence of the
identities of its composing characters, which is
more efficient under the DNN framework to
avoid OOV, because even if a word is not en-
countered during training, at least a similar one
with a character less or more was encountered
during training, generating nearly similar activa-
tion of the stochastic binary units and leading to
similar result as the original word. The same ex-
act word need not be present during training
phase, instead only a similar word is enough so
that the word is not considered OOV. This is a
direct result of encoding words as sequence of
their constituting characters.
Context features: Considering the features
vector of the preceding and/or succeeding words
or characters can improve significantly the clas-
sification accuracy. This is what we refer to as
context features. Context features are essential to
syntactic and morphological diacritization tasks.
For morphological diacritization context is just
the surrounding characters, while for syntactic
diacritization context is represented by the sur-
</bodyText>
<page confidence="0.998508">
69
</page>
<bodyText confidence="0.999912448275862">
rounding words. We denote the depth of the pre-
ceding context by N and the succeeding context
elements by M.
Context class labels: The context does not on-
ly include the features’ vectors of inputs, it can
also include the context of class labels. For ex-
ample, the decision of the classifier for the pre-
vious diacritic can be re-considered as an input
feature for the current diacritic classification.
This results in something like an auto-regressive
model, where the previous decisions affect the
next one recursively
Part-of-Speech tags: are essential features to
discriminate syntactic diacritics cases, where
syntactic diacritics restoration is strongly related
to grammatically parsing and analyzing the sen-
tence into its syntactic language units or PoS.
There are many models for Arabic PoS tags. In
this work we adopt the one in Rashwan et al.
(2011), which sets 62 context-free atomic units
to represent all possible Arabic language PoS
tags. A very rich dataset of Arabic words, ex-
tracted from different sources, is used to train the
system (available on http://www.RDI-
eg.com/RDI/TrainingData is where to download
TRN_DB_II). PoS tags are manually annotated
for this dataset by expert Arabic linguistics. A
DNN is trained on this dataset to identify differ-
ent PoS tags.
</bodyText>
<sectionHeader confidence="0.998037" genericHeader="method">
7 Datasets
</sectionHeader>
<bodyText confidence="0.861902277777778">
In all the coming experiments one of the fol-
lowing datasets is used:
- TRN_DB_I: This is a 750,000 words dataset,
collected from different sources and manual-
ly annotated by expert linguistics with every
word PoS and Morphological quadruples.
- TRN_DB_II: This is 2500,000 words train
set.
- TST_DB: This is 11,000 words test data set.
For more information refer to Rashwan et al.
(2009, 2011).
- ATB: LDC Arabic Tree Bank.
For TRN_DB_I, PoS tags are available as ready fea-
tures added manually. When the manually PoS tags
are used as input features, the dataset is referred to as
TRN_DB_I – Ready PoS. While, when our PoS-DNN
nets are used, a derivative dataset with only raw text
is referred as TRN_DB_I – Raw text.
</bodyText>
<sectionHeader confidence="0.913543" genericHeader="method">
8 System evaluation
</sectionHeader>
<subsectionHeader confidence="0.994829">
8.1 Effect of CSR method
</subsectionHeader>
<bodyText confidence="0.9404732">
The objective of this experiment is to show the
effect of CSR method. The test set is TST_DB.
Results in TABLE III show improvement
around 2% in all tested datasets. This represents
17.09% improvement of error.
</bodyText>
<tableCaption confidence="0.429259">
TABLE III EFFECT OF CSR
</tableCaption>
<table confidence="0.617624">
Dataset Accuracy with CSR(%)
TRN_DB_I – 90.2 88.2
Ready PoS
TRN_DB_I – 88.2 86.2
Raw text
</table>
<subsectionHeader confidence="0.998233">
8.2 Effect of class context learning
</subsectionHeader>
<bodyText confidence="0.991669571428571">
The objective of this experiment is to evaluate
the effect of employing sequential class labels
model. Test set is TST_DB. The results in TA-
BLE IV show that employing this feature offers
1% to 2% improvement of accuracy over basic
DBN model alone. This represents 15.625% im-
provement of error.
</bodyText>
<figure confidence="0.6477565">
TABLE IV EFFECT OF CLASS LABELS
CONTEXT ON SYNTACTIC DIACRITIZA-
TION
Accuracy
without class
labels context
(%)
TRN_DB_I – Ready 88.3 87.2
PoS
TRN_DB_I – Raw 86.7 85.1
text
TRN_DB_I + 86.3 84.3
TRN_DB_II /
TST DB
</figure>
<subsectionHeader confidence="0.9937215">
8.3 Effect of last character feature for syn-
tactic case
</subsectionHeader>
<bodyText confidence="0.999891857142857">
The identity of the last character of a word is a
critical feature for syntactic diacritization task.
The dataset used for training is TRN_DB_I and
for testing TST_DB. TABLE V shows the effect
of utilizing this feature. A significant error im-
provement of about 4% is witnessed with this
new feature.
</bodyText>
<sectionHeader confidence="0.6682405" genericHeader="method">
TABLE V EFFECT OF LAST CHARACTER FEATURE
ON SYNTACTIC DIACRITIZATION
</sectionHeader>
<subsectionHeader confidence="0.337258">
Accuracy (%)
</subsectionHeader>
<bodyText confidence="0.657128">
With last character 88.2
Without last character 84.5
</bodyText>
<figure confidence="0.989336625">
Accuracy
without CSR
(%)
Accuracy
with
class labels
context (%)
Dataset
</figure>
<page confidence="0.98853">
70
</page>
<bodyText confidence="0.999850625">
Justification to this strong improvement is that;
Arabic language prohibits some diacritics from
being placed over some characters. For example
fatha on”و” is prohibited phonetically. Also, it
favors some diacritics over some character like
fatheten on “ا”. A rule based system would have
set a rule for that, however, in DNN framework,
the system is left to learn such rules.
</bodyText>
<subsectionHeader confidence="0.9584835">
8.4 Effect of character level encoding of the
word
</subsectionHeader>
<bodyText confidence="0.999803666666667">
The word identity is an important feature for
diacritization task. The dataset used for training
is TRN_DB_I and for testing TST_DB. TABLE
VI shows the effect of utilizing this feature. A
significant error improvement of about 2% is
witnessed with this feature.
</bodyText>
<figure confidence="0.3586216">
TABLE VI EFFECT OF CHARACTER LEVEL WORD
ENCODING ON SYNTACTIC DIACRITIZATION
Encoding Accuracy (%)
Word level 88.2
Character level 86.3
</figure>
<bodyText confidence="0.995518272727273">
“Word level” could lead to many out of voca-
bulary (OOV) cases, in addition to long vector.
On the other hand, “Character level” is more ef-
ficient under the DNN framework to avoid OOV
suffered in Rashwan et al. (2009, 2011), because
even if a word is not encountered during training,
but a similar one with a character less or more
was encountered, then a nearly similar activation
of the stochastic binary units would be generated,
leading to similar result to the most similar word
existing in training data set.
</bodyText>
<subsectionHeader confidence="0.987422">
8.5 Comparison to other systems
</subsectionHeader>
<bodyText confidence="0.99990715625">
The objective of this experiment is to evaluate
the performance of the proposed system for
Arabic diacritization versus the architecture in
Rashwan et al. (2009, 2011)., the MaxEnt model
proposed in Zitouni et al. (2006) and the MADA
system Habash and Rambow (2007). These sys-
tems represent the state of the art Arabic diacriti-
zation systems, with the best reported accuracy
in literature. The evaluation was done on all the
datasets as explained in Rashwan et al. (2011).
The PoS features are extracted using the DNN-
PoS tagger, since TRN_DB_II / TST_DB dataset
contains only raw text without ready PoS fea-
tures.
Results in TABLE VIII show that the proposed
system achieves improved performance by
around 1.2% over the system in 0Rashwan et al.
(2011), which represents 9.23% of the error, eva-
luated on the (TRN_DB_I + TRN_DB_II /
TST_DB) dataset. Also, on ATB standard data-
set, the proposed system achieves 0.9% im-
provement over the best result in literature using
the same training and testing data same as evalu-
ation in Rashwan et al. (2011) was done.
Another comparison is done when the dataset
TRN_DB_I is used with ready PoS features. Re-
sults in show that the proposed system achieves
better performance by 3.2% over the system in
Rashwan et al. (2011), which represents 24.6%
of the error. The importance of this experiment is
to isolate the automatic PoS tagging errors from
the evaluation.
</bodyText>
<sectionHeader confidence="0.582679" genericHeader="method">
TABLE VII COMPARISON TO HYBRID AR-
CHITECTURE WITH READY PoS FEATURES
</sectionHeader>
<figure confidence="0.975035939393939">
System Syntactical
accuracy (%)
Deep network + CSR 90.2
Hybrid Architecture 88.3
0Rashwan et al.
(2011)
TABLE VIII COMPARISON TO OTHER SYSTEMS
Deep network + CSR
(This paper)
TRN_DB_I +
TRN_DB_II / TST_DB
Hybrid Architecture
– Rashwan et al.
(2009, 2011)
TRN_DB_I +
TRN_DB_II / TST_DB
ATB
ATB
MaxEnt - Zitouni et
al. (2006)
ATB
MADA - Habash and
Rambow (2007)
Case-ending Morphological
accuracy (%) accuracy (%)
88.2 97
88.4 97
87 96.4
87.5 96.2
82 94.5
85.1 95.2
System Dataset
ATB
</figure>
<page confidence="0.990845">
71
</page>
<sectionHeader confidence="0.993443" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99996195">
In this paper the problem of Arabic diacritiza-
tion restoration is tackled under the deep learning
framework taking advantage of DBN model
training. As part of the proposed deep system, a
PoS tagger for Arabic transcript is proposed as
well using deep networks. The first contribution
is the introduction of the Confused Sub-set Reso-
lution (CSR) architecture to enhance the accura-
cy.
Design of features vector played a key role in
error improvement. Specifically, using features
like last character identity had valuable contribu-
tion to error improvement by about 4%. Includ-
ing class labels context features in auto-
regressive fashion has also good impact of 1.1%
on error improvement. Finally, encoding of word
as sequence of characters enables to reduce OOV
cases and enhance the accuracy.
CSR enables to purify the cross confusions be-
tween diacritics. A network-of-network architec-
ture formed of group of classifiers, each working
to resolve a set of confusions, is directly generat-
ed to enhance the overall accuracy by about 2%.
The identified confusions and the architecture go
smoothly with the grammatical and syntactical
rules of the Arabic language.
Evaluation of the proposed system is made on
two different datasets; custom and standard, both
available online to enable replicating the experi-
ments. Details of features vectors formatting and
the used features are presented to facilitate re-
sults re-generation. The standard LDC Arabic
Tree Bank dataset is used to bench mark the sys-
tem against the best three systems in literature,
showing that our system outperforms all pre-
viously published baselines. The effect of each
proposed method is presented separately. Results
show improvements ranging from 1.2% to 2.8%
over the best reported results representing 22%
improvement of the error.
</bodyText>
<sectionHeader confidence="0.99512" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.991250416666667">
Rashwan, Mohsen A A; Attia, Mohamed; Abdou,
Sherif M.; Abdou, S.; Rafea, Ahmed A. 2009. “A
Hybrid System for Automatic Arabic Diacritization”,
International Conference on Natural Language
Processing and Knowledge Engineering, pp 1-8, 24-
27.
Rashwan, Mohsen A A , Al-Badrashiny, Mohamed A
S A A; Attia, Mohamed; Abdou, Sherif M.; Rafea,
Ahmed A. 2011. “A Stochastic Arabic Diacritizer
Based on a Hybrid of Factorized and Unfactorized
Textual Features”, IEEE Transactions on Audio,
Speech, and Language Processing, vol. 19, issue 1, pp
166-175.
I. Zitouni; J. S. Sorensen; R. Sarikaya, 2006. “Maxi-
mum Entropy Based Restoration of Arabic Diacrit-
ics”, Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(ACL); Workshop on Computational Approaches to
Semitic Languages; Sydney-Australia
N. Habash; O. Rambo. 2007. “Arabic Diacritization
through Full Morphological Tagging”, Proceedings of
the 8th Meeting of the North American Chapter of the
Association for Computational Linguistics (ACL);
Human Language Technologies Conference (HLT-
NAACL).
G. E. Hinton; S. Osindero; Y. Teh, “A fast learning
algorithm for deep belief nets” Neural Computation,
vol. 18, pp. 1527–1554, 2006.
Ruslan Salakhutdinov. 2009.“Learning Deep Genera-
tive Models” PhD thesis, Graduate Department of
Computer Science, University of Toronto,
Raafat, H.; Rashwan, M.A.A. 1993. “A tree structured
neural network, Proceedings of the Second Interna-
tional Conference on Document Analysis and Recog-
nition” , pp. 939 – 941, ISBN: 0-8186-4960-7
Hai-Son Le ; Oparin, I. ; Allauzen, A. ; Gauvain, J.,
2011. “Structured Output Layer neural network lan-
guage model” 2011 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pp. 5524 - 5527, ISBN: 978-1-4577-0537-3
http://www.RDI-eg.com/RDI/TrainingData is where
to download TRN_DB_II.
http://www.RDI-eg.com/RDI/TestData is where to
download TST_DB
LDC Arabic Tree Bank Part 3,
http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?c
atalogId=LDC2005T20
</reference>
<page confidence="0.998077">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.230045">
<title confidence="0.999483">Automatic Arabic diacritics restoration based on deep nets</title>
<author confidence="0.999333">Mohsen A A Rashwan Ahmad A Al Sallab Hazem M Raafat Ahmed Rafea</author>
<affiliation confidence="0.83421875">Dept., Cairo Uni- Dept., Cairo Uni- Computer Science Computer Science versity, versity Kuwait Dept., versity University in Cairo mohsen_rashwan ahmad.elsallab hazem Rafea</affiliation>
<email confidence="0.968763">@rdi-eg.com@gmail.com@cs.ku.edu.kw@aucegypt.edu</email>
<abstract confidence="0.997178">In this paper, Arabic diacritics restoration problem is tackled under the deep learning framework presenting Confused Subset Resolution (CSR) method to improve the classification accuracy, in addition to Arabic Part-of-Speech (PoS) tagging framework using deep neural nets. Special focus is given to syntactic diacritization, which still suffer low accuracy as indicated by related works. Evaluation is done versus state-of-the-art systems reported in literature, with quite challenging datasets, collected from different domains. Standard datasets like LDC Arabic Tree Bank is used in addition to custom ones available online for results replication. Results show significant improvement of the proposed techniques over other approaches, reducing the syntactic classification error to 9.9% and morphological classification error to 3% compared to 12.7% and 3.8% of the best reported results in literature, improving the error by 22% over the best reported</abstract>
<intro confidence="0.67704">systems</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohsen A A Rashwan</author>
<author>Mohamed Attia</author>
<author>Sherif M Abdou</author>
<author>S Abdou</author>
<author>Ahmed A Rafea</author>
</authors>
<date>2009</date>
<booktitle>A Hybrid System for Automatic Arabic Diacritization”, International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2914" citStr="Rashwan et al. (2009" startWordPosition="435" endWordPosition="439">ation, spelling correction, word sense disambiguation,...etc, that require diacritizing the script as a pre-processing step before applying the core application technique. In its basic form, the problem can be reduced to a pattern classification problem, with seven diacritics classes being the targets. In addition, the diacritics classification can be divided into syntactical diacritization, caring about caseending and morphological diacritization, caring about the rest of the word diacritics. So far, morphological part of the problem is almost solved, leaving a marginal error of around 3-4%, Rashwan et al. (2009, 2011). On the other hand, syntactical diacritization errors are still high, hitting a ceiling that is claimed to be asymptotic and cannot be squeezed any further, Rashwan et al. (2009, 2011). For this reason, we focus our effort to squeeze this error beyond the least 12.5% error obtained in Rashwan et al. (2009, 2011). Recently, a significant advancement in the area of deep learning has been witnessed, with the development of a generative model; Deep Belief Nets (DBN), with a fast algorithm for inference of the model parameters. Deep Neural Networks (DNN) shall be the basic machine learning </context>
<context position="7572" citStr="Rashwan et al. (2009" startWordPosition="1163" endWordPosition="1166">. Focus will be around three works strongly related to what is proposed here, and having the best results in literature. Zitouni et al. (2006) apply Maximum Entropy classification to the problem taking the advantage of the MaxEnt framework to combine different features together, like lexical, segment-based, and PoS features. Segmentation involves getting the prefix, suffix, and stem of the word. PoS features are also generated under the MaxEnt framework. Habash and Rambow (2007) perform Morphological Analysis and Disambiguation of Arabic (MADA) system, and then apply SVM classification. Last, Rashwan et al. (2009 and 2011) propose a hybrid approach composed of two stages: first, maximum marginal probability via A* lattice search and n-grams probability estimation. When full-form words are OOV, the system switches to the second mode which factorizes each Arabic word into all its possible morphological constituents, then uses also the same techniques used by the first mode to get the most likely sequence of morphemes, hence the most likely diacritization. The latter system shall be our baseline, since it gives the best results in literature so far, and the dataset used to evaluate it is available at our</context>
<context position="15328" citStr="Rashwan et al. (2009" startWordPosition="2500" endWordPosition="2503">eten 2647 6976 2720 660 1144 408 0 231 Kasreten 4560 3378 32588 801 303 4868 0 951 Fatha 438 475 1458 92755 11671 8340 0 1980 Damma 262 727 579 5858 72994 14995 0 952 Kasra 59 184 3275 2682 3657 220357 0 1970 Shadda 2 78 86 51 75 0 416 4 Sukkun 3 128 271 1150 630 1565 0 73983 Figure 4 CSR model for syntactic Arabic diacritization task 6 Features The input to text processing tasks is a transcript or document containing raw text. For Arabic diacritization task specifically, a set of features have proved good performance in literature, such as morphological lexemes, PoS, word identity,...etc see Rashwan et al. (2009, 2011), Zitouni et al. (2006) and Habash and Rambow (2007). In this section the features employed in our features vector are described. Last character identity: case-ending diacritization is about adding diacritics on the last character of the word. Arabic language prohibits some diacritics from being placed over some characters. For example fatha on”و” is phonetically forbidden. Also, it favors some diacritics over some character like fatheten on “ا”. A rule based system would have set a rule for that, however, in DNN framework, the system is left to learn such rules. Hence, the last charact</context>
<context position="19161" citStr="Rashwan et al. (2009" startWordPosition="3122" endWordPosition="3125">lable on http://www.RDIeg.com/RDI/TrainingData is where to download TRN_DB_II). PoS tags are manually annotated for this dataset by expert Arabic linguistics. A DNN is trained on this dataset to identify different PoS tags. 7 Datasets In all the coming experiments one of the following datasets is used: - TRN_DB_I: This is a 750,000 words dataset, collected from different sources and manually annotated by expert linguistics with every word PoS and Morphological quadruples. - TRN_DB_II: This is 2500,000 words train set. - TST_DB: This is 11,000 words test data set. For more information refer to Rashwan et al. (2009, 2011). - ATB: LDC Arabic Tree Bank. For TRN_DB_I, PoS tags are available as ready features added manually. When the manually PoS tags are used as input features, the dataset is referred to as TRN_DB_I – Ready PoS. While, when our PoS-DNN nets are used, a derivative dataset with only raw text is referred as TRN_DB_I – Raw text. 8 System evaluation 8.1 Effect of CSR method The objective of this experiment is to show the effect of CSR method. The test set is TST_DB. Results in TABLE III show improvement around 2% in all tested datasets. This represents 17.09% improvement of error. TABLE III EFF</context>
<context position="22011" citStr="Rashwan et al. (2009" startWordPosition="3604" endWordPosition="3607">e word The word identity is an important feature for diacritization task. The dataset used for training is TRN_DB_I and for testing TST_DB. TABLE VI shows the effect of utilizing this feature. A significant error improvement of about 2% is witnessed with this feature. TABLE VI EFFECT OF CHARACTER LEVEL WORD ENCODING ON SYNTACTIC DIACRITIZATION Encoding Accuracy (%) Word level 88.2 Character level 86.3 “Word level” could lead to many out of vocabulary (OOV) cases, in addition to long vector. On the other hand, “Character level” is more efficient under the DNN framework to avoid OOV suffered in Rashwan et al. (2009, 2011), because even if a word is not encountered during training, but a similar one with a character less or more was encountered, then a nearly similar activation of the stochastic binary units would be generated, leading to similar result to the most similar word existing in training data set. 8.5 Comparison to other systems The objective of this experiment is to evaluate the performance of the proposed system for Arabic diacritization versus the architecture in Rashwan et al. (2009, 2011)., the MaxEnt model proposed in Zitouni et al. (2006) and the MADA system Habash and Rambow (2007). Th</context>
<context position="24056" citStr="Rashwan et al. (2009" startWordPosition="3945" endWordPosition="3948">aset TRN_DB_I is used with ready PoS features. Results in show that the proposed system achieves better performance by 3.2% over the system in Rashwan et al. (2011), which represents 24.6% of the error. The importance of this experiment is to isolate the automatic PoS tagging errors from the evaluation. TABLE VII COMPARISON TO HYBRID ARCHITECTURE WITH READY PoS FEATURES System Syntactical accuracy (%) Deep network + CSR 90.2 Hybrid Architecture 88.3 0Rashwan et al. (2011) TABLE VIII COMPARISON TO OTHER SYSTEMS Deep network + CSR (This paper) TRN_DB_I + TRN_DB_II / TST_DB Hybrid Architecture – Rashwan et al. (2009, 2011) TRN_DB_I + TRN_DB_II / TST_DB ATB ATB MaxEnt - Zitouni et al. (2006) ATB MADA - Habash and Rambow (2007) Case-ending Morphological accuracy (%) accuracy (%) 88.2 97 88.4 97 87 96.4 87.5 96.2 82 94.5 85.1 95.2 System Dataset ATB 71 9 Conclusion In this paper the problem of Arabic diacritization restoration is tackled under the deep learning framework taking advantage of DBN model training. As part of the proposed deep system, a PoS tagger for Arabic transcript is proposed as well using deep networks. The first contribution is the introduction of the Confused Sub-set Resolution (CSR) arc</context>
</contexts>
<marker>Rashwan, Attia, Abdou, Abdou, Rafea, 2009</marker>
<rawString>Rashwan, Mohsen A A; Attia, Mohamed; Abdou, Sherif M.; Abdou, S.; Rafea, Ahmed A. 2009. “A Hybrid System for Automatic Arabic Diacritization”, International Conference on Natural Language Processing and Knowledge Engineering, pp 1-8, 24-27.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rashwan</author>
</authors>
<journal>Mohsen A A , Al-Badrashiny, Mohamed A S A A; Attia, Mohamed; Abdou, Sherif M.; Rafea,</journal>
<marker>Rashwan, </marker>
<rawString>Rashwan, Mohsen A A , Al-Badrashiny, Mohamed A S A A; Attia, Mohamed; Abdou, Sherif M.; Rafea,</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ahmed</author>
</authors>
<title>A Stochastic Arabic Diacritizer Based on a Hybrid of Factorized and Unfactorized Textual Features”,</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<pages>166--175</pages>
<marker>Ahmed, 2011</marker>
<rawString>Ahmed A. 2011. “A Stochastic Arabic Diacritizer Based on a Hybrid of Factorized and Unfactorized Textual Features”, IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, issue 1, pp 166-175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>J S Sorensen</author>
<author>R Sarikaya</author>
</authors>
<title>Maximum Entropy Based Restoration of Arabic Diacritics”,</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL); Workshop on Computational Approaches to Semitic Languages; Sydney-Australia</booktitle>
<contexts>
<context position="7094" citStr="Zitouni et al. (2006)" startWordPosition="1092" endWordPosition="1095">ed. The next section describes the system evaluation experiments. Experimental results include an error analysis study of the effect of each feature and method on the system performance, in addition to benchmarking against state-of-the art systems in literature, evaluated on standard datasets. Finally, the paper is concluded with the main results and conclusion. 2 Related work There have been many attempts to approach the Arabic diacritization problem by different techniques. Focus will be around three works strongly related to what is proposed here, and having the best results in literature. Zitouni et al. (2006) apply Maximum Entropy classification to the problem taking the advantage of the MaxEnt framework to combine different features together, like lexical, segment-based, and PoS features. Segmentation involves getting the prefix, suffix, and stem of the word. PoS features are also generated under the MaxEnt framework. Habash and Rambow (2007) perform Morphological Analysis and Disambiguation of Arabic (MADA) system, and then apply SVM classification. Last, Rashwan et al. (2009 and 2011) propose a hybrid approach composed of two stages: first, maximum marginal probability via A* lattice search and</context>
<context position="15358" citStr="Zitouni et al. (2006)" startWordPosition="2505" endWordPosition="2508">408 0 231 Kasreten 4560 3378 32588 801 303 4868 0 951 Fatha 438 475 1458 92755 11671 8340 0 1980 Damma 262 727 579 5858 72994 14995 0 952 Kasra 59 184 3275 2682 3657 220357 0 1970 Shadda 2 78 86 51 75 0 416 4 Sukkun 3 128 271 1150 630 1565 0 73983 Figure 4 CSR model for syntactic Arabic diacritization task 6 Features The input to text processing tasks is a transcript or document containing raw text. For Arabic diacritization task specifically, a set of features have proved good performance in literature, such as morphological lexemes, PoS, word identity,...etc see Rashwan et al. (2009, 2011), Zitouni et al. (2006) and Habash and Rambow (2007). In this section the features employed in our features vector are described. Last character identity: case-ending diacritization is about adding diacritics on the last character of the word. Arabic language prohibits some diacritics from being placed over some characters. For example fatha on”و” is phonetically forbidden. Also, it favors some diacritics over some character like fatheten on “ا”. A rule based system would have set a rule for that, however, in DNN framework, the system is left to learn such rules. Hence, the last character identity is an effective fe</context>
<context position="22562" citStr="Zitouni et al. (2006)" startWordPosition="3694" endWordPosition="3697"> under the DNN framework to avoid OOV suffered in Rashwan et al. (2009, 2011), because even if a word is not encountered during training, but a similar one with a character less or more was encountered, then a nearly similar activation of the stochastic binary units would be generated, leading to similar result to the most similar word existing in training data set. 8.5 Comparison to other systems The objective of this experiment is to evaluate the performance of the proposed system for Arabic diacritization versus the architecture in Rashwan et al. (2009, 2011)., the MaxEnt model proposed in Zitouni et al. (2006) and the MADA system Habash and Rambow (2007). These systems represent the state of the art Arabic diacritization systems, with the best reported accuracy in literature. The evaluation was done on all the datasets as explained in Rashwan et al. (2011). The PoS features are extracted using the DNNPoS tagger, since TRN_DB_II / TST_DB dataset contains only raw text without ready PoS features. Results in TABLE VIII show that the proposed system achieves improved performance by around 1.2% over the system in 0Rashwan et al. (2011), which represents 9.23% of the error, evaluated on the (TRN_DB_I + T</context>
<context position="24132" citStr="Zitouni et al. (2006)" startWordPosition="3959" endWordPosition="3962">posed system achieves better performance by 3.2% over the system in Rashwan et al. (2011), which represents 24.6% of the error. The importance of this experiment is to isolate the automatic PoS tagging errors from the evaluation. TABLE VII COMPARISON TO HYBRID ARCHITECTURE WITH READY PoS FEATURES System Syntactical accuracy (%) Deep network + CSR 90.2 Hybrid Architecture 88.3 0Rashwan et al. (2011) TABLE VIII COMPARISON TO OTHER SYSTEMS Deep network + CSR (This paper) TRN_DB_I + TRN_DB_II / TST_DB Hybrid Architecture – Rashwan et al. (2009, 2011) TRN_DB_I + TRN_DB_II / TST_DB ATB ATB MaxEnt - Zitouni et al. (2006) ATB MADA - Habash and Rambow (2007) Case-ending Morphological accuracy (%) accuracy (%) 88.2 97 88.4 97 87 96.4 87.5 96.2 82 94.5 85.1 95.2 System Dataset ATB 71 9 Conclusion In this paper the problem of Arabic diacritization restoration is tackled under the deep learning framework taking advantage of DBN model training. As part of the proposed deep system, a PoS tagger for Arabic transcript is proposed as well using deep networks. The first contribution is the introduction of the Confused Sub-set Resolution (CSR) architecture to enhance the accuracy. Design of features vector played a key ro</context>
</contexts>
<marker>Zitouni, Sorensen, Sarikaya, 2006</marker>
<rawString>I. Zitouni; J. S. Sorensen; R. Sarikaya, 2006. “Maximum Entropy Based Restoration of Arabic Diacritics”, Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL); Workshop on Computational Approaches to Semitic Languages; Sydney-Australia</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambo</author>
</authors>
<title>Arabic Diacritization through Full Morphological Tagging”,</title>
<date>2007</date>
<booktitle>Proceedings of the 8th Meeting of the North American Chapter of the Association for Computational Linguistics (ACL); Human Language Technologies Conference (HLTNAACL).</booktitle>
<marker>Habash, Rambo, 2007</marker>
<rawString>N. Habash; O. Rambo. 2007. “Arabic Diacritization through Full Morphological Tagging”, Proceedings of the 8th Meeting of the North American Chapter of the Association for Computational Linguistics (ACL); Human Language Technologies Conference (HLTNAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>S Osindero</author>
<author>Y Teh</author>
</authors>
<title>A fast learning algorithm for deep belief nets”</title>
<date>2006</date>
<journal>Neural Computation,</journal>
<volume>18</volume>
<pages>1527--1554</pages>
<contexts>
<context position="10059" citStr="Hinton et al. (2006)" startWordPosition="1584" endWordPosition="1587"> word. The framework in Figure 2 is employed. Three layers network architecture is used for each features extraction subnet or classification network. For the classification network a 20-20-20 architecture was used, while for PoS-tagging networks a 60-60-60 is used. The network architecture is determined empirically. By experiments it was found that the best architecture is the symmetric one, with the same width for all layers. The best width is found to be the same as the average number of ones in the training set features vectors. The neural network training undergoes DBN pre training as in Hinton et al. (2006) for 20 epochs per layer, with batch size of 1000 examples each without mini batches. Momentum is used initially with 0.5 for the first 5 epochs and then raised to 0.9 for the next epochs. The discriminative fine tuning is performed using conjugate gradient minimization for 30 epochs. For the first 6 epochs, only the upper layer is adjusted, then the rest of the layers are trained for the next epochs. Once the features are ready of a certain raw word it is fed to the DNN classifier. The resulting confusion matrix from the training phase is then fed to the CSR method to generate the tree struct</context>
<context position="11693" citStr="Hinton et al. (2006)" startWordPosition="1862" endWordPosition="1865">blem. The target classes shall be the diacritics themselves, described in TABLE I. The input is the raw MSA transcript. The task is to classify the input based on well-designed features’ vector and restore the original diacritics of the raw text. The output shall be the full diacritized text. All these diacritics can exist on case-ending character, while Fathten, Dammeten and Kasreten can never occur on non-ending character of the word root. The machine learning classifier tool chosen in this paper is the Deep Neural Network (DNN), under the framework of learning deep architecture proposed by Hinton et al. (2006). The raw text is presented to the classifier, and a group of sub-nets work to extract the desired features, like PoS tags. The network architecture is shown in Figure 2. Each sub-net is trained to extract a certain kind of features, and the obtained features’ vectors are concatenated together to form the input that is represented to the classifier network. In fact the training of features extraction nets is guided by certain desired features, like PoS tags. TABLE I ARABIC DIACRITICS CLASSES Diacritics form Class name Pronunciation on Arabic letter ب بَ Fatha e_;a /a/ بُ Damma Z-.;- /u/ ِب Kas</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>G. E. Hinton; S. Osindero; Y. Teh, “A fast learning algorithm for deep belief nets” Neural Computation, vol. 18, pp. 1527–1554, 2006.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>2009.“Learning Deep Generative Models”</title>
<tech>PhD thesis,</tech>
<institution>Graduate Department of Computer Science, University of Toronto,</institution>
<marker>Salakhutdinov, </marker>
<rawString>Ruslan Salakhutdinov. 2009.“Learning Deep Generative Models” PhD thesis, Graduate Department of Computer Science, University of Toronto,</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Raafat</author>
<author>M A A Rashwan</author>
</authors>
<title>A tree structured neural network,</title>
<date>1993</date>
<booktitle>Proceedings of the Second International Conference on Document Analysis and Recognition” ,</booktitle>
<pages>939--941</pages>
<contexts>
<context position="12699" citStr="Raafat and Rashwan (1993)" startWordPosition="2033" endWordPosition="2036">of features extraction nets is guided by certain desired features, like PoS tags. TABLE I ARABIC DIACRITICS CLASSES Diacritics form Class name Pronunciation on Arabic letter ب بَ Fatha e_;a /a/ بُ Damma Z-.;- /u/ ِب Kasra ةjs /i/ ً�� Fathten C,:�;&apos; /an/ بٌ Dammeten /un/ C ٍب Kasreten ������ /in/ بْ Sukun ن-4- No vowel بّ Shadda ةّ� Double consonant 67 This enables building a standalone system that operates on the raw text only. Figure 2 Deep network framework 5 Confused sub-set resolution method The Confused Sub-Classes Resolution (CSR) is based on confusion matrix analysis and the method by Raafat and Rashwan (1993). The output of this analysis shall be a network architecture composed of the original classifier operating with sub-classifiers to resolve confusions that were identified through confusion matrix analysis. The method starts with training a global classifier, then evaluating its performance. To enhance its accuracy, the sources of errors are analyzed by building the confusion matrix for the training set. The position of the off diagonal element identifies the pair of classes that are confused together. 5.1 Algorithm The flow chart of the CSR method is shown in Figure 3. The following steps des</context>
</contexts>
<marker>Raafat, Rashwan, 1993</marker>
<rawString>Raafat, H.; Rashwan, M.A.A. 1993. “A tree structured neural network, Proceedings of the Second International Conference on Document Analysis and Recognition” , pp. 939 – 941, ISBN: 0-8186-4960-7</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Allauzen</author>
</authors>
<title>Structured Output Layer neural network language model”</title>
<date>2011</date>
<booktitle>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<pages>5524--5527</pages>
<marker>Allauzen, 2011</marker>
<rawString>Hai-Son Le ; Oparin, I. ; Allauzen, A. ; Gauvain, J., 2011. “Structured Output Layer neural network language model” 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5524 - 5527, ISBN: 978-1-4577-0537-3 http://www.RDI-eg.com/RDI/TrainingData is where to download TRN_DB_II.</rawString>
</citation>
<citation valid="false">
<title>http://www.RDI-eg.com/RDI/TestData is where to download TST_DB LDC Arabic Tree Bank Part 3, http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?c atalogId=LDC2005T20</title>
<marker></marker>
<rawString>http://www.RDI-eg.com/RDI/TestData is where to download TST_DB LDC Arabic Tree Bank Part 3, http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?c atalogId=LDC2005T20</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002721">
<title confidence="0.898046">
Unsupervised Word Segmentation Improves
Dialectal Arabic to English Machine Translation
</title>
<author confidence="0.933969">
Kamla Al-Mannai1, Hassan Sajjad1, Alaa Khader2, Fahad Al Obaidli1,
Preslav Nakov1, Stephan Vogel1
</author>
<affiliation confidence="0.976884">
Qatar Computing Research Institute1, Carnegie Mellon University in Qatar2
</affiliation>
<email confidence="0.95986">
{kamlmannai,hsajjad,faalobaidli,pnakov,svogel}@qf.org.qa1, akhader@cmu.edu2
</email>
<sectionHeader confidence="0.994025" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998171875">
We demonstrate the feasibility of using
unsupervised morphological segmentation
for dialects of Arabic, which are poor in
linguistics resources. Our experiments us-
ing a Qatari Arabic to English machine
translation system show that unsupervised
segmentation helps to improve the transla-
tion quality as compared to using no seg-
mentation or to using ATB segmentation,
which was especially designed for Mod-
ern Standard Arabic (MSA). We use MSA
and other dialects to improve Qatari Ara-
bic to English machine translation, and we
show that a uniform segmentation scheme
across them yields an improvement of 1.5
BLEU points over using no segmentation.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99719365625">
The Arabic language has many varieties, where
the Modern Standard Arabic (MSA) coexists with
various dialects. Dialects differ from MSA and
from each other lexically, phonologically, mor-
phologically and syntactically. MSA has stan-
dard orthography and is used in formal contexts
(e.g., publications, newspaper articles, etc.), while
the dialects are usually limited to daily verbal in-
teractions. However, with the recent rise of social
media, it has become increasingly common to use
dialects in written communication as well, which
has constituted the research in dialectal Arabic
(DA) as a separate field within the broader field
of natural language processing (NLP).
As DA NLP is still in its infancy, there is lack
of basic computational resources and tools, which
are needed in order to apply standard NLP ap-
proaches to the dialects of Arabic. For instance,
statistical approaches need a lot of training data,
which makes it very hard, if not impossible, to
apply them to resource-poor languages; this is
especially true for statistical machine translation
(SMT) of Arabic dialects.
The Arabic language and its dialects are highly
inflectional, and a word can appear in many more
inflected forms compared to English. Consider the
Arabic words `,.J.ªË ,I. ªÊK� ,I. ªÊ�K, and �ñJ.ªÊK�: they
all belong to one root word I. ªË ‘playing’ /lEb/.
Each morphological variation is derived from a
root word with different affixes addressing differ-
ent functions. This causes data sparseness, and
covering all possible word forms of a root word
may not be always possible. Considering the dif-
ferent variants of Arabic, the problem is exacer-
abated as dialects could use different choices of af-
fixes for the same function. For example, the MSA
word vñJ.ªÊK� /yalEabuwn/, meaning ‘they are play-
ing’, could be found as uñJ.ªÊK� /ylEbuwn/ in Gulf,
as @ñJ.ªÊK
Ñ« /Eam yilEabuA/ in Levantine, and as
@ñJ.ªÊJ�K. /biylEabwA/ in Egyptian Arabic.
One possible solution is to use a morphological
segmenter that segments words into simpler units
such as stems and affixes, which might be covered
in the training set (Zollmann et al., 2006; Tsai et
al., 2010). When applied to dialects, this may re-
duce the lexical gap between dialects and MSA by
matching the common stems. Unfortunately, there
are no standard morphological segmentation tools
for dialects. Due to the difference in morphology,
tools designed for MSA do not work well for di-
alects. Developing rule-based segmenters for each
dialect might appear to be the ideal solution, but,
as the orthography of dialects is not standardized,
crafting linguistic rules for them is very hard.
In this paper, we focus on training an unsuper-
vised model for word segmentation, which we ap-
ply to SMT for a given Arabic dialect. We train a
pre-existing unsupervised segmentation model on
the Arabic side of the training bi-text (and on some
other monolingual data), and then we optimize its
parameters based on the resulting SMT quality.
Similarly, a multi-dialectal word segmenter could
be developed by training on multi-dialectal data.
</bodyText>
<page confidence="0.968163">
207
</page>
<note confidence="0.887834">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 207–216,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999950333333333">
In particular, we develop a Qatari Arabic to En-
glish (QA-EN) SMT system, which we train on a
small pre-existing bi-text. As part of the devel-
opment of the unsupervised segmentation model,
we also collected some additional monolingual
data for Qatari Arabic. Qatari Arabic is a subdi-
alect of the more general Gulf dialect, among with
Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we
collected additional monologual data for each of
these subdialects, and we release this data to the
research community.
We train an unsupervised segmentation tool,
Morphessor, and its MAP model (Creutz and La-
gus, 2007), using different variations of the col-
lected Qatari data. We optimize the single hy-
perparameter of the MAP model by maximizing
the translation quality of the QA-EN SMT sys-
tem in terms of BLEU. Our experimental results
demonstrate that the resulting unsupervised seg-
menter yields improvements in translation quality
when compared to (i) using no segmentation and
(ii) using an MSA-based ATB segmenter.
We further develop a multi-dialectal word seg-
mentation model, which we train on the Arabic
side of the multi-dialectal training data, which
consists of Qatari Arabic, Egyptian Arabic (EGY),
Levantine Arabic (LEV) and MSA to English,
i.e., a scaled combination of all the available par-
allel data. We train a QA-EN SMT system using
the segmented multi-dialectal data, and we show
an absolute gain of 1.5 BLEU points compared to
a baseline that uses no segmentation.
The rest of the paper is organized as follows:
First, we provide an overview of related work on
Dialectal Arabic NLP (Section 2). Next, we dis-
cuss and we illustrate the linguistic differences be-
tween different Arabic dialects in comparison with
and with a focus on Qatari Arabic (Section 3).
Then, we provide statistics about the corpora we
collected and used in our experiments, followed by
an illustration of the orthographic normalization
schemes we applied (Section 4). We next provide
a high-level description of our approach, which
uses morphological segmentation to combine re-
sources for other Arabic dialects in a QA-EN SMT
system effectively (Section 4.3). We also explain
our experimental setup and we present the results
(Section 5). We then discuss translating in the
reverse direction, i.e., into Qatari Arabic (Section
6). Finally, we point to possible directions for fu-
ture work and we conclude the paper (Section 7).
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996908791666667">
NLP for DA is still in its early stages of develop-
ment and many challenges need to be overcomed
such as the lack of suitable tools and resources.
Collecting resources for dialectal Arabic:
Several researchers have directed efforts to de-
velop DA computational resources (Maamouri et
al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and
Callison-Burch, 2011; Salama et al., 2014). Zbib
et al. (2012) built two dialectal Arabic-English
parallel corpora for Egyptian and Levantine Ara-
bic using crowdsourcing. Bouamor et al. (2014)
presented a multi-dialectal Arabic parallel corpus,
which covers five Arabic dialects besides MSA
and English. Mubarak and Darwish (2014) col-
lected a multi-dialectal corpus using Twitter. Un-
like previous work, we focus on Gulf subdialects,
particularly Qatari Arabic. The monolingual data
that we collected is a high-quality dialectal re-
source and originates from dialect-specific sources
such as novels and forums.
Adapting SMT resources for other Arabic di-
alects: Many researchers have explored the po-
tential of using MSA as a pivot language for im-
proving SMT of Arabic dialects (Bakr et al., 2008;
Sawaf, 2010; Salloum and Habash, 2011; Sajjad et
al., 2013a; Jeblee et al., 2014). This often involves
DA-MSA conversion schemes as an alternative in
the absence of DA-MSA parallel resources. In
contrast, limited work has been done on lever-
aging available resources for other dialects. Re-
cently, Zbib et al. (2012) have shown that using
a small amount of dialectal data could yield great
improvements for SMT. Here, we investigate the
potential of improving the resource adaptability of
Arabic dialects. Our work is different as we use
an unsupervised segmenter that helps in improv-
ing the lexical overlap between dialects and MSA.
Building morphological segmenters for the
Arabic dialects: Researchers have already fo-
cused efforts on crafting and extending existing
MSA tools to DA by mainly using a set of rules
(Habash et al., 2012). Habash and Rambow
(2006) presented MAGEAD, a knowledge-based
morphological analyzer and generator for Egyp-
tian and Levantine Arabic. Chiang et al. (2006)
developed a Levantine morphological analyzer on
top of an existing MSA analyzer using an explicit
knowledge base.
</bodyText>
<page confidence="0.996983">
208
</page>
<bodyText confidence="0.999781428571429">
Riesa and Yarowsky (2006) trained a supervised
trie-based model using a small lexicon of dialec-
tal affixes. In our work, we eliminate the need
for linguistic knowledge by training an unsuper-
vised model using available resources. The unsu-
pervised mode of learning allowed us to develop a
multi-dialectal morphological segmenter.
</bodyText>
<sectionHeader confidence="0.988244" genericHeader="method">
3 Arabic Dialects
</sectionHeader>
<bodyText confidence="0.999840666666667">
In this section, we highlight some of the linguis-
tic differences between Arabic dialects and MSA,
with a focus on the Qatari dialect.
</bodyText>
<subsectionHeader confidence="0.999119">
3.1 Phonological Variations
</subsectionHeader>
<bodyText confidence="0.993022913793103">
The Gulf dialect often preserves the phonological
representation of MSA, which is not the case with
many other Arabic dialects. For example, in Egyp-
tian (EGY) and in some Levantine (LEV) dialects,
the MSA consonants H~ /v/, †~ /q/, and X� /*/ are
realized as H~ /t/, glottal stop /’/, and /Z/, re-
spectively. While, their MSA pronunciations are
preserved in Gulf Arabic.
In Gulf Arabic, there are some phonological dif-
ferences between countries such as Kuwait (KW),
Saudi Arabia (SA), Bahrain (BH), Qatar (QA),
United Arab Emirates (AE), and Oman (OM).
Here, we focus our discussion on Qatari Arabic,
and we compare it to MSA and other dialects.
The QA dialect borrows two Persian characters
namely h~/J/ and ¬~ /V/. For instance, the MSA
letter h./j/ is converted to /J/ in QA, e.g., ¨AÒ2--.@
‘meeting’ is pronounced as /&lt;jtimAE/ in MSA
and /&lt;JtimAE/ in QA. The Persian character h~/J/
is also used in place of 1/4 /k/ in some MSA words
when they are used in QA. For example, 1/2ÖÞ... ‘fish’
/samak/ is pronounced i~ ÖÞ... /smaJ/ in QA, while
the EGY and the LEV dialects maintain the MSA
pronunciation. The Persian ¬~ /V/ is used to map
the sound of the English letter ’v’ in borrowed for-
eign words, e.g., ñK�YJ�s ‘video’ is pronounced as
ñK�YJ�¯ /Viydyw/ as opposed to /fiydywu/; the form
�
in which it is written in MSA.
The MSA consonant • /D/ is not used in the
QA dialect. It is substituted by /Z/ in Qatari. For
example, the MSA pronunciation /HaD/ of L�&amp;quot;
‘to encourage’ is transformed to Jy /HaZ/ in QA,
but it is maintained in EGY.
Meanwhile, the MSA consonant /Z/ is re-
alized as /D/ in EGY. For example, the MSA
pronunciation /HaZ/ of Jy ‘luck’ is maintained
in QA and transformed to /HaD/ in EGY. This
change is consistent in all words within each di-
alect. However, such phonological variations be-
tween dialects have the potential to add ambiguity
to dialectal Arabic.
The MSA consonant h./j/ can be used to distin-
guish between different dialects, particularly Gulf
subdialects. h./j/ is pronounced as ø
/y/ in KW,
BH, QA, AE, †~ /q/ in OM, much like in EGY,
and h./j/ in SA, much like in LEV. For exam-
ple, the MSA word Yj. ‚Ó ‘mosque’ /masjid/ is
pronounced as /masjid/ in MSA, SA, LEV, Y&amp;quot;®‚Ó
/masqid/ in OM, EGY, YJ
‚Ó /masyid/ in KW,
BH, QA, AE, while the MSA pronunciation is
preserved in SA. This change does not apply to
names. However, we should note that it is not con-
sistent in QA, e.g., the MSA pronunciation of h./j/
in ÉJ.k. ‘mountain’ /jabal/ and h. QK. ‘tower’ /burj/ is
preserved in QA.
</bodyText>
<subsectionHeader confidence="0.999584">
3.2 Morphological Variations
</subsectionHeader>
<bodyText confidence="0.986055695652174">
In Arabic, a root can produce surface wordforms
by means of inflectional and derivational morpho-
logical processes (Habash, 2010).
An inflectional word form is a variant of a root
word with the same meaning but expressing a dif-
ferent function, e.g., gender, number, case. It is
usually formed by adding a prefix, a suffix, or a
circumfix to a stem word. Note that Arabic di-
alects can make different lexical choices for affix-
ations compared to MSA. For example, the MSA
future prefix € /s/ is replaced by H. /b/ in QA
and by -ë /h/ in EGY and LEV. Thus, the MSA
word É¿�AJ�ƒ ‘he will eat’ /say&gt;kul/ becomes É¿AJ
K.
/biyAkil/ in QA and É¿AJ�ë /hayAkul/ in EGY and
LEV.
A derivational word form is formed by applying
a pattern to a root word, e.g., ‘player’ is derived
from ‘play’ using the pattern noun + ‘er’. An
example of an Arabic derivational form is Éª�®~K
‘do’ /tafaE˜al/. The root is Éª ¯ /faEal/ and it uses
the imperative pattern &amp;quot;+Éª ¯. In EGY, @ /A/ is
added as a prefix; so, it becomes Éª�®~K@ /AitfaE˜il/.
</bodyText>
<page confidence="0.992573">
209
</page>
<bodyText confidence="0.999808571428571">
Meanwhile, the original form is preserved in QA.
Changing the structure of a pattern in a dialect
will result in producing a new dialect-specific or-
thography for every word that is represented by the
structure. For example, the MSA word ÕÎª~K ‘learn’
/taEal˜am/ becomes ÕÎª~K@ /AitEalim/ in EGY, while
the MSA form is preserved in QA.
</bodyText>
<subsectionHeader confidence="0.999431">
3.3 Lexical Variations
</subsectionHeader>
<bodyText confidence="0.9996494">
Lexical variations are among the most obvious
differences between Arabic dialects. For exam-
ple, the MSA word @�XAÓ ‘what’ /mA*A/ would be
found as ñƒ: /$uw/ in LEV, éK�@� /&lt;yh/ in EGY, and
ñJƒ
�~ /$nuw/ in GLF. We can find lexical variations
in subdialects as well. For example, the MSA
negation word J /lan/, ‘not’, is expressed as I. Ó
/mab/ in QA, asñÓ /muw/ in KW, and as I. êÓ /ma-
hab/ in SA.
</bodyText>
<subsectionHeader confidence="0.974159">
3.4 Orthographic Variations
</subsectionHeader>
<bodyText confidence="0.999520157894737">
Due to the lack of orthographic standardization of
dialectal Arabic, some MSA words can be found
in dialectal text with both MSA and phonologi-
cal spellings. For example, the MSA word L.Ôg.
‘gathering’ /jamEap/ can be also spelled as éªÖß�
/yamEah/, which is a phonetic variation in QA.
Some dialectal words also vary in spelling due
�
to variation in their pronunciation, e.g., ¬ñ :ƒ @
/A$uwf/, a QA word meaning ‘I see’, can be also
spelled as vñk.@ /Ajuwf/.
In dialectal Arabic, different orthographic
forms are also possible for entire phrases. For
instance, words followed or preceded by pro-
nouns are commonly reduced to a single word,
e.g., AêË `Êi /glt lahA/ ‘I told her’ is written as
AêÊ�JÊ . Also, commonly used religious phrases can
be found written as a single unit, e.g., é&lt;Ë@ ZAƒ: AÓ
/mA $A’ A˜lah/ ‘God has willed it’ as é&lt;ËA :Ó.
</bodyText>
<sectionHeader confidence="0.998668" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.999488">
In the section, we present some statistics about the
Arabic dialectal data that we have collected. We
processed it to remove orthographic inconsisten-
cies. Then, we used a pre-existing unsupervised
morphological segmenter, Morfessor, in order to
segment the text.
</bodyText>
<table confidence="0.977494666666667">
Corpus QCA AVIAQA AVIAO
Sents 14.7 0.9 2
Tokens 115 6.7 15
</table>
<tableCaption confidence="0.756837333333333">
Table 1: Statistics about the collected parallel cor-
pora (in thousands). AVIAO shows the statistics
about the AVIA corpus excluding Qatari data.
</tableCaption>
<subsectionHeader confidence="0.994657">
4.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.9996918">
We did an extensive search for available monolin-
gual and bilingual resources for the Gulf dialect,
with a focus on Qatari Arabic. Tables 1 and 2
present some statistics about the corpora we col-
lected. More detailed description follows below.
</bodyText>
<subsectionHeader confidence="0.648505">
Bilingual corpora:
</subsectionHeader>
<bodyText confidence="0.999873448275862">
– The QCA speech corpus, comprises 14.7k
sentences that are phonetically transcribed from
TV broadcasts in Qatari Arabic and translated to
English; see (Elmahdy et al., 2014) for more de-
tail. The corpus was designed for speech recog-
nition and we faced several normalization-related
issues that we had to resolve before it could be
used for machine translation and language mod-
eling. One example is the usage of five Per-
sian characters to represent some sounds in Ara-
bic words. Moreover, the English side had some
grammatical and spelling errors. We normalized
the Arabic side and corrected the English side of
the corpus as described in Section 4.2. The cor-
pus can be found at http://sprosig.isle.
illinois.edu/corpora/1.
– The AVIA corpus1 is designed as a refer-
ence source of dialectal Arabic. It consists of 3k
sentences in four Gulf subdialects: Emirati (AE),
Kuwaiti (KW), Qatari (QA), and Hejazi (SA).2
The data consists of dialectal sentences that con-
tain words commonly used in daily conversation.
Monolingual corpora: We further collected
monolingual corpora consisting of a total of 2.7M
tokens for various Gulf subdialects. The Qatari
part of the data consists of 470K tokens. Most of
the corpus is a collection of novels, belonging to
the romance genre.3 For the Qatari dialect, we also
collected Qatari forum data.4
</bodyText>
<footnote confidence="0.994237285714286">
1http://terpconnect.umd.edu/˜nlynn/
AVIA/Level3/
2The website also contains small parallel corpora for
MSA, EGY and LEV to English, but here we focus on Gulf
subdialects only.
3http://forum.te3p.com/264311-52.html
4www.qatarshares.com/vb/index.php
</footnote>
<page confidence="0.983814">
210
</page>
<table confidence="0.97346225">
Corpus Novel Forum
AE BH KW OM QA SA QA
Tokens 573 244 178 372 412 614 69
Types 43 22 27 27 43 71 15
</table>
<tableCaption confidence="0.9341555">
Table 2: Statistics about the collected monolingual
corpora (in thousands of words).
</tableCaption>
<bodyText confidence="0.999692">
To the best of our knowledge, this is the first
collection of monolingual corpora for Gulf Ara-
bic subdialects. It can be helpful for, e.g., lan-
guage modeling when translating into Arabic, for
learning the similarities and differences between
Gulf subdialects, etc. Table 2 shows some statis-
tics about the data after punctuation tokenization.
</bodyText>
<subsectionHeader confidence="0.897436">
4.2 Orthographic Normalization
</subsectionHeader>
<bodyText confidence="0.996932666666667">
The inconsistency in the orthographic spelling of
the same word can increase data sparseness. Thus,
we normalize the Arabic text in the collected re-
sources by applying the reduced orthographic nor-
malization scheme, e.g., Tah Marbota is reduced to
Hah. We also normalize extended lines between
letters, e.g.,Q��º�ƒ ‘sugar’ /sukar/ is changed
to Qºƒ, and we reduce character elongations to
be just two characters long. In order to main-
tain consistency among different resources, we re-
move supplementary diacritics, e.g., J®s ‘knots’
/Euqad/ is normalized to Y~®«, and we map Per-
sian letters to their phonological correspondences
in Qatari Arabic5, i.e., À/G/ to †~ /g/, ¬~ /V/ to ¬
/f/, H/P/ to H. /b/, and P and h~/J/ to h./j/.
</bodyText>
<sectionHeader confidence="0.577637" genericHeader="method">
4
</sectionHeader>
<bodyText confidence="0.998995">
For the English texts, the orthographic varia-
tions were already normalized. However, the En-
glish side of the QCA corpus had some spelling
and grammatical errors, which we corrected man-
ually. On the grammatical side, we only corrected
a subset of the data, which we used for tuning and
testing our SMT system (see Section 5).
</bodyText>
<subsectionHeader confidence="0.997952">
4.3 Morphological Decomposition
</subsectionHeader>
<bodyText confidence="0.999900375">
There is no general Arabic morphological seg-
menter that works for all variations of Arabic. The
most commonly used segmenters for Arabic were
designed for MSA (Habash et al., 2009; Green and
DeNero, 2012). Due to the lexical and morpholog-
ical differences between dialects and MSA, these
MSA-based morphological tools do not work well
for dialects.
</bodyText>
<footnote confidence="0.855981">
5This issue relates to the QCA corpus.
</footnote>
<bodyText confidence="0.999884571428572">
In this work, we used an unsupervised morpho-
logical segmenter, Morfessor-categories MAP6,
an unsupervised model with a single hyper-
parameter (Creutz and Lagus, 2007). We chose
Morfessor because of its superior performance on
Arabic compared to other unsupervised models
(Siivola et al., 2007; Poon et al., 2009).
The model has a single hyperparameter, the per-
plexity threshold parameter B, which controls the
granularity of segmentation. The recommended
value ranges from 1 to 400 where 1 means max-
imum fine-grained segmentation, and 400 restricts
it to the least segmented output. We set the thresh-
old empirically to 70, as shown in Section 5.1.
</bodyText>
<sectionHeader confidence="0.999038" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999239580645161">
We performed an extrinsic evaluation of the varia-
tions in segmentation by building a Qatari Arabic
to English machine translation system on each of
them. We also tested Morfessor on other available
dialects and on MSA, and we will show below how
a uniform segmentation can help to better adapt re-
sources for dialects and MSA for SMT. This sec-
tion describes our experimental setup.
Datasets: We divided the QCA corpus into 1k
sentences each for development and testing, and
we used the remaining 12k for training.
We adapted parallel corpora for Egyptian, Lev-
antine and MSA to English to be used for Qatari
Arabic to English SMT. For MSA, we used par-
allel corpora of TED talks (Cettolo et al., 2012)
and the AMARA corpus (Abdelali et al., 2014),
which consists of educational videos. Since the
QCA corpus is in the speech domain, we believe
that an MSA corpus of spoken domain would be
more helpful than a text domain such as News. For
Egyptian and Levantine, we used the parallel cor-
pus provided by Zbib et al. (2012). There is no
Gulf–English parallel data available in the litera-
ture. The data that we found was a very small col-
lection of subdialects of Gulf Arabic; we did not
use it for MT experiments. However, we used the
Qatari part of the AVIA corpus to train Morfessor.
Machine translation system settings: We used
a phrase-based statistical machine translation
model as implemented in the Moses toolkit
(Koehn et al., 2007) for machine translation.
</bodyText>
<footnote confidence="0.9972835">
6This is an extension of the basic Morfessor method and
is based on a Maximum a Posteriori model.
</footnote>
<page confidence="0.99878">
211
</page>
<bodyText confidence="0.99995184">
We built separate directed word alignments
for source-to-target and target-to-source using
IBM model 4 (Brown et al., 1993), and we
symmetrized them using the grow-diag-final-and
heuristics (Koehn et al., 2003). We then extracted
phrase pairs with a maximum length of seven, and
we scored them using maximum likelihood esti-
mation with Kneser-Ney smoothing (Kneser and
Ney, 1995). We also built a lexicalized reordering
model, msd-bidirectional-fe. We built a 5-gram
language model on the English side of QCA-train
using KenLM (Heafield, 2011). Finally, we built a
log-linear model using the above features.
We tuned the model weights by optimizing
BLEU (Papineni et al., 2002) on the tuning set, us-
ing PRO (Hopkins and May, 2011) with sentence-
level BLEU+1 optimization (Nakov et al., 2012).
In testing, we used minimum Bayes risk decoding
(Kumar and Byrne, 2004), cube pruning, and the
operation sequence model (Durrani et al., 2011).
Baseline: Our baseline Qatari Arabic to English
MT system is trained on the QCA bitext without
any segmentation of Qatari Arabic. For the exper-
iments described in this paper, we used the English
side of the QCA corpus for language modeling.
</bodyText>
<subsectionHeader confidence="0.98124">
5.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999630583333333">
In this section, we first present our work on using
Morfessor for segmenting Qatari Arabic. We tried
different values of its parameter, and we trained it
using corpora of different sizes to find balanced
settings that improve SMT quality as compared
with no segmentation and with segmentation us-
ing the Stanford ATB segmenter. We further ap-
plied our selected settings to segment MSA, EGY
and LEV and used them for Qatari Arabic to En-
glish machine translation. Our results show that a
uniform segmentation scheme across different di-
alects improves machine translation.
Morfessor training variations: We trained
Morfessor using three corpora: (i) QCA,
(ii) AVIAQA plus Qatari Novels, and (iii) a com-
bination thereof. Table 3 shows the results for
our SMT system when trained on the QCA par-
allel corpus, which was segmented using different
training models of Morfessor with B = 40. The
result for segmented Qatari Arabic is always bet-
ter than the baseline, irrespective of the training
model used for segmentation. We can see that the
Morfessor model trained on a large monolingual
corpus, i.e., on (ii) or (iii), yields better results.
</bodyText>
<table confidence="0.9993564">
Morfessor BLEU OOV%
Baseline 12.2 16.6
QCA 12.5 0.6
AVIAQA, Novels 13.5 0.8
QCA, AVIAQA, Novels 13.4 0.7
</table>
<tableCaption confidence="0.9222615">
Table 3: Study of the effect of varying the train-
ing datasets for Morfessor on the Qatari to English
SMT. “Baseline” shows the output of the MT sys-
tem with no segmentation.
</tableCaption>
<table confidence="0.916273833333333">
B 10 40 70 100 130
BLEU 13.3 13.5 13.8 12.9 12.6
OOV 0.3 0.8 1.4 2.8 2.8
After merging
BLEU 12.5 13.4 13.7 12.8 12.3
OOV 1.5 1.9 3.9 6.5 9.8
</table>
<tableCaption confidence="0.87724875">
Table 4: The effect of varying the perplexity
threshold parameter B of Morfessor on SMT qual-
ity. “After merging” are the results using the post-
processed Qatari segmented data.
</tableCaption>
<bodyText confidence="0.99900475">
The high reduction in OOV in Table 3 is be-
cause of the fine-grained segmentation. We tried
different values for the perplexity parameter B
in order to find a good balance between better
BLEU scores and linguistically correct segmen-
tations. The first part of Table 4 shows the ef-
fect of different values of B on the quality of the
machine translation system trained on AVIAQA,
Qatari Novels. We achieved the best SMT score at
B = 70.
We further analyzed the output of Morfessor
at B = 70 and we noticed that it tends to gener-
ate very small segments of length two and three
characters long. The segmentation produces more
than one stem in a word and does not generate le-
gal word units. For example, the word ;LrL’Ë@ð
</bodyText>
<equation confidence="0.638823333333333">
‘and the industry’ /wAlSinAEp/ is segmented as
PRE/ð + PRE/È@ + STM/• + PRE/ à� + PRE/@
+ STM/¨ + SUF/S. We apply a post-processing
</equation>
<bodyText confidence="0.991309">
step that merges all stems in a word and affixes
between them to one stem. So, a word can have
only one stem. For example, the word ;LrL’Ë@ð
would be segmented as PRE/È@ð + STM/¨A:“ +
SUF/S. This yielded linguistically correct segmen-
tations in many cases. The second part of Table
4 shows the effect of the post-processing on the
BLEU score. We can see that it remains almost
the same with an increase in OOV rate.
</bodyText>
<page confidence="0.996066">
212
</page>
<bodyText confidence="0.999913456521739">
For rest of the experiments in this paper, we
used a value of 70 for the perplexity threshold
parameter plus the post-processing on segmenta-
tion. We trained Morfessor on the concatenation
of QCA, AVIAQA and Novels.7
Using other Arabic variations: In this section,
we present experiments using MSA, EGY and
LEV to English bitexts combined with the QCA
bitext for Qatari Arabic to English machine trans-
lation. We explored three segmentation options for
the Arabic side of the data: (i) no segmentation,
(ii) ATB segmentation, and (iii) unsupervised seg-
mentation using Morfessor.
The QCA corpus is of much smaller size com-
pared to other Arabic variants, say MSA. It is pos-
sible that in the training of the machine transla-
tion models, the large corpus dominates the QCA
corpus. In order to avoid that, we balanced the
two corpora by replicating the smaller corpus X
number of times in order to make it approximately
equal to the large corpus (Nakov and Ng, 2009).8
The complete procedure is described below.
In a nutshell, for building a machine transla-
tion system using the MSA plus Qatari corpus, we
first balanced the Qatari corpus to make it approx-
imately equal to MSA and concatenated them. For
training Morfessor, the Qatari Arabic data con-
sisted of QCA, Novels and AVIAQA, while for
SMT, it consisted of QCA only. In both cases,
we balanced it to be approximately equal to MSA.
We then trained Morfessor on the balanced (QCA,
Novels, AVIAQA) plus MSA data and we seg-
mented the Arabic side of the balanced QCA plus
MSA training data for machine translation. We
built a machine translation system on the seg-
mented data. We segmented the testing and tuning
data sets similarly. We used the same balancing
when we combined EGY-EN and LEV-EN with
the Qatari Arabic – English data.
We also tried training multiple unsupervised
models, but this yielded lower SMT quality com-
pared to using a single model trained on multi-
dialects. Using different models could result
in having different segmentation schemes, which
might not help in reducing the vocabulary mis-
match between different variants of Arabic.
</bodyText>
<footnote confidence="0.989742833333333">
7We did not see a big difference in training Morfessor
with and without the QCA corpus, and we decided to use
the complete data for training.
8Due to the spoken nature of the QCA corpus, it contains
shorter sentences. Thus, we balanced the corpora based on
the number of tokens rather than on the number of sentences.
</footnote>
<table confidence="0.9998148">
Train NONE ATB Morfessor
QCA 12.2 12.9 13.7
’QCA,MSA 12.7 13.3 14.6
’QCA,EGY 13.0 13.5 14.5
’QCA,LEV 13.8 13.7 15.2
</table>
<tableCaption confidence="0.973376">
Table 5: BLEU scores for Qatari Arabic to English
</tableCaption>
<bodyText confidence="0.997378722222222">
SMT using three different segmentation settings.
’QCA means the modified QCA corpus with num-
ber of tokens approximately equal to MSA, EGY
and LEV in the respective experiments.
Table 5 shows the results. There are two things
to point here. First, the SMT systems that used
the unsupervised morphological segmenter, Mor-
fessor, outperformed the systems that used no seg-
mentation and those using the ATB segmentation.
The Morfessor-based systems showed consistent
improvements compared to the ATB-based sys-
tems over the no-segmentation systems. This val-
idates our point that unsupervised morphological
segmentation generalizes well for a variety of di-
alects and these SMT results complement that.
The second observation is that adding a bitext for
other dialects and MSA improves machine trans-
lation quality for Qatari–English SMT.
</bodyText>
<sectionHeader confidence="0.970707" genericHeader="method">
6 Translation into Qatari Arabic
</sectionHeader>
<bodyText confidence="0.999960045454546">
Our monolingual corpora of Gulf subdialects
could be also helpful when translating English into
Qatari Arabic. We conducted a few basic experi-
ments in this direction but without segmentation.
We trained an English to Qatari Arabic SMT
system on the QCA bitext, using the same settings
as described in Section 5. We then normalized the
output of the translation system using the QCRI-
Normalizer (Sajjad et al., 2013b).9 As a language
model, we used the Arabic side of the QCA cor-
pus, novels and forum data, standalone and to-
gether. Table 6 presents the results of the effect of
varying the language model on the quality of the
SMT system. The best system shows an improve-
ment of 0.22 BLEU points absolute compared to
the baseline system that only uses the Arabic side
of the QCA corpus for LM training.
The SMT system achieved the largest gain when
adding QA forum data to the QCA data. SA and
AE monolingual data also showed good improve-
ments. This might be due to their relatively large
sizes; we need further investigation.
</bodyText>
<footnote confidence="0.968031">
9http://alt.qcri.org/tools/
</footnote>
<page confidence="0.994738">
213
</page>
<table confidence="0.999032">
LM BLEU
QCA 2.78
QCA+QA-Novels 2.64
QCA+QA-Novels+BH-Novels 2.86
QCA+QA-Novels+KW-Novels 2.78
QCA+QA-Novels+AE-Novels 2.92
QCA+QA-Novels+SA-Novels 2.96
QCA+ALL-Novels 2.80
QCA+QA-Novels+QForum 3.00
</table>
<tableCaption confidence="0.958671">
Table 6: Results for English to Qatari SMT for
</tableCaption>
<bodyText confidence="0.9139874">
varying language models. In all cases, the transla-
tion model is trained on the QCA bitext only.
Note the quite low BLEU scores, especially
compared to the reverse translation direction. One
reason is the morphologically rich nature of Qatari
Arabic, which makes translating into it a hard
problem. The small amount of training data fur-
ther adds to it. We expect to see larger gains com-
pared to Qatari Arabic to English machine transla-
tion when segmentation is used.
</bodyText>
<sectionHeader confidence="0.951732" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999949875">
We have demonstrated the feasibility of using
an unsupervised morphological segmenter to in-
crease the resource adaptability of Arabic variants.
We evaluated the segmentation on a Qatari dialect
by building a Qatari Arabic to English machine
translation system. We further adapted MSA,
EGY and LEV in the simplest machine translation
settings and we showed a consistent improvement
of 1.5 BLEU points when compared to the respec-
tive baseline system that uses no segmentation.
In the future, we would like to explore the
impact of segmentation on both the translation
model and the language model when translating
into Qatari Arabic. This involves greater chal-
lenges, as a desegmenter is required for the trans-
lation output with every segmentation scheme.
</bodyText>
<sectionHeader confidence="0.99786" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99863296969697">
Ahmed Abdelali, Francisco Guzman, Hassan Sajjad,
and Stephan Vogel. 2014. The AMARA corpus:
Building parallel language resources for the educa-
tional domain. In Proceedings of the 9th Interna-
tional Conference on Language Resources and Eval-
uation, Reykjavik, Iceland, May.
Rania Al-Sabbagh and Roxana Girju. 2010. Mining
the web for the induction of a dialectical Arabic lexi-
con. In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation, Val-
letta, Malta, May.
Hitham Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for convert-
ing written Egyptian colloquial dialect into dia-
critized Arabic. In Proceedings of the 6th Inter-
national Conference on Informatics and Systems,
Cairo, Egypt, March.
Houda Bouamor, Nizar Habash, and Kemal Oflazer.
2014. A multidialectal parallel corpus of Arabic. In
Proceedings of the 9th edition of the Language Re-
sources and Evaluation Conference, Reykjavik, Ice-
land, May.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter
estimation. Computational Linguistics, 19(2), June.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation, Trento, Italy, May.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic
dialects. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy, April.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, Port-
land, OR, June.
Mohamed Elmahdy, Mark Hasegawa-Johnson, and
Eiman Mustafawi. 2014. Development of a TV
broadcasts speech recognition system for Qatari
Arabic. In Proceedings of the 9th edition of the Lan-
guage Resources and Evaluation Conference, Reyk-
javik, Iceland, May.
Spence Green and John DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics, Jeju Island, Korea, July.
Nizar Habash and Owen Rambow. 2006. MAGEAD:
a morphological analyzer and generator for the Ara-
bic dialects. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, Sydney, Australia, July.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, pos
</reference>
<page confidence="0.994573">
214
</page>
<reference confidence="0.995125267857143">
tagging, stemming and lemmatization. In Proceed-
ings of the 2nd International Conference on Ara-
bic Language Resources and Tools (MEDAR), Cairo,
Egypt, April.
Nizar Habash, Ramy Eskander, and Abdelati Hawwari.
2012. A morphological analyzer for Egyptian Ara-
bic. In Proceedings of the 12th Meeting of the Spe-
cial Interest Group on Computational Morphology
and Phonology, Montreal, Canada, June.
Nizar Y Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies, 3(1), August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the 6th
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Scotland, UK, July.
Serena Jeblee, Weston Feely, Houda Bouamor, Alon
Lavie, Nizar Habash, and Kemal Oflazer. 2014.
Domain and Dialect Adaptation for Machine Trans-
lation into Egyptian Arabic. In Proceedings of
the Arabic Natural Language Processing Workshop,
Doha, Qatar, October.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for ngram langauge modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, Detroit,
Michigan, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology and North
American Association for Computational Linguis-
tics Conference, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Demonstra-
tion Program, Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
Boston, MA, May.
Mohamed Maamouri, Ann Bies, Tim Buckwalter,
Mona Diab, Nizar Habash, Owen Rambow, and
Dalila Tabessi. 2006. Developing and using a pi-
lot dialectal Arabic treebank. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation, Genova, Italy, May.
Hamdy Mubarak and Kareem Darwish. 2014. Using
Twitter to collect a multi-dialectal corpus of Arabic.
In Proceedings of the Arabic Natural Language Pro-
cessing Workshop, Doha, Qatar, October.
Preslav Nakov and Hwee Tou Ng. 2009. Improved
statistical machine translation for resource-poor lan-
guages using related resource-rich languages. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Suntec,
Singapore, August.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proceedings of the
24th International Conference on Computational
Linguistics, Mumbai, India, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th annual meeting on association for com-
putational linguistics, Philadelphia, PA, July.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics, Denver, CO,
June.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with appli-
cations to machine translation. In Proceedings of
the 7th Conference of the Association for Machine
Translation in the Americas, MA, USA, August.
Hassan Sajjad, Kareem Darwish, and Yonatan Be-
linkov. 2013a. Translating dialectal Arabic to En-
glish. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
Sofia, Bulgaria, August.
Hassan Sajjad, Francisco Guzman, Preslav Nakov,
Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli,
and Stephan Vogel. 2013b. QCRI at IWSLT 2013:
Experiments in Arabic-English and English-Arabic
Spoken Language Translation. In Proceedings of the
10th International Workshop on Spoken Language
Translation, Hiedelberg, Germany, December.
Ahmed Salama, Houda Bouamor, Behrang Mohit, and
Kemal Oflazer. 2014. YouDACC: the youtube di-
alectal Arabic commentary corpus. In Proceedings
of the 9th edition of the Language Resources and
Evaluation Conference, Reykjavik, Iceland, May.
Wael Salloum and Nizar Habash. 2011. Dialectal
to standard Arabic paraphrasing to improve Arabic-
English statistical machine translation. In Proceed-
ings of the First Workshop on Algorithms and Re-
sources for Modelling of Dialects and Language Va-
rieties, Edinburgh, Scotland, July.
</reference>
<page confidence="0.984686">
215
</page>
<reference confidence="0.999536542857143">
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Con-
ference of the Association for Machine Translation
in the Americas, Denver, CO, October.
Vesa Siivola, Mathias Creutz, and Mikko Kurimo.
2007. Morfessor and VariKN machine learning
tools for speech and language technology. In
Proceedings of the 8th International Conference
on Speech Communication and Technology (Inter-
speech), Antwerpen, Belgium, August.
Ming-Feng Tsai, Preslav Nakov, and Hwee Tou Ng.
2010. Morphological analysis for resource-poor
machine translation. Technical report, Kent Ridge,
Singapore, December.
Omar F Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, Portland, OR, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Montreal, Canada, June.
Andreas Zollmann, Ashish Venugopal, and Stephan
Vogel. 2006. Bridging the inflection morphol-
ogy gap for Arabic statistical machine translation.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, New York, NY, June.
</reference>
<page confidence="0.99914">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832126">
<title confidence="0.959800333333333">Unsupervised Word Segmentation Dialectal Arabic to English Machine Translation Hassan Alaa Fahad Al</title>
<author confidence="0.994242">Stephan</author>
<affiliation confidence="0.975576">Computing Research Carnegie Mellon University in</affiliation>
<abstract confidence="0.998151705882353">We demonstrate the feasibility of using unsupervised morphological segmentation for dialects of Arabic, which are poor in linguistics resources. Our experiments using a Qatari Arabic to English machine translation system show that unsupervised segmentation helps to improve the translation quality as compared to using no segmentation or to using ATB segmentation, which was especially designed for Modern Standard Arabic (MSA). We use MSA and other dialects to improve Qatari Arabic to English machine translation, and we show that a uniform segmentation scheme across them yields an improvement of 1.5 BLEU points over using no segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abdelali</author>
<author>Francisco Guzman</author>
<author>Hassan Sajjad</author>
<author>Stephan Vogel</author>
</authors>
<title>The AMARA corpus: Building parallel language resources for the educational domain.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation,</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="20382" citStr="Abdelali et al., 2014" startWordPosition="3333" endWordPosition="3336">nslation system on each of them. We also tested Morfessor on other available dialects and on MSA, and we will show below how a uniform segmentation can help to better adapt resources for dialects and MSA for SMT. This section describes our experimental setup. Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor. Machine translation system settings: We used a phrase-based stat</context>
</contexts>
<marker>Abdelali, Guzman, Sajjad, Vogel, 2014</marker>
<rawString>Ahmed Abdelali, Francisco Guzman, Hassan Sajjad, and Stephan Vogel. 2014. The AMARA corpus: Building parallel language resources for the educational domain. In Proceedings of the 9th International Conference on Language Resources and Evaluation, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rania Al-Sabbagh</author>
<author>Roxana Girju</author>
</authors>
<title>Mining the web for the induction of a dialectical Arabic lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="6983" citStr="Al-Sabbagh and Girju, 2010" startWordPosition="1090" endWordPosition="1093">fectively (Section 4.3). We also explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). 2 Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such a</context>
</contexts>
<marker>Al-Sabbagh, Girju, 2010</marker>
<rawString>Rania Al-Sabbagh and Roxana Girju. 2010. Mining the web for the induction of a dialectical Arabic lexicon. In Proceedings of the 7th International Conference on Language Resources and Evaluation, Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hitham Abo Bakr</author>
<author>Khaled Shaalan</author>
<author>Ibrahim Ziedan</author>
</authors>
<title>A hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Informatics and Systems,</booktitle>
<location>Cairo, Egypt,</location>
<contexts>
<context position="7787" citStr="Bakr et al., 2008" startWordPosition="1213" endWordPosition="1216">mor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical </context>
</contexts>
<marker>Bakr, Shaalan, Ziedan, 2008</marker>
<rawString>Hitham Abo Bakr, Khaled Shaalan, and Ibrahim Ziedan. 2008. A hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabic. In Proceedings of the 6th International Conference on Informatics and Systems, Cairo, Egypt, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Houda Bouamor</author>
<author>Nizar Habash</author>
<author>Kemal Oflazer</author>
</authors>
<title>A multidialectal parallel corpus of Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th edition of the Language Resources and Evaluation Conference,</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="7187" citStr="Bouamor et al. (2014)" startWordPosition="1120" endWordPosition="1123">oint to possible directions for future work and we conclude the paper (Section 7). 2 Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008</context>
</contexts>
<marker>Bouamor, Habash, Oflazer, 2014</marker>
<rawString>Houda Bouamor, Nizar Habash, and Kemal Oflazer. 2014. A multidialectal parallel corpus of Arabic. In Proceedings of the 9th edition of the Language Resources and Evaluation Conference, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="21323" citStr="Brown et al., 1993" startWordPosition="3493" endWordPosition="3496">le in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor. Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May,</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>Wit3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Association for Machine Translation,</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="20337" citStr="Cettolo et al., 2012" startWordPosition="3325" endWordPosition="3328">lding a Qatari Arabic to English machine translation system on each of them. We also tested Morfessor on other available dialects and on MSA, and we will show below how a uniform segmentation can help to better adapt resources for dialects and MSA for SMT. This section describes our experimental setup. Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor. Machine translation</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Conference of the European Association for Machine Translation, Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Safiullah Shareef</author>
</authors>
<title>Parsing Arabic dialects.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="8779" citStr="Chiang et al. (2006)" startWordPosition="1370" endWordPosition="1373">could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Riesa and Yarowsky (2006) trained a supervised trie-based model using a small lexicon of dialectal affixes. In our work, we eliminate the need for linguistic knowledge by training an unsupervised model using available resources. The unsupervised mode of learning allowed us to develop a multi-dialectal morphological segmenter. 3 Arabic Dialects In this section, we highlight some of the linguistic differences between Arabic dialects and MSA, with a focus on the Qatari dialect. 3</context>
</contexts>
<marker>Chiang, Diab, Habash, Rambow, Shareef, 2006</marker>
<rawString>David Chiang, Mona Diab, Nizar Habash, Owen Rambow, and Safiullah Shareef. 2006. Parsing Arabic dialects. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4850" citStr="Creutz and Lagus, 2007" startWordPosition="745" endWordPosition="749">putational Linguistics In particular, we develop a Qatari Arabic to English (QA-EN) SMT system, which we train on a small pre-existing bi-text. As part of the development of the unsupervised segmentation model, we also collected some additional monolingual data for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. We train an unsupervised segmentation tool, Morphessor, and its MAP model (Creutz and Lagus, 2007), using different variations of the collected Qatari data. We optimize the single hyperparameter of the MAP model by maximizing the translation quality of the QA-EN SMT system in terms of BLEU. Our experimental results demonstrate that the resulting unsupervised segmenter yields improvements in translation quality when compared to (i) using no segmentation and (ii) using an MSA-based ATB segmenter. We further develop a multi-dialectal word segmentation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantin</context>
<context position="19136" citStr="Creutz and Lagus, 2007" startWordPosition="3122" endWordPosition="3125">esting our SMT system (see Section 5). 4.3 Morphological Decomposition There is no general Arabic morphological segmenter that works for all variations of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5This issue relates to the QCA corpus. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6, an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, which controls the granularity of segmentation. The recommended value ranges from 1 to 400 where 1 means maximum fine-grained segmentation, and 400 restricts it to the least segmented output. We set the threshold empirically to 70, as shown in Section 5.1. 5 Experimental Setup We performed an extrinsic evaluation of the variations in segmentation by building a Qatari Arabi</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1), January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Portland, OR,</location>
<contexts>
<context position="22133" citStr="Durrani et al., 2011" startWordPosition="3621" endWordPosition="3624">lihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning, and the operation sequence model (Durrani et al., 2011). Baseline: Our baseline Qatari Arabic to English MT system is trained on the QCA bitext without any segmentation of Qatari Arabic. For the experiments described in this paper, we used the English side of the QCA corpus for language modeling. 5.1 Experimental Results In this section, we first present our work on using Morfessor for segmenting Qatari Arabic. We tried different values of its parameter, and we trained it using corpora of different sizes to find balanced settings that improve SMT quality as compared with no segmentation and with segmentation using the Stanford ATB segmenter. We fu</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Elmahdy</author>
<author>Mark Hasegawa-Johnson</author>
<author>Eiman Mustafawi</author>
</authors>
<title>Development of a TV broadcasts speech recognition system for Qatari Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th edition of the Language Resources and Evaluation Conference,</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="15532" citStr="Elmahdy et al., 2014" startWordPosition="2544" endWordPosition="2547"> 2 Tokens 115 6.7 15 Table 1: Statistics about the collected parallel corpora (in thousands). AVIAO shows the statistics about the AVIA corpus excluding Qatari data. 4.1 Data Collection We did an extensive search for available monolingual and bilingual resources for the Gulf dialect, with a focus on Qatari Arabic. Tables 1 and 2 present some statistics about the corpora we collected. More detailed description follows below. Bilingual corpora: – The QCA speech corpus, comprises 14.7k sentences that are phonetically transcribed from TV broadcasts in Qatari Arabic and translated to English; see (Elmahdy et al., 2014) for more detail. The corpus was designed for speech recognition and we faced several normalization-related issues that we had to resolve before it could be used for machine translation and language modeling. One example is the usage of five Persian characters to represent some sounds in Arabic words. Moreover, the English side had some grammatical and spelling errors. We normalized the Arabic side and corrected the English side of the corpus as described in Section 4.2. The corpus can be found at http://sprosig.isle. illinois.edu/corpora/1. – The AVIA corpus1 is designed as a reference source</context>
</contexts>
<marker>Elmahdy, Hasegawa-Johnson, Mustafawi, 2014</marker>
<rawString>Mohamed Elmahdy, Mark Hasegawa-Johnson, and Eiman Mustafawi. 2014. Development of a TV broadcasts speech recognition system for Qatari Arabic. In Proceedings of the 9th edition of the Language Resources and Evaluation Conference, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>John DeNero</author>
</authors>
<title>A class-based agreement model for generating accurately inflected translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="18788" citStr="Green and DeNero, 2012" startWordPosition="3069" endWordPosition="3072"> ¬~ /V/ to ¬ /f/, H/P/ to H. /b/, and P and h~/J/ to h./j/. 4 For the English texts, the orthographic variations were already normalized. However, the English side of the QCA corpus had some spelling and grammatical errors, which we corrected manually. On the grammatical side, we only corrected a subset of the data, which we used for tuning and testing our SMT system (see Section 5). 4.3 Morphological Decomposition There is no general Arabic morphological segmenter that works for all variations of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5This issue relates to the QCA corpus. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6, an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, which controls the granular</context>
</contexts>
<marker>Green, DeNero, 2012</marker>
<rawString>Spence Green and John DeNero. 2012. A class-based agreement model for generating accurately inflected translations. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>MAGEAD: a morphological analyzer and generator for the Arabic dialects.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="8650" citStr="Habash and Rambow (2006)" startWordPosition="1352" endWordPosition="1355">veraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Riesa and Yarowsky (2006) trained a supervised trie-based model using a small lexicon of dialectal affixes. In our work, we eliminate the need for linguistic knowledge by training an unsupervised model using available resources. The unsupervised mode of learning allowed us to develop a multi-dialectal morphological segmenter. 3 Arabic Dialects In thi</context>
</contexts>
<marker>Habash, Rambow, 2006</marker>
<rawString>Nizar Habash and Owen Rambow. 2006. MAGEAD: a morphological analyzer and generator for the Arabic dialects. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>Mada+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, pos tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR),</booktitle>
<location>Cairo, Egypt,</location>
<contexts>
<context position="18763" citStr="Habash et al., 2009" startWordPosition="3065" endWordPosition="3068">i.e., À/G/ to †~ /g/, ¬~ /V/ to ¬ /f/, H/P/ to H. /b/, and P and h~/J/ to h./j/. 4 For the English texts, the orthographic variations were already normalized. However, the English side of the QCA corpus had some spelling and grammatical errors, which we corrected manually. On the grammatical side, we only corrected a subset of the data, which we used for tuning and testing our SMT system (see Section 5). 4.3 Morphological Decomposition There is no general Arabic morphological segmenter that works for all variations of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5This issue relates to the QCA corpus. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6, an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, wh</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. Mada+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, pos tagging, stemming and lemmatization. In Proceedings of the 2nd International Conference on Arabic Language Resources and Tools (MEDAR), Cairo, Egypt, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ramy Eskander</author>
<author>Abdelati Hawwari</author>
</authors>
<title>A morphological analyzer for Egyptian Arabic.</title>
<date>2012</date>
<booktitle>In Proceedings of the 12th Meeting of the Special Interest Group on Computational Morphology and Phonology,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="8624" citStr="Habash et al., 2012" startWordPosition="1348" endWordPosition="1351">rk has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Riesa and Yarowsky (2006) trained a supervised trie-based model using a small lexicon of dialectal affixes. In our work, we eliminate the need for linguistic knowledge by training an unsupervised model using available resources. The unsupervised mode of learning allowed us to develop a multi-dialectal morphological segmenter</context>
</contexts>
<marker>Habash, Eskander, Hawwari, 2012</marker>
<rawString>Nizar Habash, Ramy Eskander, and Abdelati Hawwari. 2012. A morphological analyzer for Egyptian Arabic. In Proceedings of the 12th Meeting of the Special Interest Group on Computational Morphology and Phonology, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Y Habash</author>
</authors>
<title>Introduction to Arabic natural language processing.</title>
<date>2010</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="12076" citStr="Habash, 2010" startWordPosition="1944" endWordPosition="1945">like in EGY, and h./j/ in SA, much like in LEV. For example, the MSA word Yj. ‚Ó ‘mosque’ /masjid/ is pronounced as /masjid/ in MSA, SA, LEV, Y&amp;quot;®‚Ó /masqid/ in OM, EGY, YJ ‚Ó /masyid/ in KW, BH, QA, AE, while the MSA pronunciation is preserved in SA. This change does not apply to names. However, we should note that it is not consistent in QA, e.g., the MSA pronunciation of h./j/ in ÉJ.k. ‘mountain’ /jabal/ and h. QK. ‘tower’ /burj/ is preserved in QA. 3.2 Morphological Variations In Arabic, a root can produce surface wordforms by means of inflectional and derivational morphological processes (Habash, 2010). An inflectional word form is a variant of a root word with the same meaning but expressing a different function, e.g., gender, number, case. It is usually formed by adding a prefix, a suffix, or a circumfix to a stem word. Note that Arabic dialects can make different lexical choices for affixations compared to MSA. For example, the MSA future prefix € /s/ is replaced by H. /b/ in QA and by -ë /h/ in EGY and LEV. Thus, the MSA word É¿�AJ�ƒ ‘he will eat’ /say&gt;kul/ becomes É¿AJ K. /biyAkil/ in QA and É¿AJ�ë /hayAkul/ in EGY and LEV. A derivational word form is formed by applying a pattern to a </context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Y Habash. 2010. Introduction to Arabic natural language processing. Synthesis Lectures on Human Language Technologies, 3(1), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, UK,</location>
<contexts>
<context position="21742" citStr="Heafield, 2011" startWordPosition="3558" endWordPosition="3559">basic Morfessor method and is based on a Maximum a Posteriori model. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning, and the operation sequence model (Durrani et al., 2011). Baseline: Our baseline Qatari Arabic to English MT system is trained on the QCA bitext without any segmentation of Qatari Arabic. For the experiments described in this paper, we used the English side of the </context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the 6th Workshop on Statistical Machine Translation, Edinburgh, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Scotland, UK,</location>
<contexts>
<context position="21929" citStr="Hopkins and May, 2011" startWordPosition="3589" endWordPosition="3592">wn et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning, and the operation sequence model (Durrani et al., 2011). Baseline: Our baseline Qatari Arabic to English MT system is trained on the QCA bitext without any segmentation of Qatari Arabic. For the experiments described in this paper, we used the English side of the QCA corpus for language modeling. 5.1 Experimental Results In this section, we first present our work on using Morfessor for segmenting Qatari Arabic. We tried different values of its par</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Scotland, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serena Jeblee</author>
<author>Weston Feely</author>
<author>Houda Bouamor</author>
<author>Alon Lavie</author>
<author>Nizar Habash</author>
<author>Kemal Oflazer</author>
</authors>
<title>Domain and Dialect Adaptation for Machine Translation into Egyptian Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of the Arabic Natural Language Processing Workshop,</booktitle>
<location>Doha, Qatar,</location>
<contexts>
<context position="7870" citStr="Jeblee et al., 2014" startWordPosition="1227" endWordPosition="1230">rs five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. Building morphological segmenters for the Arabic </context>
</contexts>
<marker>Jeblee, Feely, Bouamor, Lavie, Habash, Oflazer, 2014</marker>
<rawString>Serena Jeblee, Weston Feely, Houda Bouamor, Alon Lavie, Nizar Habash, and Kemal Oflazer. 2014. Domain and Dialect Adaptation for Machine Translation into Egyptian Arabic. In Proceedings of the Arabic Natural Language Processing Workshop, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for ngram langauge modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<location>Detroit, Michigan,</location>
<contexts>
<context position="21578" citStr="Kneser and Ney, 1995" startWordPosition="3532" endWordPosition="3535"> used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning, and the operation sequence model (Durrani et al., 2011). Baseline: Our baseline Qatari Arabic to Eng</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for ngram langauge modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Detroit, Michigan, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="21410" citStr="Koehn et al., 2003" startWordPosition="3505" endWordPosition="3508"> of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor. Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Program,</booktitle>
<location>Alexandra</location>
<contexts>
<context position="21072" citStr="Koehn et al., 2007" startWordPosition="3453" endWordPosition="3456">speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor. Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram langua</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Program, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="22062" citStr="Kumar and Byrne, 2004" startWordPosition="3610" endWordPosition="3613">rs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning, and the operation sequence model (Durrani et al., 2011). Baseline: Our baseline Qatari Arabic to English MT system is trained on the QCA bitext without any segmentation of Qatari Arabic. For the experiments described in this paper, we used the English side of the QCA corpus for language modeling. 5.1 Experimental Results In this section, we first present our work on using Morfessor for segmenting Qatari Arabic. We tried different values of its parameter, and we trained it using corpora of different sizes to find balanced settings that improve SMT quality as compared with no seg</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Dalila Tabessi</author>
</authors>
<title>Developing and using a pilot dialectal Arabic treebank.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<location>Genova, Italy,</location>
<contexts>
<context position="6955" citStr="Maamouri et al., 2006" startWordPosition="1086" endWordPosition="1089">n a QA-EN SMT system effectively (Section 4.3). We also explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). 2 Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dia</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Diab, Habash, Rambow, Tabessi, 2006</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, Mona Diab, Nizar Habash, Owen Rambow, and Dalila Tabessi. 2006. Developing and using a pilot dialectal Arabic treebank. In Proceedings of the 5th International Conference on Language Resources and Evaluation, Genova, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamdy Mubarak</author>
<author>Kareem Darwish</author>
</authors>
<title>Using Twitter to collect a multi-dialectal corpus of Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of the Arabic Natural Language Processing Workshop,</booktitle>
<location>Doha, Qatar,</location>
<contexts>
<context position="7325" citStr="Mubarak and Darwish (2014)" startWordPosition="1139" endWordPosition="1142">ages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an a</context>
</contexts>
<marker>Mubarak, Darwish, 2014</marker>
<rawString>Hamdy Mubarak and Kareem Darwish. 2014. Using Twitter to collect a multi-dialectal corpus of Arabic. In Proceedings of the Arabic Natural Language Processing Workshop, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Improved statistical machine translation for resource-poor languages using related resource-rich languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Suntec, Singapore,</location>
<contexts>
<context position="26354" citStr="Nakov and Ng, 2009" startWordPosition="4359" endWordPosition="4362">ith the QCA bitext for Qatari Arabic to English machine translation. We explored three segmentation options for the Arabic side of the data: (i) no segmentation, (ii) ATB segmentation, and (iii) unsupervised segmentation using Morfessor. The QCA corpus is of much smaller size compared to other Arabic variants, say MSA. It is possible that in the training of the machine translation models, the large corpus dominates the QCA corpus. In order to avoid that, we balanced the two corpora by replicating the smaller corpus X number of times in order to make it approximately equal to the large corpus (Nakov and Ng, 2009).8 The complete procedure is described below. In a nutshell, for building a machine translation system using the MSA plus Qatari corpus, we first balanced the Qatari corpus to make it approximately equal to MSA and concatenated them. For training Morfessor, the Qatari Arabic data consisted of QCA, Novels and AVIAQA, while for SMT, it consisted of QCA only. In both cases, we balanced it to be approximately equal to MSA. We then trained Morfessor on the balanced (QCA, Novels, AVIAQA) plus MSA data and we segmented the Arabic side of the balanced QCA plus MSA training data for machine translation</context>
</contexts>
<marker>Nakov, Ng, 2009</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2009. Improved statistical machine translation for resource-poor languages using related resource-rich languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzman</author>
<author>Stephan Vogel</author>
</authors>
<title>Optimizing for sentence-level BLEU+1 yields short translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<location>Mumbai, India,</location>
<contexts>
<context position="21989" citStr="Nakov et al., 2012" startWordPosition="3598" endWordPosition="3601">inal-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning, and the operation sequence model (Durrani et al., 2011). Baseline: Our baseline Qatari Arabic to English MT system is trained on the QCA bitext without any segmentation of Qatari Arabic. For the experiments described in this paper, we used the English side of the QCA corpus for language modeling. 5.1 Experimental Results In this section, we first present our work on using Morfessor for segmenting Qatari Arabic. We tried different values of its parameter, and we trained it using corpora of different sizes t</context>
</contexts>
<marker>Nakov, Guzman, Vogel, 2012</marker>
<rawString>Preslav Nakov, Francisco Guzman, and Stephan Vogel. 2012. Optimizing for sentence-level BLEU+1 yields short translations. In Proceedings of the 24th International Conference on Computational Linguistics, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="21876" citStr="Papineni et al., 2002" startWordPosition="3578" endWordPosition="3581">to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used minimum Bayes risk decoding (Kumar and Byrne, 2004), cube pruning, and the operation sequence model (Durrani et al., 2011). Baseline: Our baseline Qatari Arabic to English MT system is trained on the QCA bitext without any segmentation of Qatari Arabic. For the experiments described in this paper, we used the English side of the QCA corpus for language modeling. 5.1 Experimental Results In this section, we first present our work on using Morfessor for segmentin</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="19282" citStr="Poon et al., 2009" startWordPosition="3145" endWordPosition="3148"> of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5This issue relates to the QCA corpus. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6, an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, which controls the granularity of segmentation. The recommended value ranges from 1 to 400 where 1 means maximum fine-grained segmentation, and 400 restricts it to the least segmented output. We set the threshold empirically to 70, as shown in Section 5.1. 5 Experimental Setup We performed an extrinsic evaluation of the variations in segmentation by building a Qatari Arabic to English machine translation system on each of them. We also tested Morfessor on other available dialects and on MSA, and we will show below h</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Denver, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>David Yarowsky</author>
</authors>
<title>Minimally supervised morphological segmentation with applications to machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>MA, USA,</location>
<contexts>
<context position="8923" citStr="Riesa and Yarowsky (2006)" startWordPosition="1392" endWordPosition="1395">ork is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Riesa and Yarowsky (2006) trained a supervised trie-based model using a small lexicon of dialectal affixes. In our work, we eliminate the need for linguistic knowledge by training an unsupervised model using available resources. The unsupervised mode of learning allowed us to develop a multi-dialectal morphological segmenter. 3 Arabic Dialects In this section, we highlight some of the linguistic differences between Arabic dialects and MSA, with a focus on the Qatari dialect. 3.1 Phonological Variations The Gulf dialect often preserves the phonological representation of MSA, which is not the case with many other Arabic</context>
</contexts>
<marker>Riesa, Yarowsky, 2006</marker>
<rawString>Jason Riesa and David Yarowsky. 2006. Minimally supervised morphological segmentation with applications to machine translation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, MA, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sajjad</author>
<author>Kareem Darwish</author>
<author>Yonatan Belinkov</author>
</authors>
<title>Translating dialectal Arabic to English.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7847" citStr="Sajjad et al., 2013" startWordPosition="1223" endWordPosition="1226">lel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. Building morphological seg</context>
<context position="29254" citStr="Sajjad et al., 2013" startWordPosition="4835" endWordPosition="4838">nd these SMT results complement that. The second observation is that adding a bitext for other dialects and MSA improves machine translation quality for Qatari–English SMT. 6 Translation into Qatari Arabic Our monolingual corpora of Gulf subdialects could be also helpful when translating English into Qatari Arabic. We conducted a few basic experiments in this direction but without segmentation. We trained an English to Qatari Arabic SMT system on the QCA bitext, using the same settings as described in Section 5. We then normalized the output of the translation system using the QCRINormalizer (Sajjad et al., 2013b).9 As a language model, we used the Arabic side of the QCA corpus, novels and forum data, standalone and together. Table 6 presents the results of the effect of varying the language model on the quality of the SMT system. The best system shows an improvement of 0.22 BLEU points absolute compared to the baseline system that only uses the Arabic side of the QCA corpus for LM training. The SMT system achieved the largest gain when adding QA forum data to the QCA data. SA and AE monolingual data also showed good improvements. This might be due to their relatively large sizes; we need further inv</context>
</contexts>
<marker>Sajjad, Darwish, Belinkov, 2013</marker>
<rawString>Hassan Sajjad, Kareem Darwish, and Yonatan Belinkov. 2013a. Translating dialectal Arabic to English. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sajjad</author>
<author>Francisco Guzman</author>
<author>Preslav Nakov</author>
<author>Ahmed Abdelali</author>
<author>Kenton Murray</author>
<author>Fahad Al Obaidli</author>
<author>Stephan Vogel</author>
</authors>
<date></date>
<booktitle>2013b. QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic Spoken Language Translation. In Proceedings of the 10th International Workshop on Spoken Language Translation,</booktitle>
<location>Hiedelberg, Germany,</location>
<marker>Sajjad, Guzman, Nakov, Abdelali, Murray, Obaidli, Vogel, </marker>
<rawString>Hassan Sajjad, Francisco Guzman, Preslav Nakov, Ahmed Abdelali, Kenton Murray, Fahad Al Obaidli, and Stephan Vogel. 2013b. QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic Spoken Language Translation. In Proceedings of the 10th International Workshop on Spoken Language Translation, Hiedelberg, Germany, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Salama</author>
<author>Houda Bouamor</author>
<author>Behrang Mohit</author>
<author>Kemal Oflazer</author>
</authors>
<title>YouDACC: the youtube dialectal Arabic commentary corpus.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th edition of the Language Resources and Evaluation Conference,</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="7038" citStr="Salama et al., 2014" startWordPosition="1098" endWordPosition="1101">up and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). 2 Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other A</context>
</contexts>
<marker>Salama, Bouamor, Mohit, Oflazer, 2014</marker>
<rawString>Ahmed Salama, Houda Bouamor, Behrang Mohit, and Kemal Oflazer. 2014. YouDACC: the youtube dialectal Arabic commentary corpus. In Proceedings of the 9th edition of the Language Resources and Evaluation Conference, Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wael Salloum</author>
<author>Nizar Habash</author>
</authors>
<title>Dialectal to standard Arabic paraphrasing to improve ArabicEnglish statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="7826" citStr="Salloum and Habash, 2011" startWordPosition="1219" endWordPosition="1222">lti-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. Build</context>
</contexts>
<marker>Salloum, Habash, 2011</marker>
<rawString>Wael Salloum and Nizar Habash. 2011. Dialectal to standard Arabic paraphrasing to improve ArabicEnglish statistical machine translation. In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties, Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sawaf</author>
</authors>
<title>Arabic dialect handling in hybrid machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="7800" citStr="Sawaf, 2010" startWordPosition="1217" endWordPosition="1218">resented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap betwe</context>
</contexts>
<marker>Sawaf, 2010</marker>
<rawString>Hassan Sawaf. 2010. Arabic dialect handling in hybrid machine translation. In Proceedings of the 9th Conference of the Association for Machine Translation in the Americas, Denver, CO, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vesa Siivola</author>
<author>Mathias Creutz</author>
<author>Mikko Kurimo</author>
</authors>
<title>Morfessor and VariKN machine learning tools for speech and language technology.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th International Conference on Speech Communication and Technology (Interspeech),</booktitle>
<location>Antwerpen, Belgium,</location>
<contexts>
<context position="19262" citStr="Siivola et al., 2007" startWordPosition="3141" endWordPosition="3144">rks for all variations of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5This issue relates to the QCA corpus. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6, an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, which controls the granularity of segmentation. The recommended value ranges from 1 to 400 where 1 means maximum fine-grained segmentation, and 400 restricts it to the least segmented output. We set the threshold empirically to 70, as shown in Section 5.1. 5 Experimental Setup We performed an extrinsic evaluation of the variations in segmentation by building a Qatari Arabic to English machine translation system on each of them. We also tested Morfessor on other available dialects and on MSA, and </context>
</contexts>
<marker>Siivola, Creutz, Kurimo, 2007</marker>
<rawString>Vesa Siivola, Mathias Creutz, and Mikko Kurimo. 2007. Morfessor and VariKN machine learning tools for speech and language technology. In Proceedings of the 8th International Conference on Speech Communication and Technology (Interspeech), Antwerpen, Belgium, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Feng Tsai</author>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Morphological analysis for resource-poor machine translation.</title>
<date>2010</date>
<tech>Technical report,</tech>
<location>Kent Ridge, Singapore,</location>
<contexts>
<context position="3141" citStr="Tsai et al., 2010" startWordPosition="476" endWordPosition="479">ms of a root word may not be always possible. Considering the different variants of Arabic, the problem is exacerabated as dialects could use different choices of affixes for the same function. For example, the MSA word vñJ.ªÊK� /yalEabuwn/, meaning ‘they are playing’, could be found as uñJ.ªÊK� /ylEbuwn/ in Gulf, as @ñJ.ªÊK Ñ« /Eam yilEabuA/ in Levantine, and as @ñJ.ªÊJ�K. /biylEabwA/ in Egyptian Arabic. One possible solution is to use a morphological segmenter that segments words into simpler units such as stems and affixes, which might be covered in the training set (Zollmann et al., 2006; Tsai et al., 2010). When applied to dialects, this may reduce the lexical gap between dialects and MSA by matching the common stems. Unfortunately, there are no standard morphological segmentation tools for dialects. Due to the difference in morphology, tools designed for MSA do not work well for dialects. Developing rule-based segmenters for each dialect might appear to be the ideal solution, but, as the orthography of dialects is not standardized, crafting linguistic rules for them is very hard. In this paper, we focus on training an unsupervised model for word segmentation, which we apply to SMT for a given </context>
</contexts>
<marker>Tsai, Nakov, Ng, 2010</marker>
<rawString>Ming-Feng Tsai, Preslav Nakov, and Hwee Tou Ng. 2010. Morphological analysis for resource-poor machine translation. Technical report, Kent Ridge, Singapore, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<location>Portland, OR,</location>
<contexts>
<context position="7016" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="1094" endWordPosition="1097">also explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). 2 Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F Zaidan and Chris Callison-Burch. 2011. The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, Portland, OR, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Erika Malchiodi</author>
<author>Jacob Devlin</author>
<author>David Stallard</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Omar F Zaidan</author>
<author>Chris CallisonBurch</author>
</authors>
<title>Machine translation of Arabic dialects.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="7058" citStr="Zbib et al. (2012)" startWordPosition="1102" endWordPosition="1105">results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). 2 Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. Adapting SMT resources for other Arabic dialects: Many</context>
<context position="20656" citStr="Zbib et al. (2012)" startWordPosition="3383" endWordPosition="3386">ed the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor. Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. 211 We built separate directed word alignments for source-to</context>
</contexts>
<marker>Zbib, Malchiodi, Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, CallisonBurch, 2012</marker>
<rawString>Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan, and Chris CallisonBurch. 2012. Machine translation of Arabic dialects. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
</authors>
<title>Bridging the inflection morphology gap for Arabic statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<location>New York, NY,</location>
<contexts>
<context position="3121" citStr="Zollmann et al., 2006" startWordPosition="472" endWordPosition="475">g all possible word forms of a root word may not be always possible. Considering the different variants of Arabic, the problem is exacerabated as dialects could use different choices of affixes for the same function. For example, the MSA word vñJ.ªÊK� /yalEabuwn/, meaning ‘they are playing’, could be found as uñJ.ªÊK� /ylEbuwn/ in Gulf, as @ñJ.ªÊK Ñ« /Eam yilEabuA/ in Levantine, and as @ñJ.ªÊJ�K. /biylEabwA/ in Egyptian Arabic. One possible solution is to use a morphological segmenter that segments words into simpler units such as stems and affixes, which might be covered in the training set (Zollmann et al., 2006; Tsai et al., 2010). When applied to dialects, this may reduce the lexical gap between dialects and MSA by matching the common stems. Unfortunately, there are no standard morphological segmentation tools for dialects. Due to the difference in morphology, tools designed for MSA do not work well for dialects. Developing rule-based segmenters for each dialect might appear to be the ideal solution, but, as the orthography of dialects is not standardized, crafting linguistic rules for them is very hard. In this paper, we focus on training an unsupervised model for word segmentation, which we apply</context>
</contexts>
<marker>Zollmann, Venugopal, Vogel, 2006</marker>
<rawString>Andreas Zollmann, Ashish Venugopal, and Stephan Vogel. 2006. Bridging the inflection morphology gap for Arabic statistical machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, New York, NY, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
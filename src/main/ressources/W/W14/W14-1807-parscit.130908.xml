<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011431">
<title confidence="0.9852">
Surprisal as a Predictor of Essay Quality
</title>
<author confidence="0.990401">
Gaurav Kharkwal Smaranda Muresan
</author>
<affiliation confidence="0.925186">
Department of Psychology Department of Computer Science
Center for Cognitive Science Center for Computational Learning Systems
Rutgers University, New Brunswick Columbia University
</affiliation>
<email confidence="0.998031">
gaurav.kharkwal@gmail.com smara@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796727272727">
Modern automated essay scoring systems
rely on identifying linguistically-relevant
features to estimate essay quality. This
paper attempts to bridge work in psy-
cholinguistics and natural language pro-
cessing by proposing sentence process-
ing complexity as a feature for automated
essay scoring, in the context of English
as a Foreign Language (EFL). To quan-
tify processing complexity we used a psy-
cholinguistic model called surprisal the-
ory. First, we investigated whether es-
says’ average surprisal values decrease
with EFL training. Preliminary results
seem to support this idea. Second, we in-
vestigated whether surprisal can be effec-
tive as a predictor of essay quality. The
results indicate an inverse correlation be-
tween surprisal and essay scores. Overall,
the results are promising and warrant fur-
ther investigation on the usability of sur-
prisal for essay scoring.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965142857143">
Standardized testing continues to be an integral
part of modern-day education, and an important
area of research in educational technologies is the
development of tools and methodologies to facil-
itate automated evaluation of standardized tests.
Unlike multiple-choice questions, automated eval-
uation of essays presents a particular challenge.
The specific issue is the identification of a suitable
evaluation rubric that can encompass the broad
range of responses that may be received.
Unsurprisingly then, much emphasis has been
placed on the development of Automated Essay
Scoring (henceforth, AES) systems. Notable AES
systems include Project Essay Grade (Page, 1966;
Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al.,
1998; Attali and Burstein, 2006), Intelligent Es-
say AssessorTM (Landauer et al., 2003), BETSY
(Rudner and Liang, 2002), and Vantage Learn-
ing’s IntelliMetricTM (Elliot, 2003). The common
thread in most modern AES systems is the iden-
tification of various observable linguistic features,
and the development of computational models that
combine those features for essay evaluation.
One aspect of an essay’s quality that almost all
AES systems do not yet fully capture is sentence
processing complexity. The ability to clearly and
concisely convey information without requiring
undue effort on the part of the reader is one hall-
mark of good writing. Decades of behavioral re-
search on language comprehension has suggested
that some sentence structures are harder to com-
prehend than others. For example, passive sen-
tences, such as the girl was pushed by the boy,
are known to be harder to process than semanti-
cally equivalent active sentences, such as the boy
pushed the girl (Slobin, 1966; Forster and Ol-
brei, 1972; Davison and Lutz, 1985; Kharkwal and
Stromswold, 2013). Thus, it is likely that the over-
all processing complexity of the sentence struc-
tures used in an essay could influence its perceived
quality.
One reason why sentence processing complex-
ity has not yet been fully utilized is the lack
of a suitable way of quantifying it. This paper
proposes the use of a psycholinguistic model of
sentence comprehension called surprisal theory
(Hale, 2001; Levy, 2008) to quantify sentence pro-
cessing complexity. The rest of the paper is orga-
nized as follows. Section 2 describes the surprisal
theory, and discusses its applicability in modeling
sentence processing complexity. Section 3 details
our investigation on whether essays’ average sur-
prisal values decrease following English as a For-
eign Language training. Section 4 presents a study
where we investigated whether surprisal can be ef-
fective as a predictor of essay quality. Lastly, Sec-
</bodyText>
<page confidence="0.985937">
54
</page>
<affiliation confidence="0.397147">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 54–60,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<table confidence="0.9931265">
The judge who angered the criminal slammed the gavel Mean
5.64 6.94 6.93 11.60 2.32 9.19 16.92 1.94 4.68 7.35
The judge who the criminal angered slammed the gavel Mean
5.64 6.94 6.93 4.20 9.21 13.73 16.65 2.21 4.69 7.80
</table>
<tableCaption confidence="0.9953315">
Table 1: Surprisal values of two example relative-clause sentences. The values were computed using a
top-down parser by Roark et al. (2009) trained on the Wall Street Journal corpus.
</tableCaption>
<bodyText confidence="0.950469">
tion 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.936522" genericHeader="method">
2 Surprisal Theory
</sectionHeader>
<bodyText confidence="0.9997244">
The surprisal theory (Hale, 2001; Levy, 2008)
estimates the word-level processing complexity
as the negative log-probability of a word given
the preceding context (usually, preceding syntac-
tic context). That is:
</bodyText>
<equation confidence="0.499647">
Complexity(wi) a − log P(wi|w1...i_1, CONTEXT)
</equation>
<bodyText confidence="0.999874066666667">
Essentially, the surprisal model measures pro-
cessing complexity at a word as a function of
how unexpected the word is in its context. Sur-
prisal is minimized (i.e. approaches zero) when a
word must appear in a given context (i.e., when
P(wi|w1...i_1,CONTEXT) = 1), and approaches
infinity as a word becomes less and less likely.
Crucially, the surprisal theory differs from n-gram
based approaches by using an underlying language
model which includes a lexicon and a syntactic
grammar (the language model is usually a Prob-
abilistic Context-Free Grammar, but not restricted
to it).
To better understand surprisal, consider the fol-
lowing two example sentences:
</bodyText>
<listItem confidence="0.99844675">
(1) The judge who angered the criminal slammed
the gavel.
(2) The judge who the criminal angered slammed
the gavel.
</listItem>
<bodyText confidence="0.999777">
Both sentences are center-embedded relative
clause sentences that differ in whether the subject
or the object is extracted from the relative clause.
Critically, they both share the same words differ-
ing only in their relative order. Behavioral stud-
ies have found that object-extracted relative clause
sentences (2) are harder to process than subject-
extracted relative clause sentences (1) (King and
Just, 1991; Gordon et al., 2001; Grodner and
Gibson, 2005; Staub, 2010; Traxler et al., 2002;
Stromswold et al., 1996). The surprisal values at
each word position of the two example sentences
are shown in Table 1.
As we can see from Table 1, the mean surprisal
value is greater for the object-extracted relative
clause sentence. Hence, the surprisal theory cor-
rectly predicts greater processing cost for that sen-
tence. Furthermore, it allows for a finer-grained
analysis of where the processing cost might occur,
specifically at the onset of the relative clause (the)
and the end (angered). Other differences, such as
greatest difficulty at the main verb are shared with
the subject-extracted relative clause, and are plau-
sible because both sentences are center-embedded.
These predictions are consistent with patterns ob-
served in behavioral studies (Staub, 2010).
In addition to relative clauses, the surprisal the-
ory has been used to model various other behav-
ioral findings (Levy, 2008; Levy and Keller, 2012).
Moreover, corpora analyses examining surprisal’s
effectiveness revealed a high correlation between
word-level suprisal values and the corresponding
reading times, which act as a proxy for processing
difficulties (Demberg and Keller, 2008; Boston et
al., 2008; Frank, 2009; Roark et al., 2009).
Thus, the surprisal theory presents itself as an
effective means of quantifying processing com-
plexity of sentences, and words within them. Next,
we discuss a series of evaluations that we per-
formed to determine whether surprisal values re-
flect quality of written essays.
</bodyText>
<sectionHeader confidence="0.996493" genericHeader="method">
3 Experiment 1
</sectionHeader>
<bodyText confidence="0.999892">
In the first experiment, we investigate whether an
essay’s mean surprisal value decreases after suit-
able English as a Foreign Language (EFL) educa-
tional training. Here, we make the assumption that
EFL training improves a person’s overall writing
quality, and that surprisal value acts as a proxy for
writing quality.
</bodyText>
<page confidence="0.994655">
55
</page>
<table confidence="0.999470833333333">
Topic Term Mean Total Syntactic Lexical
SD Mean SD Mean SD
Analysis Term 1 6.34 3.32 2.37 1.86 3.97 3.24
Arg. Term 2 6.28 3.30 2.34 1.85 3.94 3.23
Term 1 6.24 3.29 2.34 1.85 3.90 3.23
Term 2 6.15 3.36 2.28 1.85 3.87 3.24
</table>
<tableCaption confidence="0.977126">
Table 2: Means and standard deviations of total surprisal, syntactic surprisal, and lexical surprisal for
Analysis and Argumentation essays
</tableCaption>
<bodyText confidence="0.708713">
value of the ith word is calculated as:
PrefixProb(w1...z)
</bodyText>
<subsectionHeader confidence="0.996667">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999531862068965">
We used the Uppsala Student English corpus pro-
vided by the Department of English at Uppsala
University (Axelsson, 2000). The corpus con-
tained 1,489 essays written by 440 Swedish uni-
versity students of English at three different lev-
els. The total number of words was 1,221,265,
and the average length of an essay was 820 words.
The essays were written on a broad range of top-
ics, and their lengths were limited to be between
700-800 words. The topics were divided based on
student education level, with 5 essay topics written
by first-term students, 8 by second-term students,
and 1 by third-term students.
To facilitate comparison, we chose similar top-
ics from the first and second-term sets. We thus
had two sets of essays. The first set consisted
of Analysis essays which are written as a causal
analysis of some topic, such as “television and its
impact on people.” The second set consisted of
Argumentation essays where students argue for or
against a topic or viewpoint. We further imposed
the restriction that only essays written by the same
student in both terms were selected. That is, if a
student wrote an essay on a chosen topic in the first
term, but not the second, or vice-versa, their essay
was not considered. This selection resulted in 38
pairs of Analysis essays and 20 pairs of Argumen-
tation essays across the two terms, for a total of
116 essays.
</bodyText>
<subsectionHeader confidence="0.999681">
3.2 Computing Surprisal
</subsectionHeader>
<bodyText confidence="0.999903555555556">
We computed the surprisal value of each word
in an essay by using a broad-coverage top-down
parser developed by Roark et al. (2009). The
parser was trained on sections 02-24 of the Wall
Street Journal corpus of the Penn Treebank (Mar-
cus et al., 1993). Essentially, the parser com-
putes a word’s surprisal value as the negative log-
probability of the word given the preceding words
using prefix probabilities. Thus, the surprisal
</bodyText>
<equation confidence="0.993214">
SURPRISAL(wz) _ − log
PrefixProb(w1...z_1)
</equation>
<bodyText confidence="0.9999554375">
Moreover, it decomposes each word’s surprisal
value into two components: syntactic surprisal
and lexical surprisal. Syntactic surprisal measures
the degree of unexpectedness of the part-of-speech
category of a word given the word’s sentential con-
text. On the other hand, lexical surprisal measures
the degree of unexpectedness of the word itself
given its sentential context and a part-of-speech
category.
For every essay, we measured the syntactic, lex-
ical, and total (i.e., summed) surprisal values for
each word. Subsequently, the averages of the three
surprisal values were computed for every essay,
and those means were used for further analyses.
Henceforth, surprisal values for an essay refers to
their mean surprisal values.
</bodyText>
<subsectionHeader confidence="0.879509">
3.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999905">
Table 2 reports the means and standard deviations
of the three surprisal measures of the essays.1 As
can be seen, there seems to be a reduction in all
three surprisal values across terms, and second
term essays tend to have a lower mean surprisal
than first term essays. To analyze these differ-
ences, we computed linear mixed-effect regression
models (Baayen, 2008; Baayen et al., 2008) for the
two essay categories. Each model included Term
as a fixed factor and Student as a random intercept.
While our analysis shows that essays in the sec-
ond term have an overall mean surprisal values
less than than essays in the first term, these differ-
ences were not statistically significant. There are a
number of factors that could have influenced these
results. We made an assumption that only a single
term of EFL training could significantly improve
</bodyText>
<footnote confidence="0.998358333333333">
1It is important to note here that these means and standard
deviations are computed on mean surprisal values per essays
and not surprisal values at individual words.
</footnote>
<page confidence="0.957705">
56
</page>
<table confidence="0.9998826">
Total Syntactic Lexical
Score Mean SD Mean SD Mean SD
Low 6.22 0.39 2.46 0.22 3.76 0.29
Medium 6.10 0.34 2.35 0.17 3.75 0.26
High 6.09 0.28 2.27 0.14 3.82 0.24
</table>
<tableCaption confidence="0.925695">
Table 3: Means and standard deviations of total surprisal, syntactic surprisal, and lexical surprisal for the
three different essay score levels
</tableCaption>
<bodyText confidence="0.989909888888889">
essay quality, and hence decrease overall surprisal
values of essays. However, it is likely that a sin-
gle term of training is insufficient, and perhaps the
lack of a significant difference between surprisal
values reflects no improvement in essay quality
across the two terms. Unfortunately, these essays
were not previously scored, and thus we were un-
able to assess whether essay quality improved over
terms.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="method">
4 Experiment 2
</sectionHeader>
<bodyText confidence="0.999964666666667">
In the second experiment, we directly examined
whether surprisal values are related to essay qual-
ity by using a dataset of pre-scored essays.
</bodyText>
<subsectionHeader confidence="0.985667">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99998655">
For this experiment, we used a corpus of essays
written by non-native English speakers. These es-
says are a part of the Educational Testing Service’s
corpus which was used in the first shared task in
Native Language Identification (Blanchard et al.,
2013)2 .
The corpus consisted of 12,100 essays, with a
total number of 4,142,162 words, and the average
length of an essay was 342 words. The essays
were on 8 separate topics, which broadly asked
students to argue for or against a topic or a view-
point. Each essay was labeled with an English lan-
guage proficiency level (High, Medium, or Low)
based on the judgments of human assessment spe-
cialists. The distribution of the essays per score-
category was: Low = 1,325; Medium = 6,533; and
High = 4,172. In order to ensure an equitable com-
parison, and to balance each group, we decided to
choose 1,325 essays per score-category, for a total
of 3,975 essays.
</bodyText>
<subsectionHeader confidence="0.997813">
4.2 Computing Surprisal
</subsectionHeader>
<bodyText confidence="0.999696">
As in Experiment 1, for every essay we measured
the syntactic, lexical, and total surprisal values for
each word. We computed the averages of the three
</bodyText>
<footnote confidence="0.955747">
2Copyright ⃝c 2014 ETS. www.ets.org
</footnote>
<bodyText confidence="0.8931685">
surprisal values, and used those means for further
analysis.
</bodyText>
<subsectionHeader confidence="0.956744">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99986947368421">
Table 3 reports the means and standard deviations
of the three surprisal values for every essay per
score-category. We analyzed the differences be-
tween the means using linear mixed-effects regres-
sion models (Baayen, 2008; Baayen et al., 2008).
Essay Score was treated as a fixed effect and Es-
say Topic was included as a random intercept. The
results indicate that Low-scoring essays had a sig-
nificantly greater mean total surprisal value than
Medium or High-scoring essays. However, the dif-
ference in mean total surprisal values for Medium
and High-scoring essays was not significant. On
the other hand, for syntactic and lexical surprisal,
the means for all three essay score levels were sig-
nificantly different from one another.
We further evaluated the three surprisal values
by performing a correlation test between them and
the essay scores. Table 4 reports the output of the
correlation tests. All three surprisal values were
found to be significantly inversely correlated with
essay scores. However, only syntactic surprisal
obtained a correlation coefficient of a sufficiently
large magnitude of 0.39.
A similar evaluation was performed by Attali
and Burstein (2006) in their evaluation of the
features used in ETS’s e-rater system. Interest-
ingly, the magnitude of the correlation coefficient
for syntactic surprisal reported here is within the
range of coefficients corresponding to e-rater’s
features when they were correlated with TOEFL
essay scores (see Attali and Burstein, 2006, Table
2). Granted, a direct comparison between coef-
ficients is not recommended as the datasets used
were different, such a finding is still promising.
Overall, the results shed a positive light on the use
of surprisal, specifically syntactic surprisal, as a
feature for automated essay scoring.
Despite the promising pattern of our results,
</bodyText>
<page confidence="0.995524">
57
</page>
<table confidence="0.9965095">
Dep Var p t-value p-value
Total -.15 -9.87 &lt; .001
Syntactic -.39 -26.53 &lt; .001
Lexical .08 5.35 &lt; .001
</table>
<tableCaption confidence="0.999769">
Table 4: Pearson’s R coefficients between the three surprisal values and the essay scores
</tableCaption>
<bodyText confidence="0.999451142857143">
they must be taken with a grain of salt. The dataset
that we used did not contain the actual scores of
the essays, and we had to work with broad classi-
fications of essay scores into Low, Medium, and
High score levels. A possible avenue of future
work is to test whether these results hold when us-
ing finer-grain essays scores.
</bodyText>
<sectionHeader confidence="0.993759" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999910266666667">
We proposed the use of the surprisal theory to
quantify sentence processing complexity for use
as a feature in essay scoring. The results are en-
couraging, and warrant further evaluation of sur-
prisal’s effectiveness in determining essay quality.
One point of concern is that the relationship
between mean surprisal values and essay scores
is likely to vary depending on the general qual-
ity of the essays. Here, we used a corpus of es-
says written by non-native English speakers, and
as such, these essays are bound to be of a lower
overall quality than essays written by native En-
glish speakers. For example, consider the fol-
lowing, somewhat questionable, sentences chosen
from the subset of essays having a High score:
</bodyText>
<listItem confidence="0.993366666666667">
(3) Some people might think that traveling in a
group led by a tour guide is a good way.
(4) This is possible only if person understands
ideas and concept.
(5) It is an important decision, how to plan your
syllabus.
</listItem>
<bodyText confidence="0.999962860465116">
These examples suggest that even high-scoring
essays written by non-native English speakers may
not necessarily be flawless, and as such, gram-
matical acceptability may play a crucial role in
determining their overall quality. Therefore, it
is possible that for lower-quality essays, high
surprisal values reflect the presence of gram-
matical errors. On the other hand, for better-
written essays, moderate-to-high surprisal values
may reflect structural variability, which arguably
is preferable to monotonous essays with simpler
sentence structures. Thus, it is likely that the re-
lation between surprisal values and essay scores
depends on the overall quality of the essays in
general. For an equitable evaluation, further tests
will need to determine surprisal’s efficacy over a
broader range of essays.
Another critical point is the choice of corpus
used to compute surprisal. Whatever choice is
made essentially dictates and constrains the gram-
mar of the language under consideration. Here, we
used the WSJ corpus and, thus, implicitly made an
assumption about the underlying language model.
Therefore, in our case, a good essay, i.e. one with
a lower surprisal score, would be one which is
stylistically closer to the WSJ corpus. Future work
will need to investigate the role played by the un-
derlying language model, with special emphasis
on evaluating language models that are specific to
the task at hand. In other words, it would be in-
teresting to compare a surprisal model that is built
using a collection of previous essays with a sur-
prisal model that uses a broader language model.
Lastly, our evaluations were aimed at determin-
ing whether surprisal can be an effective predictor
of essay quality. Further tests will need to evaluate
how well the measure contributes to essay score
predictions when compared to related approaches
that rely on non-syntactic language models, such
as n-grams. Moreover, future work will need to
determine whether adding mean surprisal values to
an AES system results in a performance improve-
ment.
</bodyText>
<sectionHeader confidence="0.997482" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999982285714286">
We are indebted to ETS for sharing their data with
us, and supporting us through this project. This
work would not be possible without their help. We
are also thankful to the reviewers for their help-
ful and encouraging comments. The opinions set
forth in this publication are those of the author(s)
and not ETS.
</bodyText>
<sectionHeader confidence="0.996706" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.853045">
Helen B. Ajay, P. I. Tillett, and Ellis B. Page. 1973.
Analysis of essays by computer (AEC-II). Final
</reference>
<page confidence="0.997376">
58
</page>
<note confidence="0.662338">
Report to the National Center for Educational Re-
search and Development for Project, (8-0101).
</note>
<reference confidence="0.982516058252427">
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater ⃝R v. 2. The Journal of Technol-
ogy, Learning and Assessment, 4(3).
Margareta W. Axelsson. 2000. USE – the Uppsala Stu-
dent English corpus: An instrument for needs anal-
ysis. ICAME Journal, 24:155–157.
Harald R. Baayen, Douglas J. Davidson, and Dou-
glas M. Bates. 2008. Mixed-effects modeling with
crossed random effects for subjects and items. Jour-
nal of memory and language, 59(4):390–412.
Harald R. Baayen. 2008. Analyzing linguistic data:
A practical introduction to statistics using R. Cam-
bridge University Press.
Daniel Blanchard, Joel Tetreault, Derrick Higgins,
Aoife Cahill, and Martin Chodorow. 2013.
TOEFL11: A corpus of non-native english. Edu-
cational Testing Service.
Marisa Boston, John Hale, Reinhold Kliegl, Umesh
Patil, and Shravan Vasishth. 2008. Parsing costs as
predictors of reading difficulty: An evaluation using
the potsdam sentence corpus. Journal of Eye Move-
ment Research, 2(1):1–12.
Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,
and Martin Chodorow. 1998. Enriching automated
essay scoring using discourse marking. In Proceed-
ings of the Workshop on Discourse Relations and
Discourse Marking, pages 206–210.
Alice Davison and Richard Lutz. 1985. Measuring
syntactic complexity relative to discourse context.
In David R. Dowty, Lauri Karttunen, and Arnold M.
Zwicky, editors, Natural language parsing: Psy-
chological, computational, and theoretical perspec-
tives, pages 26–66. Cambridge: Cambridge Univer-
sity Press.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193–210.
Scott Elliot, 2003. Automated essay scoring: a cross
disciplinary approach, chapter IntelliMetric: From
here to validity, pages 71–86. Lawrence Erlbaum
Associates, Mahwah, NJ.
Kenneth Forster and Ilmar Olbrei. 1972. Seman-
tic heuristics and syntactic analysis. Cognition,
2(3):319–347.
Stefan L Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In Proceedings of the 31st annual
conference of the cognitive science society, pages
1139–1144. Cognitive Science Society Austin, TX.
Peter C. Gordon, Randall Hendrick, and Marcus John-
son. 2001. Memory interference during language
processing. Journal of Experimental Psychology:
Learning, Memory and Cognition, 27:1411–1423.
Daniel Grodner and Edward Gibson. 2005. Conse-
quences of the serial nature of linguistic input for
sentenial complexity. Cognitive Science, 29(2):261–
290.
John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, vol-
ume 2, pages 159–166, Pittsburgh, PA.
Gaurav Kharkwal and Karin Stromswold. 2013.
Good-enough language processing: Evidence from
sentence-video matching. Journal ofpsycholinguis-
tic research, 43(1):1–17.
Jonathan King and Marcel A. Just. 1991. Individ-
ual differences in sentence processing: The role of
working memory. Journal of Memory and Lan-
guage, 30:580–602.
Thomas K. Landauer, Darrell Laham, and Peter W.
Foltz, 2003. Automated essay scoring: a cross dis-
ciplinary approach, chapter Automated scoring and
annotation of essays with the Iintelligent Essay As-
sessor, pages 87–112. Lawrence Erlbaum Asso-
ciates, Mahwah, NJ.
Roger Levy and Frank Keller. 2012. Expectation
and locality effects in german verb-final structures.
Journal of Memory and Language.
Roger Levy. 2008. Expectation-based syntactic com-
prehension. Cognition, 106(3):1126–1177.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational linguistics, 19(2):313–330.
Ellis B. Page. 1966. The imminence of grading essays
by computer. Phi Delta Kappan, 47(5):238–243.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 324–333. Association for Com-
putational Linguistics.
Lawrence M. Rudner and Tahung Liang. 2002. Au-
tomated essay scoring using Bayes’ theorem. The
Journal of Technology, Learning and Assessment,
1(2).
Dan Slobin. 1966. Grammatical transformations
and sentence comprehension in childhood and adult-
hood. Journal of Verbal Learning and Verbal Be-
havior, 5(3):219–227.
</reference>
<page confidence="0.982924">
59
</page>
<reference confidence="0.999791363636364">
Adrian Staub. 2010. Eye movements and process-
ing difficulty in object relative clauses. Cognition,
116(1):71–86.
Karin Stromswold, David Caplan, Nathaniel Alpert,
and Scott Rauch. 1996. Localization of syntac-
tic comprehension by position emission tomogra-
phy. Brain and Language, 52:452–473.
Matthew J. Traxler, Robin K. Morris, and Rachel E.
Seely. 2002. Processing subject and object relative
clauses: Evidence from eye movements. Journal of
Memory and Language, 47:69–90.
</reference>
<page confidence="0.998412">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.939388">
<title confidence="0.989633">Surprisal as a Predictor of Essay Quality</title>
<author confidence="0.999226">Gaurav Kharkwal Smaranda Muresan</author>
<affiliation confidence="0.999943333333333">Department of Psychology Department of Computer Science Center for Cognitive Science Center for Computational Learning Systems Rutgers University, New Brunswick Columbia University</affiliation>
<email confidence="0.998621">gaurav.kharkwal@gmail.comsmara@ccls.columbia.edu</email>
<abstract confidence="0.997854565217391">Modern automated essay scoring systems rely on identifying linguistically-relevant features to estimate essay quality. This paper attempts to bridge work in psycholinguistics and natural language processing by proposing sentence processing complexity as a feature for automated essay scoring, in the context of English as a Foreign Language (EFL). To quantify processing complexity we used a psymodel called the- First, we investigated whether essays’ average surprisal values decrease with EFL training. Preliminary results seem to support this idea. Second, we investigated whether surprisal can be effective as a predictor of essay quality. The results indicate an inverse correlation between surprisal and essay scores. Overall, the results are promising and warrant further investigation on the usability of surprisal for essay scoring.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Helen B Ajay</author>
<author>P I Tillett</author>
<author>Ellis B Page</author>
</authors>
<title>Analysis of essays by computer (AEC-II).</title>
<date>1973</date>
<publisher>Final</publisher>
<contexts>
<context position="1888" citStr="Ajay et al., 1973" startWordPosition="267" endWordPosition="270">cation, and an important area of research in educational technologies is the development of tools and methodologies to facilitate automated evaluation of standardized tests. Unlike multiple-choice questions, automated evaluation of essays presents a particular challenge. The specific issue is the identification of a suitable evaluation rubric that can encompass the broad range of responses that may be received. Unsurprisingly then, much emphasis has been placed on the development of Automated Essay Scoring (henceforth, AES) systems. Notable AES systems include Project Essay Grade (Page, 1966; Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al., 1998; Attali and Burstein, 2006), Intelligent Essay AssessorTM (Landauer et al., 2003), BETSY (Rudner and Liang, 2002), and Vantage Learning’s IntelliMetricTM (Elliot, 2003). The common thread in most modern AES systems is the identification of various observable linguistic features, and the development of computational models that combine those features for essay evaluation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without </context>
</contexts>
<marker>Ajay, Tillett, Page, 1973</marker>
<rawString>Helen B. Ajay, P. I. Tillett, and Ellis B. Page. 1973. Analysis of essays by computer (AEC-II). Final</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater ⃝R v. 2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="1956" citStr="Attali and Burstein, 2006" startWordPosition="277" endWordPosition="280">hnologies is the development of tools and methodologies to facilitate automated evaluation of standardized tests. Unlike multiple-choice questions, automated evaluation of essays presents a particular challenge. The specific issue is the identification of a suitable evaluation rubric that can encompass the broad range of responses that may be received. Unsurprisingly then, much emphasis has been placed on the development of Automated Essay Scoring (henceforth, AES) systems. Notable AES systems include Project Essay Grade (Page, 1966; Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al., 1998; Attali and Burstein, 2006), Intelligent Essay AssessorTM (Landauer et al., 2003), BETSY (Rudner and Liang, 2002), and Vantage Learning’s IntelliMetricTM (Elliot, 2003). The common thread in most modern AES systems is the identification of various observable linguistic features, and the development of computational models that combine those features for essay evaluation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of </context>
<context position="15190" citStr="Attali and Burstein (2006)" startWordPosition="2416" endWordPosition="2419">-scoring essays was not significant. On the other hand, for syntactic and lexical surprisal, the means for all three essay score levels were significantly different from one another. We further evaluated the three surprisal values by performing a correlation test between them and the essay scores. Table 4 reports the output of the correlation tests. All three surprisal values were found to be significantly inversely correlated with essay scores. However, only syntactic surprisal obtained a correlation coefficient of a sufficiently large magnitude of 0.39. A similar evaluation was performed by Attali and Burstein (2006) in their evaluation of the features used in ETS’s e-rater system. Interestingly, the magnitude of the correlation coefficient for syntactic surprisal reported here is within the range of coefficients corresponding to e-rater’s features when they were correlated with TOEFL essay scores (see Attali and Burstein, 2006, Table 2). Granted, a direct comparison between coefficients is not recommended as the datasets used were different, such a finding is still promising. Overall, the results shed a positive light on the use of surprisal, specifically syntactic surprisal, as a feature for automated e</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater ⃝R v. 2. The Journal of Technology, Learning and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margareta W Axelsson</author>
</authors>
<title>USE – the Uppsala Student English corpus: An instrument for needs analysis.</title>
<date>2000</date>
<journal>ICAME Journal,</journal>
<pages>24--155</pages>
<contexts>
<context position="8414" citStr="Axelsson, 2000" startWordPosition="1303" endWordPosition="1304">and that surprisal value acts as a proxy for writing quality. 55 Topic Term Mean Total Syntactic Lexical SD Mean SD Mean SD Analysis Term 1 6.34 3.32 2.37 1.86 3.97 3.24 Arg. Term 2 6.28 3.30 2.34 1.85 3.94 3.23 Term 1 6.24 3.29 2.34 1.85 3.90 3.23 Term 2 6.15 3.36 2.28 1.85 3.87 3.24 Table 2: Means and standard deviations of total surprisal, syntactic surprisal, and lexical surprisal for Analysis and Argumentation essays value of the ith word is calculated as: PrefixProb(w1...z) 3.1 Corpus We used the Uppsala Student English corpus provided by the Department of English at Uppsala University (Axelsson, 2000). The corpus contained 1,489 essays written by 440 Swedish university students of English at three different levels. The total number of words was 1,221,265, and the average length of an essay was 820 words. The essays were written on a broad range of topics, and their lengths were limited to be between 700-800 words. The topics were divided based on student education level, with 5 essay topics written by first-term students, 8 by second-term students, and 1 by third-term students. To facilitate comparison, we chose similar topics from the first and second-term sets. We thus had two sets of es</context>
</contexts>
<marker>Axelsson, 2000</marker>
<rawString>Margareta W. Axelsson. 2000. USE – the Uppsala Student English corpus: An instrument for needs analysis. ICAME Journal, 24:155–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald R Baayen</author>
<author>Douglas J Davidson</author>
<author>Douglas M Bates</author>
</authors>
<title>Mixed-effects modeling with crossed random effects for subjects and items.</title>
<date>2008</date>
<journal>Journal of memory and language,</journal>
<pages>59--4</pages>
<contexts>
<context position="11302" citStr="Baayen et al., 2008" startWordPosition="1777" endWordPosition="1780">ly, the averages of the three surprisal values were computed for every essay, and those means were used for further analyses. Henceforth, surprisal values for an essay refers to their mean surprisal values. 3.3 Results and Discussion Table 2 reports the means and standard deviations of the three surprisal measures of the essays.1 As can be seen, there seems to be a reduction in all three surprisal values across terms, and second term essays tend to have a lower mean surprisal than first term essays. To analyze these differences, we computed linear mixed-effect regression models (Baayen, 2008; Baayen et al., 2008) for the two essay categories. Each model included Term as a fixed factor and Student as a random intercept. While our analysis shows that essays in the second term have an overall mean surprisal values less than than essays in the first term, these differences were not statistically significant. There are a number of factors that could have influenced these results. We made an assumption that only a single term of EFL training could significantly improve 1It is important to note here that these means and standard deviations are computed on mean surprisal values per essays and not surprisal va</context>
<context position="14258" citStr="Baayen et al., 2008" startWordPosition="2271" endWordPosition="2274">cided to choose 1,325 essays per score-category, for a total of 3,975 essays. 4.2 Computing Surprisal As in Experiment 1, for every essay we measured the syntactic, lexical, and total surprisal values for each word. We computed the averages of the three 2Copyright ⃝c 2014 ETS. www.ets.org surprisal values, and used those means for further analysis. 4.3 Results and Discussion Table 3 reports the means and standard deviations of the three surprisal values for every essay per score-category. We analyzed the differences between the means using linear mixed-effects regression models (Baayen, 2008; Baayen et al., 2008). Essay Score was treated as a fixed effect and Essay Topic was included as a random intercept. The results indicate that Low-scoring essays had a significantly greater mean total surprisal value than Medium or High-scoring essays. However, the difference in mean total surprisal values for Medium and High-scoring essays was not significant. On the other hand, for syntactic and lexical surprisal, the means for all three essay score levels were significantly different from one another. We further evaluated the three surprisal values by performing a correlation test between them and the essay sco</context>
</contexts>
<marker>Baayen, Davidson, Bates, 2008</marker>
<rawString>Harald R. Baayen, Douglas J. Davidson, and Douglas M. Bates. 2008. Mixed-effects modeling with crossed random effects for subjects and items. Journal of memory and language, 59(4):390–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald R Baayen</author>
</authors>
<title>Analyzing linguistic data: A practical introduction to statistics using R.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="11280" citStr="Baayen, 2008" startWordPosition="1775" endWordPosition="1776">rd. Subsequently, the averages of the three surprisal values were computed for every essay, and those means were used for further analyses. Henceforth, surprisal values for an essay refers to their mean surprisal values. 3.3 Results and Discussion Table 2 reports the means and standard deviations of the three surprisal measures of the essays.1 As can be seen, there seems to be a reduction in all three surprisal values across terms, and second term essays tend to have a lower mean surprisal than first term essays. To analyze these differences, we computed linear mixed-effect regression models (Baayen, 2008; Baayen et al., 2008) for the two essay categories. Each model included Term as a fixed factor and Student as a random intercept. While our analysis shows that essays in the second term have an overall mean surprisal values less than than essays in the first term, these differences were not statistically significant. There are a number of factors that could have influenced these results. We made an assumption that only a single term of EFL training could significantly improve 1It is important to note here that these means and standard deviations are computed on mean surprisal values per essay</context>
<context position="14236" citStr="Baayen, 2008" startWordPosition="2269" endWordPosition="2270">h group, we decided to choose 1,325 essays per score-category, for a total of 3,975 essays. 4.2 Computing Surprisal As in Experiment 1, for every essay we measured the syntactic, lexical, and total surprisal values for each word. We computed the averages of the three 2Copyright ⃝c 2014 ETS. www.ets.org surprisal values, and used those means for further analysis. 4.3 Results and Discussion Table 3 reports the means and standard deviations of the three surprisal values for every essay per score-category. We analyzed the differences between the means using linear mixed-effects regression models (Baayen, 2008; Baayen et al., 2008). Essay Score was treated as a fixed effect and Essay Topic was included as a random intercept. The results indicate that Low-scoring essays had a significantly greater mean total surprisal value than Medium or High-scoring essays. However, the difference in mean total surprisal values for Medium and High-scoring essays was not significant. On the other hand, for syntactic and lexical surprisal, the means for all three essay score levels were significantly different from one another. We further evaluated the three surprisal values by performing a correlation test between </context>
</contexts>
<marker>Baayen, 2008</marker>
<rawString>Harald R. Baayen. 2008. Analyzing linguistic data: A practical introduction to statistics using R. Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Blanchard</author>
<author>Joel Tetreault</author>
</authors>
<location>Derrick Higgins,</location>
<marker>Blanchard, Tetreault, </marker>
<rawString>Daniel Blanchard, Joel Tetreault, Derrick Higgins,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Chodorow</author>
</authors>
<title>TOEFL11: A corpus of non-native english. Educational Testing Service.</title>
<date>2013</date>
<marker>Cahill, Chodorow, 2013</marker>
<rawString>Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A corpus of non-native english. Educational Testing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marisa Boston</author>
<author>John Hale</author>
<author>Reinhold Kliegl</author>
<author>Umesh Patil</author>
<author>Shravan Vasishth</author>
</authors>
<title>Parsing costs as predictors of reading difficulty: An evaluation using the potsdam sentence corpus.</title>
<date>2008</date>
<journal>Journal of Eye Movement Research,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="7222" citStr="Boston et al., 2008" startWordPosition="1104" endWordPosition="1107">rb are shared with the subject-extracted relative clause, and are plausible because both sentences are center-embedded. These predictions are consistent with patterns observed in behavioral studies (Staub, 2010). In addition to relative clauses, the surprisal theory has been used to model various other behavioral findings (Levy, 2008; Levy and Keller, 2012). Moreover, corpora analyses examining surprisal’s effectiveness revealed a high correlation between word-level suprisal values and the corresponding reading times, which act as a proxy for processing difficulties (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009). Thus, the surprisal theory presents itself as an effective means of quantifying processing complexity of sentences, and words within them. Next, we discuss a series of evaluations that we performed to determine whether surprisal values reflect quality of written essays. 3 Experiment 1 In the first experiment, we investigate whether an essay’s mean surprisal value decreases after suitable English as a Foreign Language (EFL) educational training. Here, we make the assumption that EFL training improves a person’s overall writing quality, and that surprisal valu</context>
</contexts>
<marker>Boston, Hale, Kliegl, Patil, Vasishth, 2008</marker>
<rawString>Marisa Boston, John Hale, Reinhold Kliegl, Umesh Patil, and Shravan Vasishth. 2008. Parsing costs as predictors of reading difficulty: An evaluation using the potsdam sentence corpus. Journal of Eye Movement Research, 2(1):1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Karen Kukich</author>
<author>Susanne Wolff</author>
<author>Chi Lu</author>
<author>Martin Chodorow</author>
</authors>
<title>Enriching automated essay scoring using discourse marking.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Discourse Relations and Discourse Marking,</booktitle>
<pages>206--210</pages>
<contexts>
<context position="1928" citStr="Burstein et al., 1998" startWordPosition="273" endWordPosition="276">arch in educational technologies is the development of tools and methodologies to facilitate automated evaluation of standardized tests. Unlike multiple-choice questions, automated evaluation of essays presents a particular challenge. The specific issue is the identification of a suitable evaluation rubric that can encompass the broad range of responses that may be received. Unsurprisingly then, much emphasis has been placed on the development of Automated Essay Scoring (henceforth, AES) systems. Notable AES systems include Project Essay Grade (Page, 1966; Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al., 1998; Attali and Burstein, 2006), Intelligent Essay AssessorTM (Landauer et al., 2003), BETSY (Rudner and Liang, 2002), and Vantage Learning’s IntelliMetricTM (Elliot, 2003). The common thread in most modern AES systems is the identification of various observable linguistic features, and the development of computational models that combine those features for essay evaluation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of th</context>
</contexts>
<marker>Burstein, Kukich, Wolff, Lu, Chodorow, 1998</marker>
<rawString>Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu, and Martin Chodorow. 1998. Enriching automated essay scoring using discourse marking. In Proceedings of the Workshop on Discourse Relations and Discourse Marking, pages 206–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Davison</author>
<author>Richard Lutz</author>
</authors>
<title>Measuring syntactic complexity relative to discourse context.</title>
<date>1985</date>
<booktitle>Natural language parsing: Psychological, computational, and theoretical perspectives,</booktitle>
<pages>26--66</pages>
<editor>In David R. Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors,</editor>
<publisher>Cambridge: Cambridge University Press.</publisher>
<contexts>
<context position="2957" citStr="Davison and Lutz, 1985" startWordPosition="438" endWordPosition="441"> that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of good writing. Decades of behavioral research on language comprehension has suggested that some sentence structures are harder to comprehend than others. For example, passive sentences, such as the girl was pushed by the boy, are known to be harder to process than semantically equivalent active sentences, such as the boy pushed the girl (Slobin, 1966; Forster and Olbrei, 1972; Davison and Lutz, 1985; Kharkwal and Stromswold, 2013). Thus, it is likely that the overall processing complexity of the sentence structures used in an essay could influence its perceived quality. One reason why sentence processing complexity has not yet been fully utilized is the lack of a suitable way of quantifying it. This paper proposes the use of a psycholinguistic model of sentence comprehension called surprisal theory (Hale, 2001; Levy, 2008) to quantify sentence processing complexity. The rest of the paper is organized as follows. Section 2 describes the surprisal theory, and discusses its applicability in</context>
</contexts>
<marker>Davison, Lutz, 1985</marker>
<rawString>Alice Davison and Richard Lutz. 1985. Measuring syntactic complexity relative to discourse context. In David R. Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors, Natural language parsing: Psychological, computational, and theoretical perspectives, pages 26–66. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
<author>Frank Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>109--193</pages>
<contexts>
<context position="7201" citStr="Demberg and Keller, 2008" startWordPosition="1100" endWordPosition="1103"> difficulty at the main verb are shared with the subject-extracted relative clause, and are plausible because both sentences are center-embedded. These predictions are consistent with patterns observed in behavioral studies (Staub, 2010). In addition to relative clauses, the surprisal theory has been used to model various other behavioral findings (Levy, 2008; Levy and Keller, 2012). Moreover, corpora analyses examining surprisal’s effectiveness revealed a high correlation between word-level suprisal values and the corresponding reading times, which act as a proxy for processing difficulties (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009). Thus, the surprisal theory presents itself as an effective means of quantifying processing complexity of sentences, and words within them. Next, we discuss a series of evaluations that we performed to determine whether surprisal values reflect quality of written essays. 3 Experiment 1 In the first experiment, we investigate whether an essay’s mean surprisal value decreases after suitable English as a Foreign Language (EFL) educational training. Here, we make the assumption that EFL training improves a person’s overall writing quality, an</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>Vera Demberg and Frank Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109:193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Elliot</author>
</authors>
<title>Automated essay scoring: a cross disciplinary approach, chapter IntelliMetric: From here to validity, pages 71–86. Lawrence Erlbaum Associates,</title>
<date>2003</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="2097" citStr="Elliot, 2003" startWordPosition="299" endWordPosition="300">ted evaluation of essays presents a particular challenge. The specific issue is the identification of a suitable evaluation rubric that can encompass the broad range of responses that may be received. Unsurprisingly then, much emphasis has been placed on the development of Automated Essay Scoring (henceforth, AES) systems. Notable AES systems include Project Essay Grade (Page, 1966; Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al., 1998; Attali and Burstein, 2006), Intelligent Essay AssessorTM (Landauer et al., 2003), BETSY (Rudner and Liang, 2002), and Vantage Learning’s IntelliMetricTM (Elliot, 2003). The common thread in most modern AES systems is the identification of various observable linguistic features, and the development of computational models that combine those features for essay evaluation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of good writing. Decades of behavioral research on language comprehension has suggested that some sentence structures are harder to comprehend t</context>
</contexts>
<marker>Elliot, 2003</marker>
<rawString>Scott Elliot, 2003. Automated essay scoring: a cross disciplinary approach, chapter IntelliMetric: From here to validity, pages 71–86. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Forster</author>
<author>Ilmar Olbrei</author>
</authors>
<title>Semantic heuristics and syntactic analysis.</title>
<date>1972</date>
<journal>Cognition,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="2933" citStr="Forster and Olbrei, 1972" startWordPosition="433" endWordPosition="437">pect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of good writing. Decades of behavioral research on language comprehension has suggested that some sentence structures are harder to comprehend than others. For example, passive sentences, such as the girl was pushed by the boy, are known to be harder to process than semantically equivalent active sentences, such as the boy pushed the girl (Slobin, 1966; Forster and Olbrei, 1972; Davison and Lutz, 1985; Kharkwal and Stromswold, 2013). Thus, it is likely that the overall processing complexity of the sentence structures used in an essay could influence its perceived quality. One reason why sentence processing complexity has not yet been fully utilized is the lack of a suitable way of quantifying it. This paper proposes the use of a psycholinguistic model of sentence comprehension called surprisal theory (Hale, 2001; Levy, 2008) to quantify sentence processing complexity. The rest of the paper is organized as follows. Section 2 describes the surprisal theory, and discus</context>
</contexts>
<marker>Forster, Olbrei, 1972</marker>
<rawString>Kenneth Forster and Ilmar Olbrei. 1972. Semantic heuristics and syntactic analysis. Cognition, 2(3):319–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan L Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st annual conference of the cognitive science society,</booktitle>
<pages>1139--1144</pages>
<publisher>Cognitive Science Society</publisher>
<location>Austin, TX.</location>
<contexts>
<context position="7235" citStr="Frank, 2009" startWordPosition="1108" endWordPosition="1109">e subject-extracted relative clause, and are plausible because both sentences are center-embedded. These predictions are consistent with patterns observed in behavioral studies (Staub, 2010). In addition to relative clauses, the surprisal theory has been used to model various other behavioral findings (Levy, 2008; Levy and Keller, 2012). Moreover, corpora analyses examining surprisal’s effectiveness revealed a high correlation between word-level suprisal values and the corresponding reading times, which act as a proxy for processing difficulties (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009). Thus, the surprisal theory presents itself as an effective means of quantifying processing complexity of sentences, and words within them. Next, we discuss a series of evaluations that we performed to determine whether surprisal values reflect quality of written essays. 3 Experiment 1 In the first experiment, we investigate whether an essay’s mean surprisal value decreases after suitable English as a Foreign Language (EFL) educational training. Here, we make the assumption that EFL training improves a person’s overall writing quality, and that surprisal value acts as a p</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>Stefan L Frank. 2009. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In Proceedings of the 31st annual conference of the cognitive science society, pages 1139–1144. Cognitive Science Society Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter C Gordon</author>
<author>Randall Hendrick</author>
<author>Marcus Johnson</author>
</authors>
<title>Memory interference during language processing.</title>
<date>2001</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition,</journal>
<pages>27--1411</pages>
<contexts>
<context position="5982" citStr="Gordon et al., 2001" startWordPosition="914" endWordPosition="917">etter understand surprisal, consider the following two example sentences: (1) The judge who angered the criminal slammed the gavel. (2) The judge who the criminal angered slammed the gavel. Both sentences are center-embedded relative clause sentences that differ in whether the subject or the object is extracted from the relative clause. Critically, they both share the same words differing only in their relative order. Behavioral studies have found that object-extracted relative clause sentences (2) are harder to process than subjectextracted relative clause sentences (1) (King and Just, 1991; Gordon et al., 2001; Grodner and Gibson, 2005; Staub, 2010; Traxler et al., 2002; Stromswold et al., 1996). The surprisal values at each word position of the two example sentences are shown in Table 1. As we can see from Table 1, the mean surprisal value is greater for the object-extracted relative clause sentence. Hence, the surprisal theory correctly predicts greater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, such as greatest diff</context>
</contexts>
<marker>Gordon, Hendrick, Johnson, 2001</marker>
<rawString>Peter C. Gordon, Randall Hendrick, and Marcus Johnson. 2001. Memory interference during language processing. Journal of Experimental Psychology: Learning, Memory and Cognition, 27:1411–1423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Grodner</author>
<author>Edward Gibson</author>
</authors>
<title>Consequences of the serial nature of linguistic input for sentenial complexity.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>2</issue>
<pages>290</pages>
<contexts>
<context position="6008" citStr="Grodner and Gibson, 2005" startWordPosition="918" endWordPosition="921">risal, consider the following two example sentences: (1) The judge who angered the criminal slammed the gavel. (2) The judge who the criminal angered slammed the gavel. Both sentences are center-embedded relative clause sentences that differ in whether the subject or the object is extracted from the relative clause. Critically, they both share the same words differing only in their relative order. Behavioral studies have found that object-extracted relative clause sentences (2) are harder to process than subjectextracted relative clause sentences (1) (King and Just, 1991; Gordon et al., 2001; Grodner and Gibson, 2005; Staub, 2010; Traxler et al., 2002; Stromswold et al., 1996). The surprisal values at each word position of the two example sentences are shown in Table 1. As we can see from Table 1, the mean surprisal value is greater for the object-extracted relative clause sentence. Hence, the surprisal theory correctly predicts greater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, such as greatest difficulty at the main verb ar</context>
</contexts>
<marker>Grodner, Gibson, 2005</marker>
<rawString>Daniel Grodner and Edward Gibson. 2005. Consequences of the serial nature of linguistic input for sentenial complexity. Cognitive Science, 29(2):261– 290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3376" citStr="Hale, 2001" startWordPosition="508" endWordPosition="509">ed by the boy, are known to be harder to process than semantically equivalent active sentences, such as the boy pushed the girl (Slobin, 1966; Forster and Olbrei, 1972; Davison and Lutz, 1985; Kharkwal and Stromswold, 2013). Thus, it is likely that the overall processing complexity of the sentence structures used in an essay could influence its perceived quality. One reason why sentence processing complexity has not yet been fully utilized is the lack of a suitable way of quantifying it. This paper proposes the use of a psycholinguistic model of sentence comprehension called surprisal theory (Hale, 2001; Levy, 2008) to quantify sentence processing complexity. The rest of the paper is organized as follows. Section 2 describes the surprisal theory, and discusses its applicability in modeling sentence processing complexity. Section 3 details our investigation on whether essays’ average surprisal values decrease following English as a Foreign Language training. Section 4 presents a study where we investigated whether surprisal can be effective as a predictor of essay quality. Lastly, Sec54 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 54</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics, volume 2, pages 159–166, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gaurav Kharkwal</author>
<author>Karin Stromswold</author>
</authors>
<title>Good-enough language processing: Evidence from sentence-video matching.</title>
<date>2013</date>
<journal>Journal ofpsycholinguistic research,</journal>
<pages>43--1</pages>
<contexts>
<context position="2989" citStr="Kharkwal and Stromswold, 2013" startWordPosition="442" endWordPosition="445">tems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of good writing. Decades of behavioral research on language comprehension has suggested that some sentence structures are harder to comprehend than others. For example, passive sentences, such as the girl was pushed by the boy, are known to be harder to process than semantically equivalent active sentences, such as the boy pushed the girl (Slobin, 1966; Forster and Olbrei, 1972; Davison and Lutz, 1985; Kharkwal and Stromswold, 2013). Thus, it is likely that the overall processing complexity of the sentence structures used in an essay could influence its perceived quality. One reason why sentence processing complexity has not yet been fully utilized is the lack of a suitable way of quantifying it. This paper proposes the use of a psycholinguistic model of sentence comprehension called surprisal theory (Hale, 2001; Levy, 2008) to quantify sentence processing complexity. The rest of the paper is organized as follows. Section 2 describes the surprisal theory, and discusses its applicability in modeling sentence processing co</context>
</contexts>
<marker>Kharkwal, Stromswold, 2013</marker>
<rawString>Gaurav Kharkwal and Karin Stromswold. 2013. Good-enough language processing: Evidence from sentence-video matching. Journal ofpsycholinguistic research, 43(1):1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan King</author>
<author>Marcel A Just</author>
</authors>
<title>Individual differences in sentence processing: The role of working memory.</title>
<date>1991</date>
<journal>Journal of Memory and Language,</journal>
<pages>30--580</pages>
<contexts>
<context position="5961" citStr="King and Just, 1991" startWordPosition="910" endWordPosition="913">stricted to it). To better understand surprisal, consider the following two example sentences: (1) The judge who angered the criminal slammed the gavel. (2) The judge who the criminal angered slammed the gavel. Both sentences are center-embedded relative clause sentences that differ in whether the subject or the object is extracted from the relative clause. Critically, they both share the same words differing only in their relative order. Behavioral studies have found that object-extracted relative clause sentences (2) are harder to process than subjectextracted relative clause sentences (1) (King and Just, 1991; Gordon et al., 2001; Grodner and Gibson, 2005; Staub, 2010; Traxler et al., 2002; Stromswold et al., 1996). The surprisal values at each word position of the two example sentences are shown in Table 1. As we can see from Table 1, the mean surprisal value is greater for the object-extracted relative clause sentence. Hence, the surprisal theory correctly predicts greater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, </context>
</contexts>
<marker>King, Just, 1991</marker>
<rawString>Jonathan King and Marcel A. Just. 1991. Individual differences in sentence processing: The role of working memory. Journal of Memory and Language, 30:580–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Darrell Laham</author>
<author>Peter W Foltz</author>
</authors>
<title>Automated essay scoring: a cross disciplinary approach, chapter Automated scoring and annotation of essays with the Iintelligent Essay Assessor, pages 87–112. Lawrence Erlbaum Associates,</title>
<date>2003</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="2010" citStr="Landauer et al., 2003" startWordPosition="285" endWordPosition="288"> facilitate automated evaluation of standardized tests. Unlike multiple-choice questions, automated evaluation of essays presents a particular challenge. The specific issue is the identification of a suitable evaluation rubric that can encompass the broad range of responses that may be received. Unsurprisingly then, much emphasis has been placed on the development of Automated Essay Scoring (henceforth, AES) systems. Notable AES systems include Project Essay Grade (Page, 1966; Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al., 1998; Attali and Burstein, 2006), Intelligent Essay AssessorTM (Landauer et al., 2003), BETSY (Rudner and Liang, 2002), and Vantage Learning’s IntelliMetricTM (Elliot, 2003). The common thread in most modern AES systems is the identification of various observable linguistic features, and the development of computational models that combine those features for essay evaluation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of good writing. Decades of behavioral research on langua</context>
</contexts>
<marker>Landauer, Laham, Foltz, 2003</marker>
<rawString>Thomas K. Landauer, Darrell Laham, and Peter W. Foltz, 2003. Automated essay scoring: a cross disciplinary approach, chapter Automated scoring and annotation of essays with the Iintelligent Essay Assessor, pages 87–112. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Frank Keller</author>
</authors>
<title>Expectation and locality effects in german verb-final structures.</title>
<date>2012</date>
<journal>Journal of Memory and Language.</journal>
<contexts>
<context position="6962" citStr="Levy and Keller, 2012" startWordPosition="1069" endWordPosition="1072">ssing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, such as greatest difficulty at the main verb are shared with the subject-extracted relative clause, and are plausible because both sentences are center-embedded. These predictions are consistent with patterns observed in behavioral studies (Staub, 2010). In addition to relative clauses, the surprisal theory has been used to model various other behavioral findings (Levy, 2008; Levy and Keller, 2012). Moreover, corpora analyses examining surprisal’s effectiveness revealed a high correlation between word-level suprisal values and the corresponding reading times, which act as a proxy for processing difficulties (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009). Thus, the surprisal theory presents itself as an effective means of quantifying processing complexity of sentences, and words within them. Next, we discuss a series of evaluations that we performed to determine whether surprisal values reflect quality of written essays. 3 Experiment 1 In the first exper</context>
</contexts>
<marker>Levy, Keller, 2012</marker>
<rawString>Roger Levy and Frank Keller. 2012. Expectation and locality effects in german verb-final structures. Journal of Memory and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<volume>106</volume>
<issue>3</issue>
<contexts>
<context position="3389" citStr="Levy, 2008" startWordPosition="510" endWordPosition="511">y, are known to be harder to process than semantically equivalent active sentences, such as the boy pushed the girl (Slobin, 1966; Forster and Olbrei, 1972; Davison and Lutz, 1985; Kharkwal and Stromswold, 2013). Thus, it is likely that the overall processing complexity of the sentence structures used in an essay could influence its perceived quality. One reason why sentence processing complexity has not yet been fully utilized is the lack of a suitable way of quantifying it. This paper proposes the use of a psycholinguistic model of sentence comprehension called surprisal theory (Hale, 2001; Levy, 2008) to quantify sentence processing complexity. The rest of the paper is organized as follows. Section 2 describes the surprisal theory, and discusses its applicability in modeling sentence processing complexity. Section 3 details our investigation on whether essays’ average surprisal values decrease following English as a Foreign Language training. Section 4 presents a study where we investigated whether surprisal can be effective as a predictor of essay quality. Lastly, Sec54 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 54–60, Baltimor</context>
<context position="6938" citStr="Levy, 2008" startWordPosition="1067" endWordPosition="1068">reater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, such as greatest difficulty at the main verb are shared with the subject-extracted relative clause, and are plausible because both sentences are center-embedded. These predictions are consistent with patterns observed in behavioral studies (Staub, 2010). In addition to relative clauses, the surprisal theory has been used to model various other behavioral findings (Levy, 2008; Levy and Keller, 2012). Moreover, corpora analyses examining surprisal’s effectiveness revealed a high correlation between word-level suprisal values and the corresponding reading times, which act as a proxy for processing difficulties (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009). Thus, the surprisal theory presents itself as an effective means of quantifying processing complexity of sentences, and words within them. Next, we discuss a series of evaluations that we performed to determine whether surprisal values reflect quality of written essays. 3 Experim</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>Roger Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106(3):1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<contexts>
<context position="9936" citStr="Marcus et al., 1993" startWordPosition="1566" endWordPosition="1570">essays written by the same student in both terms were selected. That is, if a student wrote an essay on a chosen topic in the first term, but not the second, or vice-versa, their essay was not considered. This selection resulted in 38 pairs of Analysis essays and 20 pairs of Argumentation essays across the two terms, for a total of 116 essays. 3.2 Computing Surprisal We computed the surprisal value of each word in an essay by using a broad-coverage top-down parser developed by Roark et al. (2009). The parser was trained on sections 02-24 of the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993). Essentially, the parser computes a word’s surprisal value as the negative logprobability of the word given the preceding words using prefix probabilities. Thus, the surprisal SURPRISAL(wz) _ − log PrefixProb(w1...z_1) Moreover, it decomposes each word’s surprisal value into two components: syntactic surprisal and lexical surprisal. Syntactic surprisal measures the degree of unexpectedness of the part-of-speech category of a word given the word’s sentential context. On the other hand, lexical surprisal measures the degree of unexpectedness of the word itself given its sentential context and a</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellis B Page</author>
</authors>
<title>The imminence of grading essays by computer.</title>
<date>1966</date>
<journal>Phi Delta Kappan,</journal>
<volume>47</volume>
<issue>5</issue>
<contexts>
<context position="1868" citStr="Page, 1966" startWordPosition="265" endWordPosition="266">dern-day education, and an important area of research in educational technologies is the development of tools and methodologies to facilitate automated evaluation of standardized tests. Unlike multiple-choice questions, automated evaluation of essays presents a particular challenge. The specific issue is the identification of a suitable evaluation rubric that can encompass the broad range of responses that may be received. Unsurprisingly then, much emphasis has been placed on the development of Automated Essay Scoring (henceforth, AES) systems. Notable AES systems include Project Essay Grade (Page, 1966; Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al., 1998; Attali and Burstein, 2006), Intelligent Essay AssessorTM (Landauer et al., 2003), BETSY (Rudner and Liang, 2002), and Vantage Learning’s IntelliMetricTM (Elliot, 2003). The common thread in most modern AES systems is the identification of various observable linguistic features, and the development of computational models that combine those features for essay evaluation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey </context>
</contexts>
<marker>Page, 1966</marker>
<rawString>Ellis B. Page. 1966. The imminence of grading essays by computer. Phi Delta Kappan, 47(5):238–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Asaf Bachrach</author>
<author>Carlos Cardenas</author>
<author>Christophe Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>324--333</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4429" citStr="Roark et al. (2009)" startWordPosition="673" endWordPosition="676">e effective as a predictor of essay quality. Lastly, Sec54 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 54–60, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics The judge who angered the criminal slammed the gavel Mean 5.64 6.94 6.93 11.60 2.32 9.19 16.92 1.94 4.68 7.35 The judge who the criminal angered slammed the gavel Mean 5.64 6.94 6.93 4.20 9.21 13.73 16.65 2.21 4.69 7.80 Table 1: Surprisal values of two example relative-clause sentences. The values were computed using a top-down parser by Roark et al. (2009) trained on the Wall Street Journal corpus. tion 5 concludes the paper. 2 Surprisal Theory The surprisal theory (Hale, 2001; Levy, 2008) estimates the word-level processing complexity as the negative log-probability of a word given the preceding context (usually, preceding syntactic context). That is: Complexity(wi) a − log P(wi|w1...i_1, CONTEXT) Essentially, the surprisal model measures processing complexity at a word as a function of how unexpected the word is in its context. Surprisal is minimized (i.e. approaches zero) when a word must appear in a given context (i.e., when P(wi|w1...i_1,C</context>
<context position="7256" citStr="Roark et al., 2009" startWordPosition="1110" endWordPosition="1113">racted relative clause, and are plausible because both sentences are center-embedded. These predictions are consistent with patterns observed in behavioral studies (Staub, 2010). In addition to relative clauses, the surprisal theory has been used to model various other behavioral findings (Levy, 2008; Levy and Keller, 2012). Moreover, corpora analyses examining surprisal’s effectiveness revealed a high correlation between word-level suprisal values and the corresponding reading times, which act as a proxy for processing difficulties (Demberg and Keller, 2008; Boston et al., 2008; Frank, 2009; Roark et al., 2009). Thus, the surprisal theory presents itself as an effective means of quantifying processing complexity of sentences, and words within them. Next, we discuss a series of evaluations that we performed to determine whether surprisal values reflect quality of written essays. 3 Experiment 1 In the first experiment, we investigate whether an essay’s mean surprisal value decreases after suitable English as a Foreign Language (EFL) educational training. Here, we make the assumption that EFL training improves a person’s overall writing quality, and that surprisal value acts as a proxy for writing qual</context>
<context position="9817" citStr="Roark et al. (2009)" startWordPosition="1545" endWordPosition="1548">ntation essays where students argue for or against a topic or viewpoint. We further imposed the restriction that only essays written by the same student in both terms were selected. That is, if a student wrote an essay on a chosen topic in the first term, but not the second, or vice-versa, their essay was not considered. This selection resulted in 38 pairs of Analysis essays and 20 pairs of Argumentation essays across the two terms, for a total of 116 essays. 3.2 Computing Surprisal We computed the surprisal value of each word in an essay by using a broad-coverage top-down parser developed by Roark et al. (2009). The parser was trained on sections 02-24 of the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993). Essentially, the parser computes a word’s surprisal value as the negative logprobability of the word given the preceding words using prefix probabilities. Thus, the surprisal SURPRISAL(wz) _ − log PrefixProb(w1...z_1) Moreover, it decomposes each word’s surprisal value into two components: syntactic surprisal and lexical surprisal. Syntactic surprisal measures the degree of unexpectedness of the part-of-speech category of a word given the word’s sentential context. On the ot</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>Brian Roark, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 324–333. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence M Rudner</author>
<author>Tahung Liang</author>
</authors>
<title>Automated essay scoring using Bayes’ theorem.</title>
<date>2002</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="2042" citStr="Rudner and Liang, 2002" startWordPosition="290" endWordPosition="293">n of standardized tests. Unlike multiple-choice questions, automated evaluation of essays presents a particular challenge. The specific issue is the identification of a suitable evaluation rubric that can encompass the broad range of responses that may be received. Unsurprisingly then, much emphasis has been placed on the development of Automated Essay Scoring (henceforth, AES) systems. Notable AES systems include Project Essay Grade (Page, 1966; Ajay et al., 1973), ETS’s e-rater⃝R (Burstein et al., 1998; Attali and Burstein, 2006), Intelligent Essay AssessorTM (Landauer et al., 2003), BETSY (Rudner and Liang, 2002), and Vantage Learning’s IntelliMetricTM (Elliot, 2003). The common thread in most modern AES systems is the identification of various observable linguistic features, and the development of computational models that combine those features for essay evaluation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of good writing. Decades of behavioral research on language comprehension has suggested t</context>
</contexts>
<marker>Rudner, Liang, 2002</marker>
<rawString>Lawrence M. Rudner and Tahung Liang. 2002. Automated essay scoring using Bayes’ theorem. The Journal of Technology, Learning and Assessment, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Slobin</author>
</authors>
<title>Grammatical transformations and sentence comprehension in childhood and adulthood.</title>
<date>1966</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="2907" citStr="Slobin, 1966" startWordPosition="431" endWordPosition="432">uation. One aspect of an essay’s quality that almost all AES systems do not yet fully capture is sentence processing complexity. The ability to clearly and concisely convey information without requiring undue effort on the part of the reader is one hallmark of good writing. Decades of behavioral research on language comprehension has suggested that some sentence structures are harder to comprehend than others. For example, passive sentences, such as the girl was pushed by the boy, are known to be harder to process than semantically equivalent active sentences, such as the boy pushed the girl (Slobin, 1966; Forster and Olbrei, 1972; Davison and Lutz, 1985; Kharkwal and Stromswold, 2013). Thus, it is likely that the overall processing complexity of the sentence structures used in an essay could influence its perceived quality. One reason why sentence processing complexity has not yet been fully utilized is the lack of a suitable way of quantifying it. This paper proposes the use of a psycholinguistic model of sentence comprehension called surprisal theory (Hale, 2001; Levy, 2008) to quantify sentence processing complexity. The rest of the paper is organized as follows. Section 2 describes the su</context>
</contexts>
<marker>Slobin, 1966</marker>
<rawString>Dan Slobin. 1966. Grammatical transformations and sentence comprehension in childhood and adulthood. Journal of Verbal Learning and Verbal Behavior, 5(3):219–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrian Staub</author>
</authors>
<title>Eye movements and processing difficulty in object relative clauses.</title>
<date>2010</date>
<journal>Cognition,</journal>
<volume>116</volume>
<issue>1</issue>
<contexts>
<context position="6021" citStr="Staub, 2010" startWordPosition="922" endWordPosition="923">ing two example sentences: (1) The judge who angered the criminal slammed the gavel. (2) The judge who the criminal angered slammed the gavel. Both sentences are center-embedded relative clause sentences that differ in whether the subject or the object is extracted from the relative clause. Critically, they both share the same words differing only in their relative order. Behavioral studies have found that object-extracted relative clause sentences (2) are harder to process than subjectextracted relative clause sentences (1) (King and Just, 1991; Gordon et al., 2001; Grodner and Gibson, 2005; Staub, 2010; Traxler et al., 2002; Stromswold et al., 1996). The surprisal values at each word position of the two example sentences are shown in Table 1. As we can see from Table 1, the mean surprisal value is greater for the object-extracted relative clause sentence. Hence, the surprisal theory correctly predicts greater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, such as greatest difficulty at the main verb are shared with</context>
</contexts>
<marker>Staub, 2010</marker>
<rawString>Adrian Staub. 2010. Eye movements and processing difficulty in object relative clauses. Cognition, 116(1):71–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Stromswold</author>
<author>David Caplan</author>
<author>Nathaniel Alpert</author>
<author>Scott Rauch</author>
</authors>
<title>Localization of syntactic comprehension by position emission tomography. Brain and Language,</title>
<date>1996</date>
<pages>52--452</pages>
<contexts>
<context position="6069" citStr="Stromswold et al., 1996" startWordPosition="928" endWordPosition="931">judge who angered the criminal slammed the gavel. (2) The judge who the criminal angered slammed the gavel. Both sentences are center-embedded relative clause sentences that differ in whether the subject or the object is extracted from the relative clause. Critically, they both share the same words differing only in their relative order. Behavioral studies have found that object-extracted relative clause sentences (2) are harder to process than subjectextracted relative clause sentences (1) (King and Just, 1991; Gordon et al., 2001; Grodner and Gibson, 2005; Staub, 2010; Traxler et al., 2002; Stromswold et al., 1996). The surprisal values at each word position of the two example sentences are shown in Table 1. As we can see from Table 1, the mean surprisal value is greater for the object-extracted relative clause sentence. Hence, the surprisal theory correctly predicts greater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, such as greatest difficulty at the main verb are shared with the subject-extracted relative clause, and are </context>
</contexts>
<marker>Stromswold, Caplan, Alpert, Rauch, 1996</marker>
<rawString>Karin Stromswold, David Caplan, Nathaniel Alpert, and Scott Rauch. 1996. Localization of syntactic comprehension by position emission tomography. Brain and Language, 52:452–473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Traxler</author>
<author>Robin K Morris</author>
<author>Rachel E Seely</author>
</authors>
<title>Processing subject and object relative clauses: Evidence from eye movements.</title>
<date>2002</date>
<journal>Journal of Memory and Language,</journal>
<pages>47--69</pages>
<contexts>
<context position="6043" citStr="Traxler et al., 2002" startWordPosition="924" endWordPosition="927">le sentences: (1) The judge who angered the criminal slammed the gavel. (2) The judge who the criminal angered slammed the gavel. Both sentences are center-embedded relative clause sentences that differ in whether the subject or the object is extracted from the relative clause. Critically, they both share the same words differing only in their relative order. Behavioral studies have found that object-extracted relative clause sentences (2) are harder to process than subjectextracted relative clause sentences (1) (King and Just, 1991; Gordon et al., 2001; Grodner and Gibson, 2005; Staub, 2010; Traxler et al., 2002; Stromswold et al., 1996). The surprisal values at each word position of the two example sentences are shown in Table 1. As we can see from Table 1, the mean surprisal value is greater for the object-extracted relative clause sentence. Hence, the surprisal theory correctly predicts greater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) and the end (angered). Other differences, such as greatest difficulty at the main verb are shared with the subject-extracted</context>
</contexts>
<marker>Traxler, Morris, Seely, 2002</marker>
<rawString>Matthew J. Traxler, Robin K. Morris, and Rachel E. Seely. 2002. Processing subject and object relative clauses: Evidence from eye movements. Journal of Memory and Language, 47:69–90.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
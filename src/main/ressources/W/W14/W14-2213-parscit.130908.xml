<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.498896">
Data Warehouse, Bronze, Gold, STEC, Software
</title>
<author confidence="0.921914">
Doug Cooper
</author>
<affiliation confidence="0.905848">
Center for Research in Computational Linguistics
</affiliation>
<email confidence="0.987561">
doug.cooper.thailand@gmail.com
</email>
<sectionHeader confidence="0.993622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999486411764706">
We are building an analytical data warehouse
for linguistic data – primarily lexicons and
phonological data – for languages in the
Asia-Pacific region. This paper briefly out-
lines the project, making the point that the
need for improved technology for endangered
and low-density language data extends well
beyond completion of fieldwork. We suggest
that shared task evaluation challenges
(STECs) are an appropriate model to follow
for creating this technology, and that stocking
data warehouses with clean bronze-standard
data and baseline tools – no mean task – is an
effective way to elicit the broad collaboration
from linguists and computer scientists needed
to create the gold-standard data that STECs
require.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992504054054">
The call for this workshop mentions the first step
of the language documentation process, pointing
out that the promise of new technology in docu-
menting endangered languages remains unful-
filled, particularly in the context of modern re-
cording technologies.
But lack of tools extends far beyond this first
step. It encompasses the accessibility of data
long since gathered and (usually, but not always)
published, as well as applications for the data by
its most voracious consumer: the study of com-
parative and historical linguistics.
We encounter these problems daily in prelimi-
nary development of data and software resources
for a planned Asia-Pacific Linguistic Data Ware-
house. Briefly, our initial focus is on five phyla
(~2,000 languages): Austroasiatic, Austronesian,
Hmong-Mien, Kra-Dai, and Sino-Tibetan, which
form a Southeast Asian convergence area, and
individually extend well into China, India, the
Himalayas, and the Pacific. Data for languages
of Australia and New Guinea will follow.
Not all of these languages are endangered, but
many are; not all are low-density, but most are.
Our data are preferentially drawn from the sort
of lexicography gathered for comparative pur-
poses (ideally 2,500 items per language), and the
phonological, semantic, and phylogenetic data
that can be found for, or inferred from, them.
These are the only kind of data for which we are
likely to find near-complete language representa-
tion. We include smaller lexicons when neces-
sary, and intra-language dialect surveys when
available. All available metadata are incorpo-
rated, including typological and phonotactic fea-
tures, (phylogenetic) character sets, geo-physical
and demographic data, details of lexicon cover-
age, extent, or quality, and bibliographic or
source data.
Such data are not always easily found. Their
delivery packages – primarily books and journals
– may be discoverable via bibliographic meta-
data, but details of the datasets themselves are
not. As a result, traditional bibliographic docu-
mentation, accessed via portals like OLAC
(Simons and Bird, 2000) and Glottolog (Nord-
hoff and Hammarström, 2011), tends to have low
recall and precision in regard to data resource
discovery.
Our experience in acquiring and performing
methodical data audits of large quantities of
published and unpublished materials reveals sets
of lexical, grammatical, phonological, corpus,
and other materials that are regular enough in
form, and extensive enough in content, to com-
prise aggregable linguistic data supersets for the
Asia-Pacific region.
These ongoing data audits take a three-tiered
approach, separately documenting texts (to en-
able source recovery), their abstract data content
(to enable high-recall resource discovery), and
any concrete, transcribed data instances (to en-
able high-precision data aggregation).
Discovery and aggregation only open the door.
Many datasets are hand-crafted for a researcher’s
specific needs and interests, even if they fall into
larger research categories. Yet far from having
reliable algorithms for central concerns (such as
proto-language reconstruction, or subgrouping of
linguistic phyla in family trees or networks) the
field has not yet had to grapple with basic prob-
lems – such as normalizing phonological tran-
scription or gloss semantics, or accurately as-
sembling large-scale cognate sets – that will be
</bodyText>
<page confidence="0.984739">
91
</page>
<note confidence="0.8485835">
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 91–99,
Baltimore, Maryland, USA, 26 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99519425">
presented by datasets that include millions of
data items for thousands of languages, and many
more thousands of dialectal variants.
The central issue we face is the gap between:
</bodyText>
<listItem confidence="0.99723125">
• the results of published and unpublished
fieldwork, and
• their usability in downstream research and
reference applications.
</listItem>
<bodyText confidence="0.999536090909091">
In some cases this gap is painfully obvious –
as in the backlog of carefully elicited wordlists
still awaiting phonetic transcription. In others,
the gap becomes evident when we begin to as-
semble large comparable datasets from published
data; deceptively difficult, and never accom-
plished for collections broader than a single lan-
guage family, or larger than about 200 words per
language. Such tasks are still basically hand
work; often requiring the specialized knowledge
of the field researcher.
</bodyText>
<subsectionHeader confidence="0.997201">
1.1 Data life cycle: anticipate or participate
</subsectionHeader>
<bodyText confidence="0.992871109090909">
We see the need for tools as part of a new sort of
data life cycle management that extends the con-
cerns of content, format, discovery, access, cita-
tion, preservation, and rights as usually articu-
lated, notably in Bird and Simons (2003).
Simply put, producing publishable or “cor-
rect” results is not sufficient to guarantee the
downstream usability of data. Rather, data must
undergo a series of transformations as it travels
from one research specialty to the next. We hope
there will be an increasing expectation that the
data producer either anticipate or participate in
this process.
At one end of the cycle, this often requires
small, specialized datasets of the sort needed to
support software development for tasks like
automated transcription or phonemic analysis –
still open problems in the context of under-
resourced languages.
At the other, building massive datasets that are
suitable for improving and extending quantitative
comparative linguistic applications – or discover-
ing the scales at which different methods might
be most useful – has not been a priority for the
linguistics community: if a few representative
items demonstrate a relationship or support a
reconstruction convincingly, then exhaustive
coverage does not make the argument stronger.
We face a classic resource deadlock. High-
quality “last-user” datasets are not constructed
because traditional methods are too expensive
and time-consuming. However, tools for refin-
ing “first-producer” data on an industrial scale
are not built because the high-quality datasets
needed to validate them do not exist. Develop-
ment of computational methods for problems like
subgrouping tends to focus on a small number of
available datasets, while their results are criti-
cized for precisely this.
2 STECs and gold-standard data
Log jams in natural language processing are
nothing new. A shared task evaluation chal-
lenge (STEC) presents an open challenge to the
field in the context of evaluating performance on
a specific task. Originally developed in the con-
text of the TIPSTER Text Program (which initi-
ated the long-running MUC and TREC confer-
ence series) as discussed in Belz and Kilgarriff
(2006), see also Hirschmann (1998) “Over the
past twenty years, virtually every field of re-
search in human language technology (HLT) has
introduced STECS.”
The STEC is the culmination of a series of ef-
forts intended to focus and advance progress by
asking such questions as:
</bodyText>
<listItem confidence="0.924127">
• what problems need to be solved in order
to advance the field? Where are we trying
to go, and what is standing in our way?
• what kinds of necessary data are not gener-
ally available? What kinds of datasets are
too difficult for individual researchers to
create?
• what kind of functional decomposition into
simpler goals will help demonstrates and
measure progress in quantitative and quali-
tative terms?
</listItem>
<bodyText confidence="0.999909684210526">
Both data, and evaluation metrics, are made
available well before the STEC, which is often
held in conjunction with a major conference.
The task is typically initiated by the release of a
dataset; results are submitted by some deadline,
and the results of evaluation are announced be-
fore or at the conference.
The terms gold-standard and more recently,
silver-standard (for machine-generated sets) are
used to describe datasets created for use in
STECs and NLP applications. These can be
thought of as being “correct answers” for quanti-
tative evaluation (Kilgarriff 1998).
Gold-standard datasets are built to enable
comparable evaluation of alternative algorithms
or implementations. Frequently, part of the set
will be publicly released in advance to serve as
training data, while part of it is held back to pro-
vide test data (and is released at a later date).
</bodyText>
<page confidence="0.992426">
92
</page>
<bodyText confidence="0.999927076923077">
Gold-standard datasets reflect the state of the
art in an area, such as the specification of word
senses, delineation of word boundaries, or
evaluation of message sentiment, for which there
may not be any purely objective ground truth.
We can reasonably expect to allow alternative
formulations of gold-standard sets in areas in
which the state of the art may be uncertain, even
in the eyes of experts. And we can anticipate
increased critical scrutiny of previously accepted
judgments as more base data and better investi-
gative tools become available; see e.g. Round
(2013, 2014).
</bodyText>
<subsectionHeader confidence="0.801936">
2.1 STECs for low-density languages
</subsectionHeader>
<bodyText confidence="0.9992814">
In our opinion, all of the reasons for which
STECs are devised and gold-standard datasets
defined apply equally to the low-density lan-
guage problems we touched on in Section 1.
These include:
</bodyText>
<listItem confidence="0.9998671875">
• normalization and syllabification of tran-
scribed data,
• phonetic transcription of audio and ortho-
graphic data,
• morphemic analysis of transcribed data,
• extraction of a phonemic analysis from
phonetic data,
• identification of internal cognates and/or
derivationally related forms, as well as
loan-word identification and stratification,
• automated reglossing / translation (to a
standardized gloss set) of glosses and/or
definitions.
• automated inference of phylogenetic sub-
grouping.
• automated generation of proto-forms,
</listItem>
<bodyText confidence="0.99995145">
All are characterized by the same requirement
for human judgment in processing, and lack of
absolute certainty as to outcomes.
The critical difference is that (as far as we
know) STECs in NLP invariably focus on high-
density languages for which both data and exper-
tise are readily available. In contrast, low-
density languages – which presumably includes
the entire range of endangered languages – are
by their nature specialty realms, for which exper-
tise, even within a single phylum, is often widely
dispersed.
Thus, the problem we face in creating success-
ful STECs for documentary linguistics is not
simply a matter of thinking up tasks, and relying
on in-house expertise to develop gold-standard
datasets. Rather, advancing development of
computational tools requires participation from a
large community of independently working lin-
guists as well.
</bodyText>
<sectionHeader confidence="0.640932" genericHeader="method">
3 Cast bronze to net gold
</sectionHeader>
<bodyText confidence="0.9955325">
Our approach to achieving this begins by laying
the groundwork for collaboration between:
</bodyText>
<listItem confidence="0.833816">
• computer scientists who recognize the need
</listItem>
<bodyText confidence="0.870072818181818">
for better data, and will join the challenge
of solving practical problems in building
massive, comparable datasets, and
• linguists willing to help create and validate
the gold-standard reference sets and train-
ing data needed to establish quality metrics
for improving software tools.
We think this collaboration is best motivated
in the old-fashioned way: reduce participants’
vapor no data could be located (useful when documenting data availability by ISO code)
water untranscribed audio recording only
paper print/image/PDF data are in hand, but not transcribed or extracted
tin raw e-orthography and definitions (as in typical documentary dictionaries)
copper raw e-forms and glosses (as in purpose-collected comparative lexicons; e.g. Holle lists)
bronze clean electronic data and metadata, ready for hand or machine processing,
naive normalization of forms and glosses, cognate sets partially specified,
capable of demonstrating preliminary data warehouse functionality
(Software: baseline vanilla algorithms)
silver machine-normalized or grouped data, not yet verified by humans
(Software: better than baseline)
gold human-verified/accepted, machine-usable comparable datasets
(Software: (best) able to produce gold-standard results)
</bodyText>
<tableCaption confidence="0.9462065">
Table 1. Data quality standards re lexicons, cognate sets, reconstructions, and subgrouping, with par-
allels to software tools. Silver- and gold-standard are the only terms commonly used in this context.
</tableCaption>
<page confidence="0.997411">
93
</page>
<bodyText confidence="0.967215213333333">
startup costs, flatten their learning curves, high-
light expected outcomes that will advance col-
laborators’ self-interests, and help provide the
data, tools, and/or metrics that collaborators will
need to seek funding themselves.
This in itself as a long-term effort – easily 5–8
years for our region, with optimal funding –
whose thrust can be summarized as cast bronze
to net gold (see Table 1).
Locating data, and bringing it to the minimal
state required for computer applications requires
a massive amount of work. Consider just the
discovery aspect, for which the data audit men-
tioned earlier entails an ongoing, two-pronged
effort.
On one hand, we identify potential data con-
tent by acquiring as much published and unpub-
lished print material as possible, including com-
plete journal runs, monograph series, informally
published “gray literature,” extensive sets of un-
published field notes, and regular publication
backlists (notably, a half-century of works from
Pacific Linguistics, which will be added to our
on-line repository later this year).1
On the other, we systematically work through
the complete ISO 639-3 inventory (as a proxy for
the on-the ground truth, and as a means of help-
ing to perfect the standard, as well as identifying
documentary shortfalls that might be short-listed
for fieldwork) of our region, attempting to find at
least lexical content for every language.
Overall, our summary project development
plan has four steps, which relate to content and
scale, and determined our choice of a regional
focus – for which we could take responsibility –
rather than either working at greater depth on a
single phylum, or attempting to build a global
framework, and then relying primarily on outside
contributors.
First, define an area that is broad enough to be
of wide linguistic interest, and able to supply a
range of control and alternative test conditions
for both traditional and computational methods.
Even allowing for typological variation that may
be found in individual phyla, we think this usu-
ally requires a regional perspective.
1 For New Guinea, this required a special sub-project
dubbed INGA, dedicated to tracking down “invisible”
New Guinea archives held in libraries and file cabi-
nets around the world! As implied, when possible we
negotiate rights to scan and make all materials freely
available in an on-line repository, and will begin to
register DOI names (when appropriate) for texts and
data this year.
Second, locate and prepare raw data of suffi-
cient breadth and depth. We think that aiming
for blanket rather than selective coverage is ap-
propriate – it enables the broadest range of re-
search agendas by reflecting the natural state of
human migration and constant language contact.
Third, establish research goals that capture the
interest of both fields – documentary / compara-
tive / historical linguistics and computer science.
This extends the argument for complete regional
coverage, especially in convergence areas. But it
also argues for limiting scope to an area in which
it is realistically possible to actively recruit in-
volvement, conference by conference.
Finally, we need to lower barriers to participa-
tion We think we can do this by providing a
framework that allows data owners to take ad-
vantage of existing software tools, and which
provides software developers with easily custom-
ized data test beds – the analytical data ware-
house.
</bodyText>
<sectionHeader confidence="0.95045" genericHeader="method">
4 The data warehouse
</sectionHeader>
<bodyText confidence="0.999990516129032">
A data warehouse is an integrated collection of
databases that incorporates tools for sampling,
analyzing, and visualizing query results. Unlike
repository databases intended for storage and
retrieval of prepared values (perhaps for off-line
processing), data warehouses assume that data
filtering, transformation, and analysis are essen-
tial to satisfying every query. In the context of
comparative lexicons, such tasks are well beyond
the scope of existing virtual research environ-
ments such as WebLicht (Hinrichs et al 2010)
and TextGrid (Neuroth et al 2011), which focus
primarily on text corpora.
Because sampling filters allow selection of
homogeneous or representative subsamples, we
can be as inclusive as possible in regard to data
acquisition. We are not talking about data qual-
ity; rather (working within our overall criterion
of comparative lexical data) we want to avoid
excluding sets because of concerns about dataset
size or content disparity, or over-representation
of dialect survey data.
Many operations we wish to perform on or
with data involve open research questions. Al-
though users may perceive the warehouse as
providing access to tools, we intend to present it
to tool developers as a tunable test bed of data
that does not require them to deal with data man-
agement, as well as a means of using, and en-
couraging development of, open-source toolkits
such as the pioneering work of Kleiweg (2009)
</bodyText>
<page confidence="0.997104">
94
</page>
<bodyText confidence="0.999885">
and List and Moran (2013). We return to the
idea of plug-and-play operations on lexicons in
Section 6.
The warehouse also helps provide added value
to potential data contributors. Even if software is
freely available, preparing data or setting up
tools can impose substantial, even insurmount-
able, burdens on data creators, particularly in
regions in which cooperation between linguists
and computer scientists is less common than in
the US or Europe.
</bodyText>
<subsectionHeader confidence="0.996057">
4.1 Data warehouse query-flow
</subsectionHeader>
<bodyText confidence="0.992631">
In our test warehouse implementation, function-
ality is divided as follows:
</bodyText>
<listItem confidence="0.898451529411765">
• filter: define a search universe based on phy-
logenetic or phonotactic properties, geo-
physical or proximal location, lexicon char-
acteristics, or other data or metadata features.
• frame: specify data and/or metadata to be
returned, e.g. specific aspects of the form
and/or gloss, or metadata details that might
be useful for correlation testing.
• analyze: extract phone inventories, calculate
functional load, investigate lexical neighbor-
hoods, cluster data by phonological similar-
ity, etc.
• visualize: provide alternatives to tables as
appropriate, e.g. tree/graph/map layouts.
• recycle: search within returned data, use
faceting to extend searches, or let the visu-
alization serve as a chooser for a new search.
</listItem>
<bodyText confidence="0.99943012">
For brevity we discuss just one feature: filter-
ing. This lets the search universe be defined in
as much detail as possible, and is partly common
sense: our overall data universe is decidedly
lumpy due to the decision to include small sam-
ples (some &lt;100 items) when necessary, and dia-
lect surveys (perhaps with only minor differences
between doculects) when possible.
It is also intended to take advantage of the
large quantities of available metadata, whether it
is explicit / external – that is, related to the lan-
guage or doculect, or implicit / internal, i.e. can
be derived from individual datasets or samples.
Such metadata includes proposed phylogenetic
relations, typological features, geophysical and
demographic data, characteristics of lexicon
composition, extent, or quality, bibliographic or
source data, and phonological properties of the
doculect itself. Some of this metadata may be
returned with individual items as part of the data
frame.
Filter targets may be specified if appropriate.
For example, a filter might limit a search to lan-
guages that contain sesquisyllables, or instead
require that returned items be sesquisyllabic.
</bodyText>
<subsectionHeader confidence="0.81786">
4.2 An example query and result
</subsectionHeader>
<figureCaption confidence="0.888162">
Figure 1 shows the result of a relatively simple
warehouse query (using our unreleased explora-
tory implementation): a geo-constrained phy-
logenetic tree for Trans-New Guinea languages.
Tree topology follows Ethnologue 16 (Lewis,
2009) as provided by the MultiTree project
(Aristar and Ratliff, 2005); other analyses are
Figure 1 A geo-constrained phylogenetic tree (analysis by Ethnologue via MultiTree). This cluster
tree keeps low-level group nodes near their daughters, but raises the root nodes. Dialects are green,
languages yellow, and groups blue
</figureCaption>
<page confidence="0.988084">
95
</page>
<figureCaption confidence="0.992602">
Figure 2 A search for “bone” in ABVD Austronesian data (again, relations by Ethnologue via Mul-
tiTree), constrained to locations in Indonesia, and projected onto a map
</figureCaption>
<bodyText confidence="0.999781625">
readily specified. In this example dialects (from
the same sources) are arranged in a circular pat-
tern around the ISO 639-3 hub language (and,
again, other analyses could be used instead). The
same filtering and visualization routines are used
in a different manner in Figure 2, which shows
words for “bone” in Austronesian languages as
provided by ABVD (Greenhill et al, 2008).
</bodyText>
<sectionHeader confidence="0.791073" genericHeader="method">
5 Data comparability and reusability
</sectionHeader>
<bodyText confidence="0.999037">
We will finish the discussion of data warehouses
with a quick look at data comparability and re-
use. Comparability or equivalence of datasets
can be looked at in two ways
</bodyText>
<listItem confidence="0.978971166666667">
• at the content level, e.g. to ensure that the
same systems of transcription and glossing
are used for all datasets, and
• at the structural level, in identifying datasets
of comparable complexity, structure, or
available detail.
</listItem>
<bodyText confidence="0.9999418">
At the content level, normalization of forms
and glosses is the critical transformation in the
journey to gold-standard quality. We will briefly
describe our systems for normalization, Meta-
gloss and Metaphon, because they are ripe for
computational assistance. The discussion ends
with a quick introduction to Etyset, the frame-
work we intend to use to describe and distribute
structured datasets, such as those that incorporate
subgroup and cognate detail.
</bodyText>
<subsectionHeader confidence="0.924546">
5.1 Gloss, Metagloss, Etygloss
</subsectionHeader>
<bodyText confidence="0.999985382352941">
In most of our applications, a gloss is semantic
annotation provided by the wordlist author in
order to index phonological forms. Unfortu-
nately, these may be elicitation terms rather than
glosses (green? “grue.” blue? “grue”), or local
vernacular rather than common or scientific
terms for flora and fauna. Phrasing varies wildly,
and proper reading may depend on having the list
context available (short/tall, short/long). Trans-
lation may be lossy (strew or scatter as nouns)
due to differences in grammaticalization or lexi-
fication. All of these undermine comparability.
We have begun to define an intermediate,
standardized metagloss layer to express the au-
thor’s intent (if discernable). A third layer, the
etygloss, will help account for semantic shift in
labeling cognate groups; i.e. glossing empty
placeholders for proto-language reconstructions.
In the simple case all three layers are identical.
Metagloss provides a controlled vocabulary
for re-annotating or translating existing lexicon
glosses; it foregrounds the critical design link
between glossing and searching. We map this to
WordNet senses, creating a low-overhead tool for
word-sense disambiguation and facet generation.
The Metagloss controlled vocabulary can be
extended; it uses attributes to specify predictable
relationships (sheep:male:castrated for wether)
and solve lexicalization problems that arise in
gloss translation (e.g. n@strew is the noun form
of strew). Additionally, it allows definition of
lightweight ontologies; relations between Meta-
glosses that clarify semantic relations and im-
prove search fallback performance.
</bodyText>
<subsectionHeader confidence="0.995455">
5.2 Phon, Metaphon, Etyphon
</subsectionHeader>
<bodyText confidence="0.999900285714286">
Phonological forms present similarly difficult
search problems; these go beyond easily fixed
notational convention. For example, absence of
marked syllable boundaries can make phonologi-
cal searches difficult when we are interested in
the phoneme’s role (such as pre-nasalization)
rather than its sign (/n/, /m/ etc.).
</bodyText>
<page confidence="0.979322">
96
</page>
<bodyText confidence="0.999980592592593">
The same holds true for other context-
sensitive symbols (e.g. “h” as /h/, /h/, or as a pre-
pended indicator of unvoiced phonemes). A
greater problem arises from parsimonious nota-
tions that rely on commentary to clarify unwrit-
ten content, e.g. predictable vowel insertion –
these must be made explicit.
We define an intermediate layer of standard-
ized notation called metaphon: a conventional
notation that allows consistent search, while
clearly documenting (and minimizing, in com-
parison to wild-card searches) the scope of any
unavoidable approximation. A third layer, the
etyphon, allows temporary specification of a
(possibly sub-lexical) phonemic rendition prior
to any formal reconstruction.
Metaphon, like metagloss, is intimately tied to
search functionality. Normalized transcription
enables consistent extraction of phonological and
phonotactic data. It lets the search universe be
restricted to languages (or items) that have par-
ticular phonemes or features. This dynamic,
data-driven process lets us weigh relative signifi-
cance – frequency, salience, functional load – of
features in sets that are themselves results drawn
from a restricted search universe; e.g. to consider
the functional load of tones in sesquisyllables.
</bodyText>
<subsectionHeader confidence="0.998253">
5.3 Structural comparability: EtySet
</subsectionHeader>
<bodyText confidence="0.99990203030303">
The discussion thus far has focused on the form
and quality of data items. We are equally con-
cerned with what might be called structural com-
parability of data sets, because this determine the
approach we take to systematic description, dis-
semination, and re-use of cognate sets, phyloge-
netic trees, or sets of proto-form reconstructions.
This has nothing to do with tagging or inter-
change standards, which can be handled with
borrowed schemes designed for similar purposes,
e.g. Newick notation (Felsenstein, 1986) or suc-
cessors (Nakhleh, 2003). Rather, we require
nomenclature that might be used to describe their
contents, or to enable identification of sets of
comparable complexity, structure, or detail.
We think such comparison is crucial to help
research in quantitative historical linguistics
move beyond its current state, which many lin-
guists view as interesting but nevertheless ad hoc
experimentation. In other words, we would like
to see computational approaches to cognate iden-
tification, subgrouping, and proto-language re-
construction be developed and tested in envi-
ronments for which the controlled variable is
linguistic typology, with as many other factors as
possible held equal.
Similarly, we would like to be able to vary
starting conditions. For example Bouchard-Côté
et al (2013) report on a computational approach
to reconstruction given (assumed) prior knowl-
edge of subgrouping in Austronesian. However,
any one or two variables from amongst cognate
grouping, reconstruction, and phylogenetic sub-
grouping may be used to test approaches to infer-
ring or generating the third.
We refer to cognate sets, phylogenetic trees,
and reconstructed proto-forms as etysets. The
key terms of our working descriptive nomencla-
ture are outlined in Table 2.
Etysets may be bare (links only), or supported
by reconstructed forms or semantics; note that
the phylogenetic analyses provided by Eth-
nologue, Glottolog, or MultiTree may be repre-
sent with bare etysets. An internal cognate etyset
has depth (number of internal sets) and size
(number of forms in each set). A regular cognate
etyset has depth (the number of sets / implicit
number of root proto-forms) and breadth (the
number of lects represented in each cognate set).
For example a bare cognate etyset of Bah-
naric, breadth Eth:80% / depth MSEA:90%
depth includes data from 32 (of 40. according to
the Ethnologue analysis) Bahnaric languages,
and at least 450 of the 500-odd terms in the
MSEA (SIL 2002) elicitation list. Cognate
groupings are provided, but not reconstructions
or etyglosses.
breadth number of nodes or leaves at any level of a phylogenetic tree.
depth number of branch levels supplied.
degree branchy-ness – the number of branches / degree of diversity at a given node.
density a joint measure of breadth, depth, and degree.
size # of cited or reconstructed forms associated with a leaf or branch node.
coverage describes the extent of an etyset in terms of a fixed reference inventory.
phylogenetic etyset described in term of breadth, depth, degree, and size.
documented node includes metadata for approximate time depth and geographic location.
cognate etyset may be internal or regular, and contains internal or regular cognate sets.
</bodyText>
<tableCaption confidence="0.988502">
Table 2. Outline of the EtySet descriptive vocabulary.
</tableCaption>
<page confidence="0.999488">
97
</page>
<sectionHeader confidence="0.98107" genericHeader="method">
6 Operations on lexicons
</sectionHeader>
<bodyText confidence="0.9996105">
We end with a brief note about computational
tasks for and by a data warehouse that is:
</bodyText>
<listItem confidence="0.95621825">
• stocked primarily with lexical, phonological,
and phylogenetic data and relevant metadata,
• intended to support research in comparative
and historical linguistics.
</listItem>
<bodyText confidence="0.861301571428571">
These fall under the general heading of opera-
tions on lexicons. We do not draw a strict divid-
ing line between software employed to prepare
data for use in a warehouse, and software used by
the warehouse. We do exclude operations whose
implementation is likely to be closely tied to a
particular database implementation.
All would benefit from being implemented as
plug-and-play functions, requiring some, but not
excessive, programmer effort. This:
• allows head-to-head comparison of alterna-
tive algorithms, implementations, or inter-
pretations of how measurements or actions
should be carried out,
</bodyText>
<listItem confidence="0.992223">
• allows encapsulation and offloading of com-
putationally expensive algorithms; this is an
important issue for some quantitative or sta-
tistical comparative methods, and
• encourages re-use of code in building new,
alternative platforms for linguistic research.
</listItem>
<bodyText confidence="0.980052722222222">
We assume that all of these can be specified in
terms of functionality, required data inputs, and
expected data outputs, sticking to a Unix-like
model in which data can be minimally formatted
plain-text streams which, with the assistance of
tabs, parentheses, and newlines, can be inter-
preted as bags, lists, vectors, matrices, trees, and
the like. Higher-level streams (JSON, XML,
RDF, HTML) are also reasonable outputs.
For brevity’s sake, we limit examples to op-
erations on phonological forms. We could easily
list similar sets of operations – some straightfor-
ward, some not – on morphology, semantics,
alternatives for visualization, cognate identifica-
tion, phylogenetic subgrouping, proto-form gen-
eration, and the like.
Operations on phonological strings / lists
Conversion and markup of transcription
</bodyText>
<listItem confidence="0.941107146341463">
• between standardized and/or special-purpose
notations,
• to novel notations, e.g. gestural scores,
• unambiguous conversion of notation from
historical (e.g. Americanist) to IPA,
• potentially ambiguous normalization (e.g.
interpretation of /h/),
• phonetic to phonemic conversion,
• marking of syllable boundaries,
• marking of syllable-internal features (e.g.
onset, nucleus, coda),
• marking of morpheme boundaries.
Extraction / recognition of phonological features
• sonority sequence tagging.
• extraction/recognition of phones, phonation,
co-articulatory, suprasegmental features,
• count/extraction of phone/feature n-grams,
• extraction or identification of arbitrary collo-
cational features (e.g. sesquisyllable+tone),
Calculation of distance/similarity measures be-
tween strings, lists, and vectors
• weighted and unweighted edit distances,
• substring matching measures,
• vector cosine distance,
• phonologically based distance/similarity,
• language-internal distance/similarity,
• information content distance/similarity.
Clustering
• subgrouping list contents,
• “sounds like...” search (for very large sets).
Neighborhood measures
• generation of phonological neighborhoods,
• identification of neighbors,
• calculation of neighborhood size, density,
clustering coefficients.
Load measures
• calculation of functional load of phonemes,
features, collocations,
• calculation of salience of phonemes, fea-
tures, collocations,
• use in pseudo-word generation.
</listItem>
<sectionHeader confidence="0.946255" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999878571428572">
The call for this workshop foregrounds develop-
ment of software to aid in initial documentation
of endangered languages, seeks models for col-
lection and management of endangered-language
data, and means of encouraging productive inter-
action between documentary linguists and com-
puter scientists.
</bodyText>
<page confidence="0.989696">
98
</page>
<bodyText confidence="0.999937">
We suggest that these same needs exist all
down the line, encompassing low-resource lan-
guages in general, documentation long-since
completed, and analytical applications far re-
moved from fieldwork settings. We propose that
addressing them in downstream environments,
such as data warehouses and STECs, may be an
effective way to meet our common “preeminent
grand challenge:” integration of linguistic theo-
ries and analyses, relying on massive scaling up
of datasets and new computational methods, as
articulated by Bender and Good (2010).
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999422936170213">
Anthony Aristar and Martha Ratliff. 2005. MultiTree:
A digital library of language relationships. Insti-
tute for Language Information and Technology:
Ypsilanti, MI. http://multitree.org.
Anja Belz and Adam Kilgarriff. 2006. Shared-task
evaluations in HLT: Lessons for NLG. In Proceed-
ings of INLG-2006.
Emily Bender and Jeff Good. 2010. A Grand Chal-
lenge for Linguistics: Scaling Up and Integrating
Models. White paper contributed to NSF SBE 2020
initiative. http://www.nsf.gov/sbe/sbe_2020/-
2020_pdfs/Bender_Emily_81.pdf
Steven Bird and Gary Simons. 2003. Seven Dimen-
sions of Portability for Language Documentation
and Description. Language 79:2003, 557-5822.
Alexandre Bouchard-Côté, David Hall, Thomas L.
Griffiths and Dan Klein. 2013. Automated recon-
struction of ancient languages using probabilistic
models of sound change. Proceedings of the Na-
tional Academy of Sciences. http://-
www.pnas.org/content/110/11/4224
Joseph Felsenstein. 1986. The newick tree format.
http://evolution.genetics.washington.edu/phylip/ne
wicktree.html
Simon Greenhill, Robert Blust, and Russell D.. Gray.
2008. The Austronesian Basic Vocabulary Data-
base: From Bioinformatics to Lexomics. Evolu-
tionary Bioinformatics, 4:271-283. http://-
language.psy.auckland.ac.nz/austronesian
Erhard W. Hinrichs, Marie Hinrichs and Thomas Zas-
trow. 2010. WebLicht: Web-Based LRT Services
for German. In: Proceedings of the ACL 2010 Sys-
tem Demonstrations. pages 25–29.
Lynette Hirschman. 1998. The evolution of evalua-
tion: Lessons from the Message Understanding
Conferences. Computer Speech and Language,
12:283–285.
Adam Kilgarriff. 1998. Gold Standard Datasets for
Evaluating Word Sense Disambiguation Programs.
Computer Speech and Language, 12 (3) Special Is-
sue on Evaluation of Speech and Language Tech-
nology, edited by Robert Gaizauskas. 453-472.
http://www.kilgarriff.co.uk/Publications/1998-K-
CompSL.pdf For TREC see http://trec.nist.gov.
The TIPSTER site has been preserved here:
http://www.nist.gov/itl/div894/894.02/related_proj
ects/tipster/
Peter Kleiweg. 2006. RuG/L04 Software for dialecto-
metrics and cartography. Rijksuniversiteit Gronin-
gen. Faculteit der Letteren. http://www.let.rug.nl-
/kleiweg/L04/
M. Paul Lewis. 2009. Ethnologue: Languages of the
World, Sixteenth Edition. SIL International, Dal-
las, Texas.
Johann-Mattis List and Steven Moran. 2013. An
Open-Source Toolkit for Quantitative Historical
Linguistics. Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, 13–18, Sofia, Bulgaria. http://-
www.zora.uzh.ch/84667/1/P13-4003.pdf
Luay Nakhleh, Daniel Miranker, and Francois Bar-
bancon. 2003. Requirements of Phylogenetic Data-
bases. In Third IEEE Symposium on BioInformat-
ics and BioEngineering (BIBE’03). 141-148. IEEE
Press. http://www.cs.rice.edu/~nakhleh/Papers/-
bibe03_final.pdf
Heike Neuroth, Felix Lohmeier, Kathleen Marie
Smith: TextGrid. Virtual Research Environment for
the Humanities. In: The International Journal of
Digital Curation. 6, Nr. 2, 2011, S. 222–231.
Sebastian Nordhoff and Harald Hammarström. 2011.
Glottolog/Langdoc: Defining dialects, languages,
and language families as collections of resources.
783 CEUR Workshop, Proceedings of the First In-
ternational Workshop on Linked Science 2011
http://iswc2011.semanticweb.org/fileadmin/iswc/-
Papers/Workshops/LISC/nordhoff.pdf
Erich R. Round. 2013. ‘Big data’ typology and lin-
guistic phylogenetics: Design principles for valid
datasets. Presented 25 May 2013 at 21st Manches-
ter Phonology Meeting. Accessible via
https://uq.academia.edu/ErichRound.
Erich R. Round. 2014. The performance of STRUC-
TURE on linguistic datasets &amp; ‘researcher degrees
of freedom’. Presented 15 Jan 2014 at TaSil, Aar-
hus, Denmark. Accessible via
https://uq.academia.edu/ErichRound
SIL Mainland Southeast Asia Group. 2002. Southeast
Asia 436 Word List revised November 2002.
http://msea-ling.info/digidata/495/b11824.pdf
Gary Simons and Steven Bird. 2000. The seven pillars
of open language archiving:A vision statement.
http://www.language-archives.org/docs/vision.-
html.
</reference>
<page confidence="0.998959">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.889402">
<title confidence="0.988533">Data Warehouse, Bronze, Gold, STEC, Software</title>
<author confidence="0.997759">Doug Cooper</author>
<affiliation confidence="0.99449">Center for Research in Computational Linguistics</affiliation>
<email confidence="0.999057">doug.cooper.thailand@gmail.com</email>
<abstract confidence="0.994671222222222">are building an analytical warehouse for linguistic data – primarily lexicons and phonological data – for languages in the Asia-Pacific region. This paper briefly outlines the project, making the point that the need for improved technology for endangered and low-density language data extends well beyond completion of fieldwork. We suggest task evaluation challenges (STECs) are an appropriate model to follow for creating this technology, and that stocking warehouses with clean data and baseline tools – no mean task – is an effective way to elicit the broad collaboration from linguists and computer scientists needed create the that STECs require.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aristar</author>
<author>Martha Ratliff</author>
</authors>
<title>MultiTree: A digital library of language relationships.</title>
<date>2005</date>
<booktitle>Institute for Language Information and Technology:</booktitle>
<location>Ypsilanti, MI. http://multitree.org.</location>
<contexts>
<context position="20391" citStr="Aristar and Ratliff, 2005" startWordPosition="3141" endWordPosition="3144">doculect itself. Some of this metadata may be returned with individual items as part of the data frame. Filter targets may be specified if appropriate. For example, a filter might limit a search to languages that contain sesquisyllables, or instead require that returned items be sesquisyllabic. 4.2 An example query and result Figure 1 shows the result of a relatively simple warehouse query (using our unreleased exploratory implementation): a geo-constrained phylogenetic tree for Trans-New Guinea languages. Tree topology follows Ethnologue 16 (Lewis, 2009) as provided by the MultiTree project (Aristar and Ratliff, 2005); other analyses are Figure 1 A geo-constrained phylogenetic tree (analysis by Ethnologue via MultiTree). This cluster tree keeps low-level group nodes near their daughters, but raises the root nodes. Dialects are green, languages yellow, and groups blue 95 Figure 2 A search for “bone” in ABVD Austronesian data (again, relations by Ethnologue via MultiTree), constrained to locations in Indonesia, and projected onto a map readily specified. In this example dialects (from the same sources) are arranged in a circular pattern around the ISO 639-3 hub language (and, again, other analyses could be u</context>
</contexts>
<marker>Aristar, Ratliff, 2005</marker>
<rawString>Anthony Aristar and Martha Ratliff. 2005. MultiTree: A digital library of language relationships. Institute for Language Information and Technology: Ypsilanti, MI. http://multitree.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Shared-task evaluations in HLT: Lessons for NLG.</title>
<date>2006</date>
<booktitle>In Proceedings of INLG-2006.</booktitle>
<contexts>
<context position="7453" citStr="Belz and Kilgarriff (2006)" startWordPosition="1128" endWordPosition="1131">tasets needed to validate them do not exist. Development of computational methods for problems like subgrouping tends to focus on a small number of available datasets, while their results are criticized for precisely this. 2 STECs and gold-standard data Log jams in natural language processing are nothing new. A shared task evaluation challenge (STEC) presents an open challenge to the field in the context of evaluating performance on a specific task. Originally developed in the context of the TIPSTER Text Program (which initiated the long-running MUC and TREC conference series) as discussed in Belz and Kilgarriff (2006), see also Hirschmann (1998) “Over the past twenty years, virtually every field of research in human language technology (HLT) has introduced STECS.” The STEC is the culmination of a series of efforts intended to focus and advance progress by asking such questions as: • what problems need to be solved in order to advance the field? Where are we trying to go, and what is standing in our way? • what kinds of necessary data are not generally available? What kinds of datasets are too difficult for individual researchers to create? • what kind of functional decomposition into simpler goals will hel</context>
</contexts>
<marker>Belz, Kilgarriff, 2006</marker>
<rawString>Anja Belz and Adam Kilgarriff. 2006. Shared-task evaluations in HLT: Lessons for NLG. In Proceedings of INLG-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Bender</author>
<author>Jeff Good</author>
</authors>
<title>A Grand Challenge for Linguistics: Scaling Up and Integrating Models. White paper contributed to NSF SBE</title>
<date>2010</date>
<note>initiative. http://www.nsf.gov/sbe/sbe_2020/-2020_pdfs/Bender_Emily_81.pdf</note>
<marker>Bender, Good, 2010</marker>
<rawString>Emily Bender and Jeff Good. 2010. A Grand Challenge for Linguistics: Scaling Up and Integrating Models. White paper contributed to NSF SBE 2020 initiative. http://www.nsf.gov/sbe/sbe_2020/-2020_pdfs/Bender_Emily_81.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Gary Simons</author>
</authors>
<title>Seven Dimensions of Portability for Language Documentation and Description.</title>
<date>2003</date>
<journal>Language</journal>
<volume>79</volume>
<pages>557--5822</pages>
<contexts>
<context position="5526" citStr="Bird and Simons (2003)" startWordPosition="830" endWordPosition="833">mes evident when we begin to assemble large comparable datasets from published data; deceptively difficult, and never accomplished for collections broader than a single language family, or larger than about 200 words per language. Such tasks are still basically hand work; often requiring the specialized knowledge of the field researcher. 1.1 Data life cycle: anticipate or participate We see the need for tools as part of a new sort of data life cycle management that extends the concerns of content, format, discovery, access, citation, preservation, and rights as usually articulated, notably in Bird and Simons (2003). Simply put, producing publishable or “correct” results is not sufficient to guarantee the downstream usability of data. Rather, data must undergo a series of transformations as it travels from one research specialty to the next. We hope there will be an increasing expectation that the data producer either anticipate or participate in this process. At one end of the cycle, this often requires small, specialized datasets of the sort needed to support software development for tasks like automated transcription or phonemic analysis – still open problems in the context of underresourced languages</context>
</contexts>
<marker>Bird, Simons, 2003</marker>
<rawString>Steven Bird and Gary Simons. 2003. Seven Dimensions of Portability for Language Documentation and Description. Language 79:2003, 557-5822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Côté</author>
<author>David Hall</author>
<author>Thomas L Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>Automated reconstruction of ancient languages using probabilistic models of sound change.</title>
<date>2013</date>
<booktitle>Proceedings of the National Academy of Sciences. http://-www.pnas.org/content/110/11/4224</booktitle>
<contexts>
<context position="26650" citStr="Bouchard-Côté et al (2013)" startWordPosition="4078" endWordPosition="4081">omplexity, structure, or detail. We think such comparison is crucial to help research in quantitative historical linguistics move beyond its current state, which many linguists view as interesting but nevertheless ad hoc experimentation. In other words, we would like to see computational approaches to cognate identification, subgrouping, and proto-language reconstruction be developed and tested in environments for which the controlled variable is linguistic typology, with as many other factors as possible held equal. Similarly, we would like to be able to vary starting conditions. For example Bouchard-Côté et al (2013) report on a computational approach to reconstruction given (assumed) prior knowledge of subgrouping in Austronesian. However, any one or two variables from amongst cognate grouping, reconstruction, and phylogenetic subgrouping may be used to test approaches to inferring or generating the third. We refer to cognate sets, phylogenetic trees, and reconstructed proto-forms as etysets. The key terms of our working descriptive nomenclature are outlined in Table 2. Etysets may be bare (links only), or supported by reconstructed forms or semantics; note that the phylogenetic analyses provided by Ethn</context>
</contexts>
<marker>Bouchard-Côté, Hall, Griffiths, Klein, 2013</marker>
<rawString>Alexandre Bouchard-Côté, David Hall, Thomas L. Griffiths and Dan Klein. 2013. Automated reconstruction of ancient languages using probabilistic models of sound change. Proceedings of the National Academy of Sciences. http://-www.pnas.org/content/110/11/4224</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Felsenstein</author>
</authors>
<title>The newick tree format.</title>
<date>1986</date>
<note>http://evolution.genetics.washington.edu/phylip/ne wicktree.html</note>
<contexts>
<context position="25862" citStr="Felsenstein, 1986" startWordPosition="3961" endWordPosition="3962"> e.g. to consider the functional load of tones in sesquisyllables. 5.3 Structural comparability: EtySet The discussion thus far has focused on the form and quality of data items. We are equally concerned with what might be called structural comparability of data sets, because this determine the approach we take to systematic description, dissemination, and re-use of cognate sets, phylogenetic trees, or sets of proto-form reconstructions. This has nothing to do with tagging or interchange standards, which can be handled with borrowed schemes designed for similar purposes, e.g. Newick notation (Felsenstein, 1986) or successors (Nakhleh, 2003). Rather, we require nomenclature that might be used to describe their contents, or to enable identification of sets of comparable complexity, structure, or detail. We think such comparison is crucial to help research in quantitative historical linguistics move beyond its current state, which many linguists view as interesting but nevertheless ad hoc experimentation. In other words, we would like to see computational approaches to cognate identification, subgrouping, and proto-language reconstruction be developed and tested in environments for which the controlled</context>
</contexts>
<marker>Felsenstein, 1986</marker>
<rawString>Joseph Felsenstein. 1986. The newick tree format. http://evolution.genetics.washington.edu/phylip/ne wicktree.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Greenhill</author>
<author>Robert Blust</author>
<author>Russell D Gray</author>
</authors>
<title>The Austronesian Basic Vocabulary Database: From Bioinformatics to Lexomics. Evolutionary Bioinformatics,</title>
<date>2008</date>
<pages>4--271</pages>
<contexts>
<context position="21193" citStr="Greenhill et al, 2008" startWordPosition="3268" endWordPosition="3271">ises the root nodes. Dialects are green, languages yellow, and groups blue 95 Figure 2 A search for “bone” in ABVD Austronesian data (again, relations by Ethnologue via MultiTree), constrained to locations in Indonesia, and projected onto a map readily specified. In this example dialects (from the same sources) are arranged in a circular pattern around the ISO 639-3 hub language (and, again, other analyses could be used instead). The same filtering and visualization routines are used in a different manner in Figure 2, which shows words for “bone” in Austronesian languages as provided by ABVD (Greenhill et al, 2008). 5 Data comparability and reusability We will finish the discussion of data warehouses with a quick look at data comparability and reuse. Comparability or equivalence of datasets can be looked at in two ways • at the content level, e.g. to ensure that the same systems of transcription and glossing are used for all datasets, and • at the structural level, in identifying datasets of comparable complexity, structure, or available detail. At the content level, normalization of forms and glosses is the critical transformation in the journey to gold-standard quality. We will briefly describe our sy</context>
</contexts>
<marker>Greenhill, Blust, Gray, 2008</marker>
<rawString>Simon Greenhill, Robert Blust, and Russell D.. Gray. 2008. The Austronesian Basic Vocabulary Database: From Bioinformatics to Lexomics. Evolutionary Bioinformatics, 4:271-283. http://-language.psy.auckland.ac.nz/austronesian</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard W Hinrichs</author>
<author>Marie Hinrichs</author>
<author>Thomas Zastrow</author>
</authors>
<title>WebLicht: Web-Based LRT Services for German. In:</title>
<date>2010</date>
<booktitle>Proceedings of the ACL</booktitle>
<pages>25--29</pages>
<contexts>
<context position="16742" citStr="Hinrichs et al 2010" startWordPosition="2571" endWordPosition="2574">s with easily customized data test beds – the analytical data warehouse. 4 The data warehouse A data warehouse is an integrated collection of databases that incorporates tools for sampling, analyzing, and visualizing query results. Unlike repository databases intended for storage and retrieval of prepared values (perhaps for off-line processing), data warehouses assume that data filtering, transformation, and analysis are essential to satisfying every query. In the context of comparative lexicons, such tasks are well beyond the scope of existing virtual research environments such as WebLicht (Hinrichs et al 2010) and TextGrid (Neuroth et al 2011), which focus primarily on text corpora. Because sampling filters allow selection of homogeneous or representative subsamples, we can be as inclusive as possible in regard to data acquisition. We are not talking about data quality; rather (working within our overall criterion of comparative lexical data) we want to avoid excluding sets because of concerns about dataset size or content disparity, or over-representation of dialect survey data. Many operations we wish to perform on or with data involve open research questions. Although users may perceive the ware</context>
</contexts>
<marker>Hinrichs, Hinrichs, Zastrow, 2010</marker>
<rawString>Erhard W. Hinrichs, Marie Hinrichs and Thomas Zastrow. 2010. WebLicht: Web-Based LRT Services for German. In: Proceedings of the ACL 2010 System Demonstrations. pages 25–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
</authors>
<title>The evolution of evaluation: Lessons from the Message Understanding Conferences. Computer Speech and Language,</title>
<date>1998</date>
<pages>12--283</pages>
<marker>Hirschman, 1998</marker>
<rawString>Lynette Hirschman. 1998. The evolution of evaluation: Lessons from the Message Understanding Conferences. Computer Speech and Language, 12:283–285.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Gold Standard Datasets for Evaluating Word Sense Disambiguation Programs.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="8699" citStr="Kilgarriff 1998" startWordPosition="1335" endWordPosition="1336">gress in quantitative and qualitative terms? Both data, and evaluation metrics, are made available well before the STEC, which is often held in conjunction with a major conference. The task is typically initiated by the release of a dataset; results are submitted by some deadline, and the results of evaluation are announced before or at the conference. The terms gold-standard and more recently, silver-standard (for machine-generated sets) are used to describe datasets created for use in STECs and NLP applications. These can be thought of as being “correct answers” for quantitative evaluation (Kilgarriff 1998). Gold-standard datasets are built to enable comparable evaluation of alternative algorithms or implementations. Frequently, part of the set will be publicly released in advance to serve as training data, while part of it is held back to provide test data (and is released at a later date). 92 Gold-standard datasets reflect the state of the art in an area, such as the specification of word senses, delineation of word boundaries, or evaluation of message sentiment, for which there may not be any purely objective ground truth. We can reasonably expect to allow alternative formulations of gold-sta</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. Gold Standard Datasets for Evaluating Word Sense Disambiguation Programs. Computer Speech and Language, 12 (3) Special Issue on Evaluation of Speech and Language Technology, edited by Robert Gaizauskas. 453-472. http://www.kilgarriff.co.uk/Publications/1998-KCompSL.pdf For TREC see http://trec.nist.gov. The TIPSTER site has been preserved here: http://www.nist.gov/itl/div894/894.02/related_proj ects/tipster/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Kleiweg</author>
</authors>
<title>RuG/L04 Software for dialectometrics and cartography. Rijksuniversiteit Groningen. Faculteit der Letteren.</title>
<date>2006</date>
<note>http://www.let.rug.nl/kleiweg/L04/</note>
<marker>Kleiweg, 2006</marker>
<rawString>Peter Kleiweg. 2006. RuG/L04 Software for dialectometrics and cartography. Rijksuniversiteit Groningen. Faculteit der Letteren. http://www.let.rug.nl/kleiweg/L04/</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul Lewis</author>
</authors>
<title>Ethnologue: Languages of the World, Sixteenth Edition.</title>
<date>2009</date>
<publisher>SIL International,</publisher>
<location>Dallas, Texas.</location>
<contexts>
<context position="20326" citStr="Lewis, 2009" startWordPosition="3133" endWordPosition="3134">or source data, and phonological properties of the doculect itself. Some of this metadata may be returned with individual items as part of the data frame. Filter targets may be specified if appropriate. For example, a filter might limit a search to languages that contain sesquisyllables, or instead require that returned items be sesquisyllabic. 4.2 An example query and result Figure 1 shows the result of a relatively simple warehouse query (using our unreleased exploratory implementation): a geo-constrained phylogenetic tree for Trans-New Guinea languages. Tree topology follows Ethnologue 16 (Lewis, 2009) as provided by the MultiTree project (Aristar and Ratliff, 2005); other analyses are Figure 1 A geo-constrained phylogenetic tree (analysis by Ethnologue via MultiTree). This cluster tree keeps low-level group nodes near their daughters, but raises the root nodes. Dialects are green, languages yellow, and groups blue 95 Figure 2 A search for “bone” in ABVD Austronesian data (again, relations by Ethnologue via MultiTree), constrained to locations in Indonesia, and projected onto a map readily specified. In this example dialects (from the same sources) are arranged in a circular pattern around </context>
</contexts>
<marker>Lewis, 2009</marker>
<rawString>M. Paul Lewis. 2009. Ethnologue: Languages of the World, Sixteenth Edition. SIL International, Dallas, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johann-Mattis List</author>
<author>Steven Moran</author>
</authors>
<title>An Open-Source Toolkit for Quantitative Historical Linguistics.</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>84667--1</pages>
<location>13–18, Sofia,</location>
<contexts>
<context position="17664" citStr="List and Moran (2013)" startWordPosition="2724" endWordPosition="2727">criterion of comparative lexical data) we want to avoid excluding sets because of concerns about dataset size or content disparity, or over-representation of dialect survey data. Many operations we wish to perform on or with data involve open research questions. Although users may perceive the warehouse as providing access to tools, we intend to present it to tool developers as a tunable test bed of data that does not require them to deal with data management, as well as a means of using, and encouraging development of, open-source toolkits such as the pioneering work of Kleiweg (2009) 94 and List and Moran (2013). We return to the idea of plug-and-play operations on lexicons in Section 6. The warehouse also helps provide added value to potential data contributors. Even if software is freely available, preparing data or setting up tools can impose substantial, even insurmountable, burdens on data creators, particularly in regions in which cooperation between linguists and computer scientists is less common than in the US or Europe. 4.1 Data warehouse query-flow In our test warehouse implementation, functionality is divided as follows: • filter: define a search universe based on phylogenetic or phonotac</context>
</contexts>
<marker>List, Moran, 2013</marker>
<rawString>Johann-Mattis List and Steven Moran. 2013. An Open-Source Toolkit for Quantitative Historical Linguistics. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 13–18, Sofia, Bulgaria. http://-www.zora.uzh.ch/84667/1/P13-4003.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luay Nakhleh</author>
<author>Daniel Miranker</author>
<author>Francois Barbancon</author>
</authors>
<title>Requirements of Phylogenetic Databases.</title>
<date>2003</date>
<booktitle>In Third IEEE Symposium on BioInformatics and BioEngineering (BIBE’03).</booktitle>
<pages>141--148</pages>
<publisher>IEEE Press.</publisher>
<note>http://www.cs.rice.edu/~nakhleh/Papers/-bibe03_final.pdf</note>
<marker>Nakhleh, Miranker, Barbancon, 2003</marker>
<rawString>Luay Nakhleh, Daniel Miranker, and Francois Barbancon. 2003. Requirements of Phylogenetic Databases. In Third IEEE Symposium on BioInformatics and BioEngineering (BIBE’03). 141-148. IEEE Press. http://www.cs.rice.edu/~nakhleh/Papers/-bibe03_final.pdf</rawString>
</citation>
<citation valid="false">
<authors>
<author>Heike Neuroth</author>
<author>Felix Lohmeier</author>
<author>Kathleen Marie Smith TextGrid</author>
</authors>
<title>Virtual Research Environment for the Humanities. In:</title>
<journal>The International Journal of Digital Curation.</journal>
<volume>6</volume>
<pages>2011--222</pages>
<marker>Neuroth, Lohmeier, TextGrid, </marker>
<rawString>Heike Neuroth, Felix Lohmeier, Kathleen Marie Smith: TextGrid. Virtual Research Environment for the Humanities. In: The International Journal of Digital Curation. 6, Nr. 2, 2011, S. 222–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Nordhoff</author>
<author>Harald Hammarström</author>
</authors>
<title>Glottolog/Langdoc: Defining dialects, languages, and language families as collections of resources.</title>
<date>2011</date>
<booktitle>783 CEUR Workshop, Proceedings of the First International Workshop on Linked Science 2011 http://iswc2011.semanticweb.org/fileadmin/iswc/-Papers/Workshops/LISC/nordhoff.pdf</booktitle>
<contexts>
<context position="2991" citStr="Nordhoff and Hammarström, 2011" startWordPosition="443" endWordPosition="447">ntra-language dialect surveys when available. All available metadata are incorporated, including typological and phonotactic features, (phylogenetic) character sets, geo-physical and demographic data, details of lexicon coverage, extent, or quality, and bibliographic or source data. Such data are not always easily found. Their delivery packages – primarily books and journals – may be discoverable via bibliographic metadata, but details of the datasets themselves are not. As a result, traditional bibliographic documentation, accessed via portals like OLAC (Simons and Bird, 2000) and Glottolog (Nordhoff and Hammarström, 2011), tends to have low recall and precision in regard to data resource discovery. Our experience in acquiring and performing methodical data audits of large quantities of published and unpublished materials reveals sets of lexical, grammatical, phonological, corpus, and other materials that are regular enough in form, and extensive enough in content, to comprise aggregable linguistic data supersets for the Asia-Pacific region. These ongoing data audits take a three-tiered approach, separately documenting texts (to enable source recovery), their abstract data content (to enable high-recall resourc</context>
</contexts>
<marker>Nordhoff, Hammarström, 2011</marker>
<rawString>Sebastian Nordhoff and Harald Hammarström. 2011. Glottolog/Langdoc: Defining dialects, languages, and language families as collections of resources. 783 CEUR Workshop, Proceedings of the First International Workshop on Linked Science 2011 http://iswc2011.semanticweb.org/fileadmin/iswc/-Papers/Workshops/LISC/nordhoff.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erich R Round</author>
</authors>
<title>Big data’ typology and linguistic phylogenetics: Design principles for valid datasets.</title>
<date>2013</date>
<journal>Presented</journal>
<volume>25</volume>
<contexts>
<context position="9566" citStr="Round (2013" startWordPosition="1477" endWordPosition="1478">(and is released at a later date). 92 Gold-standard datasets reflect the state of the art in an area, such as the specification of word senses, delineation of word boundaries, or evaluation of message sentiment, for which there may not be any purely objective ground truth. We can reasonably expect to allow alternative formulations of gold-standard sets in areas in which the state of the art may be uncertain, even in the eyes of experts. And we can anticipate increased critical scrutiny of previously accepted judgments as more base data and better investigative tools become available; see e.g. Round (2013, 2014). 2.1 STECs for low-density languages In our opinion, all of the reasons for which STECs are devised and gold-standard datasets defined apply equally to the low-density language problems we touched on in Section 1. These include: • normalization and syllabification of transcribed data, • phonetic transcription of audio and orthographic data, • morphemic analysis of transcribed data, • extraction of a phonemic analysis from phonetic data, • identification of internal cognates and/or derivationally related forms, as well as loan-word identification and stratification, • automated reglossi</context>
</contexts>
<marker>Round, 2013</marker>
<rawString>Erich R. Round. 2013. ‘Big data’ typology and linguistic phylogenetics: Design principles for valid datasets. Presented 25 May 2013 at 21st Manchester Phonology Meeting. Accessible via https://uq.academia.edu/ErichRound.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erich R Round</author>
</authors>
<title>The performance of STRUCTURE on linguistic datasets &amp; ‘researcher degrees of freedom’.</title>
<date>2014</date>
<journal>Presented</journal>
<volume>15</volume>
<location>Aarhus,</location>
<note>Accessible via https://uq.academia.edu/ErichRound</note>
<marker>Round, 2014</marker>
<rawString>Erich R. Round. 2014. The performance of STRUCTURE on linguistic datasets &amp; ‘researcher degrees of freedom’. Presented 15 Jan 2014 at TaSil, Aarhus, Denmark. Accessible via https://uq.academia.edu/ErichRound</rawString>
</citation>
<citation valid="true">
<authors>
<author>SIL Mainland</author>
</authors>
<title>Southeast Asia Group.</title>
<date>2002</date>
<booktitle>Southeast Asia 436 Word List revised</booktitle>
<note>http://msea-ling.info/digidata/495/b11824.pdf</note>
<marker>Mainland, 2002</marker>
<rawString>SIL Mainland Southeast Asia Group. 2002. Southeast Asia 436 Word List revised November 2002. http://msea-ling.info/digidata/495/b11824.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Simons</author>
<author>Steven Bird</author>
</authors>
<title>The seven pillars of open language archiving:A vision statement.</title>
<date>2000</date>
<note>http://www.language-archives.org/docs/vision.-html.</note>
<contexts>
<context position="2944" citStr="Simons and Bird, 2000" startWordPosition="437" endWordPosition="440">smaller lexicons when necessary, and intra-language dialect surveys when available. All available metadata are incorporated, including typological and phonotactic features, (phylogenetic) character sets, geo-physical and demographic data, details of lexicon coverage, extent, or quality, and bibliographic or source data. Such data are not always easily found. Their delivery packages – primarily books and journals – may be discoverable via bibliographic metadata, but details of the datasets themselves are not. As a result, traditional bibliographic documentation, accessed via portals like OLAC (Simons and Bird, 2000) and Glottolog (Nordhoff and Hammarström, 2011), tends to have low recall and precision in regard to data resource discovery. Our experience in acquiring and performing methodical data audits of large quantities of published and unpublished materials reveals sets of lexical, grammatical, phonological, corpus, and other materials that are regular enough in form, and extensive enough in content, to comprise aggregable linguistic data supersets for the Asia-Pacific region. These ongoing data audits take a three-tiered approach, separately documenting texts (to enable source recovery), their abstr</context>
</contexts>
<marker>Simons, Bird, 2000</marker>
<rawString>Gary Simons and Steven Bird. 2000. The seven pillars of open language archiving:A vision statement. http://www.language-archives.org/docs/vision.-html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
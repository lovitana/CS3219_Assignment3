<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9993565">
Dependency-based Automatic Enumeration of Semantically Equivalent
Word Orders for Evaluating Japanese Translations
</title>
<author confidence="0.993919">
Hideki Isozaki, Natsume Kouchi Tsutomu Hirao
</author>
<affiliation confidence="0.993563">
Okayama Prefectural University NTT Communication Science Laboratories
</affiliation>
<address confidence="0.847407">
111 Kuboki, Soja-shi, Okayama, 719-1197, Japan 2-4, Hikaridai, Seika-cho, Sorakugun, Kyoto, 619-0237, Japan
</address>
<email confidence="0.996695">
isozaki@cse.oka-pu.ac.jp hirao.tsutomu@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998547304347826">
Scrambling is acceptable reordering of
verb arguments in languages such as
Japanese and German. In automatic eval-
uation of translation quality, BLEU is
the de facto standard method, but BLEU
has only very weak correlation with hu-
man judgements in case of Japanese-to-
English/English-to-Japanese translations.
Therefore, alternative methods, IMPACT
and RIBES, were proposed and they have
shown much stronger correlation than
BLEU. Now, RIBES is widely used in
recent papers on Japanese-related transla-
tions. RIBES compares word order of MT
output with manually translated reference
sentences but it does not regard scram-
bling at all. In this paper, we present a
method to enumerate scrambled sentences
from dependency trees of reference sen-
tences. Our experiments based on NTCIR
Patent MT data show that the method im-
proves sentence-level correlation between
RIBES and human-judged adequacy.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999630769230769">
Statistical Machine Translation has grown with an
automatic evaluation method BLEU (Papineni et
al., 2002). BLEU measures local word order by n-
grams and does not care about global word order.
In JE/EJ translations, this insensitivity degrades
BLEU’s correlation with human judgements.
Therefore, alternative automatic evaluation
methods are proposed. Echizen-ya and Araki
(2007) proposed IMPACT. Isozaki et al. (2010)
presented the idea of RIBES. Hirao et al. (2011)
named this method “RIBES” (Rank-based Intu-
itive Bilingual Evaluation Score). This version of
RIBES was defined as follows:
</bodyText>
<equation confidence="0.678346">
RIBES = NKT × Pα
</equation>
<tableCaption confidence="0.999074">
Table 1: Meta-evaluation of NTCIR-7 JE task data
</tableCaption>
<table confidence="0.970895">
(Spearman’s ρ, System-level correlation)
BLEU METEOR ROUGE-L IMPACT RIBES
0.515 0.490 0.903 0.826 0.947
</table>
<bodyText confidence="0.975488272727273">
where NKT (Normalized Kendall’s T) is defined
by (T + 1)/2. This NKT is used for measur-
ing word order similarity between a reference sen-
tence and an MT output sentence. Thus, RIBES
penalizes difference of global word order. P is
precision of unigrams. RIBES is defined for each
test sentence and averaged RIBES is used for eval-
uating the entire test corpus.
Table 1 is a table in an IWSLT-2012 invited
talk (http://hltc.cs.ust.hk/iwslt/slides/
Isozaki2012 slides.pdf). METEOR was pro-
posed by Banerjee and Lavie (2005). ROUGE-L
was proposed by Lin and Och (2004). According
to this table, RIBES with α = 0.2 has a very
strong correlation (Spearman’s p = 0.947) with
human-judged adequacy. For each sentence,
we use the average of adequacy scores of three
judges. Here, we call this average “Adequacy”.
We focus on Adequacy because current SMT
systems tend to output inadequate sentences.
Note that only single reference translations are
available for this task although use of multiple
references is common for BLEU.
RIBES is publicly available from http://
www.kecl.ntt.co.jp/icl/lirg/ribes/ and
was used as a standard quality measure in recent
NTCIR PatentMT tasks (Goto et al., 2011; Goto
et al., 2013). Table 2 shows the result of meta-
evaluation at NICTR-9/10 PatentMT. The table
shows that RIBES is more reliable than BLEU
and NIST.
Current RIBES has the following improve-
ments.
</bodyText>
<listItem confidence="0.964547">
• BLEU’s Brevity Penalty (BP) was introduced
</listItem>
<page confidence="0.949824">
287
</page>
<affiliation confidence="0.340356">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287–292,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<figure confidence="0.762284666666667">
jon ga
sushi-ya de o-sushi wo
tabe-ta
</figure>
<tableCaption confidence="0.9489445">
Table 2: Meta-evaluation at NTCIR-9/10
PatentMT (Spearman’s p, Goto et al. 2011, 2013)
</tableCaption>
<table confidence="0.9991172">
BLEU NIST RIBES
NTCIR-9 JE −0.042 −0.114 0.632
NTCIR-9 EJ −0.029 −0.074 0.716
NTCIR-10 JE 0.31 0.36 0.88
NTCIR-10 EJ 0.36 0.22 0.79
</table>
<tableCaption confidence="0.444895">
in order to penalize too short sentences.
</tableCaption>
<equation confidence="0.63328">
RIBES = NKT x Pα x BPβ
</equation>
<bodyText confidence="0.998560333333333">
where α = 0.25 and Q = 0.10. BLEU uses
BP for the entire test corpus, but RIBES uses
it for each sentence.
</bodyText>
<listItem confidence="0.994614333333333">
• The word alignment algorithm in the original
RIBES used only bigrams for disambiguation
when the same word appears twice or more
in one sentence. This restriction is now re-
moved, and longer n-grams are used to get a
better alignment.
</listItem>
<bodyText confidence="0.99733975">
RIBES is widely used in recent Annual Mee-
ings of the (Japanese) Association for NLP. In-
ternational conference papers on Japanese-related
translations also use RIBES. (Wu et al., 2012;
Neubig et al., 2012; Goto et al., 2012; Hayashi
et al., 2013). Dan et al. (2012) uses RIBES for
Chinese-to-Japanese translation.
However, we have to take “scrambling” into
account when we think of Japanese word order.
Scrambling is also observed in other languages
such as German. Current RIBES does not regard
this fact.
</bodyText>
<sectionHeader confidence="0.996306" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.8897915">
For instance, a Japanese sentence S1
jon ga sushi-ya de o-sushi wo tabe-ta .
(John ate sushi at a sushi restaurant.)
has the following acceptable word orders.
</bodyText>
<listItem confidence="0.997258">
1. jon ga sushi-ya de o-sushi wo tabe-ta .
2. jon ga o-sushi wo sushi-ya de tabe-ta .
3. sushi-ya de jon ga o-sushi wo tabe-ta .
4. sushi-ya de o-sushi wo jon ga tabe-ta .
5. o-sushi wo jon ga sushi-ya de tabe-ta .
6. o-sushi wo sushi-ya de jon ga tabe-ta .
</listItem>
<bodyText confidence="0.979363333333333">
The boldface short words “ga”, “de”, and
“wo”, are case markers (“Kaku joshi” in
Japanese).
</bodyText>
<figureCaption confidence="0.945877">
Figure 1: Dependency Tree of S1
</figureCaption>
<listItem confidence="0.950776333333333">
• “ga” is a nominative case marker that means
the noun phrase before it is the subject of a
following verb/adjective.
• “de” is a locative case marker that means the
noun phrase before it is the location of a fol-
lowing verb/adjective.
• “wo” is an accusative case marker that means
the noun phrase before it is the direct object
of a following verb.
</listItem>
<bodyText confidence="0.960530083333333">
The term “scrambling” stands for these accept-
able permutations. These case markers explicitly
show grammatical cases and reordering of them
does not hurt interpretation of these sentences. Al-
most all other permutations of words are not ac-
ceptable (∗).
∗ jon ga de sushi-ya o-sushi tabe-ta wo .
∗ jon de sushi-ya ga o-sushi wo tabe-ta .
∗ jon tabe-ta ga o-sushi wo sushi-ya de .
∗ sushi-ya ga jon tabe-ta de o-sushi wo .
Most readers unfamiliar with Japanese will not
understand which word order is acceptable.
</bodyText>
<subsectionHeader confidence="0.995855">
2.1 Scrambling as Post-Order Traversal of
Depenedncy Trees
</subsectionHeader>
<bodyText confidence="0.999522285714286">
Here, we describe this “scrambling” from the
viewpoint of Computer Science. Figure 1 shows
S1’s dependency tree. Each box indicates a “bun-
setsu” or a grammatical chunk of words. Each ar-
row starts from a modifier (dependent) to its head.
The root of S1 is “tabe-ta” (ate). This verb
has three modifiers:
</bodyText>
<listItem confidence="0.999151">
• “jon ga” (John is its subject)
• “sushi-ya de” (A sushi restaurant is its location)
• “o-sushi wo” (Sushi is its object)
</listItem>
<bodyText confidence="0.994571833333333">
It is well known that Japanese is a typical head-
final language. In order to generate a head-final
word order from this dependency tree, we should
output tree nodes in post-order. That is, we have
to output all children of a node N before the node
N itself.
</bodyText>
<page confidence="0.995454">
288
</page>
<figureCaption confidence="0.999711">
Figure 2: Dependency Tree of S2
</figureCaption>
<bodyText confidence="0.9999725">
All of the above acceptable word orders follows
this post-order. Even in post-order traverse, prece-
dence among children is not determined and this
fact leads to different permutations of children. In
the above example, the root “tabe-ta” has three
children, and its permutation is 3! = 6.
</bodyText>
<subsectionHeader confidence="0.995163">
2.2 Simple Case Marker Constraint
</subsectionHeader>
<bodyText confidence="0.8929992">
Figure 2 shows the dependency tree of a more
complicated sentence S2:
jon ga sushi-ya de o-sushi wo tabe-ta
ato ni kabuki wo mi-ta .
(John watched kabuki after eating sushi at a shushi
restaurant)
Kabuki is a traditional Japanese drama performed
in a theatre. In this case, the root “mi-ta”
(watched) has two children: “ato ni” (after it)
and “kabuki wo” (kabuki is its object).
• “ni” is a dative/locative case marker that
means the noun phrase before it is an indi-
rect object or a location/time of a following
verb/adjective.
In this case, we obtain 3!×2! = 12 permutations:
</bodyText>
<listItem confidence="0.9931765">
1. *S1P* ato ni kabuki wo mi-ta .
2. kabuki wo *S1P* ato ni mi-ta .
</listItem>
<bodyText confidence="0.977560966666667">
Here, *S1P* is any of the above 3! permutations
of S1. If we use S1’s 3 as *S1P* in S2’s 1, we get
sushi-ya de jon ga o-sushi wo tabe-ta
ato ni kabuki wo mi-ta .
However, we cannot accept all of these permu-
tations equally. For instance,
kabuki wo o-sushi wo sushi-ya de
jon ga tabe-ta ato ni mi-ta .
is comprehensible but strange. This strangness
comes from the two objective markers “wo” be-
fore the first verb “tabe-ta.” Which did John
eat, kabuki or sushi? Semantically, we cannot
eat kabuki (drama), and we can understand this
sentence. But syntactic ambiguity causes this
strangeness. Without semantic knowledge about
kabuki and sushi, we cannot disambiguate this
case.
For readers/listeners, we should avoid such
syntactically ambiguous sentences. Modifiers
(here, “kabuki wo”) of a verb (here, “mi-ta”,
watched) should not be placed before another verb
(here, “tabe-ta”, ate).
In Japanese, verbs and adjectives are used sim-
ilarly. In general, adjectives are not modified by
“wo” case markers. Therefore, we can place “wo”
case markers before adjectives. In the following
sentences, “atarashii” (new) is an adjective
and placing “inu wo” (A dog is the direct object)
before “atarashii” does not make the sentence
ambiguous.
</bodyText>
<listItem confidence="0.995115666666667">
• atarashii ie ni inu wo ture te itta .
((Someone) took the dog to the new house.)
• inu wo atarashii ie ni ture te itta .
</listItem>
<bodyText confidence="0.8104408125">
This idea leads to the following Simple Case
Marker Constraint:
Definition 1(Simple Case Marker Constraint)
If a reordered sentence has a case marker phrase
of a verb that precedes another verb before the
verb, the sentence is rejected. “wo” case markers
can precede adjectives before the verb.
This is a primitive heuristic constraint and there
must be better ways to make it more flexible.
If we use Nihongo Goi Taikei (Ikehara et al.,
1997), we will be able to implement such a flex-
isble constraint. For example, some verbs such
as “sai-ta” (bloomed) are never modified by
“wo” case marker phrases. Therefore, the follow-
ing sentence is not ambiguous at all although the
wo phrase precedes “sai-ta”.
</bodyText>
<listItem confidence="0.998566333333333">
• hana ga sai-ta ato ni sono ki wo mi-ta.
((Someone) saw the tree after it bloomed.)
• sono ki wo hana ga sai-ta ato ni mi-ta.
</listItem>
<subsectionHeader confidence="0.998681">
2.3 Evaluation with scrambled sentences
</subsectionHeader>
<bodyText confidence="0.998218666666667">
As we mentioned before, RIBES measures global
word order similarity between machine-translated
sentences and reference sentences. It does not re-
gard scrambling at all. When the target language
allows scrambling just like Japanese, RIBES
should consider scrambling.
Once we have a correct dependency tree of the
reference sentence, we enumerate scrambled sen-
tences by reordering children of each node. The
</bodyText>
<figure confidence="0.996553571428571">
o-sushi wo
ato ni
kabuki wo
sushi-ya de
tabe-ta
mi-ta
jon ga
</figure>
<page confidence="0.991211">
289
</page>
<bodyText confidence="0.9999555">
number of the reordered sentences depend on the
structure of the dependency tree.
Current RIBES code (RIBES-1.02.4) assumes
that every sentence has a fixed number of refer-
ences, but here the number of automatically gen-
erated reference sentences depends on the depen-
dency structure of the original reference sentence.
Therefore, we modified the code for variable num-
bers of reference sentences. RIBES-1.02.4 simply
uses the maximum value of the scores for different
reference sentences, and we followed it.
Here, we compare the following four methods.
</bodyText>
<listItem confidence="0.994527545454546">
• single: We use only single reference transla-
tions provided by the NTCIR organizers.
• postOrder: We generate all permutations of
the given reference sentence generated by
post-order traversals of its dependency tree.
This can be achieved by the following two
steps. First, we enumerate all permutations
of child nodes at each node. Then, we com-
bine these permutations. This is implemented
by cartesian products of the permutation sets.
• caseMarkers: We reorder only “case marker
</listItem>
<bodyText confidence="0.882128882352941">
(kaku joshi) phrases”. Here, a “case marker
phrase” is post-order traversal of a subtree
rooted at a case marker bunsetsu. For in-
stance, the root of the following sentence S3
has a non-case marker child “kaburi ,”
(wear) between case marker children, “jon
ga” and “zubon wo” (Trousers are the ob-
ject). Figure 3 shows its dependency tree.
jon ga shiroi boushi wo kaburi ,
kuroi zubon wo hai te iru.
(John wears a white hat and wears black trousers.)
This is implemented by removing non-case
marker nodes from the set of child nodes
to be reordered in the above “postOrder”
method. For simplicity, we do not reorder
other markers such as the topic marker “wa”
here. This is future work.
</bodyText>
<listItem confidence="0.986010111111111">
• proposed: We reorder only contiguous case
marker children of a node, and we accept sen-
tences that satisfy the aforementioned Sim-
ple Case Marker Constraint. S3’s root node
has two case marker children, but they are
not contiguous. Therefore, we do not reorder
them. We expect that the constraint inhibit
generation of incomprehensible or mislead-
ing sentences.
</listItem>
<figure confidence="0.495402">
shiroi
boushi wo kuroi
jon ga kaburi , zubon wo
hai te iru.
</figure>
<figureCaption confidence="0.994124">
Figure 3: Dependency Tree of S3
</figureCaption>
<tableCaption confidence="0.956599">
Table 3: Distribution of the number of generated
permutations
</tableCaption>
<table confidence="0.9953684">
#permutations 1 2 4 6 8 12 16 24 &gt;24
single 100 0 0 0 0 0 0 0 0
proposed 70 20 7 3 0 0 0 0 0
caseMarkers 64 23 4 6 2 2 0 2 0
postOrder 1 17 9 11 4 12 1 12 33
</table>
<sectionHeader confidence="0.999769" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.998859322580645">
We applied the above four methods to the ref-
erence sentences of human-judged 100 sentences
of NTCIR-7 Patent MT EJ task. (Fujii et al.,
2008) We applied CaboCha (Kudo and Mat-
sumoto, 2002) to the reference sentences, and
manually corrected the dependency trees because
Japanese dependency parsers are not satisfactory
in terms of sentence accuracy (Tamura et al.,
2007).
To support this manual correction, CaboCha’s
XML output was automatically converted
to dependency tree pictures by using
cabochatrees package for LATEX. http://
softcream.oka-pu.ac.jp/wp/wp-content/
uploads/cabochatrees.pdf. Then, it is easy
to find mistakes of the dependency trees. In
addition, CaboCha’s dependency accuracy is very
high (89–90%) (Kudo and Matsumoto, 2002).
Therefore, it took only one day to fix dependency
trees of one hundred reference sentences.
Table 3 shows distribution of the number of
word orders generated by the above methods. Pos-
tOrder sometimes generates tens of thousands of
permutations.
Figure 4 shows a sentence-level scatter plot
between Adequacy and RIBES for the baseline
Moses system. Each x indicates a sentence.
Arrows indicate significant improvements of
RIBES scores by the proposed method. For in-
stance, the x mark at (5.0, 0.53) corresponds to an
MT output:
</bodyText>
<page confidence="0.987483">
290
</page>
<figure confidence="0.892122833333333">
RIBES
1
0.8
0.6
0.4
0.2
</figure>
<table confidence="0.95751425">
1 2 3 4 5 Adequacy
Average of RIBES: 0.706 → 0.719
Pearson’s r: 0.607 → 0.663
Spearman’s p: 0.607 → 0.670
</table>
<tableCaption confidence="0.823193571428572">
Figure 4: Scatter plot between Adequacy and
RIBES for 100 human-judged sentences in the
output of NTCIR-7’s baseline Moses system and
the effects of the proposed method
Table 4: Improvement of sentence-level correla-
tion between Adequacy and RIBES for human-
judged NTCIR-7 EJ systems (MAIN RESULT)
</tableCaption>
<table confidence="0.999887571428571">
Pearson’s r Spearman’s p
single proposed single proposed
tsbmt 0.466 0.472 0.439 0.452
Moses 0.607 0.663 0.607 0.670
NTT 0.709 0.735 0.692 0.727
NICT-ATR 0.620 0.631 0.582 0.608
kuro 0.555 0.608 0.515 0.550
</table>
<tableCaption confidence="0.992575">
Table 5: Increase of averaged RIBES scores
</tableCaption>
<table confidence="0.987154714285714">
Adeq. RIBES
system single proposed caseMarkers postOrder
tsbmt 3.527 0.715 0.7188 0.719 0.7569
moses 2.897 0.706 0.7192 0.722 0.781
NTT 2.740 0.671 0.683 0.686 0.7565
NICT-ATR 2.587 0.655 0.664 0.670 0.749
kuro 2.420 0.629 0.638 0.647 0.752
</table>
<figure confidence="0.900987">
0
0
</figure>
<bodyText confidence="0.968052224489796">
indekkusu kohna wo zu 25 ni shimesu .
which is a Japanese translation of “FIG.25 shows
the index corner.” The reference sentence for this
sentence is
zu 25 ni indekkusu kohna wo shimeshi
te iru .
In this case, RIBES is 0.53, but all of the three
judges evaluated this as 5 of 5-point scale. That
is, RIBES disagrees with human judges. The pro-
posed method reorders this reference sentence as
follows:
indekkusu kohna wo zu 25 ni shimeshi
te iru .
This is very close to the above MT output and
RIBES is 0.884 for this automatically reordered
reference sentence. This shows that automatic re-
ordering reduces the gap between single-reference
RIBES and Adequacy.
Although RIBES strongly correlates with ade-
quacy at the system level (Table 1), it has only
mediocre correlation with adequacy at the sen-
tence level: Spearman’s p is 0.607 for the baseline
Moses system. The “proposed” method improves
it to 0.670.
We can draw similar scatter plots for each sys-
tem. Table 4 summarises such improvement of
correlations. And this is the main result of this
paper. The “proposed” method consistently im-
proves sentence-level correlation between Ade-
quacy and RIBES.
Table 5 shows increase of averaged RIBES, but
this increase is not always an improvement. We
expected that “PostOrder” generates not only ac-
ceptable sentences but also incomprehensible or
misleading sentences. This must be harmful to the
automatic evaluation by RIBES. Accoding to this
table, PostOrder gave higher RIBES scores to all
systems and correlation between RIBES and Ade-
quacy is lost as expected.
The ranking by RIBES-1.02.4 with “single”
reference sentences completely agrees with Ad-
equacy, but the weakest constraint, “postOrder”,
disagrees. Spearman’s p of the two ranks is 0.800
but Pearson’s r is as low as 0.256. It generates too
many incomprehensible/misleading word orders,
and they also raise RIBES scores of bad transla-
tions. On the other hand, “proposed” and “case-
Markers” agree with Adequacy except the ranks
of tsbmt and the baseline Moses.
</bodyText>
<sectionHeader confidence="0.993192" genericHeader="method">
4 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999266333333333">
RIBES is now widely used in Japanese-related
translation evaluation. But RIBES sometimes pe-
nalizes good sentences because it does not re-
gard scrambling. Once we have correct depen-
dency trees of reference sentences, we can auto-
matically enumerate semantically equivalent word
</bodyText>
<page confidence="0.990371">
291
</page>
<bodyText confidence="0.999711875">
orders. Less constrained reordering tend to gener-
ate syntactically ambiguous sentences. They be-
come incomprehensible or misleading sentences.
In order to avoid them, we introduced Simple
Case Marker Constraint and restricted permuta-
tions to contiguous case marker children of verbs/
adjectives. Then, sentence-level correlation coef-
ficients were improved.
The proposed enumeration method is also ap-
plicable to other automatic evaluation methods
such as BLEU, IMPACT, and ROUGE-L, but we
have to modify their codes for variable numbers of
multi-reference sentences. We will examine them
in the full paper.
We hope our method is also useful for other lan-
guages that have scrambling.
</bodyText>
<sectionHeader confidence="0.986634" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.636545">
This research was supported by NTT Communi-
cation Science Laboratories.
</bodyText>
<sectionHeader confidence="0.995695" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991564556818181">
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with im-
proved correlation with human judgements. In Proc.
of ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for MT and Summarization, pages
65–72.
Han Dan, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proceedings of SSST-6, Sixth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 57–66.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In MT Summit XI, pages 151–158.
Atsushi Fujii, Masao Uchimura, Mikio Yamamoto, and
Takehito Usturo. 2008. Overview of the patent
machine translation task at the NTCIR-7 workshop.
In Working Notes of the NTCIR Workshop Meeting
(NTCIR).
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Working Notes of the NTCIR Workshop Meeting
(NTCIR).
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statis-
tical machine translation. In Proc. of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 311–316.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the patent
machine translation task at the NTCIR-10 work-
shop. In Working Notes of the NTCIR Workshop
Meeting (NTCIR).
Katsuhiko Hayashi, Katsuhito Sudoh, Hajime Tsukada,
Jun Suzuki, and Masaaki Nagata. 2013. Shift-
reduce word reordering for machine translation.
In Proc. of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1382–1386.
Tsutomu Hirao, Hideki Isozaki, Kevin Duh, Katsuhito
Sudoh, Hajime Tsukada, and Masaaki Nagao. 2011.
RIBES: An automatic evaluation method of trans-
lation based on rank correlation (in Japanese). In
Proc. of the Annual Meeting of the Association for
Natural Language Processing, pages 1115–1118.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei — A Japanese Lexicon (in Japanese).
Iwanami Shoten.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-
suhito Sudoh, Hajime Tsukada, and Masaaki Na-
gata. 2010. Automatic evaluation of translation
quality for distant language pairs. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 944–952.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of the Conference on Computational Natural
Language Learning (CoNLL).
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of translation quality using longest com-
mon subsequences and skip-bigram statistics. In
Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), pages 605–612.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proc. of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 843–853.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311–318.
Akihiro Tamura, Hiroya Takamura, and Manabu Oku-
mura. 2007. Japanese dependency analysis using
the ancestor-descendant relation. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 600–609.
Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsu-
jii. 2012. Akamon: An open source toolkit for
tree/forest-based statistical machine translation. In
Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), pages 127–132.
</reference>
<page confidence="0.997443">
292
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.671861">
<title confidence="0.990747">Dependency-based Automatic Enumeration of Semantically Word Orders for Evaluating Japanese Translations</title>
<author confidence="0.976635">Hideki Isozaki</author>
<author confidence="0.976635">Natsume Kouchi Tsutomu Hirao</author>
<affiliation confidence="0.999674">Okayama Prefectural University NTT Communication Science Laboratories</affiliation>
<address confidence="0.98323">111 Kuboki, Soja-shi, Okayama, 719-1197, Japan 2-4, Hikaridai, Seika-cho, Sorakugun, Kyoto, 619-0237, Japan</address>
<email confidence="0.792181">isozaki@cse.oka-pu.ac.jphirao.tsutomu@lab.ntt.co.jp</email>
<abstract confidence="0.994687833333333">Scrambling is acceptable reordering of verb arguments in languages such as Japanese and German. In automatic evaluation of translation quality, BLEU is the de facto standard method, but BLEU has only very weak correlation with human judgements in case of Japanese-to- English/English-to-Japanese translations. Therefore, alternative methods, IMPACT and RIBES, were proposed and they have shown much stronger correlation than BLEU. Now, RIBES is widely used in recent papers on Japanese-related translations. RIBES compares word order of MT output with manually translated reference sentences but it does not regard scrambling at all. In this paper, we present a method to enumerate scrambled sentences from dependency trees of reference sentences. Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgements.</title>
<date>2005</date>
<booktitle>In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="2582" citStr="Banerjee and Lavie (2005)" startWordPosition="371" endWordPosition="374">ata (Spearman’s ρ, System-level correlation) BLEU METEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s T) is defined by (T + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES is defined for each test sentence and averaged RIBES is used for evaluating the entire test corpus. Table 1 is a table in an IWSLT-2012 invited talk (http://hltc.cs.ust.hk/iwslt/slides/ Isozaki2012 slides.pdf). METEOR was proposed by Banerjee and Lavie (2005). ROUGE-L was proposed by Lin and Och (2004). According to this table, RIBES with α = 0.2 has a very strong correlation (Spearman’s p = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standar</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgements. In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Dan</author>
<author>Katsuhito Sudoh</author>
<author>Xianchao Wu</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Head finalization reordering for Chinese-to-Japanese machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<pages>57--66</pages>
<contexts>
<context position="4603" citStr="Dan et al. (2012)" startWordPosition="706" endWordPosition="709"> Pα x BPβ where α = 0.25 and Q = 0.10. BLEU uses BP for the entire test corpus, but RIBES uses it for each sentence. • The word alignment algorithm in the original RIBES used only bigrams for disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 Methodology For instance, a Japanese sentence S1 jon ga sushi-ya de o-sushi wo tabe-ta . (John ate sushi at a sushi restaurant.) has the following acceptable word orders. 1. jon ga sushi-ya de o-sushi wo tabe-ta . 2. jon ga o-sushi wo sushi-ya de tabe-ta . 3. sushi-ya de jon ga o-sushi wo tabe-ta . 4. sushi-ya de o-sushi wo jon ga tabe-ta . 5. o-sushi wo</context>
</contexts>
<marker>Dan, Sudoh, Wu, Duh, Tsukada, Nagata, 2012</marker>
<rawString>Han Dan, Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2012. Head finalization reordering for Chinese-to-Japanese machine translation. In Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Echizen-ya</author>
<author>Kenji Araki</author>
</authors>
<title>Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum.</title>
<date>2007</date>
<booktitle>In MT Summit XI,</booktitle>
<pages>151--158</pages>
<contexts>
<context position="1683" citStr="Echizen-ya and Araki (2007)" startWordPosition="226" endWordPosition="229">t a method to enumerate scrambled sentences from dependency trees of reference sentences. Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy. 1 Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × Pα Table 1: Meta-evaluation of NTCIR-7 JE task data (Spearman’s ρ, System-level correlation) BLEU METEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s T) is defined by (T + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word o</context>
</contexts>
<marker>Echizen-ya, Araki, 2007</marker>
<rawString>Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic evaluation of machine translation based on recursive acquisition of an intuitive common parts continuum. In MT Summit XI, pages 151–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Uchimura</author>
<author>Mikio Yamamoto</author>
<author>Takehito Usturo</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-7 workshop.</title>
<date>2008</date>
<booktitle>In Working Notes of the NTCIR Workshop Meeting (NTCIR).</booktitle>
<contexts>
<context position="13239" citStr="Fujii et al., 2008" startWordPosition="2203" endWordPosition="2206">t they are not contiguous. Therefore, we do not reorder them. We expect that the constraint inhibit generation of incomprehensible or misleading sentences. shiroi boushi wo kuroi jon ga kaburi , zubon wo hai te iru. Figure 3: Dependency Tree of S3 Table 3: Distribution of the number of generated permutations #permutations 1 2 4 6 8 12 16 24 &gt;24 single 100 0 0 0 0 0 0 0 0 proposed 70 20 7 3 0 0 0 0 0 caseMarkers 64 23 4 6 2 2 0 2 0 postOrder 1 17 9 11 4 12 1 12 33 3 Results We applied the above four methods to the reference sentences of human-judged 100 sentences of NTCIR-7 Patent MT EJ task. (Fujii et al., 2008) We applied CaboCha (Kudo and Matsumoto, 2002) to the reference sentences, and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy (Tamura et al., 2007). To support this manual correction, CaboCha’s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX. http:// softcream.oka-pu.ac.jp/wp/wp-content/ uploads/cabochatrees.pdf. Then, it is easy to find mistakes of the dependency trees. In addition, CaboCha’s dependency accuracy is very high (89–90%) (Kudo and Matsumoto, 2</context>
</contexts>
<marker>Fujii, Uchimura, Yamamoto, Usturo, 2008</marker>
<rawString>Atsushi Fujii, Masao Uchimura, Mikio Yamamoto, and Takehito Usturo. 2008. Overview of the patent machine translation task at the NTCIR-7 workshop. In Working Notes of the NTCIR Workshop Meeting (NTCIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-9 workshop.</title>
<date>2011</date>
<booktitle>In Working Notes of the NTCIR Workshop Meeting (NTCIR).</booktitle>
<contexts>
<context position="3249" citStr="Goto et al., 2011" startWordPosition="476" endWordPosition="479">rding to this table, RIBES with α = 0.2 has a very strong correlation (Spearman’s p = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT tasks (Goto et al., 2011; Goto et al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287–292, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics jon ga sushi-ya de o-sushi wo tabe-ta Table 2: Meta-evaluation at NTCIR-9/10 PatentMT (Spearman’s p, Goto et al. 2011, 2013) BLEU NIST RIBES NTCIR-9 JE −0.042 −0.114 0.632 NTC</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Working Notes of the NTCIR Workshop Meeting (NTCIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Post-ordering by parsing for japanese-english statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>311--316</pages>
<contexts>
<context position="4561" citStr="Goto et al., 2012" startWordPosition="698" endWordPosition="701">enalize too short sentences. RIBES = NKT x Pα x BPβ where α = 0.25 and Q = 0.10. BLEU uses BP for the entire test corpus, but RIBES uses it for each sentence. • The word alignment algorithm in the original RIBES used only bigrams for disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 Methodology For instance, a Japanese sentence S1 jon ga sushi-ya de o-sushi wo tabe-ta . (John ate sushi at a sushi restaurant.) has the following acceptable word orders. 1. jon ga sushi-ya de o-sushi wo tabe-ta . 2. jon ga o-sushi wo sushi-ya de tabe-ta . 3. sushi-ya de jon ga o-sushi wo tabe-ta . 4. sushi-ya de</context>
</contexts>
<marker>Goto, Utiyama, Sumita, 2012</marker>
<rawString>Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012. Post-ordering by parsing for japanese-english statistical machine translation. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 311–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Ka Po Chow</author>
<author>Bin Lu</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-10 workshop.</title>
<date>2013</date>
<booktitle>In Working Notes of the NTCIR Workshop Meeting (NTCIR).</booktitle>
<contexts>
<context position="3269" citStr="Goto et al., 2013" startWordPosition="480" endWordPosition="483">, RIBES with α = 0.2 has a very strong correlation (Spearman’s p = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT tasks (Goto et al., 2011; Goto et al., 2013). Table 2 shows the result of metaevaluation at NICTR-9/10 PatentMT. The table shows that RIBES is more reliable than BLEU and NIST. Current RIBES has the following improvements. • BLEU’s Brevity Penalty (BP) was introduced 287 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287–292, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics jon ga sushi-ya de o-sushi wo tabe-ta Table 2: Meta-evaluation at NTCIR-9/10 PatentMT (Spearman’s p, Goto et al. 2011, 2013) BLEU NIST RIBES NTCIR-9 JE −0.042 −0.114 0.632 NTCIR-9 EJ −0.029 −0.07</context>
</contexts>
<marker>Goto, Chow, Lu, Sumita, Tsou, 2013</marker>
<rawString>Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and Benjamin K. Tsou. 2013. Overview of the patent machine translation task at the NTCIR-10 workshop. In Working Notes of the NTCIR Workshop Meeting (NTCIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhiko Hayashi</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Jun Suzuki</author>
<author>Masaaki Nagata</author>
</authors>
<title>Shiftreduce word reordering for machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1382--1386</pages>
<contexts>
<context position="4584" citStr="Hayashi et al., 2013" startWordPosition="702" endWordPosition="705">entences. RIBES = NKT x Pα x BPβ where α = 0.25 and Q = 0.10. BLEU uses BP for the entire test corpus, but RIBES uses it for each sentence. • The word alignment algorithm in the original RIBES used only bigrams for disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 Methodology For instance, a Japanese sentence S1 jon ga sushi-ya de o-sushi wo tabe-ta . (John ate sushi at a sushi restaurant.) has the following acceptable word orders. 1. jon ga sushi-ya de o-sushi wo tabe-ta . 2. jon ga o-sushi wo sushi-ya de tabe-ta . 3. sushi-ya de jon ga o-sushi wo tabe-ta . 4. sushi-ya de o-sushi wo jon ga tabe</context>
</contexts>
<marker>Hayashi, Sudoh, Tsukada, Suzuki, Nagata, 2013</marker>
<rawString>Katsuhiko Hayashi, Katsuhito Sudoh, Hajime Tsukada, Jun Suzuki, and Masaaki Nagata. 2013. Shiftreduce word reordering for machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1382–1386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Hideki Isozaki</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagao</author>
</authors>
<title>RIBES: An automatic evaluation method of translation based on rank correlation (in Japanese).</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>1115--1118</pages>
<contexts>
<context position="1771" citStr="Hirao et al. (2011)" startWordPosition="241" endWordPosition="244">periments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy. 1 Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × Pα Table 1: Meta-evaluation of NTCIR-7 JE task data (Spearman’s ρ, System-level correlation) BLEU METEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s T) is defined by (T + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES is defined for each test sentence and averaged R</context>
</contexts>
<marker>Hirao, Isozaki, Duh, Sudoh, Tsukada, Nagao, 2011</marker>
<rawString>Tsutomu Hirao, Hideki Isozaki, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, and Masaaki Nagao. 2011. RIBES: An automatic evaluation method of translation based on rank correlation (in Japanese). In Proc. of the Annual Meeting of the Association for Natural Language Processing, pages 1115–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei — A Japanese Lexicon (in Japanese). Iwanami Shoten.</booktitle>
<contexts>
<context position="9801" citStr="Ikehara et al., 1997" startWordPosition="1610" endWordPosition="1613">fore “atarashii” does not make the sentence ambiguous. • atarashii ie ni inu wo ture te itta . ((Someone) took the dog to the new house.) • inu wo atarashii ie ni ture te itta . This idea leads to the following Simple Case Marker Constraint: Definition 1(Simple Case Marker Constraint) If a reordered sentence has a case marker phrase of a verb that precedes another verb before the verb, the sentence is rejected. “wo” case markers can precede adjectives before the verb. This is a primitive heuristic constraint and there must be better ways to make it more flexible. If we use Nihongo Goi Taikei (Ikehara et al., 1997), we will be able to implement such a flexisble constraint. For example, some verbs such as “sai-ta” (bloomed) are never modified by “wo” case marker phrases. Therefore, the following sentence is not ambiguous at all although the wo phrase precedes “sai-ta”. • hana ga sai-ta ato ni sono ki wo mi-ta. ((Someone) saw the tree after it bloomed.) • sono ki wo hana ga sai-ta ato ni mi-ta. 2.3 Evaluation with scrambled sentences As we mentioned before, RIBES measures global word order similarity between machine-translated sentences and reference sentences. It does not regard scrambling at all. When t</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei — A Japanese Lexicon (in Japanese). Iwanami Shoten.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Automatic evaluation of translation quality for distant language pairs.</title>
<date>2010</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>944--952</pages>
<contexts>
<context position="1722" citStr="Isozaki et al. (2010)" startWordPosition="232" endWordPosition="235">rom dependency trees of reference sentences. Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy. 1 Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × Pα Table 1: Meta-evaluation of NTCIR-7 JE task data (Spearman’s ρ, System-level correlation) BLEU METEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s T) is defined by (T + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, Nagata, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, and Masaaki Nagata. 2010. Automatic evaluation of translation quality for distant language pairs. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="13285" citStr="Kudo and Matsumoto, 2002" startWordPosition="2210" endWordPosition="2214"> do not reorder them. We expect that the constraint inhibit generation of incomprehensible or misleading sentences. shiroi boushi wo kuroi jon ga kaburi , zubon wo hai te iru. Figure 3: Dependency Tree of S3 Table 3: Distribution of the number of generated permutations #permutations 1 2 4 6 8 12 16 24 &gt;24 single 100 0 0 0 0 0 0 0 0 proposed 70 20 7 3 0 0 0 0 0 caseMarkers 64 23 4 6 2 2 0 2 0 postOrder 1 17 9 11 4 12 1 12 33 3 Results We applied the above four methods to the reference sentences of human-judged 100 sentences of NTCIR-7 Patent MT EJ task. (Fujii et al., 2008) We applied CaboCha (Kudo and Matsumoto, 2002) to the reference sentences, and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy (Tamura et al., 2007). To support this manual correction, CaboCha’s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX. http:// softcream.oka-pu.ac.jp/wp/wp-content/ uploads/cabochatrees.pdf. Then, it is easy to find mistakes of the dependency trees. In addition, CaboCha’s dependency accuracy is very high (89–90%) (Kudo and Matsumoto, 2002). Therefore, it took only one day to fix d</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of translation quality using longest common subsequences and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>605--612</pages>
<contexts>
<context position="2626" citStr="Lin and Och (2004)" startWordPosition="379" endWordPosition="382">ETEOR ROUGE-L IMPACT RIBES 0.515 0.490 0.903 0.826 0.947 where NKT (Normalized Kendall’s T) is defined by (T + 1)/2. This NKT is used for measuring word order similarity between a reference sentence and an MT output sentence. Thus, RIBES penalizes difference of global word order. P is precision of unigrams. RIBES is defined for each test sentence and averaged RIBES is used for evaluating the entire test corpus. Table 1 is a table in an IWSLT-2012 invited talk (http://hltc.cs.ust.hk/iwslt/slides/ Isozaki2012 slides.pdf). METEOR was proposed by Banerjee and Lavie (2005). ROUGE-L was proposed by Lin and Och (2004). According to this table, RIBES with α = 0.2 has a very strong correlation (Spearman’s p = 0.947) with human-judged adequacy. For each sentence, we use the average of adequacy scores of three judges. Here, we call this average “Adequacy”. We focus on Adequacy because current SMT systems tend to output inadequate sentences. Note that only single reference translations are available for this task although use of multiple references is common for BLEU. RIBES is publicly available from http:// www.kecl.ntt.co.jp/icl/lirg/ribes/ and was used as a standard quality measure in recent NTCIR PatentMT t</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of translation quality using longest common subsequences and skip-bigram statistics. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 605–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
</authors>
<title>Inducing a discriminative parser to optimize machine translation reordering.</title>
<date>2012</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>843--853</pages>
<contexts>
<context position="4542" citStr="Neubig et al., 2012" startWordPosition="694" endWordPosition="697">22 0.79 in order to penalize too short sentences. RIBES = NKT x Pα x BPβ where α = 0.25 and Q = 0.10. BLEU uses BP for the entire test corpus, but RIBES uses it for each sentence. • The word alignment algorithm in the original RIBES used only bigrams for disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 Methodology For instance, a Japanese sentence S1 jon ga sushi-ya de o-sushi wo tabe-ta . (John ate sushi at a sushi restaurant.) has the following acceptable word orders. 1. jon ga sushi-ya de o-sushi wo tabe-ta . 2. jon ga o-sushi wo sushi-ya de tabe-ta . 3. sushi-ya de jon ga o-sushi wo tabe-</context>
</contexts>
<marker>Neubig, Watanabe, Mori, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a discriminative parser to optimize machine translation reordering. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 843–853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1411" citStr="Papineni et al., 2002" startWordPosition="188" endWordPosition="191">wn much stronger correlation than BLEU. Now, RIBES is widely used in recent papers on Japanese-related translations. RIBES compares word order of MT output with manually translated reference sentences but it does not regard scrambling at all. In this paper, we present a method to enumerate scrambled sentences from dependency trees of reference sentences. Our experiments based on NTCIR Patent MT data show that the method improves sentence-level correlation between RIBES and human-judged adequacy. 1 Introduction Statistical Machine Translation has grown with an automatic evaluation method BLEU (Papineni et al., 2002). BLEU measures local word order by ngrams and does not care about global word order. In JE/EJ translations, this insensitivity degrades BLEU’s correlation with human judgements. Therefore, alternative automatic evaluation methods are proposed. Echizen-ya and Araki (2007) proposed IMPACT. Isozaki et al. (2010) presented the idea of RIBES. Hirao et al. (2011) named this method “RIBES” (Rank-based Intuitive Bilingual Evaluation Score). This version of RIBES was defined as follows: RIBES = NKT × Pα Table 1: Meta-evaluation of NTCIR-7 JE task data (Spearman’s ρ, System-level correlation) BLEU METE</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Japanese dependency analysis using the ancestor-descendant relation.</title>
<date>2007</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>600--609</pages>
<contexts>
<context position="13466" citStr="Tamura et al., 2007" startWordPosition="2237" endWordPosition="2240">Dependency Tree of S3 Table 3: Distribution of the number of generated permutations #permutations 1 2 4 6 8 12 16 24 &gt;24 single 100 0 0 0 0 0 0 0 0 proposed 70 20 7 3 0 0 0 0 0 caseMarkers 64 23 4 6 2 2 0 2 0 postOrder 1 17 9 11 4 12 1 12 33 3 Results We applied the above four methods to the reference sentences of human-judged 100 sentences of NTCIR-7 Patent MT EJ task. (Fujii et al., 2008) We applied CaboCha (Kudo and Matsumoto, 2002) to the reference sentences, and manually corrected the dependency trees because Japanese dependency parsers are not satisfactory in terms of sentence accuracy (Tamura et al., 2007). To support this manual correction, CaboCha’s XML output was automatically converted to dependency tree pictures by using cabochatrees package for LATEX. http:// softcream.oka-pu.ac.jp/wp/wp-content/ uploads/cabochatrees.pdf. Then, it is easy to find mistakes of the dependency trees. In addition, CaboCha’s dependency accuracy is very high (89–90%) (Kudo and Matsumoto, 2002). Therefore, it took only one day to fix dependency trees of one hundred reference sentences. Table 3 shows distribution of the number of word orders generated by the above methods. PostOrder sometimes generates tens of tho</context>
</contexts>
<marker>Tamura, Takamura, Okumura, 2007</marker>
<rawString>Akihiro Tamura, Hiroya Takamura, and Manabu Okumura. 2007. Japanese dependency analysis using the ancestor-descendant relation. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 600–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Takuya Matsuzaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Akamon: An open source toolkit for tree/forest-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>127--132</pages>
<contexts>
<context position="4521" citStr="Wu et al., 2012" startWordPosition="690" endWordPosition="693">CIR-10 EJ 0.36 0.22 0.79 in order to penalize too short sentences. RIBES = NKT x Pα x BPβ where α = 0.25 and Q = 0.10. BLEU uses BP for the entire test corpus, but RIBES uses it for each sentence. • The word alignment algorithm in the original RIBES used only bigrams for disambiguation when the same word appears twice or more in one sentence. This restriction is now removed, and longer n-grams are used to get a better alignment. RIBES is widely used in recent Annual Meeings of the (Japanese) Association for NLP. International conference papers on Japanese-related translations also use RIBES. (Wu et al., 2012; Neubig et al., 2012; Goto et al., 2012; Hayashi et al., 2013). Dan et al. (2012) uses RIBES for Chinese-to-Japanese translation. However, we have to take “scrambling” into account when we think of Japanese word order. Scrambling is also observed in other languages such as German. Current RIBES does not regard this fact. 2 Methodology For instance, a Japanese sentence S1 jon ga sushi-ya de o-sushi wo tabe-ta . (John ate sushi at a sushi restaurant.) has the following acceptable word orders. 1. jon ga sushi-ya de o-sushi wo tabe-ta . 2. jon ga o-sushi wo sushi-ya de tabe-ta . 3. sushi-ya de jo</context>
</contexts>
<marker>Wu, Matsuzaki, Tsujii, 2012</marker>
<rawString>Xianchao Wu, Takuya Matsuzaki, and Jun’ichi Tsujii. 2012. Akamon: An open source toolkit for tree/forest-based statistical machine translation. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), pages 127–132.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
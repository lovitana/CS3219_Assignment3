<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.99662">
Probabilistic Type Theory for Incremental Dialogue Processing
</title>
<author confidence="0.999255">
Julian Hough and Matthew Purver
</author>
<affiliation confidence="0.994767333333333">
Cognitive Science Research Group
School of Electronic Engineering and Computer Science
Queen Mary University of London
</affiliation>
<email confidence="0.998904">
{j.hough,m.purver}@qmul.ac.uk
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999782416666667">
We present an adaptation of recent work
on probabilistic Type Theory with Records
(Cooper et al., 2014) for the purposes of
modelling the incremental semantic pro-
cessing of dialogue participants. After
presenting the formalism and dialogue
framework, we show how probabilistic
TTR type judgements can be integrated
into the inference system of an incremen-
tal dialogue system, and discuss how this
could be used to guide parsing and dia-
logue management decisions.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996948">
While classical type theory has been the predomi-
nant mathematical framework in natural language
semantics for many years (Montague, 1974, in-
ter alia), it is only recently that probabilistic type
theory has been discussed for this purpose. Sim-
ilarly, type-theoretic representations have been
used within dialogue models (Ginzburg, 2012);
and probabilistic modelling is common in dia-
logue systems (Williams and Young, 2007, inter
alia), but combinations of the two remain scarce.
Here, we attempt to make this connection, taking
(Cooper et al., 2014)’s probabilistic Type Theory
with Records (TTR) as our principal point of de-
parture, with the aim of modelling incremental in-
ference in dialogue.
To our knowledge there has been no practi-
cal integration of probabilistic type-theoretic in-
ference into a dialogue system so far; here we dis-
cuss computationally efficient methods for imple-
mentation in an extant incremental dialogue sys-
tem. This paper demonstrates their efficacy in sim-
ple referential communication domains, but we ar-
gue the methods could be extended to larger do-
mains and additionally used for on-line learning
in future work.
</bodyText>
<sectionHeader confidence="0.998449" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999839641025641">
Type Theory with Records (TTR) (Betarte and
Tasistro, 1998; Cooper, 2005) is a rich type the-
ory which has become widely used in dialogue
models, including information state models for
a variety of phenomena such as clarification re-
quests (Ginzburg, 2012; Cooper, 2012) and non-
sentential fragments (Fern´andez, 2006). It has also
been shown to be useful for incremental semantic
parsing (Purver et al., 2011), incremental genera-
tion (Hough and Purver, 2012), and recently for
grammar induction (Eshghi et al., 2013).
While the technical details will be given in sec-
tion 3, the central judgement in type theory s : T
(that a given object s is of type T) is extended
in TTR so that s can be a (potentially complex)
record and T can be a record type – e.g. s could
represent a dialogue gameboard state and T could
be a dialogue gameboard state type (Ginzburg,
2012; Cooper, 2012). As TTR is highly flexible
with a rich type system, variants have been con-
sidered with types corresponding to real-number-
valued perceptual judgements used in conjunction
with linguistic context, such as visual perceptual
information (Larsson, 2011; Dobnik et al., 2012),
demonstrating its potential for embodied learning
systems. The possibility of integration of per-
ceptron learning (Larsson, 2011) and naive Bayes
classifiers (Cooper et al., 2014) into TTR show
how linguistic processing and probabilistic con-
ceptual inference can be treated in a uniform way
within the same representation system.
Probabilistic TTR as described by Cooper et al.
(2014) replaces the categorical s : T judgement
with the real number valued p(s : T) = v where
v ∈ [0,1]. The authors show how standard proba-
bility theoretic and Bayesian equations can be ap-
plied to TTR judgements and how an agent might
learn from experience in a simple classification
game. The agent is presented with instances of
</bodyText>
<page confidence="0.974518">
80
</page>
<note confidence="0.9917325">
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 80–88,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999706648148148">
a situation and it learns with each round by updat-
ing its set of probabilistic type judgements to best
predict the type of object in focus — in this case
updating the probability judgement that something
is an apple given its observed colour and shape
p(s ∶ Tapple ∣ s ∶ TShp, s ∶ TCol) where Shp E
{shpl, shpt} and Col E {coli, colt}. From a
cognitive modelling perspective, these judgements
can be viewed as probabilistic perceptual informa-
tion derived from learning. We use similar meth-
ods in our toy domain, but show how prior judge-
ments could be constructed efficiently, and how
classifications can be made without exhaustive it-
eration through individual type classifiers.
There has also been significant experimental
work on simple referential communication games
in psycholinguistics, computational and formal
modelling. In terms of production and genera-
tion, Levelt (1989) discusses speaker strategies
for generating referring expressions in a simple
object naming game. He showed how speakers
use informationally redundant features of the ob-
jects, violating Grice’s Maxim of Quantity. In
natural language generation (NLG), referring ex-
pression generation (REG) has been widely stud-
ied (see (Krahmer and Van Deemter, 2012) for
a comprehensive survey). The incremental algo-
rithm (IA) (Dale and Reiter, 1995) is an iterative
feature selection procedure for descriptions of ob-
jects based on computing the distractor set of ref-
erents that each adjective in a referring expression
could cause to be inferred. More recently Frank
and Goodman (2012) present a Bayesian model
of optimising referring expressions based on sur-
prisal, the information-theoretic measure of how
much descriptions reduce uncertainty about their
intended referent, a measure which they claim cor-
relates strongly to human judgements.
The element of the referring expression do-
main we discuss here is incremental processing.
There is evidence from (Brennan and Schober,
2001)’s experiments that people reason at an in-
credibly time-critical level from linguistic infor-
mation. They demonstrated self-repair can speed
up semantic processing (or at least object refer-
ence) in such games, where an incorrect object
being partly vocalized and then repaired in the
instructions (e.g. “the yell-, uh, purple square”)
yields quicker response times from the onset of
the target (“purple”) than in the case of the flu-
ent instructions (“the purple square”). This exam-
ple will be addressed in section 5. First we will
set out the framework in which we want to model
such processing.
</bodyText>
<sectionHeader confidence="0.8758805" genericHeader="method">
3 Probabilistic TTR in an incremental
dialogue framework
</sectionHeader>
<bodyText confidence="0.995938047619047">
In TTR (Cooper, 2005; Cooper, 2012), the princi-
pal logical form of interest is the record type (‘RT’
from here), consisting of sequences offields of the
form [l ∶ T ] containing a label l and a type T.1
RTs can be witnessed (i.e. judged as inhabited)
by records of that type, where a record is a set of
label-value pairs [l = v ]. The central type judge-
ment in TTR that a record s is of (record) type
R, i.e. s ∶ R, can be made from the component
type judgements of individual fields; e.g. the one-
field record [l = v ] is of type [l ∶ T ] just in case
v is of type T. This is generalisable to records and
RTs with multiple fields: a record s is of RT R if
s includes fields with labels matching those occur-
ring in the fields of R, such that all fields in R are
matched, and all matched fields in s must have a
value belonging to the type of the corresponding
field in R. Thus it is possible for s to have more
fields than R and for s ∶ R to still hold, but not
vice-versa: s ∶ R cannot hold if R has more fields
than s.
</bodyText>
<equation confidence="0.997627">
⎤⎥⎥⎥⎥⎥⎥⎥⎥⎦ R2 ∶ 1 l1 ∶ T1
l2 ∶ T2′ I R3 ∶ []
</equation>
<figureCaption confidence="0.999307">
Figure 1: Example TTR record types
</figureCaption>
<bodyText confidence="0.993796357142857">
Fields can have values representing predicate
types (ptypes), such as T3 in Figure 1, and conse-
quently fields can be dependent on fields preced-
ing them (i.e. higher) in the RT, e.g. l1 is bound in
the predicate type field l3, so l3 depends on l1.
Subtypes, meets and joins A relation between
RTs we wish to explore is 5 (‘is a subtype of’),
which can be defined for RTs in terms of fields as
simply: R1 5 R2 if for all fields [l ∶ T2 ] in R2,
R1 contains [l ∶ T1 ] where T1 5 T2. In Figure 1,
both R1 5 R3 and R2 5 R3; and R1 5 R2 iff
T2 5 T2′. The transitive nature of this relation (if
R1 5 R2 and R2 5 R3 then R1 5 R3) can be used
effectively for type-theoretic inference.
</bodyText>
<footnote confidence="0.879873">
1We only introduce the elements of TTR relevant to the
phenomena discussed below. See (Cooper, 2012) for a de-
tailed formal description.
</footnote>
<equation confidence="0.9991552">
l1 ∶ T1
l2 ∶ T2
l3 ∶ T3(l1)
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣
R1 ∶
</equation>
<page confidence="0.97951">
81
</page>
<bodyText confidence="0.9997658125">
We also assume the existence of manifest (sin-
gleton) types, e.g. Ta, the type of which only a is
a member. Here, we write manifest RT fields such
as [l : Ta ] where Ta ⊑ T using the syntactic sugar
[l=a : T ]. The subtype relation effectively allows
progressive instantiation of fields (as addition of
fields to R leads to R′ where R′ ⊑ R), which is
practically useful for an incremental dialogue sys-
tem as we will explain.
We can also define meet and join types of two
or more RTs. The representation of the meet type
of two RTs R1 and R2 is the result of a merge
operation (Larsson, 2010), which in simple terms
here can be seen as union of fields. A meet type
is also equivalent to the extraction of a maxi-
mal common subtype, an operation we will call
</bodyText>
<equation confidence="0.566321">
= MaxSub(R1, R2)
</equation>
<bodyText confidence="0.9923106">
R1 and R2 here are common supertypes of the
resulting R1 ∧ R2. On the other hand, the join of
two RTs R1 and R2, the type R1 v R2 cannot be
represented by field intersection. It is defined in
terms of type checking, in that s : R1 v R2 iff
</bodyText>
<equation confidence="0.987143">
s : R1 or s : R2. It follows that if R1 ⊑ R2 then
s : R1 ∧ R2 iff s : R1, ands: R1 v R2 iff s : R2.
</equation>
<bodyText confidence="0.9999508">
While technically the maximally common su-
pertype of R1 and R2 is the join type R1 v R2,
here we introduce the maximally common simple
(non disjunctive) supertype of two RTs R1 and R2
as field intersection:
</bodyText>
<equation confidence="0.9875425">
if R1 = [ l1 : T1
l2 : T2 I and R2 = r l2 : T2 ]
L l3 : T3
MaxSuper(R1, R2) = [ l2 : T2 ]
</equation>
<bodyText confidence="0.9998885">
We will explore the usefulness of this new op-
eration in terms of RT lattices in sec. 4.
</bodyText>
<subsectionHeader confidence="0.998089">
3.1 Probabilistic TTR
</subsectionHeader>
<bodyText confidence="0.9147965">
We follow Cooper et al. (2014)’s recent extension
of TTR to include probabilistic type judgements of
the form p(s : R) = v where v ∈ [0,1], i.e. the real
valued judgement that a record s is of RT R. Here
2Here we concern ourselves with simple examples that
avoid label-type clashes between two RTs (i.e. where R1 con-
tains l1 : T1 and R2 contains l1 : T2); in these cases the op-
erations are more complex than field concatenation/sharing.
we use probabilistic TTR to model a common psy-
cholinguistic experimental set up in section 5. We
repeat some of Cooper et al.’s calculations here
for exposition, but demonstrate efficient graphical
methods for generating and incrementally retriev-
ing probabilities in section 4.
Cooper et al. (2014) define the probability of the
meet and join types of two RTs as follows:
</bodyText>
<equation confidence="0.999734333333333">
p(s : R1 ∧ R2) = p(s : R1)p(s : R2 ∣ s : R1)
p(s : R1 v R2) = p(s : R1) + p(s : R2) − p(s : R1 ∧ R2)
(1)
</equation>
<bodyText confidence="0.972433333333333">
It is practically useful, as we will describe be-
low, that the join probability can be computed in
terms of the meet. Also, there are equivalences be-
tween meets, joins and subtypes in terms of type
judgements as described above, in that assuming
if R1 ⊑ R2 then p(s : R2 ∣ s : R1) = 1, we have:
</bodyText>
<equation confidence="0.97100925">
if R1 ⊑ R2
p(s : R1 ∧ R2) = p(s : R1)
p(s : R1 v R2) = p(s : R2)
p(s : R1) ≤ p(s : R2)
</equation>
<bodyText confidence="0.9382025">
The conditional probability of a record being of
type R2 given it is of type R1 is:
</bodyText>
<equation confidence="0.999604">
p(s : R2 ∣ s : R1) = p(s : R1 ∧ s :R2) (3)
p(s : R1)
</equation>
<bodyText confidence="0.991956266666667">
We return to an explanation for these classical
probability equations holding within probabilistic
TTR in section 4.
Learning and storing probabilistic judgements
When dealing with referring expression games, or
indeed any language game, we need a way of stor-
ing perceptual experience. In probabilistic TTR
this can be achieved by positing a judgement set J
in which an agent stores probabilistic type judge-
ments.3 We refer to the sum of the value of proba-
bilistic judgements that a situation has been judged
to be of type Ri within J as ∥Ri∥J and the sum of
all probabilistic judgements in J simply as P(J);
thus the prior probability that anything is of type
Ri under the set of judgements J is ∥Ri∥J
</bodyText>
<equation confidence="0.539847">
P (J) . The
</equation>
<bodyText confidence="0.901731333333333">
conditional probability p(s : R1 ∣ s : R2) un-
der J can be reformulated in terms of these sets
of judgements:
</bodyText>
<equation confidence="0.967894">
∥R1∧R2∥J iff ∥R211J * 0
pJ(s : R1 ∣ s : R2) = ∥R2∥J w(4)
0 otherwise
</equation>
<bodyText confidence="0.430429666666667">
3(Cooper et al., 2014) characterise a type judgement as an
Austinian proposition that a situation is of a given type with
a given probability, encoded in a TTR record.
</bodyText>
<equation confidence="0.992985222222222">
MaxSub(Ri..Rn):2
if R ll : T1 and R 12 : T2
1 = l2 : T2 2 = l3 : T3 �
R1 ∧ R2 = ⎡⎢⎢⎢⎢⎢⎢⎢⎣
l1 : T1 ⎦
l2 : T2
l3
: T3
(2)
</equation>
<page confidence="0.943543">
82
</page>
<bodyText confidence="0.9999908">
where the sample spaces I IR1 ∧ R2I IS and I IR2I IS
constitute the observations of the agent so far. J
can have new judgements added to it during learn-
ing. We return to this after introducing the incre-
mental semantics needed to interface therewith.
</bodyText>
<subsectionHeader confidence="0.98395">
3.2 DS-TTR and the DyLan dialogue system
</subsectionHeader>
<bodyText confidence="0.998938727272727">
In order to permit type-theoretic inference in a
dialogue system, we need to provide suitable
TTR representations for utterances and the cur-
rent pragmatic situation from a parser, dialogue
manager and generator as instantaneously and ac-
curately as possible. For this purpose we use
an incremental framework DS-TTR (Eshghi et
al., 2013; Purver et al., 2011) which integrates
TTR representations with the inherently incre-
mental grammar formalism Dynamic Syntax (DS)
(Kempson et al., 2001).
</bodyText>
<figure confidence="0.9469955">
O, Ty(t), ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x=john e ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
e=arrive es
p=subj(e,x) t
head=p t
</figure>
<figureCaption confidence="0.999788">
Figure 2: DS-TTR tree
</figureCaption>
<bodyText confidence="0.999648333333334">
DS produces an incrementally specified, partial
logical tree as words are parsed/generated; follow-
ing Purver et al. (2011), DS tree nodes are dec-
orated not with simple atomic formulae but with
RTs, and corresponding lambda abstracts repre-
senting functions of type RT → RT (e.g. Ar
[l1 T1 ].[l2=r.l1 T1 ] where r.l1 is a path ex-
pression referring to the label l1 in r) – see Fig-
ure 2. Using the idea of manifestness of fields
as mentioned above, we have a natural represen-
tation for underspecification of leaf node content,
e.g. [x e] is unmanifest whereas [x=john e]4
is manifest and the latter is a subtype of the for-
mer. Functional application can apply incremen-
tally, allowing a RT at the root node to be com-
piled for any partial tree, which is incrementally
further specified as parsing proceeds (Hough and
Purver, 2012). Within a given parse path, due to
</bodyText>
<footnote confidence="0.998998">
4This is syntactic sugar for [ x ejohn ] and the = sign is
not the same semantically as that in a record.
</footnote>
<bodyText confidence="0.954489885714286">
DS-TTR’s monotonicity, each maximal RT of the
tree’s root node is a subtype of the parser’s previ-
ous maximal output.
Following (Eshghi et al., 2013), DS-TTR tree
nodes include a field head in all RTs which cor-
responds to the DS tree node type. We also as-
sume a neo-Davidsonian representation of predi-
cates, with fields corresponding to an event term
and to each semantic role; this allows all available
semantic information to be specified incrementally
in a strict subtyping relation (e.g. providing the
subj() field when subject but not object has been
parsed) – see Figure 2.
We implement DS-TTR parsing and genera-
tion mechanisms in the DyLan dialogue system5
within Jindigo (Skantze and Hjalmarsson, 2010),
a Java-based implementation of the incremental
unit (IU) framework of (Schlangen and Skantze,
2009). In this framework, each module has input
and output IUs which can be added as edges be-
tween vertices in module buffer graphs, and be-
come committed should the appropriate condi-
tions be fulfilled, a notion which becomes im-
portant in light of hypothesis change and repair
situations. Dependency relations between differ-
ent graphs within and between modules can be
specified by groundedIn links (see (Schlangen and
Skantze, 2009) for details).
The DyLan interpreter module (Purver et al.,
2011) uses Sato (2011)’s insight that the context of
DS parsing can be characterized in terms of a Di-
rected Acyclic Graph (DAG) with trees for nodes
and DS actions for edges. The module’s state is
characterized by three linked graphs as shown in
Figure 3:
</bodyText>
<listItem confidence="0.988738272727273">
• input: a time-linear word graph posted by the
ASR module, consisting of word hypothesis
edge IUs between vertices Wn
• processing: the internal DS parsing DAG,
which adds parse state edge IUs between ver-
tices Sn groundedIn the corresponding word
hypothesis edge IU
• output: a concept graph consisting of domain
concept IUs (RTs) as edges between vertices
Cn, groundedIn the corresponding path in the
DS parsing DAG
</listItem>
<bodyText confidence="0.9878305">
Here, our interest is principally in the parser out-
put, to support incremental inference; a DS-TTR
generator is also included which uses RTs as goal
concepts (Hough and Purver, 2012) and uses the
</bodyText>
<footnote confidence="0.974382">
5Available from http://dylan.sourceforge.net/
</footnote>
<figure confidence="0.8334835">
Ty(e),
�x=john e
head=x e �
Ty(e → t),
Ar [ head e ] .
x=r.head e
e=arrive es ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
p=subj(e,x) t
head=p t
⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
</figure>
<page confidence="0.990671">
83
</page>
<bodyText confidence="0.996210333333333">
same parse graph as the interpreter to allow self-
monitoring and compound contributions, but we
omit the details here.
</bodyText>
<figureCaption confidence="0.981989">
Figure 3: Normal incremental parsing in Dylan
</figureCaption>
<sectionHeader confidence="0.9271885" genericHeader="method">
4 Order theoretic and graphical methods
for probabilistic TTR
</sectionHeader>
<bodyText confidence="0.999674612903226">
RT lattices to encode domain knowledge To
support efficient inference in DyLan, we represent
dialogue domain concepts via partially ordered
sets (posets) of RT judgements, following similar
insights used in inducing DS-TTR actions (Eshghi
et al., 2013). A poset has several advantages over
an unordered list of un-decomposed record types:
the possibility of incremental type-checking; in-
creased speed of type-checking, particularly for
pairs of/multiple type judgements; immediate use
of type judgements to guide system decisions; in-
ference from negation; and the inclusion of learn-
ing within a domain. We leave the final challenge
for future work, but discuss the others here.
We can construct a poset of type judgements
for any single RT by decomposing it into its con-
stituent supertype judgements in a record type lat-
tice. Representationally, as per set-theoretic lat-
tices, this can be visualised as a Hasse diagram
such as Figure 4, however here the ordering arrows
show 5 (‘subtype of’) relations from descendant to
ancestor nodes.
To characterize an RT lattice G ordered by 5,
we adapt Knuth (2005)’s description of lattices in
line with standard order theory: for a pair of RT
elements Rx and Ry, their lower bound is the set
of all Rz E G such that Rz 5 Rx and Rz 5 Ry.
In the event that a unique greatest lower bound ex-
ists, this is their meet, which in G happily corre-
sponds to the TTR meet type Rx ∧ Ry. Dually, if
their unique least upper bound exists, this is their
</bodyText>
<equation confidence="0.88277">
R1200 = [] = T
R120 = I a : b ] R121 = [ c : d ] R110 = [ e : f ]
R10 = [a:b
c: d J R11 = [a:b
e : f J R12 = [c:d
e : f J
a: b
c : d ⎤⎥⎥⎥⎥⎥⎥⎥⎦
e: f
</equation>
<figureCaption confidence="0.962075">
Figure 4: Record Type lattice ordered by the sub-
type relation
</figureCaption>
<bodyText confidence="0.99872558974359">
join and in TTR terms is Max5uper(Rx, Ry) but
not necessarily their join type Rx v Ry as here
we concern ourselves with simple RTs. One el-
ement covers another if it is a direct successor to
it in the subtype ordering relation hierarchy. G
has a greatest element (T) and least element (1),
with the atoms being the elements that cover 1;
in Figure 4 if R1 is viewed as 1 , the atoms are
R{10,11,12}. An RT element Rx has a comple-
ment if there is a unique element -,Rx such that
Max5uper(Rx, -,Rx) = T and Rx ∧ -,Rx = 1
(the lattice in Figure 4 is complemented as this
holds for every element).
Graphically, the join of two elements can be
found by following the connecting edges upward
until they first converge on a single RT, giving us
Max5uper(R10, R12) = R121 in Figure 4, and the
meet can be found by following the lines down-
ward until they connect to give their meet type,
i.e. R10 ∧ R12 = R1.
If we consider R1 to be a domain concept in
a dialogue system, we can see how its RT lattice
G can be used for incremental inference. As in-
crementally specified RTs become available from
the interpreter they are matched to those in G to
determine how far down towards the final domain
concept R1 our current state allows us to be. Dif-
ferent sequences of words/utterances lead to dif-
ferent paths. However, any practical dialogue sys-
tem must entertain more than one possible domain
concept as an outcome; G must therefore contain
multiple possible final concepts, constituting its
atoms, each with several possible dialogue move
sequences, which correspond to possible down-
ward paths – e.g. see the structure of Figure 5.
Our aim here is to associate each RT in G with a
probabilistic judgement.
Initial lattice construction We define a simple
bottom-up procedure in Algorithm 1 to build a RT
</bodyText>
<equation confidence="0.94897">
⎡⎢⎢⎢⎢⎢⎢⎢⎣
R1 =
= 1
</equation>
<page confidence="0.988735">
84
</page>
<bodyText confidence="0.999305545454546">
lattice G of all possible simple domain RTs and
their prior probabilistic judgements, initialised by
the disjunction of possible final state judgements
(the priors),6 along with the absurdity ⊥, stipu-
lated a priori as the least element with probability
0 and the meet type of the atomic priors. The al-
gorithm recursively removes one field from the RT
being processed at a time (except fields referenced
in a remaining dependent ptype field), then orders
the new supertype RT in G appropriately.
Each node in G contains its RT Ri and a sum
of probability judgements {∥Rk∥J + .. + ∥Rn∥J}
corresponding to the probabilities of the priors it
stands in a supertype relation to. These sums are
propagated up from child to parent node as it is
constructed. It terminates when all simple maxi-
mal supertypes7 have been processed, leaving the
maximally common supertype as ⊤ (possibly the
empty type [ ]), associated with the entire proba-
bility mass P(J), which constitutes the denomina-
tor to all judgements- given this, only the numer-
ator of equation ∥Ri∥J
</bodyText>
<figure confidence="0.936449909090909">
P (J) needs to be stored at each
node.
Algorithm 1 Probabilistic TTR record type lattice
construction algorithm
INPUT: priors D use the initial prior judgements for G’s atoms
OUTPUT: G
G = newGraph(priors) D P(J) set to equal sum of prior probs
agenda = priors D Initialise agenda
while not agenda is empty do
RT = agenda.pop()
for field ∈ RT do
if field ∈ RT.paths then D Do not remove bound fields
continue
superRT = RT - field
if superRT ∈ G then D not new? order w.r.t. RT and inherit RT’s priors
G.order(RT.address,G.getNode(superRT),⊑)
else D new?
superNode = G.newNode(superRT) D create new node w. empty priors
for node ∈ G do D order superNode w.r.t. other nodes in G
if superRT.fields ⊂ node.fields then
G.order(node,superNode,⊑) D superNode inherits node’s priors
agenda.append(superRT) D add to agenda for further supertyping
</figure>
<bodyText confidence="0.9890946">
Direct inference from the lattice To explain
how our approach models incremental inference,
we assume Brennan and Schober (2001)’s experi-
mental referring game domain described in section
2: three distinct domain situation RTs R1, R2 and
R3 correspond to a purple square, a yellow square
and a yellow circle, respectively.
The RT lattice G constructed initially upon ob-
servation of the game (by instructor or instructee)
shown in Figure 5 uses a uniform distribution for
</bodyText>
<footnote confidence="0.700966">
6Although the priors’ disjunctive probability sums to 1 af-
ter G is constructed, i.e. in Figure 5 ∥R1∥J+∥R2∥J +∥R3∥J = 1,
P (J)
the real values initially assigned to them need not sum to
unity, as they form the atoms of G (see (Knuth, 2005)).
7Note that it does not generate the join types but maximal
common supertypes defined by field intersection.
</footnote>
<bodyText confidence="0.996030642857143">
the three disjunctive final situations. Each node
shows an RT Ri on the left and the derivation of
its prior probability pJ(Ri) that any game situa-
tion record will be of type Ri on the right, purely
in terms of the relevant priors and the global de-
nominator P(J).
G can be searched to make inferences in light
of partial information from an ongoing utterance.
We model inference as predicting the likelihood
of relevant type judgements Ry ∈ G of a situa-
tion s, given the judgement s : Rx we have so far.
To do this we use conditional probability judge-
ments following Knuth’s work on distributive lat-
tices, using the ⊑ relation to give a choice function:
</bodyText>
<equation confidence="0.853152666666667">
⎧ ⎪⎪⎪⎪1 if Rx ⊑ Ry
pJ(s : Ry ∣ s : Rx) = ⎨ 0 if Rx ∧ Ry = ⊥
⎪⎪⎪⎪ ⎩p otherwise, where 0 ≤ p ≤ 1
</equation>
<bodyText confidence="0.976889125">
The third case is the degree of inclusion of Ry
in Rx, and can be calculated using the conditional
probability calculation (4) in sec. 3. For nega-
tive RTs, a lattice generated from Algorithm 1 will
be distributive but not guaranteed to be comple-
mented, however we can still derive pJ(s : Ry ∣
s : _Rx) by obtaining pJ(s : Ry) in G modulo the
probability mass of Rx and that of its subtypes:
</bodyText>
<equation confidence="0.9982016">
(0 if Ry ⊑ Rx
pJ(s : Ry ∣ s : _Rx) = jIPJ (s:Ry)−p J(s:Rx∧Ry)
otherwise
pJ (s:⊤)−pJ (s:Rx)
(6)
</equation>
<bodyText confidence="0.999141363636364">
The subtype relations and atomic, join and meet
types’ probabilities required for (1) - (6) can be
calculated efficiently through graphical search al-
gorithms by characterising G as a DAG: the re-
verse direction of the subtype ordering edges can
be viewed as reachability edges, making ⊤ the
source and ⊥ the sink. With this characterisation,
if Rx is reachable from Ry then Rx ⊑ Ry.
In DAG terms, the probability of the meet of
two RTs Rx and Ry can be found at their highest
common descendant node – e.g. pJ(R4 ∧ R5) in
Figure 5 can be found as 13 directly at R1. Note if
Rx is reachable from Ry, i.e. Rx ⊑ Ry, then due
to the equivalences listed in (2), pJ(Rx ∧ Ry) can
be found directly at Rx. If the meet of two nodes
is ⊥ (e.g. R4 and R3 in Figure 5), then their meet
probability is 0 as pJ(⊥)=0.
While the lattice does not have direct access to
the join types of its elements, a join type prob-
ability pJ(Rx v Ry) can be calculated in terms
of pJ(Rx ∧ Ry) by the join equation in (1),
which holds for all probabilistic distributive lat-
</bodyText>
<figure confidence="0.487299">
(5)
</figure>
<page confidence="0.969327">
85
</page>
<figureCaption confidence="0.996032">
Figure 5: Record type lattice with initial uniform prior probablities
</figureCaption>
<equation confidence="0.996541035714286">
R8 = [ x ∶ind ] ∥R1∥J +∥R2∥J +∥R3∥J = T = 1
P (J)
PRIORS:
∥R1∥J = 1 3
∥R2∥J = 1 3
∥R3∥J = 13
⎡⎢⎢⎢⎢⎢⎢⎢⎣
∥R3∥J
P(J)
x ∶ ind ⎤⎥⎥
coly ∶ yellow(x) ⎥⎥⎥⎥⎥⎦
shpc ∶ circle(x)
∥R2∥J R3 =
P(J)
x ∶ ind∥R1∥J +∥R2∥J x ∶ ind ∥R1∥J x ∶ ind ∥R2∥J +∥R3∥J x ∶ ind ∥R3∥J
R4 = shpsq ∶ square(x) P(J) R5 = colp ∶ purple(x) P(J) R6 = coly ∶ yell P(J) Rq = shpc ∶ circl e(x) P(J)
x ∶ ind ⎤⎥⎥
colp ∶ purple(x) ⎥⎥⎥⎥⎥⎦
shpsq ∶ square(x)
R0 = 1 = 0
x ∶ ind ⎤⎥⎥
coly ∶ yellow(x) ⎥⎥⎥⎥⎥⎦
shpsq ∶ square(x)
⎡⎢⎢⎢⎢⎢⎢⎢⎣
R1 =
∥R1∥J R2 =
P(J)
⎡⎢⎢⎢⎢⎢⎢⎢⎣
</equation>
<bodyText confidence="0.999465166666667">
tices (Knuth, 2005).8 As regards efficiency, worst
case complexity for finding the meet probability at
the common descendant of Rx and Ry is a linear
O(m + n) where m and n are the number of edges
in the downward (possibly forked) paths Rx -+ 1
and Ry -+ 1.9
</bodyText>
<sectionHeader confidence="0.789718" genericHeader="method">
5 Simulating incremental inference and
self-repair processing
</sectionHeader>
<bodyText confidence="0.981621540983607">
Interpretation in DyLan and its interface to the
RT lattice G follows evidence that dialogue agents
parse self-repairs efficiently and that repaired di-
alogue content (reparanda) is given special sta-
tus but not removed from the discourse context.
To model Brennan and Schober (2001)’s findings
of disfluent spoken instructions speeding up ob-
ject recognition (see section 2), we demonstrate
a self-repair parse in Figure 6 for “The yell-, uh,
purple square” in the simple game of predicting
the final situation from {R1, R2, R3} continuously
given the type judgements made so far. We de-
scribe the stages T1-T4 in terms of the current
word being processed- see Figure 6:
At T1:‘the’ the interpreter will not yield a sub-
type checkable in G so we can only condition on
R8 (T), giving us pJ(s ∶ Ri ∣ s ∶ R8) = 13 for
i ∈ {1, 2, 3}, equivalent to the priors. At T2:
8The search for the meet probability is generalisable to
conjunctive types by searching for the conjuncts’ highest
common descendant. The join probability is generalisable to
the disjunctive probability of multiple types, used, albeit pro-
gramatically, in Algorithm 1 for calculating a node’s proba-
bility from its child nodes.
9While we do not give details here, simple graphical
search algorithms for conjunctive and disjunctive multiple
types are linear in the number of conjuncts and disjuncts, sav-
ing considerable time in comparison to the algebraic calcula-
tions of the sum and product rules for distributive lattices.
‘yell-’, the best partial word hypothesis is now
“yellow”;10 the interpreter therefore outputs an RT
which matches the type judgement s ∶ R6 (i.e. that
the object is a yellow object). Taking this judge-
ment as the conditioning evidence using function
(5) we get pJ(s ∶ R1 ∣ s ∶ R6) = 0 and us-
ing (4) we get pJ(s ∶ R2 ∣ s ∶ R6) = 0.5 and
pJ(s ∶ R3 ∣ s ∶ R6) = 0.5 (see the schematic
probability distribution at stage T2 in Figure 6 for
the three objects). The meet type probabilities
required for the conditional probabilities can be
found graphically as described above.
At T3:‘uh purple’, low probability in the in-
terpreter output causes a self-repair to be recog-
nised, enforcing backtracking on the parse graph
which informally operates as follows (see Hough
and Purver (2012)) :
Self-repair:
IF from parsing word W the edge 5En is in-
sufficiently likely to be constructed from ver-
tex 5n OR IF there is no sufficiently likely
judgement p(s ∶ Rx) for Rx ∈ G
THEN parse word W from vertex 5n−1. IF
successful add a new edge to the top path,
without removing any committed edges be-
ginning at 5n−1; ELSE set n=n−1 and repeat.
This algorithm is consistent with a local model
for self-repair backtracking found in corpora
(Shriberg and Stolcke, 1998; Hough and Purver,
2013). As regards inference in G, upon detection
of a self-repair that revokes s ∶ R6, the type judge-
ment s ∶ ¬R6, i.e. that this is not a yellow object,
</bodyText>
<footnote confidence="0.924955">
10In practice, ASR modules yielding partial results are less
reliable than their non-incremental counterparts, but progress
is being made here (Schlangen and Skantze, 2009).
</footnote>
<page confidence="0.995609">
86
</page>
<figureCaption confidence="0.998705">
Figure 6: Incremental DS-TTR self-repair parsing. Inter-graph groundedIn links go top to bottom.
</figureCaption>
<bodyText confidence="0.999402">
is immediately available as conditioning evidence.
Using (6) our distribution of RT judgements now
shifts: pJ(s ∶ R1 I s ∶ ¬R6) = 1, pJ(s ∶ R2 I
s ∶ ¬R6) = 0 and pJ(s ∶ R3 I s ∶ ¬R6) = 0 be-
fore “purple” has been parsed – thus providing a
probabilistic explanation for increased subsequent
processing speed. Finally at T4: ‘square’ given
pJ(s ∶ R1 I s ∶ R1) = 1 and R1∧R2 = R1∧R3 = ⊥,
the distribution remains unchanged.
The system’s processing models how listen-
ers reason about the revocation itself rather than
predicting the outcome through positive evidence
alone, in line with (Brennan and Schober, 2001)’s
results.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="method">
6 Extensions
</sectionHeader>
<bodyText confidence="0.999457666666667">
Dialogue and self-repair in the wild To move
towards domain-generality, generating the lattice
of all possible dialogue situations for interesting
domains is computationally intractable. We in-
tend instead to consider incrementally occurring
issues that can be modelled as questions (Lars-
son, 2002). Given one or more issues manifest in
the dialogue at any time, it is plausible to gener-
ate small lattices dynamically to estimate possible
answers, and also assign a real-valued relevance
measure to questions that can be asked to resolve
the issues. We are exploring how this could be
implemented using the inquiry calculus (Knuth,
2005), which defines information theoretic rele-
vance in terms of a probabilistic question lattice,
and furthermore how this could be used to model
the cause of self-repair as a time critical trade-off
between relevance and accuracy.
Learning in a dialogue While not our focus
here, lattice G’s probabilities can be updated
through observations after its initial construction.
If a reference game is played over several rounds,
the choice of referring expression can change
based on mutually salient functions from words
to situations- see e.g. (DeVault and Stone, 2009).
Our currently frequentist approach to learning is:
given an observation of an existing RT Ri is made
with probability v, then I IRiI IJ, the overall denom-
inator P(J) , and the nodes in the upward path
from Ri to ⊤ are incremented by v. The approach
could be converted to Bayesian update learning by
using the prior probabilities in G for calculating v
before it is added. Furthermore, observations can
be added to G that include novel RTs: due to the
DAG structure of G, their subtype ordering and
probability effects can be integrated efficiently.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999965375">
We have discussed efficient methods for construct-
ing probabilistic TTR domain concept lattices or-
dered by the subtype relation and their use in
incremental dialogue frameworks, demonstrating
their efficacy for realistic self-repair processing.
We wish to explore inclusion of join types, the
scalability of RT lattices to other domains and
their learning capacity in future work.
</bodyText>
<sectionHeader confidence="0.997762" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.974571">
We thank the two TTNLS reviewers for their com-
ments. Purver is supported in part by the European
Community’s Seventh Framework Programme un-
der grant agreement no 611733 (ConCreTe).
</bodyText>
<page confidence="0.99856">
87
</page>
<sectionHeader confidence="0.998343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999753605769231">
G. Betarte and A. Tasistro. 1998. Extension of Martin-
L¨of type theory with record types and subtyping. In
G. Sambin and J. Smith, editors, 25 Years of Con-
structive Type Theory. Oxford University Press.
S. Brennan and M. Schober. 2001. How listeners
compensate for disfluencies in spontaneous speech.
Journal ofMemory and Language, 44(2):274–296.
R. Cooper, S. Dobnik, S. Lappin, and S. Larsson. 2014.
A probabilistic rich type theory for semantic inter-
pretation. In Proceedings of the EACL Workshop
on Type Theory and Natural Language Semantics
(TTNLS).
R. Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation,
15(2):99–112.
R. Cooper. 2012. Type theory and semantics in flux.
In R. Kempson, N. Asher, and T. Fernando, edi-
tors, Handbook of the Philosophy of Science, vol-
ume 14: Philosophy of Linguistics, pages 271–323.
North Holland.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the gricean maxims in the generation of re-
ferring expressions. Cognitive Science, 19(2):233–
263.
D. DeVault and M. Stone. 2009. Learning to interpret
utterances using dialogue history. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL).
S. Dobnik, R. Cooper, and S. Larsson. 2012. Mod-
elling language, action, and perception in type the-
ory with records. In Proceedings of the 7th Inter-
national Workshop on Constraint Solving and Lan-
guage Processing (CSLP12).
A. Eshghi, J. Hough, and M. Purver. 2013. Incre-
mental grammar induction from child-directed di-
alogue utterances. In Proceedings of the 4th An-
nual Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL).
R. Fern´andez. 2006. Non-Sentential Utterances in Di-
alogue: Classification, Resolution and Use. Ph.D.
thesis, King’s College London, University of Lon-
don.
M. C. Frank and N. D. Goodman. 2012. Predicting
pragmatic reasoning in language games. Science,
336(6084):998–998.
J. Ginzburg. 2012. The Interactive Stance: Meaning
for Conversation. Oxford University Press.
J. Hough and M. Purver. 2012. Processing self-repairs
in an incremental type-theoretic dialogue system. In
Proceedings of the 16th SemDial Workshop on the
Semantics and Pragmatics ofDialogue (SeineDial).
J. Hough and M. Purver. 2013. Modelling expectation
in the self-repair processing of annotat-, um, listen-
ers. In Proceedings of the 17th SemDial Workshop
on the Semantics and Pragmatics of Dialogue (Di-
alDam).
R. Kempson, W. Meyer-Viol, and D. Gabbay. 2001.
Dynamic Syntax: The Flow of Language Under-
standing. Blackwell.
K. H. Knuth. 2005. Lattice duality: The origin of prob-
ability and entropy. Neurocomputing, 67:245–274.
E. Krahmer and K. Van Deemter. 2012. Computa-
tional generation of referring expressions: A survey.
Computational Linguistics, 38(1):173–218.
S. Larsson. 2002. Issue-based Dialogue Management.
Ph.D. thesis, G¨oteborg University. Also published
as Gothenburg Monographs in Linguistics 21.
S. Larsson. 2010. Accommodating innovative mean-
ing in dialogue. Proc. of Londial, SemDial Work-
shop, pages 83–90.
S. Larsson. 2011. The TTR perceptron: Dynamic
perceptual meanings and semantic coordination. In
Proceedings of the 15th Workshop on the Semantics
and Pragmatics of Dialogue (SemDial 2011 - Los
Angelogue).
W. Levelt. 1989. Speaking: From Intention to Articu-
lation. MIT Press.
R. Montague. 1974. Formal Philosophy: Selected Pa-
pers of Richard Montague. Yale University Press.
M. Purver, A. Eshghi, and J. Hough. 2011. Incremen-
tal semantic construction in a dialogue system. In
J. Bos and S. Pulman, editors, Proceedings of the
9th International Conference on Computational Se-
mantics.
Y. Sato. 2011. Local ambiguity, search strate-
gies and parsing in Dynamic Syntax. In E. Gre-
goromichelaki, R. Kempson, and C. Howes, editors,
The Dynamics of Lexical Interfaces. CSLI Publica-
tions.
D. Schlangen and G. Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009).
E. Shriberg and A. Stolcke. 1998. How far do speakers
back up in repairs? A quantitative model. In Pro-
ceedings of the International Conference on Spoken
Language Processing.
G. Skantze and A. Hjalmarsson. 2010. Towards incre-
mental speech generation in dialogue systems. In
Proceedings of the SIGDIAL 2010 Conference.
J. Williams and S. Young. 2007. Scaling POMDPs
for spoken dialog management. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2116–2129.
</reference>
<page confidence="0.999402">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858669">
<title confidence="0.999944">Probabilistic Type Theory for Incremental Dialogue Processing</title>
<author confidence="0.998652">Julian Hough</author>
<author confidence="0.998652">Matthew</author>
<affiliation confidence="0.956411666666667">Cognitive Science Research School of Electronic Engineering and Computer Queen Mary University of</affiliation>
<abstract confidence="0.998865538461538">We present an adaptation of recent work on probabilistic Type Theory with Records (Cooper et al., 2014) for the purposes of modelling the incremental semantic processing of dialogue participants. After presenting the formalism and dialogue framework, we show how probabilistic TTR type judgements can be integrated into the inference system of an incremental dialogue system, and discuss how this could be used to guide parsing and dialogue management decisions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Betarte</author>
<author>A Tasistro</author>
</authors>
<title>Extension of MartinL¨of type theory with record types and subtyping.</title>
<date>1998</date>
<editor>In G. Sambin and J. Smith, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1943" citStr="Betarte and Tasistro, 1998" startWordPosition="289" endWordPosition="292">ds (TTR) as our principal point of departure, with the aim of modelling incremental inference in dialogue. To our knowledge there has been no practical integration of probabilistic type-theoretic inference into a dialogue system so far; here we discuss computationally efficient methods for implementation in an extant incremental dialogue system. This paper demonstrates their efficacy in simple referential communication domains, but we argue the methods could be extended to larger domains and additionally used for on-line learning in future work. 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is ext</context>
</contexts>
<marker>Betarte, Tasistro, 1998</marker>
<rawString>G. Betarte and A. Tasistro. 1998. Extension of MartinL¨of type theory with record types and subtyping. In G. Sambin and J. Smith, editors, 25 Years of Constructive Type Theory. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brennan</author>
<author>M Schober</author>
</authors>
<title>How listeners compensate for disfluencies in spontaneous speech.</title>
<date>2001</date>
<journal>Journal ofMemory and Language,</journal>
<volume>44</volume>
<issue>2</issue>
<contexts>
<context position="5887" citStr="Brennan and Schober, 2001" startWordPosition="918" endWordPosition="921">erative feature selection procedure for descriptions of objects based on computing the distractor set of referents that each adjective in a referring expression could cause to be inferred. More recently Frank and Goodman (2012) present a Bayesian model of optimising referring expressions based on surprisal, the information-theoretic measure of how much descriptions reduce uncertainty about their intended referent, a measure which they claim correlates strongly to human judgements. The element of the referring expression domain we discuss here is incremental processing. There is evidence from (Brennan and Schober, 2001)’s experiments that people reason at an incredibly time-critical level from linguistic information. They demonstrated self-repair can speed up semantic processing (or at least object reference) in such games, where an incorrect object being partly vocalized and then repaired in the instructions (e.g. “the yell-, uh, purple square”) yields quicker response times from the onset of the target (“purple”) than in the case of the fluent instructions (“the purple square”). This example will be addressed in section 5. First we will set out the framework in which we want to model such processing. 3 Pro</context>
<context position="22563" citStr="Brennan and Schober (2001)" startWordPosition="4016" endWordPosition="4019">RT do if field ∈ RT.paths then D Do not remove bound fields continue superRT = RT - field if superRT ∈ G then D not new? order w.r.t. RT and inherit RT’s priors G.order(RT.address,G.getNode(superRT),⊑) else D new? superNode = G.newNode(superRT) D create new node w. empty priors for node ∈ G do D order superNode w.r.t. other nodes in G if superRT.fields ⊂ node.fields then G.order(node,superNode,⊑) D superNode inherits node’s priors agenda.append(superRT) D add to agenda for further supertyping Direct inference from the lattice To explain how our approach models incremental inference, we assume Brennan and Schober (2001)’s experimental referring game domain described in section 2: three distinct domain situation RTs R1, R2 and R3 correspond to a purple square, a yellow square and a yellow circle, respectively. The RT lattice G constructed initially upon observation of the game (by instructor or instructee) shown in Figure 5 uses a uniform distribution for 6Although the priors’ disjunctive probability sums to 1 after G is constructed, i.e. in Figure 5 ∥R1∥J+∥R2∥J +∥R3∥J = 1, P (J) the real values initially assigned to them need not sum to unity, as they form the atoms of G (see (Knuth, 2005)). 7Note that it do</context>
<context position="26725" citStr="Brennan and Schober (2001)" startWordPosition="4816" endWordPosition="4819">1∥J R2 = P(J) ⎡⎢⎢⎢⎢⎢⎢⎢⎣ tices (Knuth, 2005).8 As regards efficiency, worst case complexity for finding the meet probability at the common descendant of Rx and Ry is a linear O(m + n) where m and n are the number of edges in the downward (possibly forked) paths Rx -+ 1 and Ry -+ 1.9 5 Simulating incremental inference and self-repair processing Interpretation in DyLan and its interface to the RT lattice G follows evidence that dialogue agents parse self-repairs efficiently and that repaired dialogue content (reparanda) is given special status but not removed from the discourse context. To model Brennan and Schober (2001)’s findings of disfluent spoken instructions speeding up object recognition (see section 2), we demonstrate a self-repair parse in Figure 6 for “The yell-, uh, purple square” in the simple game of predicting the final situation from {R1, R2, R3} continuously given the type judgements made so far. We describe the stages T1-T4 in terms of the current word being processed- see Figure 6: At T1:‘the’ the interpreter will not yield a subtype checkable in G so we can only condition on R8 (T), giving us pJ(s ∶ Ri ∣ s ∶ R8) = 13 for i ∈ {1, 2, 3}, equivalent to the priors. At T2: 8The search for the me</context>
<context position="30224" citStr="Brennan and Schober, 2001" startWordPosition="5430" endWordPosition="5433">go top to bottom. is immediately available as conditioning evidence. Using (6) our distribution of RT judgements now shifts: pJ(s ∶ R1 I s ∶ ¬R6) = 1, pJ(s ∶ R2 I s ∶ ¬R6) = 0 and pJ(s ∶ R3 I s ∶ ¬R6) = 0 before “purple” has been parsed – thus providing a probabilistic explanation for increased subsequent processing speed. Finally at T4: ‘square’ given pJ(s ∶ R1 I s ∶ R1) = 1 and R1∧R2 = R1∧R3 = ⊥, the distribution remains unchanged. The system’s processing models how listeners reason about the revocation itself rather than predicting the outcome through positive evidence alone, in line with (Brennan and Schober, 2001)’s results. 6 Extensions Dialogue and self-repair in the wild To move towards domain-generality, generating the lattice of all possible dialogue situations for interesting domains is computationally intractable. We intend instead to consider incrementally occurring issues that can be modelled as questions (Larsson, 2002). Given one or more issues manifest in the dialogue at any time, it is plausible to generate small lattices dynamically to estimate possible answers, and also assign a real-valued relevance measure to questions that can be asked to resolve the issues. We are exploring how this </context>
</contexts>
<marker>Brennan, Schober, 2001</marker>
<rawString>S. Brennan and M. Schober. 2001. How listeners compensate for disfluencies in spontaneous speech. Journal ofMemory and Language, 44(2):274–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cooper</author>
<author>S Dobnik</author>
<author>S Lappin</author>
<author>S Larsson</author>
</authors>
<title>A probabilistic rich type theory for semantic interpretation.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL Workshop on Type Theory and Natural Language Semantics (TTNLS).</booktitle>
<contexts>
<context position="1278" citStr="Cooper et al., 2014" startWordPosition="183" endWordPosition="186">used to guide parsing and dialogue management decisions. 1 Introduction While classical type theory has been the predominant mathematical framework in natural language semantics for many years (Montague, 1974, inter alia), it is only recently that probabilistic type theory has been discussed for this purpose. Similarly, type-theoretic representations have been used within dialogue models (Ginzburg, 2012); and probabilistic modelling is common in dialogue systems (Williams and Young, 2007, inter alia), but combinations of the two remain scarce. Here, we attempt to make this connection, taking (Cooper et al., 2014)’s probabilistic Type Theory with Records (TTR) as our principal point of departure, with the aim of modelling incremental inference in dialogue. To our knowledge there has been no practical integration of probabilistic type-theoretic inference into a dialogue system so far; here we discuss computationally efficient methods for implementation in an extant incremental dialogue system. This paper demonstrates their efficacy in simple referential communication domains, but we argue the methods could be extended to larger domains and additionally used for on-line learning in future work. 2 Previou</context>
<context position="3210" citStr="Cooper et al., 2014" startWordPosition="497" endWordPosition="500">mplex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with linguistic context, such as visual perceptual information (Larsson, 2011; Dobnik et al., 2012), demonstrating its potential for embodied learning systems. The possibility of integration of perceptron learning (Larsson, 2011) and naive Bayes classifiers (Cooper et al., 2014) into TTR show how linguistic processing and probabilistic conceptual inference can be treated in a uniform way within the same representation system. Probabilistic TTR as described by Cooper et al. (2014) replaces the categorical s : T judgement with the real number valued p(s : T) = v where v ∈ [0,1]. The authors show how standard probability theoretic and Bayesian equations can be applied to TTR judgements and how an agent might learn from experience in a simple classification game. The agent is presented with instances of 80 Proceedings of the EACL 2014 Workshop on Type Theory and Natural </context>
<context position="10046" citStr="Cooper et al. (2014)" startWordPosition="1765" endWordPosition="1768">ield intersection. It is defined in terms of type checking, in that s : R1 v R2 iff s : R1 or s : R2. It follows that if R1 ⊑ R2 then s : R1 ∧ R2 iff s : R1, ands: R1 v R2 iff s : R2. While technically the maximally common supertype of R1 and R2 is the join type R1 v R2, here we introduce the maximally common simple (non disjunctive) supertype of two RTs R1 and R2 as field intersection: if R1 = [ l1 : T1 l2 : T2 I and R2 = r l2 : T2 ] L l3 : T3 MaxSuper(R1, R2) = [ l2 : T2 ] We will explore the usefulness of this new operation in terms of RT lattices in sec. 4. 3.1 Probabilistic TTR We follow Cooper et al. (2014)’s recent extension of TTR to include probabilistic type judgements of the form p(s : R) = v where v ∈ [0,1], i.e. the real valued judgement that a record s is of RT R. Here 2Here we concern ourselves with simple examples that avoid label-type clashes between two RTs (i.e. where R1 contains l1 : T1 and R2 contains l1 : T2); in these cases the operations are more complex than field concatenation/sharing. we use probabilistic TTR to model a common psycholinguistic experimental set up in section 5. We repeat some of Cooper et al.’s calculations here for exposition, but demonstrate efficient graph</context>
<context position="12363" citStr="Cooper et al., 2014" startWordPosition="2226" endWordPosition="2229">ilistic TTR this can be achieved by positing a judgement set J in which an agent stores probabilistic type judgements.3 We refer to the sum of the value of probabilistic judgements that a situation has been judged to be of type Ri within J as ∥Ri∥J and the sum of all probabilistic judgements in J simply as P(J); thus the prior probability that anything is of type Ri under the set of judgements J is ∥Ri∥J P (J) . The conditional probability p(s : R1 ∣ s : R2) under J can be reformulated in terms of these sets of judgements: ∥R1∧R2∥J iff ∥R211J * 0 pJ(s : R1 ∣ s : R2) = ∥R2∥J w(4) 0 otherwise 3(Cooper et al., 2014) characterise a type judgement as an Austinian proposition that a situation is of a given type with a given probability, encoded in a TTR record. MaxSub(Ri..Rn):2 if R ll : T1 and R 12 : T2 1 = l2 : T2 2 = l3 : T3 � R1 ∧ R2 = ⎡⎢⎢⎢⎢⎢⎢⎢⎣ l1 : T1 ⎦ l2 : T2 l3 : T3 (2) 82 where the sample spaces I IR1 ∧ R2I IS and I IR2I IS constitute the observations of the agent so far. J can have new judgements added to it during learning. We return to this after introducing the incremental semantics needed to interface therewith. 3.2 DS-TTR and the DyLan dialogue system In order to permit type-theoretic infere</context>
</contexts>
<marker>Cooper, Dobnik, Lappin, Larsson, 2014</marker>
<rawString>R. Cooper, S. Dobnik, S. Lappin, and S. Larsson. 2014. A probabilistic rich type theory for semantic interpretation. In Proceedings of the EACL Workshop on Type Theory and Natural Language Semantics (TTNLS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cooper</author>
</authors>
<title>Records and record types in semantic theory.</title>
<date>2005</date>
<journal>Journal of Logic and Computation,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="1958" citStr="Cooper, 2005" startWordPosition="293" endWordPosition="294">int of departure, with the aim of modelling incremental inference in dialogue. To our knowledge there has been no practical integration of probabilistic type-theoretic inference into a dialogue system so far; here we discuss computationally efficient methods for implementation in an extant incremental dialogue system. This paper demonstrates their efficacy in simple referential communication domains, but we argue the methods could be extended to larger domains and additionally used for on-line learning in future work. 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so</context>
<context position="6559" citStr="Cooper, 2005" startWordPosition="1029" endWordPosition="1030">ical level from linguistic information. They demonstrated self-repair can speed up semantic processing (or at least object reference) in such games, where an incorrect object being partly vocalized and then repaired in the instructions (e.g. “the yell-, uh, purple square”) yields quicker response times from the onset of the target (“purple”) than in the case of the fluent instructions (“the purple square”). This example will be addressed in section 5. First we will set out the framework in which we want to model such processing. 3 Probabilistic TTR in an incremental dialogue framework In TTR (Cooper, 2005; Cooper, 2012), the principal logical form of interest is the record type (‘RT’ from here), consisting of sequences offields of the form [l ∶ T ] containing a label l and a type T.1 RTs can be witnessed (i.e. judged as inhabited) by records of that type, where a record is a set of label-value pairs [l = v ]. The central type judgement in TTR that a record s is of (record) type R, i.e. s ∶ R, can be made from the component type judgements of individual fields; e.g. the onefield record [l = v ] is of type [l ∶ T ] just in case v is of type T. This is generalisable to records and RTs with multip</context>
</contexts>
<marker>Cooper, 2005</marker>
<rawString>R. Cooper. 2005. Records and record types in semantic theory. Journal of Logic and Computation, 15(2):99–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cooper</author>
</authors>
<title>Type theory and semantics in flux. In</title>
<date>2012</date>
<booktitle>Handbook of the Philosophy of Science, volume 14: Philosophy of Linguistics,</booktitle>
<pages>271--323</pages>
<editor>R. Kempson, N. Asher, and T. Fernando, editors,</editor>
<publisher>North Holland.</publisher>
<contexts>
<context position="2153" citStr="Cooper, 2012" startWordPosition="325" endWordPosition="326">ystem so far; here we discuss computationally efficient methods for implementation in an extant incremental dialogue system. This paper demonstrates their efficacy in simple referential communication domains, but we argue the methods could be extended to larger domains and additionally used for on-line learning in future work. 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper</context>
<context position="6574" citStr="Cooper, 2012" startWordPosition="1031" endWordPosition="1032">m linguistic information. They demonstrated self-repair can speed up semantic processing (or at least object reference) in such games, where an incorrect object being partly vocalized and then repaired in the instructions (e.g. “the yell-, uh, purple square”) yields quicker response times from the onset of the target (“purple”) than in the case of the fluent instructions (“the purple square”). This example will be addressed in section 5. First we will set out the framework in which we want to model such processing. 3 Probabilistic TTR in an incremental dialogue framework In TTR (Cooper, 2005; Cooper, 2012), the principal logical form of interest is the record type (‘RT’ from here), consisting of sequences offields of the form [l ∶ T ] containing a label l and a type T.1 RTs can be witnessed (i.e. judged as inhabited) by records of that type, where a record is a set of label-value pairs [l = v ]. The central type judgement in TTR that a record s is of (record) type R, i.e. s ∶ R, can be made from the component type judgements of individual fields; e.g. the onefield record [l = v ] is of type [l ∶ T ] just in case v is of type T. This is generalisable to records and RTs with multiple fields: a re</context>
<context position="8415" citStr="Cooper, 2012" startWordPosition="1419" endWordPosition="1420">r) in the RT, e.g. l1 is bound in the predicate type field l3, so l3 depends on l1. Subtypes, meets and joins A relation between RTs we wish to explore is 5 (‘is a subtype of’), which can be defined for RTs in terms of fields as simply: R1 5 R2 if for all fields [l ∶ T2 ] in R2, R1 contains [l ∶ T1 ] where T1 5 T2. In Figure 1, both R1 5 R3 and R2 5 R3; and R1 5 R2 iff T2 5 T2′. The transitive nature of this relation (if R1 5 R2 and R2 5 R3 then R1 5 R3) can be used effectively for type-theoretic inference. 1We only introduce the elements of TTR relevant to the phenomena discussed below. See (Cooper, 2012) for a detailed formal description. l1 ∶ T1 l2 ∶ T2 l3 ∶ T3(l1) ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ R1 ∶ 81 We also assume the existence of manifest (singleton) types, e.g. Ta, the type of which only a is a member. Here, we write manifest RT fields such as [l : Ta ] where Ta ⊑ T using the syntactic sugar [l=a : T ]. The subtype relation effectively allows progressive instantiation of fields (as addition of fields to R leads to R′ where R′ ⊑ R), which is practically useful for an incremental dialogue system as we will explain. We can also define meet and join types of two or more RTs. The representation of the meet typ</context>
</contexts>
<marker>Cooper, 2012</marker>
<rawString>R. Cooper. 2012. Type theory and semantics in flux. In R. Kempson, N. Asher, and T. Fernando, editors, Handbook of the Philosophy of Science, volume 14: Philosophy of Linguistics, pages 271–323. North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263</pages>
<contexts>
<context position="5252" citStr="Dale and Reiter, 1995" startWordPosition="822" endWordPosition="825"> been significant experimental work on simple referential communication games in psycholinguistics, computational and formal modelling. In terms of production and generation, Levelt (1989) discusses speaker strategies for generating referring expressions in a simple object naming game. He showed how speakers use informationally redundant features of the objects, violating Grice’s Maxim of Quantity. In natural language generation (NLG), referring expression generation (REG) has been widely studied (see (Krahmer and Van Deemter, 2012) for a comprehensive survey). The incremental algorithm (IA) (Dale and Reiter, 1995) is an iterative feature selection procedure for descriptions of objects based on computing the distractor set of referents that each adjective in a referring expression could cause to be inferred. More recently Frank and Goodman (2012) present a Bayesian model of optimising referring expressions based on surprisal, the information-theoretic measure of how much descriptions reduce uncertainty about their intended referent, a measure which they claim correlates strongly to human judgements. The element of the referring expression domain we discuss here is incremental processing. There is eviden</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233– 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D DeVault</author>
<author>M Stone</author>
</authors>
<title>Learning to interpret utterances using dialogue history.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="31449" citStr="DeVault and Stone, 2009" startWordPosition="5618" endWordPosition="5621">ould be implemented using the inquiry calculus (Knuth, 2005), which defines information theoretic relevance in terms of a probabilistic question lattice, and furthermore how this could be used to model the cause of self-repair as a time critical trade-off between relevance and accuracy. Learning in a dialogue While not our focus here, lattice G’s probabilities can be updated through observations after its initial construction. If a reference game is played over several rounds, the choice of referring expression can change based on mutually salient functions from words to situations- see e.g. (DeVault and Stone, 2009). Our currently frequentist approach to learning is: given an observation of an existing RT Ri is made with probability v, then I IRiI IJ, the overall denominator P(J) , and the nodes in the upward path from Ri to ⊤ are incremented by v. The approach could be converted to Bayesian update learning by using the prior probabilities in G for calculating v before it is added. Furthermore, observations can be added to G that include novel RTs: due to the DAG structure of G, their subtype ordering and probability effects can be integrated efficiently. 7 Conclusion We have discussed efficient methods </context>
</contexts>
<marker>DeVault, Stone, 2009</marker>
<rawString>D. DeVault and M. Stone. 2009. Learning to interpret utterances using dialogue history. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dobnik</author>
<author>R Cooper</author>
<author>S Larsson</author>
</authors>
<title>Modelling language, action, and perception in type theory with records.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th International Workshop on Constraint Solving and Language Processing (CSLP12).</booktitle>
<contexts>
<context position="3030" citStr="Dobnik et al., 2012" startWordPosition="472" endWordPosition="475"> technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with linguistic context, such as visual perceptual information (Larsson, 2011; Dobnik et al., 2012), demonstrating its potential for embodied learning systems. The possibility of integration of perceptron learning (Larsson, 2011) and naive Bayes classifiers (Cooper et al., 2014) into TTR show how linguistic processing and probabilistic conceptual inference can be treated in a uniform way within the same representation system. Probabilistic TTR as described by Cooper et al. (2014) replaces the categorical s : T judgement with the real number valued p(s : T) = v where v ∈ [0,1]. The authors show how standard probability theoretic and Bayesian equations can be applied to TTR judgements and how</context>
</contexts>
<marker>Dobnik, Cooper, Larsson, 2012</marker>
<rawString>S. Dobnik, R. Cooper, and S. Larsson. 2012. Modelling language, action, and perception in type theory with records. In Proceedings of the 7th International Workshop on Constraint Solving and Language Processing (CSLP12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Eshghi</author>
<author>J Hough</author>
<author>M Purver</author>
</authors>
<title>Incremental grammar induction from child-directed dialogue utterances.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL).</booktitle>
<contexts>
<context position="2399" citStr="Eshghi et al., 2013" startWordPosition="361" endWordPosition="364"> be extended to larger domains and additionally used for on-line learning in future work. 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with linguistic context, such as visual perceptual information (Lars</context>
<context position="13257" citStr="Eshghi et al., 2013" startWordPosition="2395" endWordPosition="2398">the sample spaces I IR1 ∧ R2I IS and I IR2I IS constitute the observations of the agent so far. J can have new judgements added to it during learning. We return to this after introducing the incremental semantics needed to interface therewith. 3.2 DS-TTR and the DyLan dialogue system In order to permit type-theoretic inference in a dialogue system, we need to provide suitable TTR representations for utterances and the current pragmatic situation from a parser, dialogue manager and generator as instantaneously and accurately as possible. For this purpose we use an incremental framework DS-TTR (Eshghi et al., 2013; Purver et al., 2011) which integrates TTR representations with the inherently incremental grammar formalism Dynamic Syntax (DS) (Kempson et al., 2001). O, Ty(t), ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x=john e ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ e=arrive es p=subj(e,x) t head=p t Figure 2: DS-TTR tree DS produces an incrementally specified, partial logical tree as words are parsed/generated; following Purver et al. (2011), DS tree nodes are decorated not with simple atomic formulae but with RTs, and corresponding lambda abstracts representing functions of type RT → RT (e.g. Ar [l1 T1 ].[l2=r.l1 T1 ] where r.l1 is a path expression referrin</context>
<context position="14627" citStr="Eshghi et al., 2013" startWordPosition="2633" endWordPosition="2636">cation of leaf node content, e.g. [x e] is unmanifest whereas [x=john e]4 is manifest and the latter is a subtype of the former. Functional application can apply incrementally, allowing a RT at the root node to be compiled for any partial tree, which is incrementally further specified as parsing proceeds (Hough and Purver, 2012). Within a given parse path, due to 4This is syntactic sugar for [ x ejohn ] and the = sign is not the same semantically as that in a record. DS-TTR’s monotonicity, each maximal RT of the tree’s root node is a subtype of the parser’s previous maximal output. Following (Eshghi et al., 2013), DS-TTR tree nodes include a field head in all RTs which corresponds to the DS tree node type. We also assume a neo-Davidsonian representation of predicates, with fields corresponding to an event term and to each semantic role; this allows all available semantic information to be specified incrementally in a strict subtyping relation (e.g. providing the subj() field when subject but not object has been parsed) – see Figure 2. We implement DS-TTR parsing and generation mechanisms in the DyLan dialogue system5 within Jindigo (Skantze and Hjalmarsson, 2010), a Java-based implementation of the in</context>
<context position="17300" citStr="Eshghi et al., 2013" startWordPosition="3066" endWordPosition="3069">eforge.net/ Ty(e), �x=john e head=x e � Ty(e → t), Ar [ head e ] . x=r.head e e=arrive es ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ p=subj(e,x) t head=p t ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 83 same parse graph as the interpreter to allow selfmonitoring and compound contributions, but we omit the details here. Figure 3: Normal incremental parsing in Dylan 4 Order theoretic and graphical methods for probabilistic TTR RT lattices to encode domain knowledge To support efficient inference in DyLan, we represent dialogue domain concepts via partially ordered sets (posets) of RT judgements, following similar insights used in inducing DS-TTR actions (Eshghi et al., 2013). A poset has several advantages over an unordered list of un-decomposed record types: the possibility of incremental type-checking; increased speed of type-checking, particularly for pairs of/multiple type judgements; immediate use of type judgements to guide system decisions; inference from negation; and the inclusion of learning within a domain. We leave the final challenge for future work, but discuss the others here. We can construct a poset of type judgements for any single RT by decomposing it into its constituent supertype judgements in a record type lattice. Representationally, as per</context>
</contexts>
<marker>Eshghi, Hough, Purver, 2013</marker>
<rawString>A. Eshghi, J. Hough, and M. Purver. 2013. Incremental grammar induction from child-directed dialogue utterances. In Proceedings of the 4th Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fern´andez</author>
</authors>
<title>Non-Sentential Utterances in Dialogue: Classification, Resolution and Use.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>King’s College London, University of London.</institution>
<marker>Fern´andez, 2006</marker>
<rawString>R. Fern´andez. 2006. Non-Sentential Utterances in Dialogue: Classification, Resolution and Use. Ph.D. thesis, King’s College London, University of London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C Frank</author>
<author>N D Goodman</author>
</authors>
<title>Predicting pragmatic reasoning in language games.</title>
<date>2012</date>
<journal>Science,</journal>
<volume>336</volume>
<issue>6084</issue>
<contexts>
<context position="5488" citStr="Frank and Goodman (2012)" startWordPosition="860" endWordPosition="863">eferring expressions in a simple object naming game. He showed how speakers use informationally redundant features of the objects, violating Grice’s Maxim of Quantity. In natural language generation (NLG), referring expression generation (REG) has been widely studied (see (Krahmer and Van Deemter, 2012) for a comprehensive survey). The incremental algorithm (IA) (Dale and Reiter, 1995) is an iterative feature selection procedure for descriptions of objects based on computing the distractor set of referents that each adjective in a referring expression could cause to be inferred. More recently Frank and Goodman (2012) present a Bayesian model of optimising referring expressions based on surprisal, the information-theoretic measure of how much descriptions reduce uncertainty about their intended referent, a measure which they claim correlates strongly to human judgements. The element of the referring expression domain we discuss here is incremental processing. There is evidence from (Brennan and Schober, 2001)’s experiments that people reason at an incredibly time-critical level from linguistic information. They demonstrated self-repair can speed up semantic processing (or at least object reference) in such</context>
</contexts>
<marker>Frank, Goodman, 2012</marker>
<rawString>M. C. Frank and N. D. Goodman. 2012. Predicting pragmatic reasoning in language games. Science, 336(6084):998–998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ginzburg</author>
</authors>
<title>The Interactive Stance: Meaning for Conversation.</title>
<date>2012</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1065" citStr="Ginzburg, 2012" startWordPosition="151" endWordPosition="152">ter presenting the formalism and dialogue framework, we show how probabilistic TTR type judgements can be integrated into the inference system of an incremental dialogue system, and discuss how this could be used to guide parsing and dialogue management decisions. 1 Introduction While classical type theory has been the predominant mathematical framework in natural language semantics for many years (Montague, 1974, inter alia), it is only recently that probabilistic type theory has been discussed for this purpose. Similarly, type-theoretic representations have been used within dialogue models (Ginzburg, 2012); and probabilistic modelling is common in dialogue systems (Williams and Young, 2007, inter alia), but combinations of the two remain scarce. Here, we attempt to make this connection, taking (Cooper et al., 2014)’s probabilistic Type Theory with Records (TTR) as our principal point of departure, with the aim of modelling incremental inference in dialogue. To our knowledge there has been no practical integration of probabilistic type-theoretic inference into a dialogue system so far; here we discuss computationally efficient methods for implementation in an extant incremental dialogue system. </context>
<context position="2745" citStr="Ginzburg, 2012" startWordPosition="432" endWordPosition="433">g, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with linguistic context, such as visual perceptual information (Larsson, 2011; Dobnik et al., 2012), demonstrating its potential for embodied learning systems. The possibility of integration of perceptron learning (Larsson, 2011) and naive Bayes classifiers (Cooper et al., 2014) into TTR show how linguistic processing and probabilistic conceptual inference can be treated in a uniform way within the same represe</context>
</contexts>
<marker>Ginzburg, 2012</marker>
<rawString>J. Ginzburg. 2012. The Interactive Stance: Meaning for Conversation. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hough</author>
<author>M Purver</author>
</authors>
<title>Processing self-repairs in an incremental type-theoretic dialogue system.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th SemDial Workshop on the Semantics and Pragmatics ofDialogue (SeineDial).</booktitle>
<contexts>
<context position="2341" citStr="Hough and Purver, 2012" startWordPosition="352" endWordPosition="355">rential communication domains, but we argue the methods could be extended to larger domains and additionally used for on-line learning in future work. 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with lingu</context>
<context position="14337" citStr="Hough and Purver, 2012" startWordPosition="2578" endWordPosition="2581"> corresponding lambda abstracts representing functions of type RT → RT (e.g. Ar [l1 T1 ].[l2=r.l1 T1 ] where r.l1 is a path expression referring to the label l1 in r) – see Figure 2. Using the idea of manifestness of fields as mentioned above, we have a natural representation for underspecification of leaf node content, e.g. [x e] is unmanifest whereas [x=john e]4 is manifest and the latter is a subtype of the former. Functional application can apply incrementally, allowing a RT at the root node to be compiled for any partial tree, which is incrementally further specified as parsing proceeds (Hough and Purver, 2012). Within a given parse path, due to 4This is syntactic sugar for [ x ejohn ] and the = sign is not the same semantically as that in a record. DS-TTR’s monotonicity, each maximal RT of the tree’s root node is a subtype of the parser’s previous maximal output. Following (Eshghi et al., 2013), DS-TTR tree nodes include a field head in all RTs which corresponds to the DS tree node type. We also assume a neo-Davidsonian representation of predicates, with fields corresponding to an event term and to each semantic role; this allows all available semantic information to be specified incrementally in a</context>
<context position="16632" citStr="Hough and Purver, 2012" startWordPosition="2962" endWordPosition="2965"> as shown in Figure 3: • input: a time-linear word graph posted by the ASR module, consisting of word hypothesis edge IUs between vertices Wn • processing: the internal DS parsing DAG, which adds parse state edge IUs between vertices Sn groundedIn the corresponding word hypothesis edge IU • output: a concept graph consisting of domain concept IUs (RTs) as edges between vertices Cn, groundedIn the corresponding path in the DS parsing DAG Here, our interest is principally in the parser output, to support incremental inference; a DS-TTR generator is also included which uses RTs as goal concepts (Hough and Purver, 2012) and uses the 5Available from http://dylan.sourceforge.net/ Ty(e), �x=john e head=x e � Ty(e → t), Ar [ head e ] . x=r.head e e=arrive es ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ p=subj(e,x) t head=p t ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ 83 same parse graph as the interpreter to allow selfmonitoring and compound contributions, but we omit the details here. Figure 3: Normal incremental parsing in Dylan 4 Order theoretic and graphical methods for probabilistic TTR RT lattices to encode domain knowledge To support efficient inference in DyLan, we represent dialogue domain concepts via partially ordered sets (posets) of RT judgements, following si</context>
<context position="28702" citStr="Hough and Purver (2012)" startWordPosition="5161" endWordPosition="5164">e object is a yellow object). Taking this judgement as the conditioning evidence using function (5) we get pJ(s ∶ R1 ∣ s ∶ R6) = 0 and using (4) we get pJ(s ∶ R2 ∣ s ∶ R6) = 0.5 and pJ(s ∶ R3 ∣ s ∶ R6) = 0.5 (see the schematic probability distribution at stage T2 in Figure 6 for the three objects). The meet type probabilities required for the conditional probabilities can be found graphically as described above. At T3:‘uh purple’, low probability in the interpreter output causes a self-repair to be recognised, enforcing backtracking on the parse graph which informally operates as follows (see Hough and Purver (2012)) : Self-repair: IF from parsing word W the edge 5En is insufficiently likely to be constructed from vertex 5n OR IF there is no sufficiently likely judgement p(s ∶ Rx) for Rx ∈ G THEN parse word W from vertex 5n−1. IF successful add a new edge to the top path, without removing any committed edges beginning at 5n−1; ELSE set n=n−1 and repeat. This algorithm is consistent with a local model for self-repair backtracking found in corpora (Shriberg and Stolcke, 1998; Hough and Purver, 2013). As regards inference in G, upon detection of a self-repair that revokes s ∶ R6, the type judgement s ∶ ¬R6,</context>
</contexts>
<marker>Hough, Purver, 2012</marker>
<rawString>J. Hough and M. Purver. 2012. Processing self-repairs in an incremental type-theoretic dialogue system. In Proceedings of the 16th SemDial Workshop on the Semantics and Pragmatics ofDialogue (SeineDial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hough</author>
<author>M Purver</author>
</authors>
<title>Modelling expectation in the self-repair processing of annotat-, um, listeners.</title>
<date>2013</date>
<booktitle>In Proceedings of the 17th SemDial Workshop on the Semantics and Pragmatics of Dialogue (DialDam).</booktitle>
<contexts>
<context position="29193" citStr="Hough and Purver, 2013" startWordPosition="5250" endWordPosition="5253">repair to be recognised, enforcing backtracking on the parse graph which informally operates as follows (see Hough and Purver (2012)) : Self-repair: IF from parsing word W the edge 5En is insufficiently likely to be constructed from vertex 5n OR IF there is no sufficiently likely judgement p(s ∶ Rx) for Rx ∈ G THEN parse word W from vertex 5n−1. IF successful add a new edge to the top path, without removing any committed edges beginning at 5n−1; ELSE set n=n−1 and repeat. This algorithm is consistent with a local model for self-repair backtracking found in corpora (Shriberg and Stolcke, 1998; Hough and Purver, 2013). As regards inference in G, upon detection of a self-repair that revokes s ∶ R6, the type judgement s ∶ ¬R6, i.e. that this is not a yellow object, 10In practice, ASR modules yielding partial results are less reliable than their non-incremental counterparts, but progress is being made here (Schlangen and Skantze, 2009). 86 Figure 6: Incremental DS-TTR self-repair parsing. Inter-graph groundedIn links go top to bottom. is immediately available as conditioning evidence. Using (6) our distribution of RT judgements now shifts: pJ(s ∶ R1 I s ∶ ¬R6) = 1, pJ(s ∶ R2 I s ∶ ¬R6) = 0 and pJ(s ∶ R3 I s ∶</context>
</contexts>
<marker>Hough, Purver, 2013</marker>
<rawString>J. Hough and M. Purver. 2013. Modelling expectation in the self-repair processing of annotat-, um, listeners. In Proceedings of the 17th SemDial Workshop on the Semantics and Pragmatics of Dialogue (DialDam).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kempson</author>
<author>W Meyer-Viol</author>
<author>D Gabbay</author>
</authors>
<title>Dynamic Syntax: The Flow of Language Understanding.</title>
<date>2001</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="13409" citStr="Kempson et al., 2001" startWordPosition="2417" endWordPosition="2420"> We return to this after introducing the incremental semantics needed to interface therewith. 3.2 DS-TTR and the DyLan dialogue system In order to permit type-theoretic inference in a dialogue system, we need to provide suitable TTR representations for utterances and the current pragmatic situation from a parser, dialogue manager and generator as instantaneously and accurately as possible. For this purpose we use an incremental framework DS-TTR (Eshghi et al., 2013; Purver et al., 2011) which integrates TTR representations with the inherently incremental grammar formalism Dynamic Syntax (DS) (Kempson et al., 2001). O, Ty(t), ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x=john e ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ e=arrive es p=subj(e,x) t head=p t Figure 2: DS-TTR tree DS produces an incrementally specified, partial logical tree as words are parsed/generated; following Purver et al. (2011), DS tree nodes are decorated not with simple atomic formulae but with RTs, and corresponding lambda abstracts representing functions of type RT → RT (e.g. Ar [l1 T1 ].[l2=r.l1 T1 ] where r.l1 is a path expression referring to the label l1 in r) – see Figure 2. Using the idea of manifestness of fields as mentioned above, we have a natural representation for underspecifica</context>
</contexts>
<marker>Kempson, Meyer-Viol, Gabbay, 2001</marker>
<rawString>R. Kempson, W. Meyer-Viol, and D. Gabbay. 2001. Dynamic Syntax: The Flow of Language Understanding. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K H Knuth</author>
</authors>
<title>Lattice duality: The origin of probability and entropy.</title>
<date>2005</date>
<journal>Neurocomputing,</journal>
<pages>67--245</pages>
<contexts>
<context position="18152" citStr="Knuth (2005)" startWordPosition="3205" endWordPosition="3206"> judgements to guide system decisions; inference from negation; and the inclusion of learning within a domain. We leave the final challenge for future work, but discuss the others here. We can construct a poset of type judgements for any single RT by decomposing it into its constituent supertype judgements in a record type lattice. Representationally, as per set-theoretic lattices, this can be visualised as a Hasse diagram such as Figure 4, however here the ordering arrows show 5 (‘subtype of’) relations from descendant to ancestor nodes. To characterize an RT lattice G ordered by 5, we adapt Knuth (2005)’s description of lattices in line with standard order theory: for a pair of RT elements Rx and Ry, their lower bound is the set of all Rz E G such that Rz 5 Rx and Rz 5 Ry. In the event that a unique greatest lower bound exists, this is their meet, which in G happily corresponds to the TTR meet type Rx ∧ Ry. Dually, if their unique least upper bound exists, this is their R1200 = [] = T R120 = I a : b ] R121 = [ c : d ] R110 = [ e : f ] R10 = [a:b c: d J R11 = [a:b e : f J R12 = [c:d e : f J a: b c : d ⎤⎥⎥⎥⎥⎥⎥⎥⎦ e: f Figure 4: Record Type lattice ordered by the subtype relation join and in TTR</context>
<context position="23144" citStr="Knuth, 2005" startWordPosition="4119" endWordPosition="4120">sume Brennan and Schober (2001)’s experimental referring game domain described in section 2: three distinct domain situation RTs R1, R2 and R3 correspond to a purple square, a yellow square and a yellow circle, respectively. The RT lattice G constructed initially upon observation of the game (by instructor or instructee) shown in Figure 5 uses a uniform distribution for 6Although the priors’ disjunctive probability sums to 1 after G is constructed, i.e. in Figure 5 ∥R1∥J+∥R2∥J +∥R3∥J = 1, P (J) the real values initially assigned to them need not sum to unity, as they form the atoms of G (see (Knuth, 2005)). 7Note that it does not generate the join types but maximal common supertypes defined by field intersection. the three disjunctive final situations. Each node shows an RT Ri on the left and the derivation of its prior probability pJ(Ri) that any game situation record will be of type Ri on the right, purely in terms of the relevant priors and the global denominator P(J). G can be searched to make inferences in light of partial information from an ongoing utterance. We model inference as predicting the likelihood of relevant type judgements Ry ∈ G of a situation s, given the judgement s : Rx w</context>
<context position="26142" citStr="Knuth, 2005" startWordPosition="4720" endWordPosition="4721">5) 85 Figure 5: Record type lattice with initial uniform prior probablities R8 = [ x ∶ind ] ∥R1∥J +∥R2∥J +∥R3∥J = T = 1 P (J) PRIORS: ∥R1∥J = 1 3 ∥R2∥J = 1 3 ∥R3∥J = 13 ⎡⎢⎢⎢⎢⎢⎢⎢⎣ ∥R3∥J P(J) x ∶ ind ⎤⎥⎥ coly ∶ yellow(x) ⎥⎥⎥⎥⎥⎦ shpc ∶ circle(x) ∥R2∥J R3 = P(J) x ∶ ind∥R1∥J +∥R2∥J x ∶ ind ∥R1∥J x ∶ ind ∥R2∥J +∥R3∥J x ∶ ind ∥R3∥J R4 = shpsq ∶ square(x) P(J) R5 = colp ∶ purple(x) P(J) R6 = coly ∶ yell P(J) Rq = shpc ∶ circl e(x) P(J) x ∶ ind ⎤⎥⎥ colp ∶ purple(x) ⎥⎥⎥⎥⎥⎦ shpsq ∶ square(x) R0 = 1 = 0 x ∶ ind ⎤⎥⎥ coly ∶ yellow(x) ⎥⎥⎥⎥⎥⎦ shpsq ∶ square(x) ⎡⎢⎢⎢⎢⎢⎢⎢⎣ R1 = ∥R1∥J R2 = P(J) ⎡⎢⎢⎢⎢⎢⎢⎢⎣ tices (Knuth, 2005).8 As regards efficiency, worst case complexity for finding the meet probability at the common descendant of Rx and Ry is a linear O(m + n) where m and n are the number of edges in the downward (possibly forked) paths Rx -+ 1 and Ry -+ 1.9 5 Simulating incremental inference and self-repair processing Interpretation in DyLan and its interface to the RT lattice G follows evidence that dialogue agents parse self-repairs efficiently and that repaired dialogue content (reparanda) is given special status but not removed from the discourse context. To model Brennan and Schober (2001)’s findings of di</context>
<context position="30885" citStr="Knuth, 2005" startWordPosition="5533" endWordPosition="5534"> in the wild To move towards domain-generality, generating the lattice of all possible dialogue situations for interesting domains is computationally intractable. We intend instead to consider incrementally occurring issues that can be modelled as questions (Larsson, 2002). Given one or more issues manifest in the dialogue at any time, it is plausible to generate small lattices dynamically to estimate possible answers, and also assign a real-valued relevance measure to questions that can be asked to resolve the issues. We are exploring how this could be implemented using the inquiry calculus (Knuth, 2005), which defines information theoretic relevance in terms of a probabilistic question lattice, and furthermore how this could be used to model the cause of self-repair as a time critical trade-off between relevance and accuracy. Learning in a dialogue While not our focus here, lattice G’s probabilities can be updated through observations after its initial construction. If a reference game is played over several rounds, the choice of referring expression can change based on mutually salient functions from words to situations- see e.g. (DeVault and Stone, 2009). Our currently frequentist approach</context>
</contexts>
<marker>Knuth, 2005</marker>
<rawString>K. H. Knuth. 2005. Lattice duality: The origin of probability and entropy. Neurocomputing, 67:245–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>K Van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Krahmer, Van Deemter, 2012</marker>
<rawString>E. Krahmer and K. Van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
</authors>
<title>Issue-based Dialogue Management.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>G¨oteborg University.</institution>
<note>Also published as Gothenburg Monographs in Linguistics 21.</note>
<contexts>
<context position="30546" citStr="Larsson, 2002" startWordPosition="5476" endWordPosition="5478">t T4: ‘square’ given pJ(s ∶ R1 I s ∶ R1) = 1 and R1∧R2 = R1∧R3 = ⊥, the distribution remains unchanged. The system’s processing models how listeners reason about the revocation itself rather than predicting the outcome through positive evidence alone, in line with (Brennan and Schober, 2001)’s results. 6 Extensions Dialogue and self-repair in the wild To move towards domain-generality, generating the lattice of all possible dialogue situations for interesting domains is computationally intractable. We intend instead to consider incrementally occurring issues that can be modelled as questions (Larsson, 2002). Given one or more issues manifest in the dialogue at any time, it is plausible to generate small lattices dynamically to estimate possible answers, and also assign a real-valued relevance measure to questions that can be asked to resolve the issues. We are exploring how this could be implemented using the inquiry calculus (Knuth, 2005), which defines information theoretic relevance in terms of a probabilistic question lattice, and furthermore how this could be used to model the cause of self-repair as a time critical trade-off between relevance and accuracy. Learning in a dialogue While not </context>
</contexts>
<marker>Larsson, 2002</marker>
<rawString>S. Larsson. 2002. Issue-based Dialogue Management. Ph.D. thesis, G¨oteborg University. Also published as Gothenburg Monographs in Linguistics 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
</authors>
<title>Accommodating innovative meaning in dialogue.</title>
<date>2010</date>
<booktitle>Proc. of Londial, SemDial Workshop,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="9088" citStr="Larsson, 2010" startWordPosition="1553" endWordPosition="1554">(l1) ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎣ R1 ∶ 81 We also assume the existence of manifest (singleton) types, e.g. Ta, the type of which only a is a member. Here, we write manifest RT fields such as [l : Ta ] where Ta ⊑ T using the syntactic sugar [l=a : T ]. The subtype relation effectively allows progressive instantiation of fields (as addition of fields to R leads to R′ where R′ ⊑ R), which is practically useful for an incremental dialogue system as we will explain. We can also define meet and join types of two or more RTs. The representation of the meet type of two RTs R1 and R2 is the result of a merge operation (Larsson, 2010), which in simple terms here can be seen as union of fields. A meet type is also equivalent to the extraction of a maximal common subtype, an operation we will call = MaxSub(R1, R2) R1 and R2 here are common supertypes of the resulting R1 ∧ R2. On the other hand, the join of two RTs R1 and R2, the type R1 v R2 cannot be represented by field intersection. It is defined in terms of type checking, in that s : R1 v R2 iff s : R1 or s : R2. It follows that if R1 ⊑ R2 then s : R1 ∧ R2 iff s : R1, ands: R1 v R2 iff s : R2. While technically the maximally common supertype of R1 and R2 is the join type</context>
</contexts>
<marker>Larsson, 2010</marker>
<rawString>S. Larsson. 2010. Accommodating innovative meaning in dialogue. Proc. of Londial, SemDial Workshop, pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Larsson</author>
</authors>
<title>The TTR perceptron: Dynamic perceptual meanings and semantic coordination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Workshop on the Semantics and Pragmatics of Dialogue (SemDial</booktitle>
<contexts>
<context position="3008" citStr="Larsson, 2011" startWordPosition="470" endWordPosition="471">013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued perceptual judgements used in conjunction with linguistic context, such as visual perceptual information (Larsson, 2011; Dobnik et al., 2012), demonstrating its potential for embodied learning systems. The possibility of integration of perceptron learning (Larsson, 2011) and naive Bayes classifiers (Cooper et al., 2014) into TTR show how linguistic processing and probabilistic conceptual inference can be treated in a uniform way within the same representation system. Probabilistic TTR as described by Cooper et al. (2014) replaces the categorical s : T judgement with the real number valued p(s : T) = v where v ∈ [0,1]. The authors show how standard probability theoretic and Bayesian equations can be applied to </context>
</contexts>
<marker>Larsson, 2011</marker>
<rawString>S. Larsson. 2011. The TTR perceptron: Dynamic perceptual meanings and semantic coordination. In Proceedings of the 15th Workshop on the Semantics and Pragmatics of Dialogue (SemDial 2011 - Los Angelogue).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4818" citStr="Levelt (1989)" startWordPosition="760" endWordPosition="761">s ∶ TShp, s ∶ TCol) where Shp E {shpl, shpt} and Col E {coli, colt}. From a cognitive modelling perspective, these judgements can be viewed as probabilistic perceptual information derived from learning. We use similar methods in our toy domain, but show how prior judgements could be constructed efficiently, and how classifications can be made without exhaustive iteration through individual type classifiers. There has also been significant experimental work on simple referential communication games in psycholinguistics, computational and formal modelling. In terms of production and generation, Levelt (1989) discusses speaker strategies for generating referring expressions in a simple object naming game. He showed how speakers use informationally redundant features of the objects, violating Grice’s Maxim of Quantity. In natural language generation (NLG), referring expression generation (REG) has been widely studied (see (Krahmer and Van Deemter, 2012) for a comprehensive survey). The incremental algorithm (IA) (Dale and Reiter, 1995) is an iterative feature selection procedure for descriptions of objects based on computing the distractor set of referents that each adjective in a referring express</context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>W. Levelt. 1989. Speaking: From Intention to Articulation. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>Formal Philosophy: Selected Papers of Richard Montague.</title>
<date>1974</date>
<publisher>Yale University Press.</publisher>
<contexts>
<context position="866" citStr="Montague, 1974" startWordPosition="122" endWordPosition="123">We present an adaptation of recent work on probabilistic Type Theory with Records (Cooper et al., 2014) for the purposes of modelling the incremental semantic processing of dialogue participants. After presenting the formalism and dialogue framework, we show how probabilistic TTR type judgements can be integrated into the inference system of an incremental dialogue system, and discuss how this could be used to guide parsing and dialogue management decisions. 1 Introduction While classical type theory has been the predominant mathematical framework in natural language semantics for many years (Montague, 1974, inter alia), it is only recently that probabilistic type theory has been discussed for this purpose. Similarly, type-theoretic representations have been used within dialogue models (Ginzburg, 2012); and probabilistic modelling is common in dialogue systems (Williams and Young, 2007, inter alia), but combinations of the two remain scarce. Here, we attempt to make this connection, taking (Cooper et al., 2014)’s probabilistic Type Theory with Records (TTR) as our principal point of departure, with the aim of modelling incremental inference in dialogue. To our knowledge there has been no practic</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>R. Montague. 1974. Formal Philosophy: Selected Papers of Richard Montague. Yale University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>A Eshghi</author>
<author>J Hough</author>
</authors>
<title>Incremental semantic construction in a dialogue system.</title>
<date>2011</date>
<booktitle>Proceedings of the 9th International Conference on Computational Semantics.</booktitle>
<editor>In J. Bos and S. Pulman, editors,</editor>
<contexts>
<context position="2292" citStr="Purver et al., 2011" startWordPosition="345" endWordPosition="348">per demonstrates their efficacy in simple referential communication domains, but we argue the methods could be extended to larger domains and additionally used for on-line learning in future work. 2 Previous Work Type Theory with Records (TTR) (Betarte and Tasistro, 1998; Cooper, 2005) is a rich type theory which has become widely used in dialogue models, including information state models for a variety of phenomena such as clarification requests (Ginzburg, 2012; Cooper, 2012) and nonsentential fragments (Fern´andez, 2006). It has also been shown to be useful for incremental semantic parsing (Purver et al., 2011), incremental generation (Hough and Purver, 2012), and recently for grammar induction (Eshghi et al., 2013). While the technical details will be given in section 3, the central judgement in type theory s : T (that a given object s is of type T) is extended in TTR so that s can be a (potentially complex) record and T can be a record type – e.g. s could represent a dialogue gameboard state and T could be a dialogue gameboard state type (Ginzburg, 2012; Cooper, 2012). As TTR is highly flexible with a rich type system, variants have been considered with types corresponding to real-numbervalued per</context>
<context position="13279" citStr="Purver et al., 2011" startWordPosition="2399" endWordPosition="2402">R1 ∧ R2I IS and I IR2I IS constitute the observations of the agent so far. J can have new judgements added to it during learning. We return to this after introducing the incremental semantics needed to interface therewith. 3.2 DS-TTR and the DyLan dialogue system In order to permit type-theoretic inference in a dialogue system, we need to provide suitable TTR representations for utterances and the current pragmatic situation from a parser, dialogue manager and generator as instantaneously and accurately as possible. For this purpose we use an incremental framework DS-TTR (Eshghi et al., 2013; Purver et al., 2011) which integrates TTR representations with the inherently incremental grammar formalism Dynamic Syntax (DS) (Kempson et al., 2001). O, Ty(t), ⎡⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣ x=john e ⎤⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦ e=arrive es p=subj(e,x) t head=p t Figure 2: DS-TTR tree DS produces an incrementally specified, partial logical tree as words are parsed/generated; following Purver et al. (2011), DS tree nodes are decorated not with simple atomic formulae but with RTs, and corresponding lambda abstracts representing functions of type RT → RT (e.g. Ar [l1 T1 ].[l2=r.l1 T1 ] where r.l1 is a path expression referring to the label l1 in r</context>
<context position="15781" citStr="Purver et al., 2011" startWordPosition="2818" endWordPosition="2821">ntze and Hjalmarsson, 2010), a Java-based implementation of the incremental unit (IU) framework of (Schlangen and Skantze, 2009). In this framework, each module has input and output IUs which can be added as edges between vertices in module buffer graphs, and become committed should the appropriate conditions be fulfilled, a notion which becomes important in light of hypothesis change and repair situations. Dependency relations between different graphs within and between modules can be specified by groundedIn links (see (Schlangen and Skantze, 2009) for details). The DyLan interpreter module (Purver et al., 2011) uses Sato (2011)’s insight that the context of DS parsing can be characterized in terms of a Directed Acyclic Graph (DAG) with trees for nodes and DS actions for edges. The module’s state is characterized by three linked graphs as shown in Figure 3: • input: a time-linear word graph posted by the ASR module, consisting of word hypothesis edge IUs between vertices Wn • processing: the internal DS parsing DAG, which adds parse state edge IUs between vertices Sn groundedIn the corresponding word hypothesis edge IU • output: a concept graph consisting of domain concept IUs (RTs) as edges between </context>
</contexts>
<marker>Purver, Eshghi, Hough, 2011</marker>
<rawString>M. Purver, A. Eshghi, and J. Hough. 2011. Incremental semantic construction in a dialogue system. In J. Bos and S. Pulman, editors, Proceedings of the 9th International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Sato</author>
</authors>
<title>Local ambiguity, search strategies and parsing in Dynamic Syntax.</title>
<date>2011</date>
<editor>In E. Gregoromichelaki, R. Kempson, and C. Howes, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="15798" citStr="Sato (2011)" startWordPosition="2823" endWordPosition="2824">), a Java-based implementation of the incremental unit (IU) framework of (Schlangen and Skantze, 2009). In this framework, each module has input and output IUs which can be added as edges between vertices in module buffer graphs, and become committed should the appropriate conditions be fulfilled, a notion which becomes important in light of hypothesis change and repair situations. Dependency relations between different graphs within and between modules can be specified by groundedIn links (see (Schlangen and Skantze, 2009) for details). The DyLan interpreter module (Purver et al., 2011) uses Sato (2011)’s insight that the context of DS parsing can be characterized in terms of a Directed Acyclic Graph (DAG) with trees for nodes and DS actions for edges. The module’s state is characterized by three linked graphs as shown in Figure 3: • input: a time-linear word graph posted by the ASR module, consisting of word hypothesis edge IUs between vertices Wn • processing: the internal DS parsing DAG, which adds parse state edge IUs between vertices Sn groundedIn the corresponding word hypothesis edge IU • output: a concept graph consisting of domain concept IUs (RTs) as edges between vertices Cn, grou</context>
</contexts>
<marker>Sato, 2011</marker>
<rawString>Y. Sato. 2011. Local ambiguity, search strategies and parsing in Dynamic Syntax. In E. Gregoromichelaki, R. Kempson, and C. Howes, editors, The Dynamics of Lexical Interfaces. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schlangen</author>
<author>G Skantze</author>
</authors>
<title>A general, abstract model of incremental dialogue processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<contexts>
<context position="15289" citStr="Schlangen and Skantze, 2009" startWordPosition="2739" endWordPosition="2742">d head in all RTs which corresponds to the DS tree node type. We also assume a neo-Davidsonian representation of predicates, with fields corresponding to an event term and to each semantic role; this allows all available semantic information to be specified incrementally in a strict subtyping relation (e.g. providing the subj() field when subject but not object has been parsed) – see Figure 2. We implement DS-TTR parsing and generation mechanisms in the DyLan dialogue system5 within Jindigo (Skantze and Hjalmarsson, 2010), a Java-based implementation of the incremental unit (IU) framework of (Schlangen and Skantze, 2009). In this framework, each module has input and output IUs which can be added as edges between vertices in module buffer graphs, and become committed should the appropriate conditions be fulfilled, a notion which becomes important in light of hypothesis change and repair situations. Dependency relations between different graphs within and between modules can be specified by groundedIn links (see (Schlangen and Skantze, 2009) for details). The DyLan interpreter module (Purver et al., 2011) uses Sato (2011)’s insight that the context of DS parsing can be characterized in terms of a Directed Acycl</context>
<context position="29514" citStr="Schlangen and Skantze, 2009" startWordPosition="5304" endWordPosition="5307"> parse word W from vertex 5n−1. IF successful add a new edge to the top path, without removing any committed edges beginning at 5n−1; ELSE set n=n−1 and repeat. This algorithm is consistent with a local model for self-repair backtracking found in corpora (Shriberg and Stolcke, 1998; Hough and Purver, 2013). As regards inference in G, upon detection of a self-repair that revokes s ∶ R6, the type judgement s ∶ ¬R6, i.e. that this is not a yellow object, 10In practice, ASR modules yielding partial results are less reliable than their non-incremental counterparts, but progress is being made here (Schlangen and Skantze, 2009). 86 Figure 6: Incremental DS-TTR self-repair parsing. Inter-graph groundedIn links go top to bottom. is immediately available as conditioning evidence. Using (6) our distribution of RT judgements now shifts: pJ(s ∶ R1 I s ∶ ¬R6) = 1, pJ(s ∶ R2 I s ∶ ¬R6) = 0 and pJ(s ∶ R3 I s ∶ ¬R6) = 0 before “purple” has been parsed – thus providing a probabilistic explanation for increased subsequent processing speed. Finally at T4: ‘square’ given pJ(s ∶ R1 I s ∶ R1) = 1 and R1∧R2 = R1∧R3 = ⊥, the distribution remains unchanged. The system’s processing models how listeners reason about the revocation itsel</context>
</contexts>
<marker>Schlangen, Skantze, 2009</marker>
<rawString>D. Schlangen and G. Skantze. 2009. A general, abstract model of incremental dialogue processing. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>How far do speakers back up in repairs? A quantitative model.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="29168" citStr="Shriberg and Stolcke, 1998" startWordPosition="5246" endWordPosition="5249">preter output causes a self-repair to be recognised, enforcing backtracking on the parse graph which informally operates as follows (see Hough and Purver (2012)) : Self-repair: IF from parsing word W the edge 5En is insufficiently likely to be constructed from vertex 5n OR IF there is no sufficiently likely judgement p(s ∶ Rx) for Rx ∈ G THEN parse word W from vertex 5n−1. IF successful add a new edge to the top path, without removing any committed edges beginning at 5n−1; ELSE set n=n−1 and repeat. This algorithm is consistent with a local model for self-repair backtracking found in corpora (Shriberg and Stolcke, 1998; Hough and Purver, 2013). As regards inference in G, upon detection of a self-repair that revokes s ∶ R6, the type judgement s ∶ ¬R6, i.e. that this is not a yellow object, 10In practice, ASR modules yielding partial results are less reliable than their non-incremental counterparts, but progress is being made here (Schlangen and Skantze, 2009). 86 Figure 6: Incremental DS-TTR self-repair parsing. Inter-graph groundedIn links go top to bottom. is immediately available as conditioning evidence. Using (6) our distribution of RT judgements now shifts: pJ(s ∶ R1 I s ∶ ¬R6) = 1, pJ(s ∶ R2 I s ∶ ¬R6</context>
</contexts>
<marker>Shriberg, Stolcke, 1998</marker>
<rawString>E. Shriberg and A. Stolcke. 1998. How far do speakers back up in repairs? A quantitative model. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>A Hjalmarsson</author>
</authors>
<title>Towards incremental speech generation in dialogue systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the SIGDIAL 2010 Conference.</booktitle>
<contexts>
<context position="15188" citStr="Skantze and Hjalmarsson, 2010" startWordPosition="2725" endWordPosition="2728">the parser’s previous maximal output. Following (Eshghi et al., 2013), DS-TTR tree nodes include a field head in all RTs which corresponds to the DS tree node type. We also assume a neo-Davidsonian representation of predicates, with fields corresponding to an event term and to each semantic role; this allows all available semantic information to be specified incrementally in a strict subtyping relation (e.g. providing the subj() field when subject but not object has been parsed) – see Figure 2. We implement DS-TTR parsing and generation mechanisms in the DyLan dialogue system5 within Jindigo (Skantze and Hjalmarsson, 2010), a Java-based implementation of the incremental unit (IU) framework of (Schlangen and Skantze, 2009). In this framework, each module has input and output IUs which can be added as edges between vertices in module buffer graphs, and become committed should the appropriate conditions be fulfilled, a notion which becomes important in light of hypothesis change and repair situations. Dependency relations between different graphs within and between modules can be specified by groundedIn links (see (Schlangen and Skantze, 2009) for details). The DyLan interpreter module (Purver et al., 2011) uses S</context>
</contexts>
<marker>Skantze, Hjalmarsson, 2010</marker>
<rawString>G. Skantze and A. Hjalmarsson. 2010. Towards incremental speech generation in dialogue systems. In Proceedings of the SIGDIAL 2010 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Scaling POMDPs for spoken dialog management.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<contexts>
<context position="1150" citStr="Williams and Young, 2007" startWordPosition="162" endWordPosition="165">ic TTR type judgements can be integrated into the inference system of an incremental dialogue system, and discuss how this could be used to guide parsing and dialogue management decisions. 1 Introduction While classical type theory has been the predominant mathematical framework in natural language semantics for many years (Montague, 1974, inter alia), it is only recently that probabilistic type theory has been discussed for this purpose. Similarly, type-theoretic representations have been used within dialogue models (Ginzburg, 2012); and probabilistic modelling is common in dialogue systems (Williams and Young, 2007, inter alia), but combinations of the two remain scarce. Here, we attempt to make this connection, taking (Cooper et al., 2014)’s probabilistic Type Theory with Records (TTR) as our principal point of departure, with the aim of modelling incremental inference in dialogue. To our knowledge there has been no practical integration of probabilistic type-theoretic inference into a dialogue system so far; here we discuss computationally efficient methods for implementation in an extant incremental dialogue system. This paper demonstrates their efficacy in simple referential communication domains, b</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>J. Williams and S. Young. 2007. Scaling POMDPs for spoken dialog management. IEEE Transactions on Audio, Speech, and Language Processing, 15(7):2116–2129.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
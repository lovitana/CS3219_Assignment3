<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.87619">
Grammatical error correction using hybrid systems and type filtering
</title>
<author confidence="0.91483">
Mariano Felice Zheng Yuan Øistein E. Andersen
Helen Yannakoudakis Ekaterina Kochmar
</author>
<affiliation confidence="0.997854">
Computer Laboratory, University of Cambridge, United Kingdom
</affiliation>
<email confidence="0.98912">
{mf501,zy249,oa223,hy260,ek3581@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997251" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991615384615">
This paper describes our submission to the
CoNLL 2014 shared task on grammatical
error correction using a hybrid approach,
which includes both a rule-based and an
SMT system augmented by a large web-
based language model. Furthermore, we
demonstrate that correction type estima-
tion can be used to remove unnecessary
corrections, improving precision without
harming recall. Our best hybrid system
achieves state-of-the-art results, ranking
first on the original test set and second on
the test set with alternative annotations.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999907446808511">
Grammatical error correction has attracted con-
siderable interest in the last few years, especially
through a series of ‘shared tasks’. These efforts
have helped to provide a common ground for eval-
uating and comparing systems while encouraging
research in the field. These shared tasks have pri-
marily focused on English as a second or foreign
language and addressed different error types. The
HOO 2011 task (Dale and Kilgarriff, 2011), for
example, included all error types whereas HOO
2012 (Dale et al., 2012) and the CoNLL 2013
shared task (Ng et al., 2013) were restricted to only
two and five types respectively.
In this paper, we describe our submission to the
CoNLL 2014 shared task (Ng et al., 2014), which
involves correcting all the errors in essays writ-
ten in English by students at the National Univer-
sity of Singapore. An all-type task poses a greater
challenge, since correcting open-class types (such
as spelling or collocation errors) requires different
correction strategies than those in closed classes
(such as determiners or prepositions).
In this scenario, hybrid systems or combinations
of correction modules seem more appropriate and
typically produce good results. In fact, most of
the participating teams in previous shared tasks
have used a combination of modules or systems
for their submissions, even for correcting closed-
class types (Dahlmeier et al., 2011; Bhaskar et
al., 2011; Rozovskaya et al., 2011; Ivanova et al.,
2011; Rozovskaya et al., 2013; Yoshimoto et al.,
2013; Xing et al., 2013; Kunchukuttan et al., 2013;
Putra and Szabo, 2013; Xiang et al., 2013).
In line with previous research, we present a hy-
brid approach that employs a rule-based error cor-
rection system and an ad-hoc statistical machine
translation (SMT) system, as well as a large-scale
language model to rank alternative corrections and
an error type filtering technique.
The remainder of this paper is organised as fol-
lows: Section 2 describes our approach and each
component in detail, Section 3 presents our experi-
ments using the CoNLL 2014 shared task develop-
ment set and Section 4 reports our official results
on the test set. Finally, we discuss the performance
of our system and present an error analysis in Sec-
tion 5 and conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.996732" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999977142857143">
We tackle the error correction task using a pipeline
of processes that combines results from multiple
systems. Figure 1 shows the interaction of the
components in our final hybrid system, producing
the results submitted to the CoNLL 2014 shared
task. The following sections describe each of these
components in detail.
</bodyText>
<sectionHeader confidence="0.847404" genericHeader="method">
2.1 Rule-based error correction system
(RBS)
</sectionHeader>
<bodyText confidence="0.99966975">
The rule-based system is a component of the Self-
Assessment and Tutoring (SAT) system, a web
service developed at the University of Cambridge
aimed at helping intermediate learners of English
</bodyText>
<page confidence="0.979237">
15
</page>
<note confidence="0.9887085">
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 15–24,
Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.8482975">
Figure 1: Overview of components and interac-
tions in our final hybrid system.
</figureCaption>
<bodyText confidence="0.996659033333333">
in their writing tasks1 (Andersen et al., 2013). The
original SAT system provides three main function-
alities: 1) text assessment, producing an overall
score for a piece of text, 2) sentence evaluation,
producing a sentence-level quality score, and 3)
word-level feedback, suggesting specific correc-
tions for frequent errors. Since the focus of the
shared task is on strict correction (as opposed to
detection), we only used the word-level feedback
component of the SAT system.
This module uses rules automatically derived
from the Cambridge Learner Corpus2 (CLC)
(Nicholls, 2003) that are aimed at detecting error-
ful unigrams, bigrams and trigrams. In order to
ensure high precision, rules are based on n-grams
that have been annotated as incorrect at least five
times and at least ninety per cent of the times
they occur. In addition to these corpus-derived
rules, many cases of incorrect but plausible deriva-
tional and inflectional morphology are detected by
means of rules derived from a machine-readable
dictionary. For further details on specific compo-
nents, we refer the reader to the aforementioned
paper.
Given an input text, the rule-based system pro-
duces an XML file containing a list of suggested
corrections. These corrections can either be ap-
plied to the original text or used to generate mul-
tiple correction candidates, as described in Sec-
tion 2.3.
</bodyText>
<subsectionHeader confidence="0.999566">
2.2 SMT system
</subsectionHeader>
<bodyText confidence="0.9928515">
We follow a similar approach to the one described
by Yuan and Felice (2013) in order to train an SMT
</bodyText>
<footnote confidence="0.979181333333333">
1The latest version of the system, called ‘Write
&amp; Improve’, is available at http://www.cambridge
english.org/writeandimprovebeta/.
2More information at http://www.cambridge
.org/elt/catalogue/subject/custom/item36
46603/
</footnote>
<listItem confidence="0.966754568181818">
system that can ‘translate’ from incorrect into cor-
rect English. Our training data comprises a set of
different parallel corpora, where the original (in-
correct) sentences constitute the source side and
corrected versions based on gold standard anno-
tations constitute the target side. These corpora
include:
• the NUCLE v3.1 corpus (Dahlmeier et al.,
2013), containing around 1,400 essays writ-
ten in English by students at the National
University of Singapore (approx. 1,220,257
tokens in 57,152 sentences),
• phrase alignments involving corrections ex-
tracted automatically from the NUCLE cor-
pus (with up to 7 tokens per side), which are
used to boost the probability of phrase align-
ments that involve corrections so as to im-
prove recall,
• the CoNLL 2014 shared task development
set, containing 50 essays from the previous
year’s test set (approx. 29,207 tokens in 1,382
sentences),
• the First Certificate in English (FCE) cor-
pus (Yannakoudakis et al., 2011), contain-
ing 1,244 exam scripts and 2 essays per
script (approx. 532,033 tokens in 16,068 sen-
tences),
• a subset of the International English Lan-
guage Testing System (IELTS) examination
dataset extracted from the CLC corpus, con-
taining 2,498 exam scripts and 2 essays per
script (approx. 1,361,841 tokens in 64,628
sentences), and
• a set of sentences from the English Vo-
cabulary Profile3 (EVP), which have been
modified to include artificially generated er-
rors (approx. 351,517 tokens in 18,830 sen-
tences). The original correct sentences are a
subset of the CLC and come from examina-
tions at different proficiency levels. The ar-
tificial error generation method aims at repli-
cating frequent error patterns observed in the
NUCLE corpus on error-free sentences, as
described by Yuan and Felice (2013).
</listItem>
<footnote confidence="0.994959666666667">
3Sentences were automatically scraped from http://
www.englishprofile.org/index.php?option=
com_content&amp;view=article&amp;id=4&amp;Itemid=5
</footnote>
<page confidence="0.99903">
16
</page>
<bodyText confidence="0.999885166666667">
Word alignment was carried out using pialign
(Neubig et al., 2011), after we found it outper-
formed GIZA++ (Och and Ney, 2000; Och and
Ney, 2003) and Berkeley Aligner (Liang et al.,
2006; DeNero and Klein, 2007) in terms of pre-
cision and F0.5 on the development set. Instead
of using heuristics to extract phrases from the
word alignments learnt by GIZA++ or Berker-
ley Aligner, pialign created a phrase table directly
from model probabilities.
In addition to the features already defined by pi-
align, we added character-level Levenshtein dis-
tance to each mapping in the phrase table. This
was done to allow for the fact that, in error correc-
tion, most words translate into themselves and er-
rors are often similar to their correct forms. Equal
weights were assigned to these features.
We then built a lexical reordering model using
the alignments created by pialign. The maximum
phrase length was set to 7, as recommended in the
SMT literature (Koehn et al., 2003; Koehn, 2014).
The IRSTLM Toolkit (Federico et al., 2008)
was used to build a 4-gram target language model
with Kneser–Ney smoothing (Kneser and Ney,
1995) on the correct sentences from the NUCLE,
full CLC and EVP corpora.
Decoding was performed with Moses (Koehn et
al., 2007), using the default settings and weights.
No tuning process was applied. The resulting sys-
tem was used to produce the 10 best correction
candidates for each sentence in the dataset, which
were further processed by other modules.
Segmentation, tokenisation and part-of-speech
tagging were performed using NLTK (Bird et
al., 2009) for consistency with the shared task
datasets.
</bodyText>
<subsectionHeader confidence="0.999223">
2.3 Candidate generation
</subsectionHeader>
<bodyText confidence="0.999408153846154">
In order to integrate corrections from multiple sys-
tems, we developed a method to generate all the
possible corrected versions of a sentence (candi-
dates). Candidates are generated by computing all
possible combinations of corrections (irrespective
of the system from which they originate), includ-
ing the original tokens to allow for a ‘no correc-
tion’ option. The list of candidates produced for
each sentence always includes the original (un-
modified) sentence plus any other versions derived
from system corrections.
In order for a combination of corrections to gen-
erate a valid candidate, all the corrections must be
</bodyText>
<figureCaption confidence="0.98702">
Figure 2: An example showing the candidate gen-
eration process.
</figureCaption>
<table confidence="0.9836025">
Model CE ME UE P R F0.5
SMT IRSTLM 651 2766 1832 0.2621 0.1905 0.2438
Microsoft Web 666 2751 1344 0.3313 0.1949 0.2907
N-grams
</table>
<tableCaption confidence="0.7420544">
Table 1: Performance of language models on the
development set after ranking the SMT system’s
10-best candidates per sentence. CE: correct ed-
its, ME: missed edits, UE: unnecessary edits, P:
precision, R: recall.
</tableCaption>
<bodyText confidence="0.999731857142857">
compatible; otherwise, the candidate is discarded.
We consider two or more corrections to be com-
patible if they do not overlap, in an attempt to
avoid introducing accidental errors. In addition,
if different correction sets produce the same can-
didate, we only keep one. Figure 2 illustrates the
candidate generation process.
</bodyText>
<subsectionHeader confidence="0.961985">
2.4 Language model ranking
</subsectionHeader>
<bodyText confidence="0.999424954545454">
Generated candidates are ranked using a language
model (LM), with the most probable candidate be-
ing selected as the final corrected version.
We tried two different alternatives for ranking:
1) using the target LM embedded in our SMT sys-
tem (described in Section 2.2) and 2) using a large
n-gram LM built from web data. In the latter
case, we used Microsoft Web N-gram Services,
which provide access to large smoothed n-gram
language models (with n=2,3,4,5) built from web
documents (Gao et al., 2010). All our experiments
are based on the 5-gram ‘bing-body:apr10’ model.
The ranking performance of these two models
was evaluated on the 10-best hypotheses generated
by the SMT system for each sentence in the devel-
opment set. Table 1 shows the results from the
M2 Scorer (Dahlmeier and Ng, 2012), the official
scorer for the shared task that, unlike previous ver-
sions, weights precision twice as much as recall.
Results show that using Microsoft’s Web LM
yields better performance, which is unsurprising
given the vast amounts of data used to build that
</bodyText>
<page confidence="0.997513">
17
</page>
<table confidence="0.979744333333333">
System CE ME UE P R F0.5
RBS 95 3322 107 0.4703 0.0278 0.1124
SMT 452 2965 690 0.3958 0.1323 0.2830
</table>
<tableCaption confidence="0.896092">
Table 2: Results of individual systems on the de-
velopment set.
</tableCaption>
<bodyText confidence="0.995548833333333">
model. For this reason, we adopt Microsoft’s
model for all further experiments.
We also note that without normalisation, higher
probabilities may be assigned to shorter sentences,
which can introduce a bias towards preferring
deletions or skipping insertions.
</bodyText>
<subsectionHeader confidence="0.985904">
2.5 Type filtering
</subsectionHeader>
<bodyText confidence="0.999994647058824">
Analysing performance by error type is very valu-
able for system development and tuning. How-
ever, this can only be performed for corrections
in the gold standard (either matched or missed).
To estimate types for unnecessary corrections, we
defined a set of heuristics that analyse differences
in word forms and part-of-speech tags between
the original phrases and their system corrections,
based on common patterns observed in the train-
ing data. We had previously used a similar strat-
egy to classify errors in our CoNLL 2013 shared
task submission (Yuan and Felice, 2013) but have
now included a few improvements and rules for
new types. Estimation accuracy is 50.92% on the
training set and 67.57% on the development set,
which we consider to be acceptable for our pur-
poses given that the final test set is more similar to
the development set.
Identifying types for system corrections is not
only useful during system development but can
also be exploited to filter out and reduce the num-
ber of proposed corrections. More specifically, if
a system proposes a much higher number of un-
necessary corrections than correct suggestions for
a specific error type, we can assume the system is
actually degrading the quality of the original text,
in which case it is preferable to filter out those er-
ror types. Such decisions will lower the total num-
ber of unnecessary edits, thus improving overall
precision. However, they will also harm recall,
unless the number of matched corrections for the
error type is zero (i.e. unless Ptype = 0). To avoid
this, only corrections for types having zero preci-
sion should be removed.
</bodyText>
<sectionHeader confidence="0.99069" genericHeader="method">
3 Experiments and results
</sectionHeader>
<bodyText confidence="0.9999484">
We carried out a series of experiments on the de-
velopment set using different pipelines and com-
binations of systems in order to find an optimal
setting. The following sections describe them in
detail.
</bodyText>
<subsectionHeader confidence="0.998774">
3.1 Individual system performance
</subsectionHeader>
<bodyText confidence="0.999912285714286">
Our first set of experiments were aimed at inves-
tigating individual system performance on the de-
velopment set, which is reported in Table 2. Re-
sults show that the SMT system has much better
performance, which is expected given that it has
been trained on texts similar to those in the test
set.
</bodyText>
<subsectionHeader confidence="0.995896">
3.2 Pipelines
</subsectionHeader>
<bodyText confidence="0.999980529411765">
Since corrections from the RBS and SMT systems
are often complementary, we set out to explore
combination schemes that would integrate correc-
tions from both systems. Table 3 shows results for
different combinations, where RBS and SMT in-
dicate all corrections from the respective systems,
subscript ‘c’ indicates candidates generated from
a system’s individual corrections, subscript ‘10-
best’ indicates the 10-best list of candidates pro-
duced by the SMT system, ‘&gt;’ indicates a pipeline
where the output of one system is the input to the
other and ‘+’ indicates a combination of candi-
dates from different systems. All these pipelines
use the RBS system as the first processing step in
order to perform an initial correction, which is ex-
tremely beneficial for the SMT system.
Results reveal that the differences between
these pipelines are small in terms of F0.5, although
there are noticeable variations in precision and re-
call. The best results are achieved when the 10
best hypotheses from the SMT system are ranked
with Microsoft’s LM, which confirms our results
in Table 1 showing that the SMT LM is outper-
formed by a larger web-based model.
A simple pipeline using the RBS system first
and the SMT system second (#3) yields per-
formance that is better than (or comparable to)
pipelines #1, #2 and #4, suggesting that there is no
real benefit in using more sophisticated pipelines
when only the best hypothesis from the SMT sys-
tem is used. However, performance is improved
when the 10 best SMT hypotheses are considered.
The only difference between pipelines #5 and #6
lies in the way corrections from the RBS system
</bodyText>
<page confidence="0.994685">
18
</page>
<table confidence="0.997600142857143">
# Pipeline CE ME UE P R F0.5 ↑
1 RBS &gt; SMTc &gt; LM 372 3045 481 0.4361 0.1088 0.2723
2 RBSc + SMTc &gt; LM 400 3017 485 0.4520 0.1171 0.2875
3 RBS &gt; SMT 476 2941 738 0.3921 0.1393 0.2877
4 RBSc &gt; LM &gt; SMT 471 2946 718 0.3961 0.1378 0.2881
5 RBS &gt; SMT10-best &gt; LM 678 2739 1368 0.3314 0.1984 0.2922
6 RBSc &gt; LM &gt; SMT10-best &gt; LM 681 2736 1366 0.3327 0.1993 0.2934
</table>
<tableCaption confidence="0.918295">
Table 3: Results for different system pipelines on the development set.
</tableCaption>
<table confidence="0.999300666666667">
System CE ME UE P R F0.5
RBSc &gt; LM &gt; SMT10-best &gt; LM 681 2736 1366 0.3327 0.1993 0.2934
RBSc &gt; LM &gt; SMT10-best &gt; LM &gt; Filter 681 2736 1350 0.3353 0.1993 0.2950
</table>
<tableCaption confidence="0.998613">
Table 4: Results for individual systems on the development set.
</tableCaption>
<bodyText confidence="0.9788476">
are handled. In the first case, all corrections are
applied at once whereas in the second, the sug-
gested corrections are used to generate candidates
that are subsequently ranked by our LM, often dis-
carding some of the suggested corrections.
</bodyText>
<subsectionHeader confidence="0.997629">
3.3 Filtering
</subsectionHeader>
<bodyText confidence="0.999927692307692">
As described in Section 2.5, we can evaluate per-
formance by error type in order to identify and re-
move unnecessary corrections. In particular, we
tried to optimise our best hybrid system (#6) by
filtering out types with zero precision. Table 5
shows type-specific performance for this system,
where three zero-precision types can be identi-
fied: Reordering (a subset of Others that we treat
separately), Srun (run-ons/comma splices) and Wa
(acronyms). Although reordering was explicitly
disabled in our SMT system, a translation table
can still include this type of mappings if they are
observed in the training data (e.g. ‘you also can’
→ ‘you can also’).
In order to remove such undesired corrections,
the following procedure was applied: first, in-
dividual corrections were extracted by compar-
ing the original and corrected sentences; second,
the type of each extracted correction was pre-
dicted, subsequently deleting those that matched
unwanted types (i.e. reordering, Srun or Wa); fi-
nally, the set of remaining corrections was applied
to the original text. This method improves pre-
cision while preserving recall (see Table 4), al-
though the resulting improvement is not statisti-
cally significant (paired t-test, p &gt; 0.05).
</bodyText>
<sectionHeader confidence="0.998458" genericHeader="method">
4 Official evaluation results
</sectionHeader>
<bodyText confidence="0.9999845">
Our submission to the CoNLL 2014 shared task is
the result of our best hybrid system, described in
the previous section and summarised in Figure 1.
The official test set comprised 50 new essays (ap-
prox. 30,144 tokens in 1,312 sentences) written in
response to two prompts, one of which was also
included in the training data.
Systems were evaluated using the M2 Scorer,
which uses F0.5 as its overall measure. As in previ-
ous years, there were two evaluation rounds. The
first one was based on the original gold-standard
annotations provided by the shared-task organis-
ers whereas the second was based on a revised
version including alternative annotations submit-
ted by the participating teams. Our submitted sys-
tem achieved the first and second place respec-
tively. The official results of our submission in
both evaluation rounds are reported in Table 6.
</bodyText>
<sectionHeader confidence="0.99663" genericHeader="method">
5 Discussion and error analysis
</sectionHeader>
<bodyText confidence="0.999983571428571">
In order to assess how our system performed per
error type on the test set, we ran our type estima-
tion script and obtained the results shown in Ta-
ble 7. Although these results are estimated and
therefore not completely accurate,4 they can still
provide valuable insights, at least at a coarse level.
The following sections discuss our main findings.
</bodyText>
<subsectionHeader confidence="0.994915">
5.1 Type performance
</subsectionHeader>
<bodyText confidence="0.998677333333333">
According to Table 7, our system achieves the best
performance for types WOadv (adverb/adjective
position) and Wtone (tone), but these results are
</bodyText>
<footnote confidence="0.993565">
4Estimation accuracy was found to be 57.90% on the test
set.
</footnote>
<page confidence="0.989319">
19
</page>
<table confidence="0.999914774193548">
Error type CE ME UE P R F0.5
ArtOrDet 222 465 225 0.4966 0.3231 0.4485
Cit 0 6 0 – 0.0000 –
Mec 31 151 15 0.6739 0.1703 0.4235
Nn 138 256 136 0.5036 0.3503 0.4631
Npos 4 25 45 0.0816 0.1379 0.0889
Others 1 34 12 0.0769 0.0286 0.0575
Pform 1 25 22 0.0435 0.0385 0.0424
Pref 1 38 5 0.1667 0.0256 0.0794
Prep 61 249 177 0.2563 0.1968 0.2417
Reordering 0 1 12 0.0000 0.0000 –
Rloc- 13 115 80 0.1398 0.1016 0.1300
SVA 32 86 25 0.5614 0.2712 0.4624
Sfrag 0 4 0 – 0.0000 –
Smod 0 16 0 – 0.0000 –
Spar 4 30 0 1.0000 0.1176 0.4000
Srun 0 55 28 0.0000 0.0000 –
Ssub 7 64 15 0.3182 0.0986 0.2201
Trans 13 128 36 0.2653 0.0922 0.1929
Um 0 34 0 – 0.0000 –
V0 2 16 3 0.4000 0.1111 0.2632
Vform 28 90 68 0.2917 0.2373 0.2789
Vm 9 86 41 0.1800 0.0947 0.1525
Vt 18 137 53 0.2535 0.1161 0.2050
WOadv 0 12 0 – 0.0000 –
WOinc 2 35 71 0.0274 0.0541 0.0304
Wa 0 5 2 0.0000 0.0000 –
Wci 28 400 241 0.1041 0.0654 0.0931
Wform 65 161 54 0.5462 0.2876 0.4630
Wtone 1 12 0 1.0000 0.0769 0.2941
TOTAL 681 2736 1366 0.3327 0.1993 0.2934
</table>
<tableCaption confidence="0.977874333333333">
Table 5: Type-specific performance of our best hy-
brid system on the development set. Types with
zero precision are marked in bold.
</tableCaption>
<table confidence="0.992138333333333">
Test set CE ME UE P R F0.5
Original 772 1793 1172 0.3971 0.3010 0.3733
Revised 913 1749 1042 0.4670 0.3430 0.4355
</table>
<tableCaption confidence="0.9853075">
Table 6: Official results of our system on the orig-
inal and revised test sets.
</tableCaption>
<bodyText confidence="0.999590705882353">
not truly representative as they only account for a
small fraction of the test data (0.64% and 0.36%
respectively).
The third best performing type is Mec, which
comprises mechanical errors (such as punctuation,
capitalisation and spelling mistakes) and repre-
sents 11.58% of the errors in the data. The remark-
ably high precision obtained for this error type
suggests that our system is especially suitable for
correcting such errors.
We also found that our system was particularly
good at enforcing different types of agreement, as
demonstrated by the results for SVA (subject–verb
agreement), Pref (pronoun reference), Nn (noun
number) and Vform (verb form) types, which add
up to 22.80% of the errors. The following example
shows a successful correction:
</bodyText>
<table confidence="0.999176551724138">
Error type CE ME UE P R F0.5
ArtOrDet 185 192 206 0.4731 0.4907 0.4766
Mec 86 219 16 0.8431 0.2820 0.6031
Nn 122 106 143 0.4604 0.5351 0.4736
Npos 2 13 59 0.0328 0.1333 0.0386
Others 0 30 10 0.0000 0.0000 –
Pform 8 26 21 0.2759 0.2353 0.2667
Pref 19 77 12 0.6129 0.1979 0.4318
Prep 100 159 144 0.4098 0.3861 0.4049
Reordering 0 0 7 0.0000 – –
Rloc- 23 89 116 0.1655 0.2054 0.1722
SVA 38 85 31 0.5507 0.3089 0.4762
Sfrag 0 4 0 – 0.0000 –
Smod 0 2 0 – 0.0000 –
Spar 0 10 0 – 0.0000 –
Srun 0 14 1 0.0000 0.0000 –
Ssub 8 39 19 0.2963 0.1702 0.2581
Trans 17 54 39 0.3036 0.2394 0.2881
Um 2 21 0 1.0000 0.0870 0.3226
V0 8 20 15 0.3478 0.2857 0.3333
Vform 31 93 46 0.4026 0.2500 0.3588
Vm 7 27 35 0.1667 0.2059 0.1733
Vt 26 108 40 0.3939 0.1940 0.3266
WOadv 10 11 0 1.0000 0.4762 0.8197
WOinc 1 33 37 0.0263 0.0294 0.0269
Wci 33 305 146 0.1844 0.0976 0.1565
Wform 42 49 29 0.5915 0.4615 0.5600
Wtone 4 7 0 1.0000 0.3636 0.7407
TOTAL 772 1793 1172 0.3971 0.3010 0.3733
</table>
<tableCaption confidence="0.998374">
Table 7: Type-specific performance of our submit-
</tableCaption>
<equation confidence="0.46321">
ted system on the original test set.
ORIGINAL SENTENCE:
</equation>
<bodyText confidence="0.974893">
He or she has the right not to tell anyone.
</bodyText>
<sectionHeader confidence="0.568144" genericHeader="method">
SYSTEM HYPOTHESIS:
</sectionHeader>
<bodyText confidence="0.986505">
They have the right not to tell anyone.
</bodyText>
<sectionHeader confidence="0.457582" genericHeader="method">
GOLD STANDARD:
</sectionHeader>
<bodyText confidence="0.997512666666667">
They have the right not to tell anyone.
In other cases, our system seems to do a good
job despite gold-standard annotations:
</bodyText>
<sectionHeader confidence="0.644877" genericHeader="method">
ORIGINAL SENTENCE:
</sectionHeader>
<bodyText confidence="0.9723925">
This is because his or her relatives have the
right to know about this.
</bodyText>
<sectionHeader confidence="0.623443" genericHeader="method">
SYSTEM HYPOTHESIS:
</sectionHeader>
<bodyText confidence="0.989353">
This is because their relatives have the right
to know about this.
</bodyText>
<sectionHeader confidence="0.602494" genericHeader="method">
GOLD STANDARD:
</sectionHeader>
<bodyText confidence="0.992186428571429">
This is because his or her relatives have the
right to know about this. (unchanged)
The worst performance is observed for Others
(including Reordering) and Srun, which only ac-
count for 1.69% of the errors. We also note that
Reordering and Srun errors, which had explicitly
been filtered out, still appear in our final results,
</bodyText>
<page confidence="0.987595">
20
</page>
<bodyText confidence="0.999961888888889">
which is due to differences in the edit extraction
algorithms used by the M2 Scorer and our own im-
plementation. According to our estimations, our
system has poor performance on the Wci type (the
second most frequent), suggesting it is not very
successful at correcting idioms and collocations.
Corrections for more complex error types such
as Um (unclear meaning), which are beyond the
scope of this shared task, are inevitably missed.
</bodyText>
<subsectionHeader confidence="0.993315">
5.2 Deletions
</subsectionHeader>
<bodyText confidence="0.9995595">
We have also observed that many mismatches be-
tween our system’s corrections and the gold stan-
dard are caused by unnecessary deletions, as in the
following example:
</bodyText>
<equation confidence="0.563004">
ORIGINAL SENTENCE:
I could understand the feeling of the carrier.
SYSTEM HYPOTHESIS:
I understand the feeling of the carrier.
GOLD STANDARD:
I could understand the feeling of the carrier.
(unchanged)
</equation>
<bodyText confidence="0.998531466666667">
This effect is the result of using 10-best hy-
potheses from the SMT system together with LM
ranking. Hypotheses from an SMT system can in-
clude many malformed sentences which are effec-
tively discarded by the embedded target language
model and additional heuristics. However, rank-
ing these raw hypotheses with external systems
can favour deletions, as language models will gen-
erally assign higher probabilities to shorter sen-
tences. A common remedy for this is normali-
sation but we found it made no difference in our
experiments.
In other cases, deletions can be ascribed to dif-
ferences in the domain of the training and test sets,
as observed in this example:
</bodyText>
<listItem confidence="0.3770375">
ORIGINAL SENTENCE:
Nowadays, social media are able to dissemi-
nate information faster than any other media.
SYSTEM HYPOTHESIS:
Nowadays, the media are able to disseminate
information faster than any other media.
GOLD STANDARD:
Nowadays, social media are able to dissemi-
nate information faster than any other media.
(unchanged)
</listItem>
<subsectionHeader confidence="0.996576">
5.3 Uncredited corrections
</subsectionHeader>
<bodyText confidence="0.99986475">
Our analysis also reveals a number of cases where
the system introduces changes that are not in-
cluded in the gold standard but we consider im-
prove the quality of a sentence. For example:
</bodyText>
<figure confidence="0.684400375">
ORIGINAL SENTENCE:
Demon is not easily to be defeated and it is
required much of energy and psychological
support.
SYSTEM HYPOTHESIS:
Demon is not easily defeated and it requires
a lot of energy and psychological support.
GOLD STANDARD:
</figure>
<figureCaption confidence="0.530193333333333">
The demon is not easily defeated and it re-
quires much energy and psychological sup-
port.
</figureCaption>
<bodyText confidence="0.999828">
Adding alternative corrections to the gold stan-
dard alleviates this problem, although the list of
alternatives will inevitably be incomplete.
There are also a number of cases where the sen-
tences are considered incorrect as part of a longer
text but are acceptable when they are evaluated in
isolation. Consider the following examples:
</bodyText>
<equation confidence="0.893626428571428">
ORIGINAL SENTENCE:
The opposite is also true.
SYSTEM HYPOTHESIS:
The opposite is true.
GOLD STANDARD:
The opposite is also true. (unchanged)
ORIGINAL SENTENCE:
</equation>
<bodyText confidence="0.8959755">
It has erased the boundaries of distance and
time.
</bodyText>
<sectionHeader confidence="0.402587" genericHeader="method">
SYSTEM HYPOTHESIS:
</sectionHeader>
<bodyText confidence="0.987234">
It has erased the boundaries of distance and
time . (unchanged)
</bodyText>
<sectionHeader confidence="0.461557" genericHeader="method">
GOLD STANDARD:
</sectionHeader>
<bodyText confidence="0.99657275">
They have erased the boundaries of distance
and time.
In both cases, system hypotheses are perfectly
grammatical but they are considered incorrect
when analysed in context. Such mismatch is the
result of discrepancies between the annotation and
evaluation criteria: while the gold standard is an-
notated taking discourse into account, system cor-
</bodyText>
<page confidence="0.997945">
21
</page>
<bodyText confidence="0.997481111111111">
rections are proposed in isolation, completely de-
void of discursive context.
Finally, the inability of the M2 Scorer to com-
bine corrections from different annotators (as op-
posed to selecting only one annotator’s corrections
for the whole sentence) can also result in underes-
timations of performance. However, it is clear that
exploring these combinations during evaluation is
a challenging task itself.
</bodyText>
<sectionHeader confidence="0.996659" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999972142857143">
We have presented a hybrid approach to error cor-
rection that combines a rule-based and an SMT
error correction system. We have explored dif-
ferent combination strategies, including sequen-
tial pipelines, candidate generation and ranking.
In addition, we have demonstrated that error type
estimations can be used to filter out unnecessary
corrections and improve precision without harm-
ing recall.
Results of our best hybrid system on the offi-
cial CoNLL 2014 test set yield F0.5=0.3733 for
the original annotations and F0.5=0.4355 for alter-
native corrections, placing our system in the first
and second place respectively.
Error analysis reveals that our system is partic-
ularly good at correcting mechanical errors and
agreement but is often penalised for unnecessary
deletions. However, a thorough inspection shows
that the system tends to produce very fluent sen-
tences, even if they do not match gold standard
annotations.
</bodyText>
<sectionHeader confidence="0.982687" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999689">
We would like to thank Marek Rei for his valuable
feedback and suggestions as well as Cambridge
English Language Assessment, a division of Cam-
bridge Assessment, for supporting this research.
</bodyText>
<sectionHeader confidence="0.998709" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998568434782609">
Øistein E. Andersen, Helen Yannakoudakis, Fiona
Barker, and Tim Parish. 2013. Developing and test-
ing a self-assessment and tutoring system. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications, BEA
2013, pages 32–41, Atlanta, GA, USA, June. Asso-
ciation for Computational Linguistics.
Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal, and
Sivaji Bandyopadhyay. 2011. May I check the
English of your paper!!! In Proceedings of the
Generation Challenges Session at the 13th Euro-
pean Workshop on Natural Language Generation,
pages 250–253, Nancy, France, September. Associ-
ation for Computational Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O’Reilly Media Inc.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL 2012, pages 568–572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.
2011. NUS at the HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 257–259, Nancy, France,
September. Association for Computational Linguis-
tics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, BEA 2013, pages 22–31, Atlanta, Georgia,
USA, June.
Robert Dale and Adam Kilgarriff. 2011. Helping
Our Own: The HOO 2011 Pilot Shared Task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 242–249, Nancy, France,
September. Association for Computational Linguis-
tics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A Report on the Preposition
and Determiner Error Correction Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54–62,
Montr´eal, Canada, June. Association for Computa-
tional Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17–24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of the 9th Annual Conference of the Interna-
tional Speech Communication Association, INTER-
SPEECH 2008, pages 1618–1621, Brisbane, Aus-
tralia, September. ISCA.
Jianfeng Gao, Patrick Nguyen, Xiaolong Li, Chris
Thrasher, Mu Li, and Kuansan Wang. 2010. A
Comparative Study of Bing Web N-gram Language
Models for Web Search and Natural Language Pro-
cessing. In Web N-gram Workshop, Workshop of the
</reference>
<page confidence="0.991054">
22
</page>
<reference confidence="0.991847089285714">
33rd Annual International ACM SIGIR Conference
(SIGIR 2010), pages 16–21, Geneva, Switzerland,
July.
Elitza Ivanova, Delphine Bernhard, and Cyril Grouin.
2011. Handling Outlandish Occurrences: Using
Rules and Lexicons for Correcting NLP Articles. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 254–256, Nancy, France,
September. Association for Computational Linguis-
tics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181–184, Detroit, Michigan, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1 of NAACL ’03, pages 48–54, Edmonton,
Canada. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Prague, Czech Republic.
Association for Computational Linguistics.
Philipp Koehn, 2014. Moses: Statistical Ma-
chine Translation System – User Manual and Code
Guide. University of Edinburgh, April. Available
online at http://www.statmt.org/moses/
manual/manual.pdf.
Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhat-
tacharyya. 2013. IITB System for CoNLL 2013
Shared Task: A Hybrid Approach to Grammati-
cal Error Correction. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 82–87, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104–111, New York City,
USA, June. Association for Computational Linguis-
tics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 632–
641, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 Shared Task on Grammatical Error Correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1–12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 Shared Task
on Grammatical Error Correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task), Baltimore, Maryland, USA, June. As-
sociation for Computational Linguistics. To appear.
Diane Nicholls. 2003. The Cambridge Learner Cor-
pus: Error coding and analysis for lexicography and
ELT. In Dawn Archer, Paul Rayson, Andrew Wil-
son, and Tony McEnery, editors, Proceedings of
the Corpus Linguistics 2003 conference, pages 572–
581, Lancaster, UK. University Centre for Computer
Corpus Research on Language, Lancaster Univer-
sity.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’00, pages 440–447, Hong
Kong, October. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.
Desmond Darma Putra and Lili Szabo. 2013. UdS
at CoNLL 2013 Shared Task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 88–95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 263–266, Nancy, France, September. Associ-
ation for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The University of Illinois
System in the CoNLL-2013 Shared Task. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 13–19, Sofia, Bulgaria, August. Association
for Computational Linguistics.
</reference>
<page confidence="0.970727">
23
</page>
<reference confidence="0.999295432432432">
Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,
Wen Zheng, and Chongqiang Wei. 2013. A hy-
brid model for grammatical error correction. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 115–122, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S.
Chao, and Xiaodong Zeng. 2013. UM-Checker: A
Hybrid System for English Grammatical Error Cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 34–42, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180–189, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL Grammatical Er-
ror Correction Shared Task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 26–33,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52–61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999175">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.529131">
<title confidence="0.997782">Grammatical error correction using hybrid systems and type filtering</title>
<author confidence="0.9247815">Mariano Felice Zheng Yuan Øistein E Andersen Helen Yannakoudakis Ekaterina</author>
<affiliation confidence="0.587937">Computer Laboratory, University of Cambridge, United Kingdom</affiliation>
<abstract confidence="0.998961357142857">This paper describes our submission to the CoNLL 2014 shared task on grammatical error correction using a hybrid approach, which includes both a rule-based and an SMT system augmented by a large webbased language model. Furthermore, we demonstrate that correction type estimation can be used to remove unnecessary corrections, improving precision without harming recall. Our best hybrid system achieves state-of-the-art results, ranking first on the original test set and second on the test set with alternative annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Øistein E Andersen</author>
<author>Helen Yannakoudakis</author>
<author>Fiona Barker</author>
<author>Tim Parish</author>
</authors>
<title>Developing and testing a self-assessment and tutoring system.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, BEA 2013,</booktitle>
<pages>32--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, GA, USA,</location>
<contexts>
<context position="3955" citStr="Andersen et al., 2013" startWordPosition="608" endWordPosition="611">owing sections describe each of these components in detail. 2.1 Rule-based error correction system (RBS) The rule-based system is a component of the SelfAssessment and Tutoring (SAT) system, a web service developed at the University of Cambridge aimed at helping intermediate learners of English 15 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 15–24, Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics Figure 1: Overview of components and interactions in our final hybrid system. in their writing tasks1 (Andersen et al., 2013). The original SAT system provides three main functionalities: 1) text assessment, producing an overall score for a piece of text, 2) sentence evaluation, producing a sentence-level quality score, and 3) word-level feedback, suggesting specific corrections for frequent errors. Since the focus of the shared task is on strict correction (as opposed to detection), we only used the word-level feedback component of the SAT system. This module uses rules automatically derived from the Cambridge Learner Corpus2 (CLC) (Nicholls, 2003) that are aimed at detecting errorful unigrams, bigrams and trigrams</context>
</contexts>
<marker>Andersen, Yannakoudakis, Barker, Parish, 2013</marker>
<rawString>Øistein E. Andersen, Helen Yannakoudakis, Fiona Barker, and Tim Parish. 2013. Developing and testing a self-assessment and tutoring system. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, BEA 2013, pages 32–41, Atlanta, GA, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinaki Bhaskar</author>
<author>Aniruddha Ghosh</author>
<author>Santanu Pal</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>I check the English of your paper!!!</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>250--253</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="2213" citStr="Bhaskar et al., 2011" startWordPosition="330" endWordPosition="333">nts at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 pres</context>
</contexts>
<marker>Bhaskar, Ghosh, Pal, Bandyopadhyay, 2011</marker>
<rawString>Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal, and Sivaji Bandyopadhyay. 2011. May I check the English of your paper!!! In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 250–253, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.</booktitle>
<contexts>
<context position="9068" citStr="Bird et al., 2009" startWordPosition="1412" endWordPosition="1415">et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al., 2009) for consistency with the shared task datasets. 2.3 Candidate generation In order to integrate corrections from multiple systems, we developed a method to generate all the possible corrected versions of a sentence (candidates). Candidates are generated by computing all possible combinations of corrections (irrespective of the system from which they originate), including the original tokens to allow for a ‘no correction’ option. The list of candidates produced for each sentence always includes the original (unmodified) sentence plus any other versions derived from system corrections. In order f</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2012,</booktitle>
<pages>568--572</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="11307" citStr="Dahlmeier and Ng, 2012" startWordPosition="1774" endWordPosition="1777">tives for ranking: 1) using the target LM embedded in our SMT system (described in Section 2.2) and 2) using a large n-gram LM built from web data. In the latter case, we used Microsoft Web N-gram Services, which provide access to large smoothed n-gram language models (with n=2,3,4,5) built from web documents (Gao et al., 2010). All our experiments are based on the 5-gram ‘bing-body:apr10’ model. The ranking performance of these two models was evaluated on the 10-best hypotheses generated by the SMT system for each sentence in the development set. Table 1 shows the results from the M2 Scorer (Dahlmeier and Ng, 2012), the official scorer for the shared task that, unlike previous versions, weights precision twice as much as recall. Results show that using Microsoft’s Web LM yields better performance, which is unsurprising given the vast amounts of data used to build that 17 System CE ME UE P R F0.5 RBS 95 3322 107 0.4703 0.0278 0.1124 SMT 452 2965 690 0.3958 0.1323 0.2830 Table 2: Results of individual systems on the development set. model. For this reason, we adopt Microsoft’s model for all further experiments. We also note that without normalisation, higher probabilities may be assigned to shorter senten</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2012, pages 568–572, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Thanh Phu Tran</author>
</authors>
<title>NUS at the HOO 2011 Pilot Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>257--259</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="2191" citStr="Dahlmeier et al., 2011" startWordPosition="326" endWordPosition="329">tten in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in </context>
</contexts>
<marker>Dahlmeier, Ng, Tran, 2011</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran. 2011. NUS at the HOO 2011 Pilot Shared Task. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 257–259, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications, BEA 2013,</booktitle>
<pages>22--31</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="5964" citStr="Dahlmeier et al., 2013" startWordPosition="916" endWordPosition="919">an and Felice (2013) in order to train an SMT 1The latest version of the system, called ‘Write &amp; Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ system that can ‘translate’ from incorrect into correct English. Our training data comprises a set of different parallel corpora, where the original (incorrect) sentences constitute the source side and corrected versions based on gold standard annotations constitute the target side. These corpora include: • the NUCLE v3.1 corpus (Dahlmeier et al., 2013), containing around 1,400 essays written in English by students at the National University of Singapore (approx. 1,220,257 tokens in 57,152 sentences), • phrase alignments involving corrections extracted automatically from the NUCLE corpus (with up to 7 tokens per side), which are used to boost the probability of phrase alignments that involve corrections so as to improve recall, • the CoNLL 2014 shared task development set, containing 50 essays from the previous year’s test set (approx. 29,207 tokens in 1,382 sentences), • the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications, BEA 2013, pages 22–31, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 Pilot Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>242--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="1241" citStr="Dale and Kilgarriff, 2011" startWordPosition="175" endWordPosition="178">without harming recall. Our best hybrid system achieves state-of-the-art results, ranking first on the original test set and second on the test set with alternative annotations. 1 Introduction Grammatical error correction has attracted considerable interest in the last few years, especially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as deter</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping Our Own: The HOO 2011 Pilot Shared Task. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 242–249, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1317" citStr="Dale et al., 2012" startWordPosition="188" endWordPosition="191">nking first on the original test set and second on the test set with alternative annotations. 1 Introduction Grammatical error correction has attracted considerable interest in the last few years, especially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54–62, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7712" citStr="DeNero and Klein, 2007" startWordPosition="1189" endWordPosition="1192">correct sentences are a subset of the CLC and come from examinations at different proficiency levels. The artificial error generation method aims at replicating frequent error patterns observed in the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). 3Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&amp;view=article&amp;id=4&amp;Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reor</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 17–24, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Annual Conference of the International Speech Communication Association, INTERSPEECH</booktitle>
<pages>1618--1621</pages>
<publisher>ISCA.</publisher>
<location>Brisbane, Australia,</location>
<contexts>
<context position="8520" citStr="Federico et al., 2008" startWordPosition="1325" endWordPosition="1328">hrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reordering model using the alignments created by pialign. The maximum phrase length was set to 7, as recommended in the SMT literature (Koehn et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al., 2009) for consistency with the shared task datasets. 2.3 </context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proceedings of the 9th Annual Conference of the International Speech Communication Association, INTERSPEECH 2008, pages 1618–1621, Brisbane, Australia, September. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Xiaolong Li</author>
<author>Chris Thrasher</author>
<author>Mu Li</author>
<author>Kuansan Wang</author>
</authors>
<title>A Comparative Study of Bing Web N-gram Language Models for Web Search and Natural Language Processing.</title>
<date>2010</date>
<booktitle>In Web N-gram Workshop, Workshop of the 33rd Annual International ACM SIGIR Conference (SIGIR 2010),</booktitle>
<pages>16--21</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="11013" citStr="Gao et al., 2010" startWordPosition="1725" endWordPosition="1728"> same candidate, we only keep one. Figure 2 illustrates the candidate generation process. 2.4 Language model ranking Generated candidates are ranked using a language model (LM), with the most probable candidate being selected as the final corrected version. We tried two different alternatives for ranking: 1) using the target LM embedded in our SMT system (described in Section 2.2) and 2) using a large n-gram LM built from web data. In the latter case, we used Microsoft Web N-gram Services, which provide access to large smoothed n-gram language models (with n=2,3,4,5) built from web documents (Gao et al., 2010). All our experiments are based on the 5-gram ‘bing-body:apr10’ model. The ranking performance of these two models was evaluated on the 10-best hypotheses generated by the SMT system for each sentence in the development set. Table 1 shows the results from the M2 Scorer (Dahlmeier and Ng, 2012), the official scorer for the shared task that, unlike previous versions, weights precision twice as much as recall. Results show that using Microsoft’s Web LM yields better performance, which is unsurprising given the vast amounts of data used to build that 17 System CE ME UE P R F0.5 RBS 95 3322 107 0.4</context>
</contexts>
<marker>Gao, Nguyen, Li, Thrasher, Li, Wang, 2010</marker>
<rawString>Jianfeng Gao, Patrick Nguyen, Xiaolong Li, Chris Thrasher, Mu Li, and Kuansan Wang. 2010. A Comparative Study of Bing Web N-gram Language Models for Web Search and Natural Language Processing. In Web N-gram Workshop, Workshop of the 33rd Annual International ACM SIGIR Conference (SIGIR 2010), pages 16–21, Geneva, Switzerland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elitza Ivanova</author>
<author>Delphine Bernhard</author>
<author>Cyril Grouin</author>
</authors>
<title>Handling Outlandish Occurrences: Using Rules and Lexicons for Correcting NLP Articles.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>254--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="2260" citStr="Ivanova et al., 2011" startWordPosition="338" endWordPosition="341"> all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 share</context>
</contexts>
<marker>Ivanova, Bernhard, Grouin, 2011</marker>
<rawString>Elitza Ivanova, Delphine Bernhard, and Cyril Grouin. 2011. Handling Outlandish Occurrences: Using Rules and Lexicons for Correcting NLP Articles. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 254–256, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume I,</booktitle>
<pages>181--184</pages>
<location>Detroit, Michigan,</location>
<contexts>
<context position="8618" citStr="Kneser and Ney, 1995" startWordPosition="1341" endWordPosition="1344">gn, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reordering model using the alignments created by pialign. The maximum phrase length was set to 7, as recommended in the SMT literature (Koehn et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al., 2009) for consistency with the shared task datasets. 2.3 Candidate generation In order to integrate corrections from multiple systems, we developed a metho</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume I, pages 181–184, Detroit, Michigan, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Edmonton, Canada.</location>
<contexts>
<context position="8462" citStr="Koehn et al., 2003" startWordPosition="1316" endWordPosition="1319">nt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reordering model using the alignments created by pialign. The maximum phrase length was set to 7, as recommended in the SMT literature (Koehn et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al.,</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, volume 1 of NAACL ’03, pages 48–54, Edmonton, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8740" citStr="Koehn et al., 2007" startWordPosition="1362" endWordPosition="1365">hat, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reordering model using the alignments created by pialign. The maximum phrase length was set to 7, as recommended in the SMT literature (Koehn et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al., 2009) for consistency with the shared task datasets. 2.3 Candidate generation In order to integrate corrections from multiple systems, we developed a method to generate all the possible corrected versions of a sentence (candidates). Candidates are generated by computing all po</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Moses: Statistical Machine Translation System – User Manual and Code Guide.</title>
<date>2014</date>
<institution>University of Edinburgh,</institution>
<note>Available online at http://www.statmt.org/moses/ manual/manual.pdf.</note>
<contexts>
<context position="8476" citStr="Koehn, 2014" startWordPosition="1320" endWordPosition="1321">erley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reordering model using the alignments created by pialign. The maximum phrase length was set to 7, as recommended in the SMT literature (Koehn et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al., 2009) for con</context>
</contexts>
<marker>Koehn, 2014</marker>
<rawString>Philipp Koehn, 2014. Moses: Statistical Machine Translation System – User Manual and Code Guide. University of Edinburgh, April. Available online at http://www.statmt.org/moses/ manual/manual.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Kunchukuttan</author>
<author>Ritesh Shah</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>IITB System for CoNLL</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>82--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2355" citStr="Kunchukuttan et al., 2013" startWordPosition="354" endWordPosition="357">ing or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 shared task development set and Section 4 reports our official results on the test set. Finally, we </context>
</contexts>
<marker>Kunchukuttan, Shah, Bhattacharyya, 2013</marker>
<rawString>Anoop Kunchukuttan, Ritesh Shah, and Pushpak Bhattacharyya. 2013. IITB System for CoNLL 2013 Shared Task: A Hybrid Approach to Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 82–87, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="7687" citStr="Liang et al., 2006" startWordPosition="1185" endWordPosition="1188">nces). The original correct sentences are a subset of the CLC and come from examinations at different proficiency levels. The artificial error generation method aims at replicating frequent error patterns observed in the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). 3Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&amp;view=article&amp;id=4&amp;Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>632--641</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7568" citStr="Neubig et al., 2011" startWordPosition="1163" endWordPosition="1166">Profile3 (EVP), which have been modified to include artificially generated errors (approx. 351,517 tokens in 18,830 sentences). The original correct sentences are a subset of the CLC and come from examinations at different proficiency levels. The artificial error generation method aims at replicating frequent error patterns observed in the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). 3Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&amp;view=article&amp;id=4&amp;Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate </context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 632– 641, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<title>The CoNLL2013 Shared Task on Grammatical Error Correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1366" citStr="Ng et al., 2013" startWordPosition="198" endWordPosition="201">the test set with alternative annotations. 1 Introduction Grammatical error correction has attracted considerable interest in the last few years, especially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typ</context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL2013 Shared Task on Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
<author>Christopher Bryant</author>
</authors>
<title>The CoNLL-2014 Shared Task on Grammatical Error Correction.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task),</booktitle>
<location>Baltimore, Maryland, USA,</location>
<note>To appear.</note>
<contexts>
<context position="1513" citStr="Ng et al., 2014" startWordPosition="224" endWordPosition="227">specially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for th</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task), Baltimore, Maryland, USA, June. Association for Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Nicholls</author>
</authors>
<title>The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT.</title>
<date>2003</date>
<booktitle>Proceedings of the Corpus Linguistics 2003 conference,</booktitle>
<pages>572--581</pages>
<editor>In Dawn Archer, Paul Rayson, Andrew Wilson, and Tony McEnery, editors,</editor>
<institution>UK. University Centre for Computer Corpus Research on Language, Lancaster University.</institution>
<location>Lancaster,</location>
<contexts>
<context position="4487" citStr="Nicholls, 2003" startWordPosition="690" endWordPosition="691">actions in our final hybrid system. in their writing tasks1 (Andersen et al., 2013). The original SAT system provides three main functionalities: 1) text assessment, producing an overall score for a piece of text, 2) sentence evaluation, producing a sentence-level quality score, and 3) word-level feedback, suggesting specific corrections for frequent errors. Since the focus of the shared task is on strict correction (as opposed to detection), we only used the word-level feedback component of the SAT system. This module uses rules automatically derived from the Cambridge Learner Corpus2 (CLC) (Nicholls, 2003) that are aimed at detecting errorful unigrams, bigrams and trigrams. In order to ensure high precision, rules are based on n-grams that have been annotated as incorrect at least five times and at least ninety per cent of the times they occur. In addition to these corpus-derived rules, many cases of incorrect but plausible derivational and inflectional morphology are detected by means of rules derived from a machine-readable dictionary. For further details on specific components, we refer the reader to the aforementioned paper. Given an input text, the rule-based system produces an XML file co</context>
</contexts>
<marker>Nicholls, 2003</marker>
<rawString>Diane Nicholls. 2003. The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT. In Dawn Archer, Paul Rayson, Andrew Wilson, and Tony McEnery, editors, Proceedings of the Corpus Linguistics 2003 conference, pages 572– 581, Lancaster, UK. University Centre for Computer Corpus Research on Language, Lancaster University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>440--447</pages>
<institution>Hong Kong, October. Association for Computational Linguistics.</institution>
<contexts>
<context position="7626" citStr="Och and Ney, 2000" startWordPosition="1174" endWordPosition="1177">lly generated errors (approx. 351,517 tokens in 18,830 sentences). The original correct sentences are a subset of the CLC and come from examinations at different proficiency levels. The artificial error generation method aims at replicating frequent error patterns observed in the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). 3Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&amp;view=article&amp;id=4&amp;Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their corr</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 440–447, Hong Kong, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7646" citStr="Och and Ney, 2003" startWordPosition="1178" endWordPosition="1181">s (approx. 351,517 tokens in 18,830 sentences). The original correct sentences are a subset of the CLC and come from examinations at different proficiency levels. The artificial error generation method aims at replicating frequent error patterns observed in the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). 3Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&amp;view=article&amp;id=4&amp;Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal wei</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Darma Putra</author>
<author>Lili Szabo</author>
</authors>
<title>UdS at CoNLL</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>88--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2378" citStr="Putra and Szabo, 2013" startWordPosition="358" endWordPosition="361">requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 shared task development set and Section 4 reports our official results on the test set. Finally, we discuss the performance</context>
</contexts>
<marker>Putra, Szabo, 2013</marker>
<rawString>Desmond Darma Putra and Lili Szabo. 2013. UdS at CoNLL 2013 Shared Task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 88–95, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Joshua Gioja</author>
<author>Dan Roth</author>
</authors>
<title>University of Illinois System in HOO Text Correction Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>263--266</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="2238" citStr="Rozovskaya et al., 2011" startWordPosition="334" endWordPosition="337">iversity of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments usin</context>
</contexts>
<marker>Rozovskaya, Sammons, Gioja, Roth, 2011</marker>
<rawString>Alla Rozovskaya, Mark Sammons, Joshua Gioja, and Dan Roth. 2011. University of Illinois System in HOO Text Correction Shared Task. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 263–266, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Kai-Wei Chang</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>The University of Illinois System in the CoNLL-2013 Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>13--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2285" citStr="Rozovskaya et al., 2013" startWordPosition="342" endWordPosition="345"> greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 shared task development set an</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, and Dan Roth. 2013. The University of Illinois System in the CoNLL-2013 Shared Task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13–19, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Xiang</author>
<author>Bo Yuan</author>
<author>Yaoyun Zhang</author>
<author>Xiaolong Wang</author>
<author>Wen Zheng</author>
<author>Chongqiang Wei</author>
</authors>
<title>A hybrid model for grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>115--122</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2399" citStr="Xiang et al., 2013" startWordPosition="362" endWordPosition="365">ection strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 shared task development set and Section 4 reports our official results on the test set. Finally, we discuss the performance of our system and pr</context>
</contexts>
<marker>Xiang, Yuan, Zhang, Wang, Zheng, Wei, 2013</marker>
<rawString>Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang, Wen Zheng, and Chongqiang Wei. 2013. A hybrid model for grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115–122, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junwen Xing</author>
<author>Longyue Wang</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Xiaodong Zeng</author>
</authors>
<title>UM-Checker: A Hybrid System for English Grammatical Error Correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>34--42</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2328" citStr="Xing et al., 2013" startWordPosition="350" endWordPosition="353">ypes (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 shared task development set and Section 4 reports our official results on</context>
</contexts>
<marker>Xing, Wang, Wong, Chao, Zeng, 2013</marker>
<rawString>Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Xiaodong Zeng. 2013. UM-Checker: A Hybrid System for English Grammatical Error Correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34–42, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A new dataset and method for automatically grading esol texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>180--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6568" citStr="Yannakoudakis et al., 2011" startWordPosition="1014" endWordPosition="1017">Dahlmeier et al., 2013), containing around 1,400 essays written in English by students at the National University of Singapore (approx. 1,220,257 tokens in 57,152 sentences), • phrase alignments involving corrections extracted automatically from the NUCLE corpus (with up to 7 tokens per side), which are used to boost the probability of phrase alignments that involve corrections so as to improve recall, • the CoNLL 2014 shared task development set, containing 50 essays from the previous year’s test set (approx. 29,207 tokens in 1,382 sentences), • the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011), containing 1,244 exam scripts and 2 essays per script (approx. 532,033 tokens in 16,068 sentences), • a subset of the International English Language Testing System (IELTS) examination dataset extracted from the CLC corpus, containing 2,498 exam scripts and 2 essays per script (approx. 1,361,841 tokens in 64,628 sentences), and • a set of sentences from the English Vocabulary Profile3 (EVP), which have been modified to include artificially generated errors (approx. 351,517 tokens in 18,830 sentences). The original correct sentences are a subset of the CLC and come from examinations at differe</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A new dataset and method for automatically grading esol texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 180–189, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ippei Yoshimoto</author>
</authors>
<title>Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, and Yuji</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>26--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Yoshimoto, 2013</marker>
<rawString>Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto. 2013. NAIST at 2013 CoNLL Grammatical Error Correction Shared Task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 26–33, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Yuan</author>
<author>Mariano Felice</author>
</authors>
<title>Constrained grammatical error correction using statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>52--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5361" citStr="Yuan and Felice (2013)" startWordPosition="834" endWordPosition="837">ion to these corpus-derived rules, many cases of incorrect but plausible derivational and inflectional morphology are detected by means of rules derived from a machine-readable dictionary. For further details on specific components, we refer the reader to the aforementioned paper. Given an input text, the rule-based system produces an XML file containing a list of suggested corrections. These corrections can either be applied to the original text or used to generate multiple correction candidates, as described in Section 2.3. 2.2 SMT system We follow a similar approach to the one described by Yuan and Felice (2013) in order to train an SMT 1The latest version of the system, called ‘Write &amp; Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ system that can ‘translate’ from incorrect into correct English. Our training data comprises a set of different parallel corpora, where the original (incorrect) sentences constitute the source side and corrected versions based on gold standard annotations constitute the target side. These corpora include: • the NUCLE v3.1 corpus (Dahlmeier et al., 20</context>
<context position="7366" citStr="Yuan and Felice (2013)" startWordPosition="1143" endWordPosition="1146">amination dataset extracted from the CLC corpus, containing 2,498 exam scripts and 2 essays per script (approx. 1,361,841 tokens in 64,628 sentences), and • a set of sentences from the English Vocabulary Profile3 (EVP), which have been modified to include artificially generated errors (approx. 351,517 tokens in 18,830 sentences). The original correct sentences are a subset of the CLC and come from examinations at different proficiency levels. The artificial error generation method aims at replicating frequent error patterns observed in the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). 3Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&amp;view=article&amp;id=4&amp;Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the fe</context>
<context position="12581" citStr="Yuan and Felice, 2013" startWordPosition="1981" endWordPosition="1984">eletions or skipping insertions. 2.5 Type filtering Analysing performance by error type is very valuable for system development and tuning. However, this can only be performed for corrections in the gold standard (either matched or missed). To estimate types for unnecessary corrections, we defined a set of heuristics that analyse differences in word forms and part-of-speech tags between the original phrases and their system corrections, based on common patterns observed in the training data. We had previously used a similar strategy to classify errors in our CoNLL 2013 shared task submission (Yuan and Felice, 2013) but have now included a few improvements and rules for new types. Estimation accuracy is 50.92% on the training set and 67.57% on the development set, which we consider to be acceptable for our purposes given that the final test set is more similar to the development set. Identifying types for system corrections is not only useful during system development but can also be exploited to filter out and reduce the number of proposed corrections. More specifically, if a system proposes a much higher number of unnecessary corrections than correct suggestions for a specific error type, we can assume</context>
</contexts>
<marker>Yuan, Felice, 2013</marker>
<rawString>Zheng Yuan and Mariano Felice. 2013. Constrained grammatical error correction using statistical machine translation. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 52–61, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
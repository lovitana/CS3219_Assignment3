<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009199">
<title confidence="0.980431">
Feature Norms of German Noun Compounds
</title>
<author confidence="0.999537">
Stephen Roller Sabine Schulte im Walde
</author>
<affiliation confidence="0.998717">
Department of Computer Science Institut f¨ur Maschinelle Sprachverarbeitung
The University of Texas at Austin Universit¨at Stuttgart, Germany
</affiliation>
<email confidence="0.993828">
roller@cs.utexas.edu schulte@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.99378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999679714285714">
This paper presents a new data collection
of feature norms for 572 German noun-
noun compounds. The feature norms com-
plement existing data sets for the same
targets, including compositionality rat-
ings, association norms, and images. We
demonstrate that the feature norms are po-
tentially useful for research on the noun-
noun compounds and their semantic trans-
parency: The feature overlap of the com-
pounds and their constituents correlates
with human ratings on the compound–
constituent degrees of compositionality,
ρ = 0.46.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968604166667">
Feature norms are short descriptions of typical at-
tributes for a set of objects. They often describe
the visual appearance (a firetruck is red), function
or purpose (a cup holds liquid), location (mush-
rooms grow in forests), and relationships between
objects (a cheetah is a cat). The underlying fea-
tures are usually elicited by asking a subject to
carefully describe a cue object, and recording their
responses.
Feature norms have been widely used in psy-
cholinguistic research on conceptual representa-
tions in semantic memory. Prominent collections
have been pursued by McRae et al. (2005) for liv-
ing vs. non-living basic-level concepts; by Vin-
son and Vigliocco (2008) for objects and events;
and by Wu and Barsalou (2009) for noun and noun
phrase objects. In recent years, feature norms have
also acted as a loose proxy for perceptual infor-
mation in data-intensive computational models of
semantic tasks, in order to bridge the gap between
language and the real world (Andrews et al., 2009;
Silberer and Lapata, 2012; Roller and Schulte im
Walde, 2013).
In this paper, we present a new resource of fea-
ture norms for a set of 572 concrete, depictable
German nouns. More specifically, these nouns in-
clude 244 noun-noun compounds and their corre-
sponding constituents. For example, we include
features for ‘Schneeball‘ (‘snowball’), ‘Schnee‘
(‘snow’), and ‘Ball‘ (‘ball’). Table 1 presents
the most prominent features of this example com-
pound and its constituents. Our collection com-
plements existing data sets for the same targets,
including compositionality ratings (von der Heide
and Borgwaldt, 2009); associations (Schulte im
Walde et al., 2012; Schulte im Walde and Borg-
waldt, 2014); and images (Roller and Schulte im
Walde, 2013).
The remainder of this paper details the col-
lection process of the feature norms, discusses
two forms of cleansing and normalization we em-
ployed, and performs quantitative and qualitative
analyses. We find that the normalization proce-
dures improve quality in terms of feature tokens
per feature type, that the normalized feature norms
have a desirable distribution of features per cue,
and that the feature norms are useful in semantic
models to predict compositionality.
</bodyText>
<sectionHeader confidence="0.977467" genericHeader="method">
2 Feature Norm Collection
</sectionHeader>
<bodyText confidence="0.9999375">
We employ Amazon Mechanical Turk (AMT)1 for
data collection. AMT is an online crowdsourc-
ing platform where requesters post small, atomic
tasks which require manual completion by hu-
mans. Workers can complete these tasks, called
HITs, in order to earn a small bounty.
</bodyText>
<subsectionHeader confidence="0.998484">
2.1 Setup and Data
</subsectionHeader>
<bodyText confidence="0.99953625">
Workers were presented with a simple page asking
them to describe the typical attributes of a given
noun. They were explicitly informed in English
that only native German speakers should complete
</bodyText>
<footnote confidence="0.99805">
1http://www.mturk.com
</footnote>
<page confidence="0.984894">
104
</page>
<note confidence="0.4678825">
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 104–108,
Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.966681">
Schneeball ‘snowball’ Schnee ‘snow’ Ball ‘ball’
ist kalt ‘is cold’ 8 ist kalt ‘is cold’ 13 ist rund ‘is round’ 14
ist rund ‘is round’ 7 ist weiß ‘is white’ 13 zum Spielen ‘for playing’ 4
aus Schnee ‘made from snow’ 7 im Winter ‘in the winter’ 6 rollt ‘rolls’ 2
ist weiß ‘is white’ 7 f¨allt ‘falls’ 3 wird geworfen ‘is thrown’ 2
formt man ‘is formed’ 2 schmilzt ‘melts’ 2 ist bunt ‘is colorful’ 2
wirft man ‘is thrown’ 2 hat Flocken ‘has flakes’ 2 Fußball ‘football’ 2
mit den H¨anden ‘with hands’ 2 ist w¨assrig ‘is watery’ 1 Basketball ‘basketball’ 2
</bodyText>
<tableCaption confidence="0.978644">
Table 1: Most frequent features for example compound Schneeball and its constituents.
</tableCaption>
<bodyText confidence="0.999895566666667">
the tasks. All other instructions were given in Ger-
man. Workers were given 7 example features for
the nouns ‘Tisch‘ (‘table’) and ‘Katze‘ (‘cat’), and
instructed to provide typical attributes per noun.
Initially, workers were required to provide 6-10
features per cue and were only paid $0.02 per hit,
but very few workers completed the hits. After
lowering the requirements and increasing the re-
ward, we received many more workers and col-
lected the data more quickly. Workers could also
mark a word as unfamiliar or provide additional
commentary if desired.
We collected responses from September 21,
2012 until January 31, 2013. Workers who were
obvious spammers were rejected and not rewarded
payment. Typically spammers pasted text from
Google, Wikipedia, or the task instructions and
were easy to spot. Users who failed to follow in-
structions (responded in English, did not provide
the minimum number of features, or gave nonsen-
sical responses) were also rejected without pay-
ment. Users who put in a good faith effort and
consistently gave reasonable responses had all of
their responses accepted and rewarded.
In total, 98 different workers completed at least
one accepted hit, but the top 25 workers accounted
for nearly 90% of the responses. We accepted
28,404 different response tokens over 18,996 re-
sponse types for 572 different cues, or roughly 50
features per cue.
</bodyText>
<sectionHeader confidence="0.927229" genericHeader="method">
3 Cleansing and Normalization
</sectionHeader>
<bodyText confidence="0.999860285714286">
We provide two cleaned and normalized versions
of our feature norms.2 In the first version, we cor-
rect primarily orthographic mistakes such as in-
consistent capitalization, spelling errors, and sur-
face usage, but feature norms remain otherwise
unchanged. This version will likely be more useful
to researchers interested in more subtle variations
</bodyText>
<footnote confidence="0.580687333333333">
2The norms can be downloaded from
www.ims.uni-stuttgart.de/forschung/ressourcen/
experiment-daten/feature-norms.en.html.
</footnote>
<bodyText confidence="0.999400909090909">
and distinctions made by the workers.
The second version of our feature norms are
more aggressively normalized, to reduce the quan-
tity of unique and low frequency responses while
maintaining the spirit of the original response. The
resulting data is considerably less sparse than the
orthographically normalized version. This version
is likely to be more useful for research that is
highly affected by sparse data, such as multimodal
experiments (Andrews et al., 2009; Silberer and
Lapata, 2012; Roller and Schulte im Walde, 2013).
</bodyText>
<subsectionHeader confidence="0.998403">
3.1 Orthographic Normalization
</subsectionHeader>
<bodyText confidence="0.991431034482759">
Orthographic normalization is performed in four
automatic passes and one manual pass in the fol-
lowing order:
Letter Case Normalization: Many workers
inconsistently capitalize the first word of feature
norms as though they are writing a complete sen-
tence. For example, ‘ist rund‘ and ‘Ist rund‘ (‘is
round’) were both provided for the cue ‘Ball‘.
We cannot normalize capitalization by simply us-
ing lowercase everywhere, as the first letter of
German nouns should always be capitalized. To
handle the most common instances, we lowercase
the first letter of features that began with articles,
modal verbs, prepositions, conjunctions, or the
high-frequency verbs ‘kommt‘, ‘wird‘, and ‘ist‘.
Umlaut Normalization: The same German
word may sometimes be spelled differently be-
cause some workers use German keyboards
(which have the letters a, o, u, and ß), and oth-
ers use English keyboards (which do not). We
automatically normalize to the umlaut form (i.e.
‘gruen‘ to ‘gr¨un‘, ‘weiss‘ to ‘weiß‘) whenever two
workers gave both versions for the same cue.
Spelling Correction: We automatically correct
common misspellings (such as errecihen →erre-
ichen), using a list from previous collection exper-
iments (Schulte im Walde et al., 2008; Schulte im
Walde et al., 2012). The list was created semi-
automatically, and manually corrected.
</bodyText>
<page confidence="0.996739">
105
</page>
<bodyText confidence="0.9935778">
Usage of ‘ist‘ and ‘hat‘: Workers sometimes
drop the verbs ‘ist‘ (‘is’) and ‘hat‘ (‘has’), e.g. the
worker writes only ‘rund‘ (‘round’) instead of ‘ist
rund‘, or ‘Obst‘ (‘fruit’) instead of ‘hat Obst‘. We
normalize to the ‘ist‘ and ‘hat‘ forms when two
workers gave both versions for the same cue. Note
that we cannot automatically do this across sepa-
rate cues, as the relationship may change: a tree
has fruit, but a banana is fruit.
Manual correction: Following the above auto-
matic normalizations, we manually review all non-
unique responses. In this pass, responses are nor-
malized and corrected with respect to punctuation,
capitalization, spelling, and orthography. Roughly
170 response types are modified in this phase.
</bodyText>
<subsectionHeader confidence="0.997996">
3.2 Variant Normalization
</subsectionHeader>
<bodyText confidence="0.9998182">
The second manual pass consists of more aggres-
sive normalization of expression variants. In this
pass, features are manually edited to minimize the
number of feature types while preserving as much
semantic meaning as possible:
</bodyText>
<listItem confidence="0.998301">
• Replacing plurals with singulars;
• Removing modal verbs, e.g. ‘kann Kunst
sein‘ (‘can be art’) to ‘ist Kunst‘;
• Removing quantifiers and hedges, e.g. ‘ist
meistens blau‘ (‘is mostly blue’) to ‘ist blau‘;
• Splitting into atomic norms, e.g. ‘ist weiß
oder schwarz‘ (‘is white or black’) to ‘ist
weiß‘ and ‘ist schwarz‘, or ‘jagt im Wald‘
(‘hunts in forest’) to ‘jagt‘ and ‘im Wald‘;
• Simplifying verbiage, e.g. ‘ist in der Farbe
schwarz‘ (‘is in the color black’) to ‘ist
schwarz‘.
</listItem>
<bodyText confidence="0.999822">
These selected normalizations are by no means
comprehensive or exhaustive, but do handle a
large portion of the cases. In total, we modify
roughly 5400 tokens over 1300 types.
</bodyText>
<sectionHeader confidence="0.987811" genericHeader="method">
4 Quantitative Analysis
</sectionHeader>
<bodyText confidence="0.999932388888889">
In the following two analyses, we explore the type
and token counts of our feature norms across the
steps in the cleansing process, and analyze the un-
derlying distributions of the features per cues.
Type and Token counts Table 2 shows the to-
ken and type counts for all features in each step
of the cleansing process. We also present the
counts for non-idiosyncratic features, or features
which are provided for at least two distinct cues.
The orthographic normalizations generally lower
the number of total and non-idiosyncratic types,
and increase the number of non-idiosyncratic to-
kens. This indicates we are successfully identify-
ing and correcting many simple orthographic er-
rors, resulting in a less sparse matrix. The nec-
essary amount of manual correction is relatively
low, indicating we are able to catch the majority
of mistakes using simple, automatic methods.
</bodyText>
<table confidence="0.9998683">
Data Version Total Non-idiosyncratic
of Responses
Types Tokens Types Tokens
Raw 18,996 28,404 2,029 10,675
Case 18,848 28,404 2,018 10,801
Umlaut 18,700 28,404 1,967 10,817
Spelling 18,469 28,404 1,981 11,072
ist/hat 18,317 28,404 1,924 11,075
Manual 18,261 28,404 1,889 11,106
Aggressive 17,503 28,739 1,374 11,848
</table>
<tableCaption confidence="0.999858">
Table 2: Counts in the cleansing process.
</tableCaption>
<bodyText confidence="0.996366088235294">
The more aggressively normalized norms are
considerably different than the orthographically
normalized norms. Notably, the number of total
tokens increases from the atomic splits. The data
is also less sparse and more robust, as indicated by
the drops in both total and non-idiosyncratic types.
Furthermore, the number of non-idiosyncratic to-
kens also increases considerably, indicating we
were able to find numerous edge cases and place
them in existing, frequently-used bins.
Number of Features per Cue Another impor-
tant aspect of the data set is the number of features
per cue. An ideal feature norm data set would con-
tain a roughly equal number of (non-idiosyncratic)
features for every cue; if most of the features are
underrepresented, with a majority of the features
lying in only a few cues, then our data set may
only properly represent for these few, heavily rep-
resented cues.
Figure 1 shows the number of features per cue
for (a) all features and (b) the non-idiosyncratic
features, for the aggressively normalized data set.
In the first histogram, we see a clear bimodal dis-
tribution around the number of features per cue.
This is an artifact of the two parts of our collec-
tion process: the shorter, wider distribution corre-
sponds to the first part of collection, where work-
ers gave more responses for less reward. The
taller, skinnier distribution corresponds to the sec-
ond half of collection, when workers were re-
warded more for less work. The second collec-
tion procedure was clearly effective in raising the
number of hits completed, but resulted in fewer
features per cue.
</bodyText>
<page confidence="0.991387">
106
</page>
<figure confidence="0.993099">
80
60
40
20
0
</figure>
<figureCaption confidence="0.999999">
Figure 1: Distribution of features per cue.
</figureCaption>
<bodyText confidence="0.999901090909091">
In the second histogram, we see only the non-
idiosyncratic features for each cue. Unlike the first
histogram, we see only one mode with a relatively
long tail. This indicates that mandating more fea-
tures per worker (as in the first collection process)
often results in more idiosyncratic features, and
not necessarily a stronger representation of each
cue. We also see that roughly 85% of the cues have
at least 9 non-idiosyncratic features each. In sum-
mary, our representations are nicely distributed for
the majority of cues.
</bodyText>
<sectionHeader confidence="0.968892" genericHeader="method">
5 Qualitative Analysis
</sectionHeader>
<bodyText confidence="0.982128695652174">
Our main motivation to collect the feature norms
for the German noun compounds and their con-
stituents was that the features provide insight into
the semantic properties of the compounds and
their constituents and should therefore represent a
valuable resource for cognitive and computational
linguistics research on compositionality. The fol-
lowing two case studies demonstrate that the fea-
ture norms indeed have that potential.
Predicting the Compositionality The first case
study relies on a simple feature overlap measure
to predict the degree of compositionality of the
compound–constituent pairs of nouns: We use the
proportion of shared features of the compound and
a constituent with respect to the total number of
features of the compound. The degree of compo-
sitionality of a compound noun is calculated with
respect to each constituent of the compound.
For example, if a compound noun N0 received a
total of 30 features (tokens), out of which it shares
20 with the first constituent N1 and 10 with the
second constituent N2, the predicted degrees of
compositionality are 20
</bodyText>
<figure confidence="0.321567666666667">
30 = 0.67 for N0–N1, and
10 = 0.33 for N0–N2. The predicted degrees of
30
</figure>
<bodyText confidence="0.989824615384616">
compositionality are compared against the mean
compositionality judgments as collected by von
der Heide and Borgwaldt (2009), using the Spear-
man rank-order correlation coefficient. The result-
ing correlations are ρ = 0.45, p &lt; .000001 for the
standard normalized norms, and ρ = 0.46,p &lt;
.000001 for the aggressively normalized norms,
which we consider a surprisingly successful result
concerning our simple measure. Focusing on the
compound–head pairs, the feature norms reached
ρ = 0.57 and ρ = 0.59, respectively.
Perceptual Model Information As mentioned
in the Introduction, feature norms have also acted
as a loose proxy for perceptual information in
data-intensive computational models of semantic
tasks. The second case study is taken from Roller
and Schulte im Walde (2013), who integrated fea-
ture norms as one type of perceptual informa-
tion into an extension and variations of the LDA
model by Andrews et al. (2009). A bimodal LDA
model integrating textual co-occurrence features
and our feature norms significantly outperformed
the LDA model that only relied on the textual co-
occurrence. The evaluation of the LDA models
was performed on the same compositionality rat-
ings as described in the previous paragraph.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999855266666667">
This paper presented a new collection of feature
norms for 572 German noun-noun compounds.
The feature norms complement existing data sets
for the same targets, including compositionality
ratings, association norms, and images.
We have described our collection process, and
the cleaning and normalization, and we have
shown both the orthographically normalized and
more aggressively normalized feature norms to be
of higher quality than the raw responses in terms
of types per token, and that the normalized feature
norms have a desirable distribution of features per
cue. We also demonstrated by two case studies
that the norms represent a valuable resource for
research on compositionality.
</bodyText>
<figure confidence="0.999739916666667">
0 20 40 60
Features / Cue
Count
75
50
25
0
(b) Non−idiosyncratic Norms
Count
0 25 50 75 100 125
Features / Cue
(a) All Norms
</figure>
<page confidence="0.984678">
107
</page>
<sectionHeader confidence="0.997508" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999930958333334">
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463.
Ken McRae, George S. Cree, Mark S. Seidenberg, and
Chris McNorgan. 2005. Semantic feature pro-
duction norms for a large set of living and nonliv-
ing things. Behavior Research Methods, 37(4):547–
559.
Stephen Roller and Stephen Schulte im Walde. 2013.
A multimodal LDA model integrating textual, cog-
nitive and visual modalities. In Proceedings of
the 2013 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1146–
1157, Seattle, Washington, USA.
Sabine Schulte im Walde and Susanne Borgwaldt.
2014. Association norms for German noun com-
pounds and their constituents. Under review.
Sabine Schulte im Walde, Alissa Melinger, Michael
Roth, and Andrea Weber. 2008. An empirical char-
acterisation of response types in German association
norms. Research on Language and Computation,
6(2):205–238.
Sabine Schulte im Walde, Susanne Borgwaldt, and
Ronny Jauch. 2012. Association norms of German
noun compounds. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 632–639, Istanbul, Turkey.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433, Jeju
Island, Korea.
David Vinson and Gabriella Vigliocco. 2008. Se-
mantic feature production norms for a large set of
objects and events. Behavior Research Methods,
40(1):183–190.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter-, Basis- und Oberbegrif-
fen. Eine explorative Studie. In Proceedings of
the 9th Norddeutsches Linguistisches Kolloquium,
pages 51–74.
Ling-ling Wu and Lawrence W. Barsalou. 2009. Per-
ceptual simulation in conceptual combination: Evi-
dence from property generation. Acta Psychologica,
132:173–189.
</reference>
<page confidence="0.998366">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.873911">
<title confidence="0.99996">Feature Norms of German Noun Compounds</title>
<author confidence="0.999773">Stephen Roller Sabine Schulte im Walde</author>
<affiliation confidence="0.9805505">Department of Computer Science Institut f¨ur Maschinelle Sprachverarbeitung The University of Texas at Austin Universit¨at Stuttgart, Germany</affiliation>
<email confidence="0.950039">roller@cs.utexas.eduschulte@ims.uni-stuttgart.de</email>
<abstract confidence="0.996905071428571">This paper presents a new data collection of feature norms for 572 German nounnoun compounds. The feature norms complement existing data sets for the same targets, including compositionality ratings, association norms, and images. We demonstrate that the feature norms are potentially useful for research on the nounnoun compounds and their semantic transparency: The feature overlap of the compounds and their constituents correlates with human ratings on the compound– constituent degrees of compositionality,</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="1807" citStr="Andrews et al., 2009" startWordPosition="274" endWordPosition="277">ue object, and recording their responses. Feature norms have been widely used in psycholinguistic research on conceptual representations in semantic memory. Prominent collections have been pursued by McRae et al. (2005) for living vs. non-living basic-level concepts; by Vinson and Vigliocco (2008) for objects and events; and by Wu and Barsalou (2009) for noun and noun phrase objects. In recent years, feature norms have also acted as a loose proxy for perceptual information in data-intensive computational models of semantic tasks, in order to bridge the gap between language and the real world (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. For example, we include features for ‘Schneeball‘ (‘snowball’), ‘Schnee‘ (‘snow’), and ‘Ball‘ (‘ball’). Table 1 presents the most prominent features of this example compound and its constituents. Our collection complements existing data sets for the same targets, including compositionality ratings (von der Heide and Borgwa</context>
<context position="6706" citStr="Andrews et al., 2009" startWordPosition="1044" endWordPosition="1047">ested in more subtle variations 2The norms can be downloaded from www.ims.uni-stuttgart.de/forschung/ressourcen/ experiment-daten/feature-norms.en.html. and distinctions made by the workers. The second version of our feature norms are more aggressively normalized, to reduce the quantity of unique and low frequency responses while maintaining the spirit of the original response. The resulting data is considerably less sparse than the orthographically normalized version. This version is likely to be more useful for research that is highly affected by sparse data, such as multimodal experiments (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). 3.1 Orthographic Normalization Orthographic normalization is performed in four automatic passes and one manual pass in the following order: Letter Case Normalization: Many workers inconsistently capitalize the first word of feature norms as though they are writing a complete sentence. For example, ‘ist rund‘ and ‘Ist rund‘ (‘is round’) were both provided for the cue ‘Ball‘. We cannot normalize capitalization by simply using lowercase everywhere, as the first letter of German nouns should always be capitalized. To handle the most </context>
<context position="15301" citStr="Andrews et al. (2009)" startWordPosition="2424" endWordPosition="2427">aggressively normalized norms, which we consider a surprisingly successful result concerning our simple measure. Focusing on the compound–head pairs, the feature norms reached ρ = 0.57 and ρ = 0.59, respectively. Perceptual Model Information As mentioned in the Introduction, feature norms have also acted as a loose proxy for perceptual information in data-intensive computational models of semantic tasks. The second case study is taken from Roller and Schulte im Walde (2013), who integrated feature norms as one type of perceptual information into an extension and variations of the LDA model by Andrews et al. (2009). A bimodal LDA model integrating textual co-occurrence features and our feature norms significantly outperformed the LDA model that only relied on the textual cooccurrence. The evaluation of the LDA models was performed on the same compositionality ratings as described in the previous paragraph. 6 Conclusion This paper presented a new collection of feature norms for 572 German noun-noun compounds. The feature norms complement existing data sets for the same targets, including compositionality ratings, association norms, and images. We have described our collection process, and the cleaning an</context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken McRae</author>
<author>George S Cree</author>
<author>Mark S Seidenberg</author>
<author>Chris McNorgan</author>
</authors>
<title>Semantic feature production norms for a large set of living and nonliving things.</title>
<date>2005</date>
<journal>Behavior Research Methods,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>559</pages>
<contexts>
<context position="1406" citStr="McRae et al. (2005)" startWordPosition="206" endWordPosition="209">ty, ρ = 0.46. 1 Introduction Feature norms are short descriptions of typical attributes for a set of objects. They often describe the visual appearance (a firetruck is red), function or purpose (a cup holds liquid), location (mushrooms grow in forests), and relationships between objects (a cheetah is a cat). The underlying features are usually elicited by asking a subject to carefully describe a cue object, and recording their responses. Feature norms have been widely used in psycholinguistic research on conceptual representations in semantic memory. Prominent collections have been pursued by McRae et al. (2005) for living vs. non-living basic-level concepts; by Vinson and Vigliocco (2008) for objects and events; and by Wu and Barsalou (2009) for noun and noun phrase objects. In recent years, feature norms have also acted as a loose proxy for perceptual information in data-intensive computational models of semantic tasks, in order to bridge the gap between language and the real world (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these</context>
</contexts>
<marker>McRae, Cree, Seidenberg, McNorgan, 2005</marker>
<rawString>Ken McRae, George S. Cree, Mark S. Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for a large set of living and nonliving things. Behavior Research Methods, 37(4):547– 559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Stephen Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1146--1157</pages>
<location>Seattle, Washington, USA.</location>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Stephen Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In Proceedings of the 2013 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1146– 1157, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Susanne Borgwaldt</author>
</authors>
<title>Association norms for German noun compounds and their constituents. Under review.</title>
<date>2014</date>
<contexts>
<context position="2501" citStr="Walde and Borgwaldt, 2014" startWordPosition="380" endWordPosition="384">this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. For example, we include features for ‘Schneeball‘ (‘snowball’), ‘Schnee‘ (‘snow’), and ‘Ball‘ (‘ball’). Table 1 presents the most prominent features of this example compound and its constituents. Our collection complements existing data sets for the same targets, including compositionality ratings (von der Heide and Borgwaldt, 2009); associations (Schulte im Walde et al., 2012; Schulte im Walde and Borgwaldt, 2014); and images (Roller and Schulte im Walde, 2013). The remainder of this paper details the collection process of the feature norms, discusses two forms of cleansing and normalization we employed, and performs quantitative and qualitative analyses. We find that the normalization procedures improve quality in terms of feature tokens per feature type, that the normalized feature norms have a desirable distribution of features per cue, and that the feature norms are useful in semantic models to predict compositionality. 2 Feature Norm Collection We employ Amazon Mechanical Turk (AMT)1 for data coll</context>
</contexts>
<marker>Walde, Borgwaldt, 2014</marker>
<rawString>Sabine Schulte im Walde and Susanne Borgwaldt. 2014. Association norms for German noun compounds and their constituents. Under review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Alissa Melinger</author>
<author>Michael Roth</author>
<author>Andrea Weber</author>
</authors>
<title>An empirical characterisation of response types in German association norms.</title>
<date>2008</date>
<journal>Research on Language and Computation,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="8028" citStr="Walde et al., 2008" startWordPosition="1248" endWordPosition="1251">sitions, conjunctions, or the high-frequency verbs ‘kommt‘, ‘wird‘, and ‘ist‘. Umlaut Normalization: The same German word may sometimes be spelled differently because some workers use German keyboards (which have the letters a, o, u, and ß), and others use English keyboards (which do not). We automatically normalize to the umlaut form (i.e. ‘gruen‘ to ‘gr¨un‘, ‘weiss‘ to ‘weiß‘) whenever two workers gave both versions for the same cue. Spelling Correction: We automatically correct common misspellings (such as errecihen →erreichen), using a list from previous collection experiments (Schulte im Walde et al., 2008; Schulte im Walde et al., 2012). The list was created semiautomatically, and manually corrected. 105 Usage of ‘ist‘ and ‘hat‘: Workers sometimes drop the verbs ‘ist‘ (‘is’) and ‘hat‘ (‘has’), e.g. the worker writes only ‘rund‘ (‘round’) instead of ‘ist rund‘, or ‘Obst‘ (‘fruit’) instead of ‘hat Obst‘. We normalize to the ‘ist‘ and ‘hat‘ forms when two workers gave both versions for the same cue. Note that we cannot automatically do this across separate cues, as the relationship may change: a tree has fruit, but a banana is fruit. Manual correction: Following the above automatic normalizations</context>
</contexts>
<marker>Walde, Melinger, Roth, Weber, 2008</marker>
<rawString>Sabine Schulte im Walde, Alissa Melinger, Michael Roth, and Andrea Weber. 2008. An empirical characterisation of response types in German association norms. Research on Language and Computation, 6(2):205–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Susanne Borgwaldt</author>
<author>Ronny Jauch</author>
</authors>
<title>Association norms of German noun compounds.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation,</booktitle>
<pages>632--639</pages>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="2462" citStr="Walde et al., 2012" startWordPosition="374" endWordPosition="377">nd Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. For example, we include features for ‘Schneeball‘ (‘snowball’), ‘Schnee‘ (‘snow’), and ‘Ball‘ (‘ball’). Table 1 presents the most prominent features of this example compound and its constituents. Our collection complements existing data sets for the same targets, including compositionality ratings (von der Heide and Borgwaldt, 2009); associations (Schulte im Walde et al., 2012; Schulte im Walde and Borgwaldt, 2014); and images (Roller and Schulte im Walde, 2013). The remainder of this paper details the collection process of the feature norms, discusses two forms of cleansing and normalization we employed, and performs quantitative and qualitative analyses. We find that the normalization procedures improve quality in terms of feature tokens per feature type, that the normalized feature norms have a desirable distribution of features per cue, and that the feature norms are useful in semantic models to predict compositionality. 2 Feature Norm Collection We employ Amaz</context>
<context position="8060" citStr="Walde et al., 2012" startWordPosition="1254" endWordPosition="1257">igh-frequency verbs ‘kommt‘, ‘wird‘, and ‘ist‘. Umlaut Normalization: The same German word may sometimes be spelled differently because some workers use German keyboards (which have the letters a, o, u, and ß), and others use English keyboards (which do not). We automatically normalize to the umlaut form (i.e. ‘gruen‘ to ‘gr¨un‘, ‘weiss‘ to ‘weiß‘) whenever two workers gave both versions for the same cue. Spelling Correction: We automatically correct common misspellings (such as errecihen →erreichen), using a list from previous collection experiments (Schulte im Walde et al., 2008; Schulte im Walde et al., 2012). The list was created semiautomatically, and manually corrected. 105 Usage of ‘ist‘ and ‘hat‘: Workers sometimes drop the verbs ‘ist‘ (‘is’) and ‘hat‘ (‘has’), e.g. the worker writes only ‘rund‘ (‘round’) instead of ‘ist rund‘, or ‘Obst‘ (‘fruit’) instead of ‘hat Obst‘. We normalize to the ‘ist‘ and ‘hat‘ forms when two workers gave both versions for the same cue. Note that we cannot automatically do this across separate cues, as the relationship may change: a tree has fruit, but a banana is fruit. Manual correction: Following the above automatic normalizations, we manually review all nonuniq</context>
</contexts>
<marker>Walde, Borgwaldt, Jauch, 2012</marker>
<rawString>Sabine Schulte im Walde, Susanne Borgwaldt, and Ronny Jauch. 2012. Association norms of German noun compounds. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 632–639, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1834" citStr="Silberer and Lapata, 2012" startWordPosition="278" endWordPosition="281">ng their responses. Feature norms have been widely used in psycholinguistic research on conceptual representations in semantic memory. Prominent collections have been pursued by McRae et al. (2005) for living vs. non-living basic-level concepts; by Vinson and Vigliocco (2008) for objects and events; and by Wu and Barsalou (2009) for noun and noun phrase objects. In recent years, feature norms have also acted as a loose proxy for perceptual information in data-intensive computational models of semantic tasks, in order to bridge the gap between language and the real world (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. For example, we include features for ‘Schneeball‘ (‘snowball’), ‘Schnee‘ (‘snow’), and ‘Ball‘ (‘ball’). Table 1 presents the most prominent features of this example compound and its constituents. Our collection complements existing data sets for the same targets, including compositionality ratings (von der Heide and Borgwaldt, 2009); associations (S</context>
<context position="6733" citStr="Silberer and Lapata, 2012" startWordPosition="1048" endWordPosition="1051">ariations 2The norms can be downloaded from www.ims.uni-stuttgart.de/forschung/ressourcen/ experiment-daten/feature-norms.en.html. and distinctions made by the workers. The second version of our feature norms are more aggressively normalized, to reduce the quantity of unique and low frequency responses while maintaining the spirit of the original response. The resulting data is considerably less sparse than the orthographically normalized version. This version is likely to be more useful for research that is highly affected by sparse data, such as multimodal experiments (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). 3.1 Orthographic Normalization Orthographic normalization is performed in four automatic passes and one manual pass in the following order: Letter Case Normalization: Many workers inconsistently capitalize the first word of feature norms as though they are writing a complete sentence. For example, ‘ist rund‘ and ‘Ist rund‘ (‘is round’) were both provided for the cue ‘Ball‘. We cannot normalize capitalization by simply using lowercase everywhere, as the first letter of German nouns should always be capitalized. To handle the most common instances, we lowerc</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vinson</author>
<author>Gabriella Vigliocco</author>
</authors>
<title>Semantic feature production norms for a large set of objects and events.</title>
<date>2008</date>
<journal>Behavior Research Methods,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="1485" citStr="Vinson and Vigliocco (2008)" startWordPosition="218" endWordPosition="222">pical attributes for a set of objects. They often describe the visual appearance (a firetruck is red), function or purpose (a cup holds liquid), location (mushrooms grow in forests), and relationships between objects (a cheetah is a cat). The underlying features are usually elicited by asking a subject to carefully describe a cue object, and recording their responses. Feature norms have been widely used in psycholinguistic research on conceptual representations in semantic memory. Prominent collections have been pursued by McRae et al. (2005) for living vs. non-living basic-level concepts; by Vinson and Vigliocco (2008) for objects and events; and by Wu and Barsalou (2009) for noun and noun phrase objects. In recent years, feature norms have also acted as a loose proxy for perceptual information in data-intensive computational models of semantic tasks, in order to bridge the gap between language and the real world (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. Fo</context>
</contexts>
<marker>Vinson, Vigliocco, 2008</marker>
<rawString>David Vinson and Gabriella Vigliocco. 2008. Semantic feature production norms for a large set of objects and events. Behavior Research Methods, 40(1):183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia von der Heide</author>
<author>Susanne Borgwaldt</author>
</authors>
<title>Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium,</booktitle>
<pages>51--74</pages>
<marker>von der Heide, Borgwaldt, 2009</marker>
<rawString>Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter-, Basis- und Oberbegriffen. Eine explorative Studie. In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling-ling Wu</author>
<author>Lawrence W Barsalou</author>
</authors>
<title>Perceptual simulation in conceptual combination: Evidence from property generation. Acta Psychologica,</title>
<date>2009</date>
<pages>132--173</pages>
<contexts>
<context position="1539" citStr="Wu and Barsalou (2009)" startWordPosition="229" endWordPosition="232">the visual appearance (a firetruck is red), function or purpose (a cup holds liquid), location (mushrooms grow in forests), and relationships between objects (a cheetah is a cat). The underlying features are usually elicited by asking a subject to carefully describe a cue object, and recording their responses. Feature norms have been widely used in psycholinguistic research on conceptual representations in semantic memory. Prominent collections have been pursued by McRae et al. (2005) for living vs. non-living basic-level concepts; by Vinson and Vigliocco (2008) for objects and events; and by Wu and Barsalou (2009) for noun and noun phrase objects. In recent years, feature norms have also acted as a loose proxy for perceptual information in data-intensive computational models of semantic tasks, in order to bridge the gap between language and the real world (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). In this paper, we present a new resource of feature norms for a set of 572 concrete, depictable German nouns. More specifically, these nouns include 244 noun-noun compounds and their corresponding constituents. For example, we include features for ‘Schneeball‘ (‘snow</context>
</contexts>
<marker>Wu, Barsalou, 2009</marker>
<rawString>Ling-ling Wu and Lawrence W. Barsalou. 2009. Perceptual simulation in conceptual combination: Evidence from property generation. Acta Psychologica, 132:173–189.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000275">
<title confidence="0.995801">
Leveraging Known Semantics for Spelling Correction
</title>
<author confidence="0.994579">
Levi King and Markus Dickinson
</author>
<affiliation confidence="0.779043">
Indiana University
Bloomington, IN USA
</affiliation>
<email confidence="0.996017">
leviking@indiana.edu,md7@indiana.edu
</email>
<sectionHeader confidence="0.98906" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.986517727272727">
Focusing on applications for analyzing learner language which evaluate semantic appropriate-
ness and accuracy, we build from previous work which modeled some aspects of interaction,
namely a picture description task (PDT), with the goal of integrating a spelling correction com-
ponent in this context. After parsing a sentence and extracting semantic relations, a surprising
number of analysis failures stem from misspellings, deviating from expected input in ways that
can be modeled when the content of the interaction is known. We thus explore the use of
spelling correction tools and language modeling to correct misspellings that often lead to errors
in obtaining semantic forms, and we show that such tools can significantly reduce the number
of unanalyzable cases. The work is useful for any context where image descriptions or some
expected content is available, but not necessarily expected linguistic forms.
KEYWORDS: picture description task, semantic analysis, spelling correction, language modeling.
</bodyText>
<note confidence="0.986944666666667">
Levi King and Markus Dickinson 2014. Leveraging known semantics for spelling correction. Proceedings of the third
workshop on NLP for computer-assisted language learning. NEALT Proceedings Series 22 / Linköping Electronic Conference
Proceedings 107: 43–58.
</note>
<page confidence="0.999695">
43
</page>
<sectionHeader confidence="0.972905" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.9964703125">
Much current work on analyzing learner language focuses on grammatical error detection and
correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent Computer-
Assisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift
and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception
to this rule is Herr Komissar, an ILT for German learners that includes rather robust content
analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built
tools and does not connect to modern NLP. Some work addresses content assessment for short
answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more
conversational interactions (see Petersen, 2010). Such interactions are both more and less
difficult to process: to provide feedback requires keeping track of the content of the interaction,
but such content can also be used to disambiguate new learner productions. We exploit this
tension in the context of spelling correction, as semantic information severely restricts the
learner’s expected content, and thus also their word forms.
Since our overarching goal is to move towards the facilitation of ILTs and language assessment
tools that maximize free interaction, we have to deal with removing impediments to interaction.
Given the preponderance of spelling errors in learner data, and specifically interactive data
(King and Dickinson, 2013), our specific goal is to use basic NLP (pre)processing—namely,
language modeling for spelling correction—to make the meaning of a learner’s sentence clearer.
We examine methods for automatically correcting misspellings, showing that preprocessing
with spelling correction tools, when information about the interactive context is known (i.e.,
the picture’s description), can greatly reduce downstream errors.
This may seem like a niche problem, but: 1) spelling errors are generally a major problem
in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we
have right now, on picture description tasks (PDTs), connects not only with a desire for more
interactive tools, but also for language assessment (Somasundaran and Chodorow, 2014); and
3) our work seeks to unpack the connection between relatively “shallow” errors, namely spelling
errors, with “deeper” errors, namely semantic ones. Unlike, for example, linguistic abstractions
such as part-of-speech, both are intimately rooted in the particular lexical items used. This then
raises the question of whether we are modeling what the learner said (modulo some spelling
variation), what the learner intended, or what the learner should have intended, an issue we
take up in section 4, after covering the background in section 3. The methods are covered in
section 5 and the evaluation in section 6.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999889272727273">
Research into the patterns of spelling errors particular to native speakers (NSs) and non-native
speakers (NNSs) highlights the challenge of applying spelling correction techniques to non-
native text. Flor et al. (2013) examined spelling errors found in the ETS Spelling Corpus (3000
GRE and TOEFL essays) and found that NNS spelling errors were more severe (i.e., had a
greater edit distance from the intended word) than NS errors. Moreover, NNSs made more
spelling errors than NSs for words of 3-7 letters, but this trend reversed for words of 8 letters or
more. These effects were shown to disappear among the most proficient NNSs in the sample,
however. Similarly, Hovermale (2010) compared the spelling errors in corpora of Japanese
learners of English to previous studies of NS spelling errors and found that the learner errors
have a greater average edit distance and are nearly twice as likely to involve the first letter of
the word. Given such variability in form, correcting spelling errors for NNSs strictly via edit
</bodyText>
<page confidence="0.998327">
44
</page>
<bodyText confidence="0.990188954545455">
distance operations would thus seem to have its limits.
Using the ETS Spelling Corpus and the ConSpell spelling correction tool, Flor (2012) demon-
strates significant gains in automatic spelling correction when modules using contextual infor-
mation are added. Four types of context, each of which benefitted spelling correction, were
explored: 1) word n-grams (length 1–5) and a web-scale language model (LM); 2) word n-
grams and the positive normalized pointwise mutual information (PNPMI) of the words within
them (based on a web-scale distributional model); 3) the entire essay (and the recurrence or
lack of a given candidate spelling correction in the essay); and 4) the text of the essay prompt.
Notably, a 3.8% improvement comes through the use of “global mutual optimization”, i.e., at
each given spelling correction decision, the module is biased not only toward other words in the
text, but also the candidate spelling lists of these other words. The work presents a strong case
for the use of n-grams with both LMs and PNPMI, as the best results come from this setting,
boosting performance 11.48% above the non-contextual spelling correction baseline.
Flor and Futagi (2012) further examine the use of context for correcting learner misspellings
and claim that three major issues contribute to the task’s difficulty: “local error density” (a
misspelled word near other misspellings) weakens n-gram approaches; poor grammar can lead
to the selection of an incorrect spelling candidate based on its agreement with nearby incorrect
words; and competition among closely related spelling candidates can lead to the selection
of an incorrect inflectional variant. These challenges indicate that for potentially error-rich
learner sentences, sentence or n-gram level contexts may be more effective when combined
with higher-level contextual information, such as task prompts and discourse-level information
about verb inflections. We explore including information about picture content.
</bodyText>
<sectionHeader confidence="0.986463" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.861958">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9538148">
In previous work (King and Dickinson, 2013),
we collected responses to a picture descrip-
tion (PDT) task to approximate interactive
behavior. The current study relies on the
same set of responses. We use a PDT be-
cause it helps constrain both form and con-
tent, without providing textual prompts that
may influence a learner. Moreover, PDTs are a
well-established tool in areas of study ranging
from SLA to Alzheimer’s disease (Ellis, 2000;
Forbes-McKay and Venneri, 2005). The use
of visual stimuli also helps model the visual
nature of online games. The stimuli are cho-
sen to elicit relatively unambiguous transitive
sentences.
Response (L1)
The man killing the beard. (Arabic)
The PDT consisted of 10 items (8 line draw-
ings and 2 photographs) intended to elicit a
single sentence each; an example is given in
Figure 1. Participants were asked to view the
image and describe the action in a complete
A man is shutting a bird. (Chinese)
A man is shooting a bird. (English)
The man shouted the bird. (Spanish)
</bodyText>
<figureCaption confidence="0.994544">
Figure 1: Example item and responses
</figureCaption>
<page confidence="0.998359">
45
</page>
<bodyText confidence="0.999916666666667">
sentence, with any tense or aspect appropriate. 25 of the 39 non-native speaker (NNS) par-
ticipants performed the task in a setting where automatic spell checking was disabled; the
remaining 14 performed the task online on their own computers, and although they were
instructed to disable spell checking, we have no way of knowing if they did so.
The NNSs were intermediate and upper-level adult English learners in an intensive English as
a Second Language program at Indiana University. This data set contains responses from 53
informants, including native speakers (NSs) (14 NSs, 39 NNSs), for a total of 530 sentences.
The distribution of first languages (L1s) is: 16 Arabic, 7 Chinese, 14 English, 2 Japanese, 4
Korean, 1 Kurdish, 1 Polish, 2 Portuguese, and 6 Spanish.
</bodyText>
<subsectionHeader confidence="0.999833">
3.2 Method
</subsectionHeader>
<bodyText confidence="0.998985818181818">
As in King and Dickinson (2013), our method to obtain a semantic form from a NNS production
takes two steps: 1) obtain a syntactic dependency representation from the off-the-shelf Stanford
parser (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form
from the parse, via a small set of hand-written rules. To illustrate this process, consider (1). This
sentence is passed through the parser to obtain the dependency parse shown in Figure 2. Based
on the presence of the nsubjpass (noun subject, passive) node, the extraction script takes the
logical subject from under the agent label, the verb from root, and the logical object from
nsubjpass. This results in the semantic triple shot(man,bird), lemmatized to shoot(man,bird),
using the Stanford CoreNLP lemmatizer (Manning et al., 2014). Very little effort is needed: the
parser is pre-built; the decision tree is small; and the extraction rules are minimal. Note, too,
that certain relations (e.g., det) are completely ignored in the extraction.
</bodyText>
<figure confidence="0.888343666666667">
(1) A bird is shot by a man.
root
vroot A bird is shot by a man
</figure>
<figureCaption confidence="0.999798">
Figure 2: The dependency parse of (1)
</figureCaption>
<bodyText confidence="0.999921846153846">
One is able to use little effort in part due to the constraints in the pictures. For figure 1, for
example, the artist, the man in the beret, and the man are all acceptable subjects, whereas if
there were multiple men in the picture, the man would not be specific enough.
Evaluation in King and Dickinson (2013) addresses two major questions. First, how accurate is
the extraction of semantic information from potentially innovative sentences? Secondly, how
much coverage does one have in a gold standard of semantic forms (triples), to capture the
variability in meaning in learner sentences? We focus more on the first question and again use
native speaker semantic forms as a proxy for a gold standard—albeit, limited by mismatches
between native and (correct) non-native ways of saying the same thing. To mitigate this and
better see the effect of spelling correction, much of our evaluation relies on hand-analysis which
determines whether a “reasonable gold standard” could contain the information (see section 4).
Semantic extraction For the purpose of evaluating an extraction system, King and Dickinson
(2013) define two major classes of errors. The first are triple errors, responses for which the
</bodyText>
<figure confidence="0.7917454">
det
nsubjpass
auxpass
agent
det
</figure>
<page confidence="0.998935">
46
</page>
<bodyText confidence="0.988826833333333">
system fails to extract one or more of the desired subject, verb, or object, based on the sentence
at hand and without regard to the target content. Second are content errors, responses for
which the system extracts the desired subject, verb and object, but the resulting triple does not
accurately describe the image (i.e., is an error of the participant’s). In this paper, we focus on
reducing the triple errors, i.e., system errors. For example, the spelling error in (2) leads to a
completely incorrect triple. We will unpack our error types in section 4.
</bodyText>
<listItem confidence="0.863841">
(2) A man swipped leaves. ⇒ leave(swipped,man)
</listItem>
<bodyText confidence="0.999436444444444">
Focusing on triple (system) errors, we have obtained 92.3% accuracy on extraction for NNS
data and roughly the same for NS data, 92.9% (King and Dickinson, 2013). Furthermore, more
than half of the errors for NNSs involve misspellings (4.1% of the total 7.7% of errors). For a
system interacting with learners, spelling errors are thus a high priority (cf. Hovermale, 2008).
Content errors are subcategorized as spelling or meaning errors, depending on whether the
resulting triple has spelling errors that do not result in real words—as in (3)—or that do result
in real but unintended words and thus convey an inappropriate meaning (e.g., shout(man,bird)
instead of shoot(man,bird)). We will see this distinction play out in the spelling correction
techniques in section 5.
</bodyText>
<listItem confidence="0.595472">
(3) The artiest is drawing a portret. ⇒ drawing(artiest,portret)
</listItem>
<bodyText confidence="0.998992733333333">
Approximately 15% of NNS triples are content errors (King and Dickinson, 2013). These cases
are ones for which the learner needs feedback, but there are two barriers in providing feedback:
1) without fixing the triple errors, they will be automatically grouped into the content error
cases, since they do not match the gold standard; and 2) even if one knows something is an
error, to obtain feedback one would ideally know the target the learner was (or should have
been) aiming for. Our approach to spelling correction addresses both of these concerns by
cleaning up the misspelled cases—including many of the “content” errors rooted in misspellings.
Semantic coverage In King and Dickinson (2013), we take a set of native speaker (NS)
responses for the same PDTs as the gold standard, garnering coverage numbers around 25%
for types and 50% for tokens—i.e., about half of correct NNS responses are not in the gold
standard. Since our focus is on improving accuracy, we use the same gold standard, but
augment the analysis with hand-evaluation of whether a response should have been in the gold
standard (section 4). Still, with spelling modifications being made to make a NNS response
more native-like, we may be able to increase coverage, i.e., to find a (gold) NS triple that
matches.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="method">
4 Error Types
</sectionHeader>
<bodyText confidence="0.999910571428571">
As alluded to above, our goal is to model a close intended meaning of every NNS sentence, in
order to provide a platform for providing feedback. By close intended meaning, we mean, a
meaning that matches some correct answer and whose corresponding form is a reasonable distance
to what the NNS wrote. Since we have not interviewed participants with follow-up questions
about the intention of their responses and cannot assume a follow-up in the general case,
we take the close intended meaning as the meaning they should have intended, given their
production.
</bodyText>
<page confidence="0.996042">
47
</page>
<bodyText confidence="0.9998092">
For evaluation, then, we want to measure the extent to which we are able to take a NNS form
and produce a plausible target meaning for their “intention,” i.e., a viable semantic triple. The
evaluation should answer: 1) Is there a valid meaning? and, if not, 2) what step in the process
prevented a valid meaning from being derived?
The entire system is outlined in section 5.2 (see Figure 3), but essentially we have this pipeline:
</bodyText>
<listItem confidence="0.9960686">
0. Sentence produced by NNS
1. (optional) Spelling corrector to generate close intended form
2. Syntactic parser to obtain word-word relations
3. Semantic extractor to obtain semantic forms
4. Comparison to gold standard
</listItem>
<bodyText confidence="0.629963">
Starting with the gold standard comparison and working backwards, we evaluate at every step:
</bodyText>
<listItem confidence="0.938391833333333">
1. Is the triple covered by the gold standard? Yes: Not an error. No: Continue to next step.
2. Should the triple be covered by a reasonable gold standard? Yes: Gold miss (“error”). No:
Continue to next step.
• Given the limited coverage of the NS gold standard, we add this manual step, so as
not to focus too much on one particular gold standard.
3. Is the form (either the NNS sentence or the close intended meaning chosen by the
</listItem>
<bodyText confidence="0.997556636363636">
correction module) well-formed and appropriate for the item but the extracted triple
is not covered by a reasonable gold standard? Yes: Triple error. (These could be
subcategorized as parser, lemmatizer, or extractor errors.) No: Form error.
Note that for our purposes here, a “good” triple should indicate an appropriate subject, verb
and object, whether directly or indirectly. In most cases, this is a complete V(S,O) triple.
For some concepts, however, a verb may imply its object, or vice versa. Item 3 of the PDT,
for example, shows a woman riding a bicycle. This could be represented as a transitive
action, resulting in a triple like ride(woman,bicycle). However, this could also be construed
as an intransitive, such as cycle(woman,NONE). Both of these triples should be considered
appropriate. A form like A woman is on a bicycle should also be considered appropriate, because
the obvious action involving a person on a bicycle is ride (or pedal, travel, etc.), even though the
extracted triple (be(woman,bicycle)) is less descriptive. An intransitive resulting in a triple like
ride(woman,NONE) is inadequate, however, because ride does not sufficiently imply the object.
Similar cases occur among the responses to item 9, which shows two boys rowing a boat. We
might consider row(boy,boat) to be an ideal triple for this item, but we also accept be(boy,boat)
here, as in Two boys are on a boat. Note that this is only acceptable because in the absence of
more detail, a reasonable person given the information that some human is performing some
action on or involving a boat would likely assume that the action involves using the boat for
its intended purpose—to travel on water, and that could be represented with a more specific
verb. We should also accept row(boy,NONE) here, because (unlike ride) the verb row sufficiently
implies its object (a boat). Similarly, boat(boy,NONE) is adequate, because as a verb, boat
indicates both the presence of a boat and the action of riding the boat.
</bodyText>
<page confidence="0.998469">
48
</page>
<sectionHeader confidence="0.989467" genericHeader="method">
5 Spelling Correction Modifications
</sectionHeader>
<subsectionHeader confidence="0.948557">
5.1 Motivation for spelling correction via language modeling
</subsectionHeader>
<bodyText confidence="0.999992">
The initial approach to this task in King and Dickinson (2013) revealed that the ability of the
system to recognize NNS responses as correct was often hindered by minor errors in spelling.
Misspellings are especially problematic here because they can derail the semantic evaluation
of a response by leading to errors in the syntactic interpretation of the sentence. Whereas
human listeners or readers can use the context and their knowledge of the language to infer the
intended pronunciation or spelling of a mispronounced or misspelled word, the initial approach
lacked any such compensatory strategies. To improve the system’s ability to handle NNS data,
we implement a spelling correction module, which we see as an attempt to endow the system
with some of the general language knowledge that a human would use upon encountering a
misspelling. Importantly, we incorporate contextual information about the picture by giving
this module access to NS responses (i.e., picture descriptions), allowing it to prefer corrected
spellings that may be relevant to the context.
We begin with a context-independent spelling corrector, Aspell (Atkinson, 1998), but on finding
mixed results with only this basic spelling correction module—due to its lack of incorporation
of context—we expand the process to include a statistical language model (LM) based on word
trigrams (section 5.2). The n-gram LM essentially takes a large body of English text, counts the
occurrences of each sequence of n words, converts these counts to relative frequencies, and
uses these relative frequencies to calculate the probability of new texts. The LM has the effect
of evaluating the likelihood of multiple possible spellings for a misspelled word (as provided by
a context-independent spelling correction module) in the context of surrounding words. In this
way, we further attempt to use contextual information and general knowledge of the language
to model the close intended meaning while overlooking minor errors in orthography.
The implementation of these tools raises some questions about about how fair or appropriate it
is to try to estimate a learner’s intended utterance, and just exactly what spelling correction
is and is not (or should and should not be). Any automatic or manual approach at correcting
malformed learner language or interpreting its meaning encounters ambiguous and challenging
cases. This is why we defined our goal in section 4 as that of deriving a close intended form,
essentially sidestepping the question of what the ultimate correction means and instead focusing
on what the correction can tell us about the linguistic utterance’s relation to the picture.
This goal, it should be pointed out, is in keeping with a prioritization on encouraging learners to
produce more language and on interaction with learners, as opposed to prioritizing grammatical
or orthographic perfection. Deriving a close intended form should be able to inform a system
towards an appropriate piece of feedback. This can also be seen as “giving the benefit of
the doubt” to learners, finding the gold item that looks close; giving the benefit of the doubt
is particularly true in the joint evaluation described in Section 6.2, where we consider each
original response as well as its corrected version.
</bodyText>
<subsectionHeader confidence="0.999408">
5.2 Spelling correction process
</subsectionHeader>
<bodyText confidence="0.99624075">
Aspell In our first attempt at spelling correction, we added a preprocessing step using Aspell,
a spelling correction tool (Atkinson, 1998). For each PDT item, the NNS sentences were passed
through Aspell. Words recognized by Aspell were not changed. For words that Aspell considered
misspelled, the ranked list of Aspell suggestions was compared with a list of words used in the
</bodyText>
<page confidence="0.997369">
49
</page>
<bodyText confidence="0.999585422222222">
NS sentences. The highest ranked suggestion that was also in the NS word list was accepted
as the corrected spelling. If no match was found, the first suggested word was accepted, and
the sentences were then passed to the rest of the pipeline. A major limitation of this correction
was the fact that misspellings resulting in real words were not addressed. For example, several
participants responded to one item with the real word shout but clearly intended shoot (cf. A
man shoots a bird). Indeed, evaluation of this simple Aspell approach revealed that it introduced
significantly more errors than it corrected. Thus, we omit this method from further discussion
and focus on a more contextually informed approach incorporating language modeling.
LM pipeline In the approach discussed
hereafter (the LM pipeline), Aspell (via the En-
chant python package (Lachowicz, 2003)) is
used to obtain a list of spelling suggestions for
all words, including those that appear to be
properly spelled. These candidate spellings
are combined to form a list of candidate sen-
tences for each response. Each candidate
sentence is then compared with an n-gram
language model to obtain a perplexity score—
i.e., a measure of how likely the sentence is,
given the LM. The candidate sentence with
the lowest perplexity is chosen automatically
as the best correction. A diagram of the entire
semantic extraction process incorporating the
spelling correction and language modeling
tools is given in Figure 3.
The computational costs of this approach
have the potential to be very great. The
number of spelling suggestions for a given
word range between zero—for egregious mis-
spellings of long words, unlike anything in the
dictionary—and up to 50, for words within
a short edit distance of known words (e.g.,
pet). The average number of suggestions for
the words in the NNS responses is roughly 31.
The average sentence length among the entire
data set of NNS responses is 7.2 words. This
would result in approximately 317.2 (nearly
55 billion) candidate sentences for a single
NNS response. We took several steps to prune
the number of candidate words and sentences
in order to make this process more manage-
able.
For this pruning, we draw on the NS re-
sponses; this decision is based on the assump-
tions that NS responses are correct and that the PDT constrains the content of responses. We
</bodyText>
<figure confidence="0.998963172413793">
Spelling Sug-
gester (Aspell)
NS
Sentences
Candidate
Spellings Filter
Stop
Words List
NNS
Sentence
Candidate Sen-
tence Combiner
N-Gram Lan-
guage Model
Sentence Selector
Stanford Depen-
dency Parser
Gigaword
Corpus
NS
Sentences
Semantic
Triple Extractor
NS Triples
(Gold)
Lemmatizer
NNS Triple
Evaluator
Result
</figure>
<figureCaption confidence="0.9769475">
Figure 3: Semantic triple evaluation pipeline.
Boxes are system components, circles are data;
double arrows indicate training, dashed arrows
show the obtaining of gold standard triples.
</figureCaption>
<page confidence="0.987866">
50
</page>
<bodyText confidence="0.999961636363636">
create a NS word list by taking every word form found in the NS responses. We also use a list of
stop words consisting of the 200 most common English words, to filter out short function words
that would create too many candidates.
This process of forming candidate sentences for a NNS response assumes that while there may
be misspellings, the number of words in the sentence is fixed. That is, a word may be replaced
by another word, but no word may be removed and no additional words may be inserted. (Rare
exceptions may occur when the spelling correction tool suggests that an unrecognized word
be split into two words.) This is a limitation of the current implementation and should be
addressed in the future, perhaps incorporating techniques for word normalization over word
lattices from the speech recognition literature, such as those in Sproat et al. (2001).
For a given NNS response in this pipeline, each token is given a status of fixed or unfixed.
Each word enters the pipeline as unfixed; it is then compared with the stop words list, and
if a match is found, the status is changed to fixed. The remaining unfixed words are then
compared with the NS word list and again, matches are fixed. For any token with a fixed
status, no candidate spelling corrections will be considered. Thus we assume that a NNS word
that matches a stop word is correct, as English learners at this level are unlikely to misspell
common function words. We also assume, given the constraints of the PDT, that a NNS word
that matches a NS word is correct.
Next, we handle misspellings where no sentential context is needed, given the contents of the
picture. Each unfixed word is passed to Enchant and a list of candidate spellings is obtained.
Note that a ranked list of spelling suggestions is generated even for words that appear to be
properly spelled. This list is compared with the NS list; if one or more matches are found, the
highest ranked candidate word is selected, and the status is fixed. If no match is found, the
status remains unfixed, and the entire list of candidate words is added to that word position.
After that pruning, a list of candidate sentences can now be generated by iterating through
candidate words for unfixed positions to generate every possible combination with all the
fixed words. As mentioned above, NNS responses in the data set contain an average of 7.2
words, and at this stage, 6.5 words are fixed and 0.7 words are unfixed, resulting in an
average of 310.7 (roughly 11) candidate sentences per NNS response, drastically reducing the
computational costs of the remaining steps in the pipeline. Many well-formed responses result
in no candidate sentences beyond the original form, while the largest number of candidates
seen among the entire set was 57,300, for a 10-word sentence.
For each NNS sentence, the original sentence and its list of candidates are passed to the language
model for evaluation. Here we use the CMU Statistical Language Modeling Toolkit (Clarkson
and Rosenfeld, 1997) in a trigram setting trained on a sample of the English Gigaword Corpus
(Graff et al., 2007) containing roughly 250 million words in 10 million sentences of newspaper
text. The candidate sentences are ranked according to their perplexity with regard to the
language model. The sentence with lowest perplexity is selected as the most likely sentence
and passed through the remaining steps of the pipeline, as shown in the lower half of Figure 3.
The source of PDT descriptions We use the NS responses here as our proxy for a description
of the picture content. This is distinct from using the NS responses as a gold standard to
compare the final triples against, as in King and Dickinson (2013); indeed, this is why we
manually check triples in this work, assuring that we truly know whether a triple is valid or not.
In either case, we will see the limitations of using NS responses for these purposes.
</bodyText>
<page confidence="0.998829">
51
</page>
<sectionHeader confidence="0.993916" genericHeader="method">
6 Spelling Correction Evaluation
</sectionHeader>
<bodyText confidence="0.998624">
Here we present the results of the modifications detailed above. At this stage, we are primarily
interested in our system’s ability to robustly extract evaluable triples, potentially in the face of
minor errors. While we present coverage scores in the following sections—calculating coverage
with respect to the particular (and limited) gold standard set of triples—we focus mainly on the
effect the modifications have on (Form) error counts.
</bodyText>
<table confidence="0.999854333333333">
NNS LM Joint(NNS) Joint(LM) Joint(Oracle)
Coverage 134 149 152 152 152
Gold Misses 125 110 125 109 141
Triple Errors 13 13 13 14 15
Form Errors 118 118 100 115 82
Total Form/Triple Errors 131 131 113 129 97
</table>
<tableCaption confidence="0.991537333333333">
Table 1: Errors types and coverage for the full set of responses (390 sentences). Joint indicates
a joint analysis of both sources (NNS &amp; LM); the default source in parentheses was chosen in
cases where neither triple was found (see Section 6.2).
</tableCaption>
<subsectionHeader confidence="0.997251">
6.1 LM pipeline errors
</subsectionHeader>
<bodyText confidence="0.99975968">
With no attempt at spelling correction, the
390 NNS responses result in a total of 131
true errors, with an additional 125 misses
due to an incomplete gold standard and cov-
erage of 134 non-errors, as seen in Table 1.
For our evaluation, we are most concerned
with reducing the Form errors—which may
result in more Gold misses, depending upon
whether a valid triple is in the gold standard
or not.
Evaluating the LM output results in reducing
the number of Gold misses by 8.3%, from
125 to 110, with the Triple and Form error
counts unchanged. But this does not tell the
full story of changes. If we look closer, as in
the first column of Table 2, we see that in
comparing the LM triples to the NNS triples,
a total of 73 responses change from one error
type to another. This includes the conversion
of 18 Form errors to non-errors (;) and three
non-errors to Form errors. An example of a
“recovered” Form error can be seen in (4). In
this case, shoots and bird are both present in
the NS gold standard responses, which helps
the LM obtain an acceptable triple.
</bodyText>
<table confidence="0.999852954545455">
Change LM J(NNS) J(O)
; 7→ ; ↔ 131 149 152
; 7→ Gold ↓ 0 0 0
; 7→ Trip. ↓ 0 0 0
; 7→ Form ↓ 3 0 0
Gold 7→ ; ↑ 0 0 0
Gold 7→ Gold ↔ 93 94 125
Gold 7→ Trip. ↓ 1 0 0
Gold 7→ Form ↓ 31 15 0
Trip. 7→ ; ↑ 0 0 0
Trip. 7→ Gold ↑ 0 1 0
Trip. 7→ Trip. ↔ 11 11 13
Trip. 7→ Form ↓ 2 2 0
Form 7→ ; ↑ 18 3 0
Form 7→ Gold ↑ 16 30 16
Form 7→ Trip. ↑ 2 2 2
Form 7→ Form ↔ 82 83 82
Changed 73 53 18
Unchanged 317 337 372
Total ↔ 317 337 372
Total ↑ 36 36 18
Total ↓ 37 17 0
</table>
<tableCaption confidence="0.987084333333333">
Table 2: The number of changes between error
types, moving from NNS to LM, from LM to
J(NNS), and from J(NNS) to J(Oracle).
</tableCaption>
<page confidence="0.996527">
52
</page>
<listItem confidence="0.9855595">
(4) a. NNS: the old man shouts to beird. ⇒ NONE(shout,NONE)
b. LM: the old man shoots to bird. ⇒ shoot(man,bird)
</listItem>
<bodyText confidence="0.99947975">
An example of a Form error introduced by the LM pipeline is seen in (5). Here we see that
given the NNS sentence, the lemmatizer was robust enough to properly arrive at shoot from the
misspelled shooted. While both shoot and shot were among the 12 words suggested by Aspell to
replace shooted, the LM preferred shouted.
</bodyText>
<listItem confidence="0.968723">
(5) a. NNS: a man shooted a bird. ⇒ shoot(man,bird)
b. LM: a man shouted a bird. ⇒ shout(man,bird)
</listItem>
<bodyText confidence="0.988021">
Additionally, the LM pipeline changes 31 Gold “errors” to Form errors. While this does not affect
coverage or the total error counts when evaluated under the current gold standard, these cases
are clearly problematic. One example is shown in (6b), modified by the LM pipeline from (6a).
</bodyText>
<listItem confidence="0.963762">
(6) a. NNS: a person was cutting fruit. ⇒ cut(person,fruit)
b. LM: a person was cutting fraud. ⇒ cut(person,fraud)
</listItem>
<bodyText confidence="0.999952166666667">
The issue stems from the fact that the LM was trained on newspaper text, leading it to prefer
words and phrases prevalent in the news, (cutting fraud), while giving higher perplexity to
those less common in news stories (cutting fruit). Other examples of this domain-based over-
correction include the changing of biking to backing, cleaning to learning, and chopping to
shipping. Besides choosing better LM training data, future work could use other methods of
analysis to avoid these problems. For example, using WordNet (Fellbaum, 1998) to discover
that “fruit” is a hypernym of “apple” (the object described by all NSs), and thus (possibly)
acceptable, would eliminate the need to process some spelling candidates via the LM.
Due to the design of the LM pipeline, these problems are compounded by the sparseness of the
gold standard. The NS responses are used to derive the gold standard, but also to derive a list
of context-appropriate words for each item. As described above, this word list is used to select
appropriate spelling candidates from the correction tool before the recombined sentences are
evaluated by the LM. The over-correction problem is exacerbated when the PDT item depicts
an action for which NSs know a specific word but NNSs may not, like raking or rowing. These
items highlight the disadvantages of relying on NS responses. For such prompts, we observe: a)
relatively high numbers of candidate sentences, because fewer candidate spellings are decided
by the NS word list, as well as: b) higher numbers of Form errors, because we shift the burden
of deciding contextually appropriate words to the LM.
</bodyText>
<subsectionHeader confidence="0.999746">
6.2 Joint Evaluation
</subsectionHeader>
<bodyText confidence="0.999879428571428">
So far, each sentence form and triple output by the LM pipeline is evaluated alone, without
regard to the NNS form or the output of the the original process. In this section we present
a joint analysis, wherein we take both the LM triple and its NNS counterpart; in cases where
one of the two triples is found in the set of NS responses, we keep that triple and ignore the
other; in cases where neither triple is found, we default to the NNS triple; we refer to this as
Joint(NNS). (A joint analysis defaulting to the LM (Joint(LM)) was also performed, but this
resulted in weaker performance, as shown in Table 1, and is omitted from the discussion). The
</bodyText>
<page confidence="0.997085">
53
</page>
<bodyText confidence="0.999946454545455">
idea behind this joint analysis is simply to give the system the choice between two triples for
a single response, using information about the picture’s contents (NS responses) to pick one,
effectively allowing us to undo any errors introduced by Aspell or the LM.
We again focus primarily on the changes in error counts. Unlike the analyses above, however,
under this joint evaluation there is an unavoidable possibility for the set of NS responses to
affect error counts. This is because a triple’s presence or absence in this set determines which of
the two triple versions is considered. Consider the following constructed example to illustrate
this concern. Under our joint analysis, given an original triple of shout(hunter,bird), which is
an error (and of course absent from the list) and an LM triple of shoot(hunter,bird), which is
correct but absent from the NS list, we default to the original triple, thus including an error
that would have been avoided if the NS list had covered the LM triple.
Such cases illustrate the fact that the error types are not equally (un)desirable. A Gold miss
is better than a Form or Triple error for us, because the Gold miss is not a system error at all
and could be covered by an improved gold standard. Likewise, a (NNS) Form error changed
to a (LM) Triple error is a partial success, because this means the spelling correction module
was successful, while the parser or semantic extractor needs improvement. To address these
issues, we perform a Joint(Oracle) experiment (section 6.2.2), in which errors were ranked by
preference, from non-error, to Gold miss, to Triple error, to Form error. In cases where neither
the NNS nor the LM triple was found and the error types were different, the oracle chooses the
preferred error type, minimizing Form errors and maximizing Gold misses. The results of this
experiment give a better approximation of the potential of the current system given an ideal set
of triples covering the content of the picture (which the NS responses serve as a proxy for).
</bodyText>
<subsectionHeader confidence="0.444945">
6.2.1 Joint(NNS) errors
</subsectionHeader>
<bodyText confidence="0.932566615384615">
The Joint(NNS) experiment gives coverage to 152 triples, and compared with the LM pipeline,
it results in a net reduction of 18 Form/Triple errors, from 131 to 113. While the error type was
changed for 53 responses, this improvement is partly the result of three Form errors converting
to non-errors (Table 2). An example of such a gain is seen in the NNS response (5a) and the
LM version (5b); this time, as the LM triple is not found, we default to the correct NNS triple,
undoing the error introduced by the LM pipeline. Another example of an LM error avoided
under the joint analysis is shown in (7b), as it defaults to the NNS response seen in (7a).
(7) a. NNS: a boy is playing a soccer alone. ⇒ play(boy,soccer)
b. LM: a boy is playing a soccer one. ⇒ play(boy,one)
Importantly, we also see 30 Form errors from the LM model become Gold misses (Table 2),
leading to an overall reduction of Form errors from 118 to 100 (Table 1). This Joint(NNS)
model, then, is doing exactly what it is designed to do: removing Form errors by changing the
spelling into something with a valid semantics.
</bodyText>
<subsectionHeader confidence="0.355364">
6.2.2 Joint(Oracle) errors
</subsectionHeader>
<bodyText confidence="0.99986025">
As mentioned in section 6.2, many positive changes introduced by the LM pipeline are not fully
realized under the LM or Joint(NNS) experiments. We investigate that here by using an oracle
to choose the preferred error type in cases where neither triple is found. As a result, 18 correct
changes introduced by the LM—but ignored by defaulting to the NNS under the Joint(NNS)
</bodyText>
<page confidence="0.993932">
54
</page>
<bodyText confidence="0.999951666666667">
setting—are retained under the Joint(Oracle) setting. These include two Form errors converted
to Triple errors and 16 Form errors converted to Gold misses (Table 2). Note that coverage
remains at 152 (Table 1).
An example of a Form error converted to a Triple error can be seen in (8b), the form and triple
derived from the NNS response in (8a). We consider (8a) to be a Form error, because cycle does
not fully describe a bicycle. In (8b), despite the fact that the LM pipeline made an appropriate
correction and returned a perfectly acceptable form, the derived triple is incorrect. This is the
result of an inappropriate parse, with rides given a plural noun (NNS) part of speech tag and
the woman rides labeled as a noun phrase.
</bodyText>
<listItem confidence="0.949543">
(8) a. NNS: the woman rides on her cycle. NONE(ride,cycle)
b. LM: the woman rides on her bicycle. NONE(ride,bicycle)
</listItem>
<bodyText confidence="0.9998201">
This kind of error is representative of a pattern among the Triple errors found across the dataset:
third person present tense verbs are regularly analyzed (via the parser’s built-in part-of-speech
tagger) as plural nouns, leading to the extraction of an incorrect triple. This is seen for rides,
boats, rows, and paints. A game setting would likely alleviate this problem by constraining
responses to the past tense, but NNSs may also need to be reminded that the simple present is
usually reserved for describing general truths.
An example from the 16 Gold misses corrected from Form errors is shown in (9b), derived from
the NNS response in (9a). Here, while (9b) is an appropriate form and triple, the triple is not
found in the gold standard, because every NS respondent described the PDT item as a raking
action, not a sweeping action.
</bodyText>
<listItem confidence="0.5176815">
(9) a. NNS: a men is swapping the leaves. swap(man,leaf)
b. LM: a man is sweeping the leaves. sweep(man,leaf)
</listItem>
<bodyText confidence="0.9996245">
Another example of a NNS Form error changed to an LM Gold miss under the Joint(NNS)
experiment is the correction of draw(artiest,portrait) (seen above in (3)) to draw(artist,portrait).
</bodyText>
<sectionHeader confidence="0.990963" genericHeader="conclusions">
7 Summary and Outlook
</sectionHeader>
<bodyText confidence="0.999983071428572">
We have implemented a system for automatically correcting NNS responses for visual stimuli
by relying on a small set of known appropriate responses to influence the correction process.
Even with a very limited gold standard, these corrections boosted coverage by 13.4% and
decreased the total rate of Form and Triple errors by 13.7% (with potential for a decrease of
25.9%, as in the oracle experiments). These results can help guide the development of systems
that aim to process the meaning of NNS statements, which contain a significantly higher rate of
spelling errors compared to NS statements. There is much to be gained with a small amount of
computational effort; as demonstrated here, more work needs to go into delineating a proper
set of appropriate responses.
Indeed, we see the construction of a robust set of appropriate responses as the most immediate
means of improving system performance. As NSs were shown to converge on a limited
vocabulary for some items, while NNSs do not, simply collecting more NS responses would
result in diminishing returns. Future work will need to uncover the best means of obtaining a
sufficient set of responses to describe a picture, whether it involves a more sophisticated and
</bodyText>
<page confidence="0.995093">
55
</page>
<bodyText confidence="0.999948681818182">
in-depth elicitation of NS responses or a deliberate attempt by the researchers at exhaustively
describing the images. Moreover, as this work will ideally lead toward a game or ILT, it may be
preferable to allow for “partial credit” (and the presentation of feedback) in the case of triples
that do not constitute a complete match but may match one or two of the subject, verb, and
object.
Similarly, as the correction module relies on the words used by NSs to influence corrections,
expanding the list of “influential” words is likely to be beneficial. While in the current study this
consisted of a simple list derived from the same responses in the gold standard, this is simply
in keeping with (King and Dickinson, 2013) and may not be optimal. A more sophisticated
approach could allow this influence to be probabilistic rather than binary, and could rely on
methods like TF-IDF to determine which words in NS responses are particularly relevant to the
item, and which words are incidental.
Another obvious source of improvement for future work is in the choice of training texts for the
LM, which was shown to have serious biases against the contents of the PDT responses, which
tend to describe physical actions or scenarios not common in newspaper text. Finding training
texts that contain the necessary kinds of sentences but also the sheer volume needed to cover
the variability of NNS responses is a challenge for future experiments in this area.
Given that this study primarily investigated transitive verbs, research on this problem will need
to examine interactions with other types of constructions, including the definition of more
elaborate semantic forms (Hahn and Meurers, 2012). Moving to a wider range of sentence
types may require the use of a semantic role labeler or similar tools and has the potential to
increase the complexity of spelling correction, due to, e.g., longer sentences.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998115">
We would like to thank the task participants, David Stringer for assistance in developing the
task, and Kathleen Bardovi-Harlig, Marlin Howard and Jayson Deese for help in recruiting
participants. We also thank Abigail Elston and Alex Rudnick for their helpful advice during
the system development. Finally, for their insightful feedback, we would like to thank the two
anonymous reviewers and the attendees of the computational linguistics colloquium series at
Indiana University.
</bodyText>
<sectionHeader confidence="0.999155" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997967818181818">
Atkinson, K. (1998). GNU Aspell. http://aspell.net.
Clarkson, P. and Rosenfeld, R. (1997). Statistical language modeling using the CMU-Cambridge
Toolkit. In Eurospeech, volume 97, pages 2707–2710.
Dale, R., Anisimoff, I., and Narroway, G. (2012). HOO 2012: A report on the preposition and
determiner error correction shared task. In Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 54–62, Montréal.
de Marneffe, M.-C., MacCartney, B., and Manning, C. D. (2006). Generating typed dependency
parses from phrase structure parses. In Proceedings of LREC 2006, Genoa, Italy.
DeSmedt, W. (1995). Herr Kommissar: An ICALL conversation simulator for intermediate
German. In Holland, V. M., Kaplan, J., and Sams, M., editors, Intelligent Language Tutors:
Theory Shaping Technology, pages 153–174. Lawrence Erlbaum, Mahwah, NJ.
</reference>
<page confidence="0.965057">
56
</page>
<reference confidence="0.999353684210526">
Ellis, R. (2000). Task-based research and language pedagogy. Language Teaching Research,
4(3):193–220.
Fellbaum, C., editor (1998). WordNet: An Electronic Lexical Database. The MIT Press, Cam-
bridge, MA.
Flor, M. (2012). Four types of context for automatic spelling correction. TAL, 53(2):61–99.
Flor, M. and Futagi, Y. (2012). On using context for automatic correction of non-word
misspellings in student essays. In Proceedings of the Seventh Workshop on Building Educational
Applications Using NLP, pages 105–115. Association for Computational Linguistics.
Flor, M., Futagi, Y., Lopez, M., and Mulholland, M. (2013). Patterns of misspellings in L2 and
L1 English: A view from the ETS Spelling Corpus. In Proceedings of the Second Learner Corpus
Research Conference (LCR 2013).
Forbes-McKay, K. and Venneri, A. (2005). Detecting subtle spontaneous language decline in
early Alzheimer’s disease with a picture description task. Neurological Sciences, 26(4):243–254.
Graff, D., Kong, J., Chen, K., and Maeda, K. (2007). English Gigaword, Third Edition.
Hahn, M. and Meurers, D. (2012). Evaluating the meaning of answers to reading comprehen-
sion questions: A semantics-based approach. In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications (BEA7), pages 326–336, Montreal, Canada.
Association for Computational Linguistics.
Heift, T. and Schulze, M. (2007). Errors and Intelligence in Computer-Assisted Language Learning:
Parsers and Pedagogues. Routledge.
Hovermale, D. (2008). SCALE: Spelling Correction Adapted for Learners of English. Pre-
CALICO Workshop on “Automatic Analysis of Learner Language: Bridging Foreign Language
Teaching Needs and NLP Possibilities”. March 18-19, 2008. San Francisco, CA.
Hovermale, D. (2010). An analysis of the spelling errors of L2 English learners. In CALICO
2010 Conference, Amherst, MA, USA.
King, L. and Dickinson, M. (2013). Shallow semantic analysis of interactive learner sentences.
In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational
Applications, pages 11–21, Atlanta, Georgia.
Klein, D. and Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of ACL-03,
Sapporo, Japan.
Lachowicz, D. (2003). Enchant. http://abisource.com/projects/enchant.
Leacock, C., Chodorow, M., Gamon, M., and Tetreault, J. (2010). Automated Grammatical
Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies.
Morgan Claypool.
Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., and McClosky, D. (2014).
The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60,
Baltimore, Maryland. Association for Computational Linguistics.
</reference>
<page confidence="0.980367">
57
</page>
<reference confidence="0.999475857142857">
Meurers, D. (2012). Natural language processing and language learning. In Chapelle, C. A.,
editor, Encyclopedia of Applied Linguistics. Blackwell.
Meurers, D., Ziai, R., Ott, N., and Bailey, S. (2011). Integrating parallel analysis modules
to evaluate the meaning of answers to reading comprehension questions. Special Issue on
Free-text Automatic Evaluation. International Journal of Continuing Engineering Education and
Life-Long Learning (IJCEELL), 21(4):355–369.
Petersen, K. A. (2010). Implicit Corrective Feedback in Computer-Guided Interaction: Does Mode
Matter? PhD thesis, Georgetown University, Washington, DC.
Somasundaran, S. and Chodorow, M. (2014). Automated measures of specific vocabulary
knowledge from constructed responses (‘Use these words to write a sentence based on this
picture’). In Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational
Applications, pages 1–11, Baltimore, Maryland.
Sproat, R., Black, A. W., Chen, S., Kumar, S., Ostendorf, M., and Richards, C. (2001). Normal-
ization of non-standard words. Computer Speech &amp; Language, 15(3):287–333.
</reference>
<page confidence="0.999262">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999945">Leveraging Known Semantics for Spelling Correction</title>
<author confidence="0.996987">King Dickinson</author>
<affiliation confidence="1">Indiana University</affiliation>
<address confidence="0.998702">Bloomington, IN USA</address>
<abstract confidence="0.990892727272727">Focusing on applications for analyzing learner language which evaluate semantic appropriateness and accuracy, we build from previous work which modeled some aspects of interaction, namely a picture description task (PDT), with the goal of integrating a spelling correction component in this context. After parsing a sentence and extracting semantic relations, a surprising number of analysis failures stem from misspellings, deviating from expected input in ways that can be modeled when the content of the interaction is known. We thus explore the use of spelling correction tools and language modeling to correct misspellings that often lead to errors in obtaining semantic forms, and we show that such tools can significantly reduce the number of unanalyzable cases. The work is useful for any context where image descriptions or some expected content is available, but not necessarily expected linguistic forms. description task, semantic analysis, spelling correction, language modeling.</abstract>
<note confidence="0.75540275">King and Markus Dickinson 2014. Leveraging known semantics for spelling correction. of the third on NLP for computer-assisted language learning. Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 43–58. 43 1 Motivation Much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent Computer- Assisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift</note>
<abstract confidence="0.9641806">and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception this rule is an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more conversational interactions (see Petersen, 2010). Such interactions are both more and less difficult to process: to provide feedback requires keeping track of the content of the interaction, but such content can also be used to disambiguate new learner productions. We exploit this tension in the context of spelling correction, as semantic information severely restricts the learner’s expected content, and thus also their word forms. Since our overarching goal is to move towards the facilitation of ILTs and language assessment tools that maximize free interaction, we have to deal with removing impediments to interaction. Given the preponderance of spelling errors in learner data, and specifically interactive data (King and Dickinson, 2013), our specific goal is to use basic NLP (pre)processing—namely, language modeling for spelling correction—to make the meaning of a learner’s sentence clearer. We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture’s description), can greatly reduce downstream errors. This may seem like a niche problem, but: 1) spelling errors are generally a major problem in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we have right now, on picture description tasks (PDTs), connects not only with a desire for more interactive tools, but also for language assessment (Somasundaran and Chodorow, 2014); and 3) our work seeks to unpack the connection between relatively “shallow” errors, namely spelling errors, with “deeper” errors, namely semantic ones. Unlike, for example, linguistic abstractions such as part-of-speech, both are intimately rooted in the particular lexical items used. This then raises the question of whether we are modeling what the learner said (modulo some spelling variation), what the learner intended, or what the learner should have intended, an issue we take up in section 4, after covering the background in section 3. The methods are covered in section 5 and the evaluation in section 6. 2 Related Work Research into the patterns of spelling errors particular to native speakers (NSs) and non-native speakers (NNSs) highlights the challenge of applying spelling correction techniques to nonnative text. Flor et al. (2013) examined spelling errors found in the ETS Spelling Corpus (3000 GRE and TOEFL essays) and found that NNS spelling errors were more severe (i.e., had a greater edit distance from the intended word) than NS errors. Moreover, NNSs made more spelling errors than NSs for words of 3-7 letters, but this trend reversed for words of 8 letters or more. These effects were shown to disappear among the most proficient NNSs in the sample, however. Similarly, Hovermale (2010) compared the spelling errors in corpora of Japanese learners of English to previous studies of NS spelling errors and found that the learner errors have a greater average edit distance and are nearly twice as likely to involve the first letter of the word. Given such variability in form, correcting spelling errors for NNSs strictly via edit 44 distance operations would thus seem to have its limits. Using the ETS Spelling Corpus and the ConSpell spelling correction tool, Flor (2012) demonstrates significant gains in automatic spelling correction when modules using contextual information are added. Four types of context, each of which benefitted spelling correction, were 1) word (length 1–5) and a web-scale language model (LM); 2) word grams and the positive normalized pointwise mutual information (PNPMI) of the words within them (based on a web-scale distributional model); 3) the entire essay (and the recurrence or lack of a given candidate spelling correction in the essay); and 4) the text of the essay prompt. Notably, a 3.8% improvement comes through the use of “global mutual optimization”, i.e., at each given spelling correction decision, the module is biased not only toward other words in the text, but also the candidate spelling lists of these other words. The work presents a strong case the use of with both LMs and PNPMI, as the best results come from this setting, boosting performance 11.48% above the non-contextual spelling correction baseline. Flor and Futagi (2012) further examine the use of context for correcting learner misspellings and claim that three major issues contribute to the task’s difficulty: “local error density” (a word near other misspellings) weakens approaches; poor grammar can lead to the selection of an incorrect spelling candidate based on its agreement with nearby incorrect words; and competition among closely related spelling candidates can lead to the selection of an incorrect inflectional variant. These challenges indicate that for potentially error-rich sentences, sentence or level contexts may be more effective when combined with higher-level contextual information, such as task prompts and discourse-level information about verb inflections. We explore including information about picture content. 3 Background 3.1 Data In previous work (King and Dickinson, 2013), we collected responses to a picture description (PDT) task to approximate interactive behavior. The current study relies on the same set of responses. We use a PDT because it helps constrain both form and content, without providing textual prompts that may influence a learner. Moreover, PDTs are a well-established tool in areas of study ranging from SLA to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). The use of visual stimuli also helps model the visual nature of online games. The stimuli are chosen to elicit relatively unambiguous transitive sentences. Response (L1) The man killing the beard. (Arabic) The PDT consisted of 10 items (8 line drawings and 2 photographs) intended to elicit a single sentence each; an example is given in Figure 1. Participants were asked to view the image and describe the action in a complete A man is shutting a bird. (Chinese) A man is shooting a bird. (English) The man shouted the bird. (Spanish) Figure 1: Example item and responses 45 sentence, with any tense or aspect appropriate. 25 of the 39 non-native speaker (NNS) participants performed the task in a setting where automatic spell checking was disabled; the remaining 14 performed the task online on their own computers, and although they were instructed to disable spell checking, we have no way of knowing if they did so. The NNSs were intermediate and upper-level adult English learners in an intensive English as a Second Language program at Indiana University. This data set contains responses from 53 informants, including native speakers (NSs) (14 NSs, 39 NNSs), for a total of 530 sentences.</abstract>
<note confidence="0.7194855">The distribution of first languages (L1s) is: 16 Arabic, 7 Chinese, 14 English, 2 Japanese, 4 Korean, 1 Kurdish, 1 Polish, 2 Portuguese, and 6 Spanish. 3.2 Method As in King and Dickinson (2013), our method to obtain a semantic form from a NNS production</note>
<abstract confidence="0.997725275862069">takes two steps: 1) obtain a syntactic dependency representation from the off-the-shelf Stanford parser (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of hand-written rules. To illustrate this process, consider (1). This sentence is passed through the parser to obtain the dependency parse shown in Figure 2. Based the presence of the subject, passive) node, the extraction script takes the subject from under the the verb from and the logical object from This results in the semantic triple lemmatized to using the Stanford CoreNLP lemmatizer (Manning et al., 2014). Very little effort is needed: the parser is pre-built; the decision tree is small; and the extraction rules are minimal. Note, too, certain relations (e.g., are completely ignored in the extraction. (1) A bird is shot by a man. root bird is shot by a man Figure 2: The dependency parse of (1) One is able to use little effort in part due to the constraints in the pictures. For figure 1, for man in the and man all acceptable subjects, whereas if were multiple men in the picture, man not be specific enough. Evaluation in King and Dickinson (2013) addresses two major questions. First, how accurate is the extraction of semantic information from potentially innovative sentences? Secondly, how much coverage does one have in a gold standard of semantic forms (triples), to capture the variability in meaning in learner sentences? We focus more on the first question and again use native speaker semantic forms as a proxy for a gold standard—albeit, limited by mismatches between native and (correct) non-native ways of saying the same thing. To mitigate this and better see the effect of spelling correction, much of our evaluation relies on hand-analysis which determines whether a “reasonable gold standard” could contain the information (see section 4). extraction the purpose of evaluating an extraction system, King and Dickinson define two major classes of errors. The first are responses for which the det nsubjpass auxpass agent det 46 system fails to extract one or more of the desired subject, verb, or object, based on the sentence hand and without regard to the target content. Second are responses for which the system extracts the desired subject, verb and object, but the resulting triple does not accurately describe the image (i.e., is an error of the participant’s). In this paper, we focus on reducing the triple errors, i.e., system errors. For example, the spelling error in (2) leads to a completely incorrect triple. We will unpack our error types in section 4. A man swipped leaves. Focusing on triple (system) errors, we have obtained 92.3% accuracy on extraction for NNS data and roughly the same for NS data, 92.9% (King and Dickinson, 2013). Furthermore, more than half of the errors for NNSs involve misspellings (4.1% of the total 7.7% of errors). For a system interacting with learners, spelling errors are thus a high priority (cf. Hovermale, 2008). errors are subcategorized as depending on whether the resulting triple has spelling errors that do not result in real words—as in (3)—or that do result real but unintended words and thus convey an inappropriate meaning (e.g., of We will see this distinction play out in the spelling correction techniques in section 5. The artiest is drawing a portret. Approximately 15% of NNS triples are content errors (King and Dickinson, 2013). These cases are ones for which the learner needs feedback, but there are two barriers in providing feedback: 1) without fixing the triple errors, they will be automatically grouped into the content error cases, since they do not match the gold standard; and 2) even if one knows something is an error, to obtain feedback one would ideally know the target the learner was (or should have been) aiming for. Our approach to spelling correction addresses both of these concerns by cleaning up the misspelled cases—including many of the “content” errors rooted in misspellings. coverage King and Dickinson (2013), we take a set of native speaker (NS) responses for the same PDTs as the gold standard, garnering coverage numbers around 25% types and 50% for tokens—i.e., about half of responses are not in the gold standard. Since our focus is on improving accuracy, we use the same gold standard, but augment the analysis with hand-evaluation of whether a response should have been in the gold standard (section 4). Still, with spelling modifications being made to make a NNS response more native-like, we may be able to increase coverage, i.e., to find a (gold) NS triple that matches. 4 Error Types As alluded to above, our goal is to model a close intended meaning of every NNS sentence, in to provide a platform for providing feedback. By intended we mean, meaning that matches some correct answer and whose corresponding form is a reasonable distance what the NNS Since we have not interviewed participants with follow-up questions about the intention of their responses and cannot assume a follow-up in the general case, we take the close intended meaning as the meaning they should have intended, given their production. 47 For evaluation, then, we want to measure the extent to which we are able to take a NNS form and produce a plausible target meaning for their “intention,” i.e., a viable semantic triple. The evaluation should answer: 1) Is there a valid meaning? and, if not, 2) what step in the process prevented a valid meaning from being derived? The entire system is outlined in section 5.2 (see Figure 3), but essentially we have this pipeline: 0. Sentence produced by NNS 1. (optional) Spelling corrector to generate close intended form 2. Syntactic parser to obtain word-word relations 3. Semantic extractor to obtain semantic forms 4. Comparison to gold standard Starting with the gold standard comparison and working backwards, we evaluate at every step: Is the triple covered by the gold standard? an error. to next step. Should the triple be covered by a reasonable gold standard? (“error”). Continue to next step. • Given the limited coverage of the NS gold standard, we add this manual step, so as not to focus too much on one particular gold standard. 3. Is the form (either the NNS sentence or the close intended meaning chosen by the correction module) well-formed and appropriate for the item but the extracted triple not covered by a reasonable gold standard? (These could be as parser, lemmatizer, or extractor errors.) Note that for our purposes here, a “good” triple should indicate an appropriate subject, verb object, whether directly or indirectly. In most cases, this is a complete For some concepts, however, a verb may imply its object, or vice versa. Item 3 of the PDT, for example, shows a woman riding a bicycle. This could be represented as a transitive resulting in a triple like However, this could also be construed an intransitive, such as Both of these triples should be considered A form like woman is on a bicycle also be considered appropriate, because obvious action involving a person on a bicycle is etc.), even though the triple is less descriptive. An intransitive resulting in a triple like inadequate, however, because not sufficiently imply the object. Similar cases occur among the responses to item 9, which shows two boys rowing a boat. We consider be an ideal triple for this item, but we also accept as in boys are on a Note that this is only acceptable because in the absence of more detail, a reasonable person given the information that some human is performing some action on or involving a boat would likely assume that the action involves using the boat for its intended purpose—to travel on water, and that could be represented with a more specific We should also accept because (unlike the verb its object (a boat). Similarly, adequate, because as a verb, indicates both the presence of a boat and the action of riding the boat. 48 5 Spelling Correction Modifications 5.1 Motivation for spelling correction via language modeling The initial approach to this task in King and Dickinson (2013) revealed that the ability of the system to recognize NNS responses as correct was often hindered by minor errors in spelling. Misspellings are especially problematic here because they can derail the semantic evaluation of a response by leading to errors in the syntactic interpretation of the sentence. Whereas human listeners or readers can use the context and their knowledge of the language to infer the intended pronunciation or spelling of a mispronounced or misspelled word, the initial approach lacked any such compensatory strategies. To improve the system’s ability to handle NNS data, we implement a spelling correction module, which we see as an attempt to endow the system with some of the general language knowledge that a human would use upon encountering a misspelling. Importantly, we incorporate contextual information about the picture by giving this module access to NS responses (i.e., picture descriptions), allowing it to prefer corrected spellings that may be relevant to the context. We begin with a context-independent spelling corrector, Aspell (Atkinson, 1998), but on finding mixed results with only this basic spelling correction module—due to its lack of incorporation of context—we expand the process to include a statistical language model (LM) based on word (section 5.2). The LM essentially takes a large body of English text, counts the of each sequence of converts these counts to relative frequencies, and uses these relative frequencies to calculate the probability of new texts. The LM has the effect of evaluating the likelihood of multiple possible spellings for a misspelled word (as provided by a context-independent spelling correction module) in the context of surrounding words. In this way, we further attempt to use contextual information and general knowledge of the language to model the close intended meaning while overlooking minor errors in orthography. The implementation of these tools raises some questions about about how fair or appropriate it is to try to estimate a learner’s intended utterance, and just exactly what spelling correction is and is not (or should and should not be). Any automatic or manual approach at correcting malformed learner language or interpreting its meaning encounters ambiguous and challenging cases. This is why we defined our goal in section 4 as that of deriving a close intended form, sidestepping the question of what the ultimate correction instead focusing on what the correction can tell us about the linguistic utterance’s relation to the picture. This goal, it should be pointed out, is in keeping with a prioritization on encouraging learners to produce more language and on interaction with learners, as opposed to prioritizing grammatical or orthographic perfection. Deriving a close intended form should be able to inform a system towards an appropriate piece of feedback. This can also be seen as “giving the benefit of the doubt” to learners, finding the gold item that looks close; giving the benefit of the doubt is particularly true in the joint evaluation described in Section 6.2, where we consider each original response as well as its corrected version. 5.2 Spelling correction process our first attempt at spelling correction, we added a preprocessing step using Aspell, a spelling correction tool (Atkinson, 1998). For each PDT item, the NNS sentences were passed through Aspell. Words recognized by Aspell were not changed. For words that Aspell considered misspelled, the ranked list of Aspell suggestions was compared with a list of words used in the 49 NS sentences. The highest ranked suggestion that was also in the NS word list was accepted as the corrected spelling. If no match was found, the first suggested word was accepted, and the sentences were then passed to the rest of the pipeline. A major limitation of this correction was the fact that misspellings resulting in real words were not addressed. For example, several responded to one item with the real word clearly intended shoots a Indeed, evaluation of this simple Aspell approach revealed that it introduced significantly more errors than it corrected. Thus, we omit this method from further discussion and focus on a more contextually informed approach incorporating language modeling. pipeline the approach discussed (the Aspell (via the Enchant python package (Lachowicz, 2003)) is used to obtain a list of spelling suggestions for all words, including those that appear to be properly spelled. These candidate spellings are combined to form a list of candidate sentences for each response. Each candidate is then compared with an language model to obtain a perplexity score— i.e., a measure of how likely the sentence is, given the LM. The candidate sentence with the lowest perplexity is chosen automatically as the best correction. A diagram of the entire semantic extraction process incorporating the spelling correction and language modeling tools is given in Figure 3. The computational costs of this approach have the potential to be very great. The number of spelling suggestions for a given word range between zero—for egregious misspellings of long words, unlike anything in the dictionary—and up to 50, for words within a short edit distance of known words (e.g., The average number of suggestions for the words in the NNS responses is roughly 31. The average sentence length among the entire data set of NNS responses is 7.2 words. This result in approximately (nearly 55 billion) candidate sentences for a single NNS response. We took several steps to prune the number of candidate words and sentences in order to make this process more manageable. For this pruning, we draw on the NS rethis decision is based on the assumptions that NS responses are correct and that the PDT constrains the content of responses. We</abstract>
<title confidence="0.792839">Spelling Suggester (Aspell) Sentences Candidate Spellings Filter Words List NNS Sentence Candidate Sentence Combiner guage Sentence Selector Stanford Dependency Parser Gigaword Corpus Sentences Triple Extractor NS Triples (Gold) Lemmatizer NNS Triple Evaluator Result</title>
<abstract confidence="0.998389178571428">Figure 3: Semantic triple evaluation pipeline. Boxes are system components, circles are data; double arrows indicate training, dashed arrows show the obtaining of gold standard triples. 50 create a NS word list by taking every word form found in the NS responses. We also use a list of stop words consisting of the 200 most common English words, to filter out short function words that would create too many candidates. This process of forming candidate sentences for a NNS response assumes that while there may be misspellings, the number of words in the sentence is fixed. That is, a word may be replaced by another word, but no word may be removed and no additional words may be inserted. (Rare exceptions may occur when the spelling correction tool suggests that an unrecognized word be split into two words.) This is a limitation of the current implementation and should be addressed in the future, perhaps incorporating techniques for word normalization over word lattices from the speech recognition literature, such as those in Sproat et al. (2001). a given NNS response in this pipeline, each token is given a status of word enters the pipeline as it is then compared with the stop words list, and a match is found, the status is changed to The remaining are then with the NS word list and again, matches are For any token with a status, no candidate spelling corrections will be considered. Thus we assume that a NNS word that matches a stop word is correct, as English learners at this level are unlikely to misspell common function words. We also assume, given the constraints of the PDT, that a NNS word that matches a NS word is correct. Next, we handle misspellings where no sentential context is needed, given the contents of the Each is passed to Enchant and a list of candidate spellings is obtained. Note that a ranked list of spelling suggestions is generated even for words that appear to be properly spelled. This list is compared with the NS list; if one or more matches are found, the ranked candidate word is selected, and the status is If no match is found, the remains and the entire list of candidate words is added to that word position. After that pruning, a list of candidate sentences can now be generated by iterating through words for to generate every possible combination with all the As mentioned above, NNS responses in the data set contain an average of 7.2 and at this stage, 6.5 words are 0.7 words are resulting in an of (roughly 11) candidate sentences per NNS response, drastically reducing the computational costs of the remaining steps in the pipeline. Many well-formed responses result in no candidate sentences beyond the original form, while the largest number of candidates seen among the entire set was 57,300, for a 10-word sentence. For each NNS sentence, the original sentence and its list of candidates are passed to the language model for evaluation. Here we use the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) in a trigram setting trained on a sample of the English Gigaword Corpus (Graff et al., 2007) containing roughly 250 million words in 10 million sentences of newspaper text. The candidate sentences are ranked according to their perplexity with regard to the language model. The sentence with lowest perplexity is selected as the most likely sentence and passed through the remaining steps of the pipeline, as shown in the lower half of Figure 3. source of PDT descriptions use the NS responses here as our proxy for a description of the picture content. This is distinct from using the NS responses as a gold standard to compare the final triples against, as in King and Dickinson (2013); indeed, this is why we manually check triples in this work, assuring that we truly know whether a triple is valid or not. In either case, we will see the limitations of using NS responses for these purposes. 51 6 Spelling Correction Evaluation Here we present the results of the modifications detailed above. At this stage, we are primarily interested in our system’s ability to robustly extract evaluable triples, potentially in the face of minor errors. While we present coverage scores in the following sections—calculating coverage with respect to the particular (and limited) gold standard set of triples—we focus mainly on the effect the modifications have on (Form) error counts.</abstract>
<note confidence="0.8170392">NNS LM Joint(NNS) Joint(LM) Joint(Oracle) Coverage 134 149 152 152 152 Gold Misses 125 110 125 109 141 Triple Errors 13 13 13 14 15 Form Errors 118 118 100 115 82</note>
<date confidence="0.287165">Errors 131 131 113 129 97</date>
<abstract confidence="0.98358324137931">1: Errors types and coverage for the full set of responses (390 sentences). a joint analysis of both sources (NNS &amp; LM); the default source in parentheses was chosen in cases where neither triple was found (see Section 6.2). 6.1 LM pipeline errors With no attempt at spelling correction, the 390 NNS responses result in a total of 131 true errors, with an additional 125 misses due to an incomplete gold standard and coverage of 134 non-errors, as seen in Table 1. For our evaluation, we are most concerned with reducing the Form errors—which may result in more Gold misses, depending upon whether a valid triple is in the gold standard or not. Evaluating the LM output results in reducing the number of Gold misses by 8.3%, from 125 to 110, with the Triple and Form error counts unchanged. But this does not tell the full story of changes. If we look closer, as in the first column of Table 2, we see that in comparing the LM triples to the NNS triples, a total of 73 responses change from one error type to another. This includes the conversion 18 Form errors to non-errors and three non-errors to Form errors. An example of a “recovered” Form error can be seen in (4). In case, both present in the NS gold standard responses, which helps the LM obtain an acceptable triple.</abstract>
<note confidence="0.647958">Change LM J(NNS) J(O) ; 7→ ; ↔ 131 149 152 7→ ↓ 0 0 0 7→ ↓ 0 0 0 7→ ↓ 3 0 0 ; ↑ 0 0 0 ↔ 93 94 125 ↓ 1 0 0 ↓ 31 15 0 ; ↑ 0 0 0 ↑ 0 1 0 ↔ 11 11 13 ↓ 2 2 0 ; ↑ 18 3 0 ↑ 16 30 16 ↑ 2 2 2 ↔ 82 83 82 Changed 73 53 18 Unchanged 317 337 372 Total ↔ 317 337 372 Total ↑ 36 36 18 Total ↓ 37 17 0</note>
<abstract confidence="0.997575348387097">Table 2: The number of changes between error types, moving from NNS to LM, from LM to J(NNS), and from J(NNS) to J(Oracle). 52 a. NNS: the old man shouts to beird. LM: the old man shoots to bird. An example of a Form error introduced by the LM pipeline is seen in (5). Here we see that the NNS sentence, the lemmatizer was robust enough to properly arrive at the While both among the 12 words suggested by Aspell to the LM preferred a. NNS: a man shooted a bird. LM: a man shouted a bird. Additionally, the LM pipeline changes 31 Gold “errors” to Form errors. While this does not affect coverage or the total error counts when evaluated under the current gold standard, these cases are clearly problematic. One example is shown in (6b), modified by the LM pipeline from (6a). a. NNS: a person was cutting fruit. LM: a person was cutting fraud. The issue stems from the fact that the LM was trained on newspaper text, leading it to prefer and phrases prevalent in the news, while giving higher perplexity to less common in news stories Other examples of this domain-based overinclude the changing of and Besides choosing better LM training data, future work could use other methods of analysis to avoid these problems. For example, using WordNet (Fellbaum, 1998) to discover that “fruit” is a hypernym of “apple” (the object described by all NSs), and thus (possibly) acceptable, would eliminate the need to process some spelling candidates via the LM. Due to the design of the LM pipeline, these problems are compounded by the sparseness of the gold standard. The NS responses are used to derive the gold standard, but also to derive a list of context-appropriate words for each item. As described above, this word list is used to select appropriate spelling candidates from the correction tool before the recombined sentences are evaluated by the LM. The over-correction problem is exacerbated when the PDT item depicts action for which NSs know a specific word but NNSs may not, like These items highlight the disadvantages of relying on NS responses. For such prompts, we observe: a) relatively high numbers of candidate sentences, because fewer candidate spellings are decided by the NS word list, as well as: b) higher numbers of Form errors, because we shift the burden of deciding contextually appropriate words to the LM. 6.2 Joint Evaluation So far, each sentence form and triple output by the LM pipeline is evaluated alone, without regard to the NNS form or the output of the the original process. In this section we present a joint analysis, wherein we take both the LM triple and its NNS counterpart; in cases where one of the two triples is found in the set of NS responses, we keep that triple and ignore the other; in cases where neither triple is found, we default to the NNS triple; we refer to this as (A joint analysis defaulting to the LM was also performed, but this resulted in weaker performance, as shown in Table 1, and is omitted from the discussion). The 53 idea behind this joint analysis is simply to give the system the choice between two triples for a single response, using information about the picture’s contents (NS responses) to pick one, effectively allowing us to undo any errors introduced by Aspell or the LM. We again focus primarily on the changes in error counts. Unlike the analyses above, however, under this joint evaluation there is an unavoidable possibility for the set of NS responses to affect error counts. This is because a triple’s presence or absence in this set determines which of the two triple versions is considered. Consider the following constructed example to illustrate concern. Under our joint analysis, given an original triple of which is error (and of course absent from the list) and an LM triple of which is correct but absent from the NS list, we default to the original triple, thus including an error that would have been avoided if the NS list had covered the LM triple. Such cases illustrate the fact that the error types are not equally (un)desirable. A Gold miss is better than a Form or Triple error for us, because the Gold miss is not a system error at all and could be covered by an improved gold standard. Likewise, a (NNS) Form error changed to a (LM) Triple error is a partial success, because this means the spelling correction module was successful, while the parser or semantic extractor needs improvement. To address these we perform a (section 6.2.2), in which errors were ranked by preference, from non-error, to Gold miss, to Triple error, to Form error. In cases where neither the NNS nor the LM triple was found and the error types were different, the oracle chooses the preferred error type, minimizing Form errors and maximizing Gold misses. The results of this experiment give a better approximation of the potential of the current system given an ideal set of triples covering the content of the picture (which the NS responses serve as a proxy for). 6.2.1 Joint(NNS) errors The Joint(NNS) experiment gives coverage to 152 triples, and compared with the LM pipeline, results in a net reduction of 18 errors, from 131 to 113. While the error type was changed for 53 responses, this improvement is partly the result of three Form errors converting to non-errors (Table 2). An example of such a gain is seen in the NNS response (5a) and the LM version (5b); this time, as the LM triple is not found, we default to the correct NNS triple, undoing the error introduced by the LM pipeline. Another example of an LM error avoided under the joint analysis is shown in (7b), as it defaults to the NNS response seen in (7a). a. NNS: a boy is playing a soccer alone. LM: a boy is playing a soccer one. Importantly, we also see 30 Form errors from the LM model become Gold misses (Table 2), leading to an overall reduction of Form errors from 118 to 100 (Table 1). This Joint(NNS) model, then, is doing exactly what it is designed to do: removing Form errors by changing the spelling into something with a valid semantics. 6.2.2 Joint(Oracle) errors As mentioned in section 6.2, many positive changes introduced by the LM pipeline are not fully realized under the LM or Joint(NNS) experiments. We investigate that here by using an oracle to choose the preferred error type in cases where neither triple is found. As a result, 18 correct changes introduced by the LM—but ignored by defaulting to the NNS under the Joint(NNS) 54 setting—are retained under the Joint(Oracle) setting. These include two Form errors converted to Triple errors and 16 Form errors converted to Gold misses (Table 2). Note that coverage remains at 152 (Table 1). An example of a Form error converted to a Triple error can be seen in (8b), the form and triple from the NNS response in (8a). We consider (8a) to be a Form error, because not fully describe a bicycle. In (8b), despite the fact that the LM pipeline made an appropriate correction and returned a perfectly acceptable form, the derived triple is incorrect. This is the of an inappropriate parse, with a plural noun (NNS) part of speech tag and woman rides as a noun phrase. (8) a. NNS: the woman rides on her cycle. NONE(ride,cycle) b. LM: the woman rides on her bicycle. NONE(ride,bicycle) This kind of error is representative of a pattern among the Triple errors found across the dataset: third person present tense verbs are regularly analyzed (via the parser’s built-in part-of-speech as plural nouns, leading to the extraction of an incorrect triple. This is seen for rows, A game setting would likely alleviate this problem by constraining responses to the past tense, but NNSs may also need to be reminded that the simple present is usually reserved for describing general truths. An example from the 16 Gold misses corrected from Form errors is shown in (9b), derived from the NNS response in (9a). Here, while (9b) is an appropriate form and triple, the triple is not in the gold standard, because every NS respondent described the PDT item as a action, not a sweeping action. (9) a. NNS: a men is swapping the leaves. swap(man,leaf) b. LM: a man is sweeping the leaves. sweep(man,leaf) Another example of a NNS Form error changed to an LM Gold miss under the Joint(NNS) is the correction of above in (3)) to 7 Summary and Outlook We have implemented a system for automatically correcting NNS responses for visual stimuli by relying on a small set of known appropriate responses to influence the correction process. Even with a very limited gold standard, these corrections boosted coverage by 13.4% and decreased the total rate of Form and Triple errors by 13.7% (with potential for a decrease of 25.9%, as in the oracle experiments). These results can help guide the development of systems that aim to process the meaning of NNS statements, which contain a significantly higher rate of spelling errors compared to NS statements. There is much to be gained with a small amount of computational effort; as demonstrated here, more work needs to go into delineating a proper set of appropriate responses. Indeed, we see the construction of a robust set of appropriate responses as the most immediate means of improving system performance. As NSs were shown to converge on a limited vocabulary for some items, while NNSs do not, simply collecting more NS responses would result in diminishing returns. Future work will need to uncover the best means of obtaining a sufficient set of responses to describe a picture, whether it involves a more sophisticated and 55 in-depth elicitation of NS responses or a deliberate attempt by the researchers at exhaustively describing the images. Moreover, as this work will ideally lead toward a game or ILT, it may be preferable to allow for “partial credit” (and the presentation of feedback) in the case of triples that do not constitute a complete match but may match one or two of the subject, verb, and object. Similarly, as the correction module relies on the words used by NSs to influence corrections, expanding the list of “influential” words is likely to be beneficial. While in the current study this consisted of a simple list derived from the same responses in the gold standard, this is simply in keeping with (King and Dickinson, 2013) and may not be optimal. A more sophisticated approach could allow this influence to be probabilistic rather than binary, and could rely on methods like TF-IDF to determine which words in NS responses are particularly relevant to the item, and which words are incidental. Another obvious source of improvement for future work is in the choice of training texts for the LM, which was shown to have serious biases against the contents of the PDT responses, which tend to describe physical actions or scenarios not common in newspaper text. Finding training texts that contain the necessary kinds of sentences but also the sheer volume needed to cover the variability of NNS responses is a challenge for future experiments in this area. Given that this study primarily investigated transitive verbs, research on this problem will need to examine interactions with other types of constructions, including the definition of more elaborate semantic forms (Hahn and Meurers, 2012). Moving to a wider range of sentence types may require the use of a semantic role labeler or similar tools and has the potential to increase the complexity of spelling correction, due to, e.g., longer sentences. Acknowledgments We would like to thank the task participants, David Stringer for assistance in developing the task, and Kathleen Bardovi-Harlig, Marlin Howard and Jayson Deese for help in recruiting participants. We also thank Abigail Elston and Alex Rudnick for their helpful advice during the system development. Finally, for their insightful feedback, we would like to thank the two anonymous reviewers and the attendees of the computational linguistics colloquium series at</abstract>
<affiliation confidence="0.938125">Indiana University.</affiliation>
<note confidence="0.9370062">References K. (1998). GNU Aspell. Clarkson, P. and Rosenfeld, R. (1997). Statistical language modeling using the CMU-Cambridge In volume 97, pages 2707–2710. Dale, R., Anisimoff, I., and Narroway, G. (2012). HOO 2012: A report on the preposition and error correction shared task. In of the Seventh Workshop on Building Applications Using pages 54–62, Montréal. de Marneffe, M.-C., MacCartney, B., and Manning, C. D. (2006). Generating typed dependency from phrase structure parses. In of LREC Genoa, Italy. DeSmedt, W. (1995). Herr Kommissar: An ICALL conversation simulator for intermediate In Holland, V. M., Kaplan, J., and Sams, M., editors, Language Tutors: Shaping pages 153–174. Lawrence Erlbaum, Mahwah, NJ. 56 R. (2000). Task-based research and language pedagogy. Teaching 4(3):193–220. C., editor (1998). An Electronic Lexical The MIT Press, Cambridge, MA. M. (2012). Four types of context for automatic spelling correction. 53(2):61–99. Flor, M. and Futagi, Y. (2012). On using context for automatic correction of non-word in student essays. In of the Seventh Workshop on Building Educational Using pages 105–115. Association for Computational Linguistics. Flor, M., Futagi, Y., Lopez, M., and Mulholland, M. (2013). Patterns of misspellings in L2 and English: A view from the ETS Spelling Corpus. In of the Second Learner Corpus Conference (LCR Forbes-McKay, K. and Venneri, A. (2005). Detecting subtle spontaneous language decline in Alzheimer’s disease with a picture description task. 26(4):243–254. Graff, D., Kong, J., Chen, K., and Maeda, K. (2007). English Gigaword, Third Edition. Hahn, M. and Meurers, D. (2012). Evaluating the meaning of answers to reading comprehenquestions: A semantics-based approach. In of the 7th Workshop on Innovative of NLP for Building Educational Applications pages 326–336, Montreal, Canada. Association for Computational Linguistics. T. and Schulze, M. (2007). and Intelligence in Computer-Assisted Language Learning: and Routledge. Hovermale, D. (2008). SCALE: Spelling Correction Adapted for Learners of English. Pre- CALICO Workshop on “Automatic Analysis of Learner Language: Bridging Foreign Language Teaching Needs and NLP Possibilities”. March 18-19, 2008. San Francisco, CA. D. (2010). An analysis of the spelling errors of L2 English learners. In Conference, Amherst, MA, King, L. and Dickinson, M. (2013). Shallow semantic analysis of interactive learner sentences. of the Eighth Workshop on Innovative Use of NLP for Building Educational pages 11–21, Atlanta, Georgia. D. and Manning, C. D. (2003). Accurate unlexicalized parsing. In of Sapporo, Japan. D. (2003). Enchant. C., Chodorow, M., Gamon, M., and Tetreault, J. (2010). Grammatical Detection for Language Synthesis Lectures on Human Language Technologies. Morgan Claypool. Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., and McClosky, D. (2014). Stanford CoreNLP natural language processing toolkit. In of 52nd Annual of the Association for Computational Linguistics: System pages 55–60, Baltimore, Maryland. Association for Computational Linguistics. 57 Meurers, D. (2012). Natural language processing and language learning. In Chapelle, C. A., of Applied Blackwell. Meurers, D., Ziai, R., Ott, N., and Bailey, S. (2011). Integrating parallel analysis modules</note>
<abstract confidence="0.855184">evaluate the meaning of answers to reading comprehension questions. Issue on Free-text Automatic Evaluation. International Journal of Continuing Engineering Education and Learning 21(4):355–369. K. A. (2010). Corrective Feedback in Computer-Guided Interaction: Does Mode thesis, Georgetown University, Washington, DC. Somasundaran, S. and Chodorow, M. (2014). Automated measures of specific vocabulary knowledge from constructed responses (‘Use these words to write a sentence based on this In of the Ninth Workshop on Innovative Use of NLP for Building Educational</abstract>
<note confidence="0.83151725">pages 1–11, Baltimore, Maryland. Sproat, R., Black, A. W., Chen, S., Kumar, S., Ostendorf, M., and Richards, C. (2001). Normalof non-standard words. Speech &amp; 15(3):287–333. 58</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Atkinson</author>
</authors>
<date>1998</date>
<note>GNU Aspell. http://aspell.net.</note>
<contexts>
<context position="19342" citStr="Atkinson, 1998" startWordPosition="3065" endWordPosition="3066">ed word, the initial approach lacked any such compensatory strategies. To improve the system’s ability to handle NNS data, we implement a spelling correction module, which we see as an attempt to endow the system with some of the general language knowledge that a human would use upon encountering a misspelling. Importantly, we incorporate contextual information about the picture by giving this module access to NS responses (i.e., picture descriptions), allowing it to prefer corrected spellings that may be relevant to the context. We begin with a context-independent spelling corrector, Aspell (Atkinson, 1998), but on finding mixed results with only this basic spelling correction module—due to its lack of incorporation of context—we expand the process to include a statistical language model (LM) based on word trigrams (section 5.2). The n-gram LM essentially takes a large body of English text, counts the occurrences of each sequence of n words, converts these counts to relative frequencies, and uses these relative frequencies to calculate the probability of new texts. The LM has the effect of evaluating the likelihood of multiple possible spellings for a misspelled word (as provided by a context-in</context>
<context position="21651" citStr="Atkinson, 1998" startWordPosition="3428" endWordPosition="3429">tizing grammatical or orthographic perfection. Deriving a close intended form should be able to inform a system towards an appropriate piece of feedback. This can also be seen as “giving the benefit of the doubt” to learners, finding the gold item that looks close; giving the benefit of the doubt is particularly true in the joint evaluation described in Section 6.2, where we consider each original response as well as its corrected version. 5.2 Spelling correction process Aspell In our first attempt at spelling correction, we added a preprocessing step using Aspell, a spelling correction tool (Atkinson, 1998). For each PDT item, the NNS sentences were passed through Aspell. Words recognized by Aspell were not changed. For words that Aspell considered misspelled, the ranked list of Aspell suggestions was compared with a list of words used in the 49 NS sentences. The highest ranked suggestion that was also in the NS word list was accepted as the corrected spelling. If no match was found, the first suggested word was accepted, and the sentences were then passed to the rest of the pipeline. A major limitation of this correction was the fact that misspellings resulting in real words were not addressed.</context>
</contexts>
<marker>Atkinson, 1998</marker>
<rawString>Atkinson, K. (1998). GNU Aspell. http://aspell.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Clarkson</author>
<author>R Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU-Cambridge Toolkit.</title>
<date>1997</date>
<booktitle>In Eurospeech,</booktitle>
<volume>97</volume>
<pages>2707--2710</pages>
<contexts>
<context position="27777" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="4443" endWordPosition="4446">d at this stage, 6.5 words are fixed and 0.7 words are unfixed, resulting in an average of 310.7 (roughly 11) candidate sentences per NNS response, drastically reducing the computational costs of the remaining steps in the pipeline. Many well-formed responses result in no candidate sentences beyond the original form, while the largest number of candidates seen among the entire set was 57,300, for a 10-word sentence. For each NNS sentence, the original sentence and its list of candidates are passed to the language model for evaluation. Here we use the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) in a trigram setting trained on a sample of the English Gigaword Corpus (Graff et al., 2007) containing roughly 250 million words in 10 million sentences of newspaper text. The candidate sentences are ranked according to their perplexity with regard to the language model. The sentence with lowest perplexity is selected as the most likely sentence and passed through the remaining steps of the pipeline, as shown in the lower half of Figure 3. The source of PDT descriptions We use the NS responses here as our proxy for a description of the picture content. This is distinct from using the NS resp</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, P. and Rosenfeld, R. (1997). Statistical language modeling using the CMU-Cambridge Toolkit. In Eurospeech, volume 97, pages 2707–2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>I Anisimoff</author>
<author>G Narroway</author>
</authors>
<title>HOO 2012: A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<location>Montréal.</location>
<contexts>
<context position="1578" citStr="Dale et al., 2012" startWordPosition="219" endWordPosition="222">any context where image descriptions or some expected content is available, but not necessarily expected linguistic forms. KEYWORDS: picture description task, semantic analysis, spelling correction, language modeling. Levi King and Markus Dickinson 2014. Leveraging known semantics for spelling correction. Proceedings of the third workshop on NLP for computer-assisted language learning. NEALT Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 43–58. 43 1 Motivation Much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent ComputerAssisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards </context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Dale, R., Anisimoff, I., and Narroway, G. (2012). HOO 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54–62, Montréal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C de Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC 2006,</booktitle>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>de Marneffe, M.-C., MacCartney, B., and Manning, C. D. (2006). Generating typed dependency parses from phrase structure parses. In Proceedings of LREC 2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W DeSmedt</author>
</authors>
<title>Herr Kommissar: An ICALL conversation simulator for intermediate German. In</title>
<date>1995</date>
<booktitle>Intelligent Language Tutors: Theory Shaping Technology,</booktitle>
<pages>153--174</pages>
<editor>Holland, V. M., Kaplan, J., and Sams, M., editors,</editor>
<location>Mahwah, NJ.</location>
<contexts>
<context position="1961" citStr="DeSmedt, 1995" startWordPosition="276" endWordPosition="277">. NEALT Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 43–58. 43 1 Motivation Much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent ComputerAssisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more conversational interactions (see Petersen, 2010). Such interactions are both more and less difficult to process: to provide feedback requires keeping track of the content of the interaction, but such content can also be used to disambiguate new learner productions. We exploit this tension in the context of spelling correction, as semantic information severely re</context>
</contexts>
<marker>DeSmedt, 1995</marker>
<rawString>DeSmedt, W. (1995). Herr Kommissar: An ICALL conversation simulator for intermediate German. In Holland, V. M., Kaplan, J., and Sams, M., editors, Intelligent Language Tutors: Theory Shaping Technology, pages 153–174. Lawrence Erlbaum, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ellis</author>
</authors>
<title>Task-based research and language pedagogy.</title>
<date>2000</date>
<journal>Language Teaching Research,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="7827" citStr="Ellis, 2000" startWordPosition="1182" endWordPosition="1183">extual information, such as task prompts and discourse-level information about verb inflections. We explore including information about picture content. 3 Background 3.1 Data In previous work (King and Dickinson, 2013), we collected responses to a picture description (PDT) task to approximate interactive behavior. The current study relies on the same set of responses. We use a PDT because it helps constrain both form and content, without providing textual prompts that may influence a learner. Moreover, PDTs are a well-established tool in areas of study ranging from SLA to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). The use of visual stimuli also helps model the visual nature of online games. The stimuli are chosen to elicit relatively unambiguous transitive sentences. Response (L1) The man killing the beard. (Arabic) The PDT consisted of 10 items (8 line drawings and 2 photographs) intended to elicit a single sentence each; an example is given in Figure 1. Participants were asked to view the image and describe the action in a complete A man is shutting a bird. (Chinese) A man is shooting a bird. (English) The man shouted the bird. (Spanish) Figure 1: Example item and re</context>
</contexts>
<marker>Ellis, 2000</marker>
<rawString>Ellis, R. (2000). Task-based research and language pedagogy. Language Teaching Research, 4(3):193–220.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, C., editor</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Fellbaum, C., editor (1998). WordNet: An Electronic Lexical Database. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Flor</author>
</authors>
<title>Four types of context for automatic spelling correction.</title>
<date>2012</date>
<journal>TAL,</journal>
<volume>53</volume>
<issue>2</issue>
<contexts>
<context position="5520" citStr="Flor (2012)" startWordPosition="829" endWordPosition="830">re. These effects were shown to disappear among the most proficient NNSs in the sample, however. Similarly, Hovermale (2010) compared the spelling errors in corpora of Japanese learners of English to previous studies of NS spelling errors and found that the learner errors have a greater average edit distance and are nearly twice as likely to involve the first letter of the word. Given such variability in form, correcting spelling errors for NNSs strictly via edit 44 distance operations would thus seem to have its limits. Using the ETS Spelling Corpus and the ConSpell spelling correction tool, Flor (2012) demonstrates significant gains in automatic spelling correction when modules using contextual information are added. Four types of context, each of which benefitted spelling correction, were explored: 1) word n-grams (length 1–5) and a web-scale language model (LM); 2) word ngrams and the positive normalized pointwise mutual information (PNPMI) of the words within them (based on a web-scale distributional model); 3) the entire essay (and the recurrence or lack of a given candidate spelling correction in the essay); and 4) the text of the essay prompt. Notably, a 3.8% improvement comes through</context>
</contexts>
<marker>Flor, 2012</marker>
<rawString>Flor, M. (2012). Four types of context for automatic spelling correction. TAL, 53(2):61–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Flor</author>
<author>Y Futagi</author>
</authors>
<title>On using context for automatic correction of non-word misspellings in student essays.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>105--115</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6563" citStr="Flor and Futagi (2012)" startWordPosition="993" endWordPosition="996">the entire essay (and the recurrence or lack of a given candidate spelling correction in the essay); and 4) the text of the essay prompt. Notably, a 3.8% improvement comes through the use of “global mutual optimization”, i.e., at each given spelling correction decision, the module is biased not only toward other words in the text, but also the candidate spelling lists of these other words. The work presents a strong case for the use of n-grams with both LMs and PNPMI, as the best results come from this setting, boosting performance 11.48% above the non-contextual spelling correction baseline. Flor and Futagi (2012) further examine the use of context for correcting learner misspellings and claim that three major issues contribute to the task’s difficulty: “local error density” (a misspelled word near other misspellings) weakens n-gram approaches; poor grammar can lead to the selection of an incorrect spelling candidate based on its agreement with nearby incorrect words; and competition among closely related spelling candidates can lead to the selection of an incorrect inflectional variant. These challenges indicate that for potentially error-rich learner sentences, sentence or n-gram level contexts may b</context>
</contexts>
<marker>Flor, Futagi, 2012</marker>
<rawString>Flor, M. and Futagi, Y. (2012). On using context for automatic correction of non-word misspellings in student essays. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 105–115. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Flor</author>
<author>Y Futagi</author>
<author>M Lopez</author>
<author>M Mulholland</author>
</authors>
<title>Patterns of misspellings in L2 and L1 English: A view from the ETS Spelling Corpus.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Learner Corpus Research Conference (LCR</booktitle>
<contexts>
<context position="3510" citStr="Flor et al., 2013" startWordPosition="505" endWordPosition="508">ifically interactive data (King and Dickinson, 2013), our specific goal is to use basic NLP (pre)processing—namely, language modeling for spelling correction—to make the meaning of a learner’s sentence clearer. We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture’s description), can greatly reduce downstream errors. This may seem like a niche problem, but: 1) spelling errors are generally a major problem in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we have right now, on picture description tasks (PDTs), connects not only with a desire for more interactive tools, but also for language assessment (Somasundaran and Chodorow, 2014); and 3) our work seeks to unpack the connection between relatively “shallow” errors, namely spelling errors, with “deeper” errors, namely semantic ones. Unlike, for example, linguistic abstractions such as part-of-speech, both are intimately rooted in the particular lexical items used. This then raises the question of whether we are modeling what the learner said (modulo some spelling varia</context>
</contexts>
<marker>Flor, Futagi, Lopez, Mulholland, 2013</marker>
<rawString>Flor, M., Futagi, Y., Lopez, M., and Mulholland, M. (2013). Patterns of misspellings in L2 and L1 English: A view from the ETS Spelling Corpus. In Proceedings of the Second Learner Corpus Research Conference (LCR 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-McKay</author>
<author>A Venneri</author>
</authors>
<title>Detecting subtle spontaneous language decline in early Alzheimer’s disease with a picture description task.</title>
<date>2005</date>
<journal>Neurological Sciences,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="7860" citStr="Forbes-McKay and Venneri, 2005" startWordPosition="1184" endWordPosition="1187">ation, such as task prompts and discourse-level information about verb inflections. We explore including information about picture content. 3 Background 3.1 Data In previous work (King and Dickinson, 2013), we collected responses to a picture description (PDT) task to approximate interactive behavior. The current study relies on the same set of responses. We use a PDT because it helps constrain both form and content, without providing textual prompts that may influence a learner. Moreover, PDTs are a well-established tool in areas of study ranging from SLA to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). The use of visual stimuli also helps model the visual nature of online games. The stimuli are chosen to elicit relatively unambiguous transitive sentences. Response (L1) The man killing the beard. (Arabic) The PDT consisted of 10 items (8 line drawings and 2 photographs) intended to elicit a single sentence each; an example is given in Figure 1. Participants were asked to view the image and describe the action in a complete A man is shutting a bird. (Chinese) A man is shooting a bird. (English) The man shouted the bird. (Spanish) Figure 1: Example item and responses 45 sentence, with any ten</context>
</contexts>
<marker>Forbes-McKay, Venneri, 2005</marker>
<rawString>Forbes-McKay, K. and Venneri, A. (2005). Detecting subtle spontaneous language decline in early Alzheimer’s disease with a picture description task. Neurological Sciences, 26(4):243–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>J Kong</author>
<author>K Chen</author>
<author>K Maeda</author>
</authors>
<title>English Gigaword, Third Edition.</title>
<date>2007</date>
<contexts>
<context position="27870" citStr="Graff et al., 2007" startWordPosition="4460" endWordPosition="4463">ly 11) candidate sentences per NNS response, drastically reducing the computational costs of the remaining steps in the pipeline. Many well-formed responses result in no candidate sentences beyond the original form, while the largest number of candidates seen among the entire set was 57,300, for a 10-word sentence. For each NNS sentence, the original sentence and its list of candidates are passed to the language model for evaluation. Here we use the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997) in a trigram setting trained on a sample of the English Gigaword Corpus (Graff et al., 2007) containing roughly 250 million words in 10 million sentences of newspaper text. The candidate sentences are ranked according to their perplexity with regard to the language model. The sentence with lowest perplexity is selected as the most likely sentence and passed through the remaining steps of the pipeline, as shown in the lower half of Figure 3. The source of PDT descriptions We use the NS responses here as our proxy for a description of the picture content. This is distinct from using the NS responses as a gold standard to compare the final triples against, as in King and Dickinson (2013</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>Graff, D., Kong, J., Chen, K., and Maeda, K. (2007). English Gigaword, Third Edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hahn</author>
<author>D Meurers</author>
</authors>
<title>Evaluating the meaning of answers to reading comprehension questions: A semantics-based approach.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA7),</booktitle>
<pages>326--336</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<contexts>
<context position="43023" citStr="Hahn and Meurers, 2012" startWordPosition="7097" endWordPosition="7100">ning texts for the LM, which was shown to have serious biases against the contents of the PDT responses, which tend to describe physical actions or scenarios not common in newspaper text. Finding training texts that contain the necessary kinds of sentences but also the sheer volume needed to cover the variability of NNS responses is a challenge for future experiments in this area. Given that this study primarily investigated transitive verbs, research on this problem will need to examine interactions with other types of constructions, including the definition of more elaborate semantic forms (Hahn and Meurers, 2012). Moving to a wider range of sentence types may require the use of a semantic role labeler or similar tools and has the potential to increase the complexity of spelling correction, due to, e.g., longer sentences. Acknowledgments We would like to thank the task participants, David Stringer for assistance in developing the task, and Kathleen Bardovi-Harlig, Marlin Howard and Jayson Deese for help in recruiting participants. We also thank Abigail Elston and Alex Rudnick for their helpful advice during the system development. Finally, for their insightful feedback, we would like to thank the two a</context>
</contexts>
<marker>Hahn, Meurers, 2012</marker>
<rawString>Hahn, M. and Meurers, D. (2012). Evaluating the meaning of answers to reading comprehension questions: A semantics-based approach. In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA7), pages 326–336, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Heift</author>
<author>M Schulze</author>
</authors>
<title>Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues.</title>
<date>2007</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="1748" citStr="Heift and Schulze, 2007" startWordPosition="242" endWordPosition="245">ic analysis, spelling correction, language modeling. Levi King and Markus Dickinson 2014. Leveraging known semantics for spelling correction. Proceedings of the third workshop on NLP for computer-assisted language learning. NEALT Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 43–58. 43 1 Motivation Much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent ComputerAssisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more conversational interactions (see Petersen, 2010). Such interactions are both more and less difficult to process: to provide feedback requires keeping t</context>
</contexts>
<marker>Heift, Schulze, 2007</marker>
<rawString>Heift, T. and Schulze, M. (2007). Errors and Intelligence in Computer-Assisted Language Learning: Parsers and Pedagogues. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hovermale</author>
</authors>
<title>SCALE: Spelling Correction Adapted for Learners of English. PreCALICO Workshop on “Automatic Analysis of Learner Language: Bridging Foreign Language Teaching Needs and NLP Possibilities”.</title>
<date>2008</date>
<location>San Francisco, CA.</location>
<contexts>
<context position="12575" citStr="Hovermale, 2008" startWordPosition="1966" endWordPosition="1967">his paper, we focus on reducing the triple errors, i.e., system errors. For example, the spelling error in (2) leads to a completely incorrect triple. We will unpack our error types in section 4. (2) A man swipped leaves. ⇒ leave(swipped,man) Focusing on triple (system) errors, we have obtained 92.3% accuracy on extraction for NNS data and roughly the same for NS data, 92.9% (King and Dickinson, 2013). Furthermore, more than half of the errors for NNSs involve misspellings (4.1% of the total 7.7% of errors). For a system interacting with learners, spelling errors are thus a high priority (cf. Hovermale, 2008). Content errors are subcategorized as spelling or meaning errors, depending on whether the resulting triple has spelling errors that do not result in real words—as in (3)—or that do result in real but unintended words and thus convey an inappropriate meaning (e.g., shout(man,bird) instead of shoot(man,bird)). We will see this distinction play out in the spelling correction techniques in section 5. (3) The artiest is drawing a portret. ⇒ drawing(artiest,portret) Approximately 15% of NNS triples are content errors (King and Dickinson, 2013). These cases are ones for which the learner needs feed</context>
</contexts>
<marker>Hovermale, 2008</marker>
<rawString>Hovermale, D. (2008). SCALE: Spelling Correction Adapted for Learners of English. PreCALICO Workshop on “Automatic Analysis of Learner Language: Bridging Foreign Language Teaching Needs and NLP Possibilities”. March 18-19, 2008. San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hovermale</author>
</authors>
<title>An analysis of the spelling errors of L2 English learners.</title>
<date>2010</date>
<booktitle>In CALICO 2010 Conference,</booktitle>
<location>Amherst, MA, USA.</location>
<contexts>
<context position="5033" citStr="Hovermale (2010)" startWordPosition="749" endWordPosition="750">peakers (NSs) and non-native speakers (NNSs) highlights the challenge of applying spelling correction techniques to nonnative text. Flor et al. (2013) examined spelling errors found in the ETS Spelling Corpus (3000 GRE and TOEFL essays) and found that NNS spelling errors were more severe (i.e., had a greater edit distance from the intended word) than NS errors. Moreover, NNSs made more spelling errors than NSs for words of 3-7 letters, but this trend reversed for words of 8 letters or more. These effects were shown to disappear among the most proficient NNSs in the sample, however. Similarly, Hovermale (2010) compared the spelling errors in corpora of Japanese learners of English to previous studies of NS spelling errors and found that the learner errors have a greater average edit distance and are nearly twice as likely to involve the first letter of the word. Given such variability in form, correcting spelling errors for NNSs strictly via edit 44 distance operations would thus seem to have its limits. Using the ETS Spelling Corpus and the ConSpell spelling correction tool, Flor (2012) demonstrates significant gains in automatic spelling correction when modules using contextual information are ad</context>
</contexts>
<marker>Hovermale, 2010</marker>
<rawString>Hovermale, D. (2010). An analysis of the spelling errors of L2 English learners. In CALICO 2010 Conference, Amherst, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L King</author>
<author>M Dickinson</author>
</authors>
<title>Shallow semantic analysis of interactive learner sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>11--21</pages>
<location>Atlanta,</location>
<contexts>
<context position="2944" citStr="King and Dickinson, 2013" startWordPosition="424" endWordPosition="427">e feedback requires keeping track of the content of the interaction, but such content can also be used to disambiguate new learner productions. We exploit this tension in the context of spelling correction, as semantic information severely restricts the learner’s expected content, and thus also their word forms. Since our overarching goal is to move towards the facilitation of ILTs and language assessment tools that maximize free interaction, we have to deal with removing impediments to interaction. Given the preponderance of spelling errors in learner data, and specifically interactive data (King and Dickinson, 2013), our specific goal is to use basic NLP (pre)processing—namely, language modeling for spelling correction—to make the meaning of a learner’s sentence clearer. We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture’s description), can greatly reduce downstream errors. This may seem like a niche problem, but: 1) spelling errors are generally a major problem in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we have ri</context>
<context position="7434" citStr="King and Dickinson, 2013" startWordPosition="1115" endWordPosition="1118">ar can lead to the selection of an incorrect spelling candidate based on its agreement with nearby incorrect words; and competition among closely related spelling candidates can lead to the selection of an incorrect inflectional variant. These challenges indicate that for potentially error-rich learner sentences, sentence or n-gram level contexts may be more effective when combined with higher-level contextual information, such as task prompts and discourse-level information about verb inflections. We explore including information about picture content. 3 Background 3.1 Data In previous work (King and Dickinson, 2013), we collected responses to a picture description (PDT) task to approximate interactive behavior. The current study relies on the same set of responses. We use a PDT because it helps constrain both form and content, without providing textual prompts that may influence a learner. Moreover, PDTs are a well-established tool in areas of study ranging from SLA to Alzheimer’s disease (Ellis, 2000; Forbes-McKay and Venneri, 2005). The use of visual stimuli also helps model the visual nature of online games. The stimuli are chosen to elicit relatively unambiguous transitive sentences. Response (L1) Th</context>
<context position="9254" citStr="King and Dickinson (2013)" startWordPosition="1420" endWordPosition="1423">4 performed the task online on their own computers, and although they were instructed to disable spell checking, we have no way of knowing if they did so. The NNSs were intermediate and upper-level adult English learners in an intensive English as a Second Language program at Indiana University. This data set contains responses from 53 informants, including native speakers (NSs) (14 NSs, 39 NNSs), for a total of 530 sentences. The distribution of first languages (L1s) is: 16 Arabic, 7 Chinese, 14 English, 2 Japanese, 4 Korean, 1 Kurdish, 1 Polish, 2 Portuguese, and 6 Spanish. 3.2 Method As in King and Dickinson (2013), our method to obtain a semantic form from a NNS production takes two steps: 1) obtain a syntactic dependency representation from the off-the-shelf Stanford parser (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of hand-written rules. To illustrate this process, consider (1). This sentence is passed through the parser to obtain the dependency parse shown in Figure 2. Based on the presence of the nsubjpass (noun subject, passive) node, the extraction script takes the logical subject from under the agent label, the verb from roo</context>
<context position="10663" citStr="King and Dickinson (2013)" startWordPosition="1658" endWordPosition="1661">). Very little effort is needed: the parser is pre-built; the decision tree is small; and the extraction rules are minimal. Note, too, that certain relations (e.g., det) are completely ignored in the extraction. (1) A bird is shot by a man. root vroot A bird is shot by a man Figure 2: The dependency parse of (1) One is able to use little effort in part due to the constraints in the pictures. For figure 1, for example, the artist, the man in the beret, and the man are all acceptable subjects, whereas if there were multiple men in the picture, the man would not be specific enough. Evaluation in King and Dickinson (2013) addresses two major questions. First, how accurate is the extraction of semantic information from potentially innovative sentences? Secondly, how much coverage does one have in a gold standard of semantic forms (triples), to capture the variability in meaning in learner sentences? We focus more on the first question and again use native speaker semantic forms as a proxy for a gold standard—albeit, limited by mismatches between native and (correct) non-native ways of saying the same thing. To mitigate this and better see the effect of spelling correction, much of our evaluation relies on hand-</context>
<context position="12363" citStr="King and Dickinson, 2013" startWordPosition="1930" endWordPosition="1933">tent. Second are content errors, responses for which the system extracts the desired subject, verb and object, but the resulting triple does not accurately describe the image (i.e., is an error of the participant’s). In this paper, we focus on reducing the triple errors, i.e., system errors. For example, the spelling error in (2) leads to a completely incorrect triple. We will unpack our error types in section 4. (2) A man swipped leaves. ⇒ leave(swipped,man) Focusing on triple (system) errors, we have obtained 92.3% accuracy on extraction for NNS data and roughly the same for NS data, 92.9% (King and Dickinson, 2013). Furthermore, more than half of the errors for NNSs involve misspellings (4.1% of the total 7.7% of errors). For a system interacting with learners, spelling errors are thus a high priority (cf. Hovermale, 2008). Content errors are subcategorized as spelling or meaning errors, depending on whether the resulting triple has spelling errors that do not result in real words—as in (3)—or that do result in real but unintended words and thus convey an inappropriate meaning (e.g., shout(man,bird) instead of shoot(man,bird)). We will see this distinction play out in the spelling correction techniques </context>
<context position="13741" citStr="King and Dickinson (2013)" startWordPosition="2147" endWordPosition="2150">013). These cases are ones for which the learner needs feedback, but there are two barriers in providing feedback: 1) without fixing the triple errors, they will be automatically grouped into the content error cases, since they do not match the gold standard; and 2) even if one knows something is an error, to obtain feedback one would ideally know the target the learner was (or should have been) aiming for. Our approach to spelling correction addresses both of these concerns by cleaning up the misspelled cases—including many of the “content” errors rooted in misspellings. Semantic coverage In King and Dickinson (2013), we take a set of native speaker (NS) responses for the same PDTs as the gold standard, garnering coverage numbers around 25% for types and 50% for tokens—i.e., about half of correct NNS responses are not in the gold standard. Since our focus is on improving accuracy, we use the same gold standard, but augment the analysis with hand-evaluation of whether a response should have been in the gold standard (section 4). Still, with spelling modifications being made to make a NNS response more native-like, we may be able to increase coverage, i.e., to find a (gold) NS triple that matches. 4 Error T</context>
<context position="18254" citStr="King and Dickinson (2013)" startWordPosition="2897" endWordPosition="2900">performing some action on or involving a boat would likely assume that the action involves using the boat for its intended purpose—to travel on water, and that could be represented with a more specific verb. We should also accept row(boy,NONE) here, because (unlike ride) the verb row sufficiently implies its object (a boat). Similarly, boat(boy,NONE) is adequate, because as a verb, boat indicates both the presence of a boat and the action of riding the boat. 48 5 Spelling Correction Modifications 5.1 Motivation for spelling correction via language modeling The initial approach to this task in King and Dickinson (2013) revealed that the ability of the system to recognize NNS responses as correct was often hindered by minor errors in spelling. Misspellings are especially problematic here because they can derail the semantic evaluation of a response by leading to errors in the syntactic interpretation of the sentence. Whereas human listeners or readers can use the context and their knowledge of the language to infer the intended pronunciation or spelling of a mispronounced or misspelled word, the initial approach lacked any such compensatory strategies. To improve the system’s ability to handle NNS data, we i</context>
<context position="28471" citStr="King and Dickinson (2013)" startWordPosition="4562" endWordPosition="4565">pus (Graff et al., 2007) containing roughly 250 million words in 10 million sentences of newspaper text. The candidate sentences are ranked according to their perplexity with regard to the language model. The sentence with lowest perplexity is selected as the most likely sentence and passed through the remaining steps of the pipeline, as shown in the lower half of Figure 3. The source of PDT descriptions We use the NS responses here as our proxy for a description of the picture content. This is distinct from using the NS responses as a gold standard to compare the final triples against, as in King and Dickinson (2013); indeed, this is why we manually check triples in this work, assuring that we truly know whether a triple is valid or not. In either case, we will see the limitations of using NS responses for these purposes. 51 6 Spelling Correction Evaluation Here we present the results of the modifications detailed above. At this stage, we are primarily interested in our system’s ability to robustly extract evaluable triples, potentially in the face of minor errors. While we present coverage scores in the following sections—calculating coverage with respect to the particular (and limited) gold standard set</context>
<context position="42050" citStr="King and Dickinson, 2013" startWordPosition="6942" endWordPosition="6945">cribing the images. Moreover, as this work will ideally lead toward a game or ILT, it may be preferable to allow for “partial credit” (and the presentation of feedback) in the case of triples that do not constitute a complete match but may match one or two of the subject, verb, and object. Similarly, as the correction module relies on the words used by NSs to influence corrections, expanding the list of “influential” words is likely to be beneficial. While in the current study this consisted of a simple list derived from the same responses in the gold standard, this is simply in keeping with (King and Dickinson, 2013) and may not be optimal. A more sophisticated approach could allow this influence to be probabilistic rather than binary, and could rely on methods like TF-IDF to determine which words in NS responses are particularly relevant to the item, and which words are incidental. Another obvious source of improvement for future work is in the choice of training texts for the LM, which was shown to have serious biases against the contents of the PDT responses, which tend to describe physical actions or scenarios not common in newspaper text. Finding training texts that contain the necessary kinds of sen</context>
</contexts>
<marker>King, Dickinson, 2013</marker>
<rawString>King, L. and Dickinson, M. (2013). Shallow semantic analysis of interactive learner sentences. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 11–21, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-03,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9470" citStr="Klein and Manning, 2003" startWordPosition="1454" endWordPosition="1457">ers in an intensive English as a Second Language program at Indiana University. This data set contains responses from 53 informants, including native speakers (NSs) (14 NSs, 39 NNSs), for a total of 530 sentences. The distribution of first languages (L1s) is: 16 Arabic, 7 Chinese, 14 English, 2 Japanese, 4 Korean, 1 Kurdish, 1 Polish, 2 Portuguese, and 6 Spanish. 3.2 Method As in King and Dickinson (2013), our method to obtain a semantic form from a NNS production takes two steps: 1) obtain a syntactic dependency representation from the off-the-shelf Stanford parser (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of hand-written rules. To illustrate this process, consider (1). This sentence is passed through the parser to obtain the dependency parse shown in Figure 2. Based on the presence of the nsubjpass (noun subject, passive) node, the extraction script takes the logical subject from under the agent label, the verb from root, and the logical object from nsubjpass. This results in the semantic triple shot(man,bird), lemmatized to shoot(man,bird), using the Stanford CoreNLP lemmatizer (Manning et al., 2014). Very little effort is needed:</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of ACL-03, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lachowicz</author>
</authors>
<date>2003</date>
<note>Enchant. http://abisource.com/projects/enchant.</note>
<contexts>
<context position="22767" citStr="Lachowicz, 2003" startWordPosition="3608" endWordPosition="3609">tation of this correction was the fact that misspellings resulting in real words were not addressed. For example, several participants responded to one item with the real word shout but clearly intended shoot (cf. A man shoots a bird). Indeed, evaluation of this simple Aspell approach revealed that it introduced significantly more errors than it corrected. Thus, we omit this method from further discussion and focus on a more contextually informed approach incorporating language modeling. LM pipeline In the approach discussed hereafter (the LM pipeline), Aspell (via the Enchant python package (Lachowicz, 2003)) is used to obtain a list of spelling suggestions for all words, including those that appear to be properly spelled. These candidate spellings are combined to form a list of candidate sentences for each response. Each candidate sentence is then compared with an n-gram language model to obtain a perplexity score— i.e., a measure of how likely the sentence is, given the LM. The candidate sentence with the lowest perplexity is chosen automatically as the best correction. A diagram of the entire semantic extraction process incorporating the spelling correction and language modeling tools is given</context>
</contexts>
<marker>Lachowicz, 2003</marker>
<rawString>Lachowicz, D. (2003). Enchant. http://abisource.com/projects/enchant.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>M Gamon</author>
<author>J Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan Claypool.</publisher>
<contexts>
<context position="3490" citStr="Leacock et al., 2010" startWordPosition="501" endWordPosition="504">learner data, and specifically interactive data (King and Dickinson, 2013), our specific goal is to use basic NLP (pre)processing—namely, language modeling for spelling correction—to make the meaning of a learner’s sentence clearer. We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture’s description), can greatly reduce downstream errors. This may seem like a niche problem, but: 1) spelling errors are generally a major problem in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we have right now, on picture description tasks (PDTs), connects not only with a desire for more interactive tools, but also for language assessment (Somasundaran and Chodorow, 2014); and 3) our work seeks to unpack the connection between relatively “shallow” errors, namely spelling errors, with “deeper” errors, namely semantic ones. Unlike, for example, linguistic abstractions such as part-of-speech, both are intimately rooted in the particular lexical items used. This then raises the question of whether we are modeling what the learner said (modulo</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Leacock, C., Chodorow, M., Gamon, M., and Tetreault, J. (2010). Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies. Morgan Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>M Surdeanu</author>
<author>J Bauer</author>
<author>J Finkel</author>
<author>S J Bethard</author>
<author>D McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="10039" citStr="Manning et al., 2014" startWordPosition="1544" endWordPosition="1547"> (de Marneffe et al., 2006; Klein and Manning, 2003), and 2) obtain a semantic form from the parse, via a small set of hand-written rules. To illustrate this process, consider (1). This sentence is passed through the parser to obtain the dependency parse shown in Figure 2. Based on the presence of the nsubjpass (noun subject, passive) node, the extraction script takes the logical subject from under the agent label, the verb from root, and the logical object from nsubjpass. This results in the semantic triple shot(man,bird), lemmatized to shoot(man,bird), using the Stanford CoreNLP lemmatizer (Manning et al., 2014). Very little effort is needed: the parser is pre-built; the decision tree is small; and the extraction rules are minimal. Note, too, that certain relations (e.g., det) are completely ignored in the extraction. (1) A bird is shot by a man. root vroot A bird is shot by a man Figure 2: The dependency parse of (1) One is able to use little effort in part due to the constraints in the pictures. For figure 1, for example, the artist, the man in the beret, and the man are all acceptable subjects, whereas if there were multiple men in the picture, the man would not be specific enough. Evaluation in K</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., and McClosky, D. (2014). The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60, Baltimore, Maryland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Meurers</author>
</authors>
<title>Natural language processing and language learning.</title>
<date>2012</date>
<booktitle>Encyclopedia of Applied Linguistics.</booktitle>
<editor>In Chapelle, C. A., editor,</editor>
<publisher>Blackwell.</publisher>
<contexts>
<context position="1764" citStr="Meurers, 2012" startWordPosition="246" endWordPosition="247">rection, language modeling. Levi King and Markus Dickinson 2014. Leveraging known semantics for spelling correction. Proceedings of the third workshop on NLP for computer-assisted language learning. NEALT Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 43–58. 43 1 Motivation Much current work on analyzing learner language focuses on grammatical error detection and correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent ComputerAssisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more conversational interactions (see Petersen, 2010). Such interactions are both more and less difficult to process: to provide feedback requires keeping track of the cont</context>
<context position="43023" citStr="Meurers, 2012" startWordPosition="7099" endWordPosition="7100">s for the LM, which was shown to have serious biases against the contents of the PDT responses, which tend to describe physical actions or scenarios not common in newspaper text. Finding training texts that contain the necessary kinds of sentences but also the sheer volume needed to cover the variability of NNS responses is a challenge for future experiments in this area. Given that this study primarily investigated transitive verbs, research on this problem will need to examine interactions with other types of constructions, including the definition of more elaborate semantic forms (Hahn and Meurers, 2012). Moving to a wider range of sentence types may require the use of a semantic role labeler or similar tools and has the potential to increase the complexity of spelling correction, due to, e.g., longer sentences. Acknowledgments We would like to thank the task participants, David Stringer for assistance in developing the task, and Kathleen Bardovi-Harlig, Marlin Howard and Jayson Deese for help in recruiting participants. We also thank Abigail Elston and Alex Rudnick for their helpful advice during the system development. Finally, for their insightful feedback, we would like to thank the two a</context>
</contexts>
<marker>Meurers, 2012</marker>
<rawString>Meurers, D. (2012). Natural language processing and language learning. In Chapelle, C. A., editor, Encyclopedia of Applied Linguistics. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Meurers</author>
<author>R Ziai</author>
<author>N Ott</author>
<author>S Bailey</author>
</authors>
<title>Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions. Special Issue on Free-text Automatic Evaluation.</title>
<date>2011</date>
<journal>International Journal of Continuing Engineering Education and Life-Long Learning (IJCEELL),</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="2134" citStr="Meurers et al., 2011" startWordPosition="303" endWordPosition="306">mmatical error detection and correction (e.g., Dale et al., 2012) and less on semantic analysis; many Intelligent ComputerAssisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more conversational interactions (see Petersen, 2010). Such interactions are both more and less difficult to process: to provide feedback requires keeping track of the content of the interaction, but such content can also be used to disambiguate new learner productions. We exploit this tension in the context of spelling correction, as semantic information severely restricts the learner’s expected content, and thus also their word forms. Since our overarching goal is to move towards the facilitation of ILTs and language assessment tools </context>
</contexts>
<marker>Meurers, Ziai, Ott, Bailey, 2011</marker>
<rawString>Meurers, D., Ziai, R., Ott, N., and Bailey, S. (2011). Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions. Special Issue on Free-text Automatic Evaluation. International Journal of Continuing Engineering Education and Life-Long Learning (IJCEELL), 21(4):355–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Petersen</author>
</authors>
<title>Implicit Corrective Feedback in Computer-Guided Interaction: Does Mode Matter? PhD thesis,</title>
<date>2010</date>
<institution>Georgetown University,</institution>
<location>Washington, DC.</location>
<contexts>
<context position="2245" citStr="Petersen, 2010" startWordPosition="321" endWordPosition="322">puterAssisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also focus more on grammatical feedback. An exception to this rule is Herr Komissar, an ILT for German learners that includes rather robust content analysis and sentence generation (DeSmedt, 1995), but this involves a great deal of hand-built tools and does not connect to modern NLP. Some work addresses content assessment for short answer tasks (Meurers et al., 2011), but there is still a need to move towards naturalistic, more conversational interactions (see Petersen, 2010). Such interactions are both more and less difficult to process: to provide feedback requires keeping track of the content of the interaction, but such content can also be used to disambiguate new learner productions. We exploit this tension in the context of spelling correction, as semantic information severely restricts the learner’s expected content, and thus also their word forms. Since our overarching goal is to move towards the facilitation of ILTs and language assessment tools that maximize free interaction, we have to deal with removing impediments to interaction. Given the preponderan</context>
</contexts>
<marker>Petersen, 2010</marker>
<rawString>Petersen, K. A. (2010). Implicit Corrective Feedback in Computer-Guided Interaction: Does Mode Matter? PhD thesis, Georgetown University, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Somasundaran</author>
<author>M Chodorow</author>
</authors>
<title>Automated measures of specific vocabulary knowledge from constructed responses (‘Use these words to write a sentence based on this picture’).</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>1--11</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="3716" citStr="Somasundaran and Chodorow, 2014" startWordPosition="537" endWordPosition="540"> sentence clearer. We examine methods for automatically correcting misspellings, showing that preprocessing with spelling correction tools, when information about the interactive context is known (i.e., the picture’s description), can greatly reduce downstream errors. This may seem like a niche problem, but: 1) spelling errors are generally a major problem in analyzing learner data (Leacock et al., 2010; Flor et al., 2013); 2) the specific focus we have right now, on picture description tasks (PDTs), connects not only with a desire for more interactive tools, but also for language assessment (Somasundaran and Chodorow, 2014); and 3) our work seeks to unpack the connection between relatively “shallow” errors, namely spelling errors, with “deeper” errors, namely semantic ones. Unlike, for example, linguistic abstractions such as part-of-speech, both are intimately rooted in the particular lexical items used. This then raises the question of whether we are modeling what the learner said (modulo some spelling variation), what the learner intended, or what the learner should have intended, an issue we take up in section 4, after covering the background in section 3. The methods are covered in section 5 and the evaluat</context>
</contexts>
<marker>Somasundaran, Chodorow, 2014</marker>
<rawString>Somasundaran, S. and Chodorow, M. (2014). Automated measures of specific vocabulary knowledge from constructed responses (‘Use these words to write a sentence based on this picture’). In Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 1–11, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
<author>A W Black</author>
<author>S Chen</author>
<author>S Kumar</author>
<author>M Ostendorf</author>
<author>C Richards</author>
</authors>
<title>Normalization of non-standard words.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="25622" citStr="Sproat et al. (2001)" startWordPosition="4075" endWordPosition="4078">forming candidate sentences for a NNS response assumes that while there may be misspellings, the number of words in the sentence is fixed. That is, a word may be replaced by another word, but no word may be removed and no additional words may be inserted. (Rare exceptions may occur when the spelling correction tool suggests that an unrecognized word be split into two words.) This is a limitation of the current implementation and should be addressed in the future, perhaps incorporating techniques for word normalization over word lattices from the speech recognition literature, such as those in Sproat et al. (2001). For a given NNS response in this pipeline, each token is given a status of fixed or unfixed. Each word enters the pipeline as unfixed; it is then compared with the stop words list, and if a match is found, the status is changed to fixed. The remaining unfixed words are then compared with the NS word list and again, matches are fixed. For any token with a fixed status, no candidate spelling corrections will be considered. Thus we assume that a NNS word that matches a stop word is correct, as English learners at this level are unlikely to misspell common function words. We also assume, given t</context>
</contexts>
<marker>Sproat, Black, Chen, Kumar, Ostendorf, Richards, 2001</marker>
<rawString>Sproat, R., Black, A. W., Chen, S., Kumar, S., Ostendorf, M., and Richards, C. (2001). Normalization of non-standard words. Computer Speech &amp; Language, 15(3):287–333.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000479">
<title confidence="0.996772">
Feature Selection for Highly Skewed Sentiment Analysis Tasks
</title>
<author confidence="0.999335">
Can Liu Sandra K¨ubler Ning Yu
</author>
<affiliation confidence="0.999972">
Indiana University Indiana University University of Kentucky
</affiliation>
<address confidence="0.700811">
Bloomington, IN, USA Bloomington, IN, USA Lexington, KY, USA
</address>
<email confidence="0.998323">
liucan@indiana.edu skuebler@indiana.edu ning.yu@uky.edu
</email>
<sectionHeader confidence="0.993892" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999738454545455">
Sentiment analysis generally uses large feature sets based on a bag-of-words approach, which
results in a situation where individual features are not very informative. In addition, many data
sets tend to be heavily skewed. We approach this combination of challenges by investigating
feature selection in order to reduce the large number of features to those that are discriminative.
We examine the performance of five feature selection methods on two sentiment analysis data
sets from different domains, each with different ratios of class imbalance.
Our finding shows that feature selection is capable of improving the classification accuracy only
in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios.
We also conclude that there does not exist a single method that performs best across data sets and
skewing ratios. However we found that TF ∗ IDF2 can help in identifying the minority class
even in highly imbalanced cases.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.973301307692307">
In recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008;
Bollen et al., 2011; Liu, 2012). Sentiment analysis is concerned with extracting opinions or emotions
from text, especially user generated web content. Specific tasks include monitoring mood and emotion;
differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinion
strength; and identifying other opinion properties. At this point, two major approaches exists: lexicon
and machine learning based. The lexicon-based approach uses high quality, often manually generated
features. The machine learning-based approach uses automatically generated feature sets, which are from
various sources of evidence (e.g., part-of-speech, n-grams, emoticons) in order to capture the nuances of
sentiment. This means that a large set of features is extracted, out of which only a small subset may be
good indicators for the sentiment.
One major problem associated with sentiment analysis of web content is that for many topics, these
data sets tend to be highly imbalanced. There is a general trend that users are willing to submit positive
reviews, but they are much more hesitant to submit reviews in the medium to low ranges. For example,
for the YouTube data set that we will use, we collected comments for YouTube videos from the comedy
category, along with their ratings. In this data set, more than 3/4 of all ratings consist of the highest rating
of 5. For other types of user generated content, they opposite may be true.
Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data
sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced
data sets during data collection, by sampling a certain number of positive and negative reviews, or by se-
lecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review
data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set
allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the
movie review data set has been used as a benchmark data set that allows for comparisons of various sen-
timent analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.926602">
2
</page>
<note confidence="0.989016">
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11,
Dublin, Ireland, August 24 2014.
</note>
<bodyText confidence="0.9998032">
and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed com-
petitive feature selection methods evaluated on the movie review data set. However, the generalizability
of such feature selection methods to imbalanced data sets, which better represent real world situations,
has not been investigated in much detail. Forman (2003) provides an extensive study of feature selection
methods for highly imbalanced data sets, but he uses document classification as task.
This current paper investigates the robustness of three feature selection methods that Forman (2003)
has shown to be successful, as well as two variants of TF * IDF. The three methods are Odds-Ratio
(OR), Information Gain (IG), and Binormal Separation (BNS). BNS has been found to perform signif-
icantly better than other methods in more highly skewed tasks (Forman, 2003). The two variants of
TF * IDF differ in the data set used for calculating document frequency. We investigate the behavior
of these methods on a subtask of sentiment analysis, namely the prediction of user ratings. For this, we
will use data sets from two different domains in order to gain insight into whether or not these feature
selection methods are robust across domains and across skewing ratios: One set consists of user reviews
from Epicurious1, an online community where recipes can be exchanged and reviewed, the other set
consists of user reviews of YouTube comedy videos.
The remainder of this paper is organized as follows: In section 2, we explain the rationale for applying
feature selection and introduce the feature selection methods that are examined in this paper. Section
3 introduces the experimental settings, including a description of the two data sets, data preprocessing,
feature representation, and definition of the binary classification tasks. In section 4, we present and
discuss the results for the feature selection methods, and in section 5, we conclude.
</bodyText>
<sectionHeader confidence="0.825253" genericHeader="method">
2 Feature Selection and Class Skewing
</sectionHeader>
<bodyText confidence="0.957999458333333">
In a larger picture, feature selection is a method (applicable both in regression and classification prob-
lems) to identify a subset of features to achieve various goals: 1) to reduce computational cost, 2) to
avoid overfitting, 3) to avoid model failure, and 4) to handle skewed data sets for classification tasks.
We concentrate on the last motivation, even though an improvement of efficiency and the reduction of
overfitting are welcome side effects. The feature selection methods studied in this paper have been used
in text classification as well, which is a more general but similar task using n-gram features. However,
since all measures are intended for binary classification problems, we reformulate the rating prediction
into a binary classification problem (see section 3.5).
Feature selection methods can be divided into wrapper and filter methods. Wrapper methods use
the classification outcome on a held-out data set to score feature subsets. Standard wrapper methods
include forward selection, backward selection, and genetic algorithms. Filter methods, in contrast, use
an independent measure rather than the error rate on the held-out data. This means that they can be
applied to larger feature sets, which may be unfeasible with wrapper methods. Since sentiment analysis
often deals with high dimensional feature representation, we will concentrate on filter methods for our
feature selection experiments.
Previous research (e.g. (Brank et al., 2002b; Forman, 2003)) has shown that Information Gain and
Odds Ratio have been used successfully across different tasks and that Binormal Separation has good
recall for the minority class under skewed class distributions. So we will investigate them in this paper.
Other filter methods are not investigated in this paper due to two main concerns: We exclude Chi-
squared and Z-score, statistical tests because they require a certain sample size. Our concern is that
their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference
and Probability Proportion Difference since they do not normalize over the sample of size of positive
and negative classes. Thus, our concern is that they may not provide a fair estimate for features from a
skewed data sets.
</bodyText>
<subsectionHeader confidence="0.924576">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.6167775">
Following Zheng et al. (2004), feature selection methods can be divided into two groups: one-sided and
two-sided measures. One-sided measures assign a high score to positively-correlated features and a low
</bodyText>
<footnote confidence="0.996836">
1www.epicurious.com
</footnote>
<page confidence="0.998309">
3
</page>
<bodyText confidence="0.998482888888889">
score to negative features while two-sided measures prefer highly distinguishing features, independent
of whether they are positively or negatively correlated. Zheng et al. (2004) note that the ratio of positive
and negative features affects precision and recall of the classification, especially for the minority class.
For one-sided methods, we have control over this ratio by selecting a specified number of features one
each side; for two-sided methods, however, we do not have this control. In this paper, we will keep a 1:1
ratio for one-sided methods. For example, if we select 1 000 features, we select the 500 highest ranked
features for the positive class, and the 500 highest ranked features for the negative class. When using
two-sided methods, the 1 000 highest ranked features are selected.
For the discussion of the feature selection methods, we use the following notations:
</bodyText>
<listItem confidence="0.999998857142857">
• S: target or positive class.
• S: negative class.
• DS: The number of documents in class S.
• DS: The number of documents in class S.
• DSf: The number of documents in class S where feature f occurs.
• DSf: The number of documents in class S where feature f occurs.
• TSf: The number of times feature f occurs in class S.
</listItem>
<subsectionHeader confidence="0.997721">
2.2 Feature Selection Methods
</subsectionHeader>
<bodyText confidence="0.999203833333333">
In addition to Information Gain, Odds Ratio and Bi-Normal Separation, TF ∗ IDF is included for
comparison purposes. We define these measures for binary classification as shown below.
Information Gain (IG): IG is a two-sided measure that estimates how much is known about an unob-
served random variable given an observed variable. It is defined as the entropy of one random variable
minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S
given a feature f:
</bodyText>
<equation confidence="0.997240666666667">
1] 1] P(f, S)
IG = H(S) − H(S|f) = P(f, S)log
fEI0,11 SE10,11 P(f)P(S)
</equation>
<bodyText confidence="0.9947735">
Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features
over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare
features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman
(2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed
that IG has a high precision with respect to the minority class.
Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds
of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that
a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated:
</bodyText>
<equation confidence="0.99815">
P(f, S)(1 − P(f, S))
OR = log
P(f, S)(1 − P(f, S))
</equation>
<bodyText confidence="0.742435666666667">
Brank et al. (2002b) showed that OR requires a high number of features to achieve a given feature
vector sparsity because it prefers rare terms. Features that occur in very few documents of class S and
do not occur in S have a small denominator, and thus a rather large OR value.
</bodyText>
<page confidence="0.995187">
4
</page>
<bodyText confidence="0.998415166666667">
Bi-Normal Separation (BNS): BNS (Forman, 2003) is a two-sided measure that regards the proba-
bility of feature f occurring in class S as the area under the normal distribution bell curve. The whole
area under the bell curve corresponds to 1, and the area for a particular feature has a corresponding
threshold along the x-axis (ranging from negative infinite to positive infinite). For a feature f, one can
find the threshold that corresponds to the probability of occurring in the positive class, and the threshold
corresponding to the probability of occurring in S. BNS measures the separation in these two thresholds:
</bodyText>
<equation confidence="0.97824">
BNS = |F−1(DSf ) − F−1( DSf )|
DS DS
</equation>
<bodyText confidence="0.998686818181818">
where F−1 is the inverse function of the standard normal cumulative probability distribution. As we can
see, the F−1 function exaggerates an input more dramatically when the input is close to 0 or 1 which
means that BNS perfers rare words.
Term Frequency * Inverse Document Frequency (TF*IDF): TF * IDF was originally proposed
for information retrieval tasks, where it measures how representative a term is for the document in which
it occurs. When TF * IDF is adopted for binary classification, we calculate the TF * IDF of a feature
w.r.t. the positive class (normalized) and the TF * IDF w.r.t. the negative class (normalized). We obtain
the absolute value of the difference of these two measures. If a feature is equally important in both
classes and thus would not contribute to classification, it receives a small value. The larger the value,
the more discriminative the feature. We apply two variants of TF * IDF, depending on how IDF is
calculated:
</bodyText>
<equation confidence="0.9813155">
TF * IDF1 = (0.5 + 0.5 x TSf x lo Ds+ DS)
maxi (TSfi)) log( DS f
TF * IDF2 = (0.5 + 0.5 x TSf x lo Ds)
maxi (TSfi)) log( DS f
</equation>
<bodyText confidence="0.9997645">
In the first variant, TF * IDF1, document frequency is based on the whole set of examples while in
the second variant, TF * IDF2, document frequency is based only on the class under consideration, S.
</bodyText>
<sectionHeader confidence="0.996112" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.998079">
3.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.998828941176471">
Epicurious Data Set: We developed a web crawler to scrape user reviews for 10 146 recipes, published
on the Epicurious website before and on April 02, 2013. On the website, each recipe is assigned a rating
of 1 to 4 forks, including the intermediate values of 1.5, 2.5, and 3.5. This is an accumulated rating over
all user reviews. (Reviews with ratings of 0 were excluded, they usually indicate that recipes have not
received any ratings.) We rounded down all the half ratings, e.g., 1.5 forks counts as 1 fork, based on
the observation that users are generous when rating recipes. Our experiments classify each recipe by
aggregating over all its reviews. While a little more than half of the recipes received 1 to 10 reviews,
there are recipes with more than 100 reviews. To avoid an advantage for highly reviewed recipes, we
randomly selected 10 reviews if a recipe has more than 10 reviews. Recipes with less than 3 reviews
were eliminated since they do not provide enough information. After these clean-up steps, the data set
has the distribution of ratings shown in table 1.
YouTube Data Set: Using the Google YouTube Data API, we collected average user ratings and user
comments for a set of YouTube videos in the category Comedy. Each video is rated from 1 to 5. The
distribution of ratings among all YouTube videos is very skewed, as illustrated in figure 1. Most videos
are rated highly; very few are rated poorly. The 1% quantile is 1.0; the 6.5% quantile is 3.0; the 40%
quantile is 4.75; the 50% quantile is 4.85; and the 77% quantile is 5.0. We selected a set of 3 000 videos.
Videos with less than 5 comments or with non-English comments are discarded.
</bodyText>
<page confidence="0.985064">
5
</page>
<table confidence="0.9683602">
rating no.
1 fork 44 recipes
2 forks 304 recipes
3 forks 1416 recipes
4 forks 1368 recipes
</table>
<tableCaption confidence="0.999287">
Table 1: The distribution of ratings in the Epicurious data set.
</tableCaption>
<figure confidence="0.610411">
Video Rating
</figure>
<figureCaption confidence="0.998602">
Figure 1: Skewing in the YouTube data set.
</figureCaption>
<subsectionHeader confidence="0.999732">
3.2 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.99999275">
Before feature extraction, basic preprocessing is conducted for both data sets individually. For the Epi-
curious data set, we perform stemming using the Porter Stemmer (Porter, 1980) to normalize words, and
rare words (≤ 4 occurrences) are removed. On the YouTube data set, we perform spelling correction
and normalization because the writing style is rather informal. Our normalizer collapses repetitions into
their original forms plus a suffix “RPT”, thus retaining this potentially helpful clue for reviewer’s strong
emotion without increasing the features due to creative spelling. For example, “loooooove” is changed
to “loveRPT” and“lolololol” to “lolRPT”. The normalizer also replaces all emoticons by either“EMOP”
for positive emoticons or “EMON” for negative emoticons. Besides a standard English dictionary, we
also use the Urban Dictionary2 since it has a better coverage of online abbreviations.
We do not filter stop words for two reasons: 1) Stop words are domain dependent, and some En-
glish stop words may be informative for sentiment analysis, and 2) uninformative words that are equally
common in both classes will be excluded by feature selection if the method is successful.
</bodyText>
<subsectionHeader confidence="0.999472">
3.3 Feature representation
</subsectionHeader>
<bodyText confidence="0.99973">
Since our focus is on settings with high numbers of features, we use a bag-of-words approach, in which
every word represents one feature, and its term frequency serves as its value. Different feature weighting
methods, including binary weighting, term frequency, and TF*IDF have been adopted in past sentiment
analysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)). (Pang et al., 2002) found that
simply using binary feature weighting performed better than using more complicated weightings in a
task of classifying positive and negative movie reviews. However, movie reviews are relatively short, so
there may not be a large difference between binary features and others. Topic classification usually uses
term frequency as feature weighting. TF * IDF and variants were shown to perform better than binary
weighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010).
Since our user rating prediction tasks aggregate all user comments into large documents and predict
ratings per recipe/YouTube video, term frequency tends to capture richer information than binary fea-
</bodyText>
<footnote confidence="0.534546">
2http://www.urbandictionary.com/
</footnote>
<figure confidence="0.876873">
1 2 3 4 5
0.0 0.5 1.0 1.5 2.0 2.5
Density
</figure>
<page confidence="0.969101">
6
</page>
<table confidence="0.9983876">
Epicurious no. POS YouTube no. NEG no. POS
ratio no. NEG ratio
1:8 348 2 784 1:10 56 559
1:1.57 348 547 1:1.57 356 559
1:1 348 348 1:1 559 559
</table>
<tableCaption confidence="0.999303">
Table 2: Skewing ratios and sizes of positive and negative classes for both data sets.
</tableCaption>
<bodyText confidence="0.999656888888889">
tures. Thus, we use term frequency weighting for simplicity, not to deviate from the focus of feature
selection methods. Since there is a considerable variance in term frequency in the features, we normalize
the feature values to [0,1] to avoid large feature values from dominating the vector operations in classifier
optimization.
For the Epicurious data, the whole feature set consists of 10 677 unigram features. For YouTube, the
full feature set of features consists of 23 232 unigram features. We evaluate the performance of feature
selection methods starting at 500 features, at a step-size of 500. For the Epicurious data, we include up to
10 500 features. For the YouTube data, we stop at 15 000 features due to prohibitively long classification
times.
</bodyText>
<subsectionHeader confidence="0.828781">
3.4 Classifier
</subsectionHeader>
<bodyText confidence="0.99999575">
The classifier we use in this paper is Support Vector Machines (SVMs) in the implementation of
SVMlight (Joachims, 1999). Because algorithm optimization is not the focus of this study, we use the
default linear kernel and other default parameter values. Classification results are evaluated by accuracy
as well as precision and recall for individual classes.
</bodyText>
<subsectionHeader confidence="0.971087">
3.5 Binary Classification
</subsectionHeader>
<bodyText confidence="0.999840076923077">
Since all feature selection methods we use in our experiments are defined under a binary classification
scenario, we need to redefine the rating prediction task. For both data sets, this means, we group the
recipes and videos into a positive and a negative class. A baseline classifier predicts every instance as the
majority class. For both data sets, the majority class is positive.
For the Epicurious data set, 1-fork and 2-fork recipes are grouped into the negative class (NEG), and
3-fork and 4-fork recipes are grouped into the positive class (POS), yielding a data set of 348 NEG and
2 784 POS recipes (skewing ratio: 1:8). The different skewing ratios we use are shown in table 2. 2/3 of
the data is used for training, and 1/3 for testing, with the split maintaining the class ratio. Note that for
the less skewed settings, all NEG instances were kept while POS instances were sampled randomly.
For the YouTube data set, we sample from all videos with rating 5 for the positive class and from
all videos with ratings between and including 1 and 3 for the negative class. This yields 559 POS and
559 NEG videos. The different skewing ratios we use are shown in table 2. 7/8 of the data is used for
training, and 1/8 for testing, with the split maintaining the class ratio.
</bodyText>
<sectionHeader confidence="0.7593085" genericHeader="method">
4 Results
4.1 Results for the Epicurious Data Set
</sectionHeader>
<bodyText confidence="0.999412555555556">
The results for the Epicurious data set with different skewing ratios are shown in figure 2. The accuracy
of the baseline is 50% for the 1:1 ratio, 61% for 1:1.57, and 88.9% for 1:8.
The results show that once we use a high number of features, all the feature selection methods perform
the same. This point where they conflate is reached at around 4 000 features for the ratios of 1:1 and
1:1.57. There, accuracy reaches around 71%. For the experiment with the highest skewing, this point is
reached much later, at around 8 000 features. For this setting, we also reach a higher accuracy of around
89%, which is to be expected since we have a stronger majority class. Note that once the conflation point
is reached, the accuracy also corresponds to the accuracy when using the full feature set. This accuracy
is always higher than that of the baseline.
</bodyText>
<page confidence="0.989361">
7
</page>
<figure confidence="0.996964534883721">
2000 4000 6000 8000 10000
Number of Features Chosen
2000 4000 6000 8000 10000
Number of Features Chosen
Epicurious 1:1− Accuracy
●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
Epicurious 1:1.57 − Accuracy
● ● ●
● ●
IG
BNS
OR
TFIDF1
TFIDF2
baseline = 0.5
Accuracy
0.60 0.65 0.70 0.75 0.80
● ● ●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
Accuracy
0.66 0.68 0.70 0.72 0.74 0.76
●
●
●
IG
BNS
OR
TFIDF1
TFIDF2
baseline
Epicurious 1:8 − Accuracy
2000 4000 6000 8000 10000
Accuracy
0.880 0.885 0.890 0.895 0.900
● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ●
● ● ● ● ●
● ● ● ● ●
● ● ● ● ● ● ●
● ● ● ● ●
●
● ●
●
●
●
IG
BNS
OR
TFIDF1
TFIDF2
baseline
Number of Features Chosen
</figure>
<figureCaption confidence="0.988042">
Figure 2: The results for the Epicurious data set.
</figureCaption>
<figure confidence="0.999146416666667">
Epicurious 1:8 − Precision NEG class
Number of Features Chosen
Epicurious 1:8 − Recall NEG class
Number of Features Chosen
2000 4000 6000 8000 10000
Precision
0.0 0.2 0.4 0.6 0.8 1.0
4
4
44 44 44 444 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
4
4
4
4
IG
BNS
OR
TFIDF1
TFIDF2
2000 4000 6000 8000 10000
Recall
0.00 0.02 0.04 0.06 0.08 0.10
4
4
44 44 44 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
4
4
4
4
IG
BNS
OR
TFIDF1
TFIDF2
</figure>
<figureCaption confidence="0.999993">
Figure 3: Precision and recall for the negative cases in the Epicurious set, given a 1:8 skewing.
</figureCaption>
<bodyText confidence="0.999954375">
The results also show that the most pronounced differences between feature selection methods occur in
the balanced data set. In the set with the highest skewing, the differences are minor, and only TF *IDF2
improves over the baseline when using 1 000 features.
Another surprising result is that TF*IDF2, OR, and BNS have a tendency to fluctuate between higher
and lower results than the setting using all features. This means that it is difficult to find a good cut-off
point for these methods. TF * IDF1 and IG show clear performance gains for the balanced setting, but
they also show more fluctuation in settings with higher skewing.
From these results, we can conclude that for sentiment analysis tasks, feature selection is useful only
in a balanced or slightly skewed cases if we are interested in accuracy. However, a look at the precision
and recall given the highest skewing (see figure 3) shows that TF * IDF2 in combination with a small
number of features is the only method that finds at least a few cases of the minority class. Thus if a
good performance on the minority class examples is more important than overall accuracy, TF * IDF2
is a good choice. One explanation is that TF * IDF2 concentrates on one class and can thus ignore the
otherwise overwhelming positive class completely. TF * IDF1 and OR have the lowest precision, and
BNS fluctuates. Where recall is concerned, TF * IDF1 and IG reach the highest recall given a small
feature set.
</bodyText>
<sectionHeader confidence="0.885122" genericHeader="evaluation">
4.2 Results for the YouTube Data Set
</sectionHeader>
<bodyText confidence="0.9999625">
The results for the YouTube data set with different skewing ratios are shown in figure 4. The accuracy of
the baseline is 50% for the 1:1 ratio, 61.08% for 1:1.57, and 90.9% for 1:10.
The results show that even though the YouTube data set is considerably smaller than the Epicurious
one, is does profit from larger numbers of selected features: For the balanced and the low skewing, there
</bodyText>
<page confidence="0.998047">
8
</page>
<figureCaption confidence="0.999866">
Figure 5: Precision and recall for the negative cases in the YouTube set, given a 1:1.57 skewing.
</figureCaption>
<bodyText confidence="0.999996214285714">
is no point at which the methods conflate. The results for the highly skewed case show that no feature
selection method is capable of finding cases of the minority class: all methods consistently identify only
one instance of the negative class. However, this may be a consequence of the small data set. In terms of
accuracy, we see that a combination of a small number of features with either IG or TF * IDF1 provides
the best results. For the YouTube data set, BNS also performs well but requires a larger set of features for
reaching its highest accuracy. We assume that this is the case because BNS has a tendency to prefer rare
words. Note that we did not test for number of features greater than 15 000 because of the computation
cost, but we can see that the performance curve for different feature selection methods tends to conflate
to the point that represents the full feature set.
If we look at the performance of different feature selection methods on identification of minority class
instances, we find that TF * IDF2 again manages to increase recall in the highly skewed case, but this
time at the expense of precision. For the 1:1.57 ratio, all methods reach a perfect precision when a high
number of features is selected, see figure 5. TF * IDF2 is the only method that reaches this precision
with small numbers of features, too. However, this is not completely consistent.
</bodyText>
<subsectionHeader confidence="0.99439">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999327">
If we compare the performance curves across data sets and skewing ratios and aim for high accuracy,
we see that there is no single feature selection method that is optimal in all situations. Thus, we have to
conclude that the choice of feature selection method is dependent on the task. In fact, the performance
of a feature selection method could depend on many factors, such as the difficulty of the classification
task, the preprocessing step, the feature generation decision, the data representation scheme (Brank et
al., 2002a), or the classification model (e.g., SVM, Maximum Entropy, Naive Bayes).
</bodyText>
<figure confidence="0.995599">
Youtube Equal − Accuracy
Youtube 1:1.57 − Amu-7
Youtube 1:10 − Accuracy
Youtube 1:1.57 − Precision NEG class
Youtube 1:1.57 − Recall NEG class
0 5000 10000 15000
Number of Features Chosen
0 5000 10000 15000
Number of Features Chosen
0 5000 10000 15000
Number of Features Chosen
</figure>
<figureCaption confidence="0.983322">
Figure 4: The results for the YouTube set.
</figureCaption>
<figure confidence="0.998553714285714">
0 5000 10000 15000
Number of Features Chosen
0 5000 10000 15000
Number of Features Chosen
IG
BNS
OR
IG
BNS
OR
TFIDF1
TFIDF2
baseline
●
●
●
●
●
● ●
baseline
● ●
●
● ●
●
Accuracy
0.50 0.55 0.60 0.65 0.70 0.75
Accuracy
0.60 0.65 0.70 0.75
</figure>
<equation confidence="0.868675548387097">
● ● TFIDF1
● ● ● ● TFIDF2
● ●
● ● ● ● ●
● ● ● ● ● ●
● ●
● ●
● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●
● ●
● ● ● ● ●
● ● ● ● ● ●
● ● ●
● ● ● ●
● ● ●
● ● ● ●
● ● ● ● ● ●
● ● ● ●
● ●
● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ●
● ● ● ● ● ● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ●
● ●
● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
</equation>
<figure confidence="0.993354952380952">
● ●
● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
Accuracy
0.80 0.85 0.90 0.95
●
●
IG
BNS
OR
TFIDF1
TFIDF2
baseline
●
●
●
0.70 0.75 0.80 0.85 0.90 0.95 1.00
0.0 0.1 0.2 0.3 0.4 0.5
●
● ●
● ● ● ●
● ● ●
● ●●●
● ● ●
● ●
●
●
●
●
● ● ● ● ● ● ● ● ● ● ● ●
● ●
IG
BNS
OR
TFIDF1
TFIDF2
●
●
●
Precision
Recall
● ●● ● ● ● ● ● ●● ●
● ● ●
●
● ● ● ● ● ●
●●
● ●● ● ● ● ● ●
● ● ● ● ●
● ● ● ● ●
● ●
● ● ● ● ● ● ● ●
● ● ● ●
● ●
●
●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ●
● ● ●
● ● ●
●
●
● ●
●●
●
●
●
●
IG
BNS
OR
TFIDF1
TFIDF2
●
●
●
●
● ● ● ●
● ● ● ● ● ● ● ● ●
●
● ●
● ● ● ● ● ●
●●
</figure>
<page confidence="0.976437">
9
</page>
<bodyText confidence="0.999992875">
We have also shown on two different data sets, each with three skewing ratios, that it is difficult for
feature selection methods to mitigate the effect of highly skewed class distributions while we can still
improve performance by using a reduced feature set for slightly skewed cases. Thus, the higher the skew-
ing of the data set is, the more difficult it is to find a feature selection method that has a positive effect,
and parameters, such as feature set size, have to be optimized very carefully. Thus, feature selection is
much less effective in highly skewed user rating tasks than in document classification.
However, if the task requires recall of the minority class, our experiments have shown that TF ∗IDF2
is able to increase this measure with a small feature set, even for highly imbalanced cases.
</bodyText>
<sectionHeader confidence="0.993717" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999934866666667">
In this paper, we investigated whether feature selection methods reported to be successful for document
classification perform robustly in sentiment classification problems with a highly skewed class distribu-
tion. Our findings show that feature selection methods are most effective when the data sets are balanced
or moderately skewed, while for highly imbalanced cases, we only saw an improvement in recall for the
minority class.
In the future, we will extend feature selection methods – originally defined for binary classification
scenarios – to handle multi-class classification problems. A simple way of implementing this is be to
break multi-class classification into several 1-vs.-all or 1-vs.-1 tasks, perform feature selection on these
binary tasks and then aggregate them. Another direction that we want to take is integrating more complex
features, such as parsing or semantic features, into the classification task to investigate how they influence
feature selection. In addition, we will compare other approaches against feature selection for handling
highly skewed data sets, including classification by rank, ensemble learning methods, and memory-
based learning with a instance-specific weighting. Finally, a more challenging task is to select features
for sentiment analysis problems where no annotation (feature selection under unsupervised learning) or
few annotations (feature selection under semi-supervised learning) are available.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998697260869565">
Basant Agarwal and Namita Mittal. 2012. Categorical probability proportion difference (CPPD): A feature selec-
tion method for sentiment classification. In Proceedings of the 2nd Workshop on Sentiment Analysis where AI
meets Psychology (SAAIP), pages 17–26.
Basant Agarwal and Namita Mittal. 2013. Sentiment classification using rough set based hybrid feature selection.
In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment &amp; Social Media
Analysis (WASSA), pages 115–119, Atlanta, GA.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011. Twitter mood predicts the stock market. Journal of Compu-
tational Science, 2:1–8.
Janez Brank, Marko Grobelnik, Nata&amp;quot;sa Milic-Frayling, and Dunja Mladenic. 2002a. An extensive empirical study
of feature selection metrics for text classification. In Proceedings of the Third International Conference on Data
Mining Methods and Databases for Engineering, Finance and Other Fields, Bologna, Italy.
Janez Brank, Marko Grobelnik, Nata&amp;quot;sa Milic-Frayling, and Dunja Mladenic. 2002b. Feature selection using
Linear Support Vector Machines. Technical Report MSR-TR-2002-63, Microsoft Research.
George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289–1305.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector Learning. MIT-Press.
Olena Kummer and Jaques Savoy. 2012. Feature selection in sentiment analysis. In Proceeding of the Conf´erence
en Recherche d’Infomations et Applications (CORIA), pages 273–284, Bordeaux, France.
Shoushan Li, Rui Xia, Chengqing Zong, and Chu-Ren Huang. 2009. A framework of feature selection methods
for text categorization. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language Processing of the AFNLP, pages 692–700, Suntec,
Singapore.
</reference>
<page confidence="0.949706">
10
</page>
<reference confidence="0.999735043478261">
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan &amp; Claypool Publishers.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 142–150, Portland, OR.
Frederick Mosteller. 1968. Association and estimation in contingency tables. Journal of the American Statistical
Association, 63(321):1–28.
Tim O’Keefe and Irena Koprinska. 2009. Feature selection and weighting methods in sentiment analysis. In Pro-
ceedings of the 14th Australasian Document Computing Symposium (ADCS), pages 67–74, Sydney, Australia.
Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
1386–1395, Uppsala, Sweden.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Lin-
guistics, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79–86, Philadelphia, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130—137.
Zhaohui Zheng, Xiayun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced
data. ACM SIGKDD Explorations Newsletter, 6(1):80–89.
</reference>
<page confidence="0.999484">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901041">
<title confidence="0.999974">Feature Selection for Highly Skewed Sentiment Analysis Tasks</title>
<author confidence="0.99717">Can Liu Sandra K¨ubler Ning Yu</author>
<affiliation confidence="0.999998">Indiana University Indiana University University of Kentucky</affiliation>
<address confidence="0.914246">Bloomington, IN, USA Bloomington, IN, USA Lexington, KY,</address>
<email confidence="0.999661">liucan@indiana.eduskuebler@indiana.eduning.yu@uky.edu</email>
<abstract confidence="0.999028">Sentiment analysis generally uses large feature sets based on a bag-of-words approach, which results in a situation where individual features are not very informative. In addition, many data sets tend to be heavily skewed. We approach this combination of challenges by investigating feature selection in order to reduce the large number of features to those that are discriminative. We examine the performance of five feature selection methods on two sentiment analysis data sets from different domains, each with different ratios of class imbalance. Our finding shows that feature selection is capable of improving the classification accuracy only in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios. We also conclude that there does not exist a single method that performs best across data sets and ratios. However we found that help in identifying the minority class even in highly imbalanced cases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Basant Agarwal</author>
<author>Namita Mittal</author>
</authors>
<title>Categorical probability proportion difference (CPPD): A feature selection method for sentiment classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP),</booktitle>
<pages>17--26</pages>
<contexts>
<context position="3617" citStr="Agarwal and Mittal (2012)" startWordPosition="553" endWordPosition="556">d. Researchers either generate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11, Dublin, Ireland, August 24 2014. and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed competitive feature selection methods evaluated on the movie review data set. However, the generalizability of suc</context>
</contexts>
<marker>Agarwal, Mittal, 2012</marker>
<rawString>Basant Agarwal and Namita Mittal. 2012. Categorical probability proportion difference (CPPD): A feature selection method for sentiment classification. In Proceedings of the 2nd Workshop on Sentiment Analysis where AI meets Psychology (SAAIP), pages 17–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Basant Agarwal</author>
<author>Namita Mittal</author>
</authors>
<title>Sentiment classification using rough set based hybrid feature selection.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA),</booktitle>
<pages>115--119</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="3644" citStr="Agarwal and Mittal (2013)" startWordPosition="557" endWordPosition="560">ate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11, Dublin, Ireland, August 24 2014. and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed competitive feature selection methods evaluated on the movie review data set. However, the generalizability of such feature selection methods</context>
</contexts>
<marker>Agarwal, Mittal, 2013</marker>
<rawString>Basant Agarwal and Namita Mittal. 2013. Sentiment classification using rough set based hybrid feature selection. In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment &amp; Social Media Analysis (WASSA), pages 115–119, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiao-Jun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<pages>2--1</pages>
<contexts>
<context position="1384" citStr="Bollen et al., 2011" startWordPosition="203" endWordPosition="206">ifferent domains, each with different ratios of class imbalance. Our finding shows that feature selection is capable of improving the classification accuracy only in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios. We also conclude that there does not exist a single method that performs best across data sets and skewing ratios. However we found that TF ∗ IDF2 can help in identifying the minority class even in highly imbalanced cases. 1 Introduction In recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008; Bollen et al., 2011; Liu, 2012). Sentiment analysis is concerned with extracting opinions or emotions from text, especially user generated web content. Specific tasks include monitoring mood and emotion; differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinion strength; and identifying other opinion properties. At this point, two major approaches exists: lexicon and machine learning based. The lexicon-based approach uses high quality, often manually generated features. The machine learning-based approach uses automatically generated feature sets, which are from va</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Brank</author>
<author>Marko Grobelnik</author>
<author>Natasa Milic-Frayling</author>
<author>Dunja Mladenic</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Data Mining Methods and Databases for Engineering, Finance and Other Fields,</booktitle>
<location>Bologna, Italy.</location>
<contexts>
<context position="7472" citStr="Brank et al., 2002" startWordPosition="1139" endWordPosition="1142">er methods. Wrapper methods use the classification outcome on a held-out data set to score feature subsets. Standard wrapper methods include forward selection, backward selection, and genetic algorithms. Filter methods, in contrast, use an independent measure rather than the error rate on the held-out data. This means that they can be applied to larger feature sets, which may be unfeasible with wrapper methods. Since sentiment analysis often deals with high dimensional feature representation, we will concentrate on filter methods for our feature selection experiments. Previous research (e.g. (Brank et al., 2002b; Forman, 2003)) has shown that Information Gain and Odds Ratio have been used successfully across different tasks and that Binormal Separation has good recall for the minority class under skewed class distributions. So we will investigate them in this paper. Other filter methods are not investigated in this paper due to two main concerns: We exclude Chisquared and Z-score, statistical tests because they require a certain sample size. Our concern is that their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference and Probability Proportion Difference</context>
<context position="10337" citStr="Brank et al. (2002" startWordPosition="1611" endWordPosition="1614">Feature Selection Methods In addition to Information Gain, Odds Ratio and Bi-Normal Separation, TF ∗ IDF is included for comparison purposes. We define these measures for binary classification as shown below. Information Gain (IG): IG is a two-sided measure that estimates how much is known about an unobserved random variable given an observed variable. It is defined as the entropy of one random variable minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S given a feature f: 1] 1] P(f, S) IG = H(S) − H(S|f) = P(f, S)log fEI0,11 SE10,11 P(f)P(S) Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman (2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed that IG has a high precision with respect to the minority class. Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds of feature f occurring in class S to the odd</context>
<context position="26769" citStr="Brank et al., 2002" startWordPosition="4628" endWordPosition="4631">ll numbers of features, too. However, this is not completely consistent. 4.3 Discussion If we compare the performance curves across data sets and skewing ratios and aim for high accuracy, we see that there is no single feature selection method that is optimal in all situations. Thus, we have to conclude that the choice of feature selection method is dependent on the task. In fact, the performance of a feature selection method could depend on many factors, such as the difficulty of the classification task, the preprocessing step, the feature generation decision, the data representation scheme (Brank et al., 2002a), or the classification model (e.g., SVM, Maximum Entropy, Naive Bayes). Youtube Equal − Accuracy Youtube 1:1.57 − Amu-7 Youtube 1:10 − Accuracy Youtube 1:1.57 − Precision NEG class Youtube 1:1.57 − Recall NEG class 0 5000 10000 15000 Number of Features Chosen 0 5000 10000 15000 Number of Features Chosen 0 5000 10000 15000 Number of Features Chosen Figure 4: The results for the YouTube set. 0 5000 10000 15000 Number of Features Chosen 0 5000 10000 15000 Number of Features Chosen IG BNS OR IG BNS OR TFIDF1 TFIDF2 baseline ● ● ● ● ● ● ● baseline ● ● ● ● ● ● Accuracy 0.50 0.55 0.60 0.65 0.70 0.</context>
</contexts>
<marker>Brank, Grobelnik, Milic-Frayling, Mladenic, 2002</marker>
<rawString>Janez Brank, Marko Grobelnik, Nata&amp;quot;sa Milic-Frayling, and Dunja Mladenic. 2002a. An extensive empirical study of feature selection metrics for text classification. In Proceedings of the Third International Conference on Data Mining Methods and Databases for Engineering, Finance and Other Fields, Bologna, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Brank</author>
<author>Marko Grobelnik</author>
<author>Natasa Milic-Frayling</author>
<author>Dunja Mladenic</author>
</authors>
<title>Feature selection using Linear Support Vector Machines.</title>
<date>2002</date>
<tech>Technical Report MSR-TR-2002-63, Microsoft Research.</tech>
<contexts>
<context position="7472" citStr="Brank et al., 2002" startWordPosition="1139" endWordPosition="1142">er methods. Wrapper methods use the classification outcome on a held-out data set to score feature subsets. Standard wrapper methods include forward selection, backward selection, and genetic algorithms. Filter methods, in contrast, use an independent measure rather than the error rate on the held-out data. This means that they can be applied to larger feature sets, which may be unfeasible with wrapper methods. Since sentiment analysis often deals with high dimensional feature representation, we will concentrate on filter methods for our feature selection experiments. Previous research (e.g. (Brank et al., 2002b; Forman, 2003)) has shown that Information Gain and Odds Ratio have been used successfully across different tasks and that Binormal Separation has good recall for the minority class under skewed class distributions. So we will investigate them in this paper. Other filter methods are not investigated in this paper due to two main concerns: We exclude Chisquared and Z-score, statistical tests because they require a certain sample size. Our concern is that their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference and Probability Proportion Difference</context>
<context position="10337" citStr="Brank et al. (2002" startWordPosition="1611" endWordPosition="1614">Feature Selection Methods In addition to Information Gain, Odds Ratio and Bi-Normal Separation, TF ∗ IDF is included for comparison purposes. We define these measures for binary classification as shown below. Information Gain (IG): IG is a two-sided measure that estimates how much is known about an unobserved random variable given an observed variable. It is defined as the entropy of one random variable minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S given a feature f: 1] 1] P(f, S) IG = H(S) − H(S|f) = P(f, S)log fEI0,11 SE10,11 P(f)P(S) Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman (2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed that IG has a high precision with respect to the minority class. Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds of feature f occurring in class S to the odd</context>
<context position="26769" citStr="Brank et al., 2002" startWordPosition="4628" endWordPosition="4631">ll numbers of features, too. However, this is not completely consistent. 4.3 Discussion If we compare the performance curves across data sets and skewing ratios and aim for high accuracy, we see that there is no single feature selection method that is optimal in all situations. Thus, we have to conclude that the choice of feature selection method is dependent on the task. In fact, the performance of a feature selection method could depend on many factors, such as the difficulty of the classification task, the preprocessing step, the feature generation decision, the data representation scheme (Brank et al., 2002a), or the classification model (e.g., SVM, Maximum Entropy, Naive Bayes). Youtube Equal − Accuracy Youtube 1:1.57 − Amu-7 Youtube 1:10 − Accuracy Youtube 1:1.57 − Precision NEG class Youtube 1:1.57 − Recall NEG class 0 5000 10000 15000 Number of Features Chosen 0 5000 10000 15000 Number of Features Chosen 0 5000 10000 15000 Number of Features Chosen Figure 4: The results for the YouTube set. 0 5000 10000 15000 Number of Features Chosen 0 5000 10000 15000 Number of Features Chosen IG BNS OR IG BNS OR TFIDF1 TFIDF2 baseline ● ● ● ● ● ● ● baseline ● ● ● ● ● ● Accuracy 0.50 0.55 0.60 0.65 0.70 0.</context>
</contexts>
<marker>Brank, Grobelnik, Milic-Frayling, Mladenic, 2002</marker>
<rawString>Janez Brank, Marko Grobelnik, Nata&amp;quot;sa Milic-Frayling, and Dunja Mladenic. 2002b. Feature selection using Linear Support Vector Machines. Technical Report MSR-TR-2002-63, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<contexts>
<context position="4371" citStr="Forman (2003)" startWordPosition="659" endWordPosition="660">oceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11, Dublin, Ireland, August 24 2014. and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed competitive feature selection methods evaluated on the movie review data set. However, the generalizability of such feature selection methods to imbalanced data sets, which better represent real world situations, has not been investigated in much detail. Forman (2003) provides an extensive study of feature selection methods for highly imbalanced data sets, but he uses document classification as task. This current paper investigates the robustness of three feature selection methods that Forman (2003) has shown to be successful, as well as two variants of TF * IDF. The three methods are Odds-Ratio (OR), Information Gain (IG), and Binormal Separation (BNS). BNS has been found to perform significantly better than other methods in more highly skewed tasks (Forman, 2003). The two variants of TF * IDF differ in the data set used for calculating document frequency</context>
<context position="7488" citStr="Forman, 2003" startWordPosition="1143" endWordPosition="1144">ethods use the classification outcome on a held-out data set to score feature subsets. Standard wrapper methods include forward selection, backward selection, and genetic algorithms. Filter methods, in contrast, use an independent measure rather than the error rate on the held-out data. This means that they can be applied to larger feature sets, which may be unfeasible with wrapper methods. Since sentiment analysis often deals with high dimensional feature representation, we will concentrate on filter methods for our feature selection experiments. Previous research (e.g. (Brank et al., 2002b; Forman, 2003)) has shown that Information Gain and Odds Ratio have been used successfully across different tasks and that Binormal Separation has good recall for the minority class under skewed class distributions. So we will investigate them in this paper. Other filter methods are not investigated in this paper due to two main concerns: We exclude Chisquared and Z-score, statistical tests because they require a certain sample size. Our concern is that their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference and Probability Proportion Difference since they do n</context>
<context position="10629" citStr="Forman (2003)" startWordPosition="1660" endWordPosition="1661">an unobserved random variable given an observed variable. It is defined as the entropy of one random variable minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S given a feature f: 1] 1] P(f, S) IG = H(S) − H(S|f) = P(f, S)log fEI0,11 SE10,11 P(f)P(S) Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman (2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed that IG has a high precision with respect to the minority class. Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated: P(f, S)(1 − P(f, S)) OR = log P(f, S)(1 − P(f, S)) Brank et al. (2002b) showed that OR requires a high number of f</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of Machine Learning Research, 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning. MIT-Press.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="18862" citStr="Joachims, 1999" startWordPosition="3054" endWordPosition="3055"> the vector operations in classifier optimization. For the Epicurious data, the whole feature set consists of 10 677 unigram features. For YouTube, the full feature set of features consists of 23 232 unigram features. We evaluate the performance of feature selection methods starting at 500 features, at a step-size of 500. For the Epicurious data, we include up to 10 500 features. For the YouTube data, we stop at 15 000 features due to prohibitively long classification times. 3.4 Classifier The classifier we use in this paper is Support Vector Machines (SVMs) in the implementation of SVMlight (Joachims, 1999). Because algorithm optimization is not the focus of this study, we use the default linear kernel and other default parameter values. Classification results are evaluated by accuracy as well as precision and recall for individual classes. 3.5 Binary Classification Since all feature selection methods we use in our experiments are defined under a binary classification scenario, we need to redefine the rating prediction task. For both data sets, this means, we group the recipes and videos into a positive and a negative class. A baseline classifier predicts every instance as the majority class. Fo</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olena Kummer</author>
<author>Jaques Savoy</author>
</authors>
<title>Feature selection in sentiment analysis.</title>
<date>2012</date>
<booktitle>In Proceeding of the Conf´erence en Recherche d’Infomations et Applications (CORIA),</booktitle>
<pages>273--284</pages>
<location>Bordeaux, France.</location>
<marker>Kummer, Savoy, 2012</marker>
<rawString>Olena Kummer and Jaques Savoy. 2012. Feature selection in sentiment analysis. In Proceeding of the Conf´erence en Recherche d’Infomations et Applications (CORIA), pages 273–284, Bordeaux, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shoushan Li</author>
<author>Rui Xia</author>
<author>Chengqing Zong</author>
<author>Chu-Ren Huang</author>
</authors>
<title>A framework of feature selection methods for text categorization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>692--700</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="10614" citStr="Li et al., 2009" startWordPosition="1656" endWordPosition="1659">ch is known about an unobserved random variable given an observed variable. It is defined as the entropy of one random variable minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S given a feature f: 1] 1] P(f, S) IG = H(S) − H(S|f) = P(f, S)log fEI0,11 SE10,11 P(f)P(S) Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman (2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed that IG has a high precision with respect to the minority class. Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated: P(f, S)(1 − P(f, S)) OR = log P(f, S)(1 − P(f, S)) Brank et al. (2002b) showed that OR requires a h</context>
</contexts>
<marker>Li, Xia, Zong, Huang, 2009</marker>
<rawString>Shoushan Li, Rui Xia, Chengqing Zong, and Chu-Ren Huang. 2009. A framework of feature selection methods for text categorization. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 692–700, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1396" citStr="Liu, 2012" startWordPosition="207" endWordPosition="208">h with different ratios of class imbalance. Our finding shows that feature selection is capable of improving the classification accuracy only in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios. We also conclude that there does not exist a single method that performs best across data sets and skewing ratios. However we found that TF ∗ IDF2 can help in identifying the minority class even in highly imbalanced cases. 1 Introduction In recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008; Bollen et al., 2011; Liu, 2012). Sentiment analysis is concerned with extracting opinions or emotions from text, especially user generated web content. Specific tasks include monitoring mood and emotion; differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinion strength; and identifying other opinion properties. At this point, two major approaches exists: lexicon and machine learning based. The lexicon-based approach uses high quality, often manually generated features. The machine learning-based approach uses automatically generated feature sets, which are from various source</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>142--150</pages>
<location>Portland, OR.</location>
<contexts>
<context position="3319" citStr="Maas et al., 2011" startWordPosition="504" endWordPosition="507">4 of all ratings consist of the highest rating of 5. For other types of user generated content, they opposite may be true. Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Langua</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142–150, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
</authors>
<title>Association and estimation in contingency tables.</title>
<date>1968</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>63</volume>
<issue>321</issue>
<contexts>
<context position="10828" citStr="Mosteller, 1968" startWordPosition="1693" endWordPosition="1694">ainty of class S given a feature f: 1] 1] P(f, S) IG = H(S) − H(S|f) = P(f, S)log fEI0,11 SE10,11 P(f)P(S) Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman (2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed that IG has a high precision with respect to the minority class. Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated: P(f, S)(1 − P(f, S)) OR = log P(f, S)(1 − P(f, S)) Brank et al. (2002b) showed that OR requires a high number of features to achieve a given feature vector sparsity because it prefers rare terms. Features that occur in very few documents of class S and do not occur in S have a small denominator, and thus a rathe</context>
</contexts>
<marker>Mosteller, 1968</marker>
<rawString>Frederick Mosteller. 1968. Association and estimation in contingency tables. Journal of the American Statistical Association, 63(321):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim O’Keefe</author>
<author>Irena Koprinska</author>
</authors>
<title>Feature selection and weighting methods in sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 14th Australasian Document Computing Symposium (ADCS),</booktitle>
<pages>67--74</pages>
<location>Sydney, Australia.</location>
<marker>O’Keefe, Koprinska, 2009</marker>
<rawString>Tim O’Keefe and Irena Koprinska. 2009. Feature selection and weighting methods in sentiment analysis. In Proceedings of the 14th Australasian Document Computing Symposium (ADCS), pages 67–74, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Paltoglou</author>
<author>Mike Thelwall</author>
</authors>
<title>A study of information retrieval weighting schemes for sentiment analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1386--1395</pages>
<location>Uppsala,</location>
<contexts>
<context position="4089" citStr="Paltoglou and Thelwall (2010)" startWordPosition="616" endWordPosition="619">eview data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2–11, Dublin, Ireland, August 24 2014. and Savoy (2012), O’Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed competitive feature selection methods evaluated on the movie review data set. However, the generalizability of such feature selection methods to imbalanced data sets, which better represent real world situations, has not been investigated in much detail. Forman (2003) provides an extensive study of feature selection methods for highly imbalanced data sets, but he uses document classification as task. This current paper investigates the robustness of three feature selection methods that Forman (2003) has shown to be successful, as well as two variants of TF * IDF. The three method</context>
<context position="16931" citStr="Paltoglou and Thelwall, 2010" startWordPosition="2740" endWordPosition="2743">in dependent, and some English stop words may be informative for sentiment analysis, and 2) uninformative words that are equally common in both classes will be excluded by feature selection if the method is successful. 3.3 Feature representation Since our focus is on settings with high numbers of features, we use a bag-of-words approach, in which every word represents one feature, and its term frequency serves as its value. Different feature weighting methods, including binary weighting, term frequency, and TF*IDF have been adopted in past sentiment analysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)). (Pang et al., 2002) found that simply using binary feature weighting performed better than using more complicated weightings in a task of classifying positive and negative movie reviews. However, movie reviews are relatively short, so there may not be a large difference between binary features and others. Topic classification usually uses term frequency as feature weighting. TF * IDF and variants were shown to perform better than binary weighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010). Since our user rating prediction tasks aggregate all user comments into </context>
</contexts>
<marker>Paltoglou, Thelwall, 2010</marker>
<rawString>Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment analysis. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1386–1395, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3271" citStr="Pang and Lee, 2004" startWordPosition="494" endWordPosition="497">ith their ratings. In this data set, more than 3/4 of all ratings consist of the highest rating of 5. For other types of user generated content, they opposite may be true. Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced data sets during data collection, by sampling a certain number of positive and negative reviews, or by selecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the movie review data set has been used as a benchmark data set that allows for comparisons of various sentiment analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 2 Proc</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1363" citStr="Pang and Lee, 2008" startWordPosition="199" endWordPosition="202">sis data sets from different domains, each with different ratios of class imbalance. Our finding shows that feature selection is capable of improving the classification accuracy only in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios. We also conclude that there does not exist a single method that performs best across data sets and skewing ratios. However we found that TF ∗ IDF2 can help in identifying the minority class even in highly imbalanced cases. 1 Introduction In recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008; Bollen et al., 2011; Liu, 2012). Sentiment analysis is concerned with extracting opinions or emotions from text, especially user generated web content. Specific tasks include monitoring mood and emotion; differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinion strength; and identifying other opinion properties. At this point, two major approaches exists: lexicon and machine learning based. The lexicon-based approach uses high quality, often manually generated features. The machine learning-based approach uses automatically generated feature se</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<journal>Program,</journal>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<volume>14</volume>
<issue>3</issue>
<pages>79--86</pages>
<location>Philadelphia, PA. Martin Porter.</location>
<contexts>
<context position="16900" citStr="Pang et al., 2002" startWordPosition="2736" endWordPosition="2739">Stop words are domain dependent, and some English stop words may be informative for sentiment analysis, and 2) uninformative words that are equally common in both classes will be excluded by feature selection if the method is successful. 3.3 Feature representation Since our focus is on settings with high numbers of features, we use a bag-of-words approach, in which every word represents one feature, and its term frequency serves as its value. Different feature weighting methods, including binary weighting, term frequency, and TF*IDF have been adopted in past sentiment analysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)). (Pang et al., 2002) found that simply using binary feature weighting performed better than using more complicated weightings in a task of classifying positive and negative movie reviews. However, movie reviews are relatively short, so there may not be a large difference between binary features and others. Topic classification usually uses term frequency as feature weighting. TF * IDF and variants were shown to perform better than binary weighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010). Since our user rating prediction tasks ag</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86, Philadelphia, PA. Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130—137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhaohui Zheng</author>
<author>Xiayun Wu</author>
<author>Rohini Srihari</author>
</authors>
<title>Feature selection for text categorization on imbalanced data.</title>
<date>2004</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="8302" citStr="Zheng et al. (2004)" startWordPosition="1272" endWordPosition="1275">tions. So we will investigate them in this paper. Other filter methods are not investigated in this paper due to two main concerns: We exclude Chisquared and Z-score, statistical tests because they require a certain sample size. Our concern is that their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference and Probability Proportion Difference since they do not normalize over the sample of size of positive and negative classes. Thus, our concern is that they may not provide a fair estimate for features from a skewed data sets. 2.1 Notation Following Zheng et al. (2004), feature selection methods can be divided into two groups: one-sided and two-sided measures. One-sided measures assign a high score to positively-correlated features and a low 1www.epicurious.com 3 score to negative features while two-sided measures prefer highly distinguishing features, independent of whether they are positively or negatively correlated. Zheng et al. (2004) note that the ratio of positive and negative features affects precision and recall of the classification, especially for the minority class. For one-sided methods, we have control over this ratio by selecting a specified </context>
</contexts>
<marker>Zheng, Wu, Srihari, 2004</marker>
<rawString>Zhaohui Zheng, Xiayun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced data. ACM SIGKDD Explorations Newsletter, 6(1):80–89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
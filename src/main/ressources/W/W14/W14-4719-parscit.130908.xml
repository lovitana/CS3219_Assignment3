<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000364">
<title confidence="0.819295">
Lexical Access Preference and Constraint Strategies for Improving
Multiword Expression Association within Semantic MT Evaluation
</title>
<author confidence="0.921356">
Dekai Wu Lo Chi-kiu Markus SAERs
</author>
<affiliation confidence="0.92824675">
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
</affiliation>
<email confidence="0.996287">
{dekai|jackielo|masaers|dekai}@cs.ust.hk
</email>
<sectionHeader confidence="0.997368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999803363636364">
We examine lexical access preferences and constraints in computing multiword expression asso-
ciations from the standpoint of a high-impact extrinsic task-based performance measure, namely
semantic machine translation evaluation. In automated MT evaluation metrics, machine transla-
tions are compared against human reference translations, which are almost never worded exactly
the same way except in the most trivial of cases. Because of this, one of the most important factors
in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative
lexical realizations of the same multiword expressions in semantic role fillers. Our results com-
paring bag-of-words, maximum alignment, and inversion transduction grammars indicate that
cognitively motivated ITGs provide superior lexical access characteristics for multiword expres-
sion associations, leading to state-of-the-art improvements in correlation with human adequacy
judgments.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.95787916">
We investigate lexical access strategies in the context of computing multiword expression associations
within automatic semantic MT evaluation metrics—a high-impact real-world extrinsic task-based per-
formance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious
issues in machine translation and automatic MT evaluation; there are simply too many forms to enumer-
ate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a
decade and a half, but until recently little has been done to use lexical semantics as the main foundation
for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al.,
2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference
and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and
Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation
adequacy.
Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et
al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of
semantic frames, by measuring the degree to which the basic event structure is preserved by translation—
the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing
that a good translation is one that can successfully be understood by a human. Across a variety of language
pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both n-
gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and
METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et
al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and
Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning
the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.955976">
144
</page>
<note confidence="0.9432525">
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153,
Dublin, Ireland, August 23, 2014.
</note>
<figureCaption confidence="0.998003">
Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations
</figureCaption>
<bodyText confidence="0.9305613">
are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL
decided to drop the predicate.
adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English
and Chinese) and different genres (formal newswire text, informal web forum text and informal public
speech).
Because of this, we have chosen to run our lexical association experiments in the context of the neces-
sity of recognizing matching semantic role fillers, approximately 85% of which are multiword expressions
in our data, the overwhelming majority of which would not be enumerated within conventional lexicons.
We compare four common lexical access approaches to aggregation, preferences, and constraints: bag-
of-words, two different types of maximal alignment, and inversion transduction grammar based methods.
</bodyText>
<sectionHeader confidence="0.98931" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999686">
The MEANT metric measures weighted f-scores over corresponding semantic frames and role fillers
in the reference and machine translations. Whereas HMEANT uses human annotation, the automatic
versions of MEANT instead replace humans with automatic SRL and alignment algorithms. MEANT
typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human ade-
quacy judgment, and is relatively easy to port to other languages, requiring only an automatic semantic
parser and a monolingual corpus of the output language, which is used to gauge lexical similarity between
the semantic role fillers of the reference and translation. More precisely, MEANT computes scores as
follows:
</bodyText>
<listItem confidence="0.992275666666667">
1. Apply an automatic shallow semantic parser to both the references and MT output. (Figure 1 shows
examples of automatic shallow semantic parses on both reference and MT.)
2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between
the references and MT output according to the lexical similarities of the predicates.
3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to
align the arguments between the reference and MT output according to the lexical similarity of role
fillers.
4. Compute the weighted f-score over the matching role labels of these aligned predicates and role
fillers according to the following definitions:
</listItem>
<page confidence="0.96448">
145
</page>
<equation confidence="0.826193333333333">
q0i,j ≡ ARG j of aligned frame i in MT
q1i,j ≡ ARG j of aligned frame i in REF
wi ≡ #tokens filled in aligned frame i of MT
0
total #tokens in MT
#tokens filled in aligned frame i of REF
total #tokens in REF
wpred ≡ weight of similarity of predicates
wj ≡ weight of similarity of ARG j
</equation>
<bodyText confidence="0.9772456">
ei,pred ≡ the pred of the aligned frame i of the machine translation
fi,pred ≡ the pred of the aligned frame i of the reference translation
ei,j ≡ the ARG j of the aligned frame i of the machine translation
fi,j ≡ the ARG j of the aligned frame i of the reference translation
s(e, f) = lexical similarity of token e and f
</bodyText>
<equation confidence="0.992114647058824">
prece,f = ∑
rece,f = eEe max s(e, f)
precision = f Ef
 |e |
∑
fEf max s(e, f)
eEe
 |f |
∑0 wpredsi,pred+∑j wj si,j
i wi wpred+∑ j wj |q?,j |
∑i w0 i
∑1 wpredsi,pred+∑ j wj si,j
i wi wpred+∑j wj |qI,j |
∑i w1i
2 · precision · recall
MEANT =
precision + recall
</equation>
<bodyText confidence="0.999991909090909">
where the possible approaches to defining the lexical associations si,pred and si,j are discussed in the
following section. q&apos;,j and qz,j are the argument of type j in frame i in MT and REF, respectively. w9
and wz are the weights for frame i in MT and REF, respectively. These weights estimate the degree
of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of
the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between
the reference translations and the MT output. There is a total of 12 weights for the set of semantic role
labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised
estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and
Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using
relative frequency of each semantic role label in the references and thus UMEANT is useful when human
judgments on adequacy of the development set are unavailable.
</bodyText>
<sectionHeader confidence="0.807891" genericHeader="method">
3 Comparison of multiword expression association approaches
</sectionHeader>
<bodyText confidence="0.9998208">
To assess alternative lexical access preferences and constraints for computing multiword expression
associations, we now consider four alternative approaches to defining the lexical similarities si,pred and
si,j, all of which employ a standard context vector model of the individual words/tokens in the multiword
expression arguments between the reference and machine translations, as descibed by Lo et al. (2012)
and Tumuluru et al. (2012).
</bodyText>
<subsectionHeader confidence="0.999945">
3.1 Bag of words (geometric mean)
</subsectionHeader>
<bodyText confidence="0.999928">
The original MEANT approaches employed standard a bag-of-words strategy for lexical association.
This baseline approach applies no alignment constraints on multiword expressions:
</bodyText>
<equation confidence="0.988837875">
∑ ∑
eEei,pred fEfi,pred lg(s(e,f))
si,pred = e |ei,pred|&apos;|fi,pred|
si,j = e ∑ ∑
eEei,j fEfi,j lg(s(e,f))
|ei,j |&apos;|fi,j |
w1i ≡
recall =
</equation>
<page confidence="0.989262">
146
</page>
<subsectionHeader confidence="0.997559">
3.2 Maximum alignment (precision-recall average)
</subsectionHeader>
<bodyText confidence="0.9998335">
In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz,� are
inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length.
</bodyText>
<equation confidence="0.9955955">
1
Si,pred = 2 1precei,pred,fi,pred + recei,pred,fi,pred�
1
=2 1precei,j,fi,j + recei,j ,fi,j �
</equation>
<subsectionHeader confidence="0.918165">
3.3 Maximum alignment (f-score)
</subsectionHeader>
<bodyText confidence="0.99999375">
The second of the maximum alignment based approaches replaces the above linear averaging of pre-
cision and recall with a proper f-score. Although this is less consistent with the previous literature, such
as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT,
and thus we include it in our comparison as a variant of the maximum alignment strategy.
</bodyText>
<equation confidence="0.9819675">
Si,pred = 2 · precei,pred,fi,pred · recei,pred,fi,pred
Si,j = precei,pred,fi,pred + recei,pred,fi,pred
2 · precei,j,fi,j ·recei,j,fi,j
precei,j ,fi,j + recei,j,fi,j
</equation>
<subsectionHeader confidence="0.846498">
3.4 Inversion transduction grammar based
</subsectionHeader>
<bodyText confidence="0.999859181818182">
There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve
the accuracy of MT evaluation metrics—despite (1) long empirical evidence the vast majority of transla-
tion patterns between human languages can be accommodated within ITG constraints, and (2) the obser-
vation that most current state-of-the-art SMT systems employ ITG decoders. Especially when considering
semantic MT metrics, ITGs would seem to be a natural strategy for multiword expression association for
several cognitively motivated reasons, having to do with language universal properties of cross-linguistic
semantic frame structure.
To begin with, it is quite natural to think of sentences as having been generated from an abstract concept
using a rewriting system: a stochastic grammar predicts how frequently any particular realization of the
abstract concept will be generated. The bilingual analogy is a transduction grammar generating a pair
of possible realizations of the same underlying concept. Stochastic transduction grammars predict how
frequently a particular pair of realizations will be generated, and thus represent a good way to evaluate
how well a pair of sentences correspond to each other.
The particular class of transduction grammars known as ITGs tackle the problem that the (bi)parsing
complexity for general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By
constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted
reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low
end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to
have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and
space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n3) time
(Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an
ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) .
At the same time, inversion transductions have also been directly shown to be more than sufficient
to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This
language universal property has an evolutionary explanation in terms of computational efficiency and
cognitive load for language learnability and interpretability (Wu, 2014).
ITGs are thus an appealing alternative for evaluating the possible links between both semantic role
fillers in different languages as well as the predicates, and how these parts fit together to form entire
semantic frames. We believe that ITGs are not only capable of generating the desired structural corre-
spondences between the semantic structures of two languages, but also provide meaningful constraints
to prevent alignments from wandering off in the wrong direction.
Following this reasoning, alternate definitions of sz,pred and sz,� can be constructed in terms of brack-
eting ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated
</bodyText>
<page confidence="0.992554">
147
</page>
<bodyText confidence="0.99988225">
nonterminal category (Wu, 1995a). The idea is to attack a potential weakness of the foregoing three
lexical association strategies, namely that word/token alignments between the reference and machine
translations are severely underconstrained. No bijectivity or permutation restrictions are applied, even
between compositional segments where this should be natural. This can cause multiword expressions of
semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inver-
sion transduction grammar can potentially better constrain permissible token alignment patterns between
aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed
permutations between semantic role fillers across the reference and machine translations for a sample
sentence from the evaluation data.
In this approach, both alignment and scoring are performed utilizing a length-normalized weighted
BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and
si,� as follows.
</bodyText>
<equation confidence="0.999148166666667">
 ( ( )) 
lg P A � ⇒ ei,pred/fi,pred|G
 
max( |ei,pred |,  |fi,pred|)
si,pred = lg−1
where si,7  ( ( )) 
lg−1 lg P A � ⇒ ei,�/fi,�|G 
= max(|ei,�|,|fi,� |)
G≡⟨{A},W�,W1,R,A⟩
R≡{A→[AA],A→⟨AA⟩,A→e/f}
p([AA]|A)=p(⟨AA⟩|A)=1
p(e/f|A)=s(e,f)
</equation>
<bodyText confidence="0.988089666666667">
Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with
e E W° U {E} denoting a token in the MT output (or the null token) and f E Wi U {E} denoting
a token in the reference translation (or the null token). The rule probability (or more accurately, rule
weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is
defined by MEANT’s lexical similarity measure on English Gigaword context vectors. To calculate the
( )
inside probability (or more accurately, inside score) of a pair of segments, P A � � e/f|G , we use the
algorithm described in Saers et al. (2009). Given this, si,pred and si,� now represent the length normalized
BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and
machine translations.
In this section we discuss experiments comparing the four altern
ative lexical access preference and
constraint strategies.
The corpus includes the Chinese input sentences, each accompanied by an English reference
translation and three participating
</bodyText>
<sectionHeader confidence="0.460182" genericHeader="method">
MT
</sectionHeader>
<bodyText confidence="0.843197913043478">
output.
We computed sentence-level correlations following the benchmark assessment procedure used by
WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012;
and Bojar,
2013), which use
T correlation coefficient, to evaluate the correlation of evaluation metrics
against human judgment on ranking the translation adequacy of the three
output. A higher
value for
T indicates more similarity to the human adequacy rankings by the evaluation met-
rics. The range of possible values of
T correlation coefficient is
1], where 1 mean
(2011a).
state-of-the-art
systems’
Macháček
Kendall’s
systems’
Kendall’s
Kendall’s
[-1,
s the
</bodyText>
<sectionHeader confidence="0.99958" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996424">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.9908875">
We compared using the DARPA GALE P2.5 Chinese-English translation test set, as used in Lo and
Wu
</bodyText>
<page confidence="0.998399">
148
</page>
<tableCaption confidence="0.901798666666667">
Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE
P2.5 data. For reference, the human HMEANT upper bound is 0.53—so the fully automatic ITG based
MEANT approximation is not far from closing the gap.
</tableCaption>
<table confidence="0.999085">
Kendall correlation
MEANT + ITG based 0.51
MEANT + maximum alignment (f-score) 0.48
MEANT + maximum alignment (average of precision &amp; recall) 0.46
MEANT + bag of words (geometric mean) 0.38
NIST 0.29
METEOR 0.20
BLEU 0.20
TER 0.20
PER 0.20
CDER 0.12
WER 0.10
</table>
<bodyText confidence="0.63695975">
systems are ranked in the same order as the human judgment by the evaluation metric; and -1 means the
systems are ranked in the reverse order as human judgment by the evaluation metric.
For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler
was used to automatically predict semantic parses.
</bodyText>
<subsectionHeader confidence="0.914459">
4.2 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999977913043478">
The sentence-level correlations in Table 1 show that the ITG based strategy outperforms other auto-
matic metrics in correlation with human adequacy judgment. Note that this was achieved with no tuning
whatsoever of the rule weights (suggesting that the performance could be further improved in the future
by slightly optimizing the ITG weights).
The ITG based strategy shows 3 points improvement over the next best strategy, which is maximal
alignment under f-score aggregation. The ITG based approach produces much higher HAJ correlations
than any of the other metrics.
In fact, the ITG based strategy even comes within a few points of the human upper bound bench-
mark HAJ correlations computed using the human labeled semantic frames and alignments used in the
HMEANT.
Data analysis reveals two reasons that the ITG based strategy correlates with human adequacy judge-
ment more closely than the other approaches. First, BITG constraints indeed provide more accurate
phrasal similarity aggregation, compared to the naive bag-of-words based heuristics. Similar results
have been observed while trying to estimate word alignment probabilities where BITG constraints out-
performed alignments from GIZA++ (Saers and Wu, 2009). Secondly, the permutation and bijectivity
constraints enforced by the ITG provide better leverage to reject token alignments when they are not
appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The
ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave
tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not
simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed
that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered
in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better
fits the semantic structure.
</bodyText>
<sectionHeader confidence="0.996908" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999829">
We have compared four alternative lexical access strategies for aggregation, preferences, and con-
straints in scoring multiword expression associations that are far too numerous to be explicitly enumerated
in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words,
</bodyText>
<page confidence="0.998489">
149
</page>
<figureCaption confidence="0.997152">
Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized using both
</figureCaption>
<bodyText confidence="0.98643025">
biparse tree and alignment matrix depictions, for the Chinese input sentence f-
L*-T-I1 vet . Both the reference and machine translations are parsed using automatic English SRL.
Compositional alignments between the semantic frames and the tokens within role filler phrases obey
inversion transduction grammars.
</bodyText>
<page confidence="0.995077">
150
</page>
<bodyText confidence="0.999844083333333">
two maximum alignment based approaches, and an inversion transduction grammar based approach.
Controlled experiments within the MEANT semantic MT evaluation framework shows that the cog-
nitively motivated ITG based strategy achieves significantly higher correlation with human adequacy
judgments of MT output quality than the more typically used lexical association approaches. The results
show how to improve upon previous research showing that MEANT’s explicit use of semantic frames
leads to state-of-the-art automatic MT evaluation, by aligning and scoring semantic frames under a sim-
ple, consistent ITG that provides empirically informative permutation and bijectivity biases, instead of
more naive maximal alignment or bag-of-words assumptions.
Cognitive studies of the lexicon are often described using intrinsic measures of quality. Our exper-
iments complement this by situating the empirical comparisons within extrinsic real-world task-based
performance measures. We believe that progress can be accelerated via a combination of intrinsic and
extrinsic measures of lexicon acquisition and access models.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999811428571429">
This material is based upon work supported in part by the Defense Advanced Research Projects
Agency (DARPA) under BOLT contract nos. HR0011-12-C-0014 and HR0011-12-C-0016, and GALE
contract nos. HR0011-06-C-0022 and HR0011-06-C-0023; by the European Union under the FP7
grant agreement no. 287658; and by the Hong Kong Research Grants Council (RGC) research grants
GRF620811, GRF621008, and GRF612806. Any opinions, findings and conclusions or recommenda-
tions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA,
the EU, or RGC.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991678458333333">
Karteek Addanki, Chi-kiu Lo, Markus Saers, and Dekai Wu. LTG vs. ITG coverage of cross-lingual verb
frame alternations. In 16th Annual Conference of the European Association for Machine Translation
(EAMT-2012), Trento, Italy, May 2012.
Alfred V. Aho and Jeffrey D. Ullman. The Theory ofParsing, Translation, and Compiling. Prentice-Halll,
Englewood Cliffs, New Jersey, 1972.
Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the European Chapter ofthe Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third Workshop on Statistical Machine Translation (WMT-
08), 2008.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Pryzbocki, and Omar Zaidan.
Findings of the 2010 joint workshop on statistical machine translation and metrics for machine trans-
lation. In Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (WMT10), pages
17–53, Uppsala, Sweden, 15-16 July 2010.
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan. Findings of the 2011
Workshop on Statistical Machine Translation. In 6th Workshop on Statistical Machine Translation
(WMT 2011), 2011.
Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. Find-
ings of the 2012 workshop on statistical machine translation. In 7th Workshop on Statistical Machine
Translation (WMT 2012), pages 10–51, 2012.
</reference>
<page confidence="0.991401">
151
</page>
<reference confidence="0.998587638297872">
George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence
statistics. In The second international conference on Human Language Technology Research
(HLT ’02), San Diego, California, 2002.
Philipp Koehn and Christof Monz. Manual and automatic evaluation of machine translation between
european languages. In Workshop on Statistical Machine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chapter of the Association for Computational Linguistics
(EACL-2006), 2006.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluat-
ing translation utility based on semantic roles. In 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies (ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. supervised weight estimation for semantic MT evaluation
metrics. In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6),
2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT 2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and Dekai Wu. Improving machine translation by training
against an automatic semantic frame based evaluation metric. In 51stAnnual Meeting ofthe Association
for Computational Linguistics (ACL 2013), 2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Improving machine translation into Chinese by tuning
against Chinese MEANT. In International Workshop on Spoken Language Translation (IWSLT 2013),
2013.
Matouš Macháček and Ondřej Bojar. Results of the WMT13 metrics shared task. In Eighth Workshop
on Statistical Machine Translation (WMT 2013), Sofia, Bulgaria, August 2013.
I. Dan Melamed. Automatic construction of clean broad-coverage translation lexicons. In 2nd Conference
of the Association for Machine Translation in the Americas (AMTA-1996), 1996.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first National Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. A evaluation tool for machine transla-
tion: Fast evaluation for MT research. In The Second International Conference on Language Resources
and Evaluation (LREC 2000), 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics
(ACL-02), pages 311–318, Philadelphia, Pennsylvania, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow semantic
parsing using support vector machines. In Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), 2004.
Markus Saers and Dekai Wu. Improving phrase-based translation via word alignments from stochastic
inversion transduction grammars. In Third Workshop on Syntax and Structure in Statistical Translation
(SSST-3), pages 28–36, Boulder, Colorado, June 2009.
</reference>
<page confidence="0.977949">
152
</page>
<reference confidence="0.998665904761905">
Markus Saers, Joakim Nivre, and Dekai Wu. Learning stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies
(IWPT’09), pages 29–32, Paris, France, October 2009.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of trans-
lation edit rate with targeted human annotation. In 7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages 223–231, Cambridge, Massachusetts, August 2006.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lex-
ical similarity of semantic role fillers for automatic semantic MT evaluation. In 26th Pacific Asia
Conference on Language, Information, and Computation (PACLIC 26), 2012.
Dekai Wu. An algorithm for simultaneously bracketing parallel texts by aligning words. In 33rd An-
nual Meeting of the Association for Computational Linguistics (ACL 95), pages 244–251, Cambridge,
Massachusetts, June 1995.
Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In Third Annual Workshop
on Very Large Corpora (WVLC-3), pages 69–81, Cambridge, Massachusetts, June 1995.
Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–403, 1997.
Dekai Wu. The magic number 4: Evolutionary pressures on semantic frame structure. In 10th Interna-
tional Conference on the Evolution ofLanguage (Evolang X), Vienna, Apr 2014.
Richard Zens and Hermann Ney. A comparative study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),
pages 144–151, Stroudsburg, Pennsylvania, 2003.
</reference>
<page confidence="0.999228">
153
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296968">
<title confidence="0.819803666666667">Lexical Access Preference and Constraint Strategies for Multiword Expression Association within Semantic MT Evaluation Lo Markus</title>
<author confidence="0.731008">Human Language Technology</author>
<affiliation confidence="0.998232">Department of Computer Science and</affiliation>
<author confidence="0.775974">Hong Kong University of Science</author>
<author confidence="0.775974">Technology</author>
<email confidence="0.984809">{dekai|jackielo|masaers|dekai}@cs.ust.hk</email>
<abstract confidence="0.989449916666667">We examine lexical access preferences and constraints in computing multiword expression associations from the standpoint of a high-impact extrinsic task-based performance measure, namely semantic machine translation evaluation. In automated MT evaluation metrics, machine translations are compared against human reference translations, which are almost never worded exactly the same way except in the most trivial of cases. Because of this, one of the most important factors in correctly predicting semantic translation adequacy is the accuracy of recognizing alternative lexical realizations of the same multiword expressions in semantic role fillers. Our results comparing bag-of-words, maximum alignment, and inversion transduction grammars indicate that cognitively motivated ITGs provide superior lexical access characteristics for multiword expression associations, leading to state-of-the-art improvements in correlation with human adequacy judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Karteek Addanki</author>
<author>Chi-kiu Lo</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>LTG vs. ITG coverage of cross-lingual verb frame alternations.</title>
<date>2012</date>
<booktitle>In 16th Annual Conference of the European Association for Machine Translation (EAMT-2012),</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="12380" citStr="Addanki et al., 2012" startWordPosition="1906" endWordPosition="1909">s. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n3) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent</context>
<context position="14273" citStr="Addanki et al., 2012" startWordPosition="2181" endWordPosition="2184">antic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,� as follows.  ( ( ))  lg P A � ⇒ ei,pred/fi,pred|G   max( |ei,pred |, |fi,pred|) si,pred = lg−1 where si,7  ( ( ))  lg−1 lg P A � ⇒ ei,�/fi,�|G  = max(|ei,�|,|fi,� |) G≡⟨{A},W�,W1,R,A⟩ R≡{A→[AA],A→⟨AA⟩,A→e/f} p([AA]|A)=p(⟨AA⟩|A)=1 p(e/f|A)=s(e,f) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e E W° U {E} denoting a token in the MT output (or the null token) and f E Wi U {E} denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is</context>
</contexts>
<marker>Addanki, Lo, Saers, Wu, 2012</marker>
<rawString>Karteek Addanki, Chi-kiu Lo, Markus Saers, and Dekai Wu. LTG vs. ITG coverage of cross-lingual verb frame alternations. In 16th Annual Conference of the European Association for Machine Translation (EAMT-2012), Trento, Italy, May 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory ofParsing, Translation, and Compiling. Prentice-Halll,</booktitle>
<location>Englewood Cliffs, New Jersey,</location>
<contexts>
<context position="11453" citStr="Aho and Ullman, 1972" startWordPosition="1762" endWordPosition="1765">: a stochastic grammar predicts how frequently any particular realization of the abstract concept will be generated. The bilingual analogy is a transduction grammar generating a pair of possible realizations of the same underlying concept. Stochastic transduction grammars predict how frequently a particular pair of realizations will be generated, and thus represent a good way to evaluate how well a pair of sentences correspond to each other. The particular class of transduction grammars known as ITGs tackle the problem that the (bi)parsing complexity for general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n3) time (Saers et al., 2009). These polynomial complexities makes it feasib</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. The Theory ofParsing, Translation, and Compiling. Prentice-Halll, Englewood Cliffs, New Jersey, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2077" citStr="Banerjee and Lavie, 2005" startWordPosition="276" endWordPosition="279">valuation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measurin</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter ofthe Association for Computational Linguistics (EACL-2006),</booktitle>
<contexts>
<context position="2324" citStr="Callison-Burch et al., 2006" startWordPosition="312" endWordPosition="315">o many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a </context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine translation research. In 11th Conference of the European Chapter ofthe Association for Computational Linguistics (EACL-2006), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Third Workshop on Statistical Machine Translation (WMT08),</booktitle>
<contexts>
<context position="15814" citStr="Callison-Burch et al., 2008" startWordPosition="2439" endWordPosition="2442">ed in Saers et al. (2009). Given this, si,pred and si,� now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations. In this section we discuss experiments comparing the four altern ative lexical access preference and constraint strategies. The corpus includes the Chinese input sentences, each accompanied by an English reference translation and three participating MT output. We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; and Bojar, 2013), which use T correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three output. A higher value for T indicates more similarity to the human adequacy rankings by the evaluation metrics. The range of possible values of T correlation coefficient is 1], where 1 mean (2011a). state-of-the-art systems’ Macháček Kendall’s systems’ Kendall’s Kendall’s [-1, s the 4 Experiments 4.1 Experimental setup We compared using the DARPA GALE P2.5 Chinese-English translation test set, as us</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further meta-evaluation of machine translation. In Third Workshop on Statistical Machine Translation (WMT08), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Pryzbocki</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (WMT10),</booktitle>
<pages>17--53</pages>
<location>Uppsala,</location>
<marker>Callison-Burch, Koehn, Monz, Peterson, Pryzbocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Pryzbocki, and Omar Zaidan. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR (WMT10), pages 17–53, Uppsala, Sweden, 15-16 July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar F Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In 6th Workshop on Statistical Machine Translation (WMT</booktitle>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan. Findings of the 2011 Workshop on Statistical Machine Translation. In 6th Workshop on Statistical Machine Translation (WMT 2011), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT 2012),</booktitle>
<pages>10--51</pages>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. Findings of the 2012 workshop on statistical machine translation. In 7th Workshop on Statistical Machine Translation (WMT 2012), pages 10–51, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In The second international conference on Human Language Technology Research (HLT ’02),</booktitle>
<location>San Diego, California,</location>
<contexts>
<context position="2042" citStr="Doddington, 2002" startWordPosition="272" endWordPosition="274">hin automatic semantic MT evaluation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive te</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In The second international conference on Human Language Technology Research (HLT ’02), San Diego, California, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation (WMT-06),</booktitle>
<contexts>
<context position="2347" citStr="Koehn and Monz, 2006" startWordPosition="316" endWordPosition="319">licitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. Manual and automatic evaluation of machine translation between european languages. In Workshop on Statistical Machine Translation (WMT-06), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDer: Efficient MT evaluation using block movements.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<contexts>
<context position="2105" citStr="Leusch et al., 2006" startWordPosition="281" endWordPosition="284">eal-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the ba</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Efficient MT evaluation using block movements. In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT</booktitle>
<contexts>
<context position="2529" citStr="Lo and Wu, 2011" startWordPosition="343" endWordPosition="346"> main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST</context>
<context position="7810" citStr="Lo and Wu (2011" startWordPosition="1225" endWordPosition="1228">l associations si,pred and si,j are discussed in the following section. q&apos;,j and qz,j are the argument of type j in frame i in MT and REF, respectively. w9 and wz are the weights for frame i in MT and REF, respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider</context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>SMT vs. AI redux: How semantic frames evaluate MT more accurately.</title>
<date>2011</date>
<booktitle>In Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11),</booktitle>
<contexts>
<context position="2529" citStr="Lo and Wu, 2011" startWordPosition="343" endWordPosition="346"> main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST</context>
<context position="7810" citStr="Lo and Wu (2011" startWordPosition="1225" endWordPosition="1228">l associations si,pred and si,j are discussed in the following section. q&apos;,j and qz,j are the argument of type j in frame i in MT and REF, respectively. w9 and wz are the weights for frame i in MT and REF, respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider</context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Unsupervised vs. supervised weight estimation for semantic MT evaluation metrics.</title>
<date>2012</date>
<booktitle>In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6),</booktitle>
<contexts>
<context position="8004" citStr="Lo and Wu, 2012" startWordPosition="1256" endWordPosition="1259"> MT and REF, respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j, all of which employ a standard context vector model of the individual words/tokens in the multiword expression</context>
</contexts>
<marker>Lo, Wu, 2012</marker>
<rawString>Chi-kiu Lo and Dekai Wu. Unsupervised vs. supervised weight estimation for semantic MT evaluation metrics. In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-6), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Can informal genres be better translated by tuning on automatic semantic metrics?</title>
<date>2013</date>
<booktitle>In 14th Machine Translation Summit (MT Summit XIV),</booktitle>
<contexts>
<context position="2570" citStr="Lo and Wu, 2013" startWordPosition="352" endWordPosition="355">urface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee</context>
<context position="4183" citStr="Lo and Wu, 2013" startWordPosition="605" endWordPosition="608">d under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL decided to drop the predicate. adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English and Chinese) and different genres (formal newswire text, informal web forum text and informal public speech). Because of this, we have chosen to run our lexical association experiments in the context of the necessity of recognizing matching semantic role fillers, approximately 85% of which are multiword expressions in our data, the overwhelming majority of which would not be enumerated within conventional lexicons. We compare four common lexical access approaches to aggregation, preferences, and constraints: bagof-words, two different ty</context>
</contexts>
<marker>Lo, Wu, 2013</marker>
<rawString>Chi-kiu Lo and Dekai Wu. Can informal genres be better translated by tuning on automatic semantic metrics? In 14th Machine Translation Summit (MT Summit XIV), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame based mt evaluation metric.</title>
<date>2013</date>
<booktitle>In 8th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="2570" citStr="Lo and Wu, 2013" startWordPosition="352" endWordPosition="355">urface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee</context>
<context position="4183" citStr="Lo and Wu, 2013" startWordPosition="605" endWordPosition="608">d under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL decided to drop the predicate. adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English and Chinese) and different genres (formal newswire text, informal web forum text and informal public speech). Because of this, we have chosen to run our lexical association experiments in the context of the necessity of recognizing matching semantic role fillers, approximately 85% of which are multiword expressions in our data, the overwhelming majority of which would not be enumerated within conventional lexicons. We compare four common lexical access approaches to aggregation, preferences, and constraints: bagof-words, two different ty</context>
</contexts>
<marker>Lo, Wu, 2013</marker>
<rawString>Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame based mt evaluation metric. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
</authors>
<title>Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<marker>Lo, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Karteek Addanki</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving machine translation by training against an automatic semantic frame based evaluation metric.</title>
<date>2013</date>
<booktitle>In 51stAnnual Meeting ofthe Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="4165" citStr="Lo et al., 2013" startWordPosition="601" endWordPosition="604">is work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL decided to drop the predicate. adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English and Chinese) and different genres (formal newswire text, informal web forum text and informal public speech). Because of this, we have chosen to run our lexical association experiments in the context of the necessity of recognizing matching semantic role fillers, approximately 85% of which are multiword expressions in our data, the overwhelming majority of which would not be enumerated within conventional lexicons. We compare four common lexical access approaches to aggregation, preferences, and constraints: bagof-words</context>
</contexts>
<marker>Lo, Addanki, Saers, Wu, 2013</marker>
<rawString>Chi-kiu Lo, Karteek Addanki, Markus Saers, and Dekai Wu. Improving machine translation by training against an automatic semantic frame based evaluation metric. In 51stAnnual Meeting ofthe Association for Computational Linguistics (ACL 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Meriem Beloucif</author>
<author>Dekai Wu</author>
</authors>
<title>Improving machine translation into Chinese by tuning against Chinese MEANT.</title>
<date>2013</date>
<booktitle>In International Workshop on Spoken Language Translation (IWSLT</booktitle>
<contexts>
<context position="4165" citStr="Lo et al., 2013" startWordPosition="601" endWordPosition="604">is work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since automatic SRL decided to drop the predicate. adequacy (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) across different languages (English and Chinese) and different genres (formal newswire text, informal web forum text and informal public speech). Because of this, we have chosen to run our lexical association experiments in the context of the necessity of recognizing matching semantic role fillers, approximately 85% of which are multiword expressions in our data, the overwhelming majority of which would not be enumerated within conventional lexicons. We compare four common lexical access approaches to aggregation, preferences, and constraints: bagof-words</context>
</contexts>
<marker>Lo, Beloucif, Wu, 2013</marker>
<rawString>Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Improving machine translation into Chinese by tuning against Chinese MEANT. In International Workshop on Spoken Language Translation (IWSLT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matouš Macháček</author>
<author>Ondřej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Eighth Workshop on Statistical Machine Translation (WMT 2013),</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3433" citStr="Macháček and Bojar, 2013" startWordPosition="498" endWordPosition="501">and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: \url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using auto</context>
</contexts>
<marker>Macháček, Bojar, 2013</marker>
<rawString>Matouš Macháček and Ondřej Bojar. Results of the WMT13 metrics shared task. In Eighth Workshop on Statistical Machine Translation (WMT 2013), Sofia, Bulgaria, August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic construction of clean broad-coverage translation lexicons.</title>
<date>1996</date>
<booktitle>In 2nd Conference of the Association for Machine Translation in the Americas (AMTA-1996),</booktitle>
<contexts>
<context position="19131" citStr="Melamed, 1996" startWordPosition="2955" endWordPosition="2956">, 2009). Secondly, the permutation and bijectivity constraints enforced by the ITG provide better leverage to reject token alignments when they are not appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. 5 Conclusion We have compared four alternative lexical access strategies for aggregation, preferences, and constraints in scoring multiword expression associations that are far too numerous to be explicitly enumerated in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words, 149 Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized</context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>I. Dan Melamed. Automatic construction of clean broad-coverage translation lexicons. In 2nd Conference of the Association for Machine Translation in the Americas (AMTA-1996), 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>1999</date>
<booktitle>In The Twenty-first National Conference on Artificial Intelligence (AAAI06),</booktitle>
<volume>21</volume>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<marker>Mihalcea, Corley, Strapparava, 1999</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures of text semantic similarity. In The Twenty-first National Conference on Artificial Intelligence (AAAI06), volume 21. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>A evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In The Second International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. A evaluation tool for machine translation: Fast evaluation for MT research. In The Second International Conference on Language Resources and Evaluation (LREC 2000), 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="2017" citStr="Papineni et al., 2002" startWordPosition="267" endWordPosition="270">rd expression associations within automatic semantic MT evaluation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 311–318, Philadelphia, Pennsylvania, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<contexts>
<context position="2839" citStr="Pradhan et al., 2004" startWordPosition="397" endWordPosition="400"> the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furt</context>
<context position="17205" citStr="Pradhan et al., 2004" startWordPosition="2663" endWordPosition="2666">und is 0.53—so the fully automatic ITG based MEANT approximation is not far from closing the gap. Kendall correlation MEANT + ITG based 0.51 MEANT + maximum alignment (f-score) 0.48 MEANT + maximum alignment (average of precision &amp; recall) 0.46 MEANT + bag of words (geometric mean) 0.38 NIST 0.29 METEOR 0.20 BLEU 0.20 TER 0.20 PER 0.20 CDER 0.12 WER 0.10 systems are ranked in the same order as the human judgment by the evaluation metric; and -1 means the systems are ranked in the reverse order as human judgment by the evaluation metric. For both reference and machine translations, the ASSERT (Pradhan et al., 2004) semantic role labeler was used to automatically predict semantic parses. 4.2 Results and discussion The sentence-level correlations in Table 1 show that the ITG based strategy outperforms other automatic metrics in correlation with human adequacy judgment. Note that this was achieved with no tuning whatsoever of the rule weights (suggesting that the performance could be further improved in the future by slightly optimizing the ITG weights). The ITG based strategy shows 3 points improvement over the next best strategy, which is maximal alignment under f-score aggregation. The ITG based approac</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow semantic parsing using support vector machines. In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving phrase-based translation via word alignments from stochastic inversion transduction grammars.</title>
<date>2009</date>
<booktitle>In Third Workshop on Syntax and Structure in Statistical Translation (SSST-3),</booktitle>
<pages>28--36</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="14250" citStr="Saers and Wu, 2009" startWordPosition="2177" endWordPosition="2180">d expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,� as follows.  ( ( ))  lg P A � ⇒ ei,pred/fi,pred|G   max( |ei,pred |, |fi,pred|) si,pred = lg−1 where si,7  ( ( ))  lg−1 lg P A � ⇒ ei,�/fi,�|G  = max(|ei,�|,|fi,� |) G≡⟨{A},W�,W1,R,A⟩ R≡{A→[AA],A→⟨AA⟩,A→e/f} p([AA]|A)=p(⟨AA⟩|A)=1 p(e/f|A)=s(e,f) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e E W° U {E} denoting a token in the MT output (or the null token) and f E Wi U {E} denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rul</context>
<context position="18524" citStr="Saers and Wu, 2009" startWordPosition="2862" endWordPosition="2865">gy even comes within a few points of the human upper bound benchmark HAJ correlations computed using the human labeled semantic frames and alignments used in the HMEANT. Data analysis reveals two reasons that the ITG based strategy correlates with human adequacy judgement more closely than the other approaches. First, BITG constraints indeed provide more accurate phrasal similarity aggregation, compared to the naive bag-of-words based heuristics. Similar results have been observed while trying to estimate word alignment probabilities where BITG constraints outperformed alignments from GIZA++ (Saers and Wu, 2009). Secondly, the permutation and bijectivity constraints enforced by the ITG provide better leverage to reject token alignments when they are not appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed</context>
</contexts>
<marker>Saers, Wu, 2009</marker>
<rawString>Markus Saers and Dekai Wu. Improving phrase-based translation via word alignments from stochastic inversion transduction grammars. In Third Workshop on Syntax and Structure in Statistical Translation (SSST-3), pages 28–36, Boulder, Colorado, June 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>29--32</pages>
<location>Paris, France,</location>
<contexts>
<context position="12006" citStr="Saers et al., 2009" startWordPosition="1850" endWordPosition="1853">y for general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n3) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for e</context>
<context position="15212" citStr="Saers et al. (2009)" startWordPosition="2353" endWordPosition="2356"> nonterminal is A, and R is a set of transduction rules with e E W° U {E} denoting a token in the MT output (or the null token) and f E Wi U {E} denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined by MEANT’s lexical similarity measure on English Gigaword context vectors. To calculate the ( ) inside probability (or more accurately, inside score) of a pair of segments, P A � � e/f|G , we use the algorithm described in Saers et al. (2009). Given this, si,pred and si,� now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations. In this section we discuss experiments comparing the four altern ative lexical access preference and constraint strategies. The corpus includes the Chinese input sentences, each accompanied by an English reference translation and three participating MT output. We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 20</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies (IWPT’09), pages 29–32, Paris, France, October 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In 7th Biennial Conference Association for Machine Translation in the Americas (AMTA</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="2164" citStr="Snover et al., 2006" startWordPosition="292" endWordPosition="295">dequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Niel3en et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who d</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of translation edit rate with targeted human annotation. In 7th Biennial Conference Association for Machine Translation in the Americas (AMTA 2006), pages 223–231, Cambridge, Massachusetts, August 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Karthik Tumuluru</author>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In 26th Pacific Asia Conference on Language, Information, and Computation (PACLIC 26),</booktitle>
<contexts>
<context position="8721" citStr="Tumuluru et al. (2012)" startWordPosition="1360" endWordPosition="1363">abel in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j, all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al. (2012) and Tumuluru et al. (2012). 3.1 Bag of words (geometric mean) The original MEANT approaches employed standard a bag-of-words strategy for lexical association. This baseline approach applies no alignment constraints on multiword expressions: ∑ ∑ eEei,pred fEfi,pred lg(s(e,f)) si,pred = e |ei,pred|&apos;|fi,pred| si,j = e ∑ ∑ eEei,j fEfi,j lg(s(e,f)) |ei,j |&apos;|fi,j | w1i ≡ recall = 146 3.2 Maximum alignment (precision-recall average) In the first maximum alignment based approach we will consider, the definitions of sz,pred and sz,� are inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phras</context>
<context position="19070" citStr="Tumuluru et al. (2012)" startWordPosition="2945" endWordPosition="2948">re BITG constraints outperformed alignments from GIZA++ (Saers and Wu, 2009). Secondly, the permutation and bijectivity constraints enforced by the ITG provide better leverage to reject token alignments when they are not appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. 5 Conclusion We have compared four alternative lexical access strategies for aggregation, preferences, and constraints in scoring multiword expression associations that are far too numerous to be explicitly enumerated in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words, 149 Figure 2: An example of alig</context>
</contexts>
<marker>Tumuluru, Lo, Wu, 2012</marker>
<rawString>Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic MT evaluation. In 26th Pacific Asia Conference on Language, Information, and Computation (PACLIC 26), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the Association for Computational Linguistics (ACL 95),</booktitle>
<pages>244--251</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="12178" citStr="Wu, 1995" startWordPosition="1876" endWordPosition="1877"> reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n3) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. An algorithm for simultaneously bracketing parallel texts by aligning words. In 33rd Annual Meeting of the Association for Computational Linguistics (ACL 95), pages 244–251, Cambridge, Massachusetts, June 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Trainable coarse bilingual grammars for parallel text bracketing.</title>
<date>1995</date>
<booktitle>In Third Annual Workshop on Very Large Corpora (WVLC-3),</booktitle>
<pages>69--81</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="12178" citStr="Wu, 1995" startWordPosition="1876" endWordPosition="1877"> reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and can be biparsed in polynomial time and space (O (n�) time and O (n4) space). It is also possible to do approximate biparsing in O (n3) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In Third Annual Workshop on Very Large Corpora (WVLC-3), pages 69–81, Cambridge, Massachusetts, June 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="10149" citStr="Wu, 1997" startWordPosition="1574" endWordPosition="1575">raging of precision and recall with a proper f-score. Although this is less consistent with the previous literature, such as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT, and thus we include it in our comparison as a variant of the maximum alignment strategy. Si,pred = 2 · precei,pred,fi,pred · recei,pred,fi,pred Si,j = precei,pred,fi,pred + recei,pred,fi,pred 2 · precei,j,fi,j ·recei,j,fi,j precei,j ,fi,j + recei,j,fi,j 3.4 Inversion transduction grammar based There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve the accuracy of MT evaluation metrics—despite (1) long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints, and (2) the observation that most current state-of-the-art SMT systems employ ITG decoders. Especially when considering semantic MT metrics, ITGs would seem to be a natural strategy for multiword expression association for several cognitively motivated reasons, having to do with language universal properties of cross-linguistic semantic frame structure. To begin with, it is quite natural to think of s</context>
<context position="14210" citStr="Wu, 1997" startWordPosition="2171" endWordPosition="2172">tural. This can cause multiword expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,� as follows.  ( ( ))  lg P A � ⇒ ei,pred/fi,pred|G   max( |ei,pred |, |fi,pred|) si,pred = lg−1 where si,7  ( ( ))  lg−1 lg P A � ⇒ ei,�/fi,�|G  = max(|ei,�|,|fi,� |) G≡⟨{A},W�,W1,R,A⟩ R≡{A→[AA],A→⟨AA⟩,A→e/f} p([AA]|A)=p(⟨AA⟩|A)=1 p(e/f|A)=s(e,f) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e E W° U {E} denoting a token in the MT output (or the null token) and f E Wi U {E} denoting a token in the reference translation (or the null token). The r</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>The magic number 4: Evolutionary pressures on semantic frame structure.</title>
<date>2014</date>
<booktitle>In 10th International Conference on the Evolution ofLanguage (Evolang X),</booktitle>
<location>Vienna,</location>
<contexts>
<context position="12560" citStr="Wu, 2014" startWordPosition="1931" endWordPosition="1932">proximate biparsing in O (n3) time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. Following this reasoning, alternate definitions of sz,pred and sz,� can be constructed in terms of bracketing ITGs (also know</context>
</contexts>
<marker>Wu, 2014</marker>
<rawString>Dekai Wu. The magic number 4: Evolutionary pressures on semantic frame structure. In 10th International Conference on the Evolution ofLanguage (Evolang X), Vienna, Apr 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>144--151</pages>
<location>Stroudsburg, Pennsylvania,</location>
<contexts>
<context position="14230" citStr="Zens and Ney, 2003" startWordPosition="2173" endWordPosition="2176">s can cause multiword expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,� as follows.  ( ( ))  lg P A � ⇒ ei,pred/fi,pred|G   max( |ei,pred |, |fi,pred|) si,pred = lg−1 where si,7  ( ( ))  lg−1 lg P A � ⇒ ei,�/fi,�|G  = max(|ei,�|,|fi,� |) G≡⟨{A},W�,W1,R,A⟩ R≡{A→[AA],A→⟨AA⟩,A→e/f} p([AA]|A)=p(⟨AA⟩|A)=1 p(e/f|A)=s(e,f) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e E W° U {E} denoting a token in the MT output (or the null token) and f E Wi U {E} denoting a token in the reference translation (or the null token). The rule probability (or </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. A comparative study on reordering constraints in statistical machine translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 144–151, Stroudsburg, Pennsylvania, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
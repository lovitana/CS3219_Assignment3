<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.081823">
<title confidence="0.99148">
Generating Annotated Graphs using the NLG Pipeline Architecture
</title>
<author confidence="0.720592">
Saad Mahamood, William Bradshaw and Ehud Reiter
</author>
<affiliation confidence="0.313475">
Arria NLG plc
Aberdeen, Scotland, United Kingdom
</affiliation>
<email confidence="0.982212">
{saad.mahamood, william.bradshaw, ehud.reiter}@arria.com
</email>
<sectionHeader confidence="0.993636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999232833333333">
The Arria NLG Engine has been extended
to generate annotated graphs: data graphs
that contain computer-generated textual
annotations to explain phenomena in those
graphs. These graphs are generated along-
side text-only data summaries.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956428571429">
Arria NLG1 develops NLG solutions, primarily in
the data-to-text area. These solutions are NLG
systems, which generate textual summaries of
large numeric data sets. Arria’s core product is
the Arria NLG Engine,2 which is configured and
customised for the needs of different clients.
Recently Arria has extended this core engine
so that it can automatically produce annotated
graphs, that is, data graphs that have textual an-
notations explaining phenomena in those graphs
(see example in Figure 1). This was developed af-
ter listening to one of our customers, whose staff
manually created annotated graphs and found this
process to be very time-consuming. The anno-
tated graph generation process is integrated into
the NLG pipeline, and is carried out in conjunc-
tion with the generation of a textual summary of a
data set.
In this short paper we summarise the relevant
research background, and briefly describe what we
have achieved in this area.
</bodyText>
<sectionHeader confidence="0.830541" genericHeader="method">
2 Background: Multimodality and NLG
</sectionHeader>
<bodyText confidence="0.995868">
Rich media such as web pages and electronic doc-
uments typically include several modalities in a
given document. A web page, for example, can
contain images, graphs, and interactive elements.
Because of this there has been an interest within
</bodyText>
<footnote confidence="0.996514">
1Arria NLG plc (https://www.arria.com)
2For more information see: https://www.arria.
com/technology-A300.php
</footnote>
<bodyText confidence="0.9798995">
the NLG community in generating multimodal
documents. However, basic questions remain as
how best to combine and integrate multiple modal-
ities within a given NLG application.
</bodyText>
<subsectionHeader confidence="0.983825">
2.1 Annotated Graphics
</subsectionHeader>
<bodyText confidence="0.99995775">
Sripada and Gao (2007) conducted a small study
where they showed scuba divers different possi-
ble outputs from their ScubaText system, including
text-only, graph-only and annotated graphs. They
found that divers preferred the annotated graph
presentation. The ScubaText software could not in
practice produce annotated graphs for arbitrary in-
put data sets and automatically set the scale based
on detected events, so this was primarily a study
of user preferences.
McCoy and colleagues have been developing
techniques to automatically generate textual sum-
maries of data graphics for visually impaired users
(Demier et al., 2008). This differs from our work
because their goal is to replace the graph, whereas
our goal is to generate an integrated text/graphics
presentation.
There were several early systems in the 1990s
(Wahlster et al., 1993; Feiner and McKeown,
1990, for example), which generated integrated
presentations of figures and texts, but these sys-
tems focused on annotating static pictures and dia-
grams, not data graphics. The WIP system, which
combined static diagram images and text, used a
deep planning approach to produce tightly inte-
grated multimodal documents; it is not clear how
robustly this approach handled new data sets and
contexts.
</bodyText>
<subsectionHeader confidence="0.997788">
2.2 Embodied Conversational Agents
</subsectionHeader>
<bodyText confidence="0.9988294">
In recent years the challenge of combining mul-
tiple modalities such as text, speech, and/or ani-
mation has been addressed in the context of Em-
bodied Conversational Agents or ECAs. One ex-
ample is the NECA system (Krenn et al., 2002).
</bodyText>
<page confidence="0.950513">
123
</page>
<bodyText confidence="0.985277523809524">
Proceedings of the 8th International Natural Language Generation Conference, pages 123–127,
Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics
It allowed two embodied agents to converse with
each other via spoken dialogue while being able
to make gestures as well. From an architectural
perspective, NECA used a pipeline architecture in
some ways similar to the standard NLG data-to-
text pipeline (Reiter and Dale, 2000). Document
Planning is handled by the Scene Generator, which
selects the dialogue content. The ‘Multi-modal
NLG’ (M-NLG) component handles Microplan-
ning and Surface Realisation, and also deals with
specifying gestures, mood, and information struc-
ture. Thus the textual output generated by the sur-
face realiser in the NECA M-NLG component is
annotated with metadata for other modalities. In
particular, information on gestures, emotions, in-
formation structure, syntactic structure and dia-
logue structure (Krenn et al., 2002) are also in-
cluded to help inform the speech synthesis and
gesture assignment modules.
</bodyText>
<subsectionHeader confidence="0.996668">
2.3 Background: Psychology
</subsectionHeader>
<bodyText confidence="0.999948909090909">
The question of whether information is best pre-
sented in text or graphics is in principle largely one
for cognitive psychology. Which type of presenta-
tion is most effective, and in which context? The
answer of course depends on the communicative
goal, the type of data being presented, the type of
user, the communication medium and other con-
textual factors.
In particular, a number of researchers (Petre,
1995, for example) have pointed out that graphical
presentations require expertise to interpret them
and hence may be more appropriate for experi-
enced users than for novices. Tufte (1983) points
out that statistical graphs can be very misleading
for people who are not used to interpreting them.
Alberdi et al (2001) report on a number of psy-
chological studies on effectiveness of data visual-
isations which were performed with clinicians in
a Neonatal Intensive Care Unit (NICU). At a high
level, these studies found that visualisations were
less effective and less used than had been hoped.
Detailed findings include the following:
</bodyText>
<listItem confidence="0.996589076923077">
• Although consultants, junior doctors, and
nurses all claimed in interviews to make
heavy use of the computer system which
displayed visualisations, when observed on-
ward only senior consultants actually did so;
junior doctors and nurses rarely looked at the
computer screen.
• Senior consultants were much better than ju-
nior staff at distinguishing real events from
noise (sensor artefacts).
• Even senior consultants could only identify
70% of key events purely from the visualisa-
tions.
</listItem>
<bodyText confidence="0.998722461538461">
Law et al (2005) followed up the above work by
explicitly comparing the effectiveness of visuali-
sations and textual summaries. The textual sum-
maries in the experiment were manually written,
but did not contain any diagnoses and instead fo-
cused on describing the data. Law et al found that
clinicians at all levels made better decisions when
showed the textual summaries; however at all lev-
els they preferred the visualisations.
A similar study with computer generated sum-
mary texts produced by the Babytalk BT45 sys-
tem (Portet et al., 2009), conducted by van der
Meulen et al (2008), found that decision quality
was best when clinicians were shown manually
written summaries; computer generated texts were
of similar effectiveness to visualisations. An er-
ror analysis of this study (Reiter et al., 2008) con-
cluded that computer generated texts were much
more effective in some contexts than in others.
An implication of the above studies is that in
many cases the ideal strategy is to produce both
text and graphics. This increases decision effec-
tiveness (since the modalities work best in differ-
ent situations), and also increases user satisfaction,
since users see the modality they like as well as the
one which is most effective for decision support.
</bodyText>
<subsectionHeader confidence="0.997956">
2.4 Annotated Graphs in NLG Engine
</subsectionHeader>
<bodyText confidence="0.998948928571429">
We have extended our NLG Engine to generate an-
notated graphs as well as texts; an example output,
generated by a demonstration system, is shown in
figure 1. This example shows a very simple textual
output; examples of more complex textual output
are on the Arria website3.
This example output shows a comparison of
performance between the FTSE 100 and a given
stock portfolio. The value of the FTSE 100 is used
as a performance benchmark to see if a given stock
portfolio is performing better or worse than com-
pared to the stock market in general.
As can be seen in the graph in figure 1, the anno-
tations are small text fragments, which are placed
</bodyText>
<footnote confidence="0.998654">
3A more detailed example is given here: https://
www.arria.com/case-study-oilgas-A231.php
</footnote>
<page confidence="0.995875">
124
</page>
<figureCaption confidence="0.988712333333333">
Figure 1: Combined text and annotated graph detailing the stock portfolio performance
Figure 2: Graph illustrating stacking capabilities
when two annotations intersect each other
</figureCaption>
<bodyText confidence="0.999875305555555">
on top of the graph, and are linked to the relevant
events in that occur in the graph. Annotations can
also be placed at the bottom of graphs and at the
sides and can rearrange themselves depending on
the space available. In figure 1 the range annota-
tion used indicates the reason for the increase in
the value of a given stock portfolio over a particu-
lar time period. If one or more annotations collide
or intersect a stacking algorithm is used prior to
presentation to rearrange the placement of collid-
ing annotations as shown in figure 2.
Figure 3 illustrates the architecture that is used
by our NLG engine. The data analysis and data
interpretation modules analyse the input data and
produce a set of messages which can be communi-
cated to the user in the generated report. The doc-
ument planner decides on which messages should
be communicated overall, and where messages
should appear (for example, situational analysis
text, diagnosis text, impact text, graph annotation,
or a combination of these). The document planner
also decides on the type of graph used, and which
data channels it displays; these data channels must
include any channels which are annotated, but in
some cases other channels are displayed as well.
Once document planning is complete, the vi-
sualisation planning module generates the graph
design, including X and Y scale and the position
of the annotations on the graph. The time range
shown in the graph is largely determined by the
annotation messages. In other words, the decision
about what data to show on the graph is partially
driven by the annotations.
The annotation microplanner and realiser gener-
ate the actual annotation texts, using special rules
which are optimised for annotations (which need
</bodyText>
<page confidence="0.997905">
125
</page>
<figureCaption confidence="0.9471715">
Figure 3: Pipeline architecture of the Arria NLG
Engine
</figureCaption>
<bodyText confidence="0.988525714285714">
to be short and have different referring expres-
sions). After this has been completed, a renderer
produces the actual annotated graph. The final
task lies with the presenter module, which recom-
bines the separately generated summary text (gen-
erated by the NLG Microplanning and Realisation
modules) with the annotated graphs.
</bodyText>
<sectionHeader confidence="0.995259" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999888388888889">
Annotated graphs are a very appealing mechanism
for combining text and data graphics into a sin-
gle multimodal information presentation; this is
shown both by the findings of Sripada and Gao
(2007) and by the experiences of our customers.
Amongst other benefits, we believe that annotated
graphs will address some of the deficiencies in
data graphics which were pointed out by Alberdi
et al (2001), by helping users (especially inexpe-
rienced ones) to more easily identify key events
in a graph and also to distinguish real events from
sensor artefacts and other noise.
We have developed software to create annotated
graphs, by modifying the standard NLG data-to-
text pipeline as described above. Our clients have
reacted very positively so far, and we are now ex-
ploring extensions, for example by making anno-
tated graphs interactive.
</bodyText>
<sectionHeader confidence="0.990297" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999775979166667">
E. Alberdi, J. C. Becher, K. J. Gilhooly, J. R.W. Hunter,
R. H. Logie, A. Lyon, N. McIntosh, and J. Reiss.
2001. Expertise and the interpretation of comput-
erised physiological data: Implications for the de-
sign of computerised physiological monitoring in
neonatal intensive care. International Journal of
Human Computer Studies, 55(3):191–216.
S. Demier, S. Carberry, and K. F. McCoy. 2008. Gen-
erating textual summaries of bar charts. In Fifth
International Natural Language Generation Con-
ference (INLG 2008), pages 7–15. Association for
Computational Linguistics.
S. Feiner and K. R. McKeown. 1990. Generating Co-
ordinated Multimedia Explanations. In Sixth Con-
ference on Artificial Intelligence Applications, vol-
ume 290-296.
B. Krenn, H. Pirker, M. Grice, S. Baumann, P. Pi-
wek, K. van Deemter, M. Schroeder, M. Klesen, and
E. Gstrein. 2002. Generation of multi-modal di-
alogue for a net environment. In Proceedings of
KONVENS-02, Saarbruecken, Germany.
A. S. Law, Y. Freer, J. Hunter, R. H. Logie, N. McIn-
tosh, and J. Quinn. 2005. A comparison of graph-
ical and textual presentations of time series data to
support medical decision making in the neonatal in-
tensive care unit. Journal of Clinical Monitoring
and Computing, 19(3):183–194.
M. Petre. 1995. Why Looking isn’t always See-
ing: Readership Skills and Graphical Programming.
Communications of the ACM, 38:33–44.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173(7-8):789–816.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
E. Reiter, A. Gatt, F. Portet, and M. van der Meulen.
2008. The Importance of Narrative and Other
Lessons from an Evaluation of an NLG System that
Summarises Clinical Data. Fifth International Natu-
ral Language Generation Conference (INLG 2008),
pages 147–155.
S. G. Sripada and F. Gao. 2007. Summarizing dive
computer data: A case study in integrating textual
and graphical presentations of numerical data. In
MOG 2007 Workshop on Multimodal Output Gen-
eration, pages 149–157.
</reference>
<page confidence="0.982988">
126
</page>
<reference confidence="0.999495692307692">
E. Tufte. 1983. The Visual Display of Quantitative
Information. Graphics Press.
M. van der Meulen, R. Logie, Y. Freer, C. Sykes,
N. McIntosh, and J. Hunter. 2008. When a graph
is poorer than 100 words: A comparison of com-
puterised natural language generation, human gen-
erated descriptions and graphical displays in neona-
tal intensive care. Applied Cognitive Psychology,
24:77–89.
W. Wahlster, E. Andr´e, W. Finkle, HJ. Profitlich, and
T. Rist. 1993. Plan-based integration of natural
language and graphics generation. Artificial Intel-
ligence, 63:387–427.
</reference>
<page confidence="0.997325">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.368525">
<title confidence="0.991013">Generating Annotated Graphs using the NLG Pipeline Architecture</title>
<author confidence="0.588491">Saad Mahamood</author>
<author confidence="0.588491">William Bradshaw</author>
<author confidence="0.588491">Ehud</author>
<affiliation confidence="0.46923">Arria NLG</affiliation>
<address confidence="0.553418">Aberdeen, Scotland, United</address>
<email confidence="0.862794">william.bradshaw,</email>
<abstract confidence="0.998120857142857">The Arria NLG Engine has been extended to generate annotated graphs: data graphs that contain computer-generated textual annotations to explain phenomena in those graphs. These graphs are generated alongside text-only data summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Alberdi</author>
<author>J C Becher</author>
<author>K J Gilhooly</author>
<author>J R W Hunter</author>
<author>R H Logie</author>
<author>A Lyon</author>
<author>N McIntosh</author>
<author>J Reiss</author>
</authors>
<title>Expertise and the interpretation of computerised physiological data: Implications for the design of computerised physiological monitoring in neonatal intensive care.</title>
<date>2001</date>
<journal>International Journal of Human Computer Studies,</journal>
<volume>55</volume>
<issue>3</issue>
<contexts>
<context position="5356" citStr="Alberdi et al (2001)" startWordPosition="809" endWordPosition="812">for cognitive psychology. Which type of presentation is most effective, and in which context? The answer of course depends on the communicative goal, the type of data being presented, the type of user, the communication medium and other contextual factors. In particular, a number of researchers (Petre, 1995, for example) have pointed out that graphical presentations require expertise to interpret them and hence may be more appropriate for experienced users than for novices. Tufte (1983) points out that statistical graphs can be very misleading for people who are not used to interpreting them. Alberdi et al (2001) report on a number of psychological studies on effectiveness of data visualisations which were performed with clinicians in a Neonatal Intensive Care Unit (NICU). At a high level, these studies found that visualisations were less effective and less used than had been hoped. Detailed findings include the following: • Although consultants, junior doctors, and nurses all claimed in interviews to make heavy use of the computer system which displayed visualisations, when observed onward only senior consultants actually did so; junior doctors and nurses rarely looked at the computer screen. • Senio</context>
</contexts>
<marker>Alberdi, Becher, Gilhooly, Hunter, Logie, Lyon, McIntosh, Reiss, 2001</marker>
<rawString>E. Alberdi, J. C. Becher, K. J. Gilhooly, J. R.W. Hunter, R. H. Logie, A. Lyon, N. McIntosh, and J. Reiss. 2001. Expertise and the interpretation of computerised physiological data: Implications for the design of computerised physiological monitoring in neonatal intensive care. International Journal of Human Computer Studies, 55(3):191–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Demier</author>
<author>S Carberry</author>
<author>K F McCoy</author>
</authors>
<title>Generating textual summaries of bar charts.</title>
<date>2008</date>
<booktitle>In Fifth International Natural Language Generation Conference (INLG</booktitle>
<pages>7--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2631" citStr="Demier et al., 2008" startWordPosition="386" endWordPosition="389"> and Gao (2007) conducted a small study where they showed scuba divers different possible outputs from their ScubaText system, including text-only, graph-only and annotated graphs. They found that divers preferred the annotated graph presentation. The ScubaText software could not in practice produce annotated graphs for arbitrary input data sets and automatically set the scale based on detected events, so this was primarily a study of user preferences. McCoy and colleagues have been developing techniques to automatically generate textual summaries of data graphics for visually impaired users (Demier et al., 2008). This differs from our work because their goal is to replace the graph, whereas our goal is to generate an integrated text/graphics presentation. There were several early systems in the 1990s (Wahlster et al., 1993; Feiner and McKeown, 1990, for example), which generated integrated presentations of figures and texts, but these systems focused on annotating static pictures and diagrams, not data graphics. The WIP system, which combined static diagram images and text, used a deep planning approach to produce tightly integrated multimodal documents; it is not clear how robustly this approach han</context>
</contexts>
<marker>Demier, Carberry, McCoy, 2008</marker>
<rawString>S. Demier, S. Carberry, and K. F. McCoy. 2008. Generating textual summaries of bar charts. In Fifth International Natural Language Generation Conference (INLG 2008), pages 7–15. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feiner</author>
<author>K R McKeown</author>
</authors>
<title>Generating Coordinated Multimedia Explanations.</title>
<date>1990</date>
<booktitle>In Sixth Conference on Artificial Intelligence Applications,</booktitle>
<volume>volume</volume>
<pages>290--296</pages>
<contexts>
<context position="2872" citStr="Feiner and McKeown, 1990" startWordPosition="425" endWordPosition="428">sentation. The ScubaText software could not in practice produce annotated graphs for arbitrary input data sets and automatically set the scale based on detected events, so this was primarily a study of user preferences. McCoy and colleagues have been developing techniques to automatically generate textual summaries of data graphics for visually impaired users (Demier et al., 2008). This differs from our work because their goal is to replace the graph, whereas our goal is to generate an integrated text/graphics presentation. There were several early systems in the 1990s (Wahlster et al., 1993; Feiner and McKeown, 1990, for example), which generated integrated presentations of figures and texts, but these systems focused on annotating static pictures and diagrams, not data graphics. The WIP system, which combined static diagram images and text, used a deep planning approach to produce tightly integrated multimodal documents; it is not clear how robustly this approach handled new data sets and contexts. 2.2 Embodied Conversational Agents In recent years the challenge of combining multiple modalities such as text, speech, and/or animation has been addressed in the context of Embodied Conversational Agents or </context>
</contexts>
<marker>Feiner, McKeown, 1990</marker>
<rawString>S. Feiner and K. R. McKeown. 1990. Generating Coordinated Multimedia Explanations. In Sixth Conference on Artificial Intelligence Applications, volume 290-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krenn</author>
<author>H Pirker</author>
<author>M Grice</author>
<author>S Baumann</author>
<author>P Piwek</author>
<author>K van Deemter</author>
<author>M Schroeder</author>
<author>M Klesen</author>
<author>E Gstrein</author>
</authors>
<title>Generation of multi-modal dialogue for a net environment.</title>
<date>2002</date>
<booktitle>In Proceedings of KONVENS-02,</booktitle>
<location>Saarbruecken, Germany.</location>
<marker>Krenn, Pirker, Grice, Baumann, Piwek, van Deemter, Schroeder, Klesen, Gstrein, 2002</marker>
<rawString>B. Krenn, H. Pirker, M. Grice, S. Baumann, P. Piwek, K. van Deemter, M. Schroeder, M. Klesen, and E. Gstrein. 2002. Generation of multi-modal dialogue for a net environment. In Proceedings of KONVENS-02, Saarbruecken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Law</author>
<author>Y Freer</author>
<author>J Hunter</author>
<author>R H Logie</author>
<author>N McIntosh</author>
<author>J Quinn</author>
</authors>
<title>A comparison of graphical and textual presentations of time series data to support medical decision making in the neonatal intensive care unit.</title>
<date>2005</date>
<journal>Journal of Clinical Monitoring and Computing,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="6178" citStr="Law et al (2005)" startWordPosition="939" endWordPosition="942">visualisations were less effective and less used than had been hoped. Detailed findings include the following: • Although consultants, junior doctors, and nurses all claimed in interviews to make heavy use of the computer system which displayed visualisations, when observed onward only senior consultants actually did so; junior doctors and nurses rarely looked at the computer screen. • Senior consultants were much better than junior staff at distinguishing real events from noise (sensor artefacts). • Even senior consultants could only identify 70% of key events purely from the visualisations. Law et al (2005) followed up the above work by explicitly comparing the effectiveness of visualisations and textual summaries. The textual summaries in the experiment were manually written, but did not contain any diagnoses and instead focused on describing the data. Law et al found that clinicians at all levels made better decisions when showed the textual summaries; however at all levels they preferred the visualisations. A similar study with computer generated summary texts produced by the Babytalk BT45 system (Portet et al., 2009), conducted by van der Meulen et al (2008), found that decision quality was </context>
</contexts>
<marker>Law, Freer, Hunter, Logie, McIntosh, Quinn, 2005</marker>
<rawString>A. S. Law, Y. Freer, J. Hunter, R. H. Logie, N. McIntosh, and J. Quinn. 2005. A comparison of graphical and textual presentations of time series data to support medical decision making in the neonatal intensive care unit. Journal of Clinical Monitoring and Computing, 19(3):183–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Petre</author>
</authors>
<title>Why Looking isn’t always Seeing: Readership Skills and Graphical Programming.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--33</pages>
<contexts>
<context position="5044" citStr="Petre, 1995" startWordPosition="761" endWordPosition="762"> information structure, syntactic structure and dialogue structure (Krenn et al., 2002) are also included to help inform the speech synthesis and gesture assignment modules. 2.3 Background: Psychology The question of whether information is best presented in text or graphics is in principle largely one for cognitive psychology. Which type of presentation is most effective, and in which context? The answer of course depends on the communicative goal, the type of data being presented, the type of user, the communication medium and other contextual factors. In particular, a number of researchers (Petre, 1995, for example) have pointed out that graphical presentations require expertise to interpret them and hence may be more appropriate for experienced users than for novices. Tufte (1983) points out that statistical graphs can be very misleading for people who are not used to interpreting them. Alberdi et al (2001) report on a number of psychological studies on effectiveness of data visualisations which were performed with clinicians in a Neonatal Intensive Care Unit (NICU). At a high level, these studies found that visualisations were less effective and less used than had been hoped. Detailed fin</context>
</contexts>
<marker>Petre, 1995</marker>
<rawString>M. Petre. 1995. Why Looking isn’t always Seeing: Readership Skills and Graphical Programming. Communications of the ACM, 38:33–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Portet</author>
<author>E Reiter</author>
<author>A Gatt</author>
<author>J Hunter</author>
<author>S Sripada</author>
<author>Y Freer</author>
<author>C Sykes</author>
</authors>
<title>Automatic generation of textual summaries from neonatal intensive care data.</title>
<date>2009</date>
<journal>Artificial Intelligence,</journal>
<pages>173--7</pages>
<contexts>
<context position="6702" citStr="Portet et al., 2009" startWordPosition="1025" endWordPosition="1028">nsultants could only identify 70% of key events purely from the visualisations. Law et al (2005) followed up the above work by explicitly comparing the effectiveness of visualisations and textual summaries. The textual summaries in the experiment were manually written, but did not contain any diagnoses and instead focused on describing the data. Law et al found that clinicians at all levels made better decisions when showed the textual summaries; however at all levels they preferred the visualisations. A similar study with computer generated summary texts produced by the Babytalk BT45 system (Portet et al., 2009), conducted by van der Meulen et al (2008), found that decision quality was best when clinicians were shown manually written summaries; computer generated texts were of similar effectiveness to visualisations. An error analysis of this study (Reiter et al., 2008) concluded that computer generated texts were much more effective in some contexts than in others. An implication of the above studies is that in many cases the ideal strategy is to produce both text and graphics. This increases decision effectiveness (since the modalities work best in different situations), and also increases user sat</context>
</contexts>
<marker>Portet, Reiter, Gatt, Hunter, Sripada, Freer, Sykes, 2009</marker>
<rawString>F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada, Y. Freer, and C. Sykes. 2009. Automatic generation of textual summaries from neonatal intensive care data. Artificial Intelligence, 173(7-8):789–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3997" citStr="Reiter and Dale, 2000" startWordPosition="598" endWordPosition="601"> speech, and/or animation has been addressed in the context of Embodied Conversational Agents or ECAs. One example is the NECA system (Krenn et al., 2002). 123 Proceedings of the 8th International Natural Language Generation Conference, pages 123–127, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics It allowed two embodied agents to converse with each other via spoken dialogue while being able to make gestures as well. From an architectural perspective, NECA used a pipeline architecture in some ways similar to the standard NLG data-totext pipeline (Reiter and Dale, 2000). Document Planning is handled by the Scene Generator, which selects the dialogue content. The ‘Multi-modal NLG’ (M-NLG) component handles Microplanning and Surface Realisation, and also deals with specifying gestures, mood, and information structure. Thus the textual output generated by the surface realiser in the NECA M-NLG component is annotated with metadata for other modalities. In particular, information on gestures, emotions, information structure, syntactic structure and dialogue structure (Krenn et al., 2002) are also included to help inform the speech synthesis and gesture assignment</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>E. Reiter and R. Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>A Gatt</author>
<author>F Portet</author>
<author>M van der Meulen</author>
</authors>
<date>2008</date>
<booktitle>The Importance of Narrative and Other Lessons from an Evaluation of an NLG System that Summarises Clinical Data. Fifth International Natural Language Generation Conference (INLG</booktitle>
<pages>147--155</pages>
<marker>Reiter, Gatt, Portet, van der Meulen, 2008</marker>
<rawString>E. Reiter, A. Gatt, F. Portet, and M. van der Meulen. 2008. The Importance of Narrative and Other Lessons from an Evaluation of an NLG System that Summarises Clinical Data. Fifth International Natural Language Generation Conference (INLG 2008), pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Sripada</author>
<author>F Gao</author>
</authors>
<title>Summarizing dive computer data: A case study in integrating textual and graphical presentations of numerical data.</title>
<date>2007</date>
<booktitle>In MOG 2007 Workshop on Multimodal Output Generation,</booktitle>
<pages>149--157</pages>
<contexts>
<context position="2026" citStr="Sripada and Gao (2007)" startWordPosition="296" endWordPosition="299">d in this area. 2 Background: Multimodality and NLG Rich media such as web pages and electronic documents typically include several modalities in a given document. A web page, for example, can contain images, graphs, and interactive elements. Because of this there has been an interest within 1Arria NLG plc (https://www.arria.com) 2For more information see: https://www.arria. com/technology-A300.php the NLG community in generating multimodal documents. However, basic questions remain as how best to combine and integrate multiple modalities within a given NLG application. 2.1 Annotated Graphics Sripada and Gao (2007) conducted a small study where they showed scuba divers different possible outputs from their ScubaText system, including text-only, graph-only and annotated graphs. They found that divers preferred the annotated graph presentation. The ScubaText software could not in practice produce annotated graphs for arbitrary input data sets and automatically set the scale based on detected events, so this was primarily a study of user preferences. McCoy and colleagues have been developing techniques to automatically generate textual summaries of data graphics for visually impaired users (Demier et al., </context>
<context position="10702" citStr="Sripada and Gao (2007)" startWordPosition="1683" endWordPosition="1686"> for annotations (which need 125 Figure 3: Pipeline architecture of the Arria NLG Engine to be short and have different referring expressions). After this has been completed, a renderer produces the actual annotated graph. The final task lies with the presenter module, which recombines the separately generated summary text (generated by the NLG Microplanning and Realisation modules) with the annotated graphs. 3 Conclusion Annotated graphs are a very appealing mechanism for combining text and data graphics into a single multimodal information presentation; this is shown both by the findings of Sripada and Gao (2007) and by the experiences of our customers. Amongst other benefits, we believe that annotated graphs will address some of the deficiencies in data graphics which were pointed out by Alberdi et al (2001), by helping users (especially inexperienced ones) to more easily identify key events in a graph and also to distinguish real events from sensor artefacts and other noise. We have developed software to create annotated graphs, by modifying the standard NLG data-totext pipeline as described above. Our clients have reacted very positively so far, and we are now exploring extensions, for example by m</context>
</contexts>
<marker>Sripada, Gao, 2007</marker>
<rawString>S. G. Sripada and F. Gao. 2007. Summarizing dive computer data: A case study in integrating textual and graphical presentations of numerical data. In MOG 2007 Workshop on Multimodal Output Generation, pages 149–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tufte</author>
</authors>
<title>The Visual Display of Quantitative Information.</title>
<date>1983</date>
<publisher>Graphics Press.</publisher>
<contexts>
<context position="5227" citStr="Tufte (1983)" startWordPosition="789" endWordPosition="790">ground: Psychology The question of whether information is best presented in text or graphics is in principle largely one for cognitive psychology. Which type of presentation is most effective, and in which context? The answer of course depends on the communicative goal, the type of data being presented, the type of user, the communication medium and other contextual factors. In particular, a number of researchers (Petre, 1995, for example) have pointed out that graphical presentations require expertise to interpret them and hence may be more appropriate for experienced users than for novices. Tufte (1983) points out that statistical graphs can be very misleading for people who are not used to interpreting them. Alberdi et al (2001) report on a number of psychological studies on effectiveness of data visualisations which were performed with clinicians in a Neonatal Intensive Care Unit (NICU). At a high level, these studies found that visualisations were less effective and less used than had been hoped. Detailed findings include the following: • Although consultants, junior doctors, and nurses all claimed in interviews to make heavy use of the computer system which displayed visualisations, when</context>
</contexts>
<marker>Tufte, 1983</marker>
<rawString>E. Tufte. 1983. The Visual Display of Quantitative Information. Graphics Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M van der Meulen</author>
<author>R Logie</author>
<author>Y Freer</author>
<author>C Sykes</author>
<author>N McIntosh</author>
<author>J Hunter</author>
</authors>
<title>When a graph is poorer than 100 words: A comparison of computerised natural language generation, human generated descriptions and graphical displays in neonatal intensive care. Applied Cognitive Psychology,</title>
<date>2008</date>
<pages>24--77</pages>
<marker>van der Meulen, Logie, Freer, Sykes, McIntosh, Hunter, 2008</marker>
<rawString>M. van der Meulen, R. Logie, Y. Freer, C. Sykes, N. McIntosh, and J. Hunter. 2008. When a graph is poorer than 100 words: A comparison of computerised natural language generation, human generated descriptions and graphical displays in neonatal intensive care. Applied Cognitive Psychology, 24:77–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Profitlich</author>
<author>T Rist</author>
</authors>
<title>Plan-based integration of natural language and graphics generation.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--387</pages>
<marker>Profitlich, Rist, 1993</marker>
<rawString>W. Wahlster, E. Andr´e, W. Finkle, HJ. Profitlich, and T. Rist. 1993. Plan-based integration of natural language and graphics generation. Artificial Intelligence, 63:387–427.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
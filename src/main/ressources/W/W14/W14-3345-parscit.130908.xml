<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000477">
<title confidence="0.993151">
Parmesan: Meteor without Paraphrases with Paraphrased References
</title>
<author confidence="0.9886">
Petra Baranˇcikov´a
</author>
<affiliation confidence="0.9870865">
Institute of Formal and Applied Linguistics
Charles University in Prague, Faculty of Mathematics and Physics
</affiliation>
<address confidence="0.760327">
Malostransk´e n´amˇest´ı 25, Prague, Czech Republic
</address>
<email confidence="0.997577">
barancikova@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999473611111111">
This paper describes Parmesan, our sub-
mission to the 2014 Workshop on Sta-
tistical Machine Translation (WMT) met-
rics task for evaluation English-to-Czech
translation. We show that the Czech Me-
teor Paraphrase tables are so noisy that
they actually can harm the performance of
the metric. However, they can be very
useful after extensive filtering in targeted
paraphrasing of Czech reference sentences
prior to the evaluation. Parmesan first per-
forms targeted paraphrasing of reference
sentences, then it computes the Meteor
score using only the exact match on these
new reference sentences. It shows sig-
nificantly higher correlation with human
judgment than Meteor on the WMT12 and
WMT13 data.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992316627450981">
The metric for automatic evaluation of machine
translation (MT) Meteor1 (Denkowski and Lavie,
2011) has shown high correlation with human
judgment since its appearance. It outperforms tra-
ditional metrics like BLEU (Papineni et al., 2002)
or NIST (Doddington, 2002) as it explicitly ad-
dresses their weaknesses – it takes into account re-
call, distinguishes between functional and content
words, allows language-specific tuning of param-
eters and many others.
Another important advantage of Meteor is that
it supports not only exact word matches between
a hypothesis and its corresponding reference sen-
tence, but also matches on the level of stems, syn-
onyms and paraphrases. The Meteor Paraphrase
tables (Denkowski and Lavie, 2010) were created
automatically using the pivot method (Bannard
and Callison-Burch, 2005) for six languages.
1We use the the version 1.4., which was recently outdated
as the new version 1.5. was released for WMT14
The basic setting of Meteor for evaluation
of Czech sentences offers two levels of matches
- exact and paraphrase. In this paper, we show the
impact of the quality of paraphrases on the perfor-
mance of Meteor. We demonstrate that the Czech
Meteor Paraphrase tables are full of noise and their
addition to the metric worsens its correlation with
human judgment. However, they can be very use-
ful (after extensive filtering) in creating new refer-
ence sentences by targeted paraphrasing.
Parmesan2 starts with a simple greedy algo-
rithm for substitution of synonymous words from
a hypothesis in its corresponding reference sen-
tence. Further, we apply Depfix (Rosa et al., 2012)
to fix grammar errors that might arise by the sub-
stitutions.
Our method is independent of the evaluation
metric used. In this paper, we use Meteor for
its consistently high correlation with human judg-
ment and we attempt to tune it further by mod-
ifying its paraphrase tables. We show that re-
ducing the size of the Meteor Paraphrase tables
is very beneficial. On the WMT12 and WMT13
data, the Meteor scores computed using only the
exact match on our new references significantly
outperform Meteor with both exact and paraphrase
match on original references. However, this result
was not confirmed by this year’s data.
We perform our experiments on English-to-
Czech translations, but the method is largely lan-
guage independent.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999634166666667">
Our paraphrasing work is inspired by Kauchak and
Barzilay (2006). They are trying to improve the
accuracy of MT evaluation of Chinese-to-English
translation by targeted paraphrasing, i.e. making
a reference closer in wording to a hypothesis (MT
output) while keeping its meaning and correctness.
</bodyText>
<footnote confidence="0.854075">
2PARaphrasing for MEteor SANs paraphrases
</footnote>
<page confidence="0.975697">
355
</page>
<bodyText confidence="0.951686703703704">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 355–361,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
Having a hypothesis H = h1,..., hn and
its corresponding reference translation R =
r1, ..., rm, they select a set of candidates C =
{(ri, hj)|ri E R \ H, hj E H \ R}.
C is reduced to pairs of words appearing
in the same WordNet (Miller, 1995) synset only.
For every pair (ri, hj) E C, hj is eval-
uated in the context r1, ..., ri−1, ❑, ri+1, ..., rm
and if confirmed, the new reference sentence
r1, ...,ri−1, hj, ri+1, ..., rm is created. This way,
several reference sentences might be created, all
with a single changed word with respect to the
original one.
In Baranˇcikov´a et al. (2014), we experiment
with several methods of paraphrasing of Czech
sentences and filtering the Czech Meteor tables.
We show that the amount of noise in the multi-
word paraphrases is very high and no automatic
filtering method we used outperforms omitting
them completely. We present an error analysis
based method of filtering paraphrases consisting
of pairs of single words, which is used in subsec-
tion 3.1. From several methods of paraphrasing,
we achieved the best results a with simple greedy
method, which is presented in section 4.
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.998946772727273">
We perform our experiments on data sets from
the English-to-Czech translation task of WMT12
(Callison-Burch et al., 2012), WMT13 (Bojar et
al., 2013) and WMT14 (Bojar et al., 2014). The
data sets contain 13/143/10 files with Czech out-
puts of MT systems. In addition, each data set con-
tains one file with corresponding reference sen-
tences and one with original English source sen-
tences. We perform morphological analysis and
tagging of the hypotheses and the reference sen-
tences using Morˇce (Spoustov´a et al., 2007).
The human judgment of hypotheses is available
as a relative ranking of performance of five sys-
tems for a sentence. We calculated the score for
every system by the “&gt; others” method (Bojar et
al., 2011), which was the WMT12 official sys-
tem score. It is computed as wins
wins+loses. We refer
to this interpretation of human judgment as silver
standard to distinguish it from the official system
scores, which were computed differently each year
(here referred to as gold standard).
</bodyText>
<footnote confidence="0.9838565">
3We use only 12 of them because two of them (FDA.2878
and online-G) have no human judgments.
</footnote>
<table confidence="0.998722">
WMT12 WMT13 WMT14
WordNet 0.26 0.22 0.24
filtered Meteor 1.53 1.29 1.39
together 1.59 1.34 1.44
</table>
<tableCaption confidence="0.856008">
Table 1: Average number of one-word paraphrases
per sentence found in WordNet, filtered Meteor ta-
bles and their union over all systems.
</tableCaption>
<subsectionHeader confidence="0.999969">
3.1 Sources of Paraphrases
</subsectionHeader>
<bodyText confidence="0.999992448275862">
We use two available sources of Czech para-
phrases – the Czech WordNet 1.9 PDT (Pala and
Smrˇz, 2004) and the Meteor Paraphrase Tables
(Denkowski and Lavie, 2010).
The Czech WordNet 1.9 PDT contains para-
phrases of high quality, however, their amount is
insufficient for our purposes. It contains 13k pairs
of synonymous lemmas and only one paraphrase
per four sentences on average is found in the data
(see Table 1). For that reason, we employ the
Czech Meteor Paraphrase tables, too. They are
quite the opposite of Czech WordNet – they are
large in size, but contain a lot of noise.
We attempt to reduce the noise in the Czech Me-
teor Paraphrase tables in the following way. We
keep only pairs consisting of single words since
we were not successful in reducing the noise ef-
fectively for the multi-word paraphrases (?).
Using Morˇce, we first perform morphological
analysis of all one-word pairs and replace the word
forms with their lemmas. We keep only pairs of
different lemmas. Further, we dispose of pairs of
words that differ in their parts of speech (POS)
or contain an unknown word (typically a foreign
word).
In this way we have reduced 684k paraphrases
in the original Czech Meteor Paraphrase tables
to only 32k pairs of lemmas. We refer to this table
as filtered Meteor.
</bodyText>
<sectionHeader confidence="0.967449" genericHeader="method">
4 Creating New References
</sectionHeader>
<bodyText confidence="0.9999631">
We create new references similarly to Kauchak
and Barzilay (2006). Let HL, RL be sets of lem-
mas from a hypothesis and a corresponding refer-
ence sentence, respectively. Then we select candi-
dates for paraphrasing in the following way: CL =
{(r, h)|r E RL � HL, h E HL � RL, rPOS =
hPOS}, where rPOS and hPOS denote the part
of speech of the respective lemma.
Further, we restrict the set CL to pairs appearing
in our paraphrase tables only. If a word has several
</bodyText>
<page confidence="0.991465">
356
</page>
<figure confidence="0.9906575">
Source
The location alone is classic.
Hypothesis
Samotn´e misto je klasick´e .
Actual placeneut is classicneut .
The place alone is classic.
Reference
Uˇz poloha je klasick´a .
Already positionfem is classicfem .
The position itself is classic.
Before Depfix
New reference
Uˇz misto je klasick´a .
Already placeneut is classicfem .
*The place itself is classic.
Uˇz misto je klasick´e .
Already placeneut is classicneut .
The place itself is classic.
</figure>
<figureCaption confidence="0.969459">
Figure 1: Example of the targeted paraphrasing. The hypothesis is grammatically correct and has very
similar meaning as the reference sentence. The new reference is closer in wording to the hypothesis,
but the agreement between the noun and the adjective is broken. Depfix resolves the error and the final
reference is correct. Number of overlapping unigrams increased from 2 to 4.
</figureCaption>
<table confidence="0.998833142857143">
metric reference WMT12 WMT13
BLEU original 0.751 0.835
new 0.834 0.891
METEOR original 0.833 0.817
new 0.927 0.891
1 -TER original 0.274 0.760
new 0.283 0.781
</table>
<tableCaption confidence="0.952768">
Table 2: Pearson’s correlation of different metrics
with the silver standard.
</tableCaption>
<bodyText confidence="0.989156272727273">
paraphrases in CL, we give preference to those
found in WordNet or even better in both WordNet
and filtered Meteor.
We proceed word by word from the beginning
of the reference sentence to its end. If a lemma
of a word appears as the first member of a pair
in restricted CL, it is replaced by the word form
from hypothesis that has its lemma as the second
element of that pair, i.e., by the paraphrase from
the hypothesis. Otherwise, the original word the
reference sentence is kept.
When integrating paraphrases to the reference
sentence, it may happen that the sentence becomes
ungrammatical, e.g., due to a broken agreement
(see Figure 1). Therefore, we apply Depfix (Rosa
et al., 2012) – a system for automatic correction
of grammatical errors that appear often in English-
to-Czech MT outputs.
Depfix analyses the input sentences using
a range of natural language processing tools. It
fixes errors using a set of linguistically-motivated
rules and a statistical component it contains.
</bodyText>
<sectionHeader confidence="0.923673" genericHeader="method">
5 Choosing a metric
</sectionHeader>
<bodyText confidence="0.999913">
Our next step is choosing a metric that correlates
well with human judgment. We experiment with
three common metrics – BLEU, Meteor and TER.
Based on the results (see Table 2), we decided to
employ Meteor in WMT14 as our metric because
it shows consistently highest correlations.
</bodyText>
<sectionHeader confidence="0.997033" genericHeader="method">
6 Meteor settings
</sectionHeader>
<bodyText confidence="0.99994375">
Based on the positive impact of filtering Meteor
Paraphrase Tables for targeted lexical paraphras-
ing of reference sentences (see the column Ba-
sic in Table 4), we experiment with the filtering
them yet again, but this time as an inner part of the
Meteor evaluation metric (i.e. for the paraphrase
match).
We experiment with seven different settings that
are presented in Table 3. All of them are cre-
ated by reducing the original Meteor Paraphrase
tables, except for the setting referred to as Word-
Net in the table. In this case, the paraphrase table
is generated from one-word paraphrases in Czech
WordNet to all their possible word forms found
in CzEng (Bojar et al., 2012).
Prior paraphrasing reference sentences and us-
ing Meteor with the No paraphr. setting for
computing scores constitutes Parmesan – our sub-
mission to the WMT14 for evaluation English-
to-Czech translation. In the tables with results,
</bodyText>
<page confidence="0.973934">
357
</page>
<figure confidence="0.58867875">
setting
Basic
One-word
Same POS
Diff. Lemma
Same Lemma
No paraphr.
WordNet
</figure>
<bodyText confidence="0.3979465">
size description of the paraphrase table
The original Meteor Paraphrase Tables
Basic without multi-word pairs
One-word + only same part-of-speech pairs
Same POS + only forms of different lemma
Same POS + only forms of same lemma
No paraphrase tables, i.e., exact match only
Paraphrase tables generated from Czech WordNet
</bodyText>
<figure confidence="0.821443857142857">
684k
181k
122k
71k
51k
0
202k
</figure>
<tableCaption confidence="0.994789">
Table 3: Different paraphrase tables for Meteor and their size (number of paraphrase pairs).
</tableCaption>
<table confidence="0.9908994">
WMT12
reference Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet
Original 0.833 0.836 0.840 0.838 0.863 0.861 0.863
Before Depfix 0.905 0.908 0.911 0.911 0.931 0.931 0.931
New 0.927 0.930 0.931 0.932 0.950 0.951 0.951
WMT13
references Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet
Original 0.817 0.820 0.823 0.821 0.850 0.848 0.850
Before Depfix 0.865 0.867 0.869 0.868 0.895 0.895 0.894
New 0.891 0.892 0.893 0.892 0.915 0.915 0.915
</table>
<tableCaption confidence="0.999751">
Table 4: Pearson’s correlation of Meteor and the silver standard.
</tableCaption>
<bodyText confidence="0.952941">
Parmesan scores are highlighted by the box and
the best scores are in bold.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="evaluation">
7 Results
</sectionHeader>
<subsectionHeader confidence="0.944608">
7.1 WMT12 and WMT13
</subsectionHeader>
<bodyText confidence="0.999661611111111">
The results of our experiments are presented in Ta-
ble 44 as Pearson’s correlation coefficient of the
Meteor scores and the human judgment. The re-
sults in both tables are very consistent. There is
a clear positive impact of the prior paraphrasing
of the reference sentences and of applying Depfix.
The results also show that independently of a ref-
erence sentence used, reducing the Meteor para-
phrase tables in evaluation is always beneficial.
We use a freely available implementation5
of Meng et al. (1992) to determine whether the
difference in correlation coefficients is statistically
significant. The tests show that Parmesan per-
forms better than original Meteor with 99% cer-
tainty on the data from WMT12 and WMT13.
Diff. Lemma and WordNet settings give the
best results on the original reference sentences.
That is because they are basically a limited version
</bodyText>
<tableCaption confidence="0.73666">
4The results of WMT13 using the gold standard are in
Table 5.
</tableCaption>
<footnote confidence="0.861901">
5http://www.cnts.ua.ac.be/∼vincent/scripts/rtest.py
</footnote>
<bodyText confidence="0.99993136">
of the paraphrase tables we use for creating our
new references, which contain both all different
lemmas of the same part of speech from Meteor
Paraphrase tables and all lemmas from the Word-
Net.
The main reason of the worse performance
of the metric when employing the Meteor Para-
phrase tables is the noise. It is especially apparent
for multi-word paraphrases (Baranˇcikov´a et al.,
2014); however, there are problems among one-
word paraphrases as well. Significant amount of
them are pairs of different word forms of a single
lemma, which may award even completely non-
grammatical sentences. This is reflected in the low
correlation of the Same Lemma setting.
Even worse is the fact that the metric may award
even parts of the hypothesis left untranslated, as
the original Meteor Paraphrase tables contain En-
glish words and their Czech translations as para-
phrases. There are for example pairs: pˇsenice -
wheat6, v˚udce - leader, vaˇrit - cook, poloostrov
- peninsula. For these reasons, the differences
among the systems are more blurred and the met-
ric performs worse than without using the para-
phrases.
</bodyText>
<footnote confidence="0.959147">
6In all examples the Czech word is the correct translation
of the English side.
</footnote>
<page confidence="0.98994">
358
</page>
<table confidence="0.9521896">
WMT13
references Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet
Original 0.856 0.859 0.862 0.860 0.885 0.883 0.884
Before Depfix 0.894 0.896 0.898 0.897 0.918 0.917 0.917
New 0.918 0.918 0.919 0.919 0.933 0.933 0.933
</table>
<tableCaption confidence="0.784309333333333">
Table 5: Pearson’s correlation of Meteor and the gold standard – Expected Wins (Bojar et al., 2013). The
results corresponds very well with the silver standard in Table 4.
frequency Basic No paraphr.
0.837 0.869
0.818 0.852
Table 6: The frequency column shows average
</tableCaption>
<bodyText confidence="0.9900406">
number of substitution per sentence using the orig-
inal Meteor Paraphrase tables only. The rest shows
Pearson’s correlation with the silver standard us-
ing these paraphrases.
We also experimented with paraphrasing using
the original Meteor Paraphrase tables for a com-
parison. We used the same pipeline as it is de-
scribed in Section 4, but used only original one-
word paraphrases from the Meteor Paraphrase ta-
bles. Even though the paraphrase tables are much
larger than our filtered Meteor tables, the amount
of substituted words is much smaller (see Table 6)
due to not being lemmatized. The Basic setting
in Table 6 corresponds well with the setting One-
word in Table 4 on original reference sentences.
The results for No paraphr. setting in Table 6 out-
performs all correlations with original references
but cannot compete with our new reference sen-
tences created by the filtered Meteor and Word-
Net.
</bodyText>
<subsectionHeader confidence="0.996026">
7.2 WMT14
</subsectionHeader>
<bodyText confidence="0.999797255813954">
The WMT14 data did not follow similar patterns
as data from two previous years. The results are
presented in Table 7 (the silver standard) and in
Table 8 (the gold standard).
While reducing the Meteor tables during the
evaluation is still beneficial, this is not entirely
valid about the prior paraphrasing of reference
sentences. The baseline correlation of Meteor
is rather high and paraphrasing sometimes helps
and sometimes harms the performance of the met-
ric. Nevertheless, the differences in correlation be-
tween the original references and the new ones are
very small (0.012 at most).
In contrast to WMT12 and WMT13, the first
phase of paraphrasing before applying Depfix
causes a drop in correlation. On the other hand,
applying Depfix is again always beneficial.
With both standards, the best result is achieved
on the original reference with the No paraphr.
and the WordNet setting. Parmesan outperforms
Meteor by a marginal difference (0.005) on the sil-
ver standard, whereas using the gold standard, Me-
teor is better by exactly the same margin. How-
ever, the correlation of the two standards is 0.997.
There is a distinctive difference between the
data from previous years and this one. In the
WMT14, the English source data for translating
to Czech are sentences originally English or pro-
fessionally translated from Czech to English. In
the previous years, on the other hand, the source
data were equally composed from all competing
languages, i.e., only fifth/sixth of data is originally
English.
One more language involved in the transla-
tion seems as a possible ground for the benefi-
cial effect of prior paraphrasing of reference sen-
tences. Therefore, we experiment with limiting
the WMT12 and WMT13 data to only sentences
that are originally Czech or English. However,
Parmesan on this limited translations again signifi-
cantly outperforms Meteor and the results (see Ta-
ble 9) follow similar patterns as on the whole data
sets.
</bodyText>
<sectionHeader confidence="0.972625" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999924545454545">
We have demonstrated a negative effect of noise
in the Czech Meteor Paraphrase tables to the per-
formance of Meteor. We have shown that large-
scale reduction of the paraphrase tables can be
very beneficial for targeted paraphrasing of ref-
erence sentences. The Meteor scores computed
without the Czech Meteor Paraphrase tables on
these new reference sentences correlates signifi-
cantly better with the human judgment than orig-
inal Meteor on the WMT12 and WMT13 data.
However, the WMT14 data has not confirmed
</bodyText>
<figure confidence="0.9856695">
WMT12
WMT13
0.75
0.61
</figure>
<page confidence="0.963077">
359
</page>
<table confidence="0.965973181818182">
WMT14
reference Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet
Original 0.963 0.967 0.965 0.968 0.970 0.973 0.973
Before Depfix 0.957 0.958 0.959 0.959 0.965 0.965 0.965
New 0.968 0.965 0.969 0.969 0.968 0.968 0.968
Table 7: Pearson’s correlation of Meteor and the silver standard.
WMT14
reference Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet
Original 0.967 0.968 0.969 0.972 0.972 0.974 0.974
Before Depfix 0.958 0.959 0.959 0.960 0.963 0.963 0.963
New 0.966 0.966 0.966 0.967 0.962 0.962 0.962
</table>
<tableCaption confidence="0.995655">
Table 8: Pearson’s correlation of Meteor and the gold standard – TrueSkill (Bojar et al., 2014). Note that
as opposed to official WMT14 results, the version 1.4 of Meteor is still used in this table.
</tableCaption>
<table confidence="0.9843826">
WMT12
reference Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet
Original 0.781 0.779 0.782 0.772 0.807 0.798 0.801
Before Depfix 0.872 0.872 0.874 0.874 0.898 0.899 0.899
New 0.897 0.897 0.897 0.897 0.923 0.923 0.923
WMT13
reference Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet
Original 0.805 0.810 0.813 0.813 0.842 0.840 0.844
Before Depfix 0.843 0.846 0.849 0.848 0.879 0.877 0.877
New 0.874 0.877 0.878 0.877 0.877 0.902 0.902
</table>
<tableCaption confidence="0.910338">
Table 9: Pearson’s correlation of Meteor and the silver standard on sentences originally Czech or English
only. In this case, the interpretation of human judgment was computed only on those sentences as well.
</tableCaption>
<bodyText confidence="0.9998747">
this result and the improvement was very small.
Furthermore, Parmesan performs even worse than
Meteor on the gold standard.
In the future, we plan to thoroughly examine the
reason for the different performance on WMT14
data. We also intend to make more sophisticated
paraphrases including word order changes and
other transformation that cannot be expressed by
simple substitution of two words. We are also con-
sidering extending Parmesan to more languages.
</bodyText>
<sectionHeader confidence="0.969767" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999436222222222">
I would like to thank Ond&amp;quot;rej Bojar for his help-
ful suggestions. This research was partially sup-
ported by the grants SVV project number 260 104
and FP7-ICT-2011-7-288487 (MosesCore). This
work has been using language resources devel-
oped and/or stored and/or distributed by the LIN-
DAT/CLARIN project of the Ministry of Edu-
cation, Youth and Sports of the Czech Republic
(project LM2010013).
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99668165">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ’05, pages 597–
604, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Petra Baran&amp;quot;cikov´a, Rudolf Rosa, and Ale&amp;quot;s Tamchyna.
2014. Improving Evaluation of English-Czech MT
through Paraphrasing. In Proceedings of the 9th In-
ternational Conference on Language Resources and
Evaluation (LREC 2014), Reykjavik, Iceland. Euro-
pean Language Resources Association.
Ond&amp;quot;rej Bojar, Milos&amp;quot; Ercegov&amp;quot;cevi´c, Martin Popel, and
Omar F. Zaidan. 2011. A Grain of Salt for the
WMT Manual Evaluation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
WMT ’11, pages 1–11, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ond&amp;quot;rej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ond&amp;quot;rej Du&amp;quot;sek, Pe-
tra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;riMar&amp;quot;sik, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tam-
</reference>
<page confidence="0.989442">
360
</page>
<reference confidence="0.999505897435897">
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921–3928. ELRA.
Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Ond&amp;quot;rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matous&amp;quot; Mach´a&amp;quot;cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 Workshop on Statistical
Machine Translation. In Proceedings of the Ninth
Workshop on Statistical Machine Translation, Bal-
timore, USA, June. Association for Computational
Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Seventh Workshop on Statis-
tical Machine Translation, pages 10–51, Montr´eal,
Canada.
Michael Denkowski and Alon Lavie. 2010.
METEOR-NEXT and the METEOR Paraphrase Ta-
bles: Improved Evaluation Support For Five Target
Languages. In Proceedings of the ACL 2010 Joint
Workshop on Statistical Machine Translation and
Metrics MATR.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
occurrence Statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ’02, pages 138–145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for Automatic Evaluation. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ’06, pages 455–462, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Xiao-Li Meng, Robert Rosenthal, and Donald B Ru-
bin. 1992. Comparing correlated correlation coeffi-
cients. Psychological bulletin, 111(1):172.
George A. Miller. 1995. WordNet: A Lexical
Database for English. COMMUNICATIONS OF
THEACM, 38:39–41.
Karel Pala and Pavel Smr&amp;quot;z. 2004. Building Czech
WordNet. In Romanian Journal of Information Sci-
ence and Technology, 7:79–88.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Rudolf Rosa, David Mare&amp;quot;cek, and Ond&amp;quot;rej Du&amp;quot;sek.
2012. DEPFIX: A System for Automatic Correc-
tion of Czech MT Outputs. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, WMT ’12, pages 362–368, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Drahomira Spoustov´a, Jan Haji&amp;quot;c, Jan Votrubec, Pavel
Krbec, and Pavel Kv&amp;quot;eto&amp;quot;n. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67–74, Praha.
</reference>
<page confidence="0.998709">
361
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.705360">
<title confidence="0.998194">Parmesan: Meteor without Paraphrases with Paraphrased References</title>
<author confidence="0.976002">Petra</author>
<affiliation confidence="0.97586">Institute of Formal and Applied Charles University in Prague, Faculty of Mathematics and</affiliation>
<address confidence="0.845723">Malostransk´e n´amˇest´ı 25, Prague, Czech</address>
<email confidence="0.943641">barancikova@ufal.mff.cuni.cz</email>
<abstract confidence="0.992965526315789">This paper describes Parmesan, our submission to the 2014 Workshop on Statistical Machine Translation (WMT) metrics task for evaluation English-to-Czech translation. We show that the Czech Meteor Paraphrase tables are so noisy that they actually can harm the performance of the metric. However, they can be very useful after extensive filtering in targeted paraphrasing of Czech reference sentences prior to the evaluation. Parmesan first performs targeted paraphrasing of reference sentences, then it computes the Meteor score using only the exact match on these new reference sentences. It shows significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>597--604</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1803" citStr="Bannard and Callison-Burch, 2005" startWordPosition="260" endWordPosition="263">orms traditional metrics like BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) as it explicitly addresses their weaknesses – it takes into account recall, distinguishes between functional and content words, allows language-specific tuning of parameters and many others. Another important advantage of Meteor is that it supports not only exact word matches between a hypothesis and its corresponding reference sentence, but also matches on the level of stems, synonyms and paraphrases. The Meteor Paraphrase tables (Denkowski and Lavie, 2010) were created automatically using the pivot method (Bannard and Callison-Burch, 2005) for six languages. 1We use the the version 1.4., which was recently outdated as the new version 1.5. was released for WMT14 The basic setting of Meteor for evaluation of Czech sentences offers two levels of matches - exact and paraphrase. In this paper, we show the impact of the quality of paraphrases on the performance of Meteor. We demonstrate that the Czech Meteor Paraphrase tables are full of noise and their addition to the metric worsens its correlation with human judgment. However, they can be very useful (after extensive filtering) in creating new reference sentences by targeted paraph</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 597– 604, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petra Barancikov´a</author>
<author>Rudolf Rosa</author>
<author>Ales Tamchyna</author>
</authors>
<title>Improving Evaluation of English-Czech MT through Paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland. European Language Resources Association.</booktitle>
<marker>Barancikov´a, Rosa, Tamchyna, 2014</marker>
<rawString>Petra Baran&amp;quot;cikov´a, Rudolf Rosa, and Ale&amp;quot;s Tamchyna. 2014. Improving Evaluation of English-Czech MT through Paraphrasing. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Milos Ercegovcevi´c</author>
<author>Martin Popel</author>
<author>Omar F Zaidan</author>
</authors>
<title>A Grain of Salt for the WMT Manual Evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Bojar, Ercegovcevi´c, Popel, Zaidan, 2011</marker>
<rawString>Ond&amp;quot;rej Bojar, Milos&amp;quot; Ercegov&amp;quot;cevi´c, Martin Popel, and Omar F. Zaidan. 2011. A Grain of Salt for the WMT Manual Evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 1–11, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Zdenek Zabokrtsk´y</author>
<author>Ondrej Dusek</author>
<author>Petra Galusc´akov´a</author>
<author>Martin Majlis</author>
<author>David Marecek</author>
<author>Michal Nov´ak JiriMarsik</author>
<author>Martin Popel</author>
<author>Ales Tamchyna</author>
</authors>
<date>2012</date>
<booktitle>The Joy of Parallelism with CzEng 1.0. In Proc. of LREC,</booktitle>
<pages>3921--3928</pages>
<publisher>ELRA.</publisher>
<marker>Bojar, Zabokrtsk´y, Dusek, Galusc´akov´a, Majlis, Marecek, JiriMarsik, Popel, Tamchyna, 2012</marker>
<rawString>Ond&amp;quot;rej Bojar, Zden&amp;quot;ek &amp;quot;Zabokrtsk´y, Ond&amp;quot;rej Du&amp;quot;sek, Petra Galu&amp;quot;s&amp;quot;c´akov´a, Martin Majli&amp;quot;s, David Mare&amp;quot;cek, Ji&amp;quot;riMar&amp;quot;sik, Michal Nov´ak, Martin Popel, and Ale&amp;quot;s Tamchyna. 2012. The Joy of Parallelism with CzEng 1.0. In Proc. of LREC, pages 3921–3928. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5134" citStr="Bojar et al., 2013" startWordPosition="811" endWordPosition="814">es and filtering the Czech Meteor tables. We show that the amount of noise in the multiword paraphrases is very high and no automatic filtering method we used outperforms omitting them completely. We present an error analysis based method of filtering paraphrases consisting of pairs of single words, which is used in subsection 3.1. From several methods of paraphrasing, we achieved the best results a with simple greedy method, which is presented in section 4. 3 Data We perform our experiments on data sets from the English-to-Czech translation task of WMT12 (Callison-Burch et al., 2012), WMT13 (Bojar et al., 2013) and WMT14 (Bojar et al., 2014). The data sets contain 13/143/10 files with Czech outputs of MT systems. In addition, each data set contains one file with corresponding reference sentences and one with original English source sentences. We perform morphological analysis and tagging of the hypotheses and the reference sentences using Morˇce (Spoustov´a et al., 2007). The human judgment of hypotheses is available as a relative ranking of performance of five systems for a sentence. We calculated the score for every system by the “&gt; others” method (Bojar et al., 2011), which was the WMT12 official</context>
<context position="15057" citStr="Bojar et al., 2013" startWordPosition="2461" endWordPosition="2464">heat6, v˚udce - leader, vaˇrit - cook, poloostrov - peninsula. For these reasons, the differences among the systems are more blurred and the metric performs worse than without using the paraphrases. 6In all examples the Czech word is the correct translation of the English side. 358 WMT13 references Basic One-word Same POS Same Lemma Diff. Lemma No paraphr. WordNet Original 0.856 0.859 0.862 0.860 0.885 0.883 0.884 Before Depfix 0.894 0.896 0.898 0.897 0.918 0.917 0.917 New 0.918 0.918 0.919 0.919 0.933 0.933 0.933 Table 5: Pearson’s correlation of Meteor and the gold standard – Expected Wins (Bojar et al., 2013). The results corresponds very well with the silver standard in Table 4. frequency Basic No paraphr. 0.837 0.869 0.818 0.852 Table 6: The frequency column shows average number of substitution per sentence using the original Meteor Paraphrase tables only. The rest shows Pearson’s correlation with the silver standard using these paraphrases. We also experimented with paraphrasing using the original Meteor Paraphrase tables for a comparison. We used the same pipeline as it is described in Section 4, but used only original oneword paraphrases from the Meteor Paraphrase tables. Even though the para</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Matous Mach´acek</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2014</date>
<booktitle>Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Mach´acek, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, 2014</marker>
<rawString>Ond&amp;quot;rej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Matous&amp;quot; Mach´a&amp;quot;cek, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, and Lucia Specia. 2014. Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5106" citStr="Callison-Burch et al., 2012" startWordPosition="806" endWordPosition="809">hods of paraphrasing of Czech sentences and filtering the Czech Meteor tables. We show that the amount of noise in the multiword paraphrases is very high and no automatic filtering method we used outperforms omitting them completely. We present an error analysis based method of filtering paraphrases consisting of pairs of single words, which is used in subsection 3.1. From several methods of paraphrasing, we achieved the best results a with simple greedy method, which is presented in section 4. 3 Data We perform our experiments on data sets from the English-to-Czech translation task of WMT12 (Callison-Burch et al., 2012), WMT13 (Bojar et al., 2013) and WMT14 (Bojar et al., 2014). The data sets contain 13/143/10 files with Czech outputs of MT systems. In addition, each data set contains one file with corresponding reference sentences and one with original English source sentences. We perform morphological analysis and tagging of the hypotheses and the reference sentences using Morˇce (Spoustov´a et al., 2007). The human judgment of hypotheses is available as a relative ranking of performance of five systems for a sentence. We calculated the score for every system by the “&gt; others” method (Bojar et al., 2011), </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR-NEXT and the METEOR Paraphrase Tables: Improved Evaluation Support For Five Target Languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation and Metrics MATR.</booktitle>
<contexts>
<context position="1718" citStr="Denkowski and Lavie, 2010" startWordPosition="249" endWordPosition="252">as shown high correlation with human judgment since its appearance. It outperforms traditional metrics like BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) as it explicitly addresses their weaknesses – it takes into account recall, distinguishes between functional and content words, allows language-specific tuning of parameters and many others. Another important advantage of Meteor is that it supports not only exact word matches between a hypothesis and its corresponding reference sentence, but also matches on the level of stems, synonyms and paraphrases. The Meteor Paraphrase tables (Denkowski and Lavie, 2010) were created automatically using the pivot method (Bannard and Callison-Burch, 2005) for six languages. 1We use the the version 1.4., which was recently outdated as the new version 1.5. was released for WMT14 The basic setting of Meteor for evaluation of Czech sentences offers two levels of matches - exact and paraphrase. In this paper, we show the impact of the quality of paraphrases on the performance of Meteor. We demonstrate that the Czech Meteor Paraphrase tables are full of noise and their addition to the metric worsens its correlation with human judgment. However, they can be very usef</context>
<context position="6496" citStr="Denkowski and Lavie, 2010" startWordPosition="1040" endWordPosition="1043"> from the official system scores, which were computed differently each year (here referred to as gold standard). 3We use only 12 of them because two of them (FDA.2878 and online-G) have no human judgments. WMT12 WMT13 WMT14 WordNet 0.26 0.22 0.24 filtered Meteor 1.53 1.29 1.39 together 1.59 1.34 1.44 Table 1: Average number of one-word paraphrases per sentence found in WordNet, filtered Meteor tables and their union over all systems. 3.1 Sources of Paraphrases We use two available sources of Czech paraphrases – the Czech WordNet 1.9 PDT (Pala and Smrˇz, 2004) and the Meteor Paraphrase Tables (Denkowski and Lavie, 2010). The Czech WordNet 1.9 PDT contains paraphrases of high quality, however, their amount is insufficient for our purposes. It contains 13k pairs of synonymous lemmas and only one paraphrase per four sentences on average is found in the data (see Table 1). For that reason, we employ the Czech Meteor Paraphrase tables, too. They are quite the opposite of Czech WordNet – they are large in size, but contain a lot of noise. We attempt to reduce the noise in the Czech Meteor Paraphrase tables in the following way. We keep only pairs consisting of single words since we were not successful in reducing </context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. METEOR-NEXT and the METEOR Paraphrase Tables: Improved Evaluation Support For Five Target Languages. In Proceedings of the ACL 2010 Joint Workshop on Statistical Machine Translation and Metrics MATR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1090" citStr="Denkowski and Lavie, 2011" startWordPosition="152" endWordPosition="155">he Czech Meteor Paraphrase tables are so noisy that they actually can harm the performance of the metric. However, they can be very useful after extensive filtering in targeted paraphrasing of Czech reference sentences prior to the evaluation. Parmesan first performs targeted paraphrasing of reference sentences, then it computes the Meteor score using only the exact match on these new reference sentences. It shows significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data. 1 Introduction The metric for automatic evaluation of machine translation (MT) Meteor1 (Denkowski and Lavie, 2011) has shown high correlation with human judgment since its appearance. It outperforms traditional metrics like BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) as it explicitly addresses their weaknesses – it takes into account recall, distinguishes between functional and content words, allows language-specific tuning of parameters and many others. Another important advantage of Meteor is that it supports not only exact word matches between a hypothesis and its corresponding reference sentence, but also matches on the level of stems, synonyms and paraphrases. The Meteor Paraphrase tables</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Cooccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1255" citStr="Doddington, 2002" startWordPosition="179" endWordPosition="180">ed paraphrasing of Czech reference sentences prior to the evaluation. Parmesan first performs targeted paraphrasing of reference sentences, then it computes the Meteor score using only the exact match on these new reference sentences. It shows significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data. 1 Introduction The metric for automatic evaluation of machine translation (MT) Meteor1 (Denkowski and Lavie, 2011) has shown high correlation with human judgment since its appearance. It outperforms traditional metrics like BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) as it explicitly addresses their weaknesses – it takes into account recall, distinguishes between functional and content words, allows language-specific tuning of parameters and many others. Another important advantage of Meteor is that it supports not only exact word matches between a hypothesis and its corresponding reference sentence, but also matches on the level of stems, synonyms and paraphrases. The Meteor Paraphrase tables (Denkowski and Lavie, 2010) were created automatically using the pivot method (Bannard and Callison-Burch, 2005) for six languages. 1We use the the version 1.4., wh</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Cooccurrence Statistics. In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for Automatic Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06,</booktitle>
<pages>455--462</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3403" citStr="Kauchak and Barzilay (2006)" startWordPosition="525" endWordPosition="528">relation with human judgment and we attempt to tune it further by modifying its paraphrase tables. We show that reducing the size of the Meteor Paraphrase tables is very beneficial. On the WMT12 and WMT13 data, the Meteor scores computed using only the exact match on our new references significantly outperform Meteor with both exact and paraphrase match on original references. However, this result was not confirmed by this year’s data. We perform our experiments on English-toCzech translations, but the method is largely language independent. 2 Related Work Our paraphrasing work is inspired by Kauchak and Barzilay (2006). They are trying to improve the accuracy of MT evaluation of Chinese-to-English translation by targeted paraphrasing, i.e. making a reference closer in wording to a hypothesis (MT output) while keeping its meaning and correctness. 2PARaphrasing for MEteor SANs paraphrases 355 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 355–361, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Having a hypothesis H = h1,..., hn and its corresponding reference translation R = r1, ..., rm, they select a set of candidates C = {(ri, hj)|ri </context>
<context position="7707" citStr="Kauchak and Barzilay (2006)" startWordPosition="1248" endWordPosition="1251">ful in reducing the noise effectively for the multi-word paraphrases (?). Using Morˇce, we first perform morphological analysis of all one-word pairs and replace the word forms with their lemmas. We keep only pairs of different lemmas. Further, we dispose of pairs of words that differ in their parts of speech (POS) or contain an unknown word (typically a foreign word). In this way we have reduced 684k paraphrases in the original Czech Meteor Paraphrase tables to only 32k pairs of lemmas. We refer to this table as filtered Meteor. 4 Creating New References We create new references similarly to Kauchak and Barzilay (2006). Let HL, RL be sets of lemmas from a hypothesis and a corresponding reference sentence, respectively. Then we select candidates for paraphrasing in the following way: CL = {(r, h)|r E RL � HL, h E HL � RL, rPOS = hPOS}, where rPOS and hPOS denote the part of speech of the respective lemma. Further, we restrict the set CL to pairs appearing in our paraphrase tables only. If a word has several 356 Source The location alone is classic. Hypothesis Samotn´e misto je klasick´e . Actual placeneut is classicneut . The place alone is classic. Reference Uˇz poloha je klasick´a . Already positionfem is </context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, pages 455–462, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Li Meng</author>
<author>Robert Rosenthal</author>
<author>Donald B Rubin</author>
</authors>
<title>Comparing correlated correlation coefficients.</title>
<date>1992</date>
<journal>Psychological bulletin,</journal>
<volume>111</volume>
<issue>1</issue>
<contexts>
<context position="13059" citStr="Meng et al. (1992)" startWordPosition="2139" endWordPosition="2142">ndard. Parmesan scores are highlighted by the box and the best scores are in bold. 7 Results 7.1 WMT12 and WMT13 The results of our experiments are presented in Table 44 as Pearson’s correlation coefficient of the Meteor scores and the human judgment. The results in both tables are very consistent. There is a clear positive impact of the prior paraphrasing of the reference sentences and of applying Depfix. The results also show that independently of a reference sentence used, reducing the Meteor paraphrase tables in evaluation is always beneficial. We use a freely available implementation5 of Meng et al. (1992) to determine whether the difference in correlation coefficients is statistically significant. The tests show that Parmesan performs better than original Meteor with 99% certainty on the data from WMT12 and WMT13. Diff. Lemma and WordNet settings give the best results on the original reference sentences. That is because they are basically a limited version 4The results of WMT13 using the gold standard are in Table 5. 5http://www.cnts.ua.ac.be/∼vincent/scripts/rtest.py of the paraphrase tables we use for creating our new references, which contain both all different lemmas of the same part of sp</context>
</contexts>
<marker>Meng, Rosenthal, Rubin, 1992</marker>
<rawString>Xiao-Li Meng, Robert Rosenthal, and Donald B Rubin. 1992. Comparing correlated correlation coefficients. Psychological bulletin, 111(1):172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>COMMUNICATIONS OF THEACM,</journal>
<pages>38--39</pages>
<contexts>
<context position="4100" citStr="Miller, 1995" startWordPosition="639" endWordPosition="640">nslation by targeted paraphrasing, i.e. making a reference closer in wording to a hypothesis (MT output) while keeping its meaning and correctness. 2PARaphrasing for MEteor SANs paraphrases 355 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 355–361, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Having a hypothesis H = h1,..., hn and its corresponding reference translation R = r1, ..., rm, they select a set of candidates C = {(ri, hj)|ri E R \ H, hj E H \ R}. C is reduced to pairs of words appearing in the same WordNet (Miller, 1995) synset only. For every pair (ri, hj) E C, hj is evaluated in the context r1, ..., ri−1, ❑, ri+1, ..., rm and if confirmed, the new reference sentence r1, ...,ri−1, hj, ri+1, ..., rm is created. This way, several reference sentences might be created, all with a single changed word with respect to the original one. In Baranˇcikov´a et al. (2014), we experiment with several methods of paraphrasing of Czech sentences and filtering the Czech Meteor tables. We show that the amount of noise in the multiword paraphrases is very high and no automatic filtering method we used outperforms omitting them </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. COMMUNICATIONS OF THEACM, 38:39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karel Pala</author>
<author>Pavel Smrz</author>
</authors>
<title>Building Czech WordNet.</title>
<date>2004</date>
<booktitle>In Romanian Journal of Information Science and Technology,</booktitle>
<pages>7--79</pages>
<marker>Pala, Smrz, 2004</marker>
<rawString>Karel Pala and Pavel Smr&amp;quot;z. 2004. Building Czech WordNet. In Romanian Journal of Information Science and Technology, 7:79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1228" citStr="Papineni et al., 2002" startWordPosition="173" endWordPosition="176">er extensive filtering in targeted paraphrasing of Czech reference sentences prior to the evaluation. Parmesan first performs targeted paraphrasing of reference sentences, then it computes the Meteor score using only the exact match on these new reference sentences. It shows significantly higher correlation with human judgment than Meteor on the WMT12 and WMT13 data. 1 Introduction The metric for automatic evaluation of machine translation (MT) Meteor1 (Denkowski and Lavie, 2011) has shown high correlation with human judgment since its appearance. It outperforms traditional metrics like BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) as it explicitly addresses their weaknesses – it takes into account recall, distinguishes between functional and content words, allows language-specific tuning of parameters and many others. Another important advantage of Meteor is that it supports not only exact word matches between a hypothesis and its corresponding reference sentence, but also matches on the level of stems, synonyms and paraphrases. The Meteor Paraphrase tables (Denkowski and Lavie, 2010) were created automatically using the pivot method (Bannard and Callison-Burch, 2005) for six languages. 1We u</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolf Rosa</author>
<author>David Marecek</author>
<author>Ondrej Dusek</author>
</authors>
<title>DEPFIX: A System for Automatic Correction of Czech MT Outputs.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12,</booktitle>
<pages>362--368</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2599" citStr="Rosa et al., 2012" startWordPosition="392" endWordPosition="395">entences offers two levels of matches - exact and paraphrase. In this paper, we show the impact of the quality of paraphrases on the performance of Meteor. We demonstrate that the Czech Meteor Paraphrase tables are full of noise and their addition to the metric worsens its correlation with human judgment. However, they can be very useful (after extensive filtering) in creating new reference sentences by targeted paraphrasing. Parmesan2 starts with a simple greedy algorithm for substitution of synonymous words from a hypothesis in its corresponding reference sentence. Further, we apply Depfix (Rosa et al., 2012) to fix grammar errors that might arise by the substitutions. Our method is independent of the evaluation metric used. In this paper, we use Meteor for its consistently high correlation with human judgment and we attempt to tune it further by modifying its paraphrase tables. We show that reducing the size of the Meteor Paraphrase tables is very beneficial. On the WMT12 and WMT13 data, the Meteor scores computed using only the exact match on our new references significantly outperform Meteor with both exact and paraphrase match on original references. However, this result was not confirmed by t</context>
<context position="9865" citStr="Rosa et al., 2012" startWordPosition="1616" endWordPosition="1619">n better in both WordNet and filtered Meteor. We proceed word by word from the beginning of the reference sentence to its end. If a lemma of a word appears as the first member of a pair in restricted CL, it is replaced by the word form from hypothesis that has its lemma as the second element of that pair, i.e., by the paraphrase from the hypothesis. Otherwise, the original word the reference sentence is kept. When integrating paraphrases to the reference sentence, it may happen that the sentence becomes ungrammatical, e.g., due to a broken agreement (see Figure 1). Therefore, we apply Depfix (Rosa et al., 2012) – a system for automatic correction of grammatical errors that appear often in Englishto-Czech MT outputs. Depfix analyses the input sentences using a range of natural language processing tools. It fixes errors using a set of linguistically-motivated rules and a statistical component it contains. 5 Choosing a metric Our next step is choosing a metric that correlates well with human judgment. We experiment with three common metrics – BLEU, Meteor and TER. Based on the results (see Table 2), we decided to employ Meteor in WMT14 as our metric because it shows consistently highest correlations. 6</context>
</contexts>
<marker>Rosa, Marecek, Dusek, 2012</marker>
<rawString>Rudolf Rosa, David Mare&amp;quot;cek, and Ond&amp;quot;rej Du&amp;quot;sek. 2012. DEPFIX: A System for Automatic Correction of Czech MT Outputs. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ’12, pages 362–368, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira Spoustov´a</author>
<author>Jan Hajic</author>
<author>Jan Votrubec</author>
<author>Pavel Krbec</author>
<author>Pavel Kveton</author>
</authors>
<title>The Best of Two Worlds: Cooperation of Statistical and Rule-Based Taggers for Czech.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL</booktitle>
<pages>67--74</pages>
<location>Praha.</location>
<marker>Spoustov´a, Hajic, Votrubec, Krbec, Kveton, 2007</marker>
<rawString>Drahomira Spoustov´a, Jan Haji&amp;quot;c, Jan Votrubec, Pavel Krbec, and Pavel Kv&amp;quot;eto&amp;quot;n. 2007. The Best of Two Worlds: Cooperation of Statistical and Rule-Based Taggers for Czech. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing, ACL 2007, pages 67–74, Praha.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
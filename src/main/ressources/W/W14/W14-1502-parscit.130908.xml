<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003421">
<title confidence="0.99209">
Distributional Composition using Higher-Order Dependency Vectors
</title>
<author confidence="0.999554">
Julie Weeds, David Weir and Jeremy Reffin
</author>
<affiliation confidence="0.995611">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.99405">
Brighton, BN1 9QH, UK
</address>
<email confidence="0.998539">
{J.E.Weeds, D.J.Weir, J.P.Reffin}@sussex.ac.uk
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997957">
This paper concerns how to apply compo-
sitional methods to vectors based on gram-
matical dependency relation vectors. We
demonstrate the potential of a novel ap-
proach which uses higher-order grammat-
ical dependency relations as features. We
apply the approach to adjective-noun com-
pounds with promising results in the pre-
diction of the vectors for (held-out) ob-
served phrases.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999902951612903">
Vector space models of semantics characterise the
meaning of a word in terms of distributional fea-
tures derived from word co-occurrences. The most
widely adopted basis for word co-occurrence is
proximity, i.e. that two words (or more generally
lexemes) are taken to co-occur when they occur
together within a certain sized window, or within
the same sentence, paragraph, or document. Lin
(1998), in contrast, took the syntactic relationship
between co-occurring words into account: the dis-
tributional features of a word are based on the
word’s grammatical dependents as found in a de-
pendency parsed corpus. For example, observing
that the word glass appears as the indirect object
of the verb fill, provides evidence that the word
glass has the distributional feature iobj:fill, where
iobj denotes the inverse indirect object grammati-
cal relation. The use of grammatical dependents as
word features has been exploited in the discovery
of tight semantic relations, such as synonymy and
hypernymy, where an evaluation against a gold
standard such as WordNet (Fellbaum, 1998) can
be made (Lin, 1998; Weeds and Weir, 2003; Cur-
ran, 2004).
Pado and Lapata (2007) took this further by
considering not just direct grammatical depen-
dents, but also including indirect dependents.
Thus, observing the sentence She filled her glass
slowly would provide evidence that the word glass
has the distributional feature iobj:advmod:slowly
where iobj:advmod captures the indirect depen-
dency relationship between glass and slowly in the
sentence.
Note that Pado and Lapata (2007) included
a basis mapping function that gave their frame-
work flexibility as to how to map paths such as
iobj:advmod:slowly onto the basis of the vector
space. Indeed, the instantiation of their framework
that they adopt in their experiments uses a ba-
sis mapping function that removes the dependency
path to leave just the word, so iobj:advmod:slowly
would be mapped to slowly.
In this paper, we are concerned with the prob-
lem of distributional semantic composition. We
show that the idea that the distributional seman-
tics of a word can be captured with higher-order
dependency relationships, provides the basis for
a simple approach to compositional distributional
semantics. While our approach is quite gen-
eral, dealing with arbitrarily high-order depen-
dency relationships, and the composition of ar-
bitrary phrases, in this paper we consider only
first and second order dependency relations, and
adjective-noun composition.
In Section 2, we illustrate our proposal by
showing how second order dependency relations
can play a role in computing the semantics of
adjective-noun composition. In Section 3 we de-
scribe a number of experiments that are intended
to evaluate the approach, with the results presented
in Section 4.
The basis for our evaluation follows Baroni and
</bodyText>
<page confidence="0.992756">
11
</page>
<note confidence="0.997314">
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11–20,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997460892857143">
Zamparelli (2010) and Guevara (2010). Typically,
compositional distributional semantic models can
be used to generate an (inferred) distributional
vector for a phrase from the (observed) distribu-
tional vectors of the phrase’s constituents. One
of the motivations for doing this is that the ob-
served distributional vectors for most phrases tend
to be very sparse, a consequence of the frequency
with which typical phrases occur in even large cor-
pora. However, there are phrases that occur suffi-
ciently frequently that a reasonable characterisa-
tion of their meaning can be captured with their
observed distributional vector. Such phrases can
be exploited in order to assess the quality of a
model of composition. This is achieved by mea-
suring the distributional similarity of the observed
and inferred distributional vectors for these high
frequency phrases.
The contributions of this paper are as follows.
We propose a novel approach to phrasal composi-
tion which uses higher order grammatical depen-
dency relations as features. We demonstrate its
potential in the context of adjective-noun compo-
sition by comparing (held-out) observed and in-
ferred phrasal vectors. Further, we compare dif-
ferent vector operations, different feature associa-
tion scores and investigate the effect of weighting
features before or after composition.
</bodyText>
<sectionHeader confidence="0.7822555" genericHeader="general terms">
2 Composition with Higher-order
Dependencies
</sectionHeader>
<bodyText confidence="0.999942222222222">
Consider the problem of adjective-noun compo-
sition. For example, what is the meaning of the
phrase small child? How does it relate to the
meanings of the lexemes small and child? Figure 1
shows a dependency analysis for the sentence The
very small wet child cried loudly. Tables 1 and
2 show the grammatical dependencies (with other
open-class words) for the lexemes small and child
which would be extracted from it.
</bodyText>
<figure confidence="0.400404">
the/D very/R small/J wet/J child/N cry/V loudly/R
</figure>
<figureCaption confidence="0.999466">
Figure 1: Example Dependency Tree
</figureCaption>
<bodyText confidence="0.99928225">
From Table 1 we see what kinds of (higher-
order) dependency paths appear in the distribu-
tional features of adjectives such as small. Simi-
larly, Table 2 indicates this for nouns such as child.
</bodyText>
<table confidence="0.530259428571428">
1st-order advmod:very/R
amod:child
2nd-order
amod:amod:wet/J
amod:nsubj:cry/V
3rd-order
amod:nsubj:advmod:loudly/R
</table>
<tableCaption confidence="0.990013">
Table 1: Grammatical Dependencies of small
</tableCaption>
<table confidence="0.960292">
1st-order amod:wet/J
amod:small/J
nsubj:cry/V
2nd-order amod:advmod:very/R
nsubj:advmod:loudly/R
</table>
<tableCaption confidence="0.996779">
Table 2: Grammatical Dependencies of child
</tableCaption>
<bodyText confidence="0.99996124137931">
It is clear that with a conventional grammatical
dependency-based approach where only first or-
der dependencies for small and child would be
considered, there will be very little overlap be-
tween the features of nouns and adjectives because
quite different grammatical relations are used in
the two types of vectors, and correspondingly lex-
emes with different parts of speech appear at the
end of these paths.
However, as our example illustrates, it is possi-
ble to align the 2nd-order feature space of adjec-
tives with the 1st-order feature space of nouns. In
this example, we have evidence that children cry
and that small things cry. Consequently, in order
to compose an adjective with a noun, we would
want to align 2nd-order features of the adjective
with 1st-order features of the noun; this gives us a
prediction of the first order features of the noun in
the context of the adjective1.
This idea extends in a straightforward way be-
yond adjective-noun composition. For example, it
is possible to align the 3rd order features of ad-
jectives with 2nd order features of nouns, which is
something that would be useful if one wanted to
compose verbs with their arguments. These argu-
ments will include adjective-noun compounds and
therefore adjective-noun compounds require 2nd-
order features which can be aligned with the first
order features of the verbs. This is, however, not
</bodyText>
<footnote confidence="0.95310325">
1Note that it would also be possible to align 2nd-order
features of the noun with 1st-order features of the adjective,
resulting in a prediction of the first order features of the ad-
jective in the context of the noun.
</footnote>
<figure confidence="0.8736814">
advmod
det
amod
amod
nsubj advmod
</figure>
<page confidence="0.990875">
12
</page>
<bodyText confidence="0.995411551020408">
something that we will pursue further in this paper.
We now clarify how features vectors are aligned
and then composed. Suppose that the lexemes w1
and w2 which we wish to compose are connected
by relation r. Let w1 be the head of the relation
and w2 be the dependent. In our example, w1 is
child, w2 is small and r is amod. We first pro-
duce a reduced vector for w2 which is designed
to lie in a comparable feature space as the vector
for w1. To do this we take the set of 2nd order
features of w2 which start with the relation r¯ and
reduce them to first order features (by removing
the r¯ at the start of the path). So in our example,
we create a reduced vector for small where fea-
tures amod:nsubj:x for some token x are reduced
to nsubj:x, features amod:amod:x for some token
x are reduced to the feature amod:x, and features
amod:nsubj:advmod:x for some token x are re-
duced to nsubj:advmod:x. Once the vector for w2
has been reduced, it can be composed with the vec-
tor for w1 using standard vector operations.
In Section 3 we describe experiments that ex-
plore the effectiveness of this approach to distri-
butional composition by measuring the similarity
of composed vectors with observed vectors for a
set of frequently occurring adjective-noun pairs
(details given below). We evaluate a number of
instantiations of our approach, and in particular,
there are three aspects of the model where alter-
native solutions are available: the choice of which
vector composition operation to use; the choice of
how to weight dependency features; and the ques-
tion as to whether feature weighting should take
place before or after composition.
Vector composition operation. We consider
each of the following seven alternatives: pointwise
addition (add), pointwise multiplication (mult),
pointwise geometric mean2 (gm), pointwise max-
imum (max), pointwise minimum (min), first ar-
gument (hd), second argument (dp). The latter
two operations simply return the first (respectively
second) of the input vectors.
Feature weighting. We consider three options.
Much work in this area has used positive pointwise
mutual information (PPMI) (Church and Hanks,
1989) to weight the features. However, PPMI is
known to over-emphasise low frequency events,
and as a result there has been a recent shift to-
wards using positive localised mutual information
</bodyText>
<equation confidence="0.937135761904762">
�
2The geometric mean of x and y is (x · y).
�
I(x, y) if I(x, y) &gt; 0
PPMI(x, y) =
0 otherwise
where I(x, y) = log P(x,y)
P (x).P (y)
( L(x, y) if L(x, y) &gt; 0
PLMI(x, y) =
0 otherwise
where L(x, y) = P(x, y).log( P (x,y)
P (x).P (y)
�
PNPMI(x, y)
N(x, y) if N(x, y) &gt; 0
=
0 otherwise
where N(x, y) = −log(P(y).log P(x,y)
1
P (x).P(y)
</equation>
<tableCaption confidence="0.852876">
Table 3: Feature Association Scores
</tableCaption>
<bodyText confidence="0.999692777777778">
(PLMI) (Scheible et al., 2013) and positive nor-
malised point wise mutual information (PNPMI)
(Bouma, 2009). For definitions, see Table 3.
Timing of feature weighting. We consider two
alternatives: we can weight features before com-
position so that the composition operation is ap-
plied to weighted vectors, or we can compose vec-
tors prior to feature weighting, in which case the
composition operation is applied to unweighted
vectors, and feature weighting is applied in the
context of making a similarity calculation. In other
work, the former order is often implied. For exam-
ple, Boleda et al. (2013) state that they use “PMI
to weight the co-occurrence matrix”. However, if
we allow the second order, features which might
have a zero association score in the context of the
the individual lexemes, could be considered sig-
nificant in the context of the phrase.
</bodyText>
<sectionHeader confidence="0.997624" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999964923076923">
Our experimental evaluation of the approach is
based on the assumption, which is commonly
made elsewhere, that where there is a reasonable
amount of corpus data available for a phrase, this
will generate a good estimate of the vector of the
phrase. It has been shown (Turney, 2012; Baroni
and Zamparelli, 2010) that such “observed” vec-
tors are indeed reasonable for adjective-noun and
noun-noun compounds. Hence, in order to evalu-
ate the compositional models under consideration
here, we compare observed phrasal vectors with
inferred phrasal vectors, where the comparison is
made using the cosine measure. We note that it is
</bodyText>
<page confidence="0.994504">
13
</page>
<bodyText confidence="0.999928">
not possible to draw conclusions from the absolute
value of the cosine score since this would favour
models which always assign higher cosine scores.
Hence, we draw conclusions from the change in
cosine score with respect to a baseline within the
same model.
</bodyText>
<sectionHeader confidence="0.345271" genericHeader="method">
Methodology
</sectionHeader>
<bodyText confidence="0.999968658227848">
For each noun and adjective which occur more
than a threshold number of times in a corpus, we
first extract conventional first order dependency
vectors. The features of these lexemes define the
semantic space, and feature probabilities (for use
in association scores) are calculated from this data.
Given a list of adjective-noun phrases, we ex-
tract first order vectors for the nouns and second
order vectors for the adjectives, which we refer to
as observed constituent vectors. We also extract
first order vectors for the nouns in the context of
the adjective, which we refer to as the observed
phrasal vector.
For each adjective-noun pair, we build bespoke
constituent vectors for the adjective and noun, in
which we remove all counts which arise from co-
occurrences with that specific adjective-noun pair.
It is these constituent vectors that are used as the
basis for inferring the vector for that particular
adjective-noun phrase.
Our rationale for this is as follows. Without this
modification, the observed constituent vectors will
contain co-occurrences which are due to the ob-
served adjective-noun vector co-occurrences. To
see why this is undesirable, suppose that one of the
adjective-noun phrases was small child. We take
the observed vector for small child to be what we
are calling the observed phrasal vector for child (in
the context of small). Suppose that when building
the observed phrasal vector, we observe the phrase
the small child cried. This will lead to a count for
the feature nsubj:cry in the observed phrasal vec-
tor for child.
But if we are not careful, this same phrase will
contribute to counts in the constituent vectors for
small and child, producing counts for the features
amod:nsubj:cry and nsubj:cry, in their respective
vectors. To see why these counts should not be in-
cluded when building the constituent vectors that
we compose to produce inferred vectors for the
adjective-noun phrase small child, consider the
case where all of the evidence for small things be-
ing things that can cry and children being things
that can crying comes from having observed the
phrase small children crying. Despite not having
learnt anything about the composition of small and
child in general, we would be able to infer the cry
feature for the phrase. An adequate model of com-
position should be able to infer this on the basis
that other small things have been seen to cry, and
that non-small children have been seen to cry.
Here, we compare the proposed approach,
based on higher order dependencies, with the
standard method of composing conventional first-
order dependency vectors. The vector operation,
hd provides a baseline for comparison which is
the same in both approaches. This baseline corre-
sponds to a composition model where the first or-
der dependencies of the phrase (i.e. the noun in the
context of the adjective) are taken to be the same
as the first order dependencies of the uncontextu-
alized noun. For example, if we have never seen
the phrase small child before, we would assume
that it means the same as the head word child.
We hypothesise that it is not possible to im-
prove on this baseline using traditional first-order
dependency relation vectors, since the vector for
the modifier does not contain features of the right
type, but that with the proposed approach, the in-
ferred vector for a phrase such as small child will
be closer than observed vector for child to the ob-
served vector for small child. We also ask the re-
lated question of whether our inferred vector for
small child is closer than the constituent vector for
small to the observed vector for small child. This
comparison is achieved through use of the vector
operation dp that ignores the vector for the head,
simply returning a first-order vector derived from
the dependent.
</bodyText>
<subsectionHeader confidence="0.97724">
Experimental Settings
</subsectionHeader>
<bodyText confidence="0.998849923076923">
Our corpus is a mid-2011 dump of WikiPedia.
This has been part-of-speech tagged, lemmatised
and dependency parsed using the Malt Parser
(Nivre, 2004). All major grammatical dependency
relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, advmod, nnmod)
have been extracted for all POS-tagged and lem-
matised nouns and adjectives occurring 100 or
more times. In past work with conventional de-
pendency relation vectors we found that using a
feature threshold of 100, weighting features with
PPMI and a cosine similarity score work well.
For experimental purposes, we have taken
</bodyText>
<page confidence="0.993002">
14
</page>
<table confidence="0.99979025">
spanish british african japanese
modern classical female natural
digital military medical musical
scientific free black white
heavy common small large
strong short long good
similar previous future original
former subsequent next possible
</table>
<tableCaption confidence="0.999645">
Table 4: Adjectives considered
</tableCaption>
<bodyText confidence="0.999734066666667">
32 of the most frequently occurring adjectives
(see Table 4). These adjectives include ones
which would generally be considered intersective
(e.g., female), subsective (e.g,, long) and non-
subsective/intensional (e.g., former) (Pustejovsky,
2013) . For all of these adjectives there are at least
100 adjective-noun phrases which occur at least
100 times in the corpus. We randomly selected
50 of the phrases for each adjective. Note that
our proposed method does not require any hyper
parameters to be set during training, nor does it
require a certain number of phrases per adjective.
For the purpose of these experiments we have a list
of 1600 adjective-noun phrases, all of which occur
at least 100 times in WikiPedia.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999886608108108">
Tables 5 and 6 summarise the average cosines for
the proposed higher-order dependency approach
and the conventional first-order dependency ap-
proach, respectively. In each case, we consider
each combination of vector operation, feature as-
sociation score, and composition timing (i.e. be-
fore, or after, vector weighting).
Table 7 shows the average improvement over
the baseline (hd), for each combination of exper-
imental variables, when considering the proposed
higher-order dependency approach. Note that this
is an average of paired differences (and not the dif-
ference of the averages in Table 6). For brevity, we
omit the results for PNPMI here, since there do not
appear to be substantial differences between using
PPMI and PNPMI. To indicate statistical signifi-
cance, we show estimated standard errors in the
means. All differences are statistically significant
(under a paired t-test) except those marked †.
From Table 5, we see that none of the com-
positional operations on conventional dependency
vectors are able to beat the baseline of selecting
the head vector (hd). This is independent of the
choice of association measure and the order in
which weighting and composition are carried out.
For the higher order dependency vectors (Tables
6 and 7), we note, in contrast, that some com-
positional operations produce large increases in
cosine score compared to the head vector alone
(hd). Table 7 examines the statistical significance
of these differences. We find that for the inter-
sective composition operations (mult, min, and
gm), performance is statistically superior to using
the head alone in all experimental conditions stud-
ied. By contrast, additive measures (add, max)
typically have no impact, or decrease performance
marginally relative to the head alone. An explana-
tion for these significant differences is that inter-
sective vector operations are able to encapsulate
the way that an adjective disambiguates and spe-
cialises the sense of the noun that it is modifying.
We also note that the alternative baseline, dp,
which estimates the features of a phrase to be the
aggregation of all things which are modified by
the adjective, performs significantly worse than
the standard baseline, hd, which estimates the fea-
tures of a phrase to be the features of the head
noun. This is consistent with the intuition that the
distributional vector for small child should more
similar to the vector for child than it is to the vec-
tor for the things that can be small.
Considering the different intersective opera-
tions, mult appears to be the best choice when
the feature association score is PPMI or PNPMI
and gm appears to be the best choice when the fea-
ture association score is PLMI.
Further, PLMI consistently gives all of the vec-
tor pairings higher cosine scores than PPMI. Since
PLMI assigns less weight to low frequency event
and more weight to high frequency events, this
suggests that all of the composition methods, in-
cluding the baseline (hd), do better at predicting
the high frequency co-occurrences. This is not sur-
prising as these will more likely have been seen
with the phrasal constituents in other contexts.
Our final observation, based on Table 6, is that
the best order in which to carry out weighting and
composition appears to depend on the choice of
feature association score. In general, it appears
better to weight the features and then compose
vectors. This is always true when using PNPMI
or PLMI. However, using PPMI, the highest per-
formance is achieved by composing the raw vec-
tors using multiplication and then weighing the
</bodyText>
<page confidence="0.99602">
15
</page>
<table confidence="0.999297111111111">
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x S x S x S x S x S x S
add 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
max 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
mult 0.06 (0.05) 0.06 (0.06) 0.06 (0.11) 0.07 (0.05) 0.07 (0.12) 0.07 (0.05)
min 0.05 (0.05) 0.06 (0.05) 0.04 (0.09) 0.05 (0.04) 0.05 (0.04) 0.04 (0.08)
gm 0.06 (0.05) 0.06 (0.05) 0.07 (0.11) 0.05 (0.04) 0.06 (0.04) 0.08 (0.11)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
</table>
<tableCaption confidence="0.863305">
Table 5: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Con-
ventional First-Order Dependency Based Approach.
</tableCaption>
<table confidence="0.9995634">
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x S x S x S x S x S x S
add 0.14 (0.06) 0.16 (0.06) 0.29 (0.21) 0.10 (0.04) 0.12 (0.05) 0.29 (0.22)
max 0.10 (0.04) 0.11 (0.04) 0.27 (0.21) 0.10 (0.04) 0.11 (0.04) 0.26 (0.21)
mult 0.30 (0.12) 0.33 (0.12) 0.40 (0.29) 0.34 (0.10) 0.32 (0.10) 0.32 (0.27)
min 0.26 (0.11) 0.27 (0.11) 0.40 (0.24) 0.24 (0.10) 0.25 (0.10) 0.37 (0.23)
gm 0.27 (0.11) 0.29 (0.11) 0.46 (0.20) 0.26 (0.10) 0.27 (0.10) 0.44 (0.22)
dp 0.10 (0.05) 0.10 (0.05) 0.20 (0.20) 0.10 (0.05) 0.10 (0.05) 0.20 (0.20)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
</table>
<tableCaption confidence="0.9755595">
Table 6: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Pro-
posed Higher-Order Dependency Based Approach
</tableCaption>
<bodyText confidence="0.999818208333333">
remaining features. This can be explained by
considering the recall and precision of the com-
posed vector’s prediction of the observed vec-
tor. If we compose using gm before weighting
vectors, we increase the recall of the prediction,
but decrease precision. Whether we use PPMI,
PNPMI or PLMI, recall of features increases from
88.8% to 99.5% and precision drops from 5.5% to
4.8%. If we compose using mult before weight-
ing vectors, contrary to expectation, recall de-
creases and precision increases. Whether we use
PPMI, PNPMI or PLMI, recall of features de-
creases from 88.8% to 59.4% but precision in-
creases from 5.5% to 18.9%. Hence, multiplica-
tion of the raw vectors is causing a lot of potential
shared features to be “lost” when the weighting
is subsequently carried out (since multiplication
stretches out the value space). This leads to an
increase in cosines when PPMI is used for weight-
ing, and a decrease in cosines when PLMI is used.
Hence, it appears that the features being removed
by multiplying the raw vectors before weighting
must be low frequency co-occurrences, which are
not observed with the phrase.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999964304347826">
In this work, we bring together ideas from sev-
eral different strands of distributional semantics:
incorporating syntactic information into the distri-
butional representation of a lexeme; representing
phrasal meaning by creating distributional repre-
sentations through composition; and representing
word meaning in context by modifying the distri-
butional representation of a word.
The use of syntactic structure in distributional
representations is not new. Two of the earliest
proponents of distributional semantics, Lin (1998)
and Lee (1999) used features based on first order
dependency relations between words in their dis-
tributional representations. More recently, Pado
and Lapata (2007) propose a semantic space based
on dependency paths. This model outperformed
traditional word-based models which do not take
syntax into account in a synonymy relation detec-
tion task and a prevalent sense acquisition task.
The problem of representing phrasal meaning
has traditionally been tackled by taking vector rep-
resentations for words (Turney and Pantel, 2010)
and combining them using some function to pro-
</bodyText>
<page confidence="0.996038">
16
</page>
<table confidence="0.999466111111111">
weight:compose compose:weight
PPMI PLMI PPMI PLMI
X S¯x x S¯x x S¯x x S¯x
add 0.01 (0.001) †0.004 (0.003) -0.03 (0.001) †0.006 (0.004)
max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003)
mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006)
min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007)
gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005)
dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007)
</table>
<tableCaption confidence="0.965616">
Table 7: Means and Standard Errors for Increases in Cosine with respect to the hd Baseline for Proposed
Higher-Order Dependency Based Approach. All differences statistically significant (under a paired t-
test) except those marked †.
</tableCaption>
<bodyText confidence="0.999648357142857">
duce a data structure that represents the phrase
or sentence. Mitchell and Lapata (2008, 2010)
found that simple additive and multiplicative func-
tions applied to proximity-based vector represen-
tations were no less effective than more com-
plex functions when performance was assessed
against human similarity judgements of simple
paired phrases.
The simple functions evaluated by Mitchell and
Lapata (2008) are generally acknowledged to have
serious theoretical limitations in their treatment
of composition. How can a commutative func-
tion such as multiplication or addition provide dif-
ferent interpretations for different word orderings
such as window glass and glass window? The
majority of attempts to rectify this have offered
a more complex, non-commutative function —
such as weighted addition — or taken the view
that some or all words are no longer simple vec-
tors. For example, in the work of Baroni and
Zamparelli (2010) and Guevara (2010), an adjec-
tive is viewed as a modifying function and rep-
resented by a matrix. Coecke et al. (2011) and
Grefenstette et al. (2013) also incorporate the no-
tion of function application from formal seman-
tics. They derived function application from syn-
tactic structure, representing functions as tensors
and arguments as vectors. The MV-RNN model
of Socher et al. (2012) broadened the Baroni and
Zamparelli (2010) approach; all words, regardless
of part-of-speech, were modelled with both a vec-
tor and a matrix. This approach also shared fea-
tures with Coecke et al. (2011) in using syntax
to guide the order of phrasal composition. These
higher order structures are typically learnt or in-
duced using a supervised machine learning tech-
nique. For example, Baroni and Zamparelli (2010)
learnt their adjectival matrixes by performing re-
gression analysis over pairs of observed nouns and
adjective-noun phrases. As a consequence of the
computational expense of the machine learning
techniques involved, implementations of these ap-
proaches typically require a considerable amount
of dimensionality reduction.
A long-standing topic in distributional seman-
tics has been the modification of a canonical repre-
sentation of a lexeme’s meaning to reflect the con-
text in which it is found. Typically, a canonical
vector for a lexeme is estimated from all corpus
occurrences and the vector then modified to reflect
the instance context (Lund and Burgess, 1996;
Erk and Pad´o, 2008; Mitchell and Lapata, 2008;
Thater et al., 2009; Thater et al., 2010; Thater et
al., 2011; Van de Cruys et al., 2011; Erk, 2012).
As described in Mitchell and Lapata (2008, 2010),
lexeme vectors have typically been modified using
simple additive and multiplicative compositional
functions. Other approaches, however, share with
our proposal the use of syntax to drive modifica-
tion of the distributional representation (Erk and
Pad´o, 2008; Thater et al., 2009; Thater et al., 2010;
Thater et al., 2011). For example, in the SVS rep-
resentation of Erk and Pad´o (2008), a word was
represented by a set of vectors: one which en-
codes its lexical meaning in terms of distribution-
ally similar words3, and one which encodes the
selectional preferences of each grammatical rela-
tion it supports. A word’s meaning vector was up-
dated in the context of another word by combining
it with the appropriate selectional preferences vec-
</bodyText>
<footnote confidence="0.9952374">
3These are referred to as second-order vectors using
the terminology of Grefenstette (1994) and Sch¨utze (1998).
However, this refers to a second-order affinity between the
words and is not related to the use of grammatical depen-
dency relations.
</footnote>
<page confidence="0.999296">
17
</page>
<bodyText confidence="0.999893625">
tor of the contextualising word.
Turney (2012) offered a model of phrasal level
similarity which combines assessments of word-
level semantic relations. This work used two
different word-level distributional representations
to encapsulate two types of similarity. Distribu-
tional similarity calculated from proximity-based
features was used to estimate domain similarity
and distributional similarity calculated from syn-
tactic pattern based features is used to estimate
functional similarity. The similarity of a pair of
compound noun phrases was computed as a func-
tion of the similarities of the components. Cru-
cially different from other models of phrasal level
similarity, it does not attempt to derive modified
vectors for phrases or words in context.
</bodyText>
<sectionHeader confidence="0.988749" genericHeader="conclusions">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999971038461539">
Vectors based on grammatical dependency rela-
tions are known to be useful in the discovery of
tight semantic relations, such as synonymy and
hypernymy, between lexemes (Lin, 1998; Weeds
and Weir, 2003; Curran, 2004). It would be use-
ful to be able to extend these methods to deter-
mine similarity between phrases (of potentially
different lengths). However, conventional ap-
proaches to composition, which have been ap-
plied to proximity-based vectors, cannot sensibly
be used on vectors that are based on grammatical
dependency relations.
In our approach, we consider the vector for a
phrase to be the vector for the head lexeme in
the context of the other phrasal constituents. Like
Pado and Lapata (2007), we extend the concept
of a grammatical dependency relation feature to
include dependency relation paths which incor-
porate higher-order dependencies between words.
We have shown how it is possible to align the de-
pendency path features for words of different syn-
tactic types, and thus produce composed vectors
which predict the features of one constituent in the
context of the other constituent.
In our experiments with AN compounds, we
have shown that these predicted vectors are closer
than the head constituent’s vector to the observed
phrasal vector. We have shown this is true even
when the observed phrase is in fact unobserved,
i.e. when its co-occurrences do not contribute to
the constituents’ vectors. Consistent with work us-
ing proximity-based vectors, we have found that
intersective operations perform substantially bet-
ter than additive operations. This can be under-
stood by viewing the intersective operations as en-
capsulating the way that adjectives can specialise
the meaning of the nouns that they modify.
We have investigated the interaction between
the vector operation used for composition, the fea-
ture association score and the timing of applying
feature weights. We have found that multiplication
works best if using PPMI to weight features, but
that geometric mean is better if using the increas-
ingly popular PLMI weighting measure. Whilst
applying an intersective composition operation be-
fore applying feature weighting does allow more
features to be retained in the predicted vector (it
is possible to achieve 99.5% recall), in general,
this does not correspond with an increase in co-
sine scores. In general, the corresponding drop in
precision (i.e., the over-prediction of unobserved
features) causes the cosine to decrease. The one
exception to this is using multiplication with the
PPMI feature weighting score. Here we actually
see a drop in recall, and an increase in precision
due to the nature of multiplication and PPMI.
One assumption that has been made throughout
the work, is that the observed phrasal vector pro-
vides a good estimate of the distributional repre-
sentation of the phrase and, consequently, the best
composition method is the one which returns the
most similar prediction. However, in general, we
notice that while the recall of the compositional
methods is good, the precision is very low. Lack of
precision may be due to the prevalence of plausi-
ble, but unobserved, co-occurrences of the phrase.
Consequently, this introduces uncertainty into the
conclusions which can be drawn from a study such
as this. Further work is required to develop effec-
tive intrinsic and extrinsic evaluations of models
of composition.
A further interesting area of study is whether
distributional models that include higher-order
grammatical dependencies can tell us more about
the lexical semantics of a word than the conven-
tional first-order models, for example by distin-
guishing semantic relations such as synonymy,
antonymy, hypernymy and co-hyponymy.
</bodyText>
<sectionHeader confidence="0.996548" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.98550625">
This work was funded by UK EPSRC project
EP/IO37458/1 “A Unified Model of Composi-
tional and Distributional Compositional Seman-
tics: Theory and Applications”.
</bodyText>
<page confidence="0.998552">
18
</page>
<sectionHeader confidence="0.942916" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993039064814815">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Gemma Boleda, Marco Baroni, The Nghia Pham, and
Louise McNally. 2013. Intensionality was only al-
leged: On adjective-noun composition in distribu-
tional semantics. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013) – Long Papers, pages 35–46, Pots-
dam, Germany, March. Association for Computa-
tional Linguistics.
Gerlof Bouma. 2009. Normalised (point wise) mu-
tual information in collocation extraction, from form
to meaning: Processing texts automatically. In Pro-
ceedings of the Biennial International Conference of
the German Society for Computational Linguistics
and Language Technology.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th Annual Meeting
on Association for Computational Linguistics, ACL
’89, pages 76–83, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2011. Mathematical foundations for a com-
positional distributed model of meaning. Linguistic
Analysis, 36(1-4):345–384.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897–906, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635–653.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Gregory Grefenstette. 1994. Corpus-derived first, sec-
ond and third-order word affinities. In Proceedings
of Euralex 1994.
Emiliano Guevara. 2010. A Regression Model of
Adjective-Noun Compositionality in Distributional
Semantics. In Proceedings of the ACL GEMS Work-
shop, pages 33–37.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 25–32, College Park, Maryland, USA, June.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistings
(COLING 1998).
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
mentation, and Computers, 28:203–208.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236–244, Columbus, Ohio,
June. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop on Incremental Parsing, pages 50–57.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161–199.
James Pustejovsky. 2013. Inference patterns with in-
tensional adjectives. In Proceedings of the IWCS
Workshop on Interoperable Semantic Annotation,
Potsdam,Germany, March. Association for Compu-
tational Linguistics.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering distributional dif-
ferences between synonyms and antonyms in a word
space model. In Proceedings of the International
Joint Conference on Natural Language Processing,
pages 489–497, Nagoya, Japan.
Heinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44–47, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.983695">
19
</page>
<reference confidence="0.999487166666667">
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948–957,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Stefan Thater, Hagen Frstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2011).
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141–188.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1012–1022, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81–88, Sapporo,
Japan.
</reference>
<page confidence="0.994567">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911529">
<title confidence="0.998505">Distributional Composition using Higher-Order Dependency Vectors</title>
<author confidence="0.999556">David Weir Weeds</author>
<affiliation confidence="0.999521">Department of University of</affiliation>
<address confidence="0.964109">Brighton, BN1 9QH,</address>
<email confidence="0.979452">D.J.Weir,</email>
<abstract confidence="0.996837454545454">This paper concerns how to apply compositional methods to vectors based on grammatical dependency relation vectors. We demonstrate the potential of a novel approach which uses higher-order grammatical dependency relations as features. We apply the approach to adjective-noun compounds with promising results in the prediction of the vectors for (held-out) observed phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11541" citStr="Baroni and Zamparelli, 2010" startWordPosition="1849" endWordPosition="1852">n implied. For example, Boleda et al. (2013) state that they use “PMI to weight the co-occurrence matrix”. However, if we allow the second order, features which might have a zero association score in the context of the the individual lexemes, could be considered significant in the context of the phrase. 3 Evaluation Our experimental evaluation of the approach is based on the assumption, which is commonly made elsewhere, that where there is a reasonable amount of corpus data available for a phrase, this will generate a good estimate of the vector of the phrase. It has been shown (Turney, 2012; Baroni and Zamparelli, 2010) that such “observed” vectors are indeed reasonable for adjective-noun and noun-noun compounds. Hence, in order to evaluate the compositional models under consideration here, we compare observed phrasal vectors with inferred phrasal vectors, where the comparison is made using the cosine measure. We note that it is 13 not possible to draw conclusions from the absolute value of the cosine score since this would favour models which always assign higher cosine scores. Hence, we draw conclusions from the change in cosine score with respect to a baseline within the same model. Methodology For each n</context>
<context position="26386" citStr="Baroni and Zamparelli (2010)" startWordPosition="4253" endWordPosition="4256">ilarity judgements of simple paired phrases. The simple functions evaluated by Mitchell and Lapata (2008) are generally acknowledged to have serious theoretical limitations in their treatment of composition. How can a commutative function such as multiplication or addition provide different interpretations for different word orderings such as window glass and glass window? The majority of attempts to rectify this have offered a more complex, non-commutative function — such as weighted addition — or taken the view that some or all words are no longer simple vectors. For example, in the work of Baroni and Zamparelli (2010) and Guevara (2010), an adjective is viewed as a modifying function and represented by a matrix. Coecke et al. (2011) and Grefenstette et al. (2013) also incorporate the notion of function application from formal semantics. They derived function application from syntactic structure, representing functions as tensors and arguments as vectors. The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke et al. (2011) in using synt</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>The Nghia Pham</author>
<author>Louise McNally</author>
</authors>
<title>Intensionality was only alleged: On adjective-noun composition in distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>35--46</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Potsdam, Germany,</location>
<contexts>
<context position="10957" citStr="Boleda et al. (2013)" startWordPosition="1751" endWordPosition="1754"> Association Scores (PLMI) (Scheible et al., 2013) and positive normalised point wise mutual information (PNPMI) (Bouma, 2009). For definitions, see Table 3. Timing of feature weighting. We consider two alternatives: we can weight features before composition so that the composition operation is applied to weighted vectors, or we can compose vectors prior to feature weighting, in which case the composition operation is applied to unweighted vectors, and feature weighting is applied in the context of making a similarity calculation. In other work, the former order is often implied. For example, Boleda et al. (2013) state that they use “PMI to weight the co-occurrence matrix”. However, if we allow the second order, features which might have a zero association score in the context of the the individual lexemes, could be considered significant in the context of the phrase. 3 Evaluation Our experimental evaluation of the approach is based on the assumption, which is commonly made elsewhere, that where there is a reasonable amount of corpus data available for a phrase, this will generate a good estimate of the vector of the phrase. It has been shown (Turney, 2012; Baroni and Zamparelli, 2010) that such “obse</context>
</contexts>
<marker>Boleda, Baroni, Pham, McNally, 2013</marker>
<rawString>Gemma Boleda, Marco Baroni, The Nghia Pham, and Louise McNally. 2013. Intensionality was only alleged: On adjective-noun composition in distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 35–46, Potsdam, Germany, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerlof Bouma</author>
</authors>
<title>Normalised (point wise) mutual information in collocation extraction, from form to meaning: Processing texts automatically.</title>
<date>2009</date>
<booktitle>In Proceedings of the Biennial International Conference of the German Society for Computational Linguistics and Language Technology.</booktitle>
<contexts>
<context position="10463" citStr="Bouma, 2009" startWordPosition="1672" endWordPosition="1673"> over-emphasise low frequency events, and as a result there has been a recent shift towards using positive localised mutual information � 2The geometric mean of x and y is (x · y). � I(x, y) if I(x, y) &gt; 0 PPMI(x, y) = 0 otherwise where I(x, y) = log P(x,y) P (x).P (y) ( L(x, y) if L(x, y) &gt; 0 PLMI(x, y) = 0 otherwise where L(x, y) = P(x, y).log( P (x,y) P (x).P (y) � PNPMI(x, y) N(x, y) if N(x, y) &gt; 0 = 0 otherwise where N(x, y) = −log(P(y).log P(x,y) 1 P (x).P(y) Table 3: Feature Association Scores (PLMI) (Scheible et al., 2013) and positive normalised point wise mutual information (PNPMI) (Bouma, 2009). For definitions, see Table 3. Timing of feature weighting. We consider two alternatives: we can weight features before composition so that the composition operation is applied to weighted vectors, or we can compose vectors prior to feature weighting, in which case the composition operation is applied to unweighted vectors, and feature weighting is applied in the context of making a similarity calculation. In other work, the former order is often implied. For example, Boleda et al. (2013) state that they use “PMI to weight the co-occurrence matrix”. However, if we allow the second order, feat</context>
</contexts>
<marker>Bouma, 2009</marker>
<rawString>Gerlof Bouma. 2009. Normalised (point wise) mutual information in collocation extraction, from form to meaning: Processing texts automatically. In Proceedings of the Biennial International Conference of the German Society for Computational Linguistics and Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ’89,</booktitle>
<pages>76--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9801" citStr="Church and Hanks, 1989" startWordPosition="1539" endWordPosition="1542">endency features; and the question as to whether feature weighting should take place before or after composition. Vector composition operation. We consider each of the following seven alternatives: pointwise addition (add), pointwise multiplication (mult), pointwise geometric mean2 (gm), pointwise maximum (max), pointwise minimum (min), first argument (hd), second argument (dp). The latter two operations simply return the first (respectively second) of the input vectors. Feature weighting. We consider three options. Much work in this area has used positive pointwise mutual information (PPMI) (Church and Hanks, 1989) to weight the features. However, PPMI is known to over-emphasise low frequency events, and as a result there has been a recent shift towards using positive localised mutual information � 2The geometric mean of x and y is (x · y). � I(x, y) if I(x, y) &gt; 0 PPMI(x, y) = 0 otherwise where I(x, y) = log P(x,y) P (x).P (y) ( L(x, y) if L(x, y) &gt; 0 PLMI(x, y) = 0 otherwise where L(x, y) = P(x, y).log( P (x,y) P (x).P (y) � PNPMI(x, y) N(x, y) if N(x, y) &gt; 0 = 0 otherwise where N(x, y) = −log(P(y).log P(x,y) 1 P (x).P(y) Table 3: Feature Association Scores (PLMI) (Scheible et al., 2013) and positive </context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography. In Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ’89, pages 76–83, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributed model of meaning. Linguistic Analysis,</title>
<date>2011</date>
<pages>36--1</pages>
<contexts>
<context position="26503" citStr="Coecke et al. (2011)" startWordPosition="4275" endWordPosition="4278">wledged to have serious theoretical limitations in their treatment of composition. How can a commutative function such as multiplication or addition provide different interpretations for different word orderings such as window glass and glass window? The majority of attempts to rectify this have offered a more complex, non-commutative function — such as weighted addition — or taken the view that some or all words are no longer simple vectors. For example, in the work of Baroni and Zamparelli (2010) and Guevara (2010), an adjective is viewed as a modifying function and represented by a matrix. Coecke et al. (2011) and Grefenstette et al. (2013) also incorporate the notion of function application from formal semantics. They derived function application from syntactic structure, representing functions as tensors and arguments as vectors. The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke et al. (2011) in using syntax to guide the order of phrasal composition. These higher order structures are typically learnt or induced using a s</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2011</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2011. Mathematical foundations for a compositional distributed model of meaning. Linguistic Analysis, 36(1-4):345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1753" citStr="Curran, 2004" startWordPosition="264" endWordPosition="266"> are based on the word’s grammatical dependents as found in a dependency parsed corpus. For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy, where an evaluation against a gold standard such as WordNet (Fellbaum, 1998) can be made (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). Pado and Lapata (2007) took this further by considering not just direct grammatical dependents, but also including indirect dependents. Thus, observing the sentence She filled her glass slowly would provide evidence that the word glass has the distributional feature iobj:advmod:slowly where iobj:advmod captures the indirect dependency relationship between glass and slowly in the sentence. Note that Pado and Lapata (2007) included a basis mapping function that gave their framework flexibility as to how to map paths such as iobj:advmod:slowly onto the basis of the vector space. Indeed, the ins</context>
<context position="30031" citStr="Curran, 2004" startWordPosition="4827" endWordPosition="4828">l similarity calculated from syntactic pattern based features is used to estimate functional similarity. The similarity of a pair of compound noun phrases was computed as a function of the similarities of the components. Crucially different from other models of phrasal level similarity, it does not attempt to derive modified vectors for phrases or words in context. 6 Conclusions and Further Work Vectors based on grammatical dependency relations are known to be useful in the discovery of tight semantic relations, such as synonymy and hypernymy, between lexemes (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). It would be useful to be able to extend these methods to determine similarity between phrases (of potentially different lengths). However, conventional approaches to composition, which have been applied to proximity-based vectors, cannot sensibly be used on vectors that are based on grammatical dependency relations. In our approach, we consider the vector for a phrase to be the vector for the head lexeme in the context of the other phrasal constituents. Like Pado and Lapata (2007), we extend the concept of a grammatical dependency relation feature to include dependency relation paths which i</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>897--906</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 897–906, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--10</pages>
<contexts>
<context position="27994" citStr="Erk, 2012" startWordPosition="4512" endWordPosition="4513">ues involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which encodes its lexical meaning in terms of distributionally similar words3, and one which encodes the selecti</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="1693" citStr="Fellbaum, 1998" startWordPosition="253" endWordPosition="254">ring words into account: the distributional features of a word are based on the word’s grammatical dependents as found in a dependency parsed corpus. For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy, where an evaluation against a gold standard such as WordNet (Fellbaum, 1998) can be made (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). Pado and Lapata (2007) took this further by considering not just direct grammatical dependents, but also including indirect dependents. Thus, observing the sentence She filled her glass slowly would provide evidence that the word glass has the distributional feature iobj:advmod:slowly where iobj:advmod captures the indirect dependency relationship between glass and slowly in the sentence. Note that Pado and Lapata (2007) included a basis mapping function that gave their framework flexibility as to how to map paths such as iobj:advmo</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>Proceedings of the 10th International Conference on Computational Semantics (IWCS</booktitle>
<contexts>
<context position="26534" citStr="Grefenstette et al. (2013)" startWordPosition="4280" endWordPosition="4283">heoretical limitations in their treatment of composition. How can a commutative function such as multiplication or addition provide different interpretations for different word orderings such as window glass and glass window? The majority of attempts to rectify this have offered a more complex, non-commutative function — such as weighted addition — or taken the view that some or all words are no longer simple vectors. For example, in the work of Baroni and Zamparelli (2010) and Guevara (2010), an adjective is viewed as a modifying function and represented by a matrix. Coecke et al. (2011) and Grefenstette et al. (2013) also incorporate the notion of function application from formal semantics. They derived function application from syntactic structure, representing functions as tensors and arguments as vectors. The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke et al. (2011) in using syntax to guide the order of phrasal composition. These higher order structures are typically learnt or induced using a supervised machine learning tech</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Corpus-derived first, second and third-order word affinities.</title>
<date>1994</date>
<booktitle>In Proceedings of Euralex</booktitle>
<contexts>
<context position="28875" citStr="Grefenstette (1994)" startWordPosition="4652" endWordPosition="4653">stributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which encodes its lexical meaning in terms of distributionally similar words3, and one which encodes the selectional preferences of each grammatical relation it supports. A word’s meaning vector was updated in the context of another word by combining it with the appropriate selectional preferences vec3These are referred to as second-order vectors using the terminology of Grefenstette (1994) and Sch¨utze (1998). However, this refers to a second-order affinity between the words and is not related to the use of grammatical dependency relations. 17 tor of the contextualising word. Turney (2012) offered a model of phrasal level similarity which combines assessments of wordlevel semantic relations. This work used two different word-level distributional representations to encapsulate two types of similarity. Distributional similarity calculated from proximity-based features was used to estimate domain similarity and distributional similarity calculated from syntactic pattern based feat</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Corpus-derived first, second and third-order word affinities. In Proceedings of Euralex 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A Regression Model of Adjective-Noun Compositionality in Distributional Semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL GEMS Workshop,</booktitle>
<pages>33--37</pages>
<contexts>
<context position="3702" citStr="Guevara (2010)" startWordPosition="562" endWordPosition="563">un composition. In Section 2, we illustrate our proposal by showing how second order dependency relations can play a role in computing the semantics of adjective-noun composition. In Section 3 we describe a number of experiments that are intended to evaluate the approach, with the results presented in Section 4. The basis for our evaluation follows Baroni and 11 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11–20, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Zamparelli (2010) and Guevara (2010). Typically, compositional distributional semantic models can be used to generate an (inferred) distributional vector for a phrase from the (observed) distributional vectors of the phrase’s constituents. One of the motivations for doing this is that the observed distributional vectors for most phrases tend to be very sparse, a consequence of the frequency with which typical phrases occur in even large corpora. However, there are phrases that occur sufficiently frequently that a reasonable characterisation of their meaning can be captured with their observed distributional vector. Such phrases </context>
<context position="26405" citStr="Guevara (2010)" startWordPosition="4258" endWordPosition="4259">ed phrases. The simple functions evaluated by Mitchell and Lapata (2008) are generally acknowledged to have serious theoretical limitations in their treatment of composition. How can a commutative function such as multiplication or addition provide different interpretations for different word orderings such as window glass and glass window? The majority of attempts to rectify this have offered a more complex, non-commutative function — such as weighted addition — or taken the view that some or all words are no longer simple vectors. For example, in the work of Baroni and Zamparelli (2010) and Guevara (2010), an adjective is viewed as a modifying function and represented by a matrix. Coecke et al. (2011) and Grefenstette et al. (2013) also incorporate the notion of function application from formal semantics. They derived function application from syntactic structure, representing functions as tensors and arguments as vectors. The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke et al. (2011) in using syntax to guide the ord</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A Regression Model of Adjective-Noun Compositionality in Distributional Semantics. In Proceedings of the ACL GEMS Workshop, pages 33–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>College Park, Maryland, USA,</location>
<contexts>
<context position="24241" citStr="Lee (1999)" startWordPosition="3924" endWordPosition="3925">o-occurrences, which are not observed with the phrase. 5 Related Work In this work, we bring together ideas from several different strands of distributional semantics: incorporating syntactic information into the distributional representation of a lexeme; representing phrasal meaning by creating distributional representations through composition; and representing word meaning in context by modifying the distributional representation of a word. The use of syntactic structure in distributional representations is not new. Two of the earliest proponents of distributional semantics, Lin (1998) and Lee (1999) used features based on first order dependency relations between words in their distributional representations. More recently, Pado and Lapata (2007) propose a semantic space based on dependency paths. This model outperformed traditional word-based models which do not take syntax into account in a synonymy relation detection task and a prevalent sense acquisition task. The problem of representing phrasal meaning has traditionally been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to pro16 weight:compose compose:weight PPMI P</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32, College Park, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistings (COLING</booktitle>
<contexts>
<context position="1015" citStr="Lin (1998)" startWordPosition="148" endWordPosition="149">es higher-order grammatical dependency relations as features. We apply the approach to adjective-noun compounds with promising results in the prediction of the vectors for (held-out) observed phrases. 1 Introduction Vector space models of semantics characterise the meaning of a word in terms of distributional features derived from word co-occurrences. The most widely adopted basis for word co-occurrence is proximity, i.e. that two words (or more generally lexemes) are taken to co-occur when they occur together within a certain sized window, or within the same sentence, paragraph, or document. Lin (1998), in contrast, took the syntactic relationship between co-occurring words into account: the distributional features of a word are based on the word’s grammatical dependents as found in a dependency parsed corpus. For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy</context>
<context position="24226" citStr="Lin (1998)" startWordPosition="3921" endWordPosition="3922">low frequency co-occurrences, which are not observed with the phrase. 5 Related Work In this work, we bring together ideas from several different strands of distributional semantics: incorporating syntactic information into the distributional representation of a lexeme; representing phrasal meaning by creating distributional representations through composition; and representing word meaning in context by modifying the distributional representation of a word. The use of syntactic structure in distributional representations is not new. Two of the earliest proponents of distributional semantics, Lin (1998) and Lee (1999) used features based on first order dependency relations between words in their distributional representations. More recently, Pado and Lapata (2007) propose a semantic space based on dependency paths. This model outperformed traditional word-based models which do not take syntax into account in a synonymy relation detection task and a prevalent sense acquisition task. The problem of representing phrasal meaning has traditionally been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to pro16 weight:compose compos</context>
<context position="29994" citStr="Lin, 1998" startWordPosition="4821" endWordPosition="4822">main similarity and distributional similarity calculated from syntactic pattern based features is used to estimate functional similarity. The similarity of a pair of compound noun phrases was computed as a function of the similarities of the components. Crucially different from other models of phrasal level similarity, it does not attempt to derive modified vectors for phrases or words in context. 6 Conclusions and Further Work Vectors based on grammatical dependency relations are known to be useful in the discovery of tight semantic relations, such as synonymy and hypernymy, between lexemes (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). It would be useful to be able to extend these methods to determine similarity between phrases (of potentially different lengths). However, conventional approaches to composition, which have been applied to proximity-based vectors, cannot sensibly be used on vectors that are based on grammatical dependency relations. In our approach, we consider the vector for a phrase to be the vector for the head lexeme in the context of the other phrasal constituents. Like Pado and Lapata (2007), we extend the concept of a grammatical dependency relation feature to incl</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International Conference on Computational Linguistings (COLING 1998).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="27844" citStr="Lund and Burgess, 1996" startWordPosition="4482" endWordPosition="4485">forming regression analysis over pairs of observed nouns and adjective-noun phrases. As a consequence of the computational expense of the machine learning techniques involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K. Lund and C. Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instrumentation, and Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="25546" citStr="Mitchell and Lapata (2008" startWordPosition="4124" endWordPosition="4127">01) †0.006 (0.004) max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003) mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006) min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007) gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005) dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007) Table 7: Means and Standard Errors for Increases in Cosine with respect to the hd Baseline for Proposed Higher-Order Dependency Based Approach. All differences statistically significant (under a paired ttest) except those marked †. duce a data structure that represents the phrase or sentence. Mitchell and Lapata (2008, 2010) found that simple additive and multiplicative functions applied to proximity-based vector representations were no less effective than more complex functions when performance was assessed against human similarity judgements of simple paired phrases. The simple functions evaluated by Mitchell and Lapata (2008) are generally acknowledged to have serious theoretical limitations in their treatment of composition. How can a commutative function such as multiplication or addition provide different interpretations for different word orderings such as window glass and glass window? The majority</context>
<context position="27892" citStr="Mitchell and Lapata, 2008" startWordPosition="4490" endWordPosition="4493">erved nouns and adjective-noun phrases. As a consequence of the computational expense of the machine learning techniques involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which enco</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Workshop on Incremental Parsing,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="16088" citStr="Nivre, 2004" startWordPosition="2603" endWordPosition="2604"> as small child will be closer than observed vector for child to the observed vector for small child. We also ask the related question of whether our inferred vector for small child is closer than the constituent vector for small to the observed vector for small child. This comparison is achieved through use of the vector operation dp that ignores the vector for the head, simply returning a first-order vector derived from the dependent. Experimental Settings Our corpus is a mid-2011 dump of WikiPedia. This has been part-of-speech tagged, lemmatised and dependency parsed using the Malt Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech (nsubj, dobj, iobj, conj, amod, advmod, nnmod) have been extracted for all POS-tagged and lemmatised nouns and adjectives occurring 100 or more times. In past work with conventional dependency relation vectors we found that using a feature threshold of 100, weighting features with PPMI and a cosine similarity score work well. For experimental purposes, we have taken 14 spanish british african japanese modern classical female natural digital military medical musical scientific free black white heavy common small l</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL Workshop on Incremental Parsing, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1777" citStr="Pado and Lapata (2007)" startWordPosition="267" endWordPosition="270">he word’s grammatical dependents as found in a dependency parsed corpus. For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy, where an evaluation against a gold standard such as WordNet (Fellbaum, 1998) can be made (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). Pado and Lapata (2007) took this further by considering not just direct grammatical dependents, but also including indirect dependents. Thus, observing the sentence She filled her glass slowly would provide evidence that the word glass has the distributional feature iobj:advmod:slowly where iobj:advmod captures the indirect dependency relationship between glass and slowly in the sentence. Note that Pado and Lapata (2007) included a basis mapping function that gave their framework flexibility as to how to map paths such as iobj:advmod:slowly onto the basis of the vector space. Indeed, the instantiation of their fram</context>
<context position="24390" citStr="Pado and Lapata (2007)" startWordPosition="3943" endWordPosition="3946">s of distributional semantics: incorporating syntactic information into the distributional representation of a lexeme; representing phrasal meaning by creating distributional representations through composition; and representing word meaning in context by modifying the distributional representation of a word. The use of syntactic structure in distributional representations is not new. Two of the earliest proponents of distributional semantics, Lin (1998) and Lee (1999) used features based on first order dependency relations between words in their distributional representations. More recently, Pado and Lapata (2007) propose a semantic space based on dependency paths. This model outperformed traditional word-based models which do not take syntax into account in a synonymy relation detection task and a prevalent sense acquisition task. The problem of representing phrasal meaning has traditionally been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to pro16 weight:compose compose:weight PPMI PLMI PPMI PLMI X S¯x x S¯x x S¯x x S¯x add 0.01 (0.001) †0.004 (0.003) -0.03 (0.001) †0.006 (0.004) max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.0</context>
<context position="30518" citStr="Pado and Lapata (2007)" startWordPosition="4905" endWordPosition="4908"> discovery of tight semantic relations, such as synonymy and hypernymy, between lexemes (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). It would be useful to be able to extend these methods to determine similarity between phrases (of potentially different lengths). However, conventional approaches to composition, which have been applied to proximity-based vectors, cannot sensibly be used on vectors that are based on grammatical dependency relations. In our approach, we consider the vector for a phrase to be the vector for the head lexeme in the context of the other phrasal constituents. Like Pado and Lapata (2007), we extend the concept of a grammatical dependency relation feature to include dependency relation paths which incorporate higher-order dependencies between words. We have shown how it is possible to align the dependency path features for words of different syntactic types, and thus produce composed vectors which predict the features of one constituent in the context of the other constituent. In our experiments with AN compounds, we have shown that these predicted vectors are closer than the head constituent’s vector to the observed phrasal vector. We have shown this is true even when the obs</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>Inference patterns with intensional adjectives.</title>
<date>2013</date>
<booktitle>In Proceedings of the IWCS Workshop on Interoperable Semantic Annotation, Potsdam,Germany,</booktitle>
<institution>March. Association for Computational Linguistics.</institution>
<contexts>
<context position="17057" citStr="Pustejovsky, 2013" startWordPosition="2742" endWordPosition="2743">eatures with PPMI and a cosine similarity score work well. For experimental purposes, we have taken 14 spanish british african japanese modern classical female natural digital military medical musical scientific free black white heavy common small large strong short long good similar previous future original former subsequent next possible Table 4: Adjectives considered 32 of the most frequently occurring adjectives (see Table 4). These adjectives include ones which would generally be considered intersective (e.g., female), subsective (e.g,, long) and nonsubsective/intensional (e.g., former) (Pustejovsky, 2013) . For all of these adjectives there are at least 100 adjective-noun phrases which occur at least 100 times in the corpus. We randomly selected 50 of the phrases for each adjective. Note that our proposed method does not require any hyper parameters to be set during training, nor does it require a certain number of phrases per adjective. For the purpose of these experiments we have a list of 1600 adjective-noun phrases, all of which occur at least 100 times in WikiPedia. 4 Results and Discussion Tables 5 and 6 summarise the average cosines for the proposed higher-order dependency approach and </context>
</contexts>
<marker>Pustejovsky, 2013</marker>
<rawString>James Pustejovsky. 2013. Inference patterns with intensional adjectives. In Proceedings of the IWCS Workshop on Interoperable Semantic Annotation, Potsdam,Germany, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Scheible</author>
<author>Sabine Schulte im Walde</author>
<author>Sylvia Springorum</author>
</authors>
<title>Uncovering distributional differences between synonyms and antonyms in a word space model.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing,</booktitle>
<pages>489--497</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="10387" citStr="Scheible et al., 2013" startWordPosition="1659" endWordPosition="1662">tion (PPMI) (Church and Hanks, 1989) to weight the features. However, PPMI is known to over-emphasise low frequency events, and as a result there has been a recent shift towards using positive localised mutual information � 2The geometric mean of x and y is (x · y). � I(x, y) if I(x, y) &gt; 0 PPMI(x, y) = 0 otherwise where I(x, y) = log P(x,y) P (x).P (y) ( L(x, y) if L(x, y) &gt; 0 PLMI(x, y) = 0 otherwise where L(x, y) = P(x, y).log( P (x,y) P (x).P (y) � PNPMI(x, y) N(x, y) if N(x, y) &gt; 0 = 0 otherwise where N(x, y) = −log(P(y).log P(x,y) 1 P (x).P(y) Table 3: Feature Association Scores (PLMI) (Scheible et al., 2013) and positive normalised point wise mutual information (PNPMI) (Bouma, 2009). For definitions, see Table 3. Timing of feature weighting. We consider two alternatives: we can weight features before composition so that the composition operation is applied to weighted vectors, or we can compose vectors prior to feature weighting, in which case the composition operation is applied to unweighted vectors, and feature weighting is applied in the context of making a similarity calculation. In other work, the former order is often implied. For example, Boleda et al. (2013) state that they use “PMI to w</context>
</contexts>
<marker>Scheible, Walde, Springorum, 2013</marker>
<rawString>Silke Scheible, Sabine Schulte im Walde, and Sylvia Springorum. 2013. Uncovering distributional differences between synonyms and antonyms in a word space model. In Proceedings of the International Joint Conference on Natural Language Processing, pages 489–497, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Heinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="26770" citStr="Socher et al. (2012)" startWordPosition="4316" endWordPosition="4319">attempts to rectify this have offered a more complex, non-commutative function — such as weighted addition — or taken the view that some or all words are no longer simple vectors. For example, in the work of Baroni and Zamparelli (2010) and Guevara (2010), an adjective is viewed as a modifying function and represented by a matrix. Coecke et al. (2011) and Grefenstette et al. (2013) also incorporate the notion of function application from formal semantics. They derived function application from syntactic structure, representing functions as tensors and arguments as vectors. The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke et al. (2011) in using syntax to guide the order of phrasal composition. These higher order structures are typically learnt or induced using a supervised machine learning technique. For example, Baroni and Zamparelli (2010) learnt their adjectival matrixes by performing regression analysis over pairs of observed nouns and adjective-noun phrases. As a consequence of the computational expense of the machine le</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Georgiana Dinu</author>
<author>Manfred Pinkal</author>
</authors>
<title>Ranking paraphrases in context.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Applied Textual Inference,</booktitle>
<pages>44--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="27913" citStr="Thater et al., 2009" startWordPosition="4494" endWordPosition="4497">oun phrases. As a consequence of the computational expense of the machine learning techniques involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which encodes its lexical meani</context>
</contexts>
<marker>Thater, Dinu, Pinkal, 2009</marker>
<rawString>Stefan Thater, Georgiana Dinu, and Manfred Pinkal. 2009. Ranking paraphrases in context. In Proceedings of the 2009 Workshop on Applied Textual Inference, pages 44–47, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>948--957</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 948–957, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen Frstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Word meaning in context: A simple and effective vector model.</title>
<date>2011</date>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<contexts>
<context position="27955" citStr="Thater et al., 2011" startWordPosition="4502" endWordPosition="4505">tational expense of the machine learning techniques involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which encodes its lexical meaning in terms of distributionally similar wo</context>
</contexts>
<marker>Thater, Frstenau, Pinkal, 2011</marker>
<rawString>Stefan Thater, Hagen Frstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="24756" citStr="Turney and Pantel, 2010" startWordPosition="3998" endWordPosition="4001">esentations is not new. Two of the earliest proponents of distributional semantics, Lin (1998) and Lee (1999) used features based on first order dependency relations between words in their distributional representations. More recently, Pado and Lapata (2007) propose a semantic space based on dependency paths. This model outperformed traditional word-based models which do not take syntax into account in a synonymy relation detection task and a prevalent sense acquisition task. The problem of representing phrasal meaning has traditionally been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to pro16 weight:compose compose:weight PPMI PLMI PPMI PLMI X S¯x x S¯x x S¯x x S¯x add 0.01 (0.001) †0.004 (0.003) -0.03 (0.001) †0.006 (0.004) max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003) mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006) min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007) gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005) dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007) Table 7: Means and Standard Errors for Increases in Cosine with respect to the hd Baseline for Proposed Higher-Order Dependency B</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<contexts>
<context position="11511" citStr="Turney, 2012" startWordPosition="1847" endWordPosition="1848"> order is often implied. For example, Boleda et al. (2013) state that they use “PMI to weight the co-occurrence matrix”. However, if we allow the second order, features which might have a zero association score in the context of the the individual lexemes, could be considered significant in the context of the phrase. 3 Evaluation Our experimental evaluation of the approach is based on the assumption, which is commonly made elsewhere, that where there is a reasonable amount of corpus data available for a phrase, this will generate a good estimate of the vector of the phrase. It has been shown (Turney, 2012; Baroni and Zamparelli, 2010) that such “observed” vectors are indeed reasonable for adjective-noun and noun-noun compounds. Hence, in order to evaluate the compositional models under consideration here, we compare observed phrasal vectors with inferred phrasal vectors, where the comparison is made using the cosine measure. We note that it is 13 not possible to draw conclusions from the absolute value of the cosine score since this would favour models which always assign higher cosine scores. Hence, we draw conclusions from the change in cosine score with respect to a baseline within the same</context>
<context position="29079" citStr="Turney (2012)" startWordPosition="4685" endWordPosition="4686">of vectors: one which encodes its lexical meaning in terms of distributionally similar words3, and one which encodes the selectional preferences of each grammatical relation it supports. A word’s meaning vector was updated in the context of another word by combining it with the appropriate selectional preferences vec3These are referred to as second-order vectors using the terminology of Grefenstette (1994) and Sch¨utze (1998). However, this refers to a second-order affinity between the words and is not related to the use of grammatical dependency relations. 17 tor of the contextualising word. Turney (2012) offered a model of phrasal level similarity which combines assessments of wordlevel semantic relations. This work used two different word-level distributional representations to encapsulate two types of similarity. Distributional similarity calculated from proximity-based features was used to estimate domain similarity and distributional similarity calculated from syntactic pattern based features is used to estimate functional similarity. The similarity of a pair of compound noun phrases was computed as a function of the similarities of the components. Crucially different from other models of</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D. Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>Latent vector weighting for word meaning in context.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1012--1022</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>Van de Cruys, Poibeau, Korhonen, 2011</marker>
<rawString>Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2011. Latent vector weighting for word meaning in context. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1012–1022, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1738" citStr="Weeds and Weir, 2003" startWordPosition="260" endWordPosition="263">nal features of a word are based on the word’s grammatical dependents as found in a dependency parsed corpus. For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy, where an evaluation against a gold standard such as WordNet (Fellbaum, 1998) can be made (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). Pado and Lapata (2007) took this further by considering not just direct grammatical dependents, but also including indirect dependents. Thus, observing the sentence She filled her glass slowly would provide evidence that the word glass has the distributional feature iobj:advmod:slowly where iobj:advmod captures the indirect dependency relationship between glass and slowly in the sentence. Note that Pado and Lapata (2007) included a basis mapping function that gave their framework flexibility as to how to map paths such as iobj:advmod:slowly onto the basis of the vector space. </context>
<context position="30016" citStr="Weeds and Weir, 2003" startWordPosition="4823" endWordPosition="4826">rity and distributional similarity calculated from syntactic pattern based features is used to estimate functional similarity. The similarity of a pair of compound noun phrases was computed as a function of the similarities of the components. Crucially different from other models of phrasal level similarity, it does not attempt to derive modified vectors for phrases or words in context. 6 Conclusions and Further Work Vectors based on grammatical dependency relations are known to be useful in the discovery of tight semantic relations, such as synonymy and hypernymy, between lexemes (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). It would be useful to be able to extend these methods to determine similarity between phrases (of potentially different lengths). However, conventional approaches to composition, which have been applied to proximity-based vectors, cannot sensibly be used on vectors that are based on grammatical dependency relations. In our approach, we consider the vector for a phrase to be the vector for the head lexeme in the context of the other phrasal constituents. Like Pado and Lapata (2007), we extend the concept of a grammatical dependency relation feature to include dependency relatio</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 81–88, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000061">
<title confidence="0.969984">
Challenges in Automating Maze Detection
</title>
<author confidence="0.717471">
Eric Morley
</author>
<note confidence="0.797805666666667">
CSLU
OHSU
Portland, OR 97239
</note>
<email confidence="0.937903">
morleye@gmail.com
</email>
<author confidence="0.976383">
Anna Eva Hallin
</author>
<affiliation confidence="0.978528">
Department of Communicative
</affiliation>
<address confidence="0.891802666666667">
Sciences and Disorders
New York University
New York, NY
</address>
<email confidence="0.999195">
ae.hallin@nyu.edu
</email>
<author confidence="0.953502">
Brian Roark
</author>
<affiliation confidence="0.889137">
Google Research
</affiliation>
<address confidence="0.962051">
New York, NY 10011
</address>
<email confidence="0.995166">
roarkbr@gmail.com
</email>
<sectionHeader confidence="0.993769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999724764705882">
SALT is a widely used annotation ap-
proach for analyzing natural language
transcripts of children. Nine annotated
corpora are distributed along with scoring
software to provide norming data. We ex-
plore automatic identification of mazes –
SALT’s version of disfluency annotations
– and find that cross-corpus generalization
is very poor. This surprising lack of cross-
corpus generalization suggests substantial
differences between the corpora. This is
the first paper to investigate the SALT cor-
pora from the lens of natural language pro-
cessing, and to compare the utility of dif-
ferent corpora collected in a clinical set-
ting to train an automatic annotation sys-
tem.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915836065574">
Assessing a child’s linguistic abilities is a critical
component of diagnosing developmental disorders
such as Specific Language Impairment or Autism
Spectrum Disorder, and for evaluating progress
made with remediation. Structured instruments
(“tests”) that elicit brief, easy to score, responses
to a sequence of items are a popular way of per-
forming such assessment. An example of a struc-
tured instrument is the CELF-4, which includes
nineteen multi-item subtests with tasks such as
object naming, word definition, reciting the days
of the week, or repeating sentences (Semel et al.,
2003). Over the past two decades, researchers
have discussed the limitations of standardized tests
and how well they tap into different language im-
pairments. Many have advocated the potential
benefits of language sample analysis (LSA) (John-
ston, 2006; Dunn et al., 1996). The analysis of
natural language samples may be particularly ben-
eficial for language assessment in ASD, where
pragmatic and social communication issues are
paramount yet may be hard to assess in a conven-
tional test format (Tager-Flusberg et al., 2009).
At present, the expense of LSA prevents it from
being more widely used. Heilmann (2010), while
arguing that LSA is not too time-consuming, esti-
mates that each minute of spoken language takes
five to manually transcribe and annotate. At this
rate, it is clearly impractical for clinicians to per-
form LSA on hours of speech. Techniques from
natural language processing could be used to build
tools to automatically annotate transcripts, thus fa-
cilitating LSA.
Here, we evaluate the utility of a set of anno-
tated corpora for automating a key annotation in
the de facto standard annotation schema for LSA:
the Systematic Analysis of Language Transcripts
(SALT) (Miller et al., 2011). SALT comprises a
scheme for coding transcripts of recorded speech,
together with software that tallies these codes,
computes scores describing utterance length and
error counts, among a range of other standard mea-
sures, and compares these scores with normative
samples. SALT codes indicate bound morphemes,
several types of grammatical errors (for example
using a pronoun of the wrong gender or case), and
mazes, which are defined as “filled pauses, false
starts, and repetitions and revisions of words, mor-
phemes and phrases” (Miller et al., 2011, p. 48).
Mazes have sparked interest in the child lan-
guage disorders literature for several reasons.
They are most often analyzed from a language
processing perspective where the disruptions are
viewed as a consequence of monitoring, detect-
ing and repairing language, potentially including
speech errors (Levelt, 1993; Postma and Kolk,
1993; Rispoli et al., 2008). Several studies have
found that as grammatical complexity and utter-
ance length increase, the number of mazes in-
creases in typically developing children and chil-
dren with language impairments (MacLachlan and
</bodyText>
<page confidence="0.992098">
69
</page>
<bodyText confidence="0.944758416666667">
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77,
Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics
Chapman, 1988; Nippold et al., 2008; Reuter-
ski¨old Wagner et al., 2000; Wetherell et al., 2007).
Mazes in narrative contexts have been shown
to differ between typical children and children
with specific language impairment (MacLachlan
and Chapman, 1988; Thordardottir and Weismer,
2001), though others have not found reliable group
differences (Guo et al., 2008; Scott and Windsor,
2000). Furthermore, outside the potential useful-
ness of looking at mazes in themselves, mazes al-
ways have to be detected and excluded in order
to calculate other standard LSA measures such
as mean length of utterance and type or token
counts. Mazes also must be excluded when ana-
lyzing speech errors, since some mazes are in fact
self-corrections of language or speech errors.
Thus, automatically delimiting mazes could be
clinically useful in several ways. First, if mazes
can be automatically detected, standard measures
such as token and type counts can be calculated
with ease, as noted above. Automatic maze detec-
tion could also be a first processing step for au-
tomatically identifying errors: error codes cannot
appear in mazes, and certain grammatical errors
may be easier to identify once mazes have been
excised. Finally, after mazes have been identified,
further analysis of the mazes themselves (e.g. the
number of word in mazes, and the placement of
mazes in the sentence) can provide supplementary
information about language formulation abilities
and word retrieval abilities (Miller et al., 2011, p.
87-89).
We use the corpora included with the SALT
software to train maze detectors. These are the
corpora that the software uses to compute refer-
ence counts. These corpora share several charac-
teristics we expect to be typical of clinical data:
they were collected under a diverse set of circum-
stances; they were annotated by different groups;
the annotations ostensibly follow the same guide-
lines; and the annotations were not designed with
automation in mind. We will investigate whether
we can extract usable generalizations from the
available data, and explore how well the auto-
mated system performs, which will be of interest
to clinicians looking to expedite LSA.
</bodyText>
<sectionHeader confidence="0.982037" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999974625">
Here we provide an overview of SALT and maze
annotations. We are not aware of any attempts
to automate maze detection, although maze de-
tection closely resembles the well-established task
of edited word detection. We also provide an
overview of the corpora included with the SALT
software, which are the ones we will use to train
maze detectors.
</bodyText>
<subsectionHeader confidence="0.98868">
2.1 SALT and Maze Annotations
</subsectionHeader>
<bodyText confidence="0.999898609756098">
The approach used in SALT has been in wide use
for nearly 30 years (Miller and Chapman, 1985),
and now also exists as a software package1 pro-
viding transcription and coding support along with
tools for aggregating statistics for manual codes
over the annotated corpora and comparing with
age norms. The SALT software is not the focus of
this investigation, so we do not discuss it further.
Following the SALT guidelines, speech should
be transcribed orthographically and verbatim. The
transcript must include and indicate: the speaker
of each utterance, partial words or stuttering, over-
lapping speech, unintelligible words, and any non-
speech sounds from the speaker. Even atypical
language, for example neologisms (novel words)
or grammatical errors (for example ‘her went’)
should be written as such.
There are three broad categories of SALT anno-
tations: indicators of 1) certain bound morphemes,
2) errors, and 3) mazes. In general, verbal suffixes
that are visible in the surface form (for example
-ing in “going”) and clitics that appear with an un-
modified root (so for example -n’t in “don’t”, but
not the -n’t in “won’t”) must be indicated. SALT
includes various codes to indicate grammatical er-
rors including, but not limited to: overgeneral-
ization errors (“goed”), extraneous words, omit-
ted words or morphemes, and inappropriate ut-
terances (e.g. answering a yes/no question with
“fight”). For more information on these standard
annotations, we refer the reader to the SALT man-
ual (Miller et al., 2011).
Here, we are interested in automatically delim-
iting mazes. In SALT, filled pauses, repetitions
and revisions are included in the umberella term
“mazes” but the manual does not include defini-
tions for any of these categories. In SALT, mazes
are simply delimited by parentheses; they have no
internal structure, and cannot be nested. Contigu-
ous spans of maze words are delimited by a single
set of parentheses, as in the following utterance:
</bodyText>
<footnote confidence="0.930936">
(1) (You have you have um there/’s only)
there/’s ten people
1http://www.saltsoftware.com/
</footnote>
<page confidence="0.998906">
70
</page>
<bodyText confidence="0.9998284">
To be clear, we define the task of automatically ap-
plying maze detections as taking unannotated tran-
scripts of speech as input, and then outputting a
binary tag for each word that indicates whether or
not it is in a maze.
</bodyText>
<subsectionHeader confidence="0.997488">
2.2 Edited Word Detection
</subsectionHeader>
<bodyText confidence="0.999980739130435">
Although we are not aware of any previous work
on automating maze detection, there is a well-
established task in natural language processing
that is quite similar: edited word detection. The
goal of edited word detection is to identify words
that have been revised or deleted by the speaker,
for example ‘to Dallas’ in the utterance ‘I want to
go to Dallas, um I mean to Denver.’. Many in-
vestigations have approached edited word detec-
tion from what Nakatani et al. (1993) have termed
‘speech-first’ perspective, meaning that edited de-
tection is performed with features from the speech
signal in addition to a transcript. These ap-
proaches, however, are not applicable to the SALT
corpora, because they only contain transcripts. As
a result, we must adopt a textfirst approach to
maze detection, using only features extracted from
a transcript.
The text-first approach to edited word detec-
tion is well established. One of the first investi-
gations taking a text-first approach was conducted
by Charniak and Johnson (2001). There, they
used boosted linear classifiers to identify edited
words. Later, Johnson and Charniak (2004) im-
proved upon the linear classifiers’ performance
with a tree adjoining grammar based noisy chan-
nel model. Zwarts and Johnson (2011) improve
the noisy channel model by adding in a reranker
that leverages features extracted with the help of a
large language model.
Qian and Liu (2013) have developed what is
currently the best-performing edited word detec-
tor, and it takes a text-first approach. Unlike the
detector proposed by Zwarts and Johnson, Qian
and Liu’s does not rely on any external data. Their
detector operates in three passes. In the first pass,
filler words (‘um’, ’uh’, ‘I mean’, ’well’, etc.) are
detected. In the second and third passes, edited
words are detected. The reason for the three passes
is that in addition to extracting features (mostly
words and part of speech tags) from the raw tran-
script, the second and third steps use features ex-
tracted from the output of previous steps. An ex-
ample of such features is adjacent words from the
utterance with filler words and some likely edited
words removed.
</bodyText>
<sectionHeader confidence="0.896129" genericHeader="method">
3 Overview of SALT Corpora
</sectionHeader>
<bodyText confidence="0.999910586206896">
We explore nine corpora included with the SALT
software. Table 1 has a high level overview of
these corpora, showing where each was collected,
the age ranges of the speakers, and the size of each
corpus both in terms of transcripts and utterances.
Note that only utterances spoken by the child are
counted, as we throw out all others.
Table 1 shows several divisions among the cor-
pora. We see that one group of corpora comes
from New Zealand, while the majority come from
North America. All of the corpora, except for Ex-
pository, include children at very different stages
of language development.
Four research groups were responsible for the
transcriptions and annotations of the corpora in
Table 1. One group produced the CONVERSA-
TION, EXPOSITORY, NARRATIVESSS, and NAR-
RATIVESTORYRETELL corpora. Another was
responsible for all of the corpora from New
Zealand. Finally, the ENNI and GILLAMNT cor-
pora were transcribed and annotated by two dif-
ferent groups. For more details on these cor-
pora, how they were collected, and the anno-
tators, we refer the reader to the SALT web-
site at http://www.saltsoftware.com/
resources/databases.html.
Some basic inspection reveals that the corpora
can be put into three groups based on the me-
dian utterance lengths, and the distribution of ut-
</bodyText>
<tableCaption confidence="0.999513">
Table 1: Description of SALT corpora
</tableCaption>
<table confidence="0.9991903">
Corpus Transcripts Utterances Age Range Speaker Location
CONVERSATION 584 82,643 2;9 – 13;3 WI &amp; CA
ENNI 377 56,108 3;11 – 10;0 Canada
EXPOSITORY 242 4,918 10;7 – 15;9 WI
GILLAMNT 500 40,102 5;0 – 11;11 USA
NARRATIVESSS 330 16,091 5;2 – 13;3 WI &amp; CA
NARRATIVESTORYRETELL 500 14,834 4;4 – 12;8 WI &amp; CA
NZCONVERSATION 248 25,503 4;5 – 7;7 NZ
NZPERSONALNARRATIVE 248 20,253 4;5 – 7;7 NZ
NZSTORYRETELL 264 2,574 4;0 – 7;7 NZ
</table>
<page confidence="0.99817">
71
</page>
<bodyText confidence="0.999936111111111">
terance2 lengths, following the groups Figure 1,
with the EXPOSITORY and CONVERSATION cor-
pora in their own groups. Note that the counts
in Figure 1 are of all of the words in each ut-
terance, including those in mazes. We see that
the corpora in Group A have a modal utterance
length ranging from seven to ten words. There are
many utterances in these corpora that are shorter
or longer than the median length. Compared to
the corpora in Group A, those in Group B have
a shorter modal utterance length, and fewer long
utterances. In Figure 1, we see that the CONVER-
SATION corpus consists mostly of very short utter-
ances. At the other extreme is the EXPOSITORY
corpus, which resembles the corpora in Group A
in terms of modal utterance length, but which gen-
erally contains longer utterances than any of the
other corpora.
</bodyText>
<sectionHeader confidence="0.995882" genericHeader="method">
4 Maze Detection Experiments
</sectionHeader>
<subsectionHeader confidence="0.985328">
4.1 Maze Detector
</subsectionHeader>
<bodyText confidence="0.935303">
We carry out our experiments in automatic maze
detection using a statistical maze detector that
learns to identify mazes from manually labeled
data using features extracted from words and auto-
matically predicted part of speech tags. The maze
detector uses the feature set shown in Table 2.
This set of features is identical to the ones used by
the ‘filler word’ detector in Qian and Liu’s disflu-
ency detector (2013). We also use the same clas-
2All of these corpora are reported to have been segmented
into c-units, which is defined as “an independent clause with
its modifiers” (Miller et al., 2011).
</bodyText>
<figureCaption confidence="0.607507571428571">
Table 2: Feature templates for maze word detection, follow-
ing Qian and Liu (2013). We extract all of the above features
from both words and POS tags, albeit separately. t0 indicates
the current word or POS tag, while t_1 is the previous one
and t1 is the following. The function I(a, b) is 1 if a and b
are identical, and otherwise 0. y_1 is the tag predicted for the
previous word.
</figureCaption>
<figure confidence="0.874302">
Category Features
Unigrams t−2, t−1, t0, t1, t2
Bigrams t−1t0, t0t1
Trigrams t−2t−1t0, t−1t0t1, t0t1t2
Logic Unigrams I(ti, t0), I(pi, p0);
−4 ≤ i ≤ 4;i =60
Logic Bigrams I(ti−2ti−1, t−1t0)
I(titi+1, t0ti+1);
−4 ≤ i ≤ 4;i =60
Predicted tag y−1
(a) Others
</figure>
<figureCaption confidence="0.9906225">
Figure 1: Histograms of utterance length (including words
in mazes) in SALT corpora
</figureCaption>
<bodyText confidence="0.999947">
sifier as the second and third steps of their system:
the Max Margin Markov Network ‘M3N’ classi-
fier in the pocketcrf toolkit (available at http://
code.google.com/p/pocketcrf/). The
M3N classifier is a kernel-based classifier that is
able to leverage the sequential nature the data in
this problem (Taskar et al., 2003). We use the fol-
lowing label set: S-O (not in maze); S-M (sin-
gle word maze); B-M (beginning of multi-word
</bodyText>
<figure confidence="0.9997285">
(a) Group A
(b) Group B
</figure>
<page confidence="0.985525">
72
</page>
<bodyText confidence="0.9997273">
maze); I-M (in multi-word maze); and E-M (end
of multi-word maze). The M3N classifier allows
us to set a unique penalty for each pair of con-
fused labels, for example penalizing an erroneous
prediction of S-O (failing to identify maze words)
more heavily than spurious predictions of maze
words (all -M labels). This ability is particularly
useful for maze detection because maze words are
so infrequent compared to words that are not in
mazes.
</bodyText>
<subsectionHeader confidence="0.94303">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999995818181818">
We split each SALT corpus into training, develop-
ment, and test partitions. Each training partition
contains 80% of the utterances the corpus, while
the development and test partitions each contain
10% of the utterances. We use the development
portion of each corpus to set the penalty matrix
system to roughly balance precision and recall.
We evaluate maze detection in terms of both
tagging performance and bracketing performance,
both of which are standard forms of evaluation
for various tasks in the Natural Language Pro-
cessing literature. Tagging performance captures
how effectively maze detection is done on a word-
by-word basis, while bracketing performance de-
scribes how well each maze is identified in its en-
tirety. For both tagging and bracketing perfor-
mance, we count the number of true and false
positives and negatives, as illustrated in Figure 2.
In tagging performance, each word gets counted
once, while in bracketing performance we com-
pare the predicted and observed maze spans. We
use these counts to compute the following metrics:
</bodyText>
<equation confidence="0.995169875">
tp
(P)recision =
tp + fp
tp
(R)ecall =
tp + fn
2PR
P + R
</equation>
<bodyText confidence="0.999425606060606">
Note that partial words and punctuation are both
ignored in evaluation. We exclude punctuation be-
cause punctuation does not need to be included
in mazes: it is not counted in summary statistics
(e.g. MLU, word count, etc.), and punctuation er-
rors are not captured by the SALT error codes.
We exclude partial words because they are always
in mazes, and therefore can be detected trivially
with a simple rule. Furthermore, because par-
tial words are excluded from evaluation, the per-
formance metrics are comparable across corpora,
even if they vary widely in the frequency of partial
words.
For both space and clarity, we do not present
the complete results of every experiment in this
paper, although they are available online3. In-
stead, we present the complete baseline results,
and then report F1 scores that are significantly
better than the baseline. We establish statistical
significance by using a randomized paired-sample
test (see Yeh (2000) or Noreen (1989)) to com-
pare the baseline system (system A) and the pro-
posed system (system B). First, we compute the
difference d in F1 score between systems A and B.
Then, we repeatedly construct a random set of pre-
dictions for each input item by choosing between
the outputs of system A and B with equal proba-
bility. We compute the F1 score of these random
predictions, and if it exceeds the F1 score of the
baseline system by at least d, we count the itera-
tion as a success. The significance level is at most
the number of successes divided by one more than
the number of trials (Noreen, 1989).
</bodyText>
<subsectionHeader confidence="0.998869">
4.3 Baseline Results
</subsectionHeader>
<bodyText confidence="0.99993075">
For each corpus, we train the maze detector on
the training partition and test it on the devel-
opment partition. The results of these runs are
in Table 3, which also includes the rank of the
size of each corpus (1 = biggest, 9 = smallest).
We see immediately that our maze detector per-
forms far better on some corpora than on oth-
ers, both in terms of tagging and bracketing per-
formance. We note that maze detection perfor-
mance is not solely determined by corpus size:
tagging performance is substantially worse on the
largest corpus (CONVERSATION) than the small-
</bodyText>
<equation confidence="0.603871">
3http://bit.ly/1dtFTPl
F1 =
</equation>
<figureCaption confidence="0.8696285">
Figure 2: Tagging and bracketing evaluation for maze detection. TP = True Positive, FP = False Positive, TN = True Negative,
FN = False Negative
Pred. ( and then it ) oh
Gold ( and then it oh )
Tag TP ×3 FN
Brack. FP, FN
</figureCaption>
<bodyText confidence="0.898148">
and then it ( um ) put his wings out .
and then it ( um ) put his wings out .
</bodyText>
<equation confidence="0.3861875">
TN ×3 TP TN ×4
TP
</equation>
<page confidence="0.957555">
73
</page>
<table confidence="0.999045181818182">
Corpus Size Rank P Tagging F1 P Bracketing F1
R R
CONVERSATION 1 0.821 0.779 0.800 0.716 0.729 0.723
ENNI 2 0.923 0.882 0.902 0.845 0.837 0.841
EXPOSITORY 8 0.703 0.680 0.691 0.620 0.615 0.618
GILLAMNT 3 0.902 0.907 0.904 0.827 0.843 0.835
NARRATIVESSS 6 0.781 0.768 0.774 0.598 0.679 0.636
NARRATIVESTORYRETELL 7 0.799 0.774 0.786 0.627 0.671 0.649
NZCONVERSATION 4 0.832 0.835 0.838 0.707 0.757 0.731
NZPERSONALNARRATIVE 5 0.842 0.835 0.838 0.707 0.757 0.731
NZSTORYRETELL 9 0.905 0.862 0.883 0.773 0.780 0.776
</table>
<tableCaption confidence="0.999808">
Table 3: Baseline maze detection performance on development sections of SALT corpora: corpus-specific models
</tableCaption>
<bodyText confidence="0.583261">
est (NZSTORYRETELL).
</bodyText>
<subsectionHeader confidence="0.731373">
4.4 Generic Model
</subsectionHeader>
<bodyText confidence="0.999987060606061">
We train a generic model for maze detection on
all of the training portions of the nine SALT cor-
pora. We use the combined development sections
of all of the corpora to tune the loss matrix for bal-
anced precision and recall. We then test the re-
sulting model on the development section of each
SALT corpus, and evaluate in terms of tagging and
bracketing accuracy.
We find that the generic model performs worse
than the baseline in terms of both tagging and
bracketing performance on six of the nine corpora
corpora. The generic model significantly improves
tagging (F1=0.925, p &lt; 0.0022) on the NZSTO-
RYRETELL corpus, but the improvement in brack-
eting performance is not significant (p &lt; 0.1635).
There is improvement of both tagging (F1=0.805,
p &lt; 0.0001) and bracketing (F1=0.677, p &lt;
0.0025) performance on the NARRATIVESSS cor-
pus. The generic model does not perform better
than the baseline corpus-specific models on any
other corpora.
The poor performance of the generic model is
somewhat surprising, as it is trained with far more
data than any of the corpus-specific models. In
many tasks in natural language processing, in-
creasing the amount of training data improves the
resulting model, although this is not necessarily
the case if the additional data is noisy or out-of-
domain. This suggests two possibilities: 1) the
language in the corpora varies substantially, per-
haps due to the speakers’ ages or the activity that
was transcribed; and 2) the maze annotations are
inconsistent between corpora.
</bodyText>
<subsectionHeader confidence="0.84174">
4.5 Multi-Corpus Models
</subsectionHeader>
<bodyText confidence="0.999987783783784">
It is possible that poor performance of the generic
model relative to the baseline corpus-specific
models can be attributed to systematic differences
between the SALT corpora. We may be able to
train a model for a set of corpora that share particu-
lar characteristics that can outperform the baseline
models because such a model could leverage more
training data. We first evaluate a model for corpora
that contain transcripts collected from children of
similar ages. We also evaluate task-specific mod-
els, specifically a maze-detection model for story
retellings, and another for conversations. These
two types of models could perform well if chil-
dren of similar ages or performing similar tasks
produce mazes in a similar manner. Finally, we
train models for each group of annotators to see
whether systematic variation in annotation stan-
dards between research groups could be respon-
sible for the generic model’s poor performance.
We train all of these models similarly to the
generic model: we pool the training sections of
the selected corpora, train the model, then test on
the development section of each selected corpus.
We use the combined development sections of the
selected corpora to tune the penalty matrix to bal-
ance precision and recall.
Again, we only report F1 scores that are higher
than the baseline model’s, and we test whether
the improvement is statistically significant. We
do not report results where just the precision or
just the recall exceeds the baseline model perfor-
mance, but not F1, because these are typically the
result of model imbalance, favoring precision at
the expense of recall or vice versa. Bear in mind
that we roughly balance precision and recall on the
combined development sets, not each corpus’s de-
velopment set individually.
</bodyText>
<sectionHeader confidence="0.622124" genericHeader="method">
4.5.1 Age-Specific Model
</sectionHeader>
<bodyText confidence="0.999692714285714">
We train a single model on the following cor-
pora: ENNI, GILLAMNT, NARRATIVESSS, and
NARRATIVESTORYRETELL. As shown in Ta-
ble 1, these corpora contain transcripts collected
from children roughly aged 4-12. In three of the
four corpora, the age-based model performs worse
than the baseline. The only exception is NAR-
</bodyText>
<page confidence="0.996766">
74
</page>
<bodyText confidence="0.9992855">
RATIVESTORYRETELL, for which the age-based
model outperforms the baseline in terms of both
tagging (F1=0.794, p &lt; 0.0673) and bracketing
(F1=0.679, p &lt; 0.0062).
</bodyText>
<subsectionHeader confidence="0.415331">
4.5.2 Task-Specific Models
</subsectionHeader>
<bodyText confidence="0.999865133333333">
We construct two task-specific models for maze
detection: one for conversations, and the other
for narrative tasks. A conversational model
trained on the CONVERSATION and NZCON-
VERSATION corpora does not improve perfor-
mance on either corpus relative to the base-
line. A model for narrative tasks trained on the
ENNI, GILLAMNT, NARRATIVESSS, NARRA-
TIVESTORYRETELL, NZPERSONALNARRATIVE
and NZSTORYRETELL corpora only improves
performance on one of these, relative to the base-
line. Specifically, the narrative task model im-
proves performance on the NARRATIVESSS cor-
pus both in terms of tagging (F1=0.797, p &lt;
0.0005) and bracketing (F1=0.693, p &lt; 0.0002).
</bodyText>
<subsectionHeader confidence="0.943317">
4.5.3 Research Group-Specific Models
</subsectionHeader>
<bodyText confidence="0.999979958333333">
There are two groups of researchers that have
annotated multiple corpora: a group in New
Zealand, which annotated the NZCONVERSA-
TION, NZPERSONALNARRATIVE, and NZSTO-
RYRETELL corpora; and another group in Wis-
consin, which annotated the CONVERSATION,
EXPOSITORY, NARRATIVESSS, and NARRA-
TIVESTORYRETELL corpora. We trained re-
search group-specific models, one for each of
these groups.
Overall, these models do not improve perfor-
mance. The New Zealand research group model
does not significantly improve performance on any
of the corpora they annotated, relative to the base-
line. The Wisconsin research group model yields
significant improvement on the NARRATIVESSS
corpus, both in terms of tagging (F1=0.803, p &lt;
0.0001) and bracketing (F1=0.699, p &lt; 0.0001)
performance. Performance on the CONVERSA-
TION and EXPOSITORY corpora is lower with
the Wisconsin research group model than with
the corpus-specific baseline models, while perfor-
mance on NARRATIVESTORYRETELL is essen-
tially the same with the two models.
</bodyText>
<sectionHeader confidence="0.998711" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999993611111112">
We compared corpus-specific models for maze de-
tection to more generic models applicable to mul-
tiple corpora, and found that the generic models
performed worse than the corpus-specific ones.
This was surprising because the more generic
models were able to leverage more training data
than the corpus specific ones, and more training
data typically improves the performance of data-
driven models such as our maze detector. These
results strongly suggest that there are substantial
differences between the nine SALT corpora.
We suspect there are many areas in which the
SALT corpora diverge from one another. One
such area may be the nature of the language: per-
haps the language differs so much between each
of the corpora that it is difficult to learn a model
appropriate for one corpus from any of the oth-
ers. Another potential source of divegence is in
transcription, which does not always follow the
SALT guidelines (Miller et al., 2011). Two of the
idiosyncracies we have observed are: more than
three X’s (or a consonant followed by multiple
X’s) to indicate unintelligble language, instead of
the conventional X, XX, and XXX for unintelligi-
ble words, phrases, and utterances, respectively;
and non-canonical transcriptions of what appear
to be filled pauses, including ‘uhm’ and ‘umhm’.
These idiosyncracies could be straightforward to
normalize using automated methods, but doing so
requires that they be identified to begin with. Fur-
thermore, although these idiosyncracies may ap-
pear to be minor, taken together they may actually
be substantial.
Another potential source of variation between
corpora is likely in the maze annotations them-
selves. SALT’s definition of mazes, “filled pauses,
false starts, and repetitions and revisions of words,
morphemes and phrases” (Miller et al., 2011, p.
48), is very short, and none of the components
is defined in the SALT manual. In contrast, the
Disfluency Annotation Stylebook for Switchboard
Corpus (Meteer et al., 1995) describes a system
of disfluency annotations over approximately 25
pages, devoting two pages to filled pauses and five
to restarts. The Switchboard disfluency annota-
tions are much richer than SALT maze annota-
tions, and we are not suggesting that they are ap-
propriate for a clinical setting. However, between
the stark contrast in detail of the two annotation
systems’ guidelines, and our finding that cross-
corpus models for maze detection perform poorly,
we recommend that SALT’s definition of mazes
and their components be elaborated and clarified.
This would be of benefit not just to those trying to
</bodyText>
<page confidence="0.995959">
75
</page>
<bodyText confidence="0.99973992">
automate the application of SALT annotations, but
also to clinicians who use SALT and depend upon
consistently annotated transcripts.
There are two clear tasks for future research that
build upon these results. First, maze detection per-
formance can surely be improved. We note, how-
ever, that evaluating maze detectors in terms of F1
score may not always be appropriate if such a de-
tector is used in a pipeline. For example, there
may be a minimum acceptable level of precision
for a maze detector used in a preprocessing step
to applying SALT error codes so that maze exci-
sion does not create additional errors. In such a
scenario, the goal would be to maximize recall at
a given level of precision.
The second task suggested by this paper is to ex-
plore the hypothesized differences within and be-
tween corpora. Such exploration could ultimately
result in more rigorous, communicable guidelines
for maze annotations, as well as other annotations
and conventions in SALT. If there are systematic
differences in maze annotations across the SALT
corpora, such exploration could suggest ways of
making the annotations consistent without com-
pletely redoing them.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999895166666667">
We would like to thank members of the ASD re-
search group at the Center for Spoken Language
Understanding at OHSU, for useful input into this
study: Jan van Santen, Alison Presmanes Hill,
Steven Bedrick, Emily Prud’hommeaux, Kyle
Gorman and Masoud Rouhizadeh. This research
was supported in part by NIH NIDCD award
R01DC012033 and NSF award #0826654. Any
opinions, findings, conclusions or recommenda-
tions expressed in this publication are those of the
authors and do not reflect the views of the NIH or
NSF.
</bodyText>
<sectionHeader confidence="0.996598" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987701904761905">
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the second meeting of the North American
Chapter of the Association for Computational Lin-
guistics on Language technologies, pages 1–9. As-
sociation for Computational Linguistics.
Michelle Dunn, Judith Flax, Martin Sliwinski, and
Dorothy Aram. 1996. The use of spontaneous lan-
guage measures as criteria for identifying children
with specific language impairment: An attempt to
reconcile clinical and research incongruence. Jour-
nal of Speech and Hearing research, 39(3):643.
Ling-yu Guo, J Bruce Tomblin, and Vicki Samel-
son. 2008. Speech disruptions in the narratives
of english-speaking children with specific language
impairment. Journal of Speech, Language, and
Hearing Research, 51(3):722–738.
John J Heilmann. 2010. Myths and realities of lan-
guage sample analysis. SIG 1 Perspectives on Lan-
guage Learning and Education, 17(1):4–8.
Mark Johnson and Eugene Charniak. 2004. A tag-
based noisy-channel model of speech repairs. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL’04), Main Vol-
ume, pages 33–39, Barcelona, Spain, July.
Judith R Johnston. 2006. Thinking about child lan-
guage: Research to practice. Thinking Publications.
Willem JM Levelt. 1993. Speaking: From intention to
articulation, volume 1. MIT press, Cambridge, MA.
Barbara G MacLachlan and Robin S Chapman. 1988.
Communication breakdowns in normal and lan-
guage learning-disabled children’s conversation and
narration. Journal of Speech and Hearing Disor-
ders, 53(1):2.
Marie W Meteer, Ann A Taylor, Robert MacIntyre,
and Rukmini Iyer. 1995. Dysfluency annotation
stylebook for the switchboard corpus. University of
Pennsylvania.
Jon Miller and Robin Chapman. 1985. Systematic
analysis of language transcripts. Madison, WI: Lan-
guage Analysis Laboratory.
Jon F Miller, Karen Andriacchi, and Ann Nockerts.
2011. Assessing language production using SALT
software: A clinician’s guide to language sample
analysis. SALT Software, LLC.
Christine Nakatani and Julia Hirschberg. 1993. A
speech-first model for repair detection and correc-
tion. In Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics,
pages 46–53, Columbus, Ohio, USA, June. Associ-
ation for Computational Linguistics.
Marilyn A Nippold, Tracy C Mansfield, Jesse L Billow,
and J Bruce Tomblin. 2008. Expository discourse
in adolescents with language impairments: Exam-
ining syntactic development. American Journal of
Speech-Language Pathology, 17(4):356–366.
Eric W Noreen. 1989. Computer intensive methods
for testing hypotheses. an introduction. 1989. John
Wiley &amp; Sons, 2(5):33.
Albert Postma and Herman Kolk. 1993. The covert
repair hypothesis: Prearticulatory repair processes in
normal and stuttered disfluencies. Journal of Speech
and Hearing Research, 36(3):472.
</reference>
<page confidence="0.854618">
76
</page>
<reference confidence="0.995779305084746">
Linguistics: Human Language Technologies, pages
703–711, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Xian Qian and Yang Liu. 2013. Disfluency detection
using multi-step stacked learning. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 820–825, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Christina Reuterski¨old Wagner, Ulrika Nettelbladt, Bir-
gitta Sahl´en, and Claes Nilholm. 2000. Conver-
sation versus narration in pre-school children with
language impairment. International Journal of Lan-
guage &amp; Communication Disorders, 35(1):83–93.
Matthew Rispoli, Pamela Hadley, and Janet Holt.
2008. Stalls and revisions: A developmental per-
spective on sentence production. Journal of Speech,
Language, and Hearing Research, 51(4):953–966.
Cheryl M Scott and Jennifer Windsor. 2000. General
language performance measures in spoken and writ-
ten narrative and expository discourse of school-age
children with language learning disabilities. Journal
of Speech, Language &amp; Hearing Research, 43(2).
Eleanor Messing Semel, Elisabeth Hemmersam Wiig,
and Wayne Secord. 2003. Clinical evaluation of
language fundamentals. The Psychological Corpo-
ration, A Harcourt Assessment Company, Toronto,
Canada, fourth edition.
Helen Tager-Flusberg, Sally Rogers, Judith Cooper,
Rebecca Landa, Catherine Lord, Rhea Paul, Ma-
bel Rice, Carol Stoel-Gammon, Amy Wetherby, and
Paul Yoder. 2009. Defining spoken language bench-
marks and selecting measures of expressive lan-
guage development for young children with autism
spectrum disorders. Journal of Speech, Language
and Hearing Research, 52(3):643.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Maximum-margin markov networks. In Neural In-
formation Processing Systems (NIPS).
Elin T Thordardottir and Susan Ellis Weismer. 2001.
Content mazes and filled pauses in narrative lan-
guage samples of children with specific language
impairment. Brain and cognition, 48(2-3):587–592.
Danielle Wetherell, Nicola Botting, and Gina Conti-
Ramsden. 2007. Narrative in adolescent specific
language impairment (sli): A comparison with peers
across two different narrative genres. International
Journal of Language &amp; Communication Disorders,
42(5):583–605.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics-Volume 2, pages 947–953. Association
for Computational Linguistics.
Simon Zwarts and Mark Johnson. 2011. The impact
of language models and loss functions on repair dis-
fluency detection. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
</reference>
<page confidence="0.999113">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.059331">
<title confidence="0.998934">Challenges in Automating Maze Detection</title>
<author confidence="0.7209315">Eric Portland</author>
<author confidence="0.7209315">OR</author>
<email confidence="0.99814">morleye@gmail.com</email>
<author confidence="0.999635">Anna Eva</author>
<affiliation confidence="0.8734435">Department of Sciences and</affiliation>
<address confidence="0.6657555">New York New York,</address>
<email confidence="0.999768">ae.hallin@nyu.edu</email>
<author confidence="0.623326">Brian Google New York</author>
<author confidence="0.623326">NY</author>
<email confidence="0.999664">roarkbr@gmail.com</email>
<abstract confidence="0.987930722222222">SALT is a widely used annotation approach for analyzing natural language transcripts of children. Nine annotated corpora are distributed along with scoring software to provide norming data. We exautomatic identification of SALT’s version of disfluency annotations – and find that cross-corpus generalization is very poor. This surprising lack of crosscorpus generalization suggests substantial differences between the corpora. This is the first paper to investigate the SALT corpora from the lens of natural language processing, and to compare the utility of different corpora collected in a clinical setting to train an automatic annotation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9957" citStr="Charniak and Johnson (2001)" startWordPosition="1566" endWordPosition="1569">tigations have approached edited word detection from what Nakatani et al. (1993) have termed ‘speech-first’ perspective, meaning that edited detection is performed with features from the speech signal in addition to a transcript. These approaches, however, are not applicable to the SALT corpora, because they only contain transcripts. As a result, we must adopt a textfirst approach to maze detection, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external d</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>Eugene Charniak and Mark Johnson. 2001. Edit detection and parsing for transcribed speech. In Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Dunn</author>
<author>Judith Flax</author>
<author>Martin Sliwinski</author>
<author>Dorothy Aram</author>
</authors>
<title>The use of spontaneous language measures as criteria for identifying children with specific language impairment: An attempt to reconcile clinical and research incongruence.</title>
<date>1996</date>
<journal>Journal of Speech and Hearing research,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="1826" citStr="Dunn et al., 1996" startWordPosition="271" endWordPosition="274">ents (“tests”) that elicit brief, easy to score, responses to a sequence of items are a popular way of performing such assessment. An example of a structured instrument is the CELF-4, which includes nineteen multi-item subtests with tasks such as object naming, word definition, reciting the days of the week, or repeating sentences (Semel et al., 2003). Over the past two decades, researchers have discussed the limitations of standardized tests and how well they tap into different language impairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al., 1996). The analysis of natural language samples may be particularly beneficial for language assessment in ASD, where pragmatic and social communication issues are paramount yet may be hard to assess in a conventional test format (Tager-Flusberg et al., 2009). At present, the expense of LSA prevents it from being more widely used. Heilmann (2010), while arguing that LSA is not too time-consuming, estimates that each minute of spoken language takes five to manually transcribe and annotate. At this rate, it is clearly impractical for clinicians to perform LSA on hours of speech. Techniques from natura</context>
</contexts>
<marker>Dunn, Flax, Sliwinski, Aram, 1996</marker>
<rawString>Michelle Dunn, Judith Flax, Martin Sliwinski, and Dorothy Aram. 1996. The use of spontaneous language measures as criteria for identifying children with specific language impairment: An attempt to reconcile clinical and research incongruence. Journal of Speech and Hearing research, 39(3):643.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ling-yu Guo</author>
<author>J Bruce Tomblin</author>
<author>Vicki Samelson</author>
</authors>
<title>Speech disruptions in the narratives of english-speaking children with specific language impairment.</title>
<date>2008</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="4454" citStr="Guo et al., 2008" startWordPosition="674" endWordPosition="677"> with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ between typical children and children with specific language impairment (MacLachlan and Chapman, 1988; Thordardottir and Weismer, 2001), though others have not found reliable group differences (Guo et al., 2008; Scott and Windsor, 2000). Furthermore, outside the potential usefulness of looking at mazes in themselves, mazes always have to be detected and excluded in order to calculate other standard LSA measures such as mean length of utterance and type or token counts. Mazes also must be excluded when analyzing speech errors, since some mazes are in fact self-corrections of language or speech errors. Thus, automatically delimiting mazes could be clinically useful in several ways. First, if mazes can be automatically detected, standard measures such as token and type counts can be calculated with eas</context>
</contexts>
<marker>Guo, Tomblin, Samelson, 2008</marker>
<rawString>Ling-yu Guo, J Bruce Tomblin, and Vicki Samelson. 2008. Speech disruptions in the narratives of english-speaking children with specific language impairment. Journal of Speech, Language, and Hearing Research, 51(3):722–738.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Heilmann</author>
</authors>
<title>Myths and realities of language sample analysis.</title>
<date>2010</date>
<booktitle>SIG 1 Perspectives on Language Learning and Education,</booktitle>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2168" citStr="Heilmann (2010)" startWordPosition="328" endWordPosition="329">., 2003). Over the past two decades, researchers have discussed the limitations of standardized tests and how well they tap into different language impairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al., 1996). The analysis of natural language samples may be particularly beneficial for language assessment in ASD, where pragmatic and social communication issues are paramount yet may be hard to assess in a conventional test format (Tager-Flusberg et al., 2009). At present, the expense of LSA prevents it from being more widely used. Heilmann (2010), while arguing that LSA is not too time-consuming, estimates that each minute of spoken language takes five to manually transcribe and annotate. At this rate, it is clearly impractical for clinicians to perform LSA on hours of speech. Techniques from natural language processing could be used to build tools to automatically annotate transcripts, thus facilitating LSA. Here, we evaluate the utility of a set of annotated corpora for automating a key annotation in the de facto standard annotation schema for LSA: the Systematic Analysis of Language Transcripts (SALT) (Miller et al., 2011). SALT co</context>
</contexts>
<marker>Heilmann, 2010</marker>
<rawString>John J Heilmann. 2010. Myths and realities of language sample analysis. SIG 1 Perspectives on Language Learning and Education, 17(1):4–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Eugene Charniak</author>
</authors>
<title>A tagbased noisy-channel model of speech repairs.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>33--39</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="10063" citStr="Johnson and Charniak (2004)" startWordPosition="1581" endWordPosition="1584">’ perspective, meaning that edited detection is performed with features from the speech signal in addition to a transcript. These approaches, however, are not applicable to the SALT corpora, because they only contain transcripts. As a result, we must adopt a textfirst approach to maze detection, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external data. Their detector operates in three passes. In the first pass, filler words (‘um’, ’uh’, ‘I mean’, ’well</context>
</contexts>
<marker>Johnson, Charniak, 2004</marker>
<rawString>Mark Johnson and Eugene Charniak. 2004. A tagbased noisy-channel model of speech repairs. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 33–39, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith R Johnston</author>
</authors>
<title>Thinking about child language: Research to practice. Thinking Publications.</title>
<date>2006</date>
<contexts>
<context position="1806" citStr="Johnston, 2006" startWordPosition="268" endWordPosition="270">ructured instruments (“tests”) that elicit brief, easy to score, responses to a sequence of items are a popular way of performing such assessment. An example of a structured instrument is the CELF-4, which includes nineteen multi-item subtests with tasks such as object naming, word definition, reciting the days of the week, or repeating sentences (Semel et al., 2003). Over the past two decades, researchers have discussed the limitations of standardized tests and how well they tap into different language impairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al., 1996). The analysis of natural language samples may be particularly beneficial for language assessment in ASD, where pragmatic and social communication issues are paramount yet may be hard to assess in a conventional test format (Tager-Flusberg et al., 2009). At present, the expense of LSA prevents it from being more widely used. Heilmann (2010), while arguing that LSA is not too time-consuming, estimates that each minute of spoken language takes five to manually transcribe and annotate. At this rate, it is clearly impractical for clinicians to perform LSA on hours of speech. Te</context>
</contexts>
<marker>Johnston, 2006</marker>
<rawString>Judith R Johnston. 2006. Thinking about child language: Research to practice. Thinking Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem JM Levelt</author>
</authors>
<title>Speaking: From intention to articulation,</title>
<date>1993</date>
<volume>1</volume>
<publisher>MIT press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3626" citStr="Levelt, 1993" startWordPosition="556" endWordPosition="557">rmative samples. SALT codes indicate bound morphemes, several types of grammatical errors (for example using a pronoun of the wrong gender or case), and mazes, which are defined as “filled pauses, false starts, and repetitions and revisions of words, morphemes and phrases” (Miller et al., 2011, p. 48). Mazes have sparked interest in the child language disorders literature for several reasons. They are most often analyzed from a language processing perspective where the disruptions are viewed as a consequence of monitoring, detecting and repairing language, potentially including speech errors (Levelt, 1993; Postma and Kolk, 1993; Rispoli et al., 2008). Several studies have found that as grammatical complexity and utterance length increase, the number of mazes increases in typically developing children and children with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have bee</context>
</contexts>
<marker>Levelt, 1993</marker>
<rawString>Willem JM Levelt. 1993. Speaking: From intention to articulation, volume 1. MIT press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara G MacLachlan</author>
<author>Robin S Chapman</author>
</authors>
<title>Communication breakdowns in normal and language learning-disabled children’s conversation and narration.</title>
<date>1988</date>
<journal>Journal of Speech and Hearing Disorders,</journal>
<volume>53</volume>
<issue>1</issue>
<contexts>
<context position="4345" citStr="MacLachlan and Chapman, 1988" startWordPosition="658" endWordPosition="661">cal complexity and utterance length increase, the number of mazes increases in typically developing children and children with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ between typical children and children with specific language impairment (MacLachlan and Chapman, 1988; Thordardottir and Weismer, 2001), though others have not found reliable group differences (Guo et al., 2008; Scott and Windsor, 2000). Furthermore, outside the potential usefulness of looking at mazes in themselves, mazes always have to be detected and excluded in order to calculate other standard LSA measures such as mean length of utterance and type or token counts. Mazes also must be excluded when analyzing speech errors, since some mazes are in fact self-corrections of language or speech errors. Thus, automatically delimiting mazes could be clinically useful in several ways. First, if ma</context>
</contexts>
<marker>MacLachlan, Chapman, 1988</marker>
<rawString>Barbara G MacLachlan and Robin S Chapman. 1988. Communication breakdowns in normal and language learning-disabled children’s conversation and narration. Journal of Speech and Hearing Disorders, 53(1):2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie W Meteer</author>
<author>Ann A Taylor</author>
<author>Robert MacIntyre</author>
<author>Rukmini Iyer</author>
</authors>
<title>Dysfluency annotation stylebook for the switchboard corpus.</title>
<date>1995</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="27705" citStr="Meteer et al., 1995" startWordPosition="4506" endWordPosition="4509"> using automated methods, but doing so requires that they be identified to begin with. Furthermore, although these idiosyncracies may appear to be minor, taken together they may actually be substantial. Another potential source of variation between corpora is likely in the maze annotations themselves. SALT’s definition of mazes, “filled pauses, false starts, and repetitions and revisions of words, morphemes and phrases” (Miller et al., 2011, p. 48), is very short, and none of the components is defined in the SALT manual. In contrast, the Disfluency Annotation Stylebook for Switchboard Corpus (Meteer et al., 1995) describes a system of disfluency annotations over approximately 25 pages, devoting two pages to filled pauses and five to restarts. The Switchboard disfluency annotations are much richer than SALT maze annotations, and we are not suggesting that they are appropriate for a clinical setting. However, between the stark contrast in detail of the two annotation systems’ guidelines, and our finding that crosscorpus models for maze detection perform poorly, we recommend that SALT’s definition of mazes and their components be elaborated and clarified. This would be of benefit not just to those trying</context>
</contexts>
<marker>Meteer, Taylor, MacIntyre, Iyer, 1995</marker>
<rawString>Marie W Meteer, Ann A Taylor, Robert MacIntyre, and Rukmini Iyer. 1995. Dysfluency annotation stylebook for the switchboard corpus. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Miller</author>
<author>Robin Chapman</author>
</authors>
<title>Systematic analysis of language transcripts.</title>
<date>1985</date>
<institution>Language Analysis Laboratory.</institution>
<location>Madison, WI:</location>
<contexts>
<context position="6739" citStr="Miller and Chapman, 1985" startWordPosition="1045" endWordPosition="1048">s from the available data, and explore how well the automated system performs, which will be of interest to clinicians looking to expedite LSA. 2 Background Here we provide an overview of SALT and maze annotations. We are not aware of any attempts to automate maze detection, although maze detection closely resembles the well-established task of edited word detection. We also provide an overview of the corpora included with the SALT software, which are the ones we will use to train maze detectors. 2.1 SALT and Maze Annotations The approach used in SALT has been in wide use for nearly 30 years (Miller and Chapman, 1985), and now also exists as a software package1 providing transcription and coding support along with tools for aggregating statistics for manual codes over the annotated corpora and comparing with age norms. The SALT software is not the focus of this investigation, so we do not discuss it further. Following the SALT guidelines, speech should be transcribed orthographically and verbatim. The transcript must include and indicate: the speaker of each utterance, partial words or stuttering, overlapping speech, unintelligible words, and any nonspeech sounds from the speaker. Even atypical language, f</context>
</contexts>
<marker>Miller, Chapman, 1985</marker>
<rawString>Jon Miller and Robin Chapman. 1985. Systematic analysis of language transcripts. Madison, WI: Language Analysis Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon F Miller</author>
<author>Karen Andriacchi</author>
<author>Ann Nockerts</author>
</authors>
<title>Assessing language production using SALT software: A clinician’s guide to language sample analysis.</title>
<date>2011</date>
<publisher>SALT Software, LLC.</publisher>
<contexts>
<context position="2759" citStr="Miller et al., 2011" startWordPosition="422" endWordPosition="425"> widely used. Heilmann (2010), while arguing that LSA is not too time-consuming, estimates that each minute of spoken language takes five to manually transcribe and annotate. At this rate, it is clearly impractical for clinicians to perform LSA on hours of speech. Techniques from natural language processing could be used to build tools to automatically annotate transcripts, thus facilitating LSA. Here, we evaluate the utility of a set of annotated corpora for automating a key annotation in the de facto standard annotation schema for LSA: the Systematic Analysis of Language Transcripts (SALT) (Miller et al., 2011). SALT comprises a scheme for coding transcripts of recorded speech, together with software that tallies these codes, computes scores describing utterance length and error counts, among a range of other standard measures, and compares these scores with normative samples. SALT codes indicate bound morphemes, several types of grammatical errors (for example using a pronoun of the wrong gender or case), and mazes, which are defined as “filled pauses, false starts, and repetitions and revisions of words, morphemes and phrases” (Miller et al., 2011, p. 48). Mazes have sparked interest in the child </context>
<context position="5584" citStr="Miller et al., 2011" startWordPosition="853" endWordPosition="856">ally detected, standard measures such as token and type counts can be calculated with ease, as noted above. Automatic maze detection could also be a first processing step for automatically identifying errors: error codes cannot appear in mazes, and certain grammatical errors may be easier to identify once mazes have been excised. Finally, after mazes have been identified, further analysis of the mazes themselves (e.g. the number of word in mazes, and the placement of mazes in the sentence) can provide supplementary information about language formulation abilities and word retrieval abilities (Miller et al., 2011, p. 87-89). We use the corpora included with the SALT software to train maze detectors. These are the corpora that the software uses to compute reference counts. These corpora share several characteristics we expect to be typical of clinical data: they were collected under a diverse set of circumstances; they were annotated by different groups; the annotations ostensibly follow the same guidelines; and the annotations were not designed with automation in mind. We will investigate whether we can extract usable generalizations from the available data, and explore how well the automated system p</context>
<context position="8156" citStr="Miller et al., 2011" startWordPosition="1269" endWordPosition="1272">rphemes, 2) errors, and 3) mazes. In general, verbal suffixes that are visible in the surface form (for example -ing in “going”) and clitics that appear with an unmodified root (so for example -n’t in “don’t”, but not the -n’t in “won’t”) must be indicated. SALT includes various codes to indicate grammatical errors including, but not limited to: overgeneralization errors (“goed”), extraneous words, omitted words or morphemes, and inappropriate utterances (e.g. answering a yes/no question with “fight”). For more information on these standard annotations, we refer the reader to the SALT manual (Miller et al., 2011). Here, we are interested in automatically delimiting mazes. In SALT, filled pauses, repetitions and revisions are included in the umberella term “mazes” but the manual does not include definitions for any of these categories. In SALT, mazes are simply delimited by parentheses; they have no internal structure, and cannot be nested. Contiguous spans of maze words are delimited by a single set of parentheses, as in the following utterance: (1) (You have you have um there/’s only) there/’s ten people 1http://www.saltsoftware.com/ 70 To be clear, we define the task of automatically applying maze d</context>
<context position="14319" citStr="Miller et al., 2011" startWordPosition="2307" endWordPosition="2310">nts 4.1 Maze Detector We carry out our experiments in automatic maze detection using a statistical maze detector that learns to identify mazes from manually labeled data using features extracted from words and automatically predicted part of speech tags. The maze detector uses the feature set shown in Table 2. This set of features is identical to the ones used by the ‘filler word’ detector in Qian and Liu’s disfluency detector (2013). We also use the same clas2All of these corpora are reported to have been segmented into c-units, which is defined as “an independent clause with its modifiers” (Miller et al., 2011). Table 2: Feature templates for maze word detection, following Qian and Liu (2013). We extract all of the above features from both words and POS tags, albeit separately. t0 indicates the current word or POS tag, while t_1 is the previous one and t1 is the following. The function I(a, b) is 1 if a and b are identical, and otherwise 0. y_1 is the tag predicted for the previous word. Category Features Unigrams t−2, t−1, t0, t1, t2 Bigrams t−1t0, t0t1 Trigrams t−2t−1t0, t−1t0t1, t0t1t2 Logic Unigrams I(ti, t0), I(pi, p0); −4 ≤ i ≤ 4;i =60 Logic Bigrams I(ti−2ti−1, t−1t0) I(titi+1, t0ti+1); −4 ≤ i</context>
<context position="26674" citStr="Miller et al., 2011" startWordPosition="4349" endWordPosition="4352"> more training data typically improves the performance of datadriven models such as our maze detector. These results strongly suggest that there are substantial differences between the nine SALT corpora. We suspect there are many areas in which the SALT corpora diverge from one another. One such area may be the nature of the language: perhaps the language differs so much between each of the corpora that it is difficult to learn a model appropriate for one corpus from any of the others. Another potential source of divegence is in transcription, which does not always follow the SALT guidelines (Miller et al., 2011). Two of the idiosyncracies we have observed are: more than three X’s (or a consonant followed by multiple X’s) to indicate unintelligble language, instead of the conventional X, XX, and XXX for unintelligible words, phrases, and utterances, respectively; and non-canonical transcriptions of what appear to be filled pauses, including ‘uhm’ and ‘umhm’. These idiosyncracies could be straightforward to normalize using automated methods, but doing so requires that they be identified to begin with. Furthermore, although these idiosyncracies may appear to be minor, taken together they may actually be</context>
</contexts>
<marker>Miller, Andriacchi, Nockerts, 2011</marker>
<rawString>Jon F Miller, Karen Andriacchi, and Ann Nockerts. 2011. Assessing language production using SALT software: A clinician’s guide to language sample analysis. SALT Software, LLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Nakatani</author>
<author>Julia Hirschberg</author>
</authors>
<title>A speech-first model for repair detection and correction.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>46--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio, USA,</location>
<marker>Nakatani, Hirschberg, 1993</marker>
<rawString>Christine Nakatani and Julia Hirschberg. 1993. A speech-first model for repair detection and correction. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 46–53, Columbus, Ohio, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Nippold</author>
<author>Tracy C Mansfield</author>
<author>Jesse L Billow</author>
<author>J Bruce Tomblin</author>
</authors>
<title>Expository discourse in adolescents with language impairments: Examining syntactic development.</title>
<date>2008</date>
<journal>American Journal of Speech-Language Pathology,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="4128" citStr="Nippold et al., 2008" startWordPosition="626" endWordPosition="629">ed as a consequence of monitoring, detecting and repairing language, potentially including speech errors (Levelt, 1993; Postma and Kolk, 1993; Rispoli et al., 2008). Several studies have found that as grammatical complexity and utterance length increase, the number of mazes increases in typically developing children and children with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ between typical children and children with specific language impairment (MacLachlan and Chapman, 1988; Thordardottir and Weismer, 2001), though others have not found reliable group differences (Guo et al., 2008; Scott and Windsor, 2000). Furthermore, outside the potential usefulness of looking at mazes in themselves, mazes always have to be detected and excluded in order to calculate other standard LSA measures such as mean length of utterance and type or token counts. Mazes also</context>
</contexts>
<marker>Nippold, Mansfield, Billow, Tomblin, 2008</marker>
<rawString>Marilyn A Nippold, Tracy C Mansfield, Jesse L Billow, and J Bruce Tomblin. 2008. Expository discourse in adolescents with language impairments: Examining syntactic development. American Journal of Speech-Language Pathology, 17(4):356–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer intensive methods for testing hypotheses. an introduction.</title>
<date>1989</date>
<pages>2--5</pages>
<publisher>John Wiley &amp; Sons,</publisher>
<contexts>
<context position="18026" citStr="Noreen (1989)" startWordPosition="2930" endWordPosition="2931"> therefore can be detected trivially with a simple rule. Furthermore, because partial words are excluded from evaluation, the performance metrics are comparable across corpora, even if they vary widely in the frequency of partial words. For both space and clarity, we do not present the complete results of every experiment in this paper, although they are available online3. Instead, we present the complete baseline results, and then report F1 scores that are significantly better than the baseline. We establish statistical significance by using a randomized paired-sample test (see Yeh (2000) or Noreen (1989)) to compare the baseline system (system A) and the proposed system (system B). First, we compute the difference d in F1 score between systems A and B. Then, we repeatedly construct a random set of predictions for each input item by choosing between the outputs of system A and B with equal probability. We compute the F1 score of these random predictions, and if it exceeds the F1 score of the baseline system by at least d, we count the iteration as a success. The significance level is at most the number of successes divided by one more than the number of trials (Noreen, 1989). 4.3 Baseline Resu</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W Noreen. 1989. Computer intensive methods for testing hypotheses. an introduction. 1989. John Wiley &amp; Sons, 2(5):33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Postma</author>
<author>Herman Kolk</author>
</authors>
<title>The covert repair hypothesis: Prearticulatory repair processes in normal and stuttered disfluencies.</title>
<date>1993</date>
<journal>Journal of Speech and Hearing Research,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="3649" citStr="Postma and Kolk, 1993" startWordPosition="558" endWordPosition="561">s. SALT codes indicate bound morphemes, several types of grammatical errors (for example using a pronoun of the wrong gender or case), and mazes, which are defined as “filled pauses, false starts, and repetitions and revisions of words, morphemes and phrases” (Miller et al., 2011, p. 48). Mazes have sparked interest in the child language disorders literature for several reasons. They are most often analyzed from a language processing perspective where the disruptions are viewed as a consequence of monitoring, detecting and repairing language, potentially including speech errors (Levelt, 1993; Postma and Kolk, 1993; Rispoli et al., 2008). Several studies have found that as grammatical complexity and utterance length increase, the number of mazes increases in typically developing children and children with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ betwe</context>
</contexts>
<marker>Postma, Kolk, 1993</marker>
<rawString>Albert Postma and Herman Kolk. 1993. The covert repair hypothesis: Prearticulatory repair processes in normal and stuttered disfluencies. Journal of Speech and Hearing Research, 36(3):472.</rawString>
</citation>
<citation valid="true">
<title>Linguistics: Human Language Technologies,</title>
<date></date>
<pages>703--711</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker></marker>
<rawString>Linguistics: Human Language Technologies, pages 703–711, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Disfluency detection using multi-step stacked learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>820--825</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="10347" citStr="Qian and Liu (2013)" startWordPosition="1628" endWordPosition="1631">ion, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external data. Their detector operates in three passes. In the first pass, filler words (‘um’, ’uh’, ‘I mean’, ’well’, etc.) are detected. In the second and third passes, edited words are detected. The reason for the three passes is that in addition to extracting features (mostly words and part of speech tags) from the raw transcript, the second and third steps use features extracted from the outp</context>
<context position="14402" citStr="Qian and Liu (2013)" startWordPosition="2321" endWordPosition="2324"> a statistical maze detector that learns to identify mazes from manually labeled data using features extracted from words and automatically predicted part of speech tags. The maze detector uses the feature set shown in Table 2. This set of features is identical to the ones used by the ‘filler word’ detector in Qian and Liu’s disfluency detector (2013). We also use the same clas2All of these corpora are reported to have been segmented into c-units, which is defined as “an independent clause with its modifiers” (Miller et al., 2011). Table 2: Feature templates for maze word detection, following Qian and Liu (2013). We extract all of the above features from both words and POS tags, albeit separately. t0 indicates the current word or POS tag, while t_1 is the previous one and t1 is the following. The function I(a, b) is 1 if a and b are identical, and otherwise 0. y_1 is the tag predicted for the previous word. Category Features Unigrams t−2, t−1, t0, t1, t2 Bigrams t−1t0, t0t1 Trigrams t−2t−1t0, t−1t0t1, t0t1t2 Logic Unigrams I(ti, t0), I(pi, p0); −4 ≤ i ≤ 4;i =60 Logic Bigrams I(ti−2ti−1, t−1t0) I(titi+1, t0ti+1); −4 ≤ i ≤ 4;i =60 Predicted tag y−1 (a) Others Figure 1: Histograms of utterance length (i</context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Disfluency detection using multi-step stacked learning. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 820–825, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Reuterski¨old Wagner</author>
<author>Ulrika Nettelbladt</author>
<author>Birgitta Sahl´en</author>
<author>Claes Nilholm</author>
</authors>
<title>Conversation versus narration in pre-school children with language impairment.</title>
<date>2000</date>
<journal>International Journal of Language &amp; Communication Disorders,</journal>
<volume>35</volume>
<issue>1</issue>
<marker>Wagner, Nettelbladt, Sahl´en, Nilholm, 2000</marker>
<rawString>Christina Reuterski¨old Wagner, Ulrika Nettelbladt, Birgitta Sahl´en, and Claes Nilholm. 2000. Conversation versus narration in pre-school children with language impairment. International Journal of Language &amp; Communication Disorders, 35(1):83–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Rispoli</author>
<author>Pamela Hadley</author>
<author>Janet Holt</author>
</authors>
<title>Stalls and revisions: A developmental perspective on sentence production.</title>
<date>2008</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<volume>51</volume>
<issue>4</issue>
<contexts>
<context position="3672" citStr="Rispoli et al., 2008" startWordPosition="562" endWordPosition="565">bound morphemes, several types of grammatical errors (for example using a pronoun of the wrong gender or case), and mazes, which are defined as “filled pauses, false starts, and repetitions and revisions of words, morphemes and phrases” (Miller et al., 2011, p. 48). Mazes have sparked interest in the child language disorders literature for several reasons. They are most often analyzed from a language processing perspective where the disruptions are viewed as a consequence of monitoring, detecting and repairing language, potentially including speech errors (Levelt, 1993; Postma and Kolk, 1993; Rispoli et al., 2008). Several studies have found that as grammatical complexity and utterance length increase, the number of mazes increases in typically developing children and children with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ between typical children and</context>
</contexts>
<marker>Rispoli, Hadley, Holt, 2008</marker>
<rawString>Matthew Rispoli, Pamela Hadley, and Janet Holt. 2008. Stalls and revisions: A developmental perspective on sentence production. Journal of Speech, Language, and Hearing Research, 51(4):953–966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheryl M Scott</author>
<author>Jennifer Windsor</author>
</authors>
<title>General language performance measures in spoken and written narrative and expository discourse of school-age children with language learning disabilities.</title>
<date>2000</date>
<journal>Journal of Speech, Language &amp; Hearing Research,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="4480" citStr="Scott and Windsor, 2000" startWordPosition="678" endWordPosition="681">airments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ between typical children and children with specific language impairment (MacLachlan and Chapman, 1988; Thordardottir and Weismer, 2001), though others have not found reliable group differences (Guo et al., 2008; Scott and Windsor, 2000). Furthermore, outside the potential usefulness of looking at mazes in themselves, mazes always have to be detected and excluded in order to calculate other standard LSA measures such as mean length of utterance and type or token counts. Mazes also must be excluded when analyzing speech errors, since some mazes are in fact self-corrections of language or speech errors. Thus, automatically delimiting mazes could be clinically useful in several ways. First, if mazes can be automatically detected, standard measures such as token and type counts can be calculated with ease, as noted above. Automat</context>
</contexts>
<marker>Scott, Windsor, 2000</marker>
<rawString>Cheryl M Scott and Jennifer Windsor. 2000. General language performance measures in spoken and written narrative and expository discourse of school-age children with language learning disabilities. Journal of Speech, Language &amp; Hearing Research, 43(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Messing Semel</author>
<author>Elisabeth Hemmersam Wiig</author>
<author>Wayne Secord</author>
</authors>
<title>Clinical evaluation of language fundamentals.</title>
<date>2003</date>
<publisher>The Psychological Corporation, A Harcourt Assessment Company,</publisher>
<location>Toronto, Canada,</location>
<note>fourth edition.</note>
<contexts>
<context position="1561" citStr="Semel et al., 2003" startWordPosition="230" endWordPosition="233">tion system. 1 Introduction Assessing a child’s linguistic abilities is a critical component of diagnosing developmental disorders such as Specific Language Impairment or Autism Spectrum Disorder, and for evaluating progress made with remediation. Structured instruments (“tests”) that elicit brief, easy to score, responses to a sequence of items are a popular way of performing such assessment. An example of a structured instrument is the CELF-4, which includes nineteen multi-item subtests with tasks such as object naming, word definition, reciting the days of the week, or repeating sentences (Semel et al., 2003). Over the past two decades, researchers have discussed the limitations of standardized tests and how well they tap into different language impairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al., 1996). The analysis of natural language samples may be particularly beneficial for language assessment in ASD, where pragmatic and social communication issues are paramount yet may be hard to assess in a conventional test format (Tager-Flusberg et al., 2009). At present, the expense of LSA prevents it from being more widely used. Heilmann</context>
</contexts>
<marker>Semel, Wiig, Secord, 2003</marker>
<rawString>Eleanor Messing Semel, Elisabeth Hemmersam Wiig, and Wayne Secord. 2003. Clinical evaluation of language fundamentals. The Psychological Corporation, A Harcourt Assessment Company, Toronto, Canada, fourth edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Tager-Flusberg</author>
<author>Sally Rogers</author>
<author>Judith Cooper</author>
<author>Rebecca Landa</author>
<author>Catherine Lord</author>
<author>Rhea Paul</author>
<author>Mabel Rice</author>
<author>Carol Stoel-Gammon</author>
<author>Amy Wetherby</author>
<author>Paul Yoder</author>
</authors>
<title>Defining spoken language benchmarks and selecting measures of expressive language development for young children with autism spectrum disorders.</title>
<date>2009</date>
<journal>Journal of Speech, Language and Hearing Research,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="2079" citStr="Tager-Flusberg et al., 2009" startWordPosition="311" endWordPosition="314"> as object naming, word definition, reciting the days of the week, or repeating sentences (Semel et al., 2003). Over the past two decades, researchers have discussed the limitations of standardized tests and how well they tap into different language impairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al., 1996). The analysis of natural language samples may be particularly beneficial for language assessment in ASD, where pragmatic and social communication issues are paramount yet may be hard to assess in a conventional test format (Tager-Flusberg et al., 2009). At present, the expense of LSA prevents it from being more widely used. Heilmann (2010), while arguing that LSA is not too time-consuming, estimates that each minute of spoken language takes five to manually transcribe and annotate. At this rate, it is clearly impractical for clinicians to perform LSA on hours of speech. Techniques from natural language processing could be used to build tools to automatically annotate transcripts, thus facilitating LSA. Here, we evaluate the utility of a set of annotated corpora for automating a key annotation in the de facto standard annotation schema for L</context>
</contexts>
<marker>Tager-Flusberg, Rogers, Cooper, Landa, Lord, Paul, Rice, Stoel-Gammon, Wetherby, Yoder, 2009</marker>
<rawString>Helen Tager-Flusberg, Sally Rogers, Judith Cooper, Rebecca Landa, Catherine Lord, Rhea Paul, Mabel Rice, Carol Stoel-Gammon, Amy Wetherby, and Paul Yoder. 2009. Defining spoken language benchmarks and selecting measures of expressive language development for young children with autism spectrum disorders. Journal of Speech, Language and Hearing Research, 52(3):643.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Maximum-margin markov networks.</title>
<date>2003</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="15363" citStr="Taskar et al., 2003" startWordPosition="2486" endWordPosition="2489">, t1, t2 Bigrams t−1t0, t0t1 Trigrams t−2t−1t0, t−1t0t1, t0t1t2 Logic Unigrams I(ti, t0), I(pi, p0); −4 ≤ i ≤ 4;i =60 Logic Bigrams I(ti−2ti−1, t−1t0) I(titi+1, t0ti+1); −4 ≤ i ≤ 4;i =60 Predicted tag y−1 (a) Others Figure 1: Histograms of utterance length (including words in mazes) in SALT corpora sifier as the second and third steps of their system: the Max Margin Markov Network ‘M3N’ classifier in the pocketcrf toolkit (available at http:// code.google.com/p/pocketcrf/). The M3N classifier is a kernel-based classifier that is able to leverage the sequential nature the data in this problem (Taskar et al., 2003). We use the following label set: S-O (not in maze); S-M (single word maze); B-M (beginning of multi-word (a) Group A (b) Group B 72 maze); I-M (in multi-word maze); and E-M (end of multi-word maze). The M3N classifier allows us to set a unique penalty for each pair of confused labels, for example penalizing an erroneous prediction of S-O (failing to identify maze words) more heavily than spurious predictions of maze words (all -M labels). This ability is particularly useful for maze detection because maze words are so infrequent compared to words that are not in mazes. 4.2 Evaluation We split</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Maximum-margin markov networks. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elin T Thordardottir</author>
<author>Susan Ellis Weismer</author>
</authors>
<title>Content mazes and filled pauses in narrative language samples of children with specific language impairment. Brain and cognition,</title>
<date>2001</date>
<pages>48--2</pages>
<contexts>
<context position="4379" citStr="Thordardottir and Weismer, 2001" startWordPosition="662" endWordPosition="665">ength increase, the number of mazes increases in typically developing children and children with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ between typical children and children with specific language impairment (MacLachlan and Chapman, 1988; Thordardottir and Weismer, 2001), though others have not found reliable group differences (Guo et al., 2008; Scott and Windsor, 2000). Furthermore, outside the potential usefulness of looking at mazes in themselves, mazes always have to be detected and excluded in order to calculate other standard LSA measures such as mean length of utterance and type or token counts. Mazes also must be excluded when analyzing speech errors, since some mazes are in fact self-corrections of language or speech errors. Thus, automatically delimiting mazes could be clinically useful in several ways. First, if mazes can be automatically detected,</context>
</contexts>
<marker>Thordardottir, Weismer, 2001</marker>
<rawString>Elin T Thordardottir and Susan Ellis Weismer. 2001. Content mazes and filled pauses in narrative language samples of children with specific language impairment. Brain and cognition, 48(2-3):587–592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danielle Wetherell</author>
<author>Nicola Botting</author>
<author>Gina ContiRamsden</author>
</authors>
<title>Narrative in adolescent specific language impairment (sli): A comparison with peers across two different narrative genres.</title>
<date>2007</date>
<journal>International Journal of Language &amp; Communication Disorders,</journal>
<volume>42</volume>
<issue>5</issue>
<contexts>
<context position="4188" citStr="Wetherell et al., 2007" startWordPosition="636" endWordPosition="639">g language, potentially including speech errors (Levelt, 1993; Postma and Kolk, 1993; Rispoli et al., 2008). Several studies have found that as grammatical complexity and utterance length increase, the number of mazes increases in typically developing children and children with language impairments (MacLachlan and 69 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 69–77, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics Chapman, 1988; Nippold et al., 2008; Reuterski¨old Wagner et al., 2000; Wetherell et al., 2007). Mazes in narrative contexts have been shown to differ between typical children and children with specific language impairment (MacLachlan and Chapman, 1988; Thordardottir and Weismer, 2001), though others have not found reliable group differences (Guo et al., 2008; Scott and Windsor, 2000). Furthermore, outside the potential usefulness of looking at mazes in themselves, mazes always have to be detected and excluded in order to calculate other standard LSA measures such as mean length of utterance and type or token counts. Mazes also must be excluded when analyzing speech errors, since some m</context>
</contexts>
<marker>Wetherell, Botting, ContiRamsden, 2007</marker>
<rawString>Danielle Wetherell, Nicola Botting, and Gina ContiRamsden. 2007. Narrative in adolescent specific language impairment (sli): A comparison with peers across two different narrative genres. International Journal of Language &amp; Communication Disorders, 42(5):583–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics-Volume 2,</booktitle>
<pages>947--953</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="18009" citStr="Yeh (2000)" startWordPosition="2927" endWordPosition="2928"> in mazes, and therefore can be detected trivially with a simple rule. Furthermore, because partial words are excluded from evaluation, the performance metrics are comparable across corpora, even if they vary widely in the frequency of partial words. For both space and clarity, we do not present the complete results of every experiment in this paper, although they are available online3. Instead, we present the complete baseline results, and then report F1 scores that are significantly better than the baseline. We establish statistical significance by using a randomized paired-sample test (see Yeh (2000) or Noreen (1989)) to compare the baseline system (system A) and the proposed system (system B). First, we compute the difference d in F1 score between systems A and B. Then, we repeatedly construct a random set of predictions for each input item by choosing between the outputs of system A and B with equal probability. We compute the F1 score of these random predictions, and if it exceeds the F1 score of the baseline system by at least d, we count the iteration as a success. The significance level is at most the number of successes divided by one more than the number of trials (Noreen, 1989). </context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th conference on Computational linguistics-Volume 2, pages 947–953. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Zwarts</author>
<author>Mark Johnson</author>
</authors>
<title>The impact of language models and loss functions on repair disfluency detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational</booktitle>
<contexts>
<context position="10196" citStr="Zwarts and Johnson (2011)" startWordPosition="1602" endWordPosition="1605">aches, however, are not applicable to the SALT corpora, because they only contain transcripts. As a result, we must adopt a textfirst approach to maze detection, using only features extracted from a transcript. The text-first approach to edited word detection is well established. One of the first investigations taking a text-first approach was conducted by Charniak and Johnson (2001). There, they used boosted linear classifiers to identify edited words. Later, Johnson and Charniak (2004) improved upon the linear classifiers’ performance with a tree adjoining grammar based noisy channel model. Zwarts and Johnson (2011) improve the noisy channel model by adding in a reranker that leverages features extracted with the help of a large language model. Qian and Liu (2013) have developed what is currently the best-performing edited word detector, and it takes a text-first approach. Unlike the detector proposed by Zwarts and Johnson, Qian and Liu’s does not rely on any external data. Their detector operates in three passes. In the first pass, filler words (‘um’, ’uh’, ‘I mean’, ’well’, etc.) are detected. In the second and third passes, edited words are detected. The reason for the three passes is that in addition</context>
</contexts>
<marker>Zwarts, Johnson, 2011</marker>
<rawString>Simon Zwarts and Mark Johnson. 2011. The impact of language models and loss functions on repair disfluency detection. In Proceedings of the 49th Annual Meeting of the Association for Computational</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
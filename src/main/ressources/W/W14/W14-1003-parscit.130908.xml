<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000850">
<title confidence="0.971684">
Comparing CRF and template-matching in phrasing tasks within a
Hybrid MT system
</title>
<author confidence="0.49693">
George Tambouratzis
</author>
<affiliation confidence="0.27002">
ILSP/Athena Res. Centre
</affiliation>
<address confidence="0.791904333333333">
6 Artemidos &amp; Epidavrou,
Paradissos Amaroussiou,
Athens, GR-15125, Greece.
</address>
<email confidence="0.999163">
giorg_t@ilsp.gr
</email>
<sectionHeader confidence="0.999504" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999207027777778">
Most current MT systems translate sentences by
operating at a sub-sentential level on parallel cor-
pora. However, this frequently necessitates pars-
ers for both SL and TL, which either (i) develop
matched segmentations that give similar outputs
in terms of phrasing over the SL and TL or (ii)
for which a mapping is externally defined be-
tween the two given segmentations. Both alterna-
tives limit portability to new languages, due to
the need for matching the appropriate tools. An-
other limitation involves the amount of parallel
texts needed. Statistical MT (SMT) (Koehn,
2010) generates high quality translations pro-
vided that large parallel corpora (of millions of
words) are available. However, this places a
strict constraint on the volume of data required to
create a functioning MT system. For this reason,
a number of researchers involved in SMT have
recently investigated the extraction of informa-
tion from monolingual corpora, including lexical
translation probabilities (Klementiev et al., 2012)
and topic-specific information (Su et al., 2012).
A related direction in MT research concerns
hybrid MT (HMT), where principles from multi-
ple MT paradigms are combined, such as for in-
stance SMT and RBMT (Rule-based MT). HMT
aims to combine the paradigms’ positive aspects
to achieve higher translation accuracy. Wu (2009)
has studied the trend of convergence of MT re-
search towards hybrid systems. Quirk et al.
(2007) have proposed an HMT system where
statistical principles are combined with Example-
Based MT (EBMT) to improve the performance
of SMT.
The PRESEMT (www.presemt.eu) methodol-
ogy (Tambouratzis et. al, 2013) supports rapid
</bodyText>
<sectionHeader confidence="0.667902" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999947225806451">
The present article focuses on improving
the performance of a hybrid Machine
Translation (MT) system, namely PRE-
SEMT. The PRESEMT methodology is
readily portable to new language pairs,
and allows the creation of MT systems
with minimal reliance on expensive re-
sources. PRESEMT is phrase-based and
uses a small parallel corpus from which
to extract structural transformations from
the source language (SL) to the target
language (TL). On the other hand, the TL
language model is extracted from large
monolingual corpora. This article exam-
ines the task of maximising the amount
of information extracted from a very lim-
ited parallel corpus. Hence, emphasis is
placed on the module that learns to seg-
ment into phrases arbitrary input text in
SL, by extrapolating information from a
limited-size parsed TL text, alleviating
the need for an SL parser. An established
method based on Conditional Random
Fields (CRF) is compared here to a much
simpler template-matching algorithm to
determine the most suitable approach for
extracting an accurate model. Experimen-
tal results indicate that for a limited-size
training set, template-matching generates
a superior model leading to higher qual-
ity translations.
</bodyText>
<page confidence="0.994383">
7
</page>
<note confidence="0.9908805">
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 7–14,
Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998196208333333">
development of hybrid MT systems for new lan-
guage pairs. The hybrid nature of PRESEMT
arises from the use of data-driven pattern recog-
nition algorithms that combine EBMT tech-
niques with statistical principles when modelling
the target language. PRESEMT utilises a very
small parallel corpus of a few hundred sentences,
together with a large TL monolingual one to de-
termine the translation. The MT process encom-
passes three stages:
Stage 1: this pre-processes the input sentence,
by tagging and lemmatising tokens and grouping
these tokens into phrases, preparing the actual
translation.
Stage 2: this comprises the main translation
engine, which in turn is divided into two phases:
Phase A: the establishment of the transla-
tion structure in terms of phrase order;
Phase B: the definition of word order and
the resolution of lexical ambiguities at an in-
tra-phrase level.
Stage 3: post-processing, where the appropri-
ate tokens are generated from lemmas.
In terms of resources, PRESEMT requires:
</bodyText>
<listItem confidence="0.888164333333333">
(i) a bilingual lemma dictionary providing
SL to TL lexical correspondences,
(ii) an extensive TL monolingual corpus,
compiled via web crawling to generate a lan-
guage model,
(iii) a very small bilingual corpus.
</listItem>
<bodyText confidence="0.984267066666667">
The bilingual corpus provides examples of the
structural transformation from SL to TL. In com-
parison to SMT, the use of a small corpus re-
duces substantially the need for locating parallel
corpora, whose procurement or development can
be extremely expensive. Instead, a small parallel
corpus can be assembled with limited recourse to
costly human resources. The small size of the
parallel corpus unavoidably places additional
requirements on the processing accuracy in order
to extract the necessary information. The main
task studied here is to extract from a parallel cor-
pus of 200 sentences appropriate structural in-
formation to describe the transformation from SL
to TL. More specifically, a module needs to be
trained to transfer a given TL phrasing scheme to
SL, so that during translation the module seg-
ments arbitrary input text into phrases in a man-
ner compatible to the TL phrasing scheme. The
question then is which method succeeds in ex-
tracting from the parallel corpus the most accu-
rate structural knowledge, to support an effective
MT system.
For transferring a TL phrasing scheme into SL,
PRESEMT relies on word and phrase alignment
of the parallel corpus. This alignment allows the
extrapolation of a model that segments the SL
text. The SL–side segmentation is limited to
phrase identification, rather than a detailed syn-
tactic analysis.
The processing of a bilingual corpus and the
elicitation of the corresponding SL-to-TL phras-
ing information involves two PRESEMT mod-
ules:
(i) The Phrase aligner module (PAM), which
performs text alignment at word and phrase level
within the parallel corpus. This language-
independent method identifies corresponding
terms within the SL and TL sides of each sen-
tence, and aligns the words between the two lan-
guages, while at the same time creating phrases
for the non-parsed side of the corpus (Sofi-
anopoulos et al., 2012).
(ii) The Phrasing model generator (PMG),
which elicits a phrasing model from this aligned
parallel corpus. PMG is trained on the aligned
parallel SL – TL sentences incorporating the
PAM output to generate a phrasing model. This
model is then employed to segment user-
specified text during translation.
A number of studies relevant to this article in-
volve the transfer of phrasing schemes from one
language to another. These studies have focussed
on extrapolating information from a resource-
rich to a resource-poor language. Yarowski et al.
(2001) have used automatically word-aligned
raw bilingual corpora to project annotations. Och
and Ney (2004) use a two-stage process via a
dynamic programming-type algorithm for align-
ing SL and TL tokens. Simard et al. (2005) pro-
pose a more advanced approach allowing non-
contiguous phrases, to cover additional linguistic
phenomena. Hwa et al. (2005) have created a
parser for a new language based on a set of paral-
lel sentences together with a parser in a fre-
quently-used language, by transferring deeper
syntactic structure and introducing fix-up rules.
Smith et al. (2009) create a TL dependency
parser by using bilingual text, a parser, and
automatically-derived word alignments.
2 Basic functionality &amp; design of phras-
ing model generator
The default PMG implementation (Tambouratzis
et al., 2011) adopts the CRF model (Lafferty at
el., 2001, Wallach, 2004) to chunk each input
</bodyText>
<page confidence="0.995776">
8
</page>
<bodyText confidence="0.999964571428571">
sentence into phrases. Earlier comparative ex-
periments have established that CRF results in a
higher accuracy of phrase detection than both
probabilistic models (such as HMMs) and small
parsers with manually-defined parsing rules.
CRF has been used by several researchers for
creating parsers (for instance Sha and Pereira,
2003, Tsuruoka et al., 2009).
Due to the expressiveness of the underlying
mathematical model, CRF requires a large num-
ber of training patterns to extract an accurate
model. Of course, the volume of training patterns
is directly dependent on the size of the parallel
corpus available. A more accurate CRF would
require the use of a large parallel corpus, though
this would compromise the portability to new
language pairs. Even by moving from handling
lemmas/tokens to part-of-speech tags when train-
ing the parser, to reduce the pattern space, it is
hard to model accurately all possible phrase
types via CRF (in particular for rarer PoS tags)
via the small corpus. On the contrary, a lower
complexity PMG model (hereafter termed PMG-
simple) may well be better suited to this data.
The work presented here is aimed at investigat-
ing whether a simpler PMG model can process
more effectively this limited-size parallel corpus
of circa 200 parallel sentences.
</bodyText>
<sectionHeader confidence="0.99078" genericHeader="introduction">
3 Detailed description of PMG-simple
</sectionHeader>
<subsectionHeader confidence="0.992002">
3.1 PMG-simple Principles
</subsectionHeader>
<bodyText confidence="0.99304235483871">
PMG-simple follows a learn-by-example concept,
where, based on the appearance of phrase pat-
terns, the system learns phrases that match ex-
actly patterns it has previously encountered. This
approach is based on the widely-used template-
matching algorithm (Duda et al., 2001), where
the aim is to match part of the input sentence to a
known phrase archetype. PMG-simple (i) does
not generate an elaborate high-order statistical
model for segmentation into phrases taking into
account preceding and ensuing tag sequences,
and (ii) cannot revise decisions so as to reach a
global optimum. Instead, PMG-simple imple-
ments a greedy search algorithm (Black, 2005),
using an ordered list of known phrases. Due to its
simple design, it suffers a number of potential
disadvantages in comparison to CRF-type ap-
proaches:
• PMG-simple only identifies exact
matches to specific patterns it has previously
seen (with some exceptions, as discussed below).
On the contrary, more sophisticated approaches
may extrapolate new knowledge. For example,
let us assume that ‘Aj’, ‘At’ and ‘No’ represent
PoS tags for adjectives, articles and nouns re-
spectively, while ‘Ac’ indicates the accusative
case. Then, if noun phrases (NP) [AjAc; AjAc;
NoAc] and [AtAc; AjAc; NoAc] are seen in
training, the unseen pattern [AtAc; AjAc; AjAc;
NoAc] may be identified as a valid NP by CRF
but not by PMG-simple.
</bodyText>
<listItem confidence="0.994940384615385">
• PMG-simple does not take into account
the wider phrase environment in its decision.
• PMG-simple, as a greedy algorithm,
does not back-track over earlier decisions and
thus may settle to sub-optimal solutions.
Conversely, PMG-simple has the following
advantages:
• As it relies on a simple learn-by-example
process, all segmentation decisions are easily
explainable, in contrast to CRF.
• The template-matching model is trained
and operates much faster than CRF.
• Finally, modifications can be integrated
</listItem>
<bodyText confidence="0.947725">
to improve the base algorithm generalisation.
These largely consist of incorporating linguistic
knowledge to allow the template-matching ap-
proach to improve language coverage and thus
address specific problems caused by the limited
training data.
</bodyText>
<subsectionHeader confidence="0.999285">
3.2 PMG-simple Steps
</subsectionHeader>
<bodyText confidence="0.996248333333333">
PMG-simple receives as input the SL-side
sentences of a bilingual corpus, segmented into
phrases. Processing consists of four main steps:
</bodyText>
<listItem confidence="0.956642352941176">
• Step 1-Accumulate &amp; count: Each sen-
tence of the bilingual corpus is scanned in turn,
using the phrases of the SL-side as training pat-
terns. More specifically, all SL-side occurring
phrases are recorded in a phrase table together
with their frequency-of-occurrence in the corpus.
• Step 2-Order: The table is ordered, based
on an ordering criterion so that phrases with a
higher likelihood of correct detection are placed
nearer the top of the phrase table. As a conse-
quence, matches are initially sought for these
phrases.
• Step 3-Generalise: Recorded phrases are
generalised, to increase the phrase table cover-
age. Thus, new valid templates are incorporated
in the phrase table, which are missing from the
limited-size training corpus. Currently, generali-
</listItem>
<page confidence="0.944733">
9
</page>
<bodyText confidence="0.999852125">
sation involves extending phrases for which all
declinable words have the same case, to other
cases. For instance, if NP [AtAc; AjAc; NoAc],
with all tokens in accusative exists in the phrase
table with a given score, NPs are also created for
nominative, genitive and vocative cases ([AtNm;
AjNm; NoNm] [AtGe; AjGe; NoGe] and [AtVo;
AjVo; NoVo]), with the same score.
</bodyText>
<listItem confidence="0.604122">
• Step 4-Remove: Phrases containing pat-
</listItem>
<bodyText confidence="0.992607394736842">
terns which are grammatically incorrect are re-
moved from the phrase table. As an example of
this step, phrases involving mixed cases are re-
moved in the present implementation.
Steps 3 and 4 allow the incorporation of lan-
guage-specific knowledge to enhance the opera-
tion of PMG-simple. However, in the experi-
ments reported in the present article, only limited
knowledge has been introduced, to evaluate how
effective this phrasing model is in a setup where
the system is not provided with large amounts of
linguistic knowledge. It is expected that by pro-
viding more language-specific knowledge, the
phrasing accuracy can be further increased over
the results reported here.
When PMG-simple is trained, it is likely that
some phrase boundaries are erroneously identi-
fied in the training data. The likelihood of such
an event is non-negligible as phrases are auto-
matically transferred using the alignment algo-
rithm from the TL-side to the SL-side. Errors
may be attributed to limited lexicon coverage or
only partial correspondence of SL-to-TL text.
However, as a rule such errors can be expected to
correspond mainly to infrequent phrases.
A mechanism for screening such errors has
been introduced in PMG-simple. This is imple-
mented as a threshold imposed on the number of
occurrences of a phrase within the training cor-
pus, normalised over the occurrences in the en-
tire corpus of the phrase tag sequence. Thus,
phrases identified very rarely in comparison to
the occurrences of their respective tag sequence
are penalised as unreliable. They are retained in
the phrase table, but are demoted to much lower
positions. This processing of the phrase table is
performed after Step 4 and represents the op-
tional final step (Step 5) of PMG-simple.
</bodyText>
<subsectionHeader confidence="0.998336">
3.3 Ordering Criteria
</subsectionHeader>
<bodyText confidence="0.999879181818182">
The choice of template-ordering criterion dic-
tates the order in which phrases are matched to
the input text. Since PMG-simple performs no
backtracking, the actual ordering affects the
segmentation accuracy substantially. A variety of
different criteria have been investigated for es-
tablishing the order of precedence with which
phrases are searched for. Out of these, only a
selection is presented here due to space restric-
tions, focussing on the most effective criteria.
These are depicted in Table 1.
</bodyText>
<figure confidence="0.88702347826087">
If phrase_freq &gt; freq_thres :
Crit1 = {[(1000*(phrase_freq/
tagseq_occur) + phrase_len*250] }
If phrase_freq &lt; freq_thres:
Crit1 = {[phrase_len *10] }
If phrase_freq &gt; freq_thres :
Crit2 ={(phrase_freq[p_index]) +
phrase_len*10000}
If phrase_freq &lt; freq_thres:
Crit2 = {phrase_len *10
+floor(100*phrase_freq/ tagseq_occur)}
If phrase_freq &gt; freq_thres :
Crit3 = {phrase_freq +
phrase_len*1000}
If phrase_freq &lt; freq_thres:
Crit3 ={phrase_len +
phrase_freq/tagseq_occur}
If phrase_freq &gt; freq_thres :
Crit4 = max {phrase_subfreq +
phrase_len*100 }
If phrase_freq &lt; freq_thres:
Crit4 = {phrase_len +
phrase_subfreq/tagseq_occur}
</figure>
<tableCaption confidence="0.971008">
Table 1: Definitions of phrase-ordering criteria.
</tableCaption>
<bodyText confidence="0.999847">
Basically, the information according to which
phrases may be ordered in the phrase table con-
sists of two types, (i) the frequency of occurrence
of a given phrase in the training corpus (denoted
as phrase_freq) and (ii) the phrase length in
terms of tokens (denoted as phrase_len). By
combining these two sources of information, dif-
ferent criteria are determined. Parameter
tagseq_occur corresponds to the number of oc-
currences of the phrase tag sequence within the
training corpus. Finally phrase_subfreq is equal
to the occurrences of a tag sequence as either an
</bodyText>
<equation confidence="0.5946255">
crit.1
crit.2
crit.3
crit.4
</equation>
<page confidence="0.965941">
10
</page>
<bodyText confidence="0.999988949152543">
entire phrase or as a sub-part of a larger phrase.
This takes into account in the frequency calcula-
tions the instances of phrases which in turn are
encapsulated within larger phrases, and is the
main point of difference between criteria crit3
and crit4.
To summarise a series of earlier experiments
involving different criteria, criteria using only
one source of information prove to be less effec-
tive. Also, criteria using non-linear combinations
of information types (i) and (ii) have been shown
to be less effective and are not reported here. All
criteria studied in the present article combine the
two aforementioned types of information in a
weighted sum, but using different multiplication
factors to emphasise one information type over
the other. The actual factors may of course be
further optimised, as the values reported in Table
1 are chosen to differ in terms of order of magni-
tude.
All criteria reported here implement Step 5, by
having a secondary formulation when the occur-
rences of a phrase fall below a threshold (pa-
rameter freq_thres). This results in assigning a
lower priority to very infrequent phrases.
A mechanism has also been introduced for the
proper handling of tokens with very infrequent
part-of-speech (PoS) tags, which typically have a
rate-of-appearance of less than 0.5% in the cor-
pus. For such tags, the likelihood of appearing in
the 200 parallel sentences is very low. Hence, in
order to split them appropriately into phrases
when they appear in input sentences, equivalence
classes have been defined. A limited number of
PoS equivalences are used, namely (i) abbrevia-
tions and foreign words are considered equiva-
lent to nouns, (ii) numerals are considered
equivalent to adjectives and (iii) pronouns are
considered equivalent to nouns. This information
is inserted in Step 3 of the phrase-ordering algo-
rithm, allowing the generation of the appropriate
phrases. Though the improvement in translation
accuracy by introducing these PoS equivalences
is not spectacular (no more than 0.005 BLEU
points) this generalisation information allows the
appropriate handling of unseen tag sequences
during translation, leading to a more robust
phrasing method.
It should be noted here that a non-greedy vari-
ant of PMG-simple has also been examined. This
was expected to be more effective, since it ex-
tends the template matching approach to take
into account a sentence-wide context. However,
it has turned out that the complexity of the non-
greedy approach is too high. By introducing
backtracking, it becomes extremely expensive
computationally to run this method for sentences
larger than 12 tokens without a substantial prun-
ing of the search space.
</bodyText>
<sectionHeader confidence="0.966034" genericHeader="method">
4 Experimental setup and results
</sectionHeader>
<subsectionHeader confidence="0.97311">
4.1 Experiment Definition
</subsectionHeader>
<bodyText confidence="0.999973810810811">
To evaluate the proposed phrasing generator, the
output of the entire translation chain up to the
final translation result is studied. This allows the
contribution of different PMG models to be
quantified using objective metrics. For the
purposes of the present article, the language pair
Greek-to-English (denoted as ELLEN) is
employed. Since the SL phrasing generated by
PMG is based on the TL phrasing scheme, the
phrase labels of the resulting SL phrases are
inherited from the TL ones. In the experiments
reported here (with English as TL), the
TreeTagger parser (Schmid, 1994) is used. Thus
the SL-side phrase types include PC, VC, ADVC
and ADJC. As TreeTagger also allows for certain
words (such as conjunctions) to remain outside
phrases, it is possible that isolated words occur in
SL too. For the purposes of modelling such
occurrences, these words form single-token
phrases, denoted as ISC (i.e. ISolated word
Chunk).
Both the parallel corpus and the evaluation
dataset employed here have been established in
the PRESEMT project, and are available over the
web (cf. www.presemt.eu/data). The parallel
corpus has been retrieved from the web (from an
EU website discussing the history of the Union),
with an average size of 18 words per sentence,
while the smallest sentence comprises 4 words
and the largest 38 words. Only minimal editing
was performed in the parallel corpus, to ensure
parallelism between SL and TL. The evaluation
set comprises 200 isolated sentences, each with a
single reference translation (Sofianopoulos et al.,
2012). These sentences have been drawn from
the internet via web crawling, being required to
have a length of between 7 and 40 tokens each.
</bodyText>
<subsectionHeader confidence="0.948857">
4.2 Experimental Results for PMG-simple
</subsectionHeader>
<tableCaption confidence="0.9614474">
Table 2 contains the translation accuracy results
obtained with PMG-simple using the criteria of
Table 1. In all experiments, the results concern
the objective evaluation of the final translation,
using four of the most widely used objective
</tableCaption>
<page confidence="0.999157">
11
</page>
<bodyText confidence="0.8006026">
evaluation metrics, namely BLEU, NIST, TER
and METEOR (NIST, 2002, Papineni et al., 2002
&amp; Snover et al., 2006). For TER a lower value
indicates a more successful translation while for
other metrics, a higher value corresponds to a
better translation. Since other components of the
MT implementation do not change, this set of
metrics provides an accurate end-to-end
measurement of the effect of the phrasing model
on the translation process. As can be seen from
Table 2, all four criteria result in translations of a
comparable accuracy. For instance, the variation
between the lowest and highest BLEU scores is
approximately 1%, while for the other metrics
this variation is even lower.
</bodyText>
<table confidence="0.999358">
Criterion BLEU NIST METEOR TER
crit 1(14/out79) 0.3643 7.3153 0.4009 48.486
crit 2(16/out87) 0.3679 7.2991 0.4009 48.590
crit 3(17out88) 0.3667 7.2937 0.4002 48.730
crit 4(148out89) 0.3637 7.2730 0.3980 48.834
</table>
<tableCaption confidence="0.9833225">
Table 2: Translation accuracy for EL-&gt;EN, using
PMG-simple with various criteria.
</tableCaption>
<table confidence="0.980104833333333">
cut-off BLEU NIST METEOR TER
freq.
0 0.3637 7.2730 0.3980 48.834
1 0.3637 7.2730 0.3980 48.834
2 0.3732 7.3511 0.4017 48.138
3 0.3660 7.2911 0.4007 48.590
</table>
<tableCaption confidence="0.898776666666667">
Table 3: Translation scores for EL-&gt;EN, using
PMG-simple with criterion 4 and various cut-off
frequencies.
</tableCaption>
<bodyText confidence="0.99885480952381">
A potential for optimisation concerns the cut-
off frequency (freq_thres) below which a phrase
is considered exceptionally infrequent and is
handled differently. Indicative results are shown
for the four metrics studied in Table 3. As can be
seen, the best results are obtained with a cut-off
frequency of 2, for the given parallel corpus. Of
course, this value is to an extent dependent on
the training set. However, based on detailed
analyses of the experimental results, it has been
found that phrases that represent hapax legomena
(i.e. phrases which occur only once) are not
reliable for chunking purposes. Here, there are
two possible explanations: (i) either such phrases
represent spurious chunkings resulting from
errors in the automatic alignment or (ii) they
represent very infrequent phrases which again
should not bias the phrasing process
disproportionately. In both cases, the activation
of the cut-off frequency improves the translation
accuracy.
</bodyText>
<subsectionHeader confidence="0.99994">
4.3 Comparison of PMG-simple to CRF
</subsectionHeader>
<bodyText confidence="0.999912">
Of course it is essential to examine how PMG-
simple translation results compare to those ob-
tained when PRESEMT is run with the standard
CRF-based phrasing model. These results are
shown in Table 4. As can be seen the optimal
performance of PMG-simple leads to an im-
proved translation accuracy over the best CRF-
based approach, with a rise of more than 6.2% in
the BLEU score. Similarly, the improvements
obtained for NIST and Meteor by introducing
PMG-simple in PRESEMT are 2.1% and 2.5%,
respectively. Finally, in the case of TER, for
which a lower score reflects a better translation,
the score is reduced by circa 3.3%. Thus, based
on the results quoted in Table 3, the performance
of PMG-simple is superior to that of the CRF-
based system for all four metrics reported. The
higher performance of PMG-simple is in agree-
ment to the observation that - as recently re-
ported for other applications (Mao at al., 2013) -
improvements over the performance of CRF and
SVM are possible by appropriately weighing
templates.
</bodyText>
<table confidence="0.9983902">
PMG BLEU NIST METEOR TER
PMG-
simple 0.3732 7.3511 0.4017 48.138
(crit.4)
CRF 0.3513 7.1966 0.3919 49.774
</table>
<tableCaption confidence="0.9128065">
Table 4: Translation accuracy for EL-&gt;EN, using
PMG-simple with crit.4 and using CRF.
</tableCaption>
<bodyText confidence="0.998594727272727">
To evaluate in more detail the results of Table
4, a preliminary statistical analysis was per-
formed. More specifically, the scores in BLEU,
NIST and TER for each of the 200 test sentences
were collected. For each of these metrics, a
paired T-test was performed comparing the
measurements obtained with (i) PMG-simple
using criterion crit.4 and (ii) CRF, over each sen-
tence. It was found that the difference in means
between the BLEU populations was indeed sta-
tistically significant at a 0.05 level. In the cases
</bodyText>
<page confidence="0.995718">
12
</page>
<bodyText confidence="0.998275666666667">
of TER and NIST measurements, though, there
was no statistically significant difference in the
two populations.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999988046511628">
PMG-simple has been proposed as a straightfor-
ward implementation to derive a phrasing model
for SL text, based on template-matching. This
operates on the same aligned corpus as the de-
fault CRF model, but is faster to train and has a
more transparent operation. The results of PMG-
simple have been compared to those of CRF,
using the final PRESEMT translation output to
gauge the phrasing effectiveness. The best results
for PMG-simple are comfortably superior to
those of CRF for all MT objective metrics used.
This indicates that PMG-simple has a sufficiently
high functionality. Though the modelling power
of CRF is higher, the template-matching ap-
proach of PMG-simple is better harmonised to
the amount of training data available. Thus
PMG-simple appears to be the phrase generator
of choice for PRESEMT.
One point that warrants further experimenta-
tion (currently under way) concerns the scaling-
up effect of larger parallel corpora on the com-
parative performance of the models. Preliminary
results with bilingual corpora of approximately
500 sentences have shown that the performance
using PMG-simple remains superior to that with
CRF, resulting in a difference of approx 0.02 for
BLEU (equivalent to a 5%-6% improvement
over the CRF baseline). In addition, PMG-simple
has been shown to perform better than CRF
when applied to the latest versions of PRESEMT,
which are currently being tested and lie beyond
the scope of this article.
Another topic of interest is to determine
whether new improved criteria can be established.
This is the subject of ongoing research.
In addition, an open question is whether the
conclusions of this study are applicable to other
thematic areas. In other words, could an ap-
proach such as PMG-simple be preferable to
CRF in other applications involving relatively
sparse data? It appears from the results summa-
rised here that this could indeed be the case,
though this remains the subject of future research.
</bodyText>
<sectionHeader confidence="0.998556" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999109888888889">
The author wishes to acknowledge the invaluable
help of Ms. Marina Vassiliou and Dr. Sokratis
Sofianopoulos, both of ILSP/Athena R.C., in
integrating PMG-simple within the PRESEMT
prototype and performing a number of experi-
ments.
The research leading to these results has received
funding from the POLYTROPON project
(KRIPIS-GSRT, MIS: 448306).
</bodyText>
<sectionHeader confidence="0.998952" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999718604651163">
Paul E. Black. 2005. Dictionary of Algorithms and
Data Structures. U.S. National Institute of Stan-
dards and Technology (NIST).
Richard O. Duda, Peter E. Hart and David G. Stork.
2001. Pattern Classification (2nd edition). Wiley
Interscience, New York, U.S.A.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas and Okan Kolak. 2005. Bootstrapping
parsers via Syntactic Projections across Parallel
Texts. Natural Language Engineering, Vol. 11, pp.
311-325.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch and David Yarowsky. 2012. Towards Statis-
tical Machine Translation without Parallel Corpora.
In Proceedings of EACL-2012 Conference, Avi-
gnon, France, 23-25 April, pp. 130-140.
Philip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, Cambridge.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labelling Se-
quence Data, In Proceedings of ICML Conference,
June 28-July 1, Williamstown, USA, pp. 282-289.
Qi Mao, and Ivor Wai-Hung Tsang. 2013. Efficient
Multitemplate Learning for Structured Production.
IEEE Transactions on Neural Networks and Learn-
ing Systems, Vol. 24, No. 2, pp. 248-261.
NIST 2002. Automatic Evaluation of Machine Trans-
lation Quality Using n-gram Co-occurrences Statis-
tics.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine
Translation. Computational Linguistics, Vol. 30,
No. 4, pp. 417-449.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. Proceed-
ings of the 40th ACL Meeting, Philadelphia, USA,
pp. 311-318.
Chris Quirk and Arul Menezes. 2006. Dependency
Treelet Translation: The convergence of statistical
and example-based machine translation? Machine
Translation, Vol. 20, pp. 43–65.
</reference>
<page confidence="0.987773">
13
</page>
<reference confidence="0.999851633802817">
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, Manchester, UK, pp. 44-49.
Fei Sha and Fernando C. N. Pereira. 2003. Shallow
Parsing with Conditional Random Fields. In Pro-
ceedings of HLT-NAACL Conference, pp. 213-220.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, Eric Gaussier, Cyril Goutte,
Kenji Yamada, Philippe Langlais, and Arne
Mauser. 2005. Translating with Non-Contiguous
Phrases. In Proceedings of the Conferences on
Human Language Technology and on Empirical
Methods in Language Processing, Vancouver,
Canada, pp. 755-762.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Hu-
man Annotation. Proceedings of the 7th Confer-
ence of the Association for Machine Translation in
the Americas, Cambridge, Massachusetts, USA,
pp. 223-231.
Sokratis Sofianopoulos, Marina Vassiliou, and
George Tambouratzis. 2012. Implementing a lan-
guage-independent MT methodology. In Proceed-
ings of the First Workshop on Multilingual Model-
ing, held within the ACL-2012 Conference, Jeju,
Republic of Korea, 13 July, pp.1-10.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong and Qun Liu. 2012.
Translation Model Adaptation for Statistical Ma-
chine Translation with Monolingual Topic Infor-
mation. In Proceedings of the 50th ACL Meeting,
Jeju, Republic of Korea, pp. 459-468.
George Tambouratzis, Fotini Simistira, Sokratis Sofi-
anopoulos, Nikolaos Tsimboukakis, and Marina
Vassiliou. 2011. A resource-light phrase scheme
for language-portable MT. In Proceedings of the
15th EAMT Conference, 30-31 May, Leuven, Bel-
gium, pp. 185-192.
George Tambouratzis, Michalis Troullinos, Sokratis
Sofianopoulos, and Marina Vassiliou. 2012. Accu-
rate phrase alignment in a bilingual corpus for
EBMT systems. In Proceedings of the 5th BUCC
Workshop, held within the LREC-2012 Confer-
ence, May 26, Istanbul, Turkey, pp. 104-111.
George Tambouratzis, Sokratis Sofianopoulos, and
Marina Vassiliou (2013) Language-independent
hybrid MT with PRESEMT. In Proceedings of
HYTRA-2013 Workshop, held within the ACL-
2013 Conference, Sofia, Bulgaria, 8 August, pp.
123-130.
Yoshimasa Tsuruoka, Jun’ichi Tsujii and Sophia
Ananiadou. 2009. Fast Full Parsing by Linear-
Chain Conditional Random Fields. In Proceedings
of the 12th EACL Conference, Athens, Greece, 30
March-3 April, pp. 790–798.
Hanna M. Wallach. 2004. Conditional Random
Fields: An Introduction. CIS Technical Report,
MS-CIS-04-21. 24 February, University of Penn-
sylvania.
Dekai Wu. 2009. Toward machine translation with
statistics and syntax and semantics. In Proceedings
of the IEEE Workshop on Automatic Speech Rec-
ognition &amp; Understanding, 13-17 November, Mer-
ano, Italy, pp. 12-21.
David Yarowsky and Grace Ngai. 2001. Inducing
Multilingual POS Taggers and NP Bracketers via
Robust Projection across Aligned Corpora. In
NAACL-2001 Conference Proceedings, pp. 200-
207.
</reference>
<page confidence="0.999253">
14
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.157024">
<title confidence="0.9801515">Comparing CRF and template-matching in phrasing tasks within a Hybrid MT system</title>
<author confidence="0.927054">George</author>
<note confidence="0.617828">ILSP/Athena Res. 6 Artemidos &amp; Paradissos Athens, GR-15125,</note>
<intro confidence="0.586092">giorg_t@ilsp.gr</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul E Black</author>
</authors>
<date>2005</date>
<booktitle>Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology (NIST).</booktitle>
<contexts>
<context position="9762" citStr="Black, 2005" startWordPosition="1531" endWordPosition="1532">cept, where, based on the appearance of phrase patterns, the system learns phrases that match exactly patterns it has previously encountered. This approach is based on the widely-used templatematching algorithm (Duda et al., 2001), where the aim is to match part of the input sentence to a known phrase archetype. PMG-simple (i) does not generate an elaborate high-order statistical model for segmentation into phrases taking into account preceding and ensuing tag sequences, and (ii) cannot revise decisions so as to reach a global optimum. Instead, PMG-simple implements a greedy search algorithm (Black, 2005), using an ordered list of known phrases. Due to its simple design, it suffers a number of potential disadvantages in comparison to CRF-type approaches: • PMG-simple only identifies exact matches to specific patterns it has previously seen (with some exceptions, as discussed below). On the contrary, more sophisticated approaches may extrapolate new knowledge. For example, let us assume that ‘Aj’, ‘At’ and ‘No’ represent PoS tags for adjectives, articles and nouns respectively, while ‘Ac’ indicates the accusative case. Then, if noun phrases (NP) [AjAc; AjAc; NoAc] and [AtAc; AjAc; NoAc] are see</context>
</contexts>
<marker>Black, 2005</marker>
<rawString>Paul E. Black. 2005. Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology (NIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
<author>David G Stork</author>
</authors>
<date>2001</date>
<booktitle>Pattern Classification (2nd edition). Wiley Interscience,</booktitle>
<location>New York, U.S.A.</location>
<contexts>
<context position="9380" citStr="Duda et al., 2001" startWordPosition="1469" endWordPosition="1472">e contrary, a lower complexity PMG model (hereafter termed PMGsimple) may well be better suited to this data. The work presented here is aimed at investigating whether a simpler PMG model can process more effectively this limited-size parallel corpus of circa 200 parallel sentences. 3 Detailed description of PMG-simple 3.1 PMG-simple Principles PMG-simple follows a learn-by-example concept, where, based on the appearance of phrase patterns, the system learns phrases that match exactly patterns it has previously encountered. This approach is based on the widely-used templatematching algorithm (Duda et al., 2001), where the aim is to match part of the input sentence to a known phrase archetype. PMG-simple (i) does not generate an elaborate high-order statistical model for segmentation into phrases taking into account preceding and ensuing tag sequences, and (ii) cannot revise decisions so as to reach a global optimum. Instead, PMG-simple implements a greedy search algorithm (Black, 2005), using an ordered list of known phrases. Due to its simple design, it suffers a number of potential disadvantages in comparison to CRF-type approaches: • PMG-simple only identifies exact matches to specific patterns i</context>
</contexts>
<marker>Duda, Hart, Stork, 2001</marker>
<rawString>Richard O. Duda, Peter E. Hart and David G. Stork. 2001. Pattern Classification (2nd edition). Wiley Interscience, New York, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via Syntactic Projections across Parallel Texts.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<pages>311--325</pages>
<contexts>
<context position="7234" citStr="Hwa et al. (2005)" startWordPosition="1129" endWordPosition="1132">ied text during translation. A number of studies relevant to this article involve the transfer of phrasing schemes from one language to another. These studies have focussed on extrapolating information from a resourcerich to a resource-poor language. Yarowski et al. (2001) have used automatically word-aligned raw bilingual corpora to project annotations. Och and Ney (2004) use a two-stage process via a dynamic programming-type algorithm for aligning SL and TL tokens. Simard et al. (2005) propose a more advanced approach allowing noncontiguous phrases, to cover additional linguistic phenomena. Hwa et al. (2005) have created a parser for a new language based on a set of parallel sentences together with a parser in a frequently-used language, by transferring deeper syntactic structure and introducing fix-up rules. Smith et al. (2009) create a TL dependency parser by using bilingual text, a parser, and automatically-derived word alignments. 2 Basic functionality &amp; design of phrasing model generator The default PMG implementation (Tambouratzis et al., 2011) adopts the CRF model (Lafferty at el., 2001, Wallach, 2004) to chunk each input 8 sentence into phrases. Earlier comparative experiments have establ</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas and Okan Kolak. 2005. Bootstrapping parsers via Syntactic Projections across Parallel Texts. Natural Language Engineering, Vol. 11, pp. 311-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ann Irvine</author>
<author>Chris CallisonBurch</author>
<author>David Yarowsky</author>
</authors>
<title>Towards Statistical Machine Translation without Parallel Corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL-2012 Conference,</booktitle>
<pages>130--140</pages>
<location>Avignon,</location>
<contexts>
<context position="1233" citStr="Klementiev et al., 2012" startWordPosition="181" endWordPosition="184">rnatives limit portability to new languages, due to the need for matching the appropriate tools. Another limitation involves the amount of parallel texts needed. Statistical MT (SMT) (Koehn, 2010) generates high quality translations provided that large parallel corpora (of millions of words) are available. However, this places a strict constraint on the volume of data required to create a functioning MT system. For this reason, a number of researchers involved in SMT have recently investigated the extraction of information from monolingual corpora, including lexical translation probabilities (Klementiev et al., 2012) and topic-specific information (Su et al., 2012). A related direction in MT research concerns hybrid MT (HMT), where principles from multiple MT paradigms are combined, such as for instance SMT and RBMT (Rule-based MT). HMT aims to combine the paradigms’ positive aspects to achieve higher translation accuracy. Wu (2009) has studied the trend of convergence of MT research towards hybrid systems. Quirk et al. (2007) have proposed an HMT system where statistical principles are combined with ExampleBased MT (EBMT) to improve the performance of SMT. The PRESEMT (www.presemt.eu) methodology (Tambou</context>
</contexts>
<marker>Klementiev, Irvine, CallisonBurch, Yarowsky, 2012</marker>
<rawString>Alexandre Klementiev, Ann Irvine, Chris CallisonBurch and David Yarowsky. 2012. Towards Statistical Machine Translation without Parallel Corpora. In Proceedings of EACL-2012 Conference, Avignon, France, 23-25 April, pp. 130-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="805" citStr="Koehn, 2010" startWordPosition="120" endWordPosition="121">eece. giorg_t@ilsp.gr 1 Introduction Most current MT systems translate sentences by operating at a sub-sentential level on parallel corpora. However, this frequently necessitates parsers for both SL and TL, which either (i) develop matched segmentations that give similar outputs in terms of phrasing over the SL and TL or (ii) for which a mapping is externally defined between the two given segmentations. Both alternatives limit portability to new languages, due to the need for matching the appropriate tools. Another limitation involves the amount of parallel texts needed. Statistical MT (SMT) (Koehn, 2010) generates high quality translations provided that large parallel corpora (of millions of words) are available. However, this places a strict constraint on the volume of data required to create a functioning MT system. For this reason, a number of researchers involved in SMT have recently investigated the extraction of information from monolingual corpora, including lexical translation probabilities (Klementiev et al., 2012) and topic-specific information (Su et al., 2012). A related direction in MT research concerns hybrid MT (HMT), where principles from multiple MT paradigms are combined, su</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philip Koehn. 2010. Statistical Machine Translation. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labelling Sequence Data,</title>
<date>2001</date>
<booktitle>In Proceedings of ICML Conference, June 28-July 1,</booktitle>
<pages>282--289</pages>
<location>Williamstown, USA,</location>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labelling Sequence Data, In Proceedings of ICML Conference, June 28-July 1, Williamstown, USA, pp. 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Mao</author>
<author>Ivor Wai-Hung Tsang</author>
</authors>
<title>Efficient Multitemplate Learning for Structured Production.</title>
<date>2013</date>
<journal>IEEE Transactions on Neural Networks and Learning Systems,</journal>
<volume>24</volume>
<pages>248--261</pages>
<marker>Mao, Tsang, 2013</marker>
<rawString>Qi Mao, and Ivor Wai-Hung Tsang. 2013. Efficient Multitemplate Learning for Structured Production. IEEE Transactions on Neural Networks and Learning Systems, Vol. 24, No. 2, pp. 248-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using n-gram Co-occurrences Statistics.</title>
<date>2002</date>
<contexts>
<context position="20873" citStr="NIST, 2002" startWordPosition="3272" endWordPosition="3273">L. The evaluation set comprises 200 isolated sentences, each with a single reference translation (Sofianopoulos et al., 2012). These sentences have been drawn from the internet via web crawling, being required to have a length of between 7 and 40 tokens each. 4.2 Experimental Results for PMG-simple Table 2 contains the translation accuracy results obtained with PMG-simple using the criteria of Table 1. In all experiments, the results concern the objective evaluation of the final translation, using four of the most widely used objective 11 evaluation metrics, namely BLEU, NIST, TER and METEOR (NIST, 2002, Papineni et al., 2002 &amp; Snover et al., 2006). For TER a lower value indicates a more successful translation while for other metrics, a higher value corresponds to a better translation. Since other components of the MT implementation do not change, this set of metrics provides an accurate end-to-end measurement of the effect of the phrasing model on the translation process. As can be seen from Table 2, all four criteria result in translations of a comparable accuracy. For instance, the variation between the lowest and highest BLEU scores is approximately 1%, while for the other metrics this v</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST 2002. Automatic Evaluation of Machine Translation Quality Using n-gram Co-occurrences Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<pages>417--449</pages>
<contexts>
<context position="6992" citStr="Och and Ney (2004)" startWordPosition="1090" endWordPosition="1093">ator (PMG), which elicits a phrasing model from this aligned parallel corpus. PMG is trained on the aligned parallel SL – TL sentences incorporating the PAM output to generate a phrasing model. This model is then employed to segment userspecified text during translation. A number of studies relevant to this article involve the transfer of phrasing schemes from one language to another. These studies have focussed on extrapolating information from a resourcerich to a resource-poor language. Yarowski et al. (2001) have used automatically word-aligned raw bilingual corpora to project annotations. Och and Ney (2004) use a two-stage process via a dynamic programming-type algorithm for aligning SL and TL tokens. Simard et al. (2005) propose a more advanced approach allowing noncontiguous phrases, to cover additional linguistic phenomena. Hwa et al. (2005) have created a parser for a new language based on a set of parallel sentences together with a parser in a frequently-used language, by transferring deeper syntactic structure and introducing fix-up rules. Smith et al. (2009) create a TL dependency parser by using bilingual text, a parser, and automatically-derived word alignments. 2 Basic functionality &amp; </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, Vol. 30, No. 4, pp. 417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th ACL Meeting,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="20896" citStr="Papineni et al., 2002" startWordPosition="3274" endWordPosition="3277">ation set comprises 200 isolated sentences, each with a single reference translation (Sofianopoulos et al., 2012). These sentences have been drawn from the internet via web crawling, being required to have a length of between 7 and 40 tokens each. 4.2 Experimental Results for PMG-simple Table 2 contains the translation accuracy results obtained with PMG-simple using the criteria of Table 1. In all experiments, the results concern the objective evaluation of the final translation, using four of the most widely used objective 11 evaluation metrics, namely BLEU, NIST, TER and METEOR (NIST, 2002, Papineni et al., 2002 &amp; Snover et al., 2006). For TER a lower value indicates a more successful translation while for other metrics, a higher value corresponds to a better translation. Since other components of the MT implementation do not change, this set of metrics provides an accurate end-to-end measurement of the effect of the phrasing model on the translation process. As can be seen from Table 2, all four criteria result in translations of a comparable accuracy. For instance, the variation between the lowest and highest BLEU scores is approximately 1%, while for the other metrics this variation is even lower.</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th ACL Meeting, Philadelphia, USA, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Dependency Treelet Translation: The convergence of statistical and example-based machine translation?</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<pages>43--65</pages>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Dependency Treelet Translation: The convergence of statistical and example-based machine translation? Machine Translation, Vol. 20, pp. 43–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="19420" citStr="Schmid, 1994" startWordPosition="3041" endWordPosition="3042">ults 4.1 Experiment Definition To evaluate the proposed phrasing generator, the output of the entire translation chain up to the final translation result is studied. This allows the contribution of different PMG models to be quantified using objective metrics. For the purposes of the present article, the language pair Greek-to-English (denoted as ELLEN) is employed. Since the SL phrasing generated by PMG is based on the TL phrasing scheme, the phrase labels of the resulting SL phrases are inherited from the TL ones. In the experiments reported here (with English as TL), the TreeTagger parser (Schmid, 1994) is used. Thus the SL-side phrase types include PC, VC, ADVC and ADJC. As TreeTagger also allows for certain words (such as conjunctions) to remain outside phrases, it is possible that isolated words occur in SL too. For the purposes of modelling such occurrences, these words form single-token phrases, denoted as ISC (i.e. ISolated word Chunk). Both the parallel corpus and the evaluation dataset employed here have been established in the PRESEMT project, and are available over the web (cf. www.presemt.eu/data). The parallel corpus has been retrieved from the web (from an EU website discussing </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK, pp. 44-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL Conference,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="8096" citStr="Sha and Pereira, 2003" startWordPosition="1264" endWordPosition="1267">endency parser by using bilingual text, a parser, and automatically-derived word alignments. 2 Basic functionality &amp; design of phrasing model generator The default PMG implementation (Tambouratzis et al., 2011) adopts the CRF model (Lafferty at el., 2001, Wallach, 2004) to chunk each input 8 sentence into phrases. Earlier comparative experiments have established that CRF results in a higher accuracy of phrase detection than both probabilistic models (such as HMMs) and small parsers with manually-defined parsing rules. CRF has been used by several researchers for creating parsers (for instance Sha and Pereira, 2003, Tsuruoka et al., 2009). Due to the expressiveness of the underlying mathematical model, CRF requires a large number of training patterns to extract an accurate model. Of course, the volume of training patterns is directly dependent on the size of the parallel corpus available. A more accurate CRF would require the use of a large parallel corpus, though this would compromise the portability to new language pairs. Even by moving from handling lemmas/tokens to part-of-speech tags when training the parser, to reduce the pattern space, it is hard to model accurately all possible phrase types via </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando C. N. Pereira. 2003. Shallow Parsing with Conditional Random Fields. In Proceedings of HLT-NAACL Conference, pp. 213-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Nicola Cancedda</author>
<author>Bruno Cavestro</author>
<author>Marc Dymetman</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Kenji Yamada</author>
<author>Philippe Langlais</author>
<author>Arne Mauser</author>
</authors>
<title>Translating with Non-Contiguous Phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conferences on Human Language Technology and on Empirical Methods in Language Processing,</booktitle>
<pages>755--762</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="7109" citStr="Simard et al. (2005)" startWordPosition="1110" endWordPosition="1113">l SL – TL sentences incorporating the PAM output to generate a phrasing model. This model is then employed to segment userspecified text during translation. A number of studies relevant to this article involve the transfer of phrasing schemes from one language to another. These studies have focussed on extrapolating information from a resourcerich to a resource-poor language. Yarowski et al. (2001) have used automatically word-aligned raw bilingual corpora to project annotations. Och and Ney (2004) use a two-stage process via a dynamic programming-type algorithm for aligning SL and TL tokens. Simard et al. (2005) propose a more advanced approach allowing noncontiguous phrases, to cover additional linguistic phenomena. Hwa et al. (2005) have created a parser for a new language based on a set of parallel sentences together with a parser in a frequently-used language, by transferring deeper syntactic structure and introducing fix-up rules. Smith et al. (2009) create a TL dependency parser by using bilingual text, a parser, and automatically-derived word alignments. 2 Basic functionality &amp; design of phrasing model generator The default PMG implementation (Tambouratzis et al., 2011) adopts the CRF model (L</context>
</contexts>
<marker>Simard, Cancedda, Cavestro, Dymetman, Gaussier, Goutte, Yamada, Langlais, Mauser, 2005</marker>
<rawString>Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada, Philippe Langlais, and Arne Mauser. 2005. Translating with Non-Contiguous Phrases. In Proceedings of the Conferences on Human Language Technology and on Empirical Methods in Language Processing, Vancouver, Canada, pp. 755-762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="20919" citStr="Snover et al., 2006" startWordPosition="3279" endWordPosition="3282">isolated sentences, each with a single reference translation (Sofianopoulos et al., 2012). These sentences have been drawn from the internet via web crawling, being required to have a length of between 7 and 40 tokens each. 4.2 Experimental Results for PMG-simple Table 2 contains the translation accuracy results obtained with PMG-simple using the criteria of Table 1. In all experiments, the results concern the objective evaluation of the final translation, using four of the most widely used objective 11 evaluation metrics, namely BLEU, NIST, TER and METEOR (NIST, 2002, Papineni et al., 2002 &amp; Snover et al., 2006). For TER a lower value indicates a more successful translation while for other metrics, a higher value corresponds to a better translation. Since other components of the MT implementation do not change, this set of metrics provides an accurate end-to-end measurement of the effect of the phrasing model on the translation process. As can be seen from Table 2, all four criteria result in translations of a comparable accuracy. For instance, the variation between the lowest and highest BLEU scores is approximately 1%, while for the other metrics this variation is even lower. Criterion BLEU NIST ME</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, Cambridge, Massachusetts, USA, pp. 223-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sokratis Sofianopoulos</author>
<author>Marina Vassiliou</author>
<author>George Tambouratzis</author>
</authors>
<title>Implementing a language-independent MT methodology.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Workshop on Multilingual Modeling, held within the ACL-2012 Conference, Jeju,</booktitle>
<pages>1--10</pages>
<location>Republic of</location>
<contexts>
<context position="6343" citStr="Sofianopoulos et al., 2012" startWordPosition="988" endWordPosition="992">. The SL–side segmentation is limited to phrase identification, rather than a detailed syntactic analysis. The processing of a bilingual corpus and the elicitation of the corresponding SL-to-TL phrasing information involves two PRESEMT modules: (i) The Phrase aligner module (PAM), which performs text alignment at word and phrase level within the parallel corpus. This languageindependent method identifies corresponding terms within the SL and TL sides of each sentence, and aligns the words between the two languages, while at the same time creating phrases for the non-parsed side of the corpus (Sofianopoulos et al., 2012). (ii) The Phrasing model generator (PMG), which elicits a phrasing model from this aligned parallel corpus. PMG is trained on the aligned parallel SL – TL sentences incorporating the PAM output to generate a phrasing model. This model is then employed to segment userspecified text during translation. A number of studies relevant to this article involve the transfer of phrasing schemes from one language to another. These studies have focussed on extrapolating information from a resourcerich to a resource-poor language. Yarowski et al. (2001) have used automatically word-aligned raw bilingual c</context>
<context position="20388" citStr="Sofianopoulos et al., 2012" startWordPosition="3193" endWordPosition="3196">unk). Both the parallel corpus and the evaluation dataset employed here have been established in the PRESEMT project, and are available over the web (cf. www.presemt.eu/data). The parallel corpus has been retrieved from the web (from an EU website discussing the history of the Union), with an average size of 18 words per sentence, while the smallest sentence comprises 4 words and the largest 38 words. Only minimal editing was performed in the parallel corpus, to ensure parallelism between SL and TL. The evaluation set comprises 200 isolated sentences, each with a single reference translation (Sofianopoulos et al., 2012). These sentences have been drawn from the internet via web crawling, being required to have a length of between 7 and 40 tokens each. 4.2 Experimental Results for PMG-simple Table 2 contains the translation accuracy results obtained with PMG-simple using the criteria of Table 1. In all experiments, the results concern the objective evaluation of the final translation, using four of the most widely used objective 11 evaluation metrics, namely BLEU, NIST, TER and METEOR (NIST, 2002, Papineni et al., 2002 &amp; Snover et al., 2006). For TER a lower value indicates a more successful translation while</context>
</contexts>
<marker>Sofianopoulos, Vassiliou, Tambouratzis, 2012</marker>
<rawString>Sokratis Sofianopoulos, Marina Vassiliou, and George Tambouratzis. 2012. Implementing a language-independent MT methodology. In Proceedings of the First Workshop on Multilingual Modeling, held within the ACL-2012 Conference, Jeju, Republic of Korea, 13 July, pp.1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinsong Su</author>
<author>Hua Wu</author>
</authors>
<title>Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong and Qun Liu.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th ACL Meeting, Jeju, Republic of Korea,</booktitle>
<pages>459--468</pages>
<marker>Su, Wu, 2012</marker>
<rawString>Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, Xiaodong Shi, Huailin Dong and Qun Liu. 2012. Translation Model Adaptation for Statistical Machine Translation with Monolingual Topic Information. In Proceedings of the 50th ACL Meeting, Jeju, Republic of Korea, pp. 459-468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tambouratzis</author>
<author>Fotini Simistira</author>
<author>Sokratis Sofianopoulos</author>
<author>Nikolaos Tsimboukakis</author>
<author>Marina Vassiliou</author>
</authors>
<title>A resource-light phrase scheme for language-portable MT.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th EAMT Conference,</booktitle>
<pages>30--31</pages>
<location>Leuven, Belgium,</location>
<contexts>
<context position="7685" citStr="Tambouratzis et al., 2011" startWordPosition="1200" endWordPosition="1203">or aligning SL and TL tokens. Simard et al. (2005) propose a more advanced approach allowing noncontiguous phrases, to cover additional linguistic phenomena. Hwa et al. (2005) have created a parser for a new language based on a set of parallel sentences together with a parser in a frequently-used language, by transferring deeper syntactic structure and introducing fix-up rules. Smith et al. (2009) create a TL dependency parser by using bilingual text, a parser, and automatically-derived word alignments. 2 Basic functionality &amp; design of phrasing model generator The default PMG implementation (Tambouratzis et al., 2011) adopts the CRF model (Lafferty at el., 2001, Wallach, 2004) to chunk each input 8 sentence into phrases. Earlier comparative experiments have established that CRF results in a higher accuracy of phrase detection than both probabilistic models (such as HMMs) and small parsers with manually-defined parsing rules. CRF has been used by several researchers for creating parsers (for instance Sha and Pereira, 2003, Tsuruoka et al., 2009). Due to the expressiveness of the underlying mathematical model, CRF requires a large number of training patterns to extract an accurate model. Of course, the volum</context>
</contexts>
<marker>Tambouratzis, Simistira, Sofianopoulos, Tsimboukakis, Vassiliou, 2011</marker>
<rawString>George Tambouratzis, Fotini Simistira, Sokratis Sofianopoulos, Nikolaos Tsimboukakis, and Marina Vassiliou. 2011. A resource-light phrase scheme for language-portable MT. In Proceedings of the 15th EAMT Conference, 30-31 May, Leuven, Belgium, pp. 185-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tambouratzis</author>
<author>Michalis Troullinos</author>
<author>Sokratis Sofianopoulos</author>
<author>Marina Vassiliou</author>
</authors>
<title>Accurate phrase alignment in a bilingual corpus for EBMT systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the 5th BUCC Workshop, held within the LREC-2012 Conference, May 26,</booktitle>
<pages>104--111</pages>
<location>Istanbul, Turkey,</location>
<marker>Tambouratzis, Troullinos, Sofianopoulos, Vassiliou, 2012</marker>
<rawString>George Tambouratzis, Michalis Troullinos, Sokratis Sofianopoulos, and Marina Vassiliou. 2012. Accurate phrase alignment in a bilingual corpus for EBMT systems. In Proceedings of the 5th BUCC Workshop, held within the LREC-2012 Conference, May 26, Istanbul, Turkey, pp. 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tambouratzis</author>
</authors>
<title>Sokratis Sofianopoulos, and Marina Vassiliou (2013) Language-independent hybrid MT with PRESEMT.</title>
<date></date>
<booktitle>In Proceedings of HYTRA-2013 Workshop, held within the ACL2013 Conference,</booktitle>
<volume>8</volume>
<pages>123--130</pages>
<location>Sofia, Bulgaria,</location>
<marker>Tambouratzis, </marker>
<rawString>George Tambouratzis, Sokratis Sofianopoulos, and Marina Vassiliou (2013) Language-independent hybrid MT with PRESEMT. In Proceedings of HYTRA-2013 Workshop, held within the ACL2013 Conference, Sofia, Bulgaria, 8 August, pp. 123-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Fast Full Parsing by LinearChain Conditional Random Fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th EACL Conference,</booktitle>
<volume>30</volume>
<pages>790--798</pages>
<location>Athens,</location>
<contexts>
<context position="8120" citStr="Tsuruoka et al., 2009" startWordPosition="1268" endWordPosition="1271"> bilingual text, a parser, and automatically-derived word alignments. 2 Basic functionality &amp; design of phrasing model generator The default PMG implementation (Tambouratzis et al., 2011) adopts the CRF model (Lafferty at el., 2001, Wallach, 2004) to chunk each input 8 sentence into phrases. Earlier comparative experiments have established that CRF results in a higher accuracy of phrase detection than both probabilistic models (such as HMMs) and small parsers with manually-defined parsing rules. CRF has been used by several researchers for creating parsers (for instance Sha and Pereira, 2003, Tsuruoka et al., 2009). Due to the expressiveness of the underlying mathematical model, CRF requires a large number of training patterns to extract an accurate model. Of course, the volume of training patterns is directly dependent on the size of the parallel corpus available. A more accurate CRF would require the use of a large parallel corpus, though this would compromise the portability to new language pairs. Even by moving from handling lemmas/tokens to part-of-speech tags when training the parser, to reduce the pattern space, it is hard to model accurately all possible phrase types via CRF (in particular for r</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii and Sophia Ananiadou. 2009. Fast Full Parsing by LinearChain Conditional Random Fields. In Proceedings of the 12th EACL Conference, Athens, Greece, 30 March-3 April, pp. 790–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Conditional Random Fields: An Introduction.</title>
<date>2004</date>
<tech>CIS Technical Report, MS-CIS-04-21. 24</tech>
<institution>February, University of Pennsylvania.</institution>
<contexts>
<context position="7745" citStr="Wallach, 2004" startWordPosition="1212" endWordPosition="1213">ed approach allowing noncontiguous phrases, to cover additional linguistic phenomena. Hwa et al. (2005) have created a parser for a new language based on a set of parallel sentences together with a parser in a frequently-used language, by transferring deeper syntactic structure and introducing fix-up rules. Smith et al. (2009) create a TL dependency parser by using bilingual text, a parser, and automatically-derived word alignments. 2 Basic functionality &amp; design of phrasing model generator The default PMG implementation (Tambouratzis et al., 2011) adopts the CRF model (Lafferty at el., 2001, Wallach, 2004) to chunk each input 8 sentence into phrases. Earlier comparative experiments have established that CRF results in a higher accuracy of phrase detection than both probabilistic models (such as HMMs) and small parsers with manually-defined parsing rules. CRF has been used by several researchers for creating parsers (for instance Sha and Pereira, 2003, Tsuruoka et al., 2009). Due to the expressiveness of the underlying mathematical model, CRF requires a large number of training patterns to extract an accurate model. Of course, the volume of training patterns is directly dependent on the size of </context>
</contexts>
<marker>Wallach, 2004</marker>
<rawString>Hanna M. Wallach. 2004. Conditional Random Fields: An Introduction. CIS Technical Report, MS-CIS-04-21. 24 February, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Toward machine translation with statistics and syntax and semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition &amp; Understanding,</booktitle>
<pages>13--17</pages>
<location>Merano, Italy,</location>
<contexts>
<context position="1555" citStr="Wu (2009)" startWordPosition="234" endWordPosition="235"> constraint on the volume of data required to create a functioning MT system. For this reason, a number of researchers involved in SMT have recently investigated the extraction of information from monolingual corpora, including lexical translation probabilities (Klementiev et al., 2012) and topic-specific information (Su et al., 2012). A related direction in MT research concerns hybrid MT (HMT), where principles from multiple MT paradigms are combined, such as for instance SMT and RBMT (Rule-based MT). HMT aims to combine the paradigms’ positive aspects to achieve higher translation accuracy. Wu (2009) has studied the trend of convergence of MT research towards hybrid systems. Quirk et al. (2007) have proposed an HMT system where statistical principles are combined with ExampleBased MT (EBMT) to improve the performance of SMT. The PRESEMT (www.presemt.eu) methodology (Tambouratzis et. al, 2013) supports rapid Abstract The present article focuses on improving the performance of a hybrid Machine Translation (MT) system, namely PRESEMT. The PRESEMT methodology is readily portable to new language pairs, and allows the creation of MT systems with minimal reliance on expensive resources. PRESEMT </context>
</contexts>
<marker>Wu, 2009</marker>
<rawString>Dekai Wu. 2009. Toward machine translation with statistics and syntax and semantics. In Proceedings of the IEEE Workshop on Automatic Speech Recognition &amp; Understanding, 13-17 November, Merano, Italy, pp. 12-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection across Aligned Corpora. In</title>
<date>2001</date>
<booktitle>NAACL-2001 Conference Proceedings,</booktitle>
<pages>200--207</pages>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection across Aligned Corpora. In NAACL-2001 Conference Proceedings, pp. 200-207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007401">
<title confidence="0.997868">
Augmenting String-to-Tree and Tree-to-String Translation with
Non-Syntactic Phrases
</title>
<author confidence="0.996778">
Matthias Huck and Hieu Hoang and Philipp Koehn
</author>
<affiliation confidence="0.998372">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.994733">
10 Crichton Street
Edinburgh EH8 9AB, UK
</address>
<email confidence="0.999058">
{mhuck,hhoang,pkoehn}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999368">
We present an effective technique to easily
augment GHKM-style syntax-based ma-
chine translation systems (Galley et al.,
2006) with phrase pairs that do not comply
with any syntactic well-formedness con-
straints. Non-syntactic phrase pairs are
distinguished from syntactic ones in or-
der to avoid harming effects. We apply
our technique in state-of-the-art string-to-
tree and tree-to-string setups. For tree-to-
string translation, we furthermore investi-
gate novel approaches for translating with
source-syntax GHKM rules in association
with input tree constraints and input tree
features.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999584126984127">
Syntax-based statistical machine translation sys-
tems utilize linguistic information that is obtained
by parsing the training data. In tree-to-string
translation, source-side syntactic tree annotation is
employed, while string-to-tree translation exploits
target-side syntax. The syntactic parse tree an-
notation constrains phrase extraction to syntacti-
cally well-formed phrase pairs: spans of syntactic
phrases must match constituents in the parse tree.
Standard phrase-based and hierarchical phrase-
based statistical machine translation systems, in
contrast, allow all phrase pairs that are consistent
with the word alignment (Koehn et al., 2003; Chi-
ang, 2005).
A restriction of the phrase inventory to syntac-
tically well-formed phrase pairs entails that possi-
bly valuable information from the training data re-
mains disregarded. While we would expect phrase
pairs that are not linguistically motivated to be less
reliable, discarding them altogether might be an
overly harsh decision. The quality of an inventory
of syntactic phrases depends heavily on the tree
annotation scheme and the quality of the syntac-
tic parses of the training data. Phrase pairs that
do not span constituents in the tree annotation ob-
tained from syntactic parses can provide reason-
able alternative segmentations or alternative trans-
lation options which prove to be valuable to the
decoder.
In this work, we augment the phrase invento-
ries of string-to-tree and tree-to-string translation
systems with phrase pairs that are not induced in
the syntax-based extraction. We extract continu-
ous phrases that are consistent with the word align-
ment, without enforcing any constraints with re-
spect to syntactic tree annotation. Non-syntactic
phrases are added as rules to the baseline syntactic
grammar with a fill-up technique. New rules are
only added if their right-hand side does not exist
yet. We extend the glue grammar with a special
glue rule to allow for application of non-syntactic
phrases during decoding. A feature in the log-
linear model combination serves to distinguish
non-syntactic phrases from syntactic ones. During
decoding, the decoder can draw on both syntactic
and non-syntactic phrase table entries and produce
derivations which resort to both types of phrases.
Such derivations yield hypotheses that make use of
the alternative segmentations and translation op-
tions provided through non-syntactic phrases. The
search space is more diverse, and in some cases
all hypotheses from purely syntax-based deriva-
tions score worse than a translation that applies
one or more non-syntactic phrases. We empiri-
cally demonstrate that this technique can lead to
substantial gains in translation quality.
Our syntactic translation models conform to the
GHKM syntax approach as proposed by Galley,
Hopkins, Knight, and Marcu (Galley et al.,
2004) with composed rules as in (Galley et al.,
2006) and (DeNeefe et al., 2007). State-of-the-
art GHKM string-to-tree systems have recently
shown very competitive performance in public
</bodyText>
<page confidence="0.985575">
486
</page>
<note confidence="0.7162115">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9979825">
evaluation campaigns (Nadejde et al., 2013; Bo-
jar et al., 2013). We apply the GHKM approach
not only in a string-to-tree setting as in previous
work, but employ it to build tree-to-string sys-
tems as well. We conduct tree-to-string translation
with text input and additionally adopt translation
with tree input and input tree constraints as sug-
gested for hierarchical translation by Hoang and
Koehn (2010). We also implement translation with
tree input and feature-driven soft tree matching.
The effect of augmenting the systems with non-
syntactic phrases is evaluated for all variants.
</bodyText>
<sectionHeader confidence="0.999012" genericHeader="introduction">
2 Outline
</sectionHeader>
<bodyText confidence="0.999912827586207">
The remainder of the paper is structured as fol-
lows: We review some of the basics of syntax-
based translation in the next section (Section 3)
and sketch the characteristics of our GHKM
string-to-tree and tree-to-string translation frame-
works.
In Section 4, we describe our technique to
augment GHKM-style syntax-based systems with
phrase pairs that do not comply with any syntactic
well-formedness constraints.
Section 5 contains the empirical part of the pa-
per. We first describe our experimental setup (5.1),
followed by a presentation of the translation re-
sults (5.2). We also include a few translation ex-
amples (5.3) in order to illustrate the differences
between the syntax-based baseline systems and
the setups augmented with non-syntactic phrases.
The empirical part is concluded with a brief dis-
cussion (5.4).
In the final part of the paper (Section 6), we
give a survey of previous work that has dealt
with problems related to overly restrictive syntac-
tic grammars for statistical machine translation,
inadequate syntactic parses, and insufficient cov-
erage of syntactic phrase inventories. A broad
spectrum of diverse methods has been proposed in
the literature, many of which are quite dissimilar
from ours but nevertheless related. We conclude
the paper in Section 7.
</bodyText>
<sectionHeader confidence="0.997739" genericHeader="method">
3 Syntax-based Translation
</sectionHeader>
<bodyText confidence="0.999811413793103">
In syntax-based translation, a probabilistic syn-
chronous context-free grammar (SCFG) is in-
duced from bilingual training corpora. The par-
allel training data is word-aligned and annotated
with syntactic parses on either target side (string-
to-tree), source side (tree-to-string), or both (tree-
to-tree). A syntactic phrase extraction procedure
extracts rules which are consistent with the word-
alignment and conform with certain syntactic va-
lidity constraints.
Extracted rules are of the form A,B —* (α,β,— ).
The right-hand side of the rule (α,β) is a bilingual
phrase pair that may contain non-terminal sym-
bols, i.e. α E (VF U NF)+ and β E (VE U NE)+,
where VF and VE denote the source and target
terminal vocabulary, and NF and NE denote the
source and target non-terminal vocabulary, respec-
tively. The non-terminals on the source side and
on the target side of rules are linked in a one-to-
one correspondence. The — relation defines this
one-to-one correspondence. The left-hand side
of the rule is a pair of source and target non-
terminals, A E NF and B E NE.
Decoding is typically carried out with a parsing-
based algorithm, in our case a customized version
of CYK+ (Chappelier and Rajman, 1998). The
parsing algorithm is extended to handle transla-
tion candidates and to incorporate language model
scores via cube pruning (Chiang, 2007).
</bodyText>
<subsectionHeader confidence="0.999343">
3.1 GHKM String-to-Tree Translation
</subsectionHeader>
<bodyText confidence="0.999936304347826">
In GHKM string-to-tree translation (Galley et al.,
2004; Galley et al., 2006; DeNeefe et al., 2007),
rules are extracted from training instances which
consist of a source sentence, a target sentence
along with its constituent parse tree, and a word
alignment matrix. This tuple is interpreted as a
directed graph (the alignment graph), with edges
pointing away from the root of the tree, and word
alignment links being edges as well. A set of
nodes (the frontier set) is determined that con-
tains only nodes with non-overlapping closure of
their spans.1 By computing frontier graph frag-
ments—fragments of the alignment graph such
that their root and all sinks are in the frontier set—
the GHKM extractor is able to induce a minimal
set of rules which explain the training instance.
The internal tree structure can be discarded to ob-
tain flat SCFG rules. Minimal rules can be assem-
bled to build larger composed rules.
Non-terminals on target sides of string-to-tree
rules are syntactified. The target non-terminal vo-
cabulary of the SCFG contains the set of labels
of the frontier nodes, which is in turn a subset
</bodyText>
<footnote confidence="0.99212425">
1The span of a node in the alignment graph is defined
as the set of source-side words that are reachable from this
node. The closure of a span is the smallest interval of source
sentence positions that covers the span.
</footnote>
<page confidence="0.997303">
487
</page>
<figure confidence="0.9648936">
TOP
PUNC.
CS-TOP
S-T
.
</figure>
<bodyText confidence="0.99959175">
In the standard phrase-based approach, in con-
trast, all continuous phrases that are consistent
with the word alignment are extracted (Och et al.,
1999; Och, 2002). The set of continuous bilingual
phrases BP(f1J,eI1,A), given a training instance
comprising a source sentence f1J , a target sentence
eI1, and a word alignment A ⊆ {1,...,I}×{1,...,J},
is defined as follows:
</bodyText>
<equation confidence="0.9996682">
{BP(f1J,eI1,A) =
hfj2
j1 ,ei2
i1i : ∃(i, j) ∈ A : i1 ≤ i ≤ i2 ∧ j1 ≤ j ≤ j2
}∧∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2
</equation>
<bodyText confidence="0.999409636363636">
Consistency for continuous phrases is based upon
merely two constraints in this definition: (1.) At
least one source and target position within the
phrase must be aligned, and (2.) words from inside
the source phrase may only be aligned to words
from inside the target phrase and vice versa. The
highlighted phrase pair from the example does not
violate these constraints.
In order to augment our GHKM syntax-based
systems with non-syntactic phrases, we obey the
following procedure:
</bodyText>
<listItem confidence="0.969942269230769">
• The set BP is extracted from all training in-
stances, and phrase translation probabilities
are computed separately from those in the
syntactic phrase inventory.
• Non-syntactic phrases are converted to rules
by providing a special left-hand side non-
terminal X.
• A phrase table fill-up method is applied to
enhance the syntactic phrase inventory with
entries from the non-syntactic phrase inven-
tory. Non-syntactic rules are only added to
the final grammar if no syntactic rule with
the same (source and target) right-hand side
is present. This method is inspired by pre-
vious work in domain adaptation (Bisazza et
al., 2011).
• The glue grammar is extended with a new
glue rule
X,Q → hX∼0X∼1,Q∼0X∼1i
that enables the system to make use of non-
syntactic rules in decoding.
• A binary feature is added to the log-linear
model (Och and Ney, 2002) to distinguish
non-syntactic rules from syntactic ones, and
to be able to assign a tuned weight to the non-
syntactic part of the grammar.
</listItem>
<sectionHeader confidence="0.968552" genericHeader="method">
5 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999984636363636">
We evaluate the effect of augmenting GHKM
syntax-based translation systems—both string-to-
tree and tree-to-string—with non-syntactic phrase
pairs on the English→German language pair using
the standard newstest sets of the Workshop on Sta-
tistical Machine Translation (WMT) for testing.2
The experiments are conducted with the open-
source Moses implementations of GHKM rule ex-
traction (Williams and Koehn, 2012) and decoding
with CYK+ parsing and cube pruning (Hoang et al.,
2009).
</bodyText>
<subsectionHeader confidence="0.984523">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999987878787879">
We work with an English–German parallel train-
ing corpus of around 4.5 M sentence pairs (af-
ter corpus cleaning). The parallel data origi-
nates from three different sources which have
been eligible for the constrained track of the
ACL 2014 Ninth Workshop on Statistical Ma-
chine Translation shared translation task: Europarl
(Koehn, 2005), News Commentary, and the Com-
mon Crawl corpus as provided on the WMT web-
site. Word alignments are created by aligning the
data in both directions with MGIZA++ (Gao and
Vogel, 2008) and symmetrizing the two trained
alignments (Och and Ney, 2003; Koehn et al.,
2003). For string-to-tree translation, we parse the
German target side with BitPar (Schmid, 2004).3
For tree-to-string translation, we parse the English
source side of the parallel data with the English
Berkeley Parser (Petrov et al., 2006).
When extracting syntactic phrases, we impose
several restrictions for composed rules, in partic-
ular a maximum number of twenty tree nodes per
rule, a maximum depth of five, and a maximum
size of five. We discard rules with non-terminals
on their right-hand side if they are singletons in the
training data.
Only the 100 best translation options per dis-
tinct source side with respect to the weighted
phrase-level model scores are loaded by the de-
coder. The decoder is configured with a maximum
chart span of 25 and a rule limit of 100.
A standard set of models is used in the base-
lines, comprising phrase translation probabilities
and lexical translation probabilities in both direc-
</bodyText>
<footnote confidence="0.9871715">
2http://www.statmt.org/wmt14/
translation-task.html
3We remove grammatical case and function information
from the annotation obtained with BitPar.
</footnote>
<page confidence="0.994246">
489
</page>
<table confidence="0.999569642857143">
system dev TER newstest2013 TER newstest2014 TER
BLEU BLEU BLEU
phrase-based 33.0 48.8 18.8 64.5 18.2 66.9
+ lexicalized reordering 34.2 48.1 19.2 64.5 18.3 67.1
string-to-string (syntax-directed extraction) 32.6 49.4 18.2 o +0.5 65.4 o −0.4 17.8 o +0.5 68.0 o−0.4
+ non-syntactic phrases 33.4 49.0 18.7 65.0 18.3 67.6
string-to-tree 33.6 48.7 19.5 o +0.3 63.9 o −0.3 18.6 o +0.5 66.9 o −0.7
+ non-syntactic phrases 34.3 48.0 19.8 63.6 19.1 66.2
tree-to-string 34.0 48.5 19.5 o −0.2 63.8 o +0.2 18.5 o +0.2 67.0 o−0.4
+ non-syntactic phrases 33.9 48.4 19.3 +0.4 64.0 −0.3 18.7 +0.3 66.6
+ input tree constraints 33.7 48.4 19.3 o +0.3 63.9 o −0.3 18.3 o +0.2 67.0 o−0.5
+ non-syntactic phrases 34.2 48.2 19.7 63.6 18.7 66.5
+ input tree features 34.3 48.3 19.6 o 63.7 o 18.6 o 67.0 o−0.5
+ non-syntactic phrases 34.4 48.1 19.9 63.4 18.8 66.5
</table>
<tableCaption confidence="0.999966">
Table 1: English-*German experimental results (truecase). BLEU scores are given in percentage.
</tableCaption>
<bodyText confidence="0.999959">
tions, word and phrase penalty, an n-gram lan-
guage model, a rule rareness penalty, and the
monolingual PCFG probability of the tree frag-
ment from which the rule was extracted (Williams
et al., 2014). Phrase translation probabilities are
smoothed via Good-Turing smoothing.
The language model (LM) is a large inter-
polated 5-gram LM with modified Kneser-Ney
smoothing (Kneser and Ney, 1995; Chen and
Goodman, 1998). The target side of the parallel
corpus and the monolingual German News Crawl
corpora are employed as training data. We use
the SRILM toolkit (Stolcke, 2002) to train the LM
and rely on KenLM (Heafield, 2011) for language
model scoring during decoding.
Model weights are optimized to maximize
BLEU (Papineni et al., 2002) with batch MIRA
(Cherry and Foster, 2012) on 1000-best lists. We
selected 2000 sentences from the newstest2008-
2012 sets as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and do each contain less than
30 words for more rapid tuning. newstest2013 and
newstest2014 are used as unseen test sets. Trans-
lation quality is measured in truecase with BLEU
and TER (Snover et al., 2006).4
We apply a phrase length limit of five when
extracting non-syntactic phrases for the fill-up of
syntactic phrase tables.
</bodyText>
<footnote confidence="0.866521">
4TER scores are computed with tercom version 0.7.25
and parameters -N -s.
</footnote>
<subsectionHeader confidence="0.998813">
5.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.999824">
Table 1 comprises the results of our empirical eval-
uation of the translation quality achieved by the
different systems.
</bodyText>
<subsubsectionHeader confidence="0.606971">
5.2.1 Phrase-based Baselines
</subsubsectionHeader>
<bodyText confidence="0.999964666666667">
We set up two phrase-based baselines for com-
parison. Their set of models is the same as for
the syntax-based baselines, with the exception of
the PCFG probability. One of the phrase-based
systems moreover utilizes a lexicalized reorder-
ing model (Galley and Manning, 2008). No non-
standard advanced features (like an operation se-
quence model or class-based LMs) are engrafted.
The maximum phrase length is five, search is car-
ried out with cube pruning at a k-best limit of
1000. A maximum number of 100 translation op-
tions per source side are taken into account.
</bodyText>
<subsectionHeader confidence="0.777111">
5.2.2 String-to-String Contrastive System
</subsectionHeader>
<bodyText confidence="0.9999824">
A further contrastive experiment is done with a
string-to-string system. The extraction method
for this string-to-string system is GHKM syntax-
directed with syntactic target-side annotation from
BitPar, as in the string-to-tree setup. We actually
extract the same rules but strip off the syntactic la-
bels. The final grammar contains rules with a sin-
gle generic non-terminal instead of syntactic ones.
Note that a side effect of this is that the phrase
inventory of the string-to-string system contains
</bodyText>
<page confidence="0.989662">
490
</page>
<bodyText confidence="0.999945941176471">
a larger amount of hierarchical phrases5 than the
string-to-tree system, though the same rules are
extracted. The reason is that we discard single-
ton hierarchical rules when we normalize the fre-
quencies after extraction. Many rules that are sin-
gletons when the syntax decoration is taken into
account have in fact been seen multiple times if
syntactic labels are not distinguished, due to pool-
ing of counts.
The string-to-string system is on newstest2013
1.0 points BLEU worse than the phrase-based
system with lexicalized reordering and on news-
test2014 0.5 points BLEU. We gain 0.5 points
BLEU on both of the test sets if we augment the
string-to-string system with non-syntactic phrases
from the standard phrase-based extractor accord-
ing to our procedure from Section 4.
</bodyText>
<subsubsectionHeader confidence="0.727045">
5.2.3 String-to-Tree System
</subsubsectionHeader>
<bodyText confidence="0.999979818181818">
The translation quality of the string-to-tree sys-
tem surpasses the translation quality of the bet-
ter phrase-based baseline slightly (by 0.3 points
BLEU on both test sets). The string-to-tree system
is clearly superior to the string-to-string system,
which verifies that syntactic non-terminals are in-
deed vital. We get a nice gain of 0.5 points BLEU
and 0.7 points TER on newstest2014 if we aug-
ment the string-to-tree system with non-syntactic
phrases. The phrase-based system is outperformed
by 0.8 points BLEU.
</bodyText>
<subsubsectionHeader confidence="0.580746">
5.2.4 Tree-to-String Systems
</subsubsectionHeader>
<bodyText confidence="0.98358205">
The tree-to-string baseline with text input per-
forms at the level of the string-to-tree baseline, but
augmenting it with non-syntactic phrases yields
only a small improvement or even harms a little
(on newstest2013).
Decoding with tree input and input tree con-
straints causes a minor loss in translation qual-
ity. We however observed a decoding speed-up. If
we employ non-syntactic phrases to augment the
tree-to-string setup with input tree constraints, we
provide the new non-syntactic rules in the gram-
mar with a particular property: their left-hand side
non-terminal X can match any constituent span in
the input sentence. The decoder would not be
able to utilize non-syntactic phrases without this
relaxation. Syntactic phrases amount to an in-
crease of up to 0.4 points BLEU (newstest2013)
5We define hierarchical phrases as rules with non-
terminals on their right-hand side, in contrast to lexical
phrases which are continuous rules with right-hand sides that
contain terminal symbols only.
and 0.5 points TER (newstest2014) in the tree-
constrained setup.
Our best tree-to-string setup takes tree input, but
involves soft matching features instead of hard in-
put tree constraints. We incorporate two features,
one that fires for matches and another one that fires
for mismatches. The motivation for not relying on
just one feature which would penalize mismatches
is that the number of syntactic non-terminals in
the derivation can differ between hypotheses. Not
all constituent spans need to be matched (or mis-
matched) by non-terminals, some can be over-
laid through larger rules.6 Tree-to-string transla-
tion with input tree features benefits from being
augmented with non-syntactic phrases by 0.2 to
0.3 points BLEU. The resulting system is mini-
mally better than the best string-to-tree system on
newstest2013, and slightly worse than it on news-
test2014.
</bodyText>
<subsectionHeader confidence="0.992065">
5.3 Translation Examples
</subsectionHeader>
<bodyText confidence="0.999930892857143">
We illustrate the differences between the syntax-
based baseline systems and the setups augmented
with non-syntactic phrases by means of two trans-
lation examples from newstest2014. Both exam-
ples are string-to-tree translations.
Figures 2 and 3 depict an example that cor-
responds well to the word-aligned training sen-
tence pair with target-side syntactic annotation
from Figure 1. Figure 2 shows the translation, seg-
mentation, and parse tree derived by the string-
to-tree baseline system as single-best output for
the preprocessed input sentence: “the lessees were
against this and also wanted longer terms .” The
reference translation is: “Die Pächter waren dage-
gen und wollten zudem längere Laufzeiten.” Fig-
ure 3 shows the translation, segmentation, and
parse tree derived by the string-to-tree system aug-
mented with non-syntactic phrases. There are
two word substitutions with respect to the ref-
erence in the latter translation, but they convey
the same meaning. The baseline translation fails
to convey the meaning, mostly because “terms”
is translated to the verb “gesehen”, which is a
wrong syntactic analysis in the given context. In-
terestingly, the segmentation applied by the two
systems is rather similar, apart from the interval
“also wanted” which cannot be translated en bloc
by the baseline. All rules in the baseline gram-
</bodyText>
<footnote confidence="0.9917915">
6Also remember that we discarded the internal tree struc-
ture to obtain flat SCFG rules.
</footnote>
<page confidence="0.994872">
491
</page>
<figure confidence="0.9991784">
Q
Q
z
&lt;/s&gt;
Q
TOP
S-TOP
.
&lt;/s&gt;
AP-
</figure>
<page confidence="0.997068">
493
</page>
<subsectionHeader confidence="0.795169">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999993708333333">
A drawback of our method is that it increases
the size of the synchronous context-free gram-
mar massively. Most phrase pairs from standard
phrase-based extraction are actually not present in
the GHKM rule set, even with composed rules.
A large fraction of the extracted non-syntactic
phrases is such added to the phrase inventory
through phrase table fill-up. Table 2 shows the
phrase inventory statistics for the different sys-
tems.
Another question relates to the glue rule appli-
cations. The application of a non-syntactic rule
is always accompanied with a respective glue rule
application in our implementation. The string-
to-tree baseline utilizes glue rules on average 3.0
times in each single-best translation (measured
on newstest2014), the string-to-tree system aug-
mented with non-syntactic phrases utilizes glue
rules on average 7.0 times. We considered an im-
plementation that allows for embedding of non-
syntactic rules into hierarchical rules (other than
the glue rules) but did not see improvements with
it as yet. Furthermore, efficiency concerns become
more relevant in such an implementation.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="evaluation">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999554">
Issues with overly restrictive syntactic grammars
for statistical machine translation, inadequate syn-
tactic parses, and insufficient coverage have been
tackled from several different directions in the lit-
erature.
A proposed approach to attain better syntac-
tic phrase inventories is to restructure the syntac-
tic parse trees in a preprocessing step (Wang et
al., 2007; Wang et al., 2010; Burkett and Klein,
2012). This line of research aims at rearranging
parse trees in a way that makes them a better fit
for the requirements of the bilingual downstream
application. Conversely, Fossum et al. (2008) re-
tain the structure of the parse trees and modify the
word alignments.
Marcu et al. (2006) relax syntactic phrase ex-
traction constraints in their SPMT Model 2 to al-
low for phrases that do not match the span of one
single constituent in the parse tree. SPMT Model 2
rules are created from spans that are consistent
with the word alignment and covered by multiple
constituents such that the union of the constituents
matches the span. Pseudo non-syntactic non-
terminals are introduced for the left-hand sides of
SPMT Model 2 rules. Special additional rules al-
low for combination of those non-syntactic left-
hand side non-terminals with genuine syntactic
non-terminals on the right-hand sides of other
rules during decoding.
Another line of research took the hierarchical
phrase-based model (Chiang, 2005; Chiang, 2007)
as a starting point and extended it with syntactic
enhancements. In their SAMT system, Zollmann
and Venugopal (2006) labeled the non-terminals
of the hierarchical model with composite symbols
derived from the syntactic tree annotation. Similar
methods have been applied with CCG labels (Al-
maghout et al., 2012). Venugopal et al. (2009)
and Stein et al. (2010) keep the grammar of the
non-terminals of the hierarchical model unlabeled
and apply the syntactic information in a separate
model. Other authors added features which fire
for phrases complying with certain syntactic prop-
erties while retaining all phrase pairs of the hier-
archical model (Marton and Resnik, 2008; Vilar et
al., 2008).
In a tree-to-tree translation setting, Chiang
(2010) proposed techniques to soften the syntac-
tic constraints. A fuzzy approach with complex
non-terminal symbols as in SAMT is employed
to overcome the limitations during phrase extrac-
tion. In decoding, substitutions of non-terminals
are not restricted to matching ones. Any left-
hand side non-terminal can substitute any right-
hand side non-terminal. The decoder decides on
the best derivation based on the tuned weights of a
large number of binary features.
Joining phrase inventories that come from mul-
tiple origins is a common method in domain adap-
tation (Bertoldi and Federico, 2009; Niehues and
Waibel, 2012) but has also been applied in the
contexts of lightly-supervised training (Schwenk,
2008; Huck et al., 2011) and of forced alignment
training (Wuebker et al., 2010). For our purposes,
we apply a fill-up method in the manner of the one
that has been shown to perform well for domain
adaptation in earlier work (Bisazza et al., 2011).
Previous research that resembles our work most
has been presented by Liu et al. (2006) and by
Hanneman and Lavie (2009).
Liu et al. (2006) allow for application of non-
syntactic phrase pairs in their tree-to-string align-
ment template (TAT) system. The translation
probabilities for the non-syntactic phrases are ob-
tained from a standard phrase-based extraction
</bodyText>
<page confidence="0.99813">
494
</page>
<bodyText confidence="0.919698">
pipeline. A non-syntactic phrase pair can how-
ever only be applied if its source side matches
a subtree in the parsed input sentence. Syn-
tactic and non-syntactic phrases are not distin-
guished, and overlap between the syntactic and
non-syntactic part of the phrase inventory is not
avoided. The decoder picks the entry with the
higher phrase translation probability, which means
that non-syntactic phrase table entries can super-
sede syntactic entries. The authors report im-
provements of 0.6 points BLEU on the 2005 NIST
Chinese→English task with four reference trans-
lations.
Hanneman and Lavie (2009) examine non-
syntactic phrases for tree-to-tree translation with
the Stat-XFER framework as developed at
Carnegie Mellon University (Lavie, 2008). They
combine syntactic and non-syntactic phrase in-
ventories and reestimate the probabilities for both
types of phrase pairs by adding up the observed
absolute frequencies. Two combination schemes
are evaluated: combination with all extractable
valid non-syntactic phrases (“direct combination”)
and combination with only those non-syntactic
phrases whose source sides are not equal to the
source side of any syntactic phrase (“syntax-
prioritized combination”). On a French→English
translation task, Hanneman and Lavie (2009) re-
port improvements of around 2.6 points BLEU by
adding non-syntactic phrases on top of their Stat-
XFER syntactic baselines. Their best setup how-
ever does not reach the performance of a stan-
dard phrase-based system, which is still 1.6 points
BLEU better.
Apart from the differences in the underly-
ing syntax-based translation technology (string-
to-tree/tree-to-string GHKM vs. TAT vs. Stat-
XFER), our work also constitutes a novel contri-
bution as compared to the previous approaches by
Liu et al. (2006) and Hanneman and Lavie (2009)
with respect to the following:
• The phrase inventory is augmented with non-
syntactic phrases by means of a fill-up tech-
nique. Overlap is prevented, whereas not
only new source sides, but also new target-
side translation options can be added.
• The probabilities of syntactic phrase pairs are
the same as in the syntax-based baseline, and
the probabilities of the non-syntactic phrase
pairs are the same as in a phrase-based sys-
tem. Counts of syntactic and non-syntactic
phrases are not summed up to obtain new es-
timates.
</bodyText>
<listItem confidence="0.913096666666667">
• Non-syntactic phrase pairs are distinguished
from syntactic ones with an additional fea-
ture.
</listItem>
<sectionHeader confidence="0.98631" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999948764705882">
String-to-tree and tree-to-string translation sys-
tems can easily be augmented with non-syntactic
phrases by means of phrase table fill-up, a special
non-terminal symbol for left-hand sides of non-
syntactic rules in the grammar, and an additional
glue rule. A binary feature enables the system to
distinguish non-syntactic phrases from syntactic
ones and—on the basis of the respective feature
weight—to favor syntactically motivated phrases
during decoding.
Our results on an English→German translation
task demonstrate the beneficial effect of augment-
ing GHKM translation systems with non-syntactic
phrase pairs. Empirical gains in translation qual-
ity are up to 0.5 points BLEU and 0.7 points TER
over the baseline on the recent test set of the shared
translation task of the ACL 2014 Ninth Workshop
on Statistical Machine Translation.
While GHKM-style syntactic translation has
typically been utilized in string-to-tree settings in
previous research, we have also adopted it to build
tree-to-string systems in this work. Source syn-
tax establishes interesting further directions for
GHKM systems. We investigated two of them: in-
put tree constraints and input tree features.
String-to-tree and tree-to-string GHKM sys-
tems perform roughly at the same level in terms
of translation quality. Our best string-to-tree
setup outperforms a phrase-based baseline by up
to 0.8 points BLEU and 0.9 points TER (on
newstest2014), our best tree-to-string setup out-
performs the phrase-based baseline by up to
0.7 points BLEU and 1.1 points TER (on news-
test2013).
</bodyText>
<sectionHeader confidence="0.984579" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998636">
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements no 287658 (EU-BRIDGE)
and no 288487 (MosesCore).
</bodyText>
<page confidence="0.999189">
495
</page>
<sectionHeader confidence="0.98409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999165691588785">
Hala Almaghout, Jie Jiang, and Andy Way. 2012. Ex-
tending CCG-based Syntactic Constraints in Hierar-
chical Phrase-Based SMT. In Proc. of the Annual
Conf. of the European Assoc. for Machine Transla-
tion (EAMT), pages 193–200, Trento, Italy, May.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation with
Monolingual Resources. In Proc. of the Workshop
on Statistical Machine Translation (WMT), pages
182–189, Athens, Greece, March.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proc. of the
Int. Workshop on Spoken Language Translation
(IWSLT), pages 136–143, San Francisco, CA, USA,
December.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut,
and Lucia Specia. 2013. Findings of the 2013
Workshop on Statistical Machine Translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation (WMT), pages 1–44, Sofia, Bulgaria, August.
David Burkett and Dan Klein. 2012. Transforming
Trees to Improve Syntactic Convergence. In Proc.
of the Conf. on Empirical Methods for Natural Lan-
guage Processing (EMNLP), Jeju Island, South Ko-
rea, July.
Jean-Cédric Chappelier and Martin Rajman. 1998. A
Generalized CYK Algorithm for Parsing Stochas-
tic CFG. In Proc. of the First Workshop on Tab-
ulation in Parsing and Deduction, pages 133–137,
Paris, France, April.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, USA, August.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proc. of the Human Language Technology Conf. /
North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 427–436,
Montréal, Canada, June.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
of the Annual Meeting of the Assoc. for Computa-
tional Linguistics (ACL), pages 263–270, Ann Ar-
bor, MI, USA, June.
David Chiang. 2007. Hierarchical Phrase-Based
Translation. Computational Linguistics, 33(2):201–
228, June.
David Chiang. 2010. Learning to Translate with
Source and Target Syntax. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 1443–1452, Uppsala, Sweden, July.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What Can Syntax-Based MT Learn
from Phrase-Based MT? In Proc. of the 2007
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 755–763,
Prague, Czech Republic, June.
Victoria Fossum, Kevin Knight, and Steven Abney.
2008. Using Syntax to Improve Word Alignment
Precision for Syntax-Based Machine Translation. In
Proc. of the Workshop on Statistical Machine Trans-
lation (WMT), pages 44–52, Columbus, OH, USA,
June.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of the Conf. on Empirical Meth-
ods for Natural Language Processing (EMNLP),
pages 847–855, Honolulu, HI, USA, October.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. of the Human Language Technology Conf.
/North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273–280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961–
968, Sydney, Australia, July.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ’08, pages 49–
57, Columbus, OH, USA, June.
Greg Hanneman and Alon Lavie. 2009. Decoding with
Syntactic and Non-syntactic Phrases in a Syntax-
based Machine Translation System. In Proceedings
of the Third Workshop on Syntax and Structure in
Statistical Translation, SSST ’09, pages 1–9, Boul-
der, CO, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proc. of the Workshop
on Statistical Machine Translation (WMT), pages
187–197, Edinburgh, Scotland, UK, July.
Hieu Hoang and Philipp Koehn. 2010. Improved
Translation with Source Syntax Labels. In Proc. of
the Workshop on Statistical Machine Translation
(WMT), pages 409–417, Uppsala, Sweden, July.
</reference>
<page confidence="0.995564">
496
</page>
<reference confidence="0.99838028440367">
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. In Proc. of the Int. Workshop on Spoken Lan-
guage Translation (IWSLT), pages 152–159, Tokyo,
Japan, December.
Matthias Huck, David Vilar, Daniel Stein, and Her-
mann Ney. 2011. Lightly-Supervised Training for
Hierarchical Phrase-Based Machine Translation. In
Proc. of the EMNLP 2011 Workshop on Unsuper-
vised Learning in NLP, pages 91–96, Edinburgh,
Scotland, UK, July.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing, vol-
ume 1, pages 181–184, Detroit, MI, USA, May.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Proc.
of the Human Language Technology Conf. / North
American Chapter of the Assoc. for Computational
Linguistics (HLT-NAACL), pages 127–133, Edmon-
ton, Canada, May/June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. of the MT
Summit X, Phuket, Thailand, September.
Alon Lavie. 2008. Stat-XFER: A General
Search-Based Syntax-Driven Framework for Ma-
chine Translation. In Alexander Gelbukh, editor,
Computational Linguistics and Intelligent Text Pro-
cessing, volume 4919 of Lecture Notes in Computer
Science, pages 362–375. Springer Berlin Heidel-
berg.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string Alignment Template for Statistical Machine
Translation. In Proc. of the 21st International Conf.
on Computational Linguistics and the 44th Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 609–616, Sydney, Australia, July.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proc. of the Conf. on Empirical Methods
for Natural Language Processing (EMNLP), pages
44–52, Sydney, Australia.
Yuval Marton and Philip Resnik. 2008. Soft Syn-
tactic Constraints for Hierarchical Phrased-Based
Translation. In Proc. of the Annual Meeting of the
Assoc. for Computational Linguistics (ACL), pages
1003–1011, Columbus, OH, USA, June.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh’s Syntax-Based Machine Transla-
tion Systems. In Proc. of the Workshop on Statistical
Machine Translation (WMT), pages 170–176, Sofia,
Bulgaria, August.
Jan Niehues and Alex Waibel. 2012. Detailed Analy-
sis of Different Strategies for Phrase Table Adapta-
tion in SMT. In Proc. of the Conf. of the Assoc. for
Machine Translation in the Americas (AMTA), San
Diego, CA, USA, October/November.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive Training and Maximum Entropy Models for Sta-
tistical Machine Translation. In Proc. of the Annual
Meeting of the Assoc. for Computational Linguistics
(ACL), pages 295–302, Philadelphia, PA, USA, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved Alignment Models for
Statistical Machine Translation. In Proc. of the
Joint SIGDAT Conf. on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP99), pages 20–28, University of Maryland,
College Park, MD, USA, June.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen University,
Aachen, Germany, October.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
Annual Meeting of the Assoc. for Computational
Linguistics (ACL), pages 311–318, Philadelphia, PA,
USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433–440, Sydney,
Australia, July.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Holger Schwenk. 2008. Investigations on Large-Scale
Lightly-Supervised Training for Statistical Machine
Translation. In Proc. of the Int. Workshop on Spo-
ken Language Translation (IWSLT), pages 182–189,
Waikiki, HI, USA, October.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proc. of the Conf. of the Assoc. for
Machine Translation in the Americas (AMTA), pages
223–231, Cambridge, MA, USA, August.
</reference>
<page confidence="0.981205">
497
</page>
<reference confidence="0.999837894736843">
Daniel Stein, Stephan Peitz, David Vilar, and Hermann
Ney. 2010. A Cocktail of Deep Syntactic Fea-
tures for Hierarchical Machine Translation. In Proc.
of the Conf. of the Assoc. for Machine Translation
in the Americas (AMTA), Denver, CO, USA, Octo-
ber/November.
Andreas Stolcke. 2002. SRILM – an Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Spoken Language Processing (ICSLP), volume 3,
Denver, CO, USA, September.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference Grammars:
Softening Syntactic Constraints to Improve Statis-
tical Machine Translation. In Proc. of the Hu-
man Language Technology Conf. /North American
Chapter of the Assoc. for Computational Linguistics
(HLT-NAACL), pages 236–244, Boulder, CO, USA,
June.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing Soft Syntax Features and Heuristics for
Hierarchical Phrase Based Machine Translation. In
Proc. of the Int. Workshop on Spoken Language
Translation (IWSLT), pages 190–197, Waikiki, HI,
USA, October.
Wei Wang, Kevin Knight, and Daniel Marcu. 2007.
Binarizing Syntax Trees to Improve Syntax-Based
Machine Translation Accuracy. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 746–754, Prague, Czech Republic,
June.
Wei Wang, Jonathan May, Kevin Knight, and Daniel
Marcu. 2010. Re-structuring, Re-labeling, and
Re-aligning for Syntax-based Machine Translation.
Computational Linguistics, 36(2):247–277, June.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proc. of the Workshop on Statistical Machine Trans-
lation (WMT), pages 388–394, Montréal, Canada,
June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh’s Syntax-Based Systems at
WMT 2014. In Proc. of the Workshop on Statistical
Machine Translation (WMT), Baltimore, MD, USA,
June.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training Phrase Translation Models with
Leaving-One-Out. In Proc. of the Annual Meeting
of the Assoc. for Computational Linguistics (ACL),
pages 475–484, Uppsala, Sweden, July.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax Augmented Machine Translation via Chart Pars-
ing. In Proc. of the Workshop on Statistical Machine
Translation (WMT), pages 138–141, New York City,
NY, USA, June.
</reference>
<page confidence="0.998069">
498
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.426467">
<title confidence="0.999203">Augmenting String-to-Tree and Tree-to-String Translation Non-Syntactic Phrases</title>
<author confidence="0.999873">Huck Hoang</author>
<affiliation confidence="0.9972945">School of University of</affiliation>
<address confidence="0.652901">10 Crichton Edinburgh EH8 9AB,</address>
<email confidence="0.995363">mhuck@inf.ed.ac.uk</email>
<email confidence="0.995363">hhoang@inf.ed.ac.uk</email>
<email confidence="0.995363">pkoehn@inf.ed.ac.uk</email>
<abstract confidence="0.9947844375">We present an effective technique to easily augment GHKM-style syntax-based machine translation systems (Galley et al., 2006) with phrase pairs that do not comply with any syntactic well-formedness constraints. Non-syntactic phrase pairs are distinguished from syntactic ones in order to avoid harming effects. We apply our technique in state-of-the-art string-totree and tree-to-string setups. For tree-tostring translation, we furthermore investigate novel approaches for translating with source-syntax GHKM rules in association with input tree constraints and input tree features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hala Almaghout</author>
<author>Jie Jiang</author>
<author>Andy Way</author>
</authors>
<title>Extending CCG-based Syntactic Constraints in Hierarchical Phrase-Based SMT.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Conf. of the European Assoc. for Machine Translation (EAMT),</booktitle>
<pages>193--200</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="23982" citStr="Almaghout et al., 2012" startWordPosition="3790" endWordPosition="3794">des of SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations durin</context>
</contexts>
<marker>Almaghout, Jiang, Way, 2012</marker>
<rawString>Hala Almaghout, Jie Jiang, and Andy Way. 2012. Extending CCG-based Syntactic Constraints in Hierarchical Phrase-Based SMT. In Proc. of the Annual Conf. of the European Assoc. for Machine Translation (EAMT), pages 193–200, Trento, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain Adaptation for Statistical Machine Translation with Monolingual Resources.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>182--189</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="24998" citStr="Bertoldi and Federico, 2009" startWordPosition="3950" endWordPosition="3953">ree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) s</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain Adaptation for Statistical Machine Translation with Monolingual Resources. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 182–189, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation.</title>
<date>2011</date>
<booktitle>In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>136--143</pages>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="10269" citStr="Bisazza et al., 2011" startWordPosition="1612" endWordPosition="1615">he set BP is extracted from all training instances, and phrase translation probabilities are computed separately from those in the syntactic phrase inventory. • Non-syntactic phrases are converted to rules by providing a special left-hand side nonterminal X. • A phrase table fill-up method is applied to enhance the syntactic phrase inventory with entries from the non-syntactic phrase inventory. Non-syntactic rules are only added to the final grammar if no syntactic rule with the same (source and target) right-hand side is present. This method is inspired by previous work in domain adaptation (Bisazza et al., 2011). • The glue grammar is extended with a new glue rule X,Q → hX∼0X∼1,Q∼0X∼1i that enables the system to make use of nonsyntactic rules in decoding. • A binary feature is added to the log-linear model (Och and Ney, 2002) to distinguish non-syntactic rules from syntactic ones, and to be able to assign a tuned weight to the nonsyntactic part of the grammar. 5 Empirical Evaluation We evaluate the effect of augmenting GHKM syntax-based translation systems—both string-totree and tree-to-string—with non-syntactic phrase pairs on the English→German language pair using the standard newstest sets of the </context>
<context position="25356" citStr="Bisazza et al., 2011" startWordPosition="4011" endWordPosition="4014">ighthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction 494 pipeline. A non-syntactic phrase pair can however only be applied if its source side matches a subtree in the parsed input sentence. Syntactic and non-syntactic phrases are not distinguished, and overlap between the syntactic and non-</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation. In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT), pages 136–143, San Francisco, CA, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4105" citStr="Bojar et al., 2013" startWordPosition="593" endWordPosition="597"> can lead to substantial gains in translation quality. Our syntactic translation models conform to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). State-of-theart GHKM string-to-tree systems have recently shown very competitive performance in public 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt translation with tree input and input tree constraints as suggested for hierarchical translation by Hoang and Koehn (2010). We also implement translation with tree input and feature-driven soft tree matching. The effect of augmenting the systems with nonsyntactic phrases is evaluated for all variants. 2 Outline The remainder of the paper is structured as follows: We review some </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 1–44, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>Transforming Trees to Improve Syntactic Convergence.</title>
<date>2012</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), Jeju Island, South Korea,</booktitle>
<contexts>
<context position="22667" citStr="Burkett and Klein, 2012" startWordPosition="3581" endWordPosition="3584">f nonsyntactic rules into hierarchical rules (other than the glue rules) but did not see improvements with it as yet. Furthermore, efficiency concerns become more relevant in such an implementation. 6 Related Work Issues with overly restrictive syntactic grammars for statistical machine translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constitu</context>
</contexts>
<marker>Burkett, Klein, 2012</marker>
<rawString>David Burkett and Dan Klein. 2012. Transforming Trees to Improve Syntactic Convergence. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), Jeju Island, South Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Cédric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A Generalized CYK Algorithm for Parsing Stochastic CFG.</title>
<date>1998</date>
<booktitle>In Proc. of the First Workshop on Tabulation in Parsing and Deduction,</booktitle>
<pages>133--137</pages>
<location>Paris, France,</location>
<contexts>
<context position="7135" citStr="Chappelier and Rajman, 1998" startWordPosition="1080" endWordPosition="1083">pair that may contain non-terminal symbols, i.e. α E (VF U NF)+ and β E (VE U NE)+, where VF and VE denote the source and target terminal vocabulary, and NF and NE denote the source and target non-terminal vocabulary, respectively. The non-terminals on the source side and on the target side of rules are linked in a one-toone correspondence. The — relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 3.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being ed</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-Cédric Chappelier and Martin Rajman. 1998. A Generalized CYK Algorithm for Parsing Stochastic CFG. In Proc. of the First Workshop on Tabulation in Parsing and Deduction, pages 133–137, Paris, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, MA, USA,</location>
<contexts>
<context position="14165" citStr="Chen and Goodman, 1998" startWordPosition="2244" endWordPosition="2247">input tree features 34.3 48.3 19.6 o 63.7 o 18.6 o 67.0 o−0.5 + non-syntactic phrases 34.4 48.1 19.9 63.4 18.8 66.5 Table 1: English-*German experimental results (truecase). BLEU scores are given in percentage. tions, word and phrase penalty, an n-gram language model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, MA, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>427--436</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="14529" citStr="Cherry and Foster, 2012" startWordPosition="2304" endWordPosition="2307">the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).4 We apply a phrase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 4TER scores are computed with tercom version 0</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 427–436, Montréal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="1520" citStr="Chiang, 2005" startWordPosition="201" endWordPosition="203">ystems utilize linguistic information that is obtained by parsing the training data. In tree-to-string translation, source-side syntactic tree annotation is employed, while string-to-tree translation exploits target-side syntax. The syntactic parse tree annotation constrains phrase extraction to syntactically well-formed phrase pairs: spans of syntactic phrases must match constituents in the parse tree. Standard phrase-based and hierarchical phrasebased statistical machine translation systems, in contrast, allow all phrase pairs that are consistent with the word alignment (Koehn et al., 2003; Chiang, 2005). A restriction of the phrase inventory to syntactically well-formed phrase pairs entails that possibly valuable information from the training data remains disregarded. While we would expect phrase pairs that are not linguistically motivated to be less reliable, discarding them altogether might be an overly harsh decision. The quality of an inventory of syntactic phrases depends heavily on the tree annotation scheme and the quality of the syntactic parses of the training data. Phrase pairs that do not span constituents in the tree annotation obtained from syntactic parses can provide reasonabl</context>
<context position="23656" citStr="Chiang, 2005" startWordPosition="3744" endWordPosition="3745"> not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constituents matches the span. Pseudo non-syntactic nonterminals are introduced for the left-hand sides of SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properti</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 263–270, Ann Arbor, MI, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrase-Based Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>228</pages>
<contexts>
<context position="7276" citStr="Chiang, 2007" startWordPosition="1103" endWordPosition="1104"> and NE denote the source and target non-terminal vocabulary, respectively. The non-terminals on the source side and on the target side of rules are linked in a one-toone correspondence. The — relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 3.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being edges as well. A set of nodes (the frontier set) is determined that contains only nodes with non-overlapping closure of their spans.1 By comput</context>
<context position="23671" citStr="Chiang, 2007" startWordPosition="3746" endWordPosition="3747"> span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constituents matches the span. Pseudo non-syntactic nonterminals are introduced for the left-hand sides of SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retain</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33(2):201– 228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to Translate with Source and Target Syntax.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>1443--1452</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="24417" citStr="Chiang (2010)" startWordPosition="3861" endWordPosition="3862">-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibe</context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to Translate with Source and Target Syntax. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 1443–1452, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What Can Syntax-Based MT Learn from Phrase-Based MT?</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>755--763</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3756" citStr="DeNeefe et al., 2007" startWordPosition="547" endWordPosition="550">hypotheses that make use of the alternative segmentations and translation options provided through non-syntactic phrases. The search space is more diverse, and in some cases all hypotheses from purely syntax-based derivations score worse than a translation that applies one or more non-syntactic phrases. We empirically demonstrate that this technique can lead to substantial gains in translation quality. Our syntactic translation models conform to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). State-of-theart GHKM string-to-tree systems have recently shown very competitive performance in public 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt translation with tree input and </context>
<context position="7413" citStr="DeNeefe et al., 2007" startWordPosition="1121" endWordPosition="1124">t side of rules are linked in a one-toone correspondence. The — relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 3.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being edges as well. A set of nodes (the frontier set) is determined that contains only nodes with non-overlapping closure of their spans.1 By computing frontier graph fragments—fragments of the alignment graph such that their root and all sinks are in the frontier set— the GHKM extrac</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What Can Syntax-Based MT Learn from Phrase-Based MT? In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 755–763, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fossum</author>
<author>Kevin Knight</author>
<author>Steven Abney</author>
</authors>
<title>Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>44--52</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="22855" citStr="Fossum et al. (2008)" startWordPosition="3612" endWordPosition="3615">ation. 6 Related Work Issues with overly restrictive syntactic grammars for statistical machine translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constituents matches the span. Pseudo non-syntactic nonterminals are introduced for the left-hand sides of SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic</context>
</contexts>
<marker>Fossum, Knight, Abney, 2008</marker>
<rawString>Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Using Syntax to Improve Word Alignment Precision for Syntax-Based Machine Translation. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 44–52, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>847--855</pages>
<location>Honolulu, HI, USA,</location>
<contexts>
<context position="15601" citStr="Galley and Manning, 2008" startWordPosition="2474" endWordPosition="2477">ase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 4TER scores are computed with tercom version 0.7.25 and parameters -N -s. 5.2 Translation Results Table 1 comprises the results of our empirical evaluation of the translation quality achieved by the different systems. 5.2.1 Phrase-based Baselines We set up two phrase-based baselines for comparison. Their set of models is the same as for the syntax-based baselines, with the exception of the PCFG probability. One of the phrase-based systems moreover utilizes a lexicalized reordering model (Galley and Manning, 2008). No nonstandard advanced features (like an operation sequence model or class-based LMs) are engrafted. The maximum phrase length is five, search is carried out with cube pruning at a k-best limit of 1000. A maximum number of 100 translation options per source side are taken into account. 5.2.2 String-to-String Contrastive System A further contrastive experiment is done with a string-to-string system. The extraction method for this string-to-string system is GHKM syntaxdirected with syntactic target-side annotation from BitPar, as in the string-to-tree setup. We actually extract the same rules</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), pages 847–855, Honolulu, HI, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>273--280</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="3681" citStr="Galley et al., 2004" startWordPosition="533" endWordPosition="536">derivations which resort to both types of phrases. Such derivations yield hypotheses that make use of the alternative segmentations and translation options provided through non-syntactic phrases. The search space is more diverse, and in some cases all hypotheses from purely syntax-based derivations score worse than a translation that applies one or more non-syntactic phrases. We empirically demonstrate that this technique can lead to substantial gains in translation quality. Our syntactic translation models conform to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). State-of-theart GHKM string-to-tree systems have recently shown very competitive performance in public 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translat</context>
<context position="7369" citStr="Galley et al., 2004" startWordPosition="1113" endWordPosition="1116">minals on the source side and on the target side of rules are linked in a one-toone correspondence. The — relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 3.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being edges as well. A set of nodes (the frontier set) is determined that contains only nodes with non-overlapping closure of their spans.1 By computing frontier graph fragments—fragments of the alignment graph such that their root and all si</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 273–280, Boston, MA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="3729" citStr="Galley et al., 2006" startWordPosition="542" endWordPosition="545">s. Such derivations yield hypotheses that make use of the alternative segmentations and translation options provided through non-syntactic phrases. The search space is more diverse, and in some cases all hypotheses from purely syntax-based derivations score worse than a translation that applies one or more non-syntactic phrases. We empirically demonstrate that this technique can lead to substantial gains in translation quality. Our syntactic translation models conform to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). State-of-theart GHKM string-to-tree systems have recently shown very competitive performance in public 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt trans</context>
<context position="7390" citStr="Galley et al., 2006" startWordPosition="1117" endWordPosition="1120">side and on the target side of rules are linked in a one-toone correspondence. The — relation defines this one-to-one correspondence. The left-hand side of the rule is a pair of source and target nonterminals, A E NF and B E NE. Decoding is typically carried out with a parsingbased algorithm, in our case a customized version of CYK+ (Chappelier and Rajman, 1998). The parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning (Chiang, 2007). 3.1 GHKM String-to-Tree Translation In GHKM string-to-tree translation (Galley et al., 2004; Galley et al., 2006; DeNeefe et al., 2007), rules are extracted from training instances which consist of a source sentence, a target sentence along with its constituent parse tree, and a word alignment matrix. This tuple is interpreted as a directed graph (the alignment graph), with edges pointing away from the root of the tree, and word alignment links being edges as well. A set of nodes (the frontier set) is determined that contains only nodes with non-overlapping closure of their spans.1 By computing frontier graph fragments—fragments of the alignment graph such that their root and all sinks are in the fronti</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proc. of the 21st International Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics, pages 961– 968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel Implementations of Word Alignment Tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ’08,</booktitle>
<pages>49--57</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="11664" citStr="Gao and Vogel, 2008" startWordPosition="1837" endWordPosition="1840">oehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 5.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singl</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ’08, pages 49– 57, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Decoding with Syntactic and Non-syntactic Phrases in a Syntaxbased Machine Translation System.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, SSST ’09,</booktitle>
<pages>1--9</pages>
<location>Boulder, CO, USA,</location>
<contexts>
<context position="25477" citStr="Hanneman and Lavie (2009)" startWordPosition="4032" endWordPosition="4035">f binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction 494 pipeline. A non-syntactic phrase pair can however only be applied if its source side matches a subtree in the parsed input sentence. Syntactic and non-syntactic phrases are not distinguished, and overlap between the syntactic and non-syntactic part of the phrase inventory is not avoided. The decoder picks the entry with the higher phrase translation pro</context>
<context position="26989" citStr="Hanneman and Lavie (2009)" startWordPosition="4253" endWordPosition="4256">ee translation with the Stat-XFER framework as developed at Carnegie Mellon University (Lavie, 2008). They combine syntactic and non-syntactic phrase inventories and reestimate the probabilities for both types of phrase pairs by adding up the observed absolute frequencies. Two combination schemes are evaluated: combination with all extractable valid non-syntactic phrases (“direct combination”) and combination with only those non-syntactic phrases whose source sides are not equal to the source side of any syntactic phrase (“syntaxprioritized combination”). On a French→English translation task, Hanneman and Lavie (2009) report improvements of around 2.6 points BLEU by adding non-syntactic phrases on top of their StatXFER syntactic baselines. Their best setup however does not reach the performance of a standard phrase-based system, which is still 1.6 points BLEU better. Apart from the differences in the underlying syntax-based translation technology (stringto-tree/tree-to-string GHKM vs. TAT vs. StatXFER), our work also constitutes a novel contribution as compared to the previous approaches by Liu et al. (2006) and Hanneman and Lavie (2009) with respect to the following: • The phrase inventory is augmented wi</context>
</contexts>
<marker>Hanneman, Lavie, 2009</marker>
<rawString>Greg Hanneman and Alon Lavie. 2009. Decoding with Syntactic and Non-syntactic Phrases in a Syntaxbased Machine Translation System. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation, SSST ’09, pages 1–9, Boulder, CO, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="14374" citStr="Heafield, 2011" startWordPosition="2282" endWordPosition="2283"> word and phrase penalty, an n-gram language model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).4 We apply a p</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 187–197, Edinburgh, Scotland, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Improved Translation with Source Syntax Labels.</title>
<date>2010</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>409--417</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="4446" citStr="Hoang and Koehn (2010)" startWordPosition="648" endWordPosition="651">ry competitive performance in public 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt translation with tree input and input tree constraints as suggested for hierarchical translation by Hoang and Koehn (2010). We also implement translation with tree input and feature-driven soft tree matching. The effect of augmenting the systems with nonsyntactic phrases is evaluated for all variants. 2 Outline The remainder of the paper is structured as follows: We review some of the basics of syntaxbased translation in the next section (Section 3) and sketch the characteristics of our GHKM string-to-tree and tree-to-string translation frameworks. In Section 4, we describe our technique to augment GHKM-style syntax-based systems with phrase pairs that do not comply with any syntactic well-formedness constraints.</context>
</contexts>
<marker>Hoang, Koehn, 2010</marker>
<rawString>Hieu Hoang and Philipp Koehn. 2010. Improved Translation with Source Syntax Labels. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 409–417, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
</authors>
<title>A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>152--159</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="11124" citStr="Hoang et al., 2009" startWordPosition="1749" endWordPosition="1752">n-syntactic rules from syntactic ones, and to be able to assign a tuned weight to the nonsyntactic part of the grammar. 5 Empirical Evaluation We evaluate the effect of augmenting GHKM syntax-based translation systems—both string-totree and tree-to-string—with non-syntactic phrase pairs on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.2 The experiments are conducted with the opensource Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 5.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2</context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation. In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT), pages 152–159, Tokyo, Japan, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Lightly-Supervised Training for Hierarchical Phrase-Based Machine Translation.</title>
<date>2011</date>
<booktitle>In Proc. of the EMNLP 2011 Workshop on Unsupervised Learning in NLP,</booktitle>
<pages>91--96</pages>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="25133" citStr="Huck et al., 2011" startWordPosition="3971" endWordPosition="3974">al symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction 494 pipeline. A</context>
</contexts>
<marker>Huck, Vilar, Stein, Ney, 2011</marker>
<rawString>Matthias Huck, David Vilar, Daniel Stein, and Hermann Ney. 2011. Lightly-Supervised Training for Hierarchical Phrase-Based Machine Translation. In Proc. of the EMNLP 2011 Workshop on Unsupervised Learning in NLP, pages 91–96, Edinburgh, Scotland, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Backing-Off for M-gram Language Modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<location>Detroit, MI, USA,</location>
<contexts>
<context position="14140" citStr="Kneser and Ney, 1995" startWordPosition="2240" endWordPosition="2243">19.7 63.6 18.7 66.5 + input tree features 34.3 48.3 19.6 o 63.7 o 18.6 o 67.0 o−0.5 + non-syntactic phrases 34.4 48.1 19.9 63.4 18.8 66.5 Table 1: English-*German experimental results (truecase). BLEU scores are given in percentage. tions, word and phrase penalty, an n-gram language model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrase</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved Backing-Off for M-gram Language Modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 181–184, Detroit, MI, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada, May/June.</location>
<contexts>
<context position="1505" citStr="Koehn et al., 2003" startWordPosition="197" endWordPosition="200">achine translation systems utilize linguistic information that is obtained by parsing the training data. In tree-to-string translation, source-side syntactic tree annotation is employed, while string-to-tree translation exploits target-side syntax. The syntactic parse tree annotation constrains phrase extraction to syntactically well-formed phrase pairs: spans of syntactic phrases must match constituents in the parse tree. Standard phrase-based and hierarchical phrasebased statistical machine translation systems, in contrast, allow all phrase pairs that are consistent with the word alignment (Koehn et al., 2003; Chiang, 2005). A restriction of the phrase inventory to syntactically well-formed phrase pairs entails that possibly valuable information from the training data remains disregarded. While we would expect phrase pairs that are not linguistically motivated to be less reliable, discarding them altogether might be an overly harsh decision. The quality of an inventory of syntactic phrases depends heavily on the tree annotation scheme and the quality of the syntactic parses of the training data. Phrase pairs that do not span constituents in the tree annotation obtained from syntactic parses can pr</context>
<context position="11748" citStr="Koehn et al., 2003" startWordPosition="1851" endWordPosition="1854"> Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. Only the 100 best translation options per distinct sourc</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of the Human Language Technology Conf. / North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 127–133, Edmonton, Canada, May/June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the MT Summit X,</booktitle>
<location>Phuket, Thailand,</location>
<contexts>
<context position="11483" citStr="Koehn, 2005" startWordPosition="1807" endWordPosition="1808">rkshop on Statistical Machine Translation (WMT) for testing.2 The experiments are conducted with the opensource Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 5.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particula</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. of the MT Summit X, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
</authors>
<title>Stat-XFER: A General Search-Based Syntax-Driven Framework for Machine Translation.</title>
<date>2008</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>4919</volume>
<pages>362--375</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="26464" citStr="Lavie, 2008" startWordPosition="4183" endWordPosition="4184">e. Syntactic and non-syntactic phrases are not distinguished, and overlap between the syntactic and non-syntactic part of the phrase inventory is not avoided. The decoder picks the entry with the higher phrase translation probability, which means that non-syntactic phrase table entries can supersede syntactic entries. The authors report improvements of 0.6 points BLEU on the 2005 NIST Chinese→English task with four reference translations. Hanneman and Lavie (2009) examine nonsyntactic phrases for tree-to-tree translation with the Stat-XFER framework as developed at Carnegie Mellon University (Lavie, 2008). They combine syntactic and non-syntactic phrase inventories and reestimate the probabilities for both types of phrase pairs by adding up the observed absolute frequencies. Two combination schemes are evaluated: combination with all extractable valid non-syntactic phrases (“direct combination”) and combination with only those non-syntactic phrases whose source sides are not equal to the source side of any syntactic phrase (“syntaxprioritized combination”). On a French→English translation task, Hanneman and Lavie (2009) report improvements of around 2.6 points BLEU by adding non-syntactic phra</context>
</contexts>
<marker>Lavie, 2008</marker>
<rawString>Alon Lavie. 2008. Stat-XFER: A General Search-Based Syntax-Driven Framework for Machine Translation. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 4919 of Lecture Notes in Computer Science, pages 362–375. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conf. on Computational Linguistics and the 44th Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="25444" citStr="Liu et al. (2006)" startWordPosition="4026" endWordPosition="4029">ights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction 494 pipeline. A non-syntactic phrase pair can however only be applied if its source side matches a subtree in the parsed input sentence. Syntactic and non-syntactic phrases are not distinguished, and overlap between the syntactic and non-syntactic part of the phrase inventory is not avoided. The decoder picks the entry with </context>
<context position="27489" citStr="Liu et al. (2006)" startWordPosition="4334" endWordPosition="4337">y syntactic phrase (“syntaxprioritized combination”). On a French→English translation task, Hanneman and Lavie (2009) report improvements of around 2.6 points BLEU by adding non-syntactic phrases on top of their StatXFER syntactic baselines. Their best setup however does not reach the performance of a standard phrase-based system, which is still 1.6 points BLEU better. Apart from the differences in the underlying syntax-based translation technology (stringto-tree/tree-to-string GHKM vs. TAT vs. StatXFER), our work also constitutes a novel contribution as compared to the previous approaches by Liu et al. (2006) and Hanneman and Lavie (2009) with respect to the following: • The phrase inventory is augmented with nonsyntactic phrases by means of a fill-up technique. Overlap is prevented, whereas not only new source sides, but also new targetside translation options can be added. • The probabilities of syntactic phrase pairs are the same as in the syntax-based baseline, and the probabilities of the non-syntactic phrase pairs are the same as in a phrase-based system. Counts of syntactic and non-syntactic phrases are not summed up to obtain new estimates. • Non-syntactic phrase pairs are distinguished fr</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring Alignment Template for Statistical Machine Translation. In Proc. of the 21st International Conf. on Computational Linguistics and the 44th Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 609–616, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>SPMT: Statistical Machine Translation with Syntactified Target Language Phrases.</title>
<date>2006</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>44--52</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="22947" citStr="Marcu et al. (2006)" startWordPosition="3629" endWordPosition="3632">ne translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that the union of the constituents matches the span. Pseudo non-syntactic nonterminals are introduced for the left-hand sides of SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phrases. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), pages 44–52, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft Syntactic Constraints for Hierarchical Phrased-Based Translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>1003--1011</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="24342" citStr="Marton and Resnik, 2008" startWordPosition="3848" endWordPosition="3851">tic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common </context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-Based Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 1003–1011, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Nadejde</author>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Machine Translation Systems.</title>
<date>2013</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>170--176</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4084" citStr="Nadejde et al., 2013" startWordPosition="589" endWordPosition="592">te that this technique can lead to substantial gains in translation quality. Our syntactic translation models conform to the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004) with composed rules as in (Galley et al., 2006) and (DeNeefe et al., 2007). State-of-theart GHKM string-to-tree systems have recently shown very competitive performance in public 486 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 486–498, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics evaluation campaigns (Nadejde et al., 2013; Bojar et al., 2013). We apply the GHKM approach not only in a string-to-tree setting as in previous work, but employ it to build tree-to-string systems as well. We conduct tree-to-string translation with text input and additionally adopt translation with tree input and input tree constraints as suggested for hierarchical translation by Hoang and Koehn (2010). We also implement translation with tree input and feature-driven soft tree matching. The effect of augmenting the systems with nonsyntactic phrases is evaluated for all variants. 2 Outline The remainder of the paper is structured as fol</context>
</contexts>
<marker>Nadejde, Williams, Koehn, 2013</marker>
<rawString>Maria Nadejde, Philip Williams, and Philipp Koehn. 2013. Edinburgh’s Syntax-Based Machine Translation Systems. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 170–176, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Detailed Analysis of Different Strategies for Phrase Table Adaptation in SMT.</title>
<date>2012</date>
<booktitle>In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA),</booktitle>
<location>San Diego, CA, USA, October/November.</location>
<contexts>
<context position="25025" citStr="Niehues and Waibel, 2012" startWordPosition="3954" endWordPosition="3957">ng, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation prob</context>
</contexts>
<marker>Niehues, Waibel, 2012</marker>
<rawString>Jan Niehues and Alex Waibel. 2012. Detailed Analysis of Different Strategies for Phrase Table Adaptation in SMT. In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA), San Diego, CA, USA, October/November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="10487" citStr="Och and Ney, 2002" startWordPosition="1653" endWordPosition="1656">special left-hand side nonterminal X. • A phrase table fill-up method is applied to enhance the syntactic phrase inventory with entries from the non-syntactic phrase inventory. Non-syntactic rules are only added to the final grammar if no syntactic rule with the same (source and target) right-hand side is present. This method is inspired by previous work in domain adaptation (Bisazza et al., 2011). • The glue grammar is extended with a new glue rule X,Q → hX∼0X∼1,Q∼0X∼1i that enables the system to make use of nonsyntactic rules in decoding. • A binary feature is added to the log-linear model (Och and Ney, 2002) to distinguish non-syntactic rules from syntactic ones, and to be able to assign a tuned weight to the nonsyntactic part of the grammar. 5 Empirical Evaluation We evaluate the effect of augmenting GHKM syntax-based translation systems—both string-totree and tree-to-string—with non-syntactic phrase pairs on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.2 The experiments are conducted with the opensource Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 295–302, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11727" citStr="Och and Ney, 2003" startWordPosition="1847" endWordPosition="1850"> et al., 2009). 5.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. Only the 100 best translation optio</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP99),</booktitle>
<pages>20--28</pages>
<institution>University of Maryland,</institution>
<location>College Park, MD, USA,</location>
<contexts>
<context position="8821" citStr="Och et al., 1999" startWordPosition="1361" endWordPosition="1364">d larger composed rules. Non-terminals on target sides of string-to-tree rules are syntactified. The target non-terminal vocabulary of the SCFG contains the set of labels of the frontier nodes, which is in turn a subset 1The span of a node in the alignment graph is defined as the set of source-side words that are reachable from this node. The closure of a span is the smallest interval of source sentence positions that covers the span. 487 TOP PUNC. CS-TOP S-T . In the standard phrase-based approach, in contrast, all continuous phrases that are consistent with the word alignment are extracted (Och et al., 1999; Och, 2002). The set of continuous bilingual phrases BP(f1J,eI1,A), given a training instance comprising a source sentence f1J , a target sentence eI1, and a word alignment A ⊆ {1,...,I}×{1,...,J}, is defined as follows: {BP(f1J,eI1,A) = hfj2 j1 ,ei2 i1i : ∃(i, j) ∈ A : i1 ≤ i ≤ i2 ∧ j1 ≤ j ≤ j2 }∧∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 Consistency for continuous phrases is based upon merely two constraints in this definition: (1.) At least one source and target position within the phrase must be aligned, and (2.) words from inside the source phrase may only be aligned to words from inside th</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In Proc. of the Joint SIGDAT Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP99), pages 20–28, University of Maryland, College Park, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen University,</institution>
<location>Aachen, Germany,</location>
<contexts>
<context position="8833" citStr="Och, 2002" startWordPosition="1365" endWordPosition="1366">rules. Non-terminals on target sides of string-to-tree rules are syntactified. The target non-terminal vocabulary of the SCFG contains the set of labels of the frontier nodes, which is in turn a subset 1The span of a node in the alignment graph is defined as the set of source-side words that are reachable from this node. The closure of a span is the smallest interval of source sentence positions that covers the span. 487 TOP PUNC. CS-TOP S-T . In the standard phrase-based approach, in contrast, all continuous phrases that are consistent with the word alignment are extracted (Och et al., 1999; Och, 2002). The set of continuous bilingual phrases BP(f1J,eI1,A), given a training instance comprising a source sentence f1J , a target sentence eI1, and a word alignment A ⊆ {1,...,I}×{1,...,J}, is defined as follows: {BP(f1J,eI1,A) = hfj2 j1 ,ei2 i1i : ∃(i, j) ∈ A : i1 ≤ i ≤ i2 ∧ j1 ≤ j ≤ j2 }∧∀(i, j) ∈ A : i1 ≤ i ≤ i2 ↔ j1 ≤ j ≤ j2 Consistency for continuous phrases is based upon merely two constraints in this definition: (1.) At least one source and target position within the phrase must be aligned, and (2.) words from inside the source phrase may only be aligned to words from inside the target phr</context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz Josef Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen University, Aachen, Germany, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="14487" citStr="Papineni et al., 2002" startWordPosition="2297" endWordPosition="2300">ability of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).4 We apply a phrase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 4TER</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11983" citStr="Petrov et al., 2006" startWordPosition="1886" endWordPosition="1889"> track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. Only the 100 best translation options per distinct source side with respect to the weighted phrase-level model scores are loaded by the decoder. The decoder is configured with a maximum chart span of 25 and a rule limit of 100. A standard set of models is used in the baselines, comprising p</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors.</title>
<date>2004</date>
<booktitle>In Proc. of the Int. Conf. on Computational Linguistics (COLING),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="11840" citStr="Schmid, 2004" startWordPosition="1866" endWordPosition="1867">ce pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vogel, 2008) and symmetrizing the two trained alignments (Och and Ney, 2003; Koehn et al., 2003). For string-to-tree translation, we parse the German target side with BitPar (Schmid, 2004).3 For tree-to-string translation, we parse the English source side of the parallel data with the English Berkeley Parser (Petrov et al., 2006). When extracting syntactic phrases, we impose several restrictions for composed rules, in particular a maximum number of twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. Only the 100 best translation options per distinct source side with respect to the weighted phrase-level model scores are loaded by the decoder. The</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proc. of the Int. Conf. on Computational Linguistics (COLING), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on Large-Scale Lightly-Supervised Training for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>182--189</pages>
<location>Waikiki, HI, USA,</location>
<contexts>
<context position="25113" citStr="Schwenk, 2008" startWordPosition="3969" endWordPosition="3970">plex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extrac</context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk. 2008. Investigations on Large-Scale Lightly-Supervised Training for Statistical Machine Translation. In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT), pages 182–189, Waikiki, HI, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA, USA,</location>
<contexts>
<context position="14959" citStr="Snover et al., 2006" startWordPosition="2374" endWordPosition="2377"> and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase with BLEU and TER (Snover et al., 2006).4 We apply a phrase length limit of five when extracting non-syntactic phrases for the fill-up of syntactic phrase tables. 4TER scores are computed with tercom version 0.7.25 and parameters -N -s. 5.2 Translation Results Table 1 comprises the results of our empirical evaluation of the translation quality achieved by the different systems. 5.2.1 Phrase-based Baselines We set up two phrase-based baselines for comparison. Their set of models is the same as for the syntax-based baselines, with the exception of the PCFG probability. One of the phrase-based systems moreover utilizes a lexicalized r</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA), pages 223–231, Cambridge, MA, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Stein</author>
<author>Stephan Peitz</author>
<author>David Vilar</author>
<author>Hermann Ney</author>
</authors>
<title>A Cocktail of Deep Syntactic Features for Hierarchical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA),</booktitle>
<location>Denver, CO, USA, October/November.</location>
<contexts>
<context position="24031" citStr="Stein et al. (2010)" startWordPosition="3800" endWordPosition="3803">llow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions o</context>
</contexts>
<marker>Stein, Peitz, Vilar, Ney, 2010</marker>
<rawString>Daniel Stein, Stephan Peitz, David Vilar, and Hermann Ney. 2010. A Cocktail of Deep Syntactic Features for Hierarchical Machine Translation. In Proc. of the Conf. of the Assoc. for Machine Translation in the Americas (AMTA), Denver, CO, USA, October/November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Spoken Language Processing (ICSLP),</booktitle>
<volume>3</volume>
<location>Denver, CO, USA,</location>
<contexts>
<context position="14323" citStr="Stolcke, 2002" startWordPosition="2272" endWordPosition="2273">case). BLEU scores are given in percentage. tions, word and phrase penalty, an n-gram language model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. We selected 2000 sentences from the newstest2008- 2012 sets as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and do each contain less than 30 words for more rapid tuning. newstest2013 and newstest2014 are used as unseen test sets. Translation quality is measured in truecase wit</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Spoken Language Processing (ICSLP), volume 3, Denver, CO, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollmann</author>
<author>Noah A Smith</author>
<author>Stephan Vogel</author>
</authors>
<title>Preference Grammars: Softening Syntactic Constraints to Improve Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>236--244</pages>
<location>Boulder, CO, USA,</location>
<contexts>
<context position="24007" citStr="Venugopal et al. (2009)" startWordPosition="3795" endWordPosition="3798">. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In d</context>
</contexts>
<marker>Venugopal, Zollmann, Smith, Vogel, 2009</marker>
<rawString>Ashish Venugopal, Andreas Zollmann, Noah A. Smith, and Stephan Vogel. 2009. Preference Grammars: Softening Syntactic Constraints to Improve Statistical Machine Translation. In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 236–244, Boulder, CO, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Analysing Soft Syntax Features and Heuristics for Hierarchical Phrase Based Machine Translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>190--197</pages>
<location>Waikiki, HI, USA,</location>
<contexts>
<context position="24363" citStr="Vilar et al., 2008" startWordPosition="3852" endWordPosition="3855">r SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree translation setting, Chiang (2010) proposed techniques to soften the syntactic constraints. A fuzzy approach with complex non-terminal symbols as in SAMT is employed to overcome the limitations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adap</context>
</contexts>
<marker>Vilar, Stein, Ney, 2008</marker>
<rawString>David Vilar, Daniel Stein, and Hermann Ney. 2008. Analysing Soft Syntax Features and Heuristics for Hierarchical Phrase Based Machine Translation. In Proc. of the Int. Workshop on Spoken Language Translation (IWSLT), pages 190–197, Waikiki, HI, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>746--754</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22622" citStr="Wang et al., 2007" startWordPosition="3573" endWordPosition="3576">ementation that allows for embedding of nonsyntactic rules into hierarchical rules (other than the glue rules) but did not see improvements with it as yet. Furthermore, efficiency concerns become more relevant in such an implementation. 6 Related Work Issues with overly restrictive syntactic grammars for statistical machine translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple con</context>
</contexts>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2007. Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 746–754, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Jonathan May</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Re-structuring, Re-labeling, and Re-aligning for Syntax-based Machine Translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="22641" citStr="Wang et al., 2010" startWordPosition="3577" endWordPosition="3580">ows for embedding of nonsyntactic rules into hierarchical rules (other than the glue rules) but did not see improvements with it as yet. Furthermore, efficiency concerns become more relevant in such an implementation. 6 Related Work Issues with overly restrictive syntactic grammars for statistical machine translation, inadequate syntactic parses, and insufficient coverage have been tackled from several different directions in the literature. A proposed approach to attain better syntactic phrase inventories is to restructure the syntactic parse trees in a preprocessing step (Wang et al., 2007; Wang et al., 2010; Burkett and Klein, 2012). This line of research aims at rearranging parse trees in a way that makes them a better fit for the requirements of the bilingual downstream application. Conversely, Fossum et al. (2008) retain the structure of the parse trees and modify the word alignments. Marcu et al. (2006) relax syntactic phrase extraction constraints in their SPMT Model 2 to allow for phrases that do not match the span of one single constituent in the parse tree. SPMT Model 2 rules are created from spans that are consistent with the word alignment and covered by multiple constituents such that</context>
</contexts>
<marker>Wang, May, Knight, Marcu, 2010</marker>
<rawString>Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, Re-labeling, and Re-aligning for Syntax-based Machine Translation. Computational Linguistics, 36(2):247–277, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>GHKM Rule Extraction and Scope-3 Parsing in Moses.</title>
<date>2012</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>388--394</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="11055" citStr="Williams and Koehn, 2012" startWordPosition="1737" endWordPosition="1740">ture is added to the log-linear model (Och and Ney, 2002) to distinguish non-syntactic rules from syntactic ones, and to be able to assign a tuned weight to the nonsyntactic part of the grammar. 5 Empirical Evaluation We evaluate the effect of augmenting GHKM syntax-based translation systems—both string-totree and tree-to-string—with non-syntactic phrase pairs on the English→German language pair using the standard newstest sets of the Workshop on Statistical Machine Translation (WMT) for testing.2 The experiments are conducted with the opensource Moses implementations of GHKM rule extraction (Williams and Koehn, 2012) and decoding with CYK+ parsing and cube pruning (Hoang et al., 2009). 5.1 Experimental Setup We work with an English–German parallel training corpus of around 4.5 M sentence pairs (after corpus cleaning). The parallel data originates from three different sources which have been eligible for the constrained track of the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task: Europarl (Koehn, 2005), News Commentary, and the Common Crawl corpus as provided on the WMT website. Word alignments are created by aligning the data in both directions with MGIZA++ (Gao and Vog</context>
</contexts>
<marker>Williams, Koehn, 2012</marker>
<rawString>Philip Williams and Philipp Koehn. 2012. GHKM Rule Extraction and Scope-3 Parsing in Moses. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 388–394, Montréal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Rico Sennrich</author>
<author>Maria Nadejde</author>
<author>Matthias Huck</author>
<author>Eva Hasler</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Systems at WMT</title>
<date>2014</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="13951" citStr="Williams et al., 2014" startWordPosition="2213" endWordPosition="2216">o−0.4 + non-syntactic phrases 33.9 48.4 19.3 +0.4 64.0 −0.3 18.7 +0.3 66.6 + input tree constraints 33.7 48.4 19.3 o +0.3 63.9 o −0.3 18.3 o +0.2 67.0 o−0.5 + non-syntactic phrases 34.2 48.2 19.7 63.6 18.7 66.5 + input tree features 34.3 48.3 19.6 o 63.7 o 18.6 o 67.0 o−0.5 + non-syntactic phrases 34.4 48.1 19.9 63.4 18.8 66.5 Table 1: English-*German experimental results (truecase). BLEU scores are given in percentage. tions, word and phrase penalty, an n-gram language model, a rule rareness penalty, and the monolingual PCFG probability of the tree fragment from which the rule was extracted (Williams et al., 2014). Phrase translation probabilities are smoothed via Good-Turing smoothing. The language model (LM) is a large interpolated 5-gram LM with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The target side of the parallel corpus and the monolingual German News Crawl corpora are employed as training data. We use the SRILM toolkit (Stolcke, 2002) to train the LM and rely on KenLM (Heafield, 2011) for language model scoring during decoding. Model weights are optimized to maximize BLEU (Papineni et al., 2002) with batch MIRA (Cherry and Foster, 2012) on 1000-best lists. W</context>
</contexts>
<marker>Williams, Sennrich, Nadejde, Huck, Hasler, Koehn, 2014</marker>
<rawString>Philip Williams, Rico Sennrich, Maria Nadejde, Matthias Huck, Eva Hasler, and Philipp Koehn. 2014. Edinburgh’s Syntax-Based Systems at WMT 2014. In Proc. of the Workshop on Statistical Machine Translation (WMT), Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Training Phrase Translation Models with Leaving-One-Out.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>475--484</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="25189" citStr="Wuebker et al., 2010" startWordPosition="3980" endWordPosition="3983">itations during phrase extraction. In decoding, substitutions of non-terminals are not restricted to matching ones. Any lefthand side non-terminal can substitute any righthand side non-terminal. The decoder decides on the best derivation based on the tuned weights of a large number of binary features. Joining phrase inventories that come from multiple origins is a common method in domain adaptation (Bertoldi and Federico, 2009; Niehues and Waibel, 2012) but has also been applied in the contexts of lightly-supervised training (Schwenk, 2008; Huck et al., 2011) and of forced alignment training (Wuebker et al., 2010). For our purposes, we apply a fill-up method in the manner of the one that has been shown to perform well for domain adaptation in earlier work (Bisazza et al., 2011). Previous research that resembles our work most has been presented by Liu et al. (2006) and by Hanneman and Lavie (2009). Liu et al. (2006) allow for application of nonsyntactic phrase pairs in their tree-to-string alignment template (TAT) system. The translation probabilities for the non-syntactic phrases are obtained from a standard phrase-based extraction 494 pipeline. A non-syntactic phrase pair can however only be applied i</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Joern Wuebker, Arne Mauser, and Hermann Ney. 2010. Training Phrase Translation Models with Leaving-One-Out. In Proc. of the Annual Meeting of the Assoc. for Computational Linguistics (ACL), pages 475–484, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax Augmented Machine Translation via Chart Parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>138--141</pages>
<location>New York City, NY, USA,</location>
<contexts>
<context position="23788" citStr="Zollmann and Venugopal (2006)" startWordPosition="3762" endWordPosition="3765"> consistent with the word alignment and covered by multiple constituents such that the union of the constituents matches the span. Pseudo non-syntactic nonterminals are introduced for the left-hand sides of SPMT Model 2 rules. Special additional rules allow for combination of those non-syntactic lefthand side non-terminals with genuine syntactic non-terminals on the right-hand sides of other rules during decoding. Another line of research took the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) as a starting point and extended it with syntactic enhancements. In their SAMT system, Zollmann and Venugopal (2006) labeled the non-terminals of the hierarchical model with composite symbols derived from the syntactic tree annotation. Similar methods have been applied with CCG labels (Almaghout et al., 2012). Venugopal et al. (2009) and Stein et al. (2010) keep the grammar of the non-terminals of the hierarchical model unlabeled and apply the syntactic information in a separate model. Other authors added features which fire for phrases complying with certain syntactic properties while retaining all phrase pairs of the hierarchical model (Marton and Resnik, 2008; Vilar et al., 2008). In a tree-to-tree trans</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In Proc. of the Workshop on Statistical Machine Translation (WMT), pages 138–141, New York City, NY, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
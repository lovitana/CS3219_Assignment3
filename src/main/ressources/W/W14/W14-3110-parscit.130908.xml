<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000348">
<title confidence="0.988844">
LDAvis: A method for visualizing and interpreting topics
</title>
<author confidence="0.998467">
Carson Sievert
</author>
<affiliation confidence="0.998339">
Iowa State University
</affiliation>
<address confidence="0.731362">
3414 Snedecor Hall
Ames, IA 50014, USA
</address>
<email confidence="0.997111">
cpsievert1@gmail.com
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975238095238">
We present LDAvis, a web-based interac-
tive visualization of topics estimated using
Latent Dirichlet Allocation that is built us-
ing a combination of R and D3. Our visu-
alization provides a global view of the top-
ics (and how they differ from each other),
while at the same time allowing for a deep
inspection of the terms most highly asso-
ciated with each individual topic. First,
we propose a novel method for choosing
which terms to present to a user to aid in
the task of topic interpretation, in which
we define the relevance of a term to a
topic. Second, we present results from a
user study that suggest that ranking terms
purely by their probability under a topic is
suboptimal for topic interpretation. Last,
we describe LDAvis, our visualization
system that allows users to flexibly explore
topic-term relationships using relevance to
better understand a fitted LDA model.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917125">
Recently much attention has been paid to visual-
izing the output of topic models fit using Latent
Dirichlet Allocation (LDA) (Gardner et al., 2010;
Chaney and Blei, 2012; Chuang et al., 2012b; Gre-
tarsson et al., 2011). Such visualizations are chal-
lenging to create because of the high dimensional-
ity of the fitted model – LDA is typically applied
to many thousands of documents, which are mod-
eled as mixtures of dozens (or hundreds) of top-
ics, which themselves are modeled as distributions
over thousands of terms (Blei et al., 2003; Griffiths
and Steyvers, 2004). The most promising basic
technique for creating LDA visualizations that are
both compact and thorough is interactivity.
We introduce an interactive visualization sys-
tem that we call LDAvis that attempts to answer
</bodyText>
<note confidence="0.94635275">
Kenneth E. Shirley
AT&amp;T Labs Research
33 Thomas Street, 26th Floor
New York, NY 10007, USA
</note>
<email confidence="0.957337">
kshirley@research.att.com
</email>
<bodyText confidence="0.997572975609756">
a few basic questions about a fitted topic model:
(1) What is the meaning of each topic?, (2) How
prevalent is each topic?, and (3) How do the topics
relate to each other? Different visual components
answer each of these questions, some of which are
original, and some of which are borrowed from ex-
isting tools.
Our visualization (illustrated in Figure 1) has
two basic pieces. First, the left panel of our visual-
ization presents a global view of the topic model,
and answers questions 2 and 3. In this view, we
plot the topics as circles in the two-dimensional
plane whose centers are determined by comput-
ing the distance between topics, and then by us-
ing multidimensional scaling to project the inter-
topic distances onto two dimensions, as is done
in (Chuang et al., 2012a). We encode each topic’s
overall prevalence using the areas of the circles,
where we sort the topics in decreasing order of
prevalence.
Second, the right panel of our visualization de-
picts a horizontal barchart whose bars represent
the individual terms that are the most useful for in-
terpreting the currently selected topic on the left,
and allows users to answer question 1, “What is
the meaning of each topic?”. A pair of overlaid
bars represent both the corpus-wide frequency of
a given term as well as the topic-specific frequency
of the term, as in (Chuang et al., 2012b).
The left and right panels of our visualization are
linked such that selecting a topic (on the left) re-
veals the most useful terms (on the right) for inter-
preting the selected topic. In addition, selecting a
term (on the right) reveals the conditional distribu-
tion over topics (on the left) for the selected term.
This kind of linked selection allows users to exam-
ine a large number of topic-term relationships in a
compact manner.
A key innovation of our system is how we deter-
mine the most useful terms for interpreting a given
topic, and how we allow users to interactively ad-
</bodyText>
<page confidence="0.993643">
63
</page>
<note confidence="0.880733">
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 63–70,
Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.874810333333333">
Figure 1: The layout of LDAvis, with the global topic view on the left, and the term barcharts (with
Topic 34 selected) on the right. Linked selections allow users to reveal aspects of the topic-term relation-
ships compactly.
</figureCaption>
<bodyText confidence="0.999968925925926">
just this determination. A topic in LDA is a multi-
nomial distribution over the (typically thousands
of) terms in the vocabulary of the corpus. To inter-
pret a topic, one typically examines a ranked list of
the most probable terms in that topic, using any-
where from three to thirty terms in the list. The
problem with interpreting topics this way is that
common terms in the corpus often appear near the
top of such lists for multiple topics, making it hard
to differentiate the meanings of these topics.
Bischof and Airoldi (2012) propose ranking
terms for a given topic in terms of both the fre-
quency of the term under that topic as well as the
term’s exclusivity to the topic, which accounts for
the degree to which it appears in that particular
topic to the exclusion of others. We propose a sim-
ilar measure that we call the relevance of a term
to a topic that allows users to flexibly rank terms
in order of usefulness for interpreting topics. We
discuss our definition of relevance, and its graphi-
cal interpretation, in detail in Section 3.1. We also
present the results of a user study conducted to de-
termine the optimal tuning parameter in the defini-
tion of relevance to aid the task of topic interpreta-
tion in Section 3.2, and we describe how we incor-
porate relevance into our interactive visualization
in Section 4.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999864">
Much work has been done recently regarding the
interpretation of topics (i.e. measuring topic “co-
herence”) as well as visualization of topic models.
</bodyText>
<subsectionHeader confidence="0.996742">
2.1 Topic Interpretation and Coherence
</subsectionHeader>
<bodyText confidence="0.995184214285714">
It is well-known that the topics inferred by LDA
are not always easily interpretable by humans.
Chang et al. (2009) established via a large
user study that standard quantitative measures of
fit, such as those summarized by Wallach et al.
(2009), do not necessarily agree with measures of
topic interpretability by humans. Ramage et al.
(2009) assert that “characterizing topics is hard”
and describe how using the top-k terms for a given
topic might not always be best, but offer few con-
crete alternatives.
AlSumait et al. (2009), Mimno et al. (2011),
and Chuang et al. (2013b) develop quantitative
methods for measuring the interpretability of top-
</bodyText>
<page confidence="0.998885">
64
</page>
<bodyText confidence="0.99997025862069">
ics based on experiments with data sets that come
with some notion of topical ground truth, such as
document metadata or expert-created topic labels.
These methods are useful for understanding, in a
global sense, which topics are interpretable (and
why), but they don’t specifically attempt to aid the
user in interpreting individual topics.
Blei and Lafferty (2009) developed “Turbo Top-
ics”, a method of identifying n-grams within LDA-
inferred topics that, when listed in decreasing or-
der of probability, provide users with extra in-
formation about the usage of terms within top-
ics. This two-stage process yields good results on
experimental data, although the resulting output
is still simply a ranked list containing a mixture
of terms and n-grams, and the usefulness of the
method for topic interpretation was not tested in a
user study.
Newman et al. (2010) describe a method for
ranking terms within topics to aid interpretability
called Pointwise Mutual Information (PMI) rank-
ing. Under PMI ranking of terms, each of the ten
most probable terms within a topic are ranked in
decreasing order of approximately how often they
occur in close proximity to the nine other most
probable terms from that topic in some large, ex-
ternal “reference” corpus, such as Wikipedia or
Google n-grams. Although this method correlated
highly with human judgments of term importance
within topics, it does not easily generalize to topic
models fit to corpora that don’t have a readily
available external source of word co-occurrences.
In contrast, Taddy (2011) uses an intrinsic mea-
sure to rank terms within topics: a quantity called
lift, defined as the ratio of a term’s probability
within a topic to its marginal probability across
the corpus. This generally decreases the rankings
of globally frequent terms, which can be helpful.
We find that it can be noisy, however, by giving
high rankings to very rare terms that occur in only
a single topic, for instance. While such terms may
contain useful topical content, if they are very rare
the topic may remain difficult to interpret.
Finally, Bischof and Airoldi (2012) develop and
implement a new statistical topic model that infers
both a term’s frequency as well as its exclusivity
– the degree to which its occurrences are limited
to only a few topics. They introduce a univari-
ate measure called a FREX score (“FRequency
and EXclusivity”) which is a weighted harmonic
mean of a term’s rank within a given topic with
respect to frequency and exclusivity, and they rec-
ommend it as a way to rank terms to aid topic in-
terpretation. We propose a similar method that is
a weighted average of the logarithms of a term’s
probability and its lift, and we justify it with a user
study and incorporate it into our interactive visu-
alization.
</bodyText>
<subsectionHeader confidence="0.980748">
2.2 Topic Model Visualization Systems
</subsectionHeader>
<bodyText confidence="0.999961619047619">
A number of visualization systems for topic mod-
els have been developed in recent years. Sev-
eral of them focus on allowing users to browse
documents, topics, and terms to learn about the
relationships between these three canonical topic
model units (Gardner et al., 2010; Chaney and
Blei, 2012; Snyder et al., 2013). These browsers
typically use lists of the most probable terms
within topics to summarize the topics, and the vi-
sualization elements are limited to barcharts or
word clouds of term probabilities for each topic,
pie charts of topic probabilities for each document,
and/or various barcharts or scatterplots related to
document metadata. Although these tools can be
useful for browsing a corpus, we seek a more com-
pact visualization, with the more narrow focus of
quickly and easily understanding the individual
topics themselves (without necessarily visualizing
documents).
Chuang et al. (2012b) develop such a tool,
called “Termite”, which visualizes the set of topic-
term distributions estimated in LDA using a ma-
trix layout. The authors introduce two measures
of the usefulness of terms for understanding a
topic model: distinctiveness and saliency. These
quantities measure how much information a term
conveys about topics by computing the Kullback-
Liebler divergence between the distribution of top-
ics given the term and the marginal distribution
of topics (distinctiveness), optionally weighted
by the term’s overall frequency (saliency). The
authors recommend saliency as a thresholding
method for selecting which terms are included in
the visualization, and they further use a seriation
method for ordering the most salient terms to high-
light differences between topics.
Termite is a compact, intuitive interactive visu-
alization of the topics in a topic model, but by only
including terms that rank high in saliency or dis-
tinctiveness, which are global properties of terms,
it is restricted to providing a global view of the
model, rather than allowing a user to deeply in-
</bodyText>
<page confidence="0.997889">
65
</page>
<bodyText confidence="0.999691">
spect individual topics by visualizing a potentially
different set of terms for every single topic. In
fact, Chuang et al. (2013a) describe the use of a
“topic-specific word ordering” as potentially use-
ful future work.
</bodyText>
<sectionHeader confidence="0.847781" genericHeader="method">
3 Relevance of terms to topics
</sectionHeader>
<bodyText confidence="0.894841">
Here we define relevance, our method for ranking
terms within topics, and we describe the results of
a user study to learn an optimal tuning parameter
in the computation of relevance.
Lift (log scale)
</bodyText>
<subsectionHeader confidence="0.833189">
3.1 Definition of Relevance P(Token  |Topic) (log scale)
</subsectionHeader>
<figure confidence="0.753605520179372">
Topic 29 of 50 (20 Newgroups data)
70.4
25.3
9.1
3.3
1.2
0.4
0 0.001 0.002 0.004 0.011 0.03
●
●
● ●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
● ● ● ●
●
exhaust
oil
lights
●plastic●
●
●
●
●
● ● ●
● ●●
●●
●●
●●
●
● ● ● remove
● eye
●●
●
●
●
light
●
●
●
water
●
●●
●
●
● ● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
● ●
●●
● ●
●
●
●
●
● ● ●
●
●
●
●●● ● ●
● ● ●●
●
●
●
●
●
●
●
●
●
● ●
●●
out
●
●
●
●
●
●
●
up
●
● ● ● ●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
● ● ● ● ● ● ●
● ● ● ● ●
● ● ●
● ● ●
● ● ●
● ● ●
● ● ● ● ●
● ●
● ● ●●● ● ●
● ●
● ● ● ● ● ● ● ●
● ●
● ● ●
● ● ● ● ● ● ●
●
● ● ● ●
●
● ●● ● ● ●
● ●
● ● ● ●● ● ●
● ● ● ● ● ● ● ●
● ● ●
● ● ● ● ● ● ● ● ●
● ●●
●●
●
● ● ●
●
●
● ● ● ●
●
● ●
●
● ● ● ●
● ● ● ●
● ● ●
● ●
● ● ● ● ●
● ● ● ●
● ● ● ● ● ● ●
● ● ●● ● ● ● ● ●
● ● ●
●
●
● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ●
● ● ●
● ●
● ● ● ●
●
● ● ● ●
● ●
● ● ● ●
● ● ●
● ● ● ●
● ● ●
● ●
● ● ●
● ● ● ● ● ● ●
● ●
● ●
● ● ● ●
● ● ● ●
● ● ● ●
● ●
Top 10 Most Relevant Boundary
● ● ● ● ●
● ●
● ● ● ● ●
● ● ●
● ● ● ● ●
●
●
●
●
lambda = 0
●
●●
●
●
●●
lambda = 1/3 ●
lambda = 2/3
●
●
lambda = 1
●
●
●
●
●
●
●
●
●
</figure>
<bodyText confidence="0.989013416666667">
Let Okw denote the probability of term w E
{1, ..., V } for topic k E {1, ..., K}, where V de-
notes the number of terms in the vocabulary, and
let pw denote the marginal probability of term w in
the corpus. One typically estimates O in LDA us-
ing Variational Bayes methods or Collapsed Gibbs
Sampling, and pw from the empirical distribution
of the corpus (optionally smoothed by including
prior weights as pseudo-counts).
We define the relevance of term w to topic k
given a weight parameter A (where 0 &lt; A &lt; 1) as:
r(w, k  |A) = A log(Okw) + (1 − A) log ( pkw ) ,
where A determines the weight given to the prob-
ability of term w under topic k relative to its lift
(measuring both on the log scale). Setting A = 1
results in the familiar ranking of terms in decreas-
ing order of their topic-specific probability, and
setting A = 0 ranks terms solely by their lift. We
wish to learn an “optimal” value of A for topic in-
terpretation from our user study.
First, though, to see how different values of A
result in different ranked term lists, consider the
plot in Figure 2. We fit a 50-topic model to the
20 Newsgroups data (details are described in Sec-
tion 3.2) and plotted log(lift) on the y-axis vs.
log(Okw) on the x-axis for each term in the vo-
cabulary (which has size V = 22,524) for a given
topic. Figure 2 shows this plot for Topic 29, which
occurred mostly in documents posted to the “Mo-
torcycles” Newsgroup, but also from documents
posted to the “Automobiles” Newsgroup and the
“Electronics” Newsgroup. Graphically, the line
separating the most relevant terms for this topic,
given A, has slope −A/(1 − A) (see Figure 2).
For this topic, the top-5 most relevant terms
given A = 1 (ranking solely by probability)
</bodyText>
<figureCaption confidence="0.676236">
Figure 2: Dotted lines separating the top-10 most
</figureCaption>
<bodyText confidence="0.955687461538462">
relevant terms for different values of A, with the
most relevant terms for A = 2/3 displayed and
highlighted in blue.
are {out, #emailaddress, #twodigitnumer, up,
#onedigitnumber}, where a “#” symbol denotes
a term that is an entity representing a class of
things. In contrast to this list, which contains glob-
ally common terms and which provides very lit-
tle meaning regarding motorcycles, automobiles,
or electronics, the top-5 most relevant terms given
A = 1/3 are {oil, plastic, pipes, fluid, and lights}.
The second set of terms is much more descriptive
of the topic being discussed than the first.
</bodyText>
<subsectionHeader confidence="0.999525">
3.2 User Study
</subsectionHeader>
<bodyText confidence="0.999995684210526">
We conducted a user study to determine whether
there was an optimal value of A in the definition of
relevance to aid topic interpretation. First, we fit
a 50-topic model to the D = 13,695 documents
in the 20 Newsgroups data which were posted to a
single Newsgroup (rather than two or more News-
groups). We used the Collapsed Gibbs Sampler
algorithm (Griffiths and Steyvers, 2004) to sample
the latent topics for each of the N = 1, 590, 376
tokens in the data, and we saved their topic assign-
ments from the last iteration (after convergence).
We then computed the 20 by 50 table, T, which
contains, in cell Tgk, the count of the number of
times a token from topic k E {1, ..., 50} was as-
signed to Newsgroup g E {1, ..., 20}, where we
defined the Newsgroup of a token to be the News-
group to which the document containing that to-
ken was posted. Some of the LDA-inferred top-
ics occurred almost exclusively (&gt; 90% of occur-
</bodyText>
<page confidence="0.978953">
66
</page>
<bodyText confidence="0.9998615">
rences) in documents from a single Newsgroup,
such as Topic 38, which was the estimated topic
for 15,705 tokens in the corpus, 14,233 of which
came from documents posted to the “Medicine”
(or “sci.med”) Newsgroup. Other topics occurred
in a wide variety of Newsgroups. One would ex-
pect these “spread-out” topics to be harder to in-
terpret than the “pure” topics like Topic 38.
In the study we recruited 29 subjects among our
colleagues (research scientists at AT&amp;T Labs with
moderate familiarity with text mining techniques
and topic models), and each subject completed an
online experiment consisting of 50 tasks, one for
each topic in the fitted LDA model. Task k (for
k E {1, ..., 50}) was to read a list of five terms,
ranked from 1-5 in order of relevance to topic k,
where A E (0, 1) was randomly sampled to com-
pute relevance. The user was instructed to identify
which “topic” the list of terms discussed from a
list of three possible “topics”, where their choices
were names of the Newsgroups. The correct an-
swer for task k (i.e. our “ground truth”) was de-
fined as the Newsgroup that contributed the most
tokens to topic k (i.e. the Newsgroup with the
largest count in the kth column of the table T), and
the two alternative choices were the Newsgroups
that contributed the second and third-most tokens
to topic k.
We anticipated that the effect of A on the proba-
bility of a user making the correct choice could be
different across topics. In particular, for “spread-
out” topics that were inherently difficult to inter-
pret, because their tokens were drawn from a wide
variety of Newsgroups (similar to a “fused” topic
in Chuang et al. (2013b)), we expected the propor-
tion of correct responses to be roughly 1/3 no mat-
ter the value of A used to compute relevance. Sim-
ilarly, for very “pure” topics, whose tokens were
drawn almost exclusively from one Newsgroup,
we expected the task to be easy for any value of A.
To account for this, we analyzed the experimental
data by fitting a varying-intercepts logistic regres-
sion model to allow each of the fifty topics to have
its own baseline difficulty level, where the effect
of A is shared across topics. We used a quadratic
function of A in the model (linear, cubic and quar-
tic functions were explored and rejected).
As expected, the baseline difficulty of each
topic varied widely. In fact, seven of the topics
were correctly identified by all 29 users,1 and one
</bodyText>
<footnote confidence="0.90359">
1Whose ground truth labels were Medicine (twice), Mis-
</footnote>
<note confidence="0.456531">
Trial data for middle tercile of topics
</note>
<figure confidence="0.64876">
Lambda (optimal value is about 0.6)
</figure>
<figureCaption confidence="0.903236333333333">
Figure 3: A plot of the proportion of correct re-
sponses in a user study vs. the value of A used to
compute the most relevant terms for each topic.
</figureCaption>
<bodyText confidence="0.99986276">
topic was incorrectly identified by all 29 users.2
For the remaining 42 topics we estimated a topic-
specific intercept term to control for the inher-
ent difficulty of identifying the topic (not just due
to its tokens being spread among multiple News-
groups, but also to account for the inherent famil-
iarity of each topic to our subject pool – subjects,
on average, were more familiar with “Cars” than
“The X Window System”, for example).
The estimated effects of A and A2 were 2.74 and
-2.34, with standard errors 1.03 and 1.00. Taken
together, their joint effect was statistically signif-
icant (x2 p-value = 0.018). To see the estimated
effect of A on the probability of correctly identi-
fying a topic, consider Figure 3. We plot binned
proportions of correct responses (on the y-axis)
vs. A (on the x-axis) for the 14 topics whose esti-
mated topic-specific intercepts fell into the middle
tercile among the 42 topics that weren’t trivial or
impossible to identify. Among these topics there
was roughly a 67% baseline probability of correct
identification. As Figure 3 shows, for these topics,
the “optimal” value of A was about 0.6, and it re-
sulted in an estimated 70% probability of correct
identification, whereas for values of A near 0 and
</bodyText>
<footnote confidence="0.9458235">
cellaneous Politics, Christianity, Gun Politics, Space (Astron-
omy), and Middle East Politics.
2The ground truth label for this topic was “Christianity”,
but the presence of the term “islam” or “quran” among the
top-5 for every value of λ led each subject to choose “Mis-
cellaneous Religion”.
</footnote>
<figure confidence="0.995579636363636">
0.0 0.2 0.4 0.6 0.8 1.0
Proportion of Correct Responses
0.9
0.8
0.7
0.6
0.5
0.4
●
●
●
●
● Binned responses (bin size = 40)
50% Intervals
95% Intervals
Quadratic Fit
●
●
●
●
●
●
</figure>
<page confidence="0.997674">
67
</page>
<bodyText confidence="0.999987441176471">
1, the estimated proportions of correct responses
were closer to 53% and 63%, respectively. We
view this as evidence that ranking terms according
to relevance, where A &lt; 1 (i.e. not strictly in de-
creasing order of probability), can improve topic
interpretability.
Note that in our experiment, we used the collec-
tion of single-posted 20 Newsgroups documents
to define our “ground truth” data. An alternative
method for collecting “ground truth” data would
have been to recruit experts to label topics from
an LDA model. We chose against this option be-
cause doing so would present a classic “chicken-
or-egg” problem: If we use expert-labeled topics
in an experiment to learn how to summarize top-
ics so that they can be interpreted (i.e. “labeled”),
we would only re-learn the way that our experts
were instructed, or allowed, to label the topics in
the first place! If, for instance, the experts were
presented with a ranked list of the most probable
terms for each topic, this would influence the in-
terpretations and labels they give to the topics, and
the experimental result would be the circular con-
clusion that ranking terms by probability allows
users to recover the “expert” labels most easily.
To avoid this, we felt strongly that we should use
data in which documents have metadata associated
with them. The 20 Newsgroups data provides an
externally validated source of topic labels, in the
sense that the labels were presented to users (in
the form of Newsgroup names), and users sub-
sequently filled in the content. It represents, es-
sentially, a crowd-sourced collection of tokens, or
content, for a certain set of topic labels.
</bodyText>
<sectionHeader confidence="0.983346" genericHeader="method">
4 The LDAvis System
</sectionHeader>
<bodyText confidence="0.999955378787879">
Our interactive, web-based visualization system,
LDAvis, has two core functionalities that enable
users to understand the topic-term relationships in
a fitted LDA model, and a number of extra features
that provide additional perspectives on the model.
First and foremost, LDAvis allows one to se-
lect a topic to reveal the most relevant terms for
that topic. In Figure 1, Topic 34 is selected, and
its 30 most relevant terms (given A = 0.34, in this
case) populate the barchart to the right (ranked
in order of relevance from top to bottom). The
widths of the gray bars represent the corpus-wide
frequencies of each term, and the widths of the
red bars represent the topic-specific frequencies of
each term. A slider allows users to change the
value of A, which can alter the rankings of terms
to aid topic interpretation. By default, A is set to
0.6, as suggested by our user study in Section 3.2.
If A = 1, terms are ranked solely by Okw, which
implies the red bars would be sorted from widest
(at the top) to narrowest (at the bottom). By com-
paring the widths of the red and gray bars for a
given term, users can quickly understand whether
a term is highly relevant to the selected topic be-
cause of its lift (a high ratio of red to gray), or
its probability (absolute width of red). The top 3
most relevant terms in Figure 1 are “law”, “court”,
and “cruel”. Note that “law” is a common term
which is generated by Topic 34 in about 40% of
its corpus-wide occurrences, whereas “cruel” is a
relatively rare term with very high lift in Topic 34
– it occurs almost exclusively in this topic. Such
properties of the topic-term relationships are read-
ily visible in LDAvis for every topic.
On the left panel, two visual features provide
a global perspective of the topics. First, the ar-
eas of the circles are proportional to the relative
prevalences of the topics in the corpus. In the
50-topic model fit to the 20 Newsgroups data,
the first three topics comprise 12%, 9%, and
6% of the corpus, and all contain common, non-
specific terms (although there are interesting dif-
ferences: Topic 2 contains formal debate-related
language such as “conclusion”, “evidence”, and
“argument”, whereas Topic 3 contains slang con-
versational language such as “kinda”, “like”, and
“yeah”). In addition to visualizing topic preva-
lence, the left pane shows inter-topic differences.
The default for computing inter-topic distances is
Jensen-Shannon divergence, although other met-
rics are enabled. The default for scaling the set of
inter-topic distances defaults to Principal Compo-
nents, but other algorithms are also enabled.
The second core feature of LDAvis is the abil-
ity to select a term (by hovering over it) to reveal
its conditional distribution over topics. This dis-
tribution is visualized by altering the areas of the
topic circles such that they are proportional to the
term-specific frequencies across the corpus. This
allows the user to verify, as discussed in Chuang et
al. (2012a), whether the multidimensional scaling
of topics has faithfully clustered similar topics in
two-dimensional space. For example, in Figure 4,
the term “file” is selected. In the majority of this
term’s occurrences, it is drawn from one of several
topics located in the upper left-hand region of the
</bodyText>
<page confidence="0.998539">
68
</page>
<figureCaption confidence="0.798428333333333">
Figure 4: The user has chosen to segment the fifty topics into four clusters, and has selected the green
cluster to populate the barchart with the most relevant terms for that cluster. Then, the user hovered over
the ninth bar from the top, “file”, to display the conditional distribution over topics for this term.
</figureCaption>
<bodyText confidence="0.99976135">
global topic view. Upon inspection, this group of
topics can be interpreted broadly as a discussion
of computer hardware and software. This verifies,
to some extent, their placement, via multidimen-
sional scaling, into the same two-dimensional re-
gion. It also suggests that the term “file” used in
this context refers to a computer file. However,
there is also conditional probability mass for the
term “file” on Topic 34. As shown in Figure 1,
Topic 34 can be interpreted as discussing the crim-
inal punishment system where “file” refers to court
filings. Similar discoveries can be made for any
term that exhibits polysemy (such as “drive” ap-
pearing in computer- and automobile-related top-
ics, for example).
Beyond its within-browser interaction capabil-
ity using D3 (Bostock et al., 2011), LDAvis
leverages the R language (R Core Team, 2014)
and specifically, the shiny package (Rstudio,
2014), to allow users to easily alter the topical
distance measurement as well as the multidimen-
sional scaling algorithm to produce the global
topic view. In addition, there is an option to ap-
ply k-means clustering to the topics (as a function
of their two-dimensional locations in the global
topic view). This is merely an effort to facilitate
semantic zooming in an LDA model with many
topics where ‘after-the-fact’ clustering may be an
easier way to estimate clusters of topics, rather
than fitting a hierarchical topic model (Blei et al.,
2003), for example. Selecting a cluster of topics
(by clicking the Voronoi region corresponding to
the cluster) reveals the most relevant terms for that
cluster of topics, where the term distribution of a
cluster of topics is defined as the weighted average
of the term distributions of the individual topics in
the cluster. In Figure 4, the green cluster of topics
is selected, and the most relevant terms, displayed
in the barchart on the right, are predominantly re-
lated to computer hardware and software.
</bodyText>
<sectionHeader confidence="0.999828" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999991666666667">
We have described a web-based, interactive visu-
alization system, LDAvis, that enables deep in-
spection of topic-term relationships in an LDA
model, while simultaneously providing a global
view of the topics, via their prevalences and sim-
ilarities to each other, in a compact space. We
</bodyText>
<page confidence="0.997332">
69
</page>
<bodyText confidence="0.999988727272727">
also propose a novel measure, relevance, by which
to rank terms within topics to aid in the task
of topic interpretation, and we present results
from a user study that show that ranking terms
in decreasing order of probability is suboptimal
for topic interpretation. The LDAvis visual-
ization system (including the user study data) is
currently available as an R package on GitHub:
https://github.com/cpsievert/LDAvis.
For future work, we anticipate performing a
larger user study to further understand how to fa-
cilitate topic interpretation in fitted LDA mod-
els, including a comparison of multiple methods,
such as ranking by Turbo Topics (Blei and Laf-
ferty, 2009) or FREX scores (Bischof and Airoldi,
2012), in addition to relevance. We also note the
need to visualize correlations between topics, as
this can provide insight into what is happening on
the document level without actually displaying en-
tire documents. Last, we seek a solution to the
problem of visualizing a large number of topics
(say, from 100 - 500 topics) in a compact way.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908682926829">
Loulwah AlSumait, Daniel Barbara, James Gentle, and
Carlotta Domeniconi. 2009. Topic Significance
Ranking of LDA Generative Models. ECML.
Jonathan M. Bischof and Edoardo M. Airoldi. 2012.
Summarizing topical content with word frequency
and exclusivity. ICML.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2012. Latent Dirichlet Allocation. JMLR.
David M. Blei and John Lafferty. 2009. Vi-
sualizing Topics with Multi-Word Expressions.
arXiv:0907.1013v1 [stat.ML], 2009
David M. Blei, Thomas L. Griffiths, Michael I. Jor-
dan, and Joshua B. Tenenbaum. 2003. Hierarchi-
cal Topic Models and the Nested Chinese Restaurant
Process. NIPS.
Michael Bostock, Vadim Ogievetsky, Jeffrey Heer
2011. D3: Data-Driven Documents. InfoVis.
Allison J.B. Chaney and David M. Blei. 2012. Visual-
izing topic models. ICWSM.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
Tea Leaves: How Humans Interpret Topic Models.
NIPS.
Jason Chuang, Daniel Ramage, Christopher D. Man-
ning and Jeffrey Heer. 2012a. Interpretation and
Trust: Designing Model-Driven Visualizations for
Text Analysis. CHI.
Jason Chuang, Christopher D. Manning and Jeffrey
Heer. 2012b. Termite: Visualization Techniques for
Assessing Textual Topic Models. AVI.
Jason Chuang, Yuening Hu, Ashley Jin, John D. Wilk-
erson, Daniel A. McFarland, Christopher D. Man-
ning and Jeffrey Heer. 2013a. Document Explo-
ration with Topic Modeling: Designing Interactive
Visualizations to Support Effective Analysis Work-
flows. NIPS Workshop on Topic Models: Computa-
tion, Application, and Evaluation.
Jason Chuang, Sonal Gupta, Christopher D. Manning
and Jeffrey Heer. 2013b. Topic Model Diagnostics:
Assessing Domain Relevance via Topical Alignment.
ICML.
Matthew J. Gardner, Joshua Lutes, Jeff Lund, Josh
Hansen, Dan Walker, Eric Ringger, and Kevin Seppi.
2010. The topic browser: An interactive tool for
browsing topic models. NIPS Workshop on Chal-
lenges of Data Visualization.
Brynjar Gretarsson, John O’Donovan, Svetlin Bostand-
jieb, Tobias Hollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2011. TopicNets: Visual
Analysis of Large Text Corpora with Topic Model-
ing. ACM Transactions on Intelligent Systems and
Technology, pp 1-26.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing Semantic Coherence in Topic Models.
EMNLP.
David Newman, Youn Noh, Edmund Talley, Sarvnaz
Karimi, and Timothy Baldwin 2010. Evaluating
Topic Models for Digital Libraries. JCDL.
R Core Team 2014. R: A Language and Envi-
ronment for Statistical Computing. http://www.R-
project.org.
R Studio, Inc. 2014. shiny: Web Application Frame-
work for R; package version 0.9.1. http://CRAN.R-
project.org/package=shiny.
Daniel Ramage, Evan Rosen and Jason Chuang and
Christopher D. Manning, and Daniel A. McFarland.
2009. Topic Modeling for the Social Sciences. NIPS
Workshop on Applications for Topic Models: Text
and Beyond.
Justin Snyder, Rebecca Knowles, Mark Dredze,
Matthew Gormley, and Travis Wolfe. 2013. Topic
Models and Metadata for Visualizing Text Corpora.
Proceedings of the 2013 NAACL HLT Demonstra-
tion Session.
Matthew A. Taddy 2011. On Estimation and Selection
for Topic Models. AISTATS.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation Methods for
Topic Models. ICML.
</reference>
<page confidence="0.998482">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893227">
<title confidence="0.997501">A method for visualizing and interpreting topics</title>
<author confidence="0.944674">Carson</author>
<affiliation confidence="0.987579">Iowa State</affiliation>
<address confidence="0.9863065">3414 Snedecor Ames, IA 50014,</address>
<email confidence="0.99889">cpsievert1@gmail.com</email>
<abstract confidence="0.999163909090909">present a web-based interactive visualization of topics estimated using Latent Dirichlet Allocation that is built using a combination of R and D3. Our visualization provides a global view of the topics (and how they differ from each other), while at the same time allowing for a deep inspection of the terms most highly associated with each individual topic. First, we propose a novel method for choosing which terms to present to a user to aid in the task of topic interpretation, in which define the a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, describe our visualization system that allows users to flexibly explore topic-term relationships using relevance to better understand a fitted LDA model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Loulwah AlSumait</author>
<author>Daniel Barbara</author>
<author>James Gentle</author>
<author>Carlotta Domeniconi</author>
</authors>
<title>Topic Significance Ranking of LDA Generative Models.</title>
<date>2009</date>
<publisher>ECML.</publisher>
<contexts>
<context position="6362" citStr="AlSumait et al. (2009)" startWordPosition="1069" endWordPosition="1072">coherence”) as well as visualization of topic models. 2.1 Topic Interpretation and Coherence It is well-known that the topics inferred by LDA are not always easily interpretable by humans. Chang et al. (2009) established via a large user study that standard quantitative measures of fit, such as those summarized by Wallach et al. (2009), do not necessarily agree with measures of topic interpretability by humans. Ramage et al. (2009) assert that “characterizing topics is hard” and describe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some notion of topical ground truth, such as document metadata or expert-created topic labels. These methods are useful for understanding, in a global sense, which topics are interpretable (and why), but they don’t specifically attempt to aid the user in interpreting individual topics. Blei and Lafferty (2009) developed “Turbo Topics”, a method of identifying n-grams within LDAinferred topics that, when listed in decrea</context>
</contexts>
<marker>AlSumait, Barbara, Gentle, Domeniconi, 2009</marker>
<rawString>Loulwah AlSumait, Daniel Barbara, James Gentle, and Carlotta Domeniconi. 2009. Topic Significance Ranking of LDA Generative Models. ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan M Bischof</author>
<author>Edoardo M Airoldi</author>
</authors>
<title>Summarizing topical content with word frequency and exclusivity.</title>
<date>2012</date>
<publisher>ICML.</publisher>
<contexts>
<context position="4835" citStr="Bischof and Airoldi (2012)" startWordPosition="807" endWordPosition="810">ight. Linked selections allow users to reveal aspects of the topic-term relationships compactly. just this determination. A topic in LDA is a multinomial distribution over the (typically thousands of) terms in the vocabulary of the corpus. To interpret a topic, one typically examines a ranked list of the most probable terms in that topic, using anywhere from three to thirty terms in the list. The problem with interpreting topics this way is that common terms in the corpus often appear near the top of such lists for multiple topics, making it hard to differentiate the meanings of these topics. Bischof and Airoldi (2012) propose ranking terms for a given topic in terms of both the frequency of the term under that topic as well as the term’s exclusivity to the topic, which accounts for the degree to which it appears in that particular topic to the exclusion of others. We propose a similar measure that we call the relevance of a term to a topic that allows users to flexibly rank terms in order of usefulness for interpreting topics. We discuss our definition of relevance, and its graphical interpretation, in detail in Section 3.1. We also present the results of a user study conducted to determine the optimal tun</context>
<context position="8589" citStr="Bischof and Airoldi (2012)" startWordPosition="1431" endWordPosition="1434">external source of word co-occurrences. In contrast, Taddy (2011) uses an intrinsic measure to rank terms within topics: a quantity called lift, defined as the ratio of a term’s probability within a topic to its marginal probability across the corpus. This generally decreases the rankings of globally frequent terms, which can be helpful. We find that it can be noisy, however, by giving high rankings to very rare terms that occur in only a single topic, for instance. While such terms may contain useful topical content, if they are very rare the topic may remain difficult to interpret. Finally, Bischof and Airoldi (2012) develop and implement a new statistical topic model that infers both a term’s frequency as well as its exclusivity – the degree to which its occurrences are limited to only a few topics. They introduce a univariate measure called a FREX score (“FRequency and EXclusivity”) which is a weighted harmonic mean of a term’s rank within a given topic with respect to frequency and exclusivity, and they recommend it as a way to rank terms to aid topic interpretation. We propose a similar method that is a weighted average of the logarithms of a term’s probability and its lift, and we justify it with a u</context>
</contexts>
<marker>Bischof, Airoldi, 2012</marker>
<rawString>Jonathan M. Bischof and Edoardo M. Airoldi. 2012. Summarizing topical content with word frequency and exclusivity. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2012</date>
<publisher>JMLR.</publisher>
<marker>Blei, Ng, Jordan, 2012</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2012. Latent Dirichlet Allocation. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John Lafferty</author>
</authors>
<title>Visualizing Topics with Multi-Word Expressions. arXiv:0907.1013v1 [stat.ML],</title>
<date>2009</date>
<contexts>
<context position="6850" citStr="Blei and Lafferty (2009)" startWordPosition="1145" endWordPosition="1148">scribe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some notion of topical ground truth, such as document metadata or expert-created topic labels. These methods are useful for understanding, in a global sense, which topics are interpretable (and why), but they don’t specifically attempt to aid the user in interpreting individual topics. Blei and Lafferty (2009) developed “Turbo Topics”, a method of identifying n-grams within LDAinferred topics that, when listed in decreasing order of probability, provide users with extra information about the usage of terms within topics. This two-stage process yields good results on experimental data, although the resulting output is still simply a ranked list containing a mixture of terms and n-grams, and the usefulness of the method for topic interpretation was not tested in a user study. Newman et al. (2010) describe a method for ranking terms within topics to aid interpretability called Pointwise Mutual Informa</context>
</contexts>
<marker>Blei, Lafferty, 2009</marker>
<rawString>David M. Blei and John Lafferty. 2009. Visualizing Topics with Multi-Word Expressions. arXiv:0907.1013v1 [stat.ML], 2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical Topic Models and the Nested Chinese Restaurant Process.</title>
<date>2003</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="1587" citStr="Blei et al., 2003" startWordPosition="260" endWordPosition="263">relationships using relevance to better understand a fitted LDA model. 1 Introduction Recently much attention has been paid to visualizing the output of topic models fit using Latent Dirichlet Allocation (LDA) (Gardner et al., 2010; Chaney and Blei, 2012; Chuang et al., 2012b; Gretarsson et al., 2011). Such visualizations are challenging to create because of the high dimensionality of the fitted model – LDA is typically applied to many thousands of documents, which are modeled as mixtures of dozens (or hundreds) of topics, which themselves are modeled as distributions over thousands of terms (Blei et al., 2003; Griffiths and Steyvers, 2004). The most promising basic technique for creating LDA visualizations that are both compact and thorough is interactivity. We introduce an interactive visualization system that we call LDAvis that attempts to answer Kenneth E. Shirley AT&amp;T Labs Research 33 Thomas Street, 26th Floor New York, NY 10007, USA kshirley@research.att.com a few basic questions about a fitted topic model: (1) What is the meaning of each topic?, (2) How prevalent is each topic?, and (3) How do the topics relate to each other? Different visual components answer each of these questions, some </context>
<context position="27246" citStr="Blei et al., 2003" startWordPosition="4905" endWordPosition="4908">age (R Core Team, 2014) and specifically, the shiny package (Rstudio, 2014), to allow users to easily alter the topical distance measurement as well as the multidimensional scaling algorithm to produce the global topic view. In addition, there is an option to apply k-means clustering to the topics (as a function of their two-dimensional locations in the global topic view). This is merely an effort to facilitate semantic zooming in an LDA model with many topics where ‘after-the-fact’ clustering may be an easier way to estimate clusters of topics, rather than fitting a hierarchical topic model (Blei et al., 2003), for example. Selecting a cluster of topics (by clicking the Voronoi region corresponding to the cluster) reveals the most relevant terms for that cluster of topics, where the term distribution of a cluster of topics is defined as the weighted average of the term distributions of the individual topics in the cluster. In Figure 4, the green cluster of topics is selected, and the most relevant terms, displayed in the barchart on the right, are predominantly related to computer hardware and software. 5 Discussion We have described a web-based, interactive visualization system, LDAvis, that enabl</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2003</marker>
<rawString>David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2003. Hierarchical Topic Models and the Nested Chinese Restaurant Process. NIPS.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Bostock</author>
</authors>
<title>Vadim Ogievetsky, Jeffrey Heer 2011. D3: Data-Driven Documents.</title>
<publisher>InfoVis.</publisher>
<marker>Bostock, </marker>
<rawString>Michael Bostock, Vadim Ogievetsky, Jeffrey Heer 2011. D3: Data-Driven Documents. InfoVis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allison J B Chaney</author>
<author>David M Blei</author>
</authors>
<title>Visualizing topic models.</title>
<date>2012</date>
<publisher>ICWSM.</publisher>
<contexts>
<context position="1224" citStr="Chaney and Blei, 2012" startWordPosition="197" endWordPosition="200">o a user to aid in the task of topic interpretation, in which we define the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to flexibly explore topic-term relationships using relevance to better understand a fitted LDA model. 1 Introduction Recently much attention has been paid to visualizing the output of topic models fit using Latent Dirichlet Allocation (LDA) (Gardner et al., 2010; Chaney and Blei, 2012; Chuang et al., 2012b; Gretarsson et al., 2011). Such visualizations are challenging to create because of the high dimensionality of the fitted model – LDA is typically applied to many thousands of documents, which are modeled as mixtures of dozens (or hundreds) of topics, which themselves are modeled as distributions over thousands of terms (Blei et al., 2003; Griffiths and Steyvers, 2004). The most promising basic technique for creating LDA visualizations that are both compact and thorough is interactivity. We introduce an interactive visualization system that we call LDAvis that attempts t</context>
<context position="9584" citStr="Chaney and Blei, 2012" startWordPosition="1603" endWordPosition="1606">d exclusivity, and they recommend it as a way to rank terms to aid topic interpretation. We propose a similar method that is a weighted average of the logarithms of a term’s probability and its lift, and we justify it with a user study and incorporate it into our interactive visualization. 2.2 Topic Model Visualization Systems A number of visualization systems for topic models have been developed in recent years. Several of them focus on allowing users to browse documents, topics, and terms to learn about the relationships between these three canonical topic model units (Gardner et al., 2010; Chaney and Blei, 2012; Snyder et al., 2013). These browsers typically use lists of the most probable terms within topics to summarize the topics, and the visualization elements are limited to barcharts or word clouds of term probabilities for each topic, pie charts of topic probabilities for each document, and/or various barcharts or scatterplots related to document metadata. Although these tools can be useful for browsing a corpus, we seek a more compact visualization, with the more narrow focus of quickly and easily understanding the individual topics themselves (without necessarily visualizing documents). Chuan</context>
</contexts>
<marker>Chaney, Blei, 2012</marker>
<rawString>Allison J.B. Chaney and David M. Blei. 2012. Visualizing topic models. ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Chang</author>
<author>Jordan Boyd-Graber</author>
<author>Sean Gerrish</author>
<author>Chong Wang</author>
<author>David M Blei</author>
</authors>
<title>Reading Tea Leaves: How Humans Interpret Topic Models.</title>
<date>2009</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="5948" citStr="Chang et al. (2009)" startWordPosition="1001" endWordPosition="1004"> detail in Section 3.1. We also present the results of a user study conducted to determine the optimal tuning parameter in the definition of relevance to aid the task of topic interpretation in Section 3.2, and we describe how we incorporate relevance into our interactive visualization in Section 4. 2 Related Work Much work has been done recently regarding the interpretation of topics (i.e. measuring topic “coherence”) as well as visualization of topic models. 2.1 Topic Interpretation and Coherence It is well-known that the topics inferred by LDA are not always easily interpretable by humans. Chang et al. (2009) established via a large user study that standard quantitative measures of fit, such as those summarized by Wallach et al. (2009), do not necessarily agree with measures of topic interpretability by humans. Ramage et al. (2009) assert that “characterizing topics is hard” and describe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some noti</context>
</contexts>
<marker>Chang, Boyd-Graber, Gerrish, Wang, Blei, 2009</marker>
<rawString>Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, and David M. Blei. 2009. Reading Tea Leaves: How Humans Interpret Topic Models. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Daniel Ramage</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Interpretation and Trust: Designing Model-Driven Visualizations for Text Analysis.</title>
<date>2012</date>
<publisher>CHI.</publisher>
<contexts>
<context position="1245" citStr="Chuang et al., 2012" startWordPosition="201" endWordPosition="204">task of topic interpretation, in which we define the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to flexibly explore topic-term relationships using relevance to better understand a fitted LDA model. 1 Introduction Recently much attention has been paid to visualizing the output of topic models fit using Latent Dirichlet Allocation (LDA) (Gardner et al., 2010; Chaney and Blei, 2012; Chuang et al., 2012b; Gretarsson et al., 2011). Such visualizations are challenging to create because of the high dimensionality of the fitted model – LDA is typically applied to many thousands of documents, which are modeled as mixtures of dozens (or hundreds) of topics, which themselves are modeled as distributions over thousands of terms (Blei et al., 2003; Griffiths and Steyvers, 2004). The most promising basic technique for creating LDA visualizations that are both compact and thorough is interactivity. We introduce an interactive visualization system that we call LDAvis that attempts to answer Kenneth E. S</context>
<context position="2723" citStr="Chuang et al., 2012" startWordPosition="449" endWordPosition="452">te to each other? Different visual components answer each of these questions, some of which are original, and some of which are borrowed from existing tools. Our visualization (illustrated in Figure 1) has two basic pieces. First, the left panel of our visualization presents a global view of the topic model, and answers questions 2 and 3. In this view, we plot the topics as circles in the two-dimensional plane whose centers are determined by computing the distance between topics, and then by using multidimensional scaling to project the intertopic distances onto two dimensions, as is done in (Chuang et al., 2012a). We encode each topic’s overall prevalence using the areas of the circles, where we sort the topics in decreasing order of prevalence. Second, the right panel of our visualization depicts a horizontal barchart whose bars represent the individual terms that are the most useful for interpreting the currently selected topic on the left, and allows users to answer question 1, “What is the meaning of each topic?”. A pair of overlaid bars represent both the corpus-wide frequency of a given term as well as the topic-specific frequency of the term, as in (Chuang et al., 2012b). The left and right p</context>
<context position="10198" citStr="Chuang et al. (2012" startWordPosition="1697" endWordPosition="1700"> 2012; Snyder et al., 2013). These browsers typically use lists of the most probable terms within topics to summarize the topics, and the visualization elements are limited to barcharts or word clouds of term probabilities for each topic, pie charts of topic probabilities for each document, and/or various barcharts or scatterplots related to document metadata. Although these tools can be useful for browsing a corpus, we seek a more compact visualization, with the more narrow focus of quickly and easily understanding the individual topics themselves (without necessarily visualizing documents). Chuang et al. (2012b) develop such a tool, called “Termite”, which visualizes the set of topicterm distributions estimated in LDA using a matrix layout. The authors introduce two measures of the usefulness of terms for understanding a topic model: distinctiveness and saliency. These quantities measure how much information a term conveys about topics by computing the KullbackLiebler divergence between the distribution of topics given the term and the marginal distribution of topics (distinctiveness), optionally weighted by the term’s overall frequency (saliency). The authors recommend saliency as a thresholding m</context>
<context position="25191" citStr="Chuang et al. (2012" startWordPosition="4570" endWordPosition="4573">erences. The default for computing inter-topic distances is Jensen-Shannon divergence, although other metrics are enabled. The default for scaling the set of inter-topic distances defaults to Principal Components, but other algorithms are also enabled. The second core feature of LDAvis is the ability to select a term (by hovering over it) to reveal its conditional distribution over topics. This distribution is visualized by altering the areas of the topic circles such that they are proportional to the term-specific frequencies across the corpus. This allows the user to verify, as discussed in Chuang et al. (2012a), whether the multidimensional scaling of topics has faithfully clustered similar topics in two-dimensional space. For example, in Figure 4, the term “file” is selected. In the majority of this term’s occurrences, it is drawn from one of several topics located in the upper left-hand region of the 68 Figure 4: The user has chosen to segment the fifty topics into four clusters, and has selected the green cluster to populate the barchart with the most relevant terms for that cluster. Then, the user hovered over the ninth bar from the top, “file”, to display the conditional distribution over top</context>
</contexts>
<marker>Chuang, Ramage, Manning, Heer, 2012</marker>
<rawString>Jason Chuang, Daniel Ramage, Christopher D. Manning and Jeffrey Heer. 2012a. Interpretation and Trust: Designing Model-Driven Visualizations for Text Analysis. CHI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Termite: Visualization Techniques for Assessing Textual Topic Models.</title>
<date>2012</date>
<publisher>AVI.</publisher>
<contexts>
<context position="1245" citStr="Chuang et al., 2012" startWordPosition="201" endWordPosition="204">task of topic interpretation, in which we define the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to flexibly explore topic-term relationships using relevance to better understand a fitted LDA model. 1 Introduction Recently much attention has been paid to visualizing the output of topic models fit using Latent Dirichlet Allocation (LDA) (Gardner et al., 2010; Chaney and Blei, 2012; Chuang et al., 2012b; Gretarsson et al., 2011). Such visualizations are challenging to create because of the high dimensionality of the fitted model – LDA is typically applied to many thousands of documents, which are modeled as mixtures of dozens (or hundreds) of topics, which themselves are modeled as distributions over thousands of terms (Blei et al., 2003; Griffiths and Steyvers, 2004). The most promising basic technique for creating LDA visualizations that are both compact and thorough is interactivity. We introduce an interactive visualization system that we call LDAvis that attempts to answer Kenneth E. S</context>
<context position="2723" citStr="Chuang et al., 2012" startWordPosition="449" endWordPosition="452">te to each other? Different visual components answer each of these questions, some of which are original, and some of which are borrowed from existing tools. Our visualization (illustrated in Figure 1) has two basic pieces. First, the left panel of our visualization presents a global view of the topic model, and answers questions 2 and 3. In this view, we plot the topics as circles in the two-dimensional plane whose centers are determined by computing the distance between topics, and then by using multidimensional scaling to project the intertopic distances onto two dimensions, as is done in (Chuang et al., 2012a). We encode each topic’s overall prevalence using the areas of the circles, where we sort the topics in decreasing order of prevalence. Second, the right panel of our visualization depicts a horizontal barchart whose bars represent the individual terms that are the most useful for interpreting the currently selected topic on the left, and allows users to answer question 1, “What is the meaning of each topic?”. A pair of overlaid bars represent both the corpus-wide frequency of a given term as well as the topic-specific frequency of the term, as in (Chuang et al., 2012b). The left and right p</context>
<context position="10198" citStr="Chuang et al. (2012" startWordPosition="1697" endWordPosition="1700"> 2012; Snyder et al., 2013). These browsers typically use lists of the most probable terms within topics to summarize the topics, and the visualization elements are limited to barcharts or word clouds of term probabilities for each topic, pie charts of topic probabilities for each document, and/or various barcharts or scatterplots related to document metadata. Although these tools can be useful for browsing a corpus, we seek a more compact visualization, with the more narrow focus of quickly and easily understanding the individual topics themselves (without necessarily visualizing documents). Chuang et al. (2012b) develop such a tool, called “Termite”, which visualizes the set of topicterm distributions estimated in LDA using a matrix layout. The authors introduce two measures of the usefulness of terms for understanding a topic model: distinctiveness and saliency. These quantities measure how much information a term conveys about topics by computing the KullbackLiebler divergence between the distribution of topics given the term and the marginal distribution of topics (distinctiveness), optionally weighted by the term’s overall frequency (saliency). The authors recommend saliency as a thresholding m</context>
<context position="25191" citStr="Chuang et al. (2012" startWordPosition="4570" endWordPosition="4573">erences. The default for computing inter-topic distances is Jensen-Shannon divergence, although other metrics are enabled. The default for scaling the set of inter-topic distances defaults to Principal Components, but other algorithms are also enabled. The second core feature of LDAvis is the ability to select a term (by hovering over it) to reveal its conditional distribution over topics. This distribution is visualized by altering the areas of the topic circles such that they are proportional to the term-specific frequencies across the corpus. This allows the user to verify, as discussed in Chuang et al. (2012a), whether the multidimensional scaling of topics has faithfully clustered similar topics in two-dimensional space. For example, in Figure 4, the term “file” is selected. In the majority of this term’s occurrences, it is drawn from one of several topics located in the upper left-hand region of the 68 Figure 4: The user has chosen to segment the fifty topics into four clusters, and has selected the green cluster to populate the barchart with the most relevant terms for that cluster. Then, the user hovered over the ninth bar from the top, “file”, to display the conditional distribution over top</context>
</contexts>
<marker>Chuang, Manning, Heer, 2012</marker>
<rawString>Jason Chuang, Christopher D. Manning and Jeffrey Heer. 2012b. Termite: Visualization Techniques for Assessing Textual Topic Models. AVI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Yuening Hu</author>
<author>Ashley Jin</author>
<author>John D Wilkerson</author>
<author>Daniel A McFarland</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Document Exploration with Topic Modeling: Designing Interactive Visualizations to Support Effective Analysis Workflows. NIPS Workshop on Topic Models: Computation, Application, and Evaluation.</title>
<date>2013</date>
<contexts>
<context position="6408" citStr="Chuang et al. (2013" startWordPosition="1078" endWordPosition="1081">els. 2.1 Topic Interpretation and Coherence It is well-known that the topics inferred by LDA are not always easily interpretable by humans. Chang et al. (2009) established via a large user study that standard quantitative measures of fit, such as those summarized by Wallach et al. (2009), do not necessarily agree with measures of topic interpretability by humans. Ramage et al. (2009) assert that “characterizing topics is hard” and describe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some notion of topical ground truth, such as document metadata or expert-created topic labels. These methods are useful for understanding, in a global sense, which topics are interpretable (and why), but they don’t specifically attempt to aid the user in interpreting individual topics. Blei and Lafferty (2009) developed “Turbo Topics”, a method of identifying n-grams within LDAinferred topics that, when listed in decreasing order of probability, provide users with </context>
<context position="11414" citStr="Chuang et al. (2013" startWordPosition="1889" endWordPosition="1892">g method for selecting which terms are included in the visualization, and they further use a seriation method for ordering the most salient terms to highlight differences between topics. Termite is a compact, intuitive interactive visualization of the topics in a topic model, but by only including terms that rank high in saliency or distinctiveness, which are global properties of terms, it is restricted to providing a global view of the model, rather than allowing a user to deeply in65 spect individual topics by visualizing a potentially different set of terms for every single topic. In fact, Chuang et al. (2013a) describe the use of a “topic-specific word ordering” as potentially useful future work. 3 Relevance of terms to topics Here we define relevance, our method for ranking terms within topics, and we describe the results of a user study to learn an optimal tuning parameter in the computation of relevance. Lift (log scale) 3.1 Definition of Relevance P(Token |Topic) (log scale) Topic 29 of 50 (20 Newgroups data) 70.4 25.3 9.1 3.3 1.2 0.4 0 0.001 0.002 0.004 0.011 0.03 ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● exhaust oil lights ●plastic● ● ● ● ● ● ● ● ● ●● ●● ●● ●● ● ● ● ● remove ● </context>
<context position="17812" citStr="Chuang et al. (2013" startWordPosition="3302" endWordPosition="3305">k k (i.e. our “ground truth”) was defined as the Newsgroup that contributed the most tokens to topic k (i.e. the Newsgroup with the largest count in the kth column of the table T), and the two alternative choices were the Newsgroups that contributed the second and third-most tokens to topic k. We anticipated that the effect of A on the probability of a user making the correct choice could be different across topics. In particular, for “spreadout” topics that were inherently difficult to interpret, because their tokens were drawn from a wide variety of Newsgroups (similar to a “fused” topic in Chuang et al. (2013b)), we expected the proportion of correct responses to be roughly 1/3 no matter the value of A used to compute relevance. Similarly, for very “pure” topics, whose tokens were drawn almost exclusively from one Newsgroup, we expected the task to be easy for any value of A. To account for this, we analyzed the experimental data by fitting a varying-intercepts logistic regression model to allow each of the fifty topics to have its own baseline difficulty level, where the effect of A is shared across topics. We used a quadratic function of A in the model (linear, cubic and quartic functions were e</context>
</contexts>
<marker>Chuang, Hu, Jin, Wilkerson, McFarland, Manning, Heer, 2013</marker>
<rawString>Jason Chuang, Yuening Hu, Ashley Jin, John D. Wilkerson, Daniel A. McFarland, Christopher D. Manning and Jeffrey Heer. 2013a. Document Exploration with Topic Modeling: Designing Interactive Visualizations to Support Effective Analysis Workflows. NIPS Workshop on Topic Models: Computation, Application, and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Sonal Gupta</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment.</title>
<date>2013</date>
<publisher>ICML.</publisher>
<contexts>
<context position="6408" citStr="Chuang et al. (2013" startWordPosition="1078" endWordPosition="1081">els. 2.1 Topic Interpretation and Coherence It is well-known that the topics inferred by LDA are not always easily interpretable by humans. Chang et al. (2009) established via a large user study that standard quantitative measures of fit, such as those summarized by Wallach et al. (2009), do not necessarily agree with measures of topic interpretability by humans. Ramage et al. (2009) assert that “characterizing topics is hard” and describe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some notion of topical ground truth, such as document metadata or expert-created topic labels. These methods are useful for understanding, in a global sense, which topics are interpretable (and why), but they don’t specifically attempt to aid the user in interpreting individual topics. Blei and Lafferty (2009) developed “Turbo Topics”, a method of identifying n-grams within LDAinferred topics that, when listed in decreasing order of probability, provide users with </context>
<context position="11414" citStr="Chuang et al. (2013" startWordPosition="1889" endWordPosition="1892">g method for selecting which terms are included in the visualization, and they further use a seriation method for ordering the most salient terms to highlight differences between topics. Termite is a compact, intuitive interactive visualization of the topics in a topic model, but by only including terms that rank high in saliency or distinctiveness, which are global properties of terms, it is restricted to providing a global view of the model, rather than allowing a user to deeply in65 spect individual topics by visualizing a potentially different set of terms for every single topic. In fact, Chuang et al. (2013a) describe the use of a “topic-specific word ordering” as potentially useful future work. 3 Relevance of terms to topics Here we define relevance, our method for ranking terms within topics, and we describe the results of a user study to learn an optimal tuning parameter in the computation of relevance. Lift (log scale) 3.1 Definition of Relevance P(Token |Topic) (log scale) Topic 29 of 50 (20 Newgroups data) 70.4 25.3 9.1 3.3 1.2 0.4 0 0.001 0.002 0.004 0.011 0.03 ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● exhaust oil lights ●plastic● ● ● ● ● ● ● ● ● ●● ●● ●● ●● ● ● ● ● remove ● </context>
<context position="17812" citStr="Chuang et al. (2013" startWordPosition="3302" endWordPosition="3305">k k (i.e. our “ground truth”) was defined as the Newsgroup that contributed the most tokens to topic k (i.e. the Newsgroup with the largest count in the kth column of the table T), and the two alternative choices were the Newsgroups that contributed the second and third-most tokens to topic k. We anticipated that the effect of A on the probability of a user making the correct choice could be different across topics. In particular, for “spreadout” topics that were inherently difficult to interpret, because their tokens were drawn from a wide variety of Newsgroups (similar to a “fused” topic in Chuang et al. (2013b)), we expected the proportion of correct responses to be roughly 1/3 no matter the value of A used to compute relevance. Similarly, for very “pure” topics, whose tokens were drawn almost exclusively from one Newsgroup, we expected the task to be easy for any value of A. To account for this, we analyzed the experimental data by fitting a varying-intercepts logistic regression model to allow each of the fifty topics to have its own baseline difficulty level, where the effect of A is shared across topics. We used a quadratic function of A in the model (linear, cubic and quartic functions were e</context>
</contexts>
<marker>Chuang, Gupta, Manning, Heer, 2013</marker>
<rawString>Jason Chuang, Sonal Gupta, Christopher D. Manning and Jeffrey Heer. 2013b. Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Gardner</author>
<author>Joshua Lutes</author>
<author>Jeff Lund</author>
<author>Josh Hansen</author>
<author>Dan Walker</author>
<author>Eric Ringger</author>
<author>Kevin Seppi</author>
</authors>
<title>The topic browser: An interactive tool for browsing topic models.</title>
<date>2010</date>
<booktitle>NIPS Workshop on Challenges of Data Visualization.</booktitle>
<contexts>
<context position="1201" citStr="Gardner et al., 2010" startWordPosition="193" endWordPosition="196">ich terms to present to a user to aid in the task of topic interpretation, in which we define the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to flexibly explore topic-term relationships using relevance to better understand a fitted LDA model. 1 Introduction Recently much attention has been paid to visualizing the output of topic models fit using Latent Dirichlet Allocation (LDA) (Gardner et al., 2010; Chaney and Blei, 2012; Chuang et al., 2012b; Gretarsson et al., 2011). Such visualizations are challenging to create because of the high dimensionality of the fitted model – LDA is typically applied to many thousands of documents, which are modeled as mixtures of dozens (or hundreds) of topics, which themselves are modeled as distributions over thousands of terms (Blei et al., 2003; Griffiths and Steyvers, 2004). The most promising basic technique for creating LDA visualizations that are both compact and thorough is interactivity. We introduce an interactive visualization system that we call</context>
<context position="9561" citStr="Gardner et al., 2010" startWordPosition="1599" endWordPosition="1602">espect to frequency and exclusivity, and they recommend it as a way to rank terms to aid topic interpretation. We propose a similar method that is a weighted average of the logarithms of a term’s probability and its lift, and we justify it with a user study and incorporate it into our interactive visualization. 2.2 Topic Model Visualization Systems A number of visualization systems for topic models have been developed in recent years. Several of them focus on allowing users to browse documents, topics, and terms to learn about the relationships between these three canonical topic model units (Gardner et al., 2010; Chaney and Blei, 2012; Snyder et al., 2013). These browsers typically use lists of the most probable terms within topics to summarize the topics, and the visualization elements are limited to barcharts or word clouds of term probabilities for each topic, pie charts of topic probabilities for each document, and/or various barcharts or scatterplots related to document metadata. Although these tools can be useful for browsing a corpus, we seek a more compact visualization, with the more narrow focus of quickly and easily understanding the individual topics themselves (without necessarily visual</context>
</contexts>
<marker>Gardner, Lutes, Lund, Hansen, Walker, Ringger, Seppi, 2010</marker>
<rawString>Matthew J. Gardner, Joshua Lutes, Jeff Lund, Josh Hansen, Dan Walker, Eric Ringger, and Kevin Seppi. 2010. The topic browser: An interactive tool for browsing topic models. NIPS Workshop on Challenges of Data Visualization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brynjar Gretarsson</author>
<author>John O’Donovan</author>
<author>Svetlin Bostandjieb</author>
<author>Tobias Hollerer</author>
<author>Arthur Asuncion</author>
<author>David Newman</author>
<author>Padhraic Smyth</author>
</authors>
<title>TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology,</booktitle>
<pages>1--26</pages>
<marker>Gretarsson, O’Donovan, Bostandjieb, Hollerer, Asuncion, Newman, Smyth, 2011</marker>
<rawString>Brynjar Gretarsson, John O’Donovan, Svetlin Bostandjieb, Tobias Hollerer, Arthur Asuncion, David Newman, and Padhraic Smyth. 2011. TopicNets: Visual Analysis of Large Text Corpora with Topic Modeling. ACM Transactions on Intelligent Systems and Technology, pp 1-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<publisher>PNAS.</publisher>
<contexts>
<context position="1618" citStr="Griffiths and Steyvers, 2004" startWordPosition="264" endWordPosition="267"> relevance to better understand a fitted LDA model. 1 Introduction Recently much attention has been paid to visualizing the output of topic models fit using Latent Dirichlet Allocation (LDA) (Gardner et al., 2010; Chaney and Blei, 2012; Chuang et al., 2012b; Gretarsson et al., 2011). Such visualizations are challenging to create because of the high dimensionality of the fitted model – LDA is typically applied to many thousands of documents, which are modeled as mixtures of dozens (or hundreds) of topics, which themselves are modeled as distributions over thousands of terms (Blei et al., 2003; Griffiths and Steyvers, 2004). The most promising basic technique for creating LDA visualizations that are both compact and thorough is interactivity. We introduce an interactive visualization system that we call LDAvis that attempts to answer Kenneth E. Shirley AT&amp;T Labs Research 33 Thomas Street, 26th Floor New York, NY 10007, USA kshirley@research.att.com a few basic questions about a fitted topic model: (1) What is the meaning of each topic?, (2) How prevalent is each topic?, and (3) How do the topics relate to each other? Different visual components answer each of these questions, some of which are original, and some</context>
<context position="15626" citStr="Griffiths and Steyvers, 2004" startWordPosition="2904" endWordPosition="2907">regarding motorcycles, automobiles, or electronics, the top-5 most relevant terms given A = 1/3 are {oil, plastic, pipes, fluid, and lights}. The second set of terms is much more descriptive of the topic being discussed than the first. 3.2 User Study We conducted a user study to determine whether there was an optimal value of A in the definition of relevance to aid topic interpretation. First, we fit a 50-topic model to the D = 13,695 documents in the 20 Newsgroups data which were posted to a single Newsgroup (rather than two or more Newsgroups). We used the Collapsed Gibbs Sampler algorithm (Griffiths and Steyvers, 2004) to sample the latent topics for each of the N = 1, 590, 376 tokens in the data, and we saved their topic assignments from the last iteration (after convergence). We then computed the 20 by 50 table, T, which contains, in cell Tgk, the count of the number of times a token from topic k E {1, ..., 50} was assigned to Newsgroup g E {1, ..., 20}, where we defined the Newsgroup of a token to be the Newsgroup to which the document containing that token was posted. Some of the LDA-inferred topics occurred almost exclusively (&gt; 90% of occur66 rences) in documents from a single Newsgroup, such as Topic</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing Semantic Coherence in Topic Models.</title>
<date>2011</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="6383" citStr="Mimno et al. (2011)" startWordPosition="1073" endWordPosition="1076">isualization of topic models. 2.1 Topic Interpretation and Coherence It is well-known that the topics inferred by LDA are not always easily interpretable by humans. Chang et al. (2009) established via a large user study that standard quantitative measures of fit, such as those summarized by Wallach et al. (2009), do not necessarily agree with measures of topic interpretability by humans. Ramage et al. (2009) assert that “characterizing topics is hard” and describe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some notion of topical ground truth, such as document metadata or expert-created topic labels. These methods are useful for understanding, in a global sense, which topics are interpretable (and why), but they don’t specifically attempt to aid the user in interpreting individual topics. Blei and Lafferty (2009) developed “Turbo Topics”, a method of identifying n-grams within LDAinferred topics that, when listed in decreasing order of probabi</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing Semantic Coherence in Topic Models. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Youn Noh</author>
<author>Edmund Talley</author>
<author>Sarvnaz Karimi</author>
<author>Timothy Baldwin</author>
</authors>
<title>Evaluating Topic Models for Digital Libraries.</title>
<date>2010</date>
<publisher>JCDL.</publisher>
<contexts>
<context position="7344" citStr="Newman et al. (2010)" startWordPosition="1227" endWordPosition="1230">ble (and why), but they don’t specifically attempt to aid the user in interpreting individual topics. Blei and Lafferty (2009) developed “Turbo Topics”, a method of identifying n-grams within LDAinferred topics that, when listed in decreasing order of probability, provide users with extra information about the usage of terms within topics. This two-stage process yields good results on experimental data, although the resulting output is still simply a ranked list containing a mixture of terms and n-grams, and the usefulness of the method for topic interpretation was not tested in a user study. Newman et al. (2010) describe a method for ranking terms within topics to aid interpretability called Pointwise Mutual Information (PMI) ranking. Under PMI ranking of terms, each of the ten most probable terms within a topic are ranked in decreasing order of approximately how often they occur in close proximity to the nine other most probable terms from that topic in some large, external “reference” corpus, such as Wikipedia or Google n-grams. Although this method correlated highly with human judgments of term importance within topics, it does not easily generalize to topic models fit to corpora that don’t have a</context>
</contexts>
<marker>Newman, Noh, Talley, Karimi, Baldwin, 2010</marker>
<rawString>David Newman, Youn Noh, Edmund Talley, Sarvnaz Karimi, and Timothy Baldwin 2010. Evaluating Topic Models for Digital Libraries. JCDL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Core</author>
</authors>
<title>Team 2014. R: A Language and Environment for Statistical Computing.</title>
<location>http://www.Rproject.org.</location>
<marker>Core, </marker>
<rawString>R Core Team 2014. R: A Language and Environment for Statistical Computing. http://www.Rproject.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Studio</author>
<author>Inc</author>
</authors>
<title>shiny: Web Application Framework for R; package version 0.9.1.</title>
<date>2014</date>
<note>http://CRAN.Rproject.org/package=shiny.</note>
<marker>Studio, Inc, 2014</marker>
<rawString>R Studio, Inc. 2014. shiny: Web Application Framework for R; package version 0.9.1. http://CRAN.Rproject.org/package=shiny.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Evan Rosen</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Daniel A McFarland</author>
</authors>
<title>Topic Modeling for the Social Sciences. NIPS Workshop on Applications for Topic Models: Text and Beyond.</title>
<date>2009</date>
<contexts>
<context position="6175" citStr="Ramage et al. (2009)" startWordPosition="1037" endWordPosition="1040"> we incorporate relevance into our interactive visualization in Section 4. 2 Related Work Much work has been done recently regarding the interpretation of topics (i.e. measuring topic “coherence”) as well as visualization of topic models. 2.1 Topic Interpretation and Coherence It is well-known that the topics inferred by LDA are not always easily interpretable by humans. Chang et al. (2009) established via a large user study that standard quantitative measures of fit, such as those summarized by Wallach et al. (2009), do not necessarily agree with measures of topic interpretability by humans. Ramage et al. (2009) assert that “characterizing topics is hard” and describe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some notion of topical ground truth, such as document metadata or expert-created topic labels. These methods are useful for understanding, in a global sense, which topics are interpretable (and why), but they don’t specifically attempt </context>
</contexts>
<marker>Ramage, Rosen, Chuang, Manning, McFarland, 2009</marker>
<rawString>Daniel Ramage, Evan Rosen and Jason Chuang and Christopher D. Manning, and Daniel A. McFarland. 2009. Topic Modeling for the Social Sciences. NIPS Workshop on Applications for Topic Models: Text and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Snyder</author>
<author>Rebecca Knowles</author>
<author>Mark Dredze</author>
<author>Matthew Gormley</author>
<author>Travis Wolfe</author>
</authors>
<title>Topic Models and Metadata for Visualizing Text Corpora.</title>
<date>2013</date>
<booktitle>Proceedings of the 2013 NAACL HLT Demonstration Session.</booktitle>
<contexts>
<context position="9606" citStr="Snyder et al., 2013" startWordPosition="1607" endWordPosition="1610"> recommend it as a way to rank terms to aid topic interpretation. We propose a similar method that is a weighted average of the logarithms of a term’s probability and its lift, and we justify it with a user study and incorporate it into our interactive visualization. 2.2 Topic Model Visualization Systems A number of visualization systems for topic models have been developed in recent years. Several of them focus on allowing users to browse documents, topics, and terms to learn about the relationships between these three canonical topic model units (Gardner et al., 2010; Chaney and Blei, 2012; Snyder et al., 2013). These browsers typically use lists of the most probable terms within topics to summarize the topics, and the visualization elements are limited to barcharts or word clouds of term probabilities for each topic, pie charts of topic probabilities for each document, and/or various barcharts or scatterplots related to document metadata. Although these tools can be useful for browsing a corpus, we seek a more compact visualization, with the more narrow focus of quickly and easily understanding the individual topics themselves (without necessarily visualizing documents). Chuang et al. (2012b) devel</context>
</contexts>
<marker>Snyder, Knowles, Dredze, Gormley, Wolfe, 2013</marker>
<rawString>Justin Snyder, Rebecca Knowles, Mark Dredze, Matthew Gormley, and Travis Wolfe. 2013. Topic Models and Metadata for Visualizing Text Corpora. Proceedings of the 2013 NAACL HLT Demonstration Session.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Matthew</author>
</authors>
<title>Taddy 2011. On Estimation and Selection for Topic Models.</title>
<publisher>AISTATS.</publisher>
<marker>Matthew, </marker>
<rawString>Matthew A. Taddy 2011. On Estimation and Selection for Topic Models. AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
<author>Iain Murray</author>
<author>Ruslan Salakhutdinov</author>
<author>David Mimno</author>
</authors>
<title>Evaluation Methods for Topic Models.</title>
<date>2009</date>
<publisher>ICML.</publisher>
<contexts>
<context position="6077" citStr="Wallach et al. (2009)" startWordPosition="1022" endWordPosition="1025">definition of relevance to aid the task of topic interpretation in Section 3.2, and we describe how we incorporate relevance into our interactive visualization in Section 4. 2 Related Work Much work has been done recently regarding the interpretation of topics (i.e. measuring topic “coherence”) as well as visualization of topic models. 2.1 Topic Interpretation and Coherence It is well-known that the topics inferred by LDA are not always easily interpretable by humans. Chang et al. (2009) established via a large user study that standard quantitative measures of fit, such as those summarized by Wallach et al. (2009), do not necessarily agree with measures of topic interpretability by humans. Ramage et al. (2009) assert that “characterizing topics is hard” and describe how using the top-k terms for a given topic might not always be best, but offer few concrete alternatives. AlSumait et al. (2009), Mimno et al. (2011), and Chuang et al. (2013b) develop quantitative methods for measuring the interpretability of top64 ics based on experiments with data sets that come with some notion of topical ground truth, such as document metadata or expert-created topic labels. These methods are useful for understanding,</context>
</contexts>
<marker>Wallach, Murray, Salakhutdinov, Mimno, 2009</marker>
<rawString>Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. 2009. Evaluation Methods for Topic Models. ICML.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
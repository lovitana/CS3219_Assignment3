<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001119">
<title confidence="0.9991655">
Dialogue Strategy Learning in Healthcare: A Systematic Approach for
Learning Dialogue Models from Data
</title>
<author confidence="0.801861">
Hamid R. Chinaei Brahim Chaib-draa
</author>
<email confidence="0.804718">
Hamidreza.Chinaei.1@ulaval.ca Brahim.Chaib-Draa@ift.ulaval.ca
</email>
<sectionHeader confidence="0.988032" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967444444444">
We aim to build dialogue agents that op-
timize the dialogue strategy, specifically
through learning the dialogue model com-
ponents from dialogue data. In this paper,
we describe our current research on au-
tomatically learning dialogue strategies in
the healthcare domain. We go through our
systematic approach of learning dialogue
model components from data, specifically
user intents and the user model, as well
as the agent reward function. We demon-
strate our experiments on healthcare data
from which we learned the dialogue model
components. We conclude by describ-
ing our current research for automatically
learning dialogue features that can be used
in representing dialogue states and learn-
ing the reward function.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985152631579">
Cognitive assistive technologies provide support
systems for the elderly, possibly with cognitive or
physical disabilities, for instance people with de-
mentia (such as Alzheimer’s disease) (Boger et al.,
2005; Pineau et al., 2011; Rudzicz et al., 2012).
Such support systems can significantly reduce the
costs of performing several tasks, currently done
by family members or employed caregivers. In
this context, (Rudzicz et al., 2012) are working
on a computerized caregiver that assist individuals
with Alzheimer’s disease (AD) to complete daily
tasks (e.g., preparing meals) using verbal commu-
nication. Thus, an important component of such
technologies is the dialogue agent.
Table 1 (left) shows sample dialogues collected
by SmartWheeler, an intelligent wheelchair for
persons with disabilities (Pineau et al., 2011). In
particular, SmartWheeler aims to minimize the
physical and cognitive load required in steering it.
</bodyText>
<page confidence="0.985307">
13
</page>
<bodyText confidence="0.999920787878788">
SmartWheeler is equipped with a dialogue agent,
thus the users can give their commands through
the spoken language besides a joystick.
The first line denoted by u1 shows the true user
utterance, which is the one that has been extracted
manually from user audio recordings. The follow-
ing line denoted by ˜u1 is the recognized user utter-
ances by automatic speech recognition (ASR). Fi-
nally, the line denoted by a1 shows the performed
action in response to the ASR output at the time
of collecting the dialogues. First, the users may
say a command in different ways. For instance for
turning right, the user may say turn right a little
please, turn right, right a little, or right.
Furthermore, challenges can occur when ASR
output is imperfect. That is, for each dialogue
utterance recognized by ASR, the dialogue agent
must estimate the human user intent and then to
perform the best action that satisfies the user in-
tent. The recognized utterance by ASR is not re-
liable for decision making. For instance, the first
utterance, u1: “turn right a little”, shows the true
user utterance. The ASR output for this utterance
is, ˜u1: [10 writer little]. As such, the action per-
formed by SmartWheeler at this dialogue turn is,
the general query action u1: PLEASE REPEAT
YOUR COMMAND.
We are working on automatically learning the
dialogue strategy by learning the dialogue model
components such as user intents, dialogue fea-
tures, and the agent’s reward function from data.
Our data includes dialogues between human users
and expert agents (such as caregivers).
</bodyText>
<sectionHeader confidence="0.908826" genericHeader="introduction">
2 Theoretical approach
</sectionHeader>
<bodyText confidence="0.92920375">
Sequential Decision Making: Figure 1 shows
the cycle of interaction between an agent (such as
a dialogue agent or a caregiver) and its environ-
ment (such as a human user). The agent performs
an action and receives an observation in return.
The observation can be used by the agent, for in-
Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 13–19,
Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics
</bodyText>
<equation confidence="0.783643666666667">
u1 : turn right u1 : turn right
˜u1 : [turn right] ˜u1 : [turn right]
a1 : TURN RIGHT s1 : turn-right-little, 60%
u2 : turn right a little u2 : turn right a little
˜u2 : [10 writer little] ˜u2 : [10 writer little]
a2 : PLEASE REPEAT YOUR COMMAND s2 : turn-right-little, 90%
u3 : turn right a little u3 : turn right a little
˜u3 : [turn right to lead a] ˜u3 : [turn right to lead a]
a3 : TURN RIGHT A LITTLE s3 : turn-right-little, 90%
u4 : stop u4 : stop
˜u4 : [stop] ˜u4 : [stop]
a4 : STOP s4 : stop, 90%
</equation>
<tableCaption confidence="0.9592065">
Table 1: Left: A sample from the SmartWheeler dialogues (Pineau et al., 2011). Right: results of learning
human user intents from patients’ noisy dialogues.
</tableCaption>
<bodyText confidence="0.999568916666667">
stance to update its state and reward. The reward
works as reinforcement from the environment that
shows how well the agent performed. In sequen-
tial decision making, the agent is required to make
decision for sequence of states rather than making
a one-shot decision. Then, the sequential decision
making is performed with the objective of maxi-
mizing the long term rewards. The sequence of
actions is called a strategy, and the major question
in sequential decision making is how to find a near
optimal strategy.
Reinforcement learning (RL): RL in (partially
observable) Markov decision processes, so called
the (PO)MDPs, is a learning approach in sequen-
tial decision making. In particular, (PO)MDPs
have been successfully applied in dialogue agents
(Roy et al., 2000; Zhang et al., 2001; Williams,
2006; Thomson and Young, 2010; Gaˇsi´c, 2011).
The (PO)MDP framework is a formal framework
to represent uncertainty explicitly while support-
ing automated strategy solving. Specifically, it
is an optimization framework that supports au-
tomated strategy solving by maximizing a “re-
ward function”.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="method">
3 Objective
</sectionHeader>
<bodyText confidence="0.999818">
SDS (Spoken dialogue system) researchers have
addressed several practical challenges of apply-
ing (PO)MDPs to SDS (Williams, 2006; Paek
and Pieraccini, 2008). Specifically, estimating the
user model and the reward function is a signifi-
cant challenge since these model components have
a direct impact on the optimized dialogue strat-
egy. Furthermore, the reward function is perhaps
the most hand-crafted aspect of the optimization
frameworks such as (PO)MDPs (Paek and Pierac-
</bodyText>
<page confidence="0.989251">
14
</page>
<bodyText confidence="0.999958952380952">
cini, 2008). Using inverse reinforcement learning
(IRL) techniques, a reward function can be deter-
mined from expert actions (such as caregiver ac-
tions) (Ng and Russell, 2000). Fortunately, learn-
ing the reward function using IRL methods have
already been proposed for the general (PO)MDP
framework (Ng and Russell, 2000; Kim et al.,
2011), paving the way for investigating its use for
dialogue (PO)MDPs. In this context, the IRL algo-
rithms require dialogue features (for instance ASR
recognitions with their confidence scores) for rep-
resenting the reward function. Extracting relevant
dialogue features is important since the dialogue
features and their representation highly affect the
learned reward function and finally the optimized
strategy.
Thus, our goals include building (PO)MDP-
based dialogue technologies that optimizes the di-
alogue strategy through learning user intents and
the user model, and reward function from dialogue
data, as follows:
</bodyText>
<listItem confidence="0.902318571428572">
1. Learning user intents and the user model
from collected dialogues, i.e., ASR recogni-
tions, or directly from acoustic data.
2. Learning the reward function.
(a) Learning useful dialogue features.
(b) Representing features in IRL for learn-
ing the reward function.
</listItem>
<bodyText confidence="0.954708428571429">
Recall Figure 1 that shows the cycle of interac-
tion between an agent (such as a dialogue agent or
a caregiver) and its environment (such as a human
user). In this figure, circles represent the learned
models. The model denoted by (PO)MDP in-
cludes the (PO)MDP model components, without
acting
</bodyText>
<figureCaption confidence="0.647914">
Figure 1: The cycle of acting/learning between the agent and environment. The circles represent the
models. The model denoted by (PO)MDP includes the (PO)MDP model components, without a reward
function, learned from step 1 in the objective section. The learned (PO)MDP model together with expert
action/observation trajectories are used in IRL to learn the reward function denoted by R, in step 2 in
the objective section. The learned (PO)MDP and reward function are used in the (PO)MDP solver to
learn/update the strategy.
</figureCaption>
<figure confidence="0.9982737">
Environment
trajectories
a/o
IRL
(PO)MDP
R
(PO)MDP
solver
learning
Agent
</figure>
<bodyText confidence="0.958156714285714">
a reward function, which have been learned from
step 1 above. The learned (PO)MDP together with
action/observation trajectories are used in IRL to
learn the reward function, denoted by R. Then,
the learned (PO)MDP and the reward function are
used in a (PO)MDP solver to learn/update the op-
timal strategy.
</bodyText>
<sectionHeader confidence="0.979417" genericHeader="method">
4 SmartWheeler data
</sectionHeader>
<bodyText confidence="0.999869411764706">
The SmartWheeler project aims to build an in-
telligent wheelchair for persons with disabil-
ities (Pineau et al., 2011). In particular,
SmartWheeler aims to minimize the physical and
cognitive load required in steering it. This project
has been initiated in 2006, and a first prototype,
shown in Figure 2, was built in-house at McGill’s
Center for Intelligent Machines.
We used the dialogues collected by
SmartWheeler to develop dialogue (PO)MDPs,
learned primarily from data. The data includes
eight dialogues with healthy users and nine dia-
logues with target users of SmartWheeler (Pineau
et al., 2011). The dialogues with target users,
who are the elderly, are somehow more noisy than
the ones with healthy users. More specifically,
the average word error rate (WER) equals 13.9%15
</bodyText>
<figureCaption confidence="0.9468425">
Figure 2: The SmartWheeler robot plat-
form (Pineau et al., 2011).
</figureCaption>
<bodyText confidence="0.999942166666667">
for the healthy user dialogues and 18.5% for the
target user dialogues. In order to perform our
experiments on a larger amount of data, we used
all the healthy and target user dialogues. In total,
there are 2853 user utterances and 422 distinct
words in the SmartWheeler dialogues.
</bodyText>
<sectionHeader confidence="0.861105" genericHeader="method">
5 Learning user intents from data
</sectionHeader>
<bodyText confidence="0.99879035">
We learned the (PO)MDP states by learning the
user intents occurred in the dialogue set using
a topic modeling approach, i.e., Hidden Topic
Markov Model (HTMM) (Gruber et al., 2007).
HTMM is a variation of Latent Dirichlet Alloca-
tion (LDA) which learns topics from text based on
co-occurrence of words and using Dirichlet dis-
tribution for generating the topics of text docu-
ments (Blei et al., 2003). HTMM adds Markovian
assumption to the LDA model in order to exploit
the Markovian property between sentences in the
documents. Thus, HTMM can be seen both as a
variation of Hidden Markov Model (HMM) and a
variation of LDA.
Our experimental results showed that HTMM
learns proper user intents that can be used as dia-
logue states, and is able to exploit the Markovian
property between dialogue utterances, adequately.
The learned states, using our proposed methods,
from SmartWheeler data are as follows: s1 :
</bodyText>
<equation confidence="0.76363375">
move-forward-little, s2 : move-backward-little,
s3 : turn-right-little, s4 : turn-left-little, s5 :
follow-left-wall, s6 : follow-right-wall, s7 :
turn-degree-right, s8 : go-door, s9 : set-speed,
</equation>
<bodyText confidence="0.984398846153846">
s10 :follow-person, s11 : stop. Table 3 shows the
learned user intents, five of them, with their top-
four words, i.e., the intent keywords.
Table 1 (right) shows results of HTMM appli-
cation on SmartWheeler for the example shown
in Table 1 (left). For instance, the second ut-
terance shows that the user actually uttered turn
right a little, but it is recognized as 10 writer lit-
tle by ASR. The most probable intent returned by
HTMM for this utterance is s3 : turn-right-little
with 90% probability. This is because HTMM
considers Markovian property for deriving intents.
As a result, in the second turn it estimates correctly
the true user intent based on the user intent in the
first turn.
The list of all SmartWheeler actions are shown
in Table 2. Each action is the right action of
one state (the user intent for a specific com-
mand). So, ideally, there should be 24 states
for SmartWheeler dialogues (There are 24 actions
other than the general query action: REPEAT).
However, we only learned 11 of the states, mainly
because of the number of dialogues. That is, not
all of the states appeared in the data frequently
enough. There are also states that do not appear
in dialogues at all.
</bodyText>
<sectionHeader confidence="0.863214" genericHeader="method">
6 Learning reward functions from data
</sectionHeader>
<bodyText confidence="0.987628">
In this section, we experiment our implementation
of the trajectory-based MDP-IRL algorithm pro-
</bodyText>
<page confidence="0.994552">
16
</page>
<table confidence="0.997713">
a1 DRIVE FORWARD A LITTLE
a2 DRIVE BACKWARD A LITTLE
a3 TURN RIGHT A LITTLE
a4 TURN LEFT A LITTLE
a5 FOLLOW THE LEFT WALL
a6 FOLLOW THE RIGHT WALL
a7 TURN RIGHT DEGREE
a8 GO THROUGH THE DOOR
a9 SET SPEED TO MEDIUM
a10 FOLLOW THE WALL
a11 STOP
a12 TURN LEFT
a13 DRIVE FORWARD
a14 APPROACH THE DOOR
a15 DRIVE BACKWARD
a16 SET SPEED TO SLOW
a17 MOVE ON SLOPE
a18 TURN AROUND
a19 PARK TO THE RIGHT
a20 TURN RIGHT
a21 DRIVE FORWARD METER
a22 PARK TO THE LEFT
a23 TURN LEFT DEGREE
a24 PLEASE REPEAT YOUR COMMAND
</table>
<tableCaption confidence="0.980655">
Table 2: The list of the possible actions, performed
by SmartWheeler.
</tableCaption>
<bodyText confidence="0.9982634">
posed by (Ng and Russell, 2000). The IRL ex-
periments are designed to verify if the introduced
IRL methods are able to learn a reward function
for the expert strategy, where the expert strategy is
represented as a (PO)MDP strategy. That is, the
expert strategy is the strategy that the underlying
(PO)MDP framework optimizes. The MDP expert
strategy for each of the (PO)MDP state is repre-
sented in Table 4. This strategy suggests perform-
ing the right action of each state.
</bodyText>
<subsectionHeader confidence="0.995763">
6.1 MDP-IRL learned rewards
</subsectionHeader>
<bodyText confidence="0.8710932">
We applied the MDP-IRL algorithm on
SmartWheeler dialogue MDP described above
using the introduced keyword features in Table 5.
The algorithm was able to learn a reward function
in which the strategy equals the expert strategy for
all states, (the expert strategy shown in Table 4).
Table 6 shows the learned reward function. Note
that, for instance for state s3: turn-right-little, the
reward of performing both actions a3: TURN
RIGHT A LITTLE and a4: FOLLOW THE
RIGHT WALL is close to 1. Nevertheless,
the optimized strategy for this reward function
suggest the correct action, i.e., TURN RIGHT A
LITTLE for this state (turn-right-little).
intent 1 intent 2 intent 3 intent 4 intent 11
forward 18.0% backward 38.0% right 20.9% left 18.9% ... stop 94.2%
move 16.1% drive 33.3% turn 17.1% turn 17.1% stopp 02.2%
little 11.4% little 10.9% little 13.1% little 13.8% scott 00.7%
drive 08.1% top 01.7% bit 07.4% right 09.0% . . . . . . but 00.2%
... ... ... ... ... ... ... ... ... ...
</bodyText>
<tableCaption confidence="0.979821">
Table 3: The learned user intents from the SmartWheeler dialogues and their top words. Each percentage
shows the probability of each word given the intent.
</tableCaption>
<table confidence="0.999632333333333">
state state description expert action expert action description
s1 move-forward-little a1 DRIVE FORWARD A LITTLE
s2 move-backward-little a2 DRIVE BACKWARD A LITTLE
s3 turn-right-little a3 TURN RIGHT A LITTLE
s4 turn-left-little a4 TURN LEFT A LITTLE
s5 follow-left-wall a5 FOLLOW THE LEFT WALL
s6 follow-right-wall a6 FOLLOW THE RIGHT WALL
s7 turn-degree-right a7 TURN RIGHT DEGREES
s8 go-door a8 GO THROUGH THE DOOR
s9 set-speed a9 SET SPEED TO MEDIUM
s10 follow-wall a10 FOLLOW THE WALL
s11 stop a11 STOP
</table>
<tableCaption confidence="0.999913">
Table 4: The learned strategy using the learned dialogue MDP from SmartWheeler dialogues.
</tableCaption>
<bodyText confidence="0.997802666666667">
forward backward right left turn go for top stop
s1 1 0 0 0 0 0 0 0 0
s2 0 1 0 0 0 0 0 0 0
s3 0 0 1 0 0 0 0 0 0
s4 0 0 0 1 0 0 0 0 0
s5 0 0 0 1 0 0 0 0 0
s6 0 0 1 0 0 0 0 0 0
s7 0 0 0 0 1 0 0 0 0
s8 0 0 0 0 0 1 0 0 0
s9 0 0 0 0 0 0 1 0 0
s10 0 0 0 0 0 0 0 1 0
s11 0 0 0 0 0 0 0 0 1
</bodyText>
<tableCaption confidence="0.672987">
Table 5: Keyword features for the SmartWheeler dialogues.
</tableCaption>
<bodyText confidence="0.915410583333333">
a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 ... REPEAT
s1 1.0 0 0 0 0 0 0 0 0 0 0 0 ... 0
s2 0 1.0 0 0 0 0 0 0 0 0 0 0 ... 0
s3 0 0 1.0 0 0 1.0 0 0 0 0 0 0 ... 0
s4 0 0 0 1.0 1.0 0 0 0 0 0 0 0 ... 0
s5 0 0 0 1.0 1.0 0 0 0 0 0 0 0 ... 0
s6 0 0 1.0 0 0 1.0 0 0 0 0 0 0 ... 0
s7 0 0 0 0 0 0 1.0 0 0 0 0 0 ... 0
s8 0 0 0 0 0 0 0 1.0 0 0 0 0 ... 0
s9 0 0 0 0 0 0 0 0 1.0 0 0 0 ... 0
s10 0 0 0 0 0 0 0 0 0 1.0 0 0 ... 0
s11 0 0 0 0 0 0 0 0 0 0 1.0 0 ... 0
</bodyText>
<tableCaption confidence="0.981851">
Table 6: The learned reward function for the learned dialogue MDP from SmartWheeler dialogues using
</tableCaption>
<bodyText confidence="0.452257">
keyword features. 17
</bodyText>
<subsectionHeader confidence="0.998853">
6.2 Choice of features
</subsectionHeader>
<bodyText confidence="0.996082965517241">
IRL needs features to represent the reward func-
tion. We propose keyword features for applying
IRL on the learned dialogue MDP/POMDP from
SmartWheeler. The keyword features are automat-
ically learned as the top-one words for each user
intent (see Table 3). There are nine learned key-
words:
forward, backward, right, left, turn, go, for,
top, stop.
The keyword features for each state of
SmartWheeler dialogue POMDP are represented
in a vector, as shown in Table 5. The fig-
ure shows that states s3, (turn-right-little) and
s6 (follow-right-wall) share the same features,
i.e., right. Moreover, states s4 (turn-left-little)
and s5 (follow-left-wall) share the same feature,
i.e., left. In our experiments, we used keyword-
action-wise feature representation. Such features
include an indicator function for each pair of state-
keyword and action. Thus, the feature size for
SmartWheeler equals 216 = 9 × 24 (9 keywords
and 24 actions).
Note that the choice of features is application
dependent. The reason for using keywords as state
features is that in the intent-based dialogue appli-
cations the states are the dialogue intents, where
each intent is described as a vector of k-top words
from the domain dialogues. Therefore, the key-
word features are relevant features for the states.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99993525">
In this paper, we described our our systematic
approach for learning dialogue (PO)MDP model
components from unannotated dialogues. In
our approach, we start by learning the dialogue
(PO)MDP states, i.e., the learned user intents from
data. The learned states were then used for learn-
ing the user model. Building off these model com-
ponents, we learned the agent’s reward function by
implementing a model-based IRL algorithm. We
demonstrated our experiments on data collected in
a healthcare domain to learn the dialogue model
components solely from data.
</bodyText>
<sectionHeader confidence="0.999153" genericHeader="method">
8 Ongoing work
</sectionHeader>
<bodyText confidence="0.999981214285714">
We are working on a variation of MDP-IRL algo-
rithm, that is a model-free trajectory-based MDP-
IRL algorithm. In the model-free MDPs, the
states are usually presented using features (and18
thus there is no defined/learned transition model).
Then, model-free MDP algorithms are used for
estimating the optimal strategy of such MDPs.
Model-free MDPs can be used in the place of
POMDPs where state features are analogous to ob-
servations.
In this context, data analysis for feature selec-
tion is highly important. Dialogue features can
be used to represent dialogue situations (as well
as the observations in the dialogue POMDPs).
Moreover, the IRL algorithms require (dialogue)
features for representing the reward function.
As mentioned earlier, the reward function of
(PO)MDPs highly affects the optimized strategy.
A relevant reward function to the dialogue agent
and users can only be learned by studying and
extracting relevant features from the dialogue do-
main. We would like to learn the relevant and
proper features that are suitable for both state fea-
tures as well as the reward representation. In par-
ticular, we are going to use the experts’ (care-
givers’) strategies in the place of a (PO)MDP strat-
egy in order to learn a reward function that ac-
counts for caregivers’ strategies.
</bodyText>
<sectionHeader confidence="0.991605" genericHeader="conclusions">
9 Acknowledgment
</sectionHeader>
<bodyText confidence="0.999951357142857">
The authors thank Jason D. Williams and Suhrid
Balakrishnan for helpful discussions in the early
development of this work. The authors also
thank Joelle Pineau for providing them with
the Smartwheeler data. The dataset has been
collected with contributions from researchers
at McGill University, ´Ecole Polytechnique de
Montr´eal, Universit´e de Montr´eal, and the Cen-
tre de r´eadaptation Lucie-Bruneau and Constance-
Lethbridge. The authors thank Ethan Selfridge
for his help in proofreading. Last but not least,
many thank to FQRNT (Fonds Qu´eb´ecois de la
recherche sur la nature et les technologies) for par-
tial financial support of this work.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999922629032258">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jennifer Boger, Pascal Poupart, Jesse Hoey, Craig
Boutilier, Geoff Fernie, and Alex Mihailidis. 2005.
A decision-theoretic approach to task assistance for
persons with dementia. In Proceedings of the 19th
International Joint Conference on Artificial Intelli-
gence (IJCAI’05), pages 1293–1299, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Milica Gaˇsi´c. 2011. Statistical Dialogue Modelling.
Ph.D. thesis, Department of Engineering, University
of Cambridge.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss.
2007. Hidden topic Markov models. In Artificial
Intelligence and Statistics (AISTATS’07), San Juan,
Puerto Rico, USA.
D. Kim, J. Kim, and K.E. Kim. 2011. Robust per-
formance evaluation of POMDP-based dialogue sys-
tems. IEEE Transactions on Audio, Speech, and
Language Processing, 19(4):1029–1040.
Andrew Y. Ng and Stuart J. Russell. 2000. Algorithms
for inverse reinforcement learning. In Proceedings
of the 17th International Conference on Machine
Learning (ICML’00), Stanford, CA, USA.
T. Paek and R. Pieraccini. 2008. Automating spoken
dialogue management design using machine learn-
ing: An industry perspective. Speech Communica-
tion, 50(8):716–729.
Joelle Pineau, Robert West, Amin Atrash, Julien Ville-
mure, and Francois Routhier. 2011. On the fea-
sibility of using a standardized test for evaluat-
ing a speech-controlled smart wheelchair. Interna-
tional Journal of Intelligent Control and Systems,
16(2):124–131.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics (ACL’00), Hong Kong.
Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Eliz-
abeth Rochon, and Carol Leonard. 2012. Commu-
nication strategies for a computerized caregiver for
individuals with alzheimer’s disease. In Proceed-
ings of the Third Workshop on Speech and Language
Processing for Assistive Technologies, (SLPAT’12),
pages 47–55, Montreal, Quebec, Canada. Associa-
tion for Computational Linguistics.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562–588.
Jason D. Williams. 2006. Partially Observable
Markov Decision Processes for Spoken Dialogue
Management. Ph.D. thesis, Department of Engi-
neering, University of Cambridge.
Bo Zhang, Qingsheng Cai, Jianfeng Mao, and Bain-
ing Guo. 2001. Planning and acting under uncer-
tainty: A new model for spoken dialogue system. In
Proceedings of the 17th Conference in Uncertainty
in Artificial Intelligence (UAI’01), Seattle, Washing-
ton, USA, August.
</reference>
<page confidence="0.999333">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.702914">
<title confidence="0.9996605">Dialogue Strategy Learning in Healthcare: A Systematic Approach for Learning Dialogue Models from Data</title>
<author confidence="0.85112">Hamid R Chinaei Brahim Chaib-draa Hamidreza Chinaei ulaval ca Brahim Chaib-Draaift ulaval ca</author>
<abstract confidence="0.999212263157895">We aim to build dialogue agents that optimize the dialogue strategy, specifically through learning the dialogue model components from dialogue data. In this paper, we describe our current research on automatically learning dialogue strategies in the healthcare domain. We go through our systematic approach of learning dialogue model components from data, specifically user intents and the user model, as well as the agent reward function. We demonstrate our experiments on healthcare data from which we learned the dialogue model components. We conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="10173" citStr="Blei et al., 2003" startWordPosition="1617" endWordPosition="1620">experiments on a larger amount of data, we used all the healthy and target user dialogues. In total, there are 2853 user utterances and 422 distinct words in the SmartWheeler dialogues. 5 Learning user intents from data We learned the (PO)MDP states by learning the user intents occurred in the dialogue set using a topic modeling approach, i.e., Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). HTMM is a variation of Latent Dirichlet Allocation (LDA) which learns topics from text based on co-occurrence of words and using Dirichlet distribution for generating the topics of text documents (Blei et al., 2003). HTMM adds Markovian assumption to the LDA model in order to exploit the Markovian property between sentences in the documents. Thus, HTMM can be seen both as a variation of Hidden Markov Model (HMM) and a variation of LDA. Our experimental results showed that HTMM learns proper user intents that can be used as dialogue states, and is able to exploit the Markovian property between dialogue utterances, adequately. The learned states, using our proposed methods, from SmartWheeler data are as follows: s1 : move-forward-little, s2 : move-backward-little, s3 : turn-right-little, s4 : turn-left-lit</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Boger</author>
<author>Pascal Poupart</author>
<author>Jesse Hoey</author>
<author>Craig Boutilier</author>
<author>Geoff Fernie</author>
<author>Alex Mihailidis</author>
</authors>
<title>A decision-theoretic approach to task assistance for persons with dementia.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI’05),</booktitle>
<pages>1293--1299</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="1149" citStr="Boger et al., 2005" startWordPosition="160" endWordPosition="163">logue model components from data, specifically user intents and the user model, as well as the agent reward function. We demonstrate our experiments on healthcare data from which we learned the dialogue model components. We conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function. 1 Introduction Cognitive assistive technologies provide support systems for the elderly, possibly with cognitive or physical disabilities, for instance people with dementia (such as Alzheimer’s disease) (Boger et al., 2005; Pineau et al., 2011; Rudzicz et al., 2012). Such support systems can significantly reduce the costs of performing several tasks, currently done by family members or employed caregivers. In this context, (Rudzicz et al., 2012) are working on a computerized caregiver that assist individuals with Alzheimer’s disease (AD) to complete daily tasks (e.g., preparing meals) using verbal communication. Thus, an important component of such technologies is the dialogue agent. Table 1 (left) shows sample dialogues collected by SmartWheeler, an intelligent wheelchair for persons with disabilities (Pineau </context>
</contexts>
<marker>Boger, Poupart, Hoey, Boutilier, Fernie, Mihailidis, 2005</marker>
<rawString>Jennifer Boger, Pascal Poupart, Jesse Hoey, Craig Boutilier, Geoff Fernie, and Alex Mihailidis. 2005. A decision-theoretic approach to task assistance for persons with dementia. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI’05), pages 1293–1299, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milica Gaˇsi´c</author>
</authors>
<title>Statistical Dialogue Modelling.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Engineering, University of Cambridge.</institution>
<marker>Gaˇsi´c, 2011</marker>
<rawString>Milica Gaˇsi´c. 2011. Statistical Dialogue Modelling. Ph.D. thesis, Department of Engineering, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
<author>Yair Weiss</author>
</authors>
<title>Hidden topic Markov models.</title>
<date>2007</date>
<booktitle>In Artificial Intelligence and Statistics (AISTATS’07),</booktitle>
<location>San Juan, Puerto Rico, USA.</location>
<contexts>
<context position="9956" citStr="Gruber et al., 2007" startWordPosition="1580" endWordPosition="1583">fically, the average word error rate (WER) equals 13.9%15 Figure 2: The SmartWheeler robot platform (Pineau et al., 2011). for the healthy user dialogues and 18.5% for the target user dialogues. In order to perform our experiments on a larger amount of data, we used all the healthy and target user dialogues. In total, there are 2853 user utterances and 422 distinct words in the SmartWheeler dialogues. 5 Learning user intents from data We learned the (PO)MDP states by learning the user intents occurred in the dialogue set using a topic modeling approach, i.e., Hidden Topic Markov Model (HTMM) (Gruber et al., 2007). HTMM is a variation of Latent Dirichlet Allocation (LDA) which learns topics from text based on co-occurrence of words and using Dirichlet distribution for generating the topics of text documents (Blei et al., 2003). HTMM adds Markovian assumption to the LDA model in order to exploit the Markovian property between sentences in the documents. Thus, HTMM can be seen both as a variation of Hidden Markov Model (HMM) and a variation of LDA. Our experimental results showed that HTMM learns proper user intents that can be used as dialogue states, and is able to exploit the Markovian property betwee</context>
</contexts>
<marker>Gruber, Rosen-Zvi, Weiss, 2007</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007. Hidden topic Markov models. In Artificial Intelligence and Statistics (AISTATS’07), San Juan, Puerto Rico, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kim</author>
<author>J Kim</author>
<author>K E Kim</author>
</authors>
<title>Robust performance evaluation of POMDP-based dialogue systems.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="6517" citStr="Kim et al., 2011" startWordPosition="1037" endWordPosition="1040">el and the reward function is a significant challenge since these model components have a direct impact on the optimized dialogue strategy. Furthermore, the reward function is perhaps the most hand-crafted aspect of the optimization frameworks such as (PO)MDPs (Paek and Pierac14 cini, 2008). Using inverse reinforcement learning (IRL) techniques, a reward function can be determined from expert actions (such as caregiver actions) (Ng and Russell, 2000). Fortunately, learning the reward function using IRL methods have already been proposed for the general (PO)MDP framework (Ng and Russell, 2000; Kim et al., 2011), paving the way for investigating its use for dialogue (PO)MDPs. In this context, the IRL algorithms require dialogue features (for instance ASR recognitions with their confidence scores) for representing the reward function. Extracting relevant dialogue features is important since the dialogue features and their representation highly affect the learned reward function and finally the optimized strategy. Thus, our goals include building (PO)MDPbased dialogue technologies that optimizes the dialogue strategy through learning user intents and the user model, and reward function from dialogue da</context>
</contexts>
<marker>Kim, Kim, Kim, 2011</marker>
<rawString>D. Kim, J. Kim, and K.E. Kim. 2011. Robust performance evaluation of POMDP-based dialogue systems. IEEE Transactions on Audio, Speech, and Language Processing, 19(4):1029–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Stuart J Russell</author>
</authors>
<title>Algorithms for inverse reinforcement learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning (ICML’00),</booktitle>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="6354" citStr="Ng and Russell, 2000" startWordPosition="1011" endWordPosition="1014"> researchers have addressed several practical challenges of applying (PO)MDPs to SDS (Williams, 2006; Paek and Pieraccini, 2008). Specifically, estimating the user model and the reward function is a significant challenge since these model components have a direct impact on the optimized dialogue strategy. Furthermore, the reward function is perhaps the most hand-crafted aspect of the optimization frameworks such as (PO)MDPs (Paek and Pierac14 cini, 2008). Using inverse reinforcement learning (IRL) techniques, a reward function can be determined from expert actions (such as caregiver actions) (Ng and Russell, 2000). Fortunately, learning the reward function using IRL methods have already been proposed for the general (PO)MDP framework (Ng and Russell, 2000; Kim et al., 2011), paving the way for investigating its use for dialogue (PO)MDPs. In this context, the IRL algorithms require dialogue features (for instance ASR recognitions with their confidence scores) for representing the reward function. Extracting relevant dialogue features is important since the dialogue features and their representation highly affect the learned reward function and finally the optimized strategy. Thus, our goals include buil</context>
<context position="12810" citStr="Ng and Russell, 2000" startWordPosition="2073" endWordPosition="2076">gorithm pro16 a1 DRIVE FORWARD A LITTLE a2 DRIVE BACKWARD A LITTLE a3 TURN RIGHT A LITTLE a4 TURN LEFT A LITTLE a5 FOLLOW THE LEFT WALL a6 FOLLOW THE RIGHT WALL a7 TURN RIGHT DEGREE a8 GO THROUGH THE DOOR a9 SET SPEED TO MEDIUM a10 FOLLOW THE WALL a11 STOP a12 TURN LEFT a13 DRIVE FORWARD a14 APPROACH THE DOOR a15 DRIVE BACKWARD a16 SET SPEED TO SLOW a17 MOVE ON SLOPE a18 TURN AROUND a19 PARK TO THE RIGHT a20 TURN RIGHT a21 DRIVE FORWARD METER a22 PARK TO THE LEFT a23 TURN LEFT DEGREE a24 PLEASE REPEAT YOUR COMMAND Table 2: The list of the possible actions, performed by SmartWheeler. posed by (Ng and Russell, 2000). The IRL experiments are designed to verify if the introduced IRL methods are able to learn a reward function for the expert strategy, where the expert strategy is represented as a (PO)MDP strategy. That is, the expert strategy is the strategy that the underlying (PO)MDP framework optimizes. The MDP expert strategy for each of the (PO)MDP state is represented in Table 4. This strategy suggests performing the right action of each state. 6.1 MDP-IRL learned rewards We applied the MDP-IRL algorithm on SmartWheeler dialogue MDP described above using the introduced keyword features in Table 5. The</context>
</contexts>
<marker>Ng, Russell, 2000</marker>
<rawString>Andrew Y. Ng and Stuart J. Russell. 2000. Algorithms for inverse reinforcement learning. In Proceedings of the 17th International Conference on Machine Learning (ICML’00), Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>R Pieraccini</author>
</authors>
<title>Automating spoken dialogue management design using machine learning: An industry perspective.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>8</issue>
<contexts>
<context position="5861" citStr="Paek and Pieraccini, 2008" startWordPosition="935" endWordPosition="938"> approach in sequential decision making. In particular, (PO)MDPs have been successfully applied in dialogue agents (Roy et al., 2000; Zhang et al., 2001; Williams, 2006; Thomson and Young, 2010; Gaˇsi´c, 2011). The (PO)MDP framework is a formal framework to represent uncertainty explicitly while supporting automated strategy solving. Specifically, it is an optimization framework that supports automated strategy solving by maximizing a “reward function”. 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of applying (PO)MDPs to SDS (Williams, 2006; Paek and Pieraccini, 2008). Specifically, estimating the user model and the reward function is a significant challenge since these model components have a direct impact on the optimized dialogue strategy. Furthermore, the reward function is perhaps the most hand-crafted aspect of the optimization frameworks such as (PO)MDPs (Paek and Pierac14 cini, 2008). Using inverse reinforcement learning (IRL) techniques, a reward function can be determined from expert actions (such as caregiver actions) (Ng and Russell, 2000). Fortunately, learning the reward function using IRL methods have already been proposed for the general (P</context>
</contexts>
<marker>Paek, Pieraccini, 2008</marker>
<rawString>T. Paek and R. Pieraccini. 2008. Automating spoken dialogue management design using machine learning: An industry perspective. Speech Communication, 50(8):716–729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joelle Pineau</author>
<author>Robert West</author>
<author>Amin Atrash</author>
<author>Julien Villemure</author>
<author>Francois Routhier</author>
</authors>
<title>On the feasibility of using a standardized test for evaluating a speech-controlled smart wheelchair.</title>
<date>2011</date>
<journal>International Journal of Intelligent Control and Systems,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1170" citStr="Pineau et al., 2011" startWordPosition="164" endWordPosition="167">ts from data, specifically user intents and the user model, as well as the agent reward function. We demonstrate our experiments on healthcare data from which we learned the dialogue model components. We conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function. 1 Introduction Cognitive assistive technologies provide support systems for the elderly, possibly with cognitive or physical disabilities, for instance people with dementia (such as Alzheimer’s disease) (Boger et al., 2005; Pineau et al., 2011; Rudzicz et al., 2012). Such support systems can significantly reduce the costs of performing several tasks, currently done by family members or employed caregivers. In this context, (Rudzicz et al., 2012) are working on a computerized caregiver that assist individuals with Alzheimer’s disease (AD) to complete daily tasks (e.g., preparing meals) using verbal communication. Thus, an important component of such technologies is the dialogue agent. Table 1 (left) shows sample dialogues collected by SmartWheeler, an intelligent wheelchair for persons with disabilities (Pineau et al., 2011). In par</context>
<context position="4520" citStr="Pineau et al., 2011" startWordPosition="731" endWordPosition="734">iation for Computational Linguistics u1 : turn right u1 : turn right ˜u1 : [turn right] ˜u1 : [turn right] a1 : TURN RIGHT s1 : turn-right-little, 60% u2 : turn right a little u2 : turn right a little ˜u2 : [10 writer little] ˜u2 : [10 writer little] a2 : PLEASE REPEAT YOUR COMMAND s2 : turn-right-little, 90% u3 : turn right a little u3 : turn right a little ˜u3 : [turn right to lead a] ˜u3 : [turn right to lead a] a3 : TURN RIGHT A LITTLE s3 : turn-right-little, 90% u4 : stop u4 : stop ˜u4 : [stop] ˜u4 : [stop] a4 : STOP s4 : stop, 90% Table 1: Left: A sample from the SmartWheeler dialogues (Pineau et al., 2011). Right: results of learning human user intents from patients’ noisy dialogues. stance to update its state and reward. The reward works as reinforcement from the environment that shows how well the agent performed. In sequential decision making, the agent is required to make decision for sequence of states rather than making a one-shot decision. Then, the sequential decision making is performed with the objective of maximizing the long term rewards. The sequence of actions is called a strategy, and the major question in sequential decision making is how to find a near optimal strategy. Reinfor</context>
<context position="8728" citStr="Pineau et al., 2011" startWordPosition="1381" endWordPosition="1384">ion. The learned (PO)MDP and reward function are used in the (PO)MDP solver to learn/update the strategy. Environment trajectories a/o IRL (PO)MDP R (PO)MDP solver learning Agent a reward function, which have been learned from step 1 above. The learned (PO)MDP together with action/observation trajectories are used in IRL to learn the reward function, denoted by R. Then, the learned (PO)MDP and the reward function are used in a (PO)MDP solver to learn/update the optimal strategy. 4 SmartWheeler data The SmartWheeler project aims to build an intelligent wheelchair for persons with disabilities (Pineau et al., 2011). In particular, SmartWheeler aims to minimize the physical and cognitive load required in steering it. This project has been initiated in 2006, and a first prototype, shown in Figure 2, was built in-house at McGill’s Center for Intelligent Machines. We used the dialogues collected by SmartWheeler to develop dialogue (PO)MDPs, learned primarily from data. The data includes eight dialogues with healthy users and nine dialogues with target users of SmartWheeler (Pineau et al., 2011). The dialogues with target users, who are the elderly, are somehow more noisy than the ones with healthy users. Mo</context>
</contexts>
<marker>Pineau, West, Atrash, Villemure, Routhier, 2011</marker>
<rawString>Joelle Pineau, Robert West, Amin Atrash, Julien Villemure, and Francois Routhier. 2011. On the feasibility of using a standardized test for evaluating a speech-controlled smart wheelchair. International Journal of Intelligent Control and Systems, 16(2):124–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Roy</author>
<author>Joelle Pineau</author>
<author>Sebastian Thrun</author>
</authors>
<title>Spoken dialogue management using probabilistic reasoning.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (ACL’00),</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="5367" citStr="Roy et al., 2000" startWordPosition="864" endWordPosition="867">cision making, the agent is required to make decision for sequence of states rather than making a one-shot decision. Then, the sequential decision making is performed with the objective of maximizing the long term rewards. The sequence of actions is called a strategy, and the major question in sequential decision making is how to find a near optimal strategy. Reinforcement learning (RL): RL in (partially observable) Markov decision processes, so called the (PO)MDPs, is a learning approach in sequential decision making. In particular, (PO)MDPs have been successfully applied in dialogue agents (Roy et al., 2000; Zhang et al., 2001; Williams, 2006; Thomson and Young, 2010; Gaˇsi´c, 2011). The (PO)MDP framework is a formal framework to represent uncertainty explicitly while supporting automated strategy solving. Specifically, it is an optimization framework that supports automated strategy solving by maximizing a “reward function”. 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of applying (PO)MDPs to SDS (Williams, 2006; Paek and Pieraccini, 2008). Specifically, estimating the user model and the reward function is a significant challenge since these m</context>
</contexts>
<marker>Roy, Pineau, Thrun, 2000</marker>
<rawString>Nicholas Roy, Joelle Pineau, and Sebastian Thrun. 2000. Spoken dialogue management using probabilistic reasoning. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (ACL’00), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rudzicz</author>
<author>Rozanne Wilson</author>
<author>Alex Mihailidis</author>
<author>Elizabeth Rochon</author>
<author>Carol Leonard</author>
</authors>
<title>Communication strategies for a computerized caregiver for individuals with alzheimer’s disease.</title>
<date>2012</date>
<booktitle>In Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies, (SLPAT’12),</booktitle>
<pages>47--55</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="1193" citStr="Rudzicz et al., 2012" startWordPosition="168" endWordPosition="171">cally user intents and the user model, as well as the agent reward function. We demonstrate our experiments on healthcare data from which we learned the dialogue model components. We conclude by describing our current research for automatically learning dialogue features that can be used in representing dialogue states and learning the reward function. 1 Introduction Cognitive assistive technologies provide support systems for the elderly, possibly with cognitive or physical disabilities, for instance people with dementia (such as Alzheimer’s disease) (Boger et al., 2005; Pineau et al., 2011; Rudzicz et al., 2012). Such support systems can significantly reduce the costs of performing several tasks, currently done by family members or employed caregivers. In this context, (Rudzicz et al., 2012) are working on a computerized caregiver that assist individuals with Alzheimer’s disease (AD) to complete daily tasks (e.g., preparing meals) using verbal communication. Thus, an important component of such technologies is the dialogue agent. Table 1 (left) shows sample dialogues collected by SmartWheeler, an intelligent wheelchair for persons with disabilities (Pineau et al., 2011). In particular, SmartWheeler a</context>
</contexts>
<marker>Rudzicz, Wilson, Mihailidis, Rochon, Leonard, 2012</marker>
<rawString>Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Elizabeth Rochon, and Carol Leonard. 2012. Communication strategies for a computerized caregiver for individuals with alzheimer’s disease. In Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies, (SLPAT’12), pages 47–55, Montreal, Quebec, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="5428" citStr="Thomson and Young, 2010" startWordPosition="874" endWordPosition="877">for sequence of states rather than making a one-shot decision. Then, the sequential decision making is performed with the objective of maximizing the long term rewards. The sequence of actions is called a strategy, and the major question in sequential decision making is how to find a near optimal strategy. Reinforcement learning (RL): RL in (partially observable) Markov decision processes, so called the (PO)MDPs, is a learning approach in sequential decision making. In particular, (PO)MDPs have been successfully applied in dialogue agents (Roy et al., 2000; Zhang et al., 2001; Williams, 2006; Thomson and Young, 2010; Gaˇsi´c, 2011). The (PO)MDP framework is a formal framework to represent uncertainty explicitly while supporting automated strategy solving. Specifically, it is an optimization framework that supports automated strategy solving by maximizing a “reward function”. 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of applying (PO)MDPs to SDS (Williams, 2006; Paek and Pieraccini, 2008). Specifically, estimating the user model and the reward function is a significant challenge since these model components have a direct impact on the optimized dialogu</context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems. Computer Speech and Language, 24(4):562–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialogue Management.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Engineering, University of Cambridge.</institution>
<contexts>
<context position="5403" citStr="Williams, 2006" startWordPosition="872" endWordPosition="873">o make decision for sequence of states rather than making a one-shot decision. Then, the sequential decision making is performed with the objective of maximizing the long term rewards. The sequence of actions is called a strategy, and the major question in sequential decision making is how to find a near optimal strategy. Reinforcement learning (RL): RL in (partially observable) Markov decision processes, so called the (PO)MDPs, is a learning approach in sequential decision making. In particular, (PO)MDPs have been successfully applied in dialogue agents (Roy et al., 2000; Zhang et al., 2001; Williams, 2006; Thomson and Young, 2010; Gaˇsi´c, 2011). The (PO)MDP framework is a formal framework to represent uncertainty explicitly while supporting automated strategy solving. Specifically, it is an optimization framework that supports automated strategy solving by maximizing a “reward function”. 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of applying (PO)MDPs to SDS (Williams, 2006; Paek and Pieraccini, 2008). Specifically, estimating the user model and the reward function is a significant challenge since these model components have a direct impact</context>
</contexts>
<marker>Williams, 2006</marker>
<rawString>Jason D. Williams. 2006. Partially Observable Markov Decision Processes for Spoken Dialogue Management. Ph.D. thesis, Department of Engineering, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Zhang</author>
<author>Qingsheng Cai</author>
<author>Jianfeng Mao</author>
<author>Baining Guo</author>
</authors>
<title>Planning and acting under uncertainty: A new model for spoken dialogue system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (UAI’01),</booktitle>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5387" citStr="Zhang et al., 2001" startWordPosition="868" endWordPosition="871"> agent is required to make decision for sequence of states rather than making a one-shot decision. Then, the sequential decision making is performed with the objective of maximizing the long term rewards. The sequence of actions is called a strategy, and the major question in sequential decision making is how to find a near optimal strategy. Reinforcement learning (RL): RL in (partially observable) Markov decision processes, so called the (PO)MDPs, is a learning approach in sequential decision making. In particular, (PO)MDPs have been successfully applied in dialogue agents (Roy et al., 2000; Zhang et al., 2001; Williams, 2006; Thomson and Young, 2010; Gaˇsi´c, 2011). The (PO)MDP framework is a formal framework to represent uncertainty explicitly while supporting automated strategy solving. Specifically, it is an optimization framework that supports automated strategy solving by maximizing a “reward function”. 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of applying (PO)MDPs to SDS (Williams, 2006; Paek and Pieraccini, 2008). Specifically, estimating the user model and the reward function is a significant challenge since these model components have</context>
</contexts>
<marker>Zhang, Cai, Mao, Guo, 2001</marker>
<rawString>Bo Zhang, Qingsheng Cai, Jianfeng Mao, and Baining Guo. 2001. Planning and acting under uncertainty: A new model for spoken dialogue system. In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (UAI’01), Seattle, Washington, USA, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001727">
<title confidence="0.7516115">
MITEXTEXPLORER: Linked brushing and mutual information for
exploratory text data analysis
</title>
<figureCaption confidence="0.98457">
Figure 1: Screenshot of MITEXTEXPLORER, analyzing geolocated tweets.
</figureCaption>
<author confidence="0.750529">
Brendan O’Connor
</author>
<affiliation confidence="0.9532175">
Machine Learning Department
Carnegie Mellon University
</affiliation>
<email confidence="0.7197625">
brenocon@cs.cmu.edu
http://brenocon.com
</email>
<bodyText confidence="0.997024823529412">
Abstract I illustrate examples of using the tool on
In this paper I describe a preliminary ex- several datasets: geo-located Twitter mes-
perimental system, MITEXTEXPLORER, sages, presidential State of the Union ad-
for textual linked brushing, which allows dresses, the ACL Anthology, and the King
an analyst to interactively explore statis- James Bible.
tical relationships between (1) terms, and 1 Introduction: Can we “just look” at
(2) document metadata (covariates). An statistical text data?
analyst can graphically select documents Exploratory data analysis (EDA) is an approach
embedded in a temporal, spatial, or other to extract meaning from data, which emphasizes
continuous space, and the tool reports learning about a dataset through an iterative pro-
terms with strong statistical associations cess of many analyses which suggest and refine
for the region. The user can then drill possible hypotheses. It is vital in early stages of a
down to specific term and term groupings, data analysis for data cleaning and sanity checks,
viewing further associations, and see how which are crucial to help ensure a dataset will be
terms are used in context. The goal is to useful. Exploratory techniques can also suggest
rapidly compare language usage across in- possible hypotheses or issues for further investi-
teresting document covariates. gation.
</bodyText>
<page confidence="0.822577">
1
</page>
<note confidence="0.8810905">
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 1–13,
Baltimore, Maryland, USA, June 27, 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999217">
Figure 2: Anscombe Quartet. (Source: Wikipedia)
</figureCaption>
<bodyText confidence="0.992744692307693">
The classical approach to EDA, as pioneered in
works such as Tukey (1977) and Cleveland (1993)
(and other work from the Bell Labs statistics group
during that period) emphasizes visual analysis un-
der nonparametric, model-free assumptions, in
������������������������������������������������� 1
which visual attributes are a fairly direct reflec-
tion of numerical or categorical aspects of data.
As a simple example, consider the well-known
Anscombe Quartet (1973), a set of four bivari-
ate example datasets. The Pearson correlation, a
very widely used measure of dependence that as-
sumes a linear Gaussian model of the data, finds
that each dataset has an identical amount of de-
pendence (r = 0.82). However, a scatterplot in-
stantly reveals that very different dependence re-
lationships hold in each dataset (Figure 2). The
scatterplot is possibly the simplest visual analysis
tool for investigating the relationship between two
variables, in which the variables’ numerical values
are mapped to horizontal and vertical space. While
the correlation coefficient is a model-based analy-
sis tool, the scatterplot is model-free (or at least, it
is effective under an arguably wider range of data
generating assumptions), which is crucial for this
example.
This nonparametric, visual approach to EDA
has been encoded into many data analysis pack-
ages, including the now-ubiquitous R language (R
Core Team, 2013), which descends from earlier
software by the Bell Labs statistics group (Becker
and Chambers, 1984). In R, tools such as his-
tograms, boxplots, barplots, dotplots, mosaicplots,
etc. are built-in, basic operators in the language.
(Wilkinson (2006)’s grammar of graphics more
extensively systematizes this approach; see also
(Wickham, 2010; Bostock et al., 2011).)
In the meantime, textual data has emerged as
a resource of increasing interest for many scien-
</bodyText>
<figureCaption confidence="0.987832">
Figure 3: Linked brushing with the anal-
</figureCaption>
<bodyText confidence="0.999265">
ysis software GGobi. More references at
source:http://www.infovis-wiki.net/index.
php?title=Linking_and_Brushing
tific, business, and government data analysis ap-
plications. Consider the use case of automated
content analysis (a.k.a. text mining) as a tool for
investigating social scientific and humanistic ques-
tions (Grimmer and Stewart, 2013; Jockers, 2013;
Shaw, 2012; O’Connor et al., 2011). The content
of the data is under question: analysts are inter-
ested in what/when/how/by-whom different con-
cepts, ideas, or attitudes are expressed in a cor-
pus, and the trends in these factors across time,
space, author communities, or other document-
level covariates (often called metadata). Compar-
isons of word statistics across covariates are ab-
solutely essential to many interesting questions or
social measurement problems, such as
</bodyText>
<listItem confidence="0.9980316">
• What topics tend to get censored by the Chi-
nese government online, and why (Bamman
et al., 2012; King et al., 2013)? Covari-
ates: whether a message is deleted by cen-
sors, time/location of message.
• What drives media bias? Do newspapers
slant their coverage in response to what read-
ers want (Gentzkow and Shapiro, 2010)? Co-
variates: political preferences of readers,
competitiveness of media markets.
</listItem>
<bodyText confidence="0.9980608">
There exist dozens, if not more, of other examples
in social scientific and humanities research; see
references in O’Connor et al. (2011); O’Connor
(2014).
In this work, I focus on the question: What
</bodyText>
<page confidence="0.989327">
2
</page>
<bodyText confidence="0.999990636363636">
should be the baseline exploratory tools for textual
data, to discover important statistical associations
between text and document covariates? Ideally,
we’d like to “just look” at the data, in the spirit of
scatterplotting the Anscombe Quartet. An analy-
sis tool to support this should not require any sta-
tistical model assumptions, and should display the
data in as direct a form as possible.
For low-dimensional, non-textual data, the base
functionality of R prescribes a broad array of use-
ful defaults: one-dimensional continuous data can
be histogrammed (hist(x)), or kernel density plot-
ted (plot(density(x))), while the relationship be-
tween two dimensions of continuous variables can
be viewed as a scatterplot (plot(x,y)); or perhaps
a boxplot for discrete x and continous y (box-
plot(x,y)); and so on. Commercial data analysis
systems such as Excel, Stata, Tableau, JMP, etc.,
have similar functionality.
These visual tools can be useful for analyz-
ing derived content statistics from text—for exam-
ple, showing a high-level topic or sentiment fre-
quency trending over time—but they cannot visu-
alize the text itself. Text data consists of a linear
sequence of high-dimensional discrete variables
(words). The most aggressive and common anal-
ysis approach, bag-of-words, eliminates the prob-
lematic sequential structure, by reducing a docu-
ment to a high-dimensional discrete counts over
words. But still, none of the above visual tools
makes sense for visualizing a word distribution;
many popular tools simply crash or become very
slow when given word count data. And besides
the issues of discrete high-dimensionality, text is
unique in that it has to be manually read in order
to more reliably understand its meaning. Natural
language processing tools can sometimes extract
partial views of text meaning, but full understand-
ing is a long ways off; and the quality of available
NLP tools varies greatly across corpora and lan-
guages. A useful exploratory tool should be able
to work with a variety of levels of sophistication
in NLP tooling, and allow the user to fall back to
manual reading when necessary.
</bodyText>
<sectionHeader confidence="0.809402" genericHeader="method">
2 MITEXTEXPLORER: linked brushing
</sectionHeader>
<subsectionHeader confidence="0.635543">
for text and covariate correlations
</subsectionHeader>
<bodyText confidence="0.984645304347826">
The analysis tool presented here, MITEXTEX-
PLORER, is designed for exploratory analysis of
relationships between document covariates—such
as time, space, or author community—against tex-
tual variables—words, or other units of meaning,
that can be counted per document. Unlike topic
model approaches to analyzing covariate-text re-
lationships (Mimno, 2012; Roberts et al., 2013),
there is no dimension reduction of the terms. In-
stead, interactivity allows a user to explore more of
the high-dimensional space, by specifying a doc-
ument selection (Q) and/or a term selection (T).
We are inspired by the linking and brushing family
of techniques in interactive data visualization, in
which an analyst can select a group of data points
under a query in one covariate space, and see the
same data selection in a different covariate space
(Figure 3; see Buja et al. (1996), and e.g. Becker
and Cleveland (1987); Buja et al. (1991); Martin
and Ward (1995); Cook and Swayne (2007)). In
our case, one of the variables is text.
The interface consists of several linked views,
which contain:
</bodyText>
<listItem confidence="0.919922363636364">
(A) a view of the documents in a two-dimensional
covariate space (e.g. scatterplot),
(B) an optional list of pinned terms,
(C) document-associated terms: a view of the rel-
atively most frequent terms for the current
document selection,
(D) term-associated terms: a view of terms that
relatively frequently co-occur with the current
term selection; and
(E) a keyword-in-context (KWIC) display of tex-
tual passages for the current term selection.
</listItem>
<bodyText confidence="0.999332777777778">
Figure 1 shows the interface viewing a corpus of
201,647 geo-located Twitter messages from 2,000
users during 2009-2012, which have been tagged
with their author’s spatial coordinates through a
mobile phone client and posted publicly; for data
analysis, their texts have been lowercased and
tokenized appropriately (Owoputi et al., 2013;
O’Connor et al., 2010). Since this type of corpus
contains casual, everyday language, it is a dataset
that may illuminate geographic patterns of slang
and lexical variation in local dialects (Eisenstein
et al., 2012, 2010).
The document covariate display (A) uses (longi-
tude, latitude) positions as the 2D space. The cor-
pus has been preprocessed to define a document as
the concatenation of messages from a single au-
thor, with its position the average location of the
author’s messages. When the interface loads, all
</bodyText>
<page confidence="0.997111">
3
</page>
<bodyText confidence="0.938662">
points in (A) are initially gray, and all other panels
are blank.
</bodyText>
<subsectionHeader confidence="0.981656">
2.1 Covariate-driven queries
</subsectionHeader>
<bodyText confidence="0.999901636363636">
A core interaction, brushing, consists of using the
mouse to select a rectangle in the (x,y) covariate
space. Figure 1 shows a selection around the Bay
Area metropolitan area (blue rectangle). Upon
selection, the document-driven term display (C)
is updated to show the relatively most frequent
terms in the document selection. Let Q denote
the set of documents that are selected by the cur-
rent covariate query. The tool ranks terms w by
their (exponentiated) pointwise mutual informa-
tion, a.k.a. lift, for Q:
</bodyText>
<equation confidence="0.9996635">
lift(w; Q) _ � p(w|Q) _ p(w, Q) (1)
p(w) p(w)p(Q)
</equation>
<bodyText confidence="0.9915812">
This quantity measures how much more frequent
the term is in the queryset, compared to the base-
line global probability in the corpus (p(w)). Prob-
abilities are calculated with simple MLE relative
frequencies, i.e.
</bodyText>
<equation confidence="0.991725">
p(w|Q) E N (2)
p(w) d∈Q ndw nw
_
E
d∈Q nd
</equation>
<bodyText confidence="0.99998880952381">
where d denotes a document ID, ndw the count
of word w in document d, and N the number
of tokens in the corpus. PMI gives results that
are much more interesting than results from rank-
ing w on raw probability within the query set
(p(w|Q)), since that simply shows grammatical
function words or other terms that are common
both in the queryset and across the corpus, and not
distinctive for the queryset.1
A well-known weakness of PMI is over-
emphasis on rare terms; terms that appear
only in the queryset, even if they appear only
once, will attain the highest PMI value. One
way to address this is through a smoothing
prior/pseudocounts/regularization, or through sta-
tistical significance ranking (see §3). For simplic-
ity, we use a minimum frequency threshold filter.
The user interface allows minimums for either lo-
cal or global term frequencies, and to easily ad-
just them, which naturally shifts the emphasis be-
tween specific and generic language. All methods
</bodyText>
<footnote confidence="0.853363333333333">
1The term “lift” is used in business applications (Provost
and Fawcett, 2013), while PMI has been used in many NLP
applications to measure word associations.
</footnote>
<bodyText confidence="0.999956657142857">
to protect against rare probabilistic events neces-
sarily involve such a tradeoff parameter that the
user ought to experiment with; given this situation,
we might prefer a transparent mechanism instead
of mathematical priors (though see also §3).
Figure 1 shows that hella is the highest ranked
term for this spatial selection (and freqency thresh-
old), occurring 7.8 times more frequently com-
pared to the overall corpus; this comports with
surveyed intuitions of Californian English speak-
ers (Bucholtz et al., 2007). For full transparency
to the user, the local and global term counts are
shown in the table. (Since hella occurred 18 times
in the queryset and 90 times globally, this im-
plies the simple conditional probability p(Q|w) _
18/90; and indeed, ranking on p(Q|w) is equiva-
lent to ranking on PMI, since exponentiated PMI
is p(Q|w)/p(Q).) The user can also sort by local
count to see the raw most-frequent term report for
the document selection. As the user reshapes the
query box, or drags it around the space, the terms
in panel (C) are updated.
Not shown are options to change the term fre-
quency representation. For exposition here, proba-
bilities are formulated as counts of tokens, but this
can be problematic for social media data, since a
single user might use a term a very large number
of times. The above analysis is conducted with
an indicator representation of terms per user, so
all frequencies refer to the probability that a user
uses the term at least once. However, the other ex-
amples in this paper use token-level frequencies,
which seem to work fine. It is an interesting statis-
tical analysis question how to derive a single range
of methods to work across these situations.
</bodyText>
<subsectionHeader confidence="0.999797">
2.2 Term selection and KWIC views
</subsectionHeader>
<bodyText confidence="0.996829307692308">
Terms in the table (C) can be clicked and selected,
forming a term selection as a set of terms T. This
action drives several additional views:
(A) documents containing the term are high-
lighted in the document covariate display
(here, in red),
(E) examples of the term’s usage, in Keyword-in-
Context style with vertical alignment for the
query term; and
(D) other terms that frequently co-occur with T
(§2.3).
The KWIC report in (E) shows examples of term’s
usage. For example, why is the term “la” in
</bodyText>
<page confidence="0.992915">
4
</page>
<figureCaption confidence="0.9664555">
Figure 4: KWIC examples of “la” usage in tweets
selected in Figure 1.
</figureCaption>
<bodyText confidence="0.999911476190476">
the PMI list? My initial thought was that this
was an example of “LA”, short for “Los Ange-
les”. But clicking on “la” instantly disproves this
hypothesis—Figure 4, showing the Los Angeles
sense, but also the “la la la” sense, as well as the
Spanish function word.
The KWIC alignment makes it easier to rapidly
browse examples, and think about a rough as-
sessment of their word sense or how they are
used. Figure 5 compares how the term “God”
is used by U.S. presidents Ronald Reagan and
Barack Obama, in a corpus of State of the Union
speeches, from two different displays of the tool.
The predominant usage is the invocation of “God
bless America” or similar, nearly ornamental, ex-
pressions, but Reagan also has substantive us-
ages, such as references to the role of religion
in schools. The vertical alignments of the right-
side context words makes it easy to see the “God
bless” word sense. I initially found this exam-
ple simply by browsing the covariate space, and
noticing “god” as a frequent term for Reagan,
though still occurring for other presidents; the
KWIC drilldown better illuminated these distinc-
tions, and suggests differences in political ideolo-
gies between the presidents.
In lots of exploratory text analysis work, espe-
cially in the topic modeling literature, it is com-
mon to look at word lists produced by a statistical
analysis method and think about what they might
mean. At least in my experience doing this, I’ve
often found that seeing examples of words in con-
text has disproved my initial intuitions. Hopefully,
supporting this activity in an interactive user inter-
face might make exploratory analysis more effec-
tive. Currently, the interface simply shows a sam-
ple of in-context usages from the document query-
set; it would be interesting to perform grouping
and stratified sampling based on local contextual
statistics. Summarizing local context by frequen-
cies could be done as a trie visualization (Watten-
berg and Vi´egas, 2008); see §5.
</bodyText>
<subsectionHeader confidence="0.998147">
2.3 Term-association queries
</subsectionHeader>
<bodyText confidence="0.999988733333333">
When a term is selected, its interaction with co-
variates is shown by highlighting documents in (B)
that contain the term. This can be thought of as
another document query: instead of being spec-
ified as a region in the covariate space, is spec-
ified as a fragment of the discrete lexical space.
As illustrated in much previous work (e.g. Church
and Hanks (1990); Turney (2001, 2002)), word-to-
word PMI scores can find other terms with similar
meanings, or having interesting semantic relation-
ships, to the target term.2
This panel ranks terms u by their association
with the query term v. The simplest method is to
analyze the relative frequencies of terms in docu-
ments that contain v,
</bodyText>
<equation confidence="0.983989">
bool-tt-epmi(u, v) = p(wi = u|v E supp(di))
p(wi = u)
</equation>
<bodyText confidence="0.9956339375">
Here, the subscript i denotes a token position in
the entire corpus, for which there is a wordtype
wi and a document ID di. In this notation, the
covariate PMI in 2.1 would be p(wi = u|di E
Q)/p(wi = u). supp(di) denotes the set of terms
that occur at least once in document di.
This measure is a very simple extension of
the document covariate selection mechanism, and
easy to understand. However, it is less satisfy-
ing for longer documents, since a larger number
of occurrences of v do not lead to a stronger asso-
ciation score. A possible extension is to consider
the joint random event of selecting two tokens i
and j in the corpus, and consider if the two to-
kens being in the same document is informative
for whether the tokens are the words (u, v), i.e.
</bodyText>
<footnote confidence="0.9850325">
2For finding terms with similar semantic meaning, dis-
tributional similarity may be more appropriate (Turney and
Pantel, 2010); this could be interesting to incorporate into the
software.
</footnote>
<page confidence="0.98193">
5
</page>
<figureCaption confidence="0.997646">
Figure 5: KWIC examples of “God” in speeches by Reagan versus Obama.
</figureCaption>
<bodyText confidence="0.980393104166667">
PMI[(wi, wj) = (u, v); di = dj], 2.4 Pinned terms
freq-tt-epmi(u, v) = p(wi = u, wj = v|di = dj)
In terms of word counts, this expression has the
form
freq-tt-epmi(u, v) = Ed ndundv
nunv
The right-side term is a normalizing constant in-
variant to u and v. The left-side term is interesting:
it can be viewed as a similarity measure, where
the numerator is the inner product of the inverted
term-document vectors n.,u and n.,v, and the de-
nominator is the product of their `1 norms. This
is a very similar form as cosine similarity, which
is another normalized inner product, except its de-
nominator is the product of the vectors’ `2 norms.
Term-to-term associations allow a navigation of
the term space, complementing the views of terms
driven by document covariates. This part of the
tool is still at a more preliminary stage of develop-
ment. One important enhancement would be ad-
justment of the context window size allowed for
co-occurrences; the formulations above assume a
context window the size of the document. Medium
sized context windows might capture more fo-
cused topical content, especially in very long dis-
courses such as speeches; and the smallest context
windows, of size 1, should be more like colloca-
tion detection (though see §3; this is arguably bet-
ter done with significance tests, not PMI).
The term PMI views of (C) and (D) are very dy-
namic, which can cause interesting terms to disap-
pear when their supporting query is changed. It is
often useful to select terms to be constantly viewed
when the document covariate queries change.
Any term can be double-clicked to be moved to
the the table of pinned terms (B). The set of terms
here does not change as the covariate query is
changed; a user can fix a set of terms and see how
their PMI scores change while looking at differ-
ent parts of the covariate space. One possible use
of term pinning is to manually build up clusters of
terms—for example, topical or synonymous term
sets—whose aggregate statistical behavior (i.e. as
a disjunctive query) may be interesting to observe.
Manually built sets of keywords are a very useful
form of text analysis; in fact, the WordSeer cor-
pus analysis tool has explicit support to help users
create them (Shrikumar, 2013).
</bodyText>
<sectionHeader confidence="0.978734" genericHeader="method">
3 Statistical term association measures
</sectionHeader>
<bodyText confidence="0.999830583333333">
There exist many measures to measure the sta-
tistical strength of an association between a term
and a document covariate, or between two terms.
A number of methods are based on significance
testing, looking for violations of a null hypothesis
that term frequencies are independent. For collo-
cation detection, which aims to find meaningful
non-compositional lexical items through frequen-
cies of neighboring words, likelihood ratio (Dun-
ning, 1993) and chi-square tests have been used
(see review in Manning and Sch¨utze (1999)). For
term-covariate associations, chi-square tests were
</bodyText>
<equation confidence="0.994082">
p(wi = u, wj = v)
N2
Ed n2d
</equation>
<page confidence="0.977767">
6
</page>
<bodyText confidence="0.998502404761905">
used by Gentzkow and Shapiro (2010) to find po-
litically loaded phrases often used by members of
one political party; this same method is often used
as a feature selection method for supervised learn-
ing (Guyon and Elisseeff, 2003).
The approach we take here is somewhat differ-
ent, being a point estimate approach, analyzing
the estimated difference (and giving poor results
when counts are small). Some related work for
topic model analysis, looking at statistical associa-
tions between words and latent topics (as opposed
to between words and observed covariates in this
work) includes Chuang et al. (2012b), whose term
saliency function measures one word’s associa-
tions against all topics; a salient term tends to have
most of its probability mass in a small set of top-
ics. The measure is a form of mutual information,3
and may be useful for our purposes here if the user
wishes to see a report of distinctive terms for a
group of several different observed covariate val-
ues at once. Blei and Lafferty (2009) ranks words
per topic by a measure inspired by TFIDF, which
like PMI downweights words that are generically
common across all topics.
Finally, hierarchical priors and regularizers can
also be used; for example, by penalizing the
log-odds parameterization of term probabilities
(Eisenstein et al., 2011; Taddy, 2013). These
methods are better in that they incorporate both
protection against small count situations, while
paying attention to effect size, as well as allow-
ing overlapping covariates and regression control
variables; but unfortunately, they are more compu-
tationally intensive, as opposed to the above mea-
sures which all work directly from sufficient count
statistics. An association measure that fulfilled all
these desiderata would be very useful. For term-
covariate analysis, Monroe et al. (2008) contains a
review of many different methods, from both po-
litical science as well as computer science; they
also propose a hierarchical prior method, and to
rank by statistical significance via the asymptotic
</bodyText>
<footnote confidence="0.8293605">
3This is apparent as follows, using notation from their sec-
tion 3.1:
</footnote>
<equation confidence="0.98013125">
�
saliency(w) = p(w)
T
p(w, T) log[p(w, T)/[p(w)p(T)]]
</equation>
<bodyText confidence="0.969055571428571">
This might be called a “half-pointwise” mutual information:
between a specific word w and the topic random variable T.
Mutual information is Ew saliency(w).
standard error of the terms’ odds ratios.
Given the large amount of previous work using
the significance approach, it merits further explo-
ration for this system.
</bodyText>
<sectionHeader confidence="0.984785" genericHeader="method">
4 Phrase selection
</sectionHeader>
<bodyText confidence="0.999986930232558">
The simplest approach to defining the terms is to
use all words (unigrams). This can be insightful,
but single words are both too coarse and too nar-
row a unit of analysis. They can be too narrow
when there are multiple ways of saying the same
thing, such as synonyms—for example, while we
have evidence about differing usages of the term
“god” in presidential rhetoric, in order to make a
claim about religious themes, we might need to
find other terms such as “creator”, “higher power”,
etc. Another problematic case is alternate names
or anaphoric references to an entity. In general,
any NLP tool that extracts interesting discrete vari-
able indicators of word meaning could be used
for mutual information and covariate exploratory
analysis—for example, a coreference system’s en-
tity ID predictions could be browsed by the system
as the term variables. (More complex concepts, of
course, would also require more UI support.)
At the same time, words can be too coarse com-
pared to the longer phrases they are contained
within, which often contain more interesting and
distinctive concepts: for example, “death tax”
and “social security” are important concepts in
U.S. politics that get missed under a unigram anal-
ysis. In fact, Sim et al. (2013)’s analysis of U.S.
politicians’ speeches found that domain experts
had a hard time understanding unigrams out-of-
context, but bigrams and trigrams worked much
better; Gentzkow and Shapiro (2010) similarly fo-
cus on partisan political phrases.
It sometimes works to simply add overlap-
ping n-grams as more terms, but sometimes odd
phrases get selected that cross constituent bound-
aries from their source sentences, and are thus not
totally meaningful. I’ve experimented with a very
strong filtering approach to phrase selection: be-
sides using all unigrams, take all n-grams up to
length 5 that have nominal part-of-speech patterns:
either the sequence consists of zero or more ad-
jectives followed by one or more noun tokens, or
all tokens were classified as names by a named
entity recognition system.4 This tends to yield
</bodyText>
<footnote confidence="0.8992225">
4For traditional text, the tool currently uses Stanford
CoreNLP; for Twitter, CMU ARK TweetNLP.
</footnote>
<equation confidence="0.929925">
p(T |w) log[p(T|w)/p(T)]
�=
T
</equation>
<page confidence="0.993401">
7
</page>
<figureCaption confidence="0.9825804">
Figure 6: MITEXTEXPLORER for paper titles in the ACL Anthology (Radev et al., 2009). Y-axis is venue
(conference or journal name), X-axis is year of publication. Unlike the other figures, docvar-associated
terms are sorted alphabetically.
Figure 7: MITEXTEXPLORER for the King James Bible. Y-axis is book, X-axis is chapter (truncated to
39).
</figureCaption>
<page confidence="0.994411">
8
</page>
<bodyText confidence="0.999929424242424">
(partial) constituents, and nouns tend to be more
interesting than other content words (perhaps be-
cause they are relatively less reliant on predicate-
argument structure to express their semantics—as
opposed to adjectives or verbs, say—and a bag-of-
terms analysis does not allow expression of argu-
ment structure.) However, for many corpora, POS
or NER taggers work poorly—for example, I’ve
seen paper titles from the ACL Anthology have
capitalized prepositions tagged as names—so sim-
pler stopword heuristics are necessary.
The phrase selection approach could be im-
proved in many ways; for example, a real noun
phrase recognizer could get important (NP PP)
constructs like “war on terror.” Furthermore,
Chuang et al. (2012a) find that while these sorts
of syntactic features are helpful in choosing useful
keyphrases to summarize of scientific abstracts,
it is also very useful to add in collocation de-
tection scores. Similarly to the PMI calculations
used here, likelihood ratio or chi-square collo-
cation detection statistics are also very rapid to
compute and may benefit from interactive adjust-
ment of decision thresholds. More generally, any
type of lexicalized linguistic structures could po-
tentially be used, such as dependency paths or
constituents from a syntactic parser, or predicate-
argument structures from a semantic parser. Lin-
guistic structures extracted from more sophisti-
cated NLP tools may indeed be better-generalized
units of linguistic meaning compared to words and
phrases, but they will still bear the same high-
dimensionality issues for data analysis purposes.
</bodyText>
<sectionHeader confidence="0.951149" genericHeader="method">
5 Related work: Exploratory text
analysis
</sectionHeader>
<bodyText confidence="0.999940214285714">
Many systems and techniques have been devel-
oped for interactive text analysis. Two such sys-
tems, WordSeer and Jigsaw, have been under de-
velopment for several years, each having had a se-
ries of user experiments and feedback. Recent and
interesting review papers and theses are available
for both of them.
The WordSeer system (Shrikumar, 2013)5 con-
tains many different interactive text visualization
tools, including syntax-based search, and was ini-
tially designed for the needs of text analysis in
the humanities; the WordSeer 3.0 system includes
a word frequency analysis component that can
compare word frequencies along document covari-
</bodyText>
<footnote confidence="0.880505">
5http://wordseer.berkeley.edu/
</footnote>
<bodyText confidence="0.9996595625">
ates. Interestingly, Shrikumar found in user stud-
ies with literary experts that data comparisons and
annotation/note-taking support were very impor-
tant capabilities to add to the system. Unique to
the work in this paper is the emphasis on condi-
tioning on document covariates to analyze rela-
tive word frequencies, and encouraging the user to
change the statistical parameters that govern text
correlation measurements. (The term pinning and
term-to-term association techniques are certainly
less developed than previous work.)
Another text analysis system is Jigsaw (G¨org
et al., 2013),6 originally developed for investiga-
tive analysis (as in law enforcement or intelli-
gence), which again has many features. It empha-
sizes visualizations based on entity extractions,
such as for names, places, and dates. G¨org et al.
note that errors in entity extraction were a major
problem for users; this might be a worthwhile ar-
gument to focus on getting something to first work
with simple words/phrases before tackling more
complex units of meaning. A section of the review
paper is entitled “Reading the documents still mat-
ters”, pointing out that analysts did not want just to
visualize high-level relationships, but also wanted
to read documents in context; this capability was
added to later versions of Jigsaw, and supports the
emphasis here on the KWIC display.
Both these systems also use variants of Watten-
berg and Vi´egas (2008)’s word tree visualization,
which gives a sequential word frequencies as a
tree (i.e., what computational linguists might call a
trie representation of a high-order Markov model).
The “God bless” word sense example from §2 in-
dicates that such statistical summarization of local
contextual information may be useful to integrate;
it is worth thinking how to integrate this against
the important need of document covariate analy-
sis, while being efficient with the use of space.
Many other systems, especially ones designed
for literary content analysis, emphasize concor-
dances and keyword searches within a text; for
example, Voyeur/Voyant (Rockwell et al., 2010),7
which also features some document covariate
analysis through temporal trend analyses for indi-
vidual terms. Another class of approaches empha-
sizes the use of document clustering or topic mod-
els (Gardner et al., 2010; Newman et al., 2010;
</bodyText>
<footnote confidence="0.99976875">
6http://www.cc.gatech.edu/gvu/ii/
jigsaw/
7http://voyant-tools.org/,
http://hermeneuti.ca/voyeur
</footnote>
<page confidence="0.996443">
9
</page>
<bodyText confidence="0.999379214285714">
Grimmer and King, 2011; Chaney and Blei, 2013),
while Overview8 emphasizes hierarchical docu-
ment clustering paired with manual tagging.
Finally, considerable research has examined
exploratory visual interfaces for information re-
trieval, in which a user specifies an information
need in order to find relevant documents or pas-
sages from a corpus (Hearst (2009), Ch. 10). In-
formation retrieval problems have some similari-
ties to text-as-data analysis in the need for an ex-
ploratory process of iterative refinement, but the
text-as-data perspective differs in that it requires
an analyst to understand content and contextual
factors across multiple or many documents.
</bodyText>
<sectionHeader confidence="0.999651" genericHeader="method">
6 Future work
</sectionHeader>
<bodyText confidence="0.999969666666667">
The current MITEXTEXPLORER system is an ex-
tremely simple prototype to explore what sorts of
“bare words” text-and-covariates analyses are pos-
sible. Several major changes will be necessary for
more serious use.
First, essential basic capabilities must be added,
such as a search box the user can use to search and
filter the term list.
Second, the document covariate display needs
to support more than just scatterplots. When there
are hundreds or more documents, summarization
is necessary in the form of histograms, kernel den-
sity plots, or other tools. For example, for a large
corpus of documents over time, a lineplot or tem-
poral histogram is more appropriate, where each
timestep has a document count. The ACL An-
thology scatterplot (Figure 6, Radev et al. (2009)),
which has hundreds of overplotted points at each
(year,venue) position, makes clear the limitations
of the current approach.
Better visual feedback for term selections here
could be useful—for example, sizing document
points monotonically with the term’s frequency
(rather than just presence/absence), or using
stacked line plots—though certain visual depic-
tions of frequency may be difficult given the Zip-
fian distribution of word frequencies.
Furthermore, document structures may be
thought of as document covariates. A single book
has interesting internal variation that could be an-
alyzed itself. Figure 7 shows the King James
Bible, which has a hierarchical structure of book,
chapter, and verse. Here, the (y,x) coordinates
</bodyText>
<footnote confidence="0.977315">
8https://www.overviewproject.org/ http:
//overview.ap.org/
</footnote>
<bodyText confidence="0.9991997">
represent books and chapters. A more special-
ized display for book-level structures, or other dis-
course structures, may be appropriate for book-
length texts.
Finally, a major goal of this work is to use anal-
ysis methods that can be computed on the fly,
but the current prototype only works with small
datasets. Hierarchical spatial indexing techniques
(e.g. r-trees), may make it possible to interactively
compute sums for covariate PMI scoring over very
large numbers of documents. Text indexing is
also important for term-driven queries and KWIC
views. Techniques from ad-hoc data querying sys-
tems may be necessary for further scale (e.g. Mel-
nik et al. (2010)).
Many other directions are possible. The proto-
type tool, as described in §2, will be available as
open-source software at: http://brenocon.
com/MiTextExplorer. It is a desktop appli-
cation written in Java.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9932855">
Thanks to Michael Heilman and Bryan Routledge,
for many discussions and creative text analysis
scripts that inspired this work. Thanks also to the
anonymous reviewers for very helpful feedback.
This research was supported in part by NSF grant
IIS-1211277 and CAREER grant IIS-1054319.
</bodyText>
<sectionHeader confidence="0.999017" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9978747">
Francis J Anscombe. Graphs in statistical analysis.
The American Statistician, 27(1):17–21, 1973.
David Bamman, Brendan O’Connor, and Noah A.
Smith. Censorship and deletion practices in
Chinese social media. First Monday, 17(3),
2012.
Richard A Becker and John M Chambers. S: an
interactive environment for data analysis and
graphics. CRC Press, 1984.
Richard A. Becker and William S. Cleveland.
Brushing scatterplots. Technometrics, 29(2):
127–142, 1987.
David M. Blei and John D. Lafferty. Topic mod-
els. Text mining: classification, clustering, and
applications, 10:71, 2009.
Michael Bostock, Vadim Ogievetsky, and Jeffrey
Heer. D3: Data-driven documents. IEEE Trans.
Visualization &amp; Comp. Graphics (Proc. Info-
Vis), 2011. URL http://vis.stanford.
edu/papers/d3.
</reference>
<page confidence="0.979672">
10
</page>
<reference confidence="0.961461134020618">
Mary Bucholtz, Nancy Bermudez, Victor Fung,
Lisa Edwards, and Rosalva Vargas. Hella
Nor Cal or totally So Cal? the per-
ceptual dialectology of California. Jour-
nal of English Linguistics, 35(4):325–352,
2007. URL http://people.duke.edu/
-eec10/hellanorcal.pdf.
Andreas Buja, John Alan McDonald, John Micha-
lak, and Werner Stuetzle. Interactive data visu-
alization using focusing and linking. In Visu-
alization, 1991. Visualization’91, Proceedings.,
IEEE Conference on, pages 156–163. IEEE,
1991.
Andreas Buja, Dianne Cook, and Deborah F
Swayne. Interactive high-dimensional data vi-
sualization. Journal of Computational and
Graphical Statistics, 5(1):78–99, 1996.
Allison J.B. Chaney and David M. Blei. Visual-
izing topic models. In Proceedings of ICWSM,
2013.
Jason Chuang, Christopher D. Manning, and
Jeffrey Heer. ”without the clutter of unim-
portant words”: Descriptive keyphrases for
text visualization. ACM Trans. on Computer-
Human Interaction, 19:1–29, 2012a. URL
http://vis.stanford.edu/papers/
keyphrases.
Jason Chuang, Christopher D. Manning, and Jef-
frey Heer. Termite: Visualization techniques for
assessing textual topic models. In Advanced Vi-
sual Interfaces, 2012b. URL http://vis.
stanford.edu/papers/termite.
K. W Church and P. Hanks. Word association
norms, mutual information, and lexicography.
Computational linguistics, 16(1):2229, 1990.
William S. Cleveland. Visualizing data. Hobart
Press, 1993.
Dianne Cook and Deborah F. Swayne. Interactive
and dynamic graphics for data analysis: with R
and GGobi. Springer, 2007.
Ted Dunning. Accurate methods for the statistics
of surprise and coincidence. Computa-
tional Linguistics, 19:61—74, 1993. doi:
10.1.1.14.5962. URL http://citeseerx.
ist.psu.edu/viewdoc/summary?
doi=10.1.1.14.5962.
J. Eisenstein, A. Ahmed, and E.P. Xing. Sparse ad-
ditive generative models of text. In Proceedings
of ICML, pages 1041–1048, 2011.
Jacob Eisenstein, Brendan O’Connor, Noah A.
Smith, and Eric P. Xing. A latent variable model
for geographic lexical variation. In Proceedings
of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1277—
1287, 2010.
Jacob Eisenstein, Brendan O’Connor, Noah A.
Smith, and Eric P. Xing. Mapping the geograph-
ical diffusion of new words. In NIPS Workshop
on Social Network and Social Media Analy-
sis, 2012. URL http://arxiv.org/abs/
1210.5268.
M.J. Gardner, J. Lutes, J. Lund, J. Hansen,
D. Walker, E. Ringger, and K. Seppi. The topic
browser: An interactive tool for browsing topic
models. In NIPS Workshop on Challenges of
Data Visualization. MIT Press, 2010.
Matthew Gentzkow and Jesse M Shapiro. What
drives media slant? evidence from us daily
newspapers. Econometrica, 78(1):35–71, 2010.
Carsten G¨org, Zhicheng Liu, and John Stasko.
Reflections on the evolution of the jigsaw vi-
sual analytics system. Information Visualiza-
tion, 2013.
Justin Grimmer and Gary King. General purpose
computer-assisted clustering and conceptualiza-
tion. Proceedings of the National Academy of
Sciences, 108(7):2643–2650, 2011.
Justin Grimmer and Brandon M Stewart. Text
as Data: The promise and pitfalls of au-
tomatic content analysis methods for polit-
ical texts. Political Analysis, 21(3):267–
297, 2013. URL http://www.stanford.
edu/-jgrimmer/tad2.pdf.
Isabelle Guyon and Andr´e Elisseeff. An introduc-
tion to variable and feature selection. The Jour-
nal of Machine Learning Research, 3:1157–
1182, 2003.
Marti Hearst. Search user interfaces. Cambridge
University Press, 2009.
Matthew L Jockers. Macroanalysis: Digital meth-
ods and literary history. University of Illinois
Press, 2013.
Gary King, Jennifer Pan, and Margaret E. Roberts.
How censorship in china allows government
criticism but silences collective expression.
American Political Science Review, 107:1–18,
2013.
</reference>
<page confidence="0.989906">
11
</page>
<reference confidence="0.998503185567011">
Christopher D Manning and Hinrich Sch¨utze.
Foundations of statistical natural language pro-
cessing. MIT press, 1999.
Allen R. Martin and Matthew O. Ward. High di-
mensional brushing for interactive exploration
of multivariate data. In Proceedings of the
6th Conference on Visualization’95, page 271.
IEEE Computer Society, 1995.
Sergey Melnik, Andrey Gubarev, Jing Jing Long,
Geoffrey Romer, Shiva Shivakumar, Matt
Tolton, and Theo Vassilakis. Dremel: interac-
tive analysis of web-scale datasets. Proceed-
ings of the VLDB Endowment, 3(1-2):330–339,
2010.
David Mimno. Topic regression. PhD thesis, Uni-
versity of Massachusetts Amherst, 2012.
B. L. Monroe, M. P. Colaresi, and K. M. Quinn.
Fightin’Words: lexical feature selection and
evaluation for identifying the content of politi-
cal conflict. Political Analysis, 16(4):372, 2008.
D. Newman, T. Baldwin, L. Cavedon, E. Huang,
S. Karimi, D. Martinez, F. Scholer, and J. Zo-
bel. Visualizing search results and document
collections using topic maps. Web Semantics:
Science, Services and Agents on the World Wide
Web, 8(2):169–175, 2010.
Brendan O’Connor. Statistical Text Analysis for
Social Science. PhD thesis, Carnegie Mellon
University, 2014.
Brendan O’Connor, Michel Krieger, and David
Ahn. TweetMotif: Exploratory search and topic
summarization for Twitter. In Proceedings of
the International AAAI Conference on Weblogs
and Social Media, 2010.
Brendan O’Connor, David Bamman, and Noah A.
Smith. Computational text analysis for social
science: Model assumptions and complexity. In
Second Workshop on Comptuational Social Sci-
ence and the Wisdom of Crowds (NIPS 2011),
2011.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. Improved part-of-speech tagging for on-
line conversational text with word clusters. In
Proceedings of NAACL, 2013.
Foster Provost and Tom Fawcett. Data Science for
Business. O’Reilly Media, 2013.
R Core Team. R: A Language and Environmentfor
Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria, 2013. URL
http://www.R-project.org/.ISBN3-
900051-07-0.
Dragomir R. Radev, Pradeep Muthukrishnan, and
Vahed Qazvinian. The ACL anthology network
corpus. In Proc. of ACL Workshop on Natu-
ral Language Processing and Information Re-
trieval for Digital Libraries, 2009.
Margaret E. Roberts, Brandon M. Stewart, and
Edoardo M. Airoldi. Structural topic models.
2013. URL http://scholar.harvard.
edu/bstewart/publications/
structural-topic-models. Work-
ing paper.
Geoffrey Rockwell, St´efan G Sinclair, Stan
Ruecker, and Peter Organisciak. Ubiquitous
text analysis. paj: The Journal of the Initiative
for Digital Humanities, Media, and Culture, 2
(1), 2010.
Ryan Shaw. Text-mining as a research
tool, 2012. URL http://aeshin.org/
textmining/.
Aditi Shrikumar. Designing an Exploratory Text
Analysis Tool for Humanities and Social Sci-
ences Research. PhD thesis, University of Cali-
fornia at Berkeley, 2013.
Yanchuan Sim, Brice Acree, Justin H Gross, and
Noah A Smith. Measuring ideological propor-
tions in political speeches. In Proceedings of
EMNLP, 2013.
Matt Taddy. Multinomial inverse regression for
text analysis. Journal of the American Statisti-
cal Association, 108(503):755–770, 2013.
John W. Tukey. Exploratory data analysis. 1977.
P. D Turney. Thumbs up or thumbs down?: seman-
tic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th
Annual Meeting on Association for Computa-
tional Linguistics, page 417424, 2002.
P. D Turney and P. Pantel. From frequency to
meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37
(1):141188, 2010. ISSN 1076-9757.
Peter Turney. Mining the web for syn-
onyms: Pmi-ir versus lsa on toefl. In
Proceedings of the Twelth European Con-
ference on Machine Learning, 2001.
URL http://nparc.cisti-icist.
</reference>
<page confidence="0.970245">
12
</page>
<reference confidence="0.991473583333333">
nrc-cnrc.gc.ca/npsi/ctrl?
action=rtdoc&amp;an=5765594.
Martin Wattenberg and Fernanda B Vi´egas. The
word tree, an interactive visual concordance.
Visualization and Computer Graphics, IEEE
Transactions on, 14(6):1221–1228, 2008.
Hadley Wickham. A layered grammar of graph-
ics. Journal of Computational and Graphical
Statistics, 19(1):328, 2010. doi: 10.1198/jcgs.
2009.07098.
Leland Wilkinson. The grammar of graphics.
Springer, 2006.
</reference>
<page confidence="0.999464">
13
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.284492">
<title confidence="0.768043">Linked brushing and mutual information</title>
<abstract confidence="0.9447245">exploratory text data analysis 1: Screenshot of analyzing geolocated tweets.</abstract>
<author confidence="0.967464">Brendan O’Connor</author>
<affiliation confidence="0.8926655">Machine Learning Department Carnegie Mellon University</affiliation>
<email confidence="0.99941">brenocon@cs.cmu.edu</email>
<web confidence="0.950592">http://brenocon.com</web>
<author confidence="0.542624">Abstract I illustrate examples of using the tool on several datasets geo-located Twitter mes-sages</author>
<author confidence="0.542624">presidential State of the Union ad-dresses</author>
<author confidence="0.542624">the ACL Anthology</author>
<author confidence="0.542624">the King James Bible</author>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis J Anscombe</author>
</authors>
<title>Graphs in statistical analysis.</title>
<date>1973</date>
<journal>The American Statistician,</journal>
<volume>27</volume>
<issue>1</issue>
<marker>Anscombe, 1973</marker>
<rawString>Francis J Anscombe. Graphs in statistical analysis. The American Statistician, 27(1):17–21, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
</authors>
<title>Censorship and deletion practices in Chinese social media.</title>
<date>2012</date>
<journal>First Monday,</journal>
<volume>17</volume>
<issue>3</issue>
<marker>Bamman, O’Connor, Smith, 2012</marker>
<rawString>David Bamman, Brendan O’Connor, and Noah A. Smith. Censorship and deletion practices in Chinese social media. First Monday, 17(3), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Becker</author>
<author>John M Chambers</author>
</authors>
<title>S: an interactive environment for data analysis and graphics.</title>
<date>1984</date>
<publisher>CRC Press,</publisher>
<contexts>
<context position="3364" citStr="Becker and Chambers, 1984" startWordPosition="492" endWordPosition="495">tool for investigating the relationship between two variables, in which the variables’ numerical values are mapped to horizontal and vertical space. While the correlation coefficient is a model-based analysis tool, the scatterplot is model-free (or at least, it is effective under an arguably wider range of data generating assumptions), which is crucial for this example. This nonparametric, visual approach to EDA has been encoded into many data analysis packages, including the now-ubiquitous R language (R Core Team, 2013), which descends from earlier software by the Bell Labs statistics group (Becker and Chambers, 1984). In R, tools such as histograms, boxplots, barplots, dotplots, mosaicplots, etc. are built-in, basic operators in the language. (Wilkinson (2006)’s grammar of graphics more extensively systematizes this approach; see also (Wickham, 2010; Bostock et al., 2011).) In the meantime, textual data has emerged as a resource of increasing interest for many scienFigure 3: Linked brushing with the analysis software GGobi. More references at source:http://www.infovis-wiki.net/index. php?title=Linking_and_Brushing tific, business, and government data analysis applications. Consider the use case of automat</context>
</contexts>
<marker>Becker, Chambers, 1984</marker>
<rawString>Richard A Becker and John M Chambers. S: an interactive environment for data analysis and graphics. CRC Press, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Becker</author>
<author>William S Cleveland</author>
</authors>
<title>Brushing scatterplots.</title>
<date>1987</date>
<journal>Technometrics,</journal>
<volume>29</volume>
<issue>2</issue>
<pages>127--142</pages>
<contexts>
<context position="8272" citStr="Becker and Cleveland (1987)" startWordPosition="1253" endWordPosition="1256"> topic model approaches to analyzing covariate-text relationships (Mimno, 2012; Roberts et al., 2013), there is no dimension reduction of the terms. Instead, interactivity allows a user to explore more of the high-dimensional space, by specifying a document selection (Q) and/or a term selection (T). We are inspired by the linking and brushing family of techniques in interactive data visualization, in which an analyst can select a group of data points under a query in one covariate space, and see the same data selection in a different covariate space (Figure 3; see Buja et al. (1996), and e.g. Becker and Cleveland (1987); Buja et al. (1991); Martin and Ward (1995); Cook and Swayne (2007)). In our case, one of the variables is text. The interface consists of several linked views, which contain: (A) a view of the documents in a two-dimensional covariate space (e.g. scatterplot), (B) an optional list of pinned terms, (C) document-associated terms: a view of the relatively most frequent terms for the current document selection, (D) term-associated terms: a view of terms that relatively frequently co-occur with the current term selection; and (E) a keyword-in-context (KWIC) display of textual passages for the curr</context>
</contexts>
<marker>Becker, Cleveland, 1987</marker>
<rawString>Richard A. Becker and William S. Cleveland. Brushing scatterplots. Technometrics, 29(2): 127–142, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>John D Lafferty</author>
</authors>
<title>Topic models. Text mining: classification, clustering, and applications,</title>
<date>2009</date>
<pages>10--71</pages>
<contexts>
<context position="21712" citStr="Blei and Lafferty (2009)" startWordPosition="3527" endWordPosition="3530"> are small). Some related work for topic model analysis, looking at statistical associations between words and latent topics (as opposed to between words and observed covariates in this work) includes Chuang et al. (2012b), whose term saliency function measures one word’s associations against all topics; a salient term tends to have most of its probability mass in a small set of topics. The measure is a form of mutual information,3 and may be useful for our purposes here if the user wishes to see a report of distinctive terms for a group of several different observed covariate values at once. Blei and Lafferty (2009) ranks words per topic by a measure inspired by TFIDF, which like PMI downweights words that are generically common across all topics. Finally, hierarchical priors and regularizers can also be used; for example, by penalizing the log-odds parameterization of term probabilities (Eisenstein et al., 2011; Taddy, 2013). These methods are better in that they incorporate both protection against small count situations, while paying attention to effect size, as well as allowing overlapping covariates and regression control variables; but unfortunately, they are more computationally intensive, as oppos</context>
</contexts>
<marker>Blei, Lafferty, 2009</marker>
<rawString>David M. Blei and John D. Lafferty. Topic models. Text mining: classification, clustering, and applications, 10:71, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bostock</author>
<author>Vadim Ogievetsky</author>
<author>Jeffrey Heer</author>
</authors>
<title>D3: Data-driven documents.</title>
<date>2011</date>
<journal>IEEE Trans. Visualization &amp; Comp. Graphics (Proc. InfoVis),</journal>
<note>URL http://vis.stanford. edu/papers/d3.</note>
<contexts>
<context position="3624" citStr="Bostock et al., 2011" startWordPosition="529" endWordPosition="532">effective under an arguably wider range of data generating assumptions), which is crucial for this example. This nonparametric, visual approach to EDA has been encoded into many data analysis packages, including the now-ubiquitous R language (R Core Team, 2013), which descends from earlier software by the Bell Labs statistics group (Becker and Chambers, 1984). In R, tools such as histograms, boxplots, barplots, dotplots, mosaicplots, etc. are built-in, basic operators in the language. (Wilkinson (2006)’s grammar of graphics more extensively systematizes this approach; see also (Wickham, 2010; Bostock et al., 2011).) In the meantime, textual data has emerged as a resource of increasing interest for many scienFigure 3: Linked brushing with the analysis software GGobi. More references at source:http://www.infovis-wiki.net/index. php?title=Linking_and_Brushing tific, business, and government data analysis applications. Consider the use case of automated content analysis (a.k.a. text mining) as a tool for investigating social scientific and humanistic questions (Grimmer and Stewart, 2013; Jockers, 2013; Shaw, 2012; O’Connor et al., 2011). The content of the data is under question: analysts are interested in</context>
</contexts>
<marker>Bostock, Ogievetsky, Heer, 2011</marker>
<rawString>Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. D3: Data-driven documents. IEEE Trans. Visualization &amp; Comp. Graphics (Proc. InfoVis), 2011. URL http://vis.stanford. edu/papers/d3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Bucholtz</author>
<author>Nancy Bermudez</author>
<author>Victor Fung</author>
</authors>
<title>Lisa Edwards, and Rosalva Vargas. Hella Nor Cal or totally So Cal? the perceptual dialectology of California.</title>
<date>2007</date>
<journal>Journal of English Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<note>URL http://people.duke.edu/ -eec10/hellanorcal.pdf.</note>
<contexts>
<context position="12291" citStr="Bucholtz et al., 2007" startWordPosition="1909" endWordPosition="1912">(Provost and Fawcett, 2013), while PMI has been used in many NLP applications to measure word associations. to protect against rare probabilistic events necessarily involve such a tradeoff parameter that the user ought to experiment with; given this situation, we might prefer a transparent mechanism instead of mathematical priors (though see also §3). Figure 1 shows that hella is the highest ranked term for this spatial selection (and freqency threshold), occurring 7.8 times more frequently compared to the overall corpus; this comports with surveyed intuitions of Californian English speakers (Bucholtz et al., 2007). For full transparency to the user, the local and global term counts are shown in the table. (Since hella occurred 18 times in the queryset and 90 times globally, this implies the simple conditional probability p(Q|w) _ 18/90; and indeed, ranking on p(Q|w) is equivalent to ranking on PMI, since exponentiated PMI is p(Q|w)/p(Q).) The user can also sort by local count to see the raw most-frequent term report for the document selection. As the user reshapes the query box, or drags it around the space, the terms in panel (C) are updated. Not shown are options to change the term frequency represen</context>
</contexts>
<marker>Bucholtz, Bermudez, Fung, 2007</marker>
<rawString>Mary Bucholtz, Nancy Bermudez, Victor Fung, Lisa Edwards, and Rosalva Vargas. Hella Nor Cal or totally So Cal? the perceptual dialectology of California. Journal of English Linguistics, 35(4):325–352, 2007. URL http://people.duke.edu/ -eec10/hellanorcal.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Buja</author>
<author>John Alan McDonald</author>
<author>John Michalak</author>
<author>Werner Stuetzle</author>
</authors>
<title>Interactive data visualization using focusing and linking.</title>
<date>1991</date>
<booktitle>In Visualization,</booktitle>
<pages>156--163</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="8292" citStr="Buja et al. (1991)" startWordPosition="1257" endWordPosition="1260">alyzing covariate-text relationships (Mimno, 2012; Roberts et al., 2013), there is no dimension reduction of the terms. Instead, interactivity allows a user to explore more of the high-dimensional space, by specifying a document selection (Q) and/or a term selection (T). We are inspired by the linking and brushing family of techniques in interactive data visualization, in which an analyst can select a group of data points under a query in one covariate space, and see the same data selection in a different covariate space (Figure 3; see Buja et al. (1996), and e.g. Becker and Cleveland (1987); Buja et al. (1991); Martin and Ward (1995); Cook and Swayne (2007)). In our case, one of the variables is text. The interface consists of several linked views, which contain: (A) a view of the documents in a two-dimensional covariate space (e.g. scatterplot), (B) an optional list of pinned terms, (C) document-associated terms: a view of the relatively most frequent terms for the current document selection, (D) term-associated terms: a view of terms that relatively frequently co-occur with the current term selection; and (E) a keyword-in-context (KWIC) display of textual passages for the current term selection. </context>
</contexts>
<marker>Buja, McDonald, Michalak, Stuetzle, 1991</marker>
<rawString>Andreas Buja, John Alan McDonald, John Michalak, and Werner Stuetzle. Interactive data visualization using focusing and linking. In Visualization, 1991. Visualization’91, Proceedings., IEEE Conference on, pages 156–163. IEEE, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Buja</author>
<author>Dianne Cook</author>
<author>Deborah F Swayne</author>
</authors>
<title>Interactive high-dimensional data visualization.</title>
<date>1996</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="8234" citStr="Buja et al. (1996)" startWordPosition="1247" endWordPosition="1250"> counted per document. Unlike topic model approaches to analyzing covariate-text relationships (Mimno, 2012; Roberts et al., 2013), there is no dimension reduction of the terms. Instead, interactivity allows a user to explore more of the high-dimensional space, by specifying a document selection (Q) and/or a term selection (T). We are inspired by the linking and brushing family of techniques in interactive data visualization, in which an analyst can select a group of data points under a query in one covariate space, and see the same data selection in a different covariate space (Figure 3; see Buja et al. (1996), and e.g. Becker and Cleveland (1987); Buja et al. (1991); Martin and Ward (1995); Cook and Swayne (2007)). In our case, one of the variables is text. The interface consists of several linked views, which contain: (A) a view of the documents in a two-dimensional covariate space (e.g. scatterplot), (B) an optional list of pinned terms, (C) document-associated terms: a view of the relatively most frequent terms for the current document selection, (D) term-associated terms: a view of terms that relatively frequently co-occur with the current term selection; and (E) a keyword-in-context (KWIC) di</context>
</contexts>
<marker>Buja, Cook, Swayne, 1996</marker>
<rawString>Andreas Buja, Dianne Cook, and Deborah F Swayne. Interactive high-dimensional data visualization. Journal of Computational and Graphical Statistics, 5(1):78–99, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allison J B Chaney</author>
<author>David M Blei</author>
</authors>
<title>Visualizing topic models.</title>
<date>2013</date>
<booktitle>In Proceedings of ICWSM,</booktitle>
<contexts>
<context position="30495" citStr="Chaney and Blei, 2013" startWordPosition="4887" endWordPosition="4890">lysis, while being efficient with the use of space. Many other systems, especially ones designed for literary content analysis, emphasize concordances and keyword searches within a text; for example, Voyeur/Voyant (Rockwell et al., 2010),7 which also features some document covariate analysis through temporal trend analyses for individual terms. Another class of approaches emphasizes the use of document clustering or topic models (Gardner et al., 2010; Newman et al., 2010; 6http://www.cc.gatech.edu/gvu/ii/ jigsaw/ 7http://voyant-tools.org/, http://hermeneuti.ca/voyeur 9 Grimmer and King, 2011; Chaney and Blei, 2013), while Overview8 emphasizes hierarchical document clustering paired with manual tagging. Finally, considerable research has examined exploratory visual interfaces for information retrieval, in which a user specifies an information need in order to find relevant documents or passages from a corpus (Hearst (2009), Ch. 10). Information retrieval problems have some similarities to text-as-data analysis in the need for an exploratory process of iterative refinement, but the text-as-data perspective differs in that it requires an analyst to understand content and contextual factors across multiple </context>
</contexts>
<marker>Chaney, Blei, 2013</marker>
<rawString>Allison J.B. Chaney and David M. Blei. Visualizing topic models. In Proceedings of ICWSM, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>without the clutter of unimportant words”: Descriptive keyphrases for text visualization.</title>
<date>2012</date>
<journal>ACM Trans. on ComputerHuman Interaction,</journal>
<volume>19</volume>
<note>URL http://vis.stanford.edu/papers/ keyphrases.</note>
<contexts>
<context position="21308" citStr="Chuang et al. (2012" startWordPosition="3454" endWordPosition="3457">v) N2 Ed n2d 6 used by Gentzkow and Shapiro (2010) to find politically loaded phrases often used by members of one political party; this same method is often used as a feature selection method for supervised learning (Guyon and Elisseeff, 2003). The approach we take here is somewhat different, being a point estimate approach, analyzing the estimated difference (and giving poor results when counts are small). Some related work for topic model analysis, looking at statistical associations between words and latent topics (as opposed to between words and observed covariates in this work) includes Chuang et al. (2012b), whose term saliency function measures one word’s associations against all topics; a salient term tends to have most of its probability mass in a small set of topics. The measure is a form of mutual information,3 and may be useful for our purposes here if the user wishes to see a report of distinctive terms for a group of several different observed covariate values at once. Blei and Lafferty (2009) ranks words per topic by a measure inspired by TFIDF, which like PMI downweights words that are generically common across all topics. Finally, hierarchical priors and regularizers can also be use</context>
<context position="26451" citStr="Chuang et al. (2012" startWordPosition="4276" endWordPosition="4279">ause they are relatively less reliant on predicateargument structure to express their semantics—as opposed to adjectives or verbs, say—and a bag-ofterms analysis does not allow expression of argument structure.) However, for many corpora, POS or NER taggers work poorly—for example, I’ve seen paper titles from the ACL Anthology have capitalized prepositions tagged as names—so simpler stopword heuristics are necessary. The phrase selection approach could be improved in many ways; for example, a real noun phrase recognizer could get important (NP PP) constructs like “war on terror.” Furthermore, Chuang et al. (2012a) find that while these sorts of syntactic features are helpful in choosing useful keyphrases to summarize of scientific abstracts, it is also very useful to add in collocation detection scores. Similarly to the PMI calculations used here, likelihood ratio or chi-square collocation detection statistics are also very rapid to compute and may benefit from interactive adjustment of decision thresholds. More generally, any type of lexicalized linguistic structures could potentially be used, such as dependency paths or constituents from a syntactic parser, or predicateargument structures from a se</context>
</contexts>
<marker>Chuang, Manning, Heer, 2012</marker>
<rawString>Jason Chuang, Christopher D. Manning, and Jeffrey Heer. ”without the clutter of unimportant words”: Descriptive keyphrases for text visualization. ACM Trans. on ComputerHuman Interaction, 19:1–29, 2012a. URL http://vis.stanford.edu/papers/ keyphrases.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Termite: Visualization techniques for assessing textual topic models.</title>
<booktitle>In Advanced Visual Interfaces, 2012b. URL</booktitle>
<note>http://vis. stanford.edu/papers/termite.</note>
<marker>Chuang, Manning, Heer, </marker>
<rawString>Jason Chuang, Christopher D. Manning, and Jeffrey Heer. Termite: Visualization techniques for assessing textual topic models. In Advanced Visual Interfaces, 2012b. URL http://vis. stanford.edu/papers/termite.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="16432" citStr="Church and Hanks (1990)" startWordPosition="2619" endWordPosition="2622">queryset; it would be interesting to perform grouping and stratified sampling based on local contextual statistics. Summarizing local context by frequencies could be done as a trie visualization (Wattenberg and Vi´egas, 2008); see §5. 2.3 Term-association queries When a term is selected, its interaction with covariates is shown by highlighting documents in (B) that contain the term. This can be thought of as another document query: instead of being specified as a region in the covariate space, is specified as a fragment of the discrete lexical space. As illustrated in much previous work (e.g. Church and Hanks (1990); Turney (2001, 2002)), word-toword PMI scores can find other terms with similar meanings, or having interesting semantic relationships, to the target term.2 This panel ranks terms u by their association with the query term v. The simplest method is to analyze the relative frequencies of terms in documents that contain v, bool-tt-epmi(u, v) = p(wi = u|v E supp(di)) p(wi = u) Here, the subscript i denotes a token position in the entire corpus, for which there is a wordtype wi and a document ID di. In this notation, the covariate PMI in 2.1 would be p(wi = u|di E Q)/p(wi = u). supp(di) denotes t</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. W Church and P. Hanks. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):2229, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William S Cleveland</author>
</authors>
<title>Visualizing data.</title>
<date>1993</date>
<publisher>Hobart Press,</publisher>
<contexts>
<context position="1960" citStr="Cleveland (1993)" startWordPosition="282" endWordPosition="283">e how which are crucial to help ensure a dataset will be terms are used in context. The goal is to useful. Exploratory techniques can also suggest rapidly compare language usage across in- possible hypotheses or issues for further investiteresting document covariates. gation. 1 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 1–13, Baltimore, Maryland, USA, June 27, 2014. c�2014 Association for Computational Linguistics Figure 2: Anscombe Quartet. (Source: Wikipedia) The classical approach to EDA, as pioneered in works such as Tukey (1977) and Cleveland (1993) (and other work from the Bell Labs statistics group during that period) emphasizes visual analysis under nonparametric, model-free assumptions, in ������������������������������������������������� 1 which visual attributes are a fairly direct reflection of numerical or categorical aspects of data. As a simple example, consider the well-known Anscombe Quartet (1973), a set of four bivariate example datasets. The Pearson correlation, a very widely used measure of dependence that assumes a linear Gaussian model of the data, finds that each dataset has an identical amount of dependence (r = 0.82)</context>
</contexts>
<marker>Cleveland, 1993</marker>
<rawString>William S. Cleveland. Visualizing data. Hobart Press, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dianne Cook</author>
<author>Deborah F Swayne</author>
</authors>
<title>Interactive and dynamic graphics for data analysis: with R and GGobi.</title>
<date>2007</date>
<publisher>Springer,</publisher>
<contexts>
<context position="8340" citStr="Cook and Swayne (2007)" startWordPosition="1265" endWordPosition="1268"> 2012; Roberts et al., 2013), there is no dimension reduction of the terms. Instead, interactivity allows a user to explore more of the high-dimensional space, by specifying a document selection (Q) and/or a term selection (T). We are inspired by the linking and brushing family of techniques in interactive data visualization, in which an analyst can select a group of data points under a query in one covariate space, and see the same data selection in a different covariate space (Figure 3; see Buja et al. (1996), and e.g. Becker and Cleveland (1987); Buja et al. (1991); Martin and Ward (1995); Cook and Swayne (2007)). In our case, one of the variables is text. The interface consists of several linked views, which contain: (A) a view of the documents in a two-dimensional covariate space (e.g. scatterplot), (B) an optional list of pinned terms, (C) document-associated terms: a view of the relatively most frequent terms for the current document selection, (D) term-associated terms: a view of terms that relatively frequently co-occur with the current term selection; and (E) a keyword-in-context (KWIC) display of textual passages for the current term selection. Figure 1 shows the interface viewing a corpus of</context>
</contexts>
<marker>Cook, Swayne, 2007</marker>
<rawString>Dianne Cook and Deborah F. Swayne. Interactive and dynamic graphics for data analysis: with R and GGobi. Springer, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>10--1</pages>
<note>URL http://citeseerx. ist.psu.edu/viewdoc/summary? doi=10.1.1.14.5962.</note>
<contexts>
<context position="20537" citStr="Dunning, 1993" startWordPosition="3328" endWordPosition="3330">rm of text analysis; in fact, the WordSeer corpus analysis tool has explicit support to help users create them (Shrikumar, 2013). 3 Statistical term association measures There exist many measures to measure the statistical strength of an association between a term and a document covariate, or between two terms. A number of methods are based on significance testing, looking for violations of a null hypothesis that term frequencies are independent. For collocation detection, which aims to find meaningful non-compositional lexical items through frequencies of neighboring words, likelihood ratio (Dunning, 1993) and chi-square tests have been used (see review in Manning and Sch¨utze (1999)). For term-covariate associations, chi-square tests were p(wi = u, wj = v) N2 Ed n2d 6 used by Gentzkow and Shapiro (2010) to find politically loaded phrases often used by members of one political party; this same method is often used as a feature selection method for supervised learning (Guyon and Elisseeff, 2003). The approach we take here is somewhat different, being a point estimate approach, analyzing the estimated difference (and giving poor results when counts are small). Some related work for topic model an</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19:61—74, 1993. doi: 10.1.1.14.5962. URL http://citeseerx. ist.psu.edu/viewdoc/summary? doi=10.1.1.14.5962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisenstein</author>
<author>A Ahmed</author>
<author>E P Xing</author>
</authors>
<title>Sparse additive generative models of text.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>1041--1048</pages>
<contexts>
<context position="22014" citStr="Eisenstein et al., 2011" startWordPosition="3572" endWordPosition="3575">s; a salient term tends to have most of its probability mass in a small set of topics. The measure is a form of mutual information,3 and may be useful for our purposes here if the user wishes to see a report of distinctive terms for a group of several different observed covariate values at once. Blei and Lafferty (2009) ranks words per topic by a measure inspired by TFIDF, which like PMI downweights words that are generically common across all topics. Finally, hierarchical priors and regularizers can also be used; for example, by penalizing the log-odds parameterization of term probabilities (Eisenstein et al., 2011; Taddy, 2013). These methods are better in that they incorporate both protection against small count situations, while paying attention to effect size, as well as allowing overlapping covariates and regression control variables; but unfortunately, they are more computationally intensive, as opposed to the above measures which all work directly from sufficient count statistics. An association measure that fulfilled all these desiderata would be very useful. For termcovariate analysis, Monroe et al. (2008) contains a review of many different methods, from both political science as well as compu</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>J. Eisenstein, A. Ahmed, and E.P. Xing. Sparse additive generative models of text. In Proceedings of ICML, pages 1041–1048, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1277--1287</pages>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. A latent variable model for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277— 1287, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Mapping the geographical diffusion of new words.</title>
<date>2012</date>
<booktitle>In NIPS Workshop on Social Network and Social Media Analysis,</booktitle>
<pages>1210--5268</pages>
<note>URL http://arxiv.org/abs/</note>
<marker>Eisenstein, O’Connor, Smith, Xing, 2012</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. Mapping the geographical diffusion of new words. In NIPS Workshop on Social Network and Social Media Analysis, 2012. URL http://arxiv.org/abs/ 1210.5268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Gardner</author>
<author>J Lutes</author>
<author>J Lund</author>
<author>J Hansen</author>
<author>D Walker</author>
<author>E Ringger</author>
<author>K Seppi</author>
</authors>
<title>The topic browser: An interactive tool for browsing topic models.</title>
<date>2010</date>
<booktitle>In NIPS Workshop on Challenges of Data Visualization.</booktitle>
<publisher>MIT Press,</publisher>
<contexts>
<context position="30327" citStr="Gardner et al., 2010" startWordPosition="4870" endWordPosition="4873">marization of local contextual information may be useful to integrate; it is worth thinking how to integrate this against the important need of document covariate analysis, while being efficient with the use of space. Many other systems, especially ones designed for literary content analysis, emphasize concordances and keyword searches within a text; for example, Voyeur/Voyant (Rockwell et al., 2010),7 which also features some document covariate analysis through temporal trend analyses for individual terms. Another class of approaches emphasizes the use of document clustering or topic models (Gardner et al., 2010; Newman et al., 2010; 6http://www.cc.gatech.edu/gvu/ii/ jigsaw/ 7http://voyant-tools.org/, http://hermeneuti.ca/voyeur 9 Grimmer and King, 2011; Chaney and Blei, 2013), while Overview8 emphasizes hierarchical document clustering paired with manual tagging. Finally, considerable research has examined exploratory visual interfaces for information retrieval, in which a user specifies an information need in order to find relevant documents or passages from a corpus (Hearst (2009), Ch. 10). Information retrieval problems have some similarities to text-as-data analysis in the need for an explorator</context>
</contexts>
<marker>Gardner, Lutes, Lund, Hansen, Walker, Ringger, Seppi, 2010</marker>
<rawString>M.J. Gardner, J. Lutes, J. Lund, J. Hansen, D. Walker, E. Ringger, and K. Seppi. The topic browser: An interactive tool for browsing topic models. In NIPS Workshop on Challenges of Data Visualization. MIT Press, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gentzkow</author>
<author>Jesse M Shapiro</author>
</authors>
<title>What drives media slant? evidence from us daily newspapers.</title>
<date>2010</date>
<journal>Econometrica,</journal>
<volume>78</volume>
<issue>1</issue>
<contexts>
<context position="4909" citStr="Gentzkow and Shapiro, 2010" startWordPosition="725" endWordPosition="728">udes are expressed in a corpus, and the trends in these factors across time, space, author communities, or other documentlevel covariates (often called metadata). Comparisons of word statistics across covariates are absolutely essential to many interesting questions or social measurement problems, such as • What topics tend to get censored by the Chinese government online, and why (Bamman et al., 2012; King et al., 2013)? Covariates: whether a message is deleted by censors, time/location of message. • What drives media bias? Do newspapers slant their coverage in response to what readers want (Gentzkow and Shapiro, 2010)? Covariates: political preferences of readers, competitiveness of media markets. There exist dozens, if not more, of other examples in social scientific and humanities research; see references in O’Connor et al. (2011); O’Connor (2014). In this work, I focus on the question: What 2 should be the baseline exploratory tools for textual data, to discover important statistical associations between text and document covariates? Ideally, we’d like to “just look” at the data, in the spirit of scatterplotting the Anscombe Quartet. An analysis tool to support this should not require any statistical mo</context>
<context position="20739" citStr="Gentzkow and Shapiro (2010)" startWordPosition="3362" endWordPosition="3365">asures to measure the statistical strength of an association between a term and a document covariate, or between two terms. A number of methods are based on significance testing, looking for violations of a null hypothesis that term frequencies are independent. For collocation detection, which aims to find meaningful non-compositional lexical items through frequencies of neighboring words, likelihood ratio (Dunning, 1993) and chi-square tests have been used (see review in Manning and Sch¨utze (1999)). For term-covariate associations, chi-square tests were p(wi = u, wj = v) N2 Ed n2d 6 used by Gentzkow and Shapiro (2010) to find politically loaded phrases often used by members of one political party; this same method is often used as a feature selection method for supervised learning (Guyon and Elisseeff, 2003). The approach we take here is somewhat different, being a point estimate approach, analyzing the estimated difference (and giving poor results when counts are small). Some related work for topic model analysis, looking at statistical associations between words and latent topics (as opposed to between words and observed covariates in this work) includes Chuang et al. (2012b), whose term saliency functio</context>
<context position="24634" citStr="Gentzkow and Shapiro (2010)" startWordPosition="3990" endWordPosition="3993">owsed by the system as the term variables. (More complex concepts, of course, would also require more UI support.) At the same time, words can be too coarse compared to the longer phrases they are contained within, which often contain more interesting and distinctive concepts: for example, “death tax” and “social security” are important concepts in U.S. politics that get missed under a unigram analysis. In fact, Sim et al. (2013)’s analysis of U.S. politicians’ speeches found that domain experts had a hard time understanding unigrams out-ofcontext, but bigrams and trigrams worked much better; Gentzkow and Shapiro (2010) similarly focus on partisan political phrases. It sometimes works to simply add overlapping n-grams as more terms, but sometimes odd phrases get selected that cross constituent boundaries from their source sentences, and are thus not totally meaningful. I’ve experimented with a very strong filtering approach to phrase selection: besides using all unigrams, take all n-grams up to length 5 that have nominal part-of-speech patterns: either the sequence consists of zero or more adjectives followed by one or more noun tokens, or all tokens were classified as names by a named entity recognition sys</context>
</contexts>
<marker>Gentzkow, Shapiro, 2010</marker>
<rawString>Matthew Gentzkow and Jesse M Shapiro. What drives media slant? evidence from us daily newspapers. Econometrica, 78(1):35–71, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carsten G¨org</author>
<author>Zhicheng Liu</author>
<author>John Stasko</author>
</authors>
<title>Reflections on the evolution of the jigsaw visual analytics system. Information Visualization,</title>
<date>2013</date>
<marker>G¨org, Liu, Stasko, 2013</marker>
<rawString>Carsten G¨org, Zhicheng Liu, and John Stasko. Reflections on the evolution of the jigsaw visual analytics system. Information Visualization, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Grimmer</author>
<author>Gary King</author>
</authors>
<title>General purpose computer-assisted clustering and conceptualization.</title>
<date>2011</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>108</volume>
<issue>7</issue>
<contexts>
<context position="30471" citStr="Grimmer and King, 2011" startWordPosition="4883" endWordPosition="4886">f document covariate analysis, while being efficient with the use of space. Many other systems, especially ones designed for literary content analysis, emphasize concordances and keyword searches within a text; for example, Voyeur/Voyant (Rockwell et al., 2010),7 which also features some document covariate analysis through temporal trend analyses for individual terms. Another class of approaches emphasizes the use of document clustering or topic models (Gardner et al., 2010; Newman et al., 2010; 6http://www.cc.gatech.edu/gvu/ii/ jigsaw/ 7http://voyant-tools.org/, http://hermeneuti.ca/voyeur 9 Grimmer and King, 2011; Chaney and Blei, 2013), while Overview8 emphasizes hierarchical document clustering paired with manual tagging. Finally, considerable research has examined exploratory visual interfaces for information retrieval, in which a user specifies an information need in order to find relevant documents or passages from a corpus (Hearst (2009), Ch. 10). Information retrieval problems have some similarities to text-as-data analysis in the need for an exploratory process of iterative refinement, but the text-as-data perspective differs in that it requires an analyst to understand content and contextual </context>
</contexts>
<marker>Grimmer, King, 2011</marker>
<rawString>Justin Grimmer and Gary King. General purpose computer-assisted clustering and conceptualization. Proceedings of the National Academy of Sciences, 108(7):2643–2650, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Grimmer</author>
<author>Brandon M Stewart</author>
</authors>
<title>Text as Data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis,</title>
<date>2013</date>
<volume>21</volume>
<issue>3</issue>
<pages>297</pages>
<note>URL http://www.stanford. edu/-jgrimmer/tad2.pdf.</note>
<contexts>
<context position="4102" citStr="Grimmer and Stewart, 2013" startWordPosition="594" endWordPosition="597">n the language. (Wilkinson (2006)’s grammar of graphics more extensively systematizes this approach; see also (Wickham, 2010; Bostock et al., 2011).) In the meantime, textual data has emerged as a resource of increasing interest for many scienFigure 3: Linked brushing with the analysis software GGobi. More references at source:http://www.infovis-wiki.net/index. php?title=Linking_and_Brushing tific, business, and government data analysis applications. Consider the use case of automated content analysis (a.k.a. text mining) as a tool for investigating social scientific and humanistic questions (Grimmer and Stewart, 2013; Jockers, 2013; Shaw, 2012; O’Connor et al., 2011). The content of the data is under question: analysts are interested in what/when/how/by-whom different concepts, ideas, or attitudes are expressed in a corpus, and the trends in these factors across time, space, author communities, or other documentlevel covariates (often called metadata). Comparisons of word statistics across covariates are absolutely essential to many interesting questions or social measurement problems, such as • What topics tend to get censored by the Chinese government online, and why (Bamman et al., 2012; King et al., 2</context>
</contexts>
<marker>Grimmer, Stewart, 2013</marker>
<rawString>Justin Grimmer and Brandon M Stewart. Text as Data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis, 21(3):267– 297, 2013. URL http://www.stanford. edu/-jgrimmer/tad2.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Andr´e Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1182</pages>
<contexts>
<context position="20933" citStr="Guyon and Elisseeff, 2003" startWordPosition="3395" endWordPosition="3398">ations of a null hypothesis that term frequencies are independent. For collocation detection, which aims to find meaningful non-compositional lexical items through frequencies of neighboring words, likelihood ratio (Dunning, 1993) and chi-square tests have been used (see review in Manning and Sch¨utze (1999)). For term-covariate associations, chi-square tests were p(wi = u, wj = v) N2 Ed n2d 6 used by Gentzkow and Shapiro (2010) to find politically loaded phrases often used by members of one political party; this same method is often used as a feature selection method for supervised learning (Guyon and Elisseeff, 2003). The approach we take here is somewhat different, being a point estimate approach, analyzing the estimated difference (and giving poor results when counts are small). Some related work for topic model analysis, looking at statistical associations between words and latent topics (as opposed to between words and observed covariates in this work) includes Chuang et al. (2012b), whose term saliency function measures one word’s associations against all topics; a salient term tends to have most of its probability mass in a small set of topics. The measure is a form of mutual information,3 and may b</context>
</contexts>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>Isabelle Guyon and Andr´e Elisseeff. An introduction to variable and feature selection. The Journal of Machine Learning Research, 3:1157– 1182, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Search user interfaces.</title>
<date>2009</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="30808" citStr="Hearst (2009)" startWordPosition="4934" endWordPosition="4935">es for individual terms. Another class of approaches emphasizes the use of document clustering or topic models (Gardner et al., 2010; Newman et al., 2010; 6http://www.cc.gatech.edu/gvu/ii/ jigsaw/ 7http://voyant-tools.org/, http://hermeneuti.ca/voyeur 9 Grimmer and King, 2011; Chaney and Blei, 2013), while Overview8 emphasizes hierarchical document clustering paired with manual tagging. Finally, considerable research has examined exploratory visual interfaces for information retrieval, in which a user specifies an information need in order to find relevant documents or passages from a corpus (Hearst (2009), Ch. 10). Information retrieval problems have some similarities to text-as-data analysis in the need for an exploratory process of iterative refinement, but the text-as-data perspective differs in that it requires an analyst to understand content and contextual factors across multiple or many documents. 6 Future work The current MITEXTEXPLORER system is an extremely simple prototype to explore what sorts of “bare words” text-and-covariates analyses are possible. Several major changes will be necessary for more serious use. First, essential basic capabilities must be added, such as a search bo</context>
</contexts>
<marker>Hearst, 2009</marker>
<rawString>Marti Hearst. Search user interfaces. Cambridge University Press, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew L Jockers</author>
</authors>
<title>Macroanalysis: Digital methods and literary history.</title>
<date>2013</date>
<publisher>University of Illinois Press,</publisher>
<contexts>
<context position="4117" citStr="Jockers, 2013" startWordPosition="598" endWordPosition="599">(2006)’s grammar of graphics more extensively systematizes this approach; see also (Wickham, 2010; Bostock et al., 2011).) In the meantime, textual data has emerged as a resource of increasing interest for many scienFigure 3: Linked brushing with the analysis software GGobi. More references at source:http://www.infovis-wiki.net/index. php?title=Linking_and_Brushing tific, business, and government data analysis applications. Consider the use case of automated content analysis (a.k.a. text mining) as a tool for investigating social scientific and humanistic questions (Grimmer and Stewart, 2013; Jockers, 2013; Shaw, 2012; O’Connor et al., 2011). The content of the data is under question: analysts are interested in what/when/how/by-whom different concepts, ideas, or attitudes are expressed in a corpus, and the trends in these factors across time, space, author communities, or other documentlevel covariates (often called metadata). Comparisons of word statistics across covariates are absolutely essential to many interesting questions or social measurement problems, such as • What topics tend to get censored by the Chinese government online, and why (Bamman et al., 2012; King et al., 2013)? Covariate</context>
</contexts>
<marker>Jockers, 2013</marker>
<rawString>Matthew L Jockers. Macroanalysis: Digital methods and literary history. University of Illinois Press, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary King</author>
<author>Jennifer Pan</author>
<author>Margaret E Roberts</author>
</authors>
<title>How censorship in china allows government criticism but silences collective expression.</title>
<date>2013</date>
<journal>American Political Science Review,</journal>
<volume>107</volume>
<contexts>
<context position="4706" citStr="King et al., 2013" startWordPosition="691" endWordPosition="694"> Stewart, 2013; Jockers, 2013; Shaw, 2012; O’Connor et al., 2011). The content of the data is under question: analysts are interested in what/when/how/by-whom different concepts, ideas, or attitudes are expressed in a corpus, and the trends in these factors across time, space, author communities, or other documentlevel covariates (often called metadata). Comparisons of word statistics across covariates are absolutely essential to many interesting questions or social measurement problems, such as • What topics tend to get censored by the Chinese government online, and why (Bamman et al., 2012; King et al., 2013)? Covariates: whether a message is deleted by censors, time/location of message. • What drives media bias? Do newspapers slant their coverage in response to what readers want (Gentzkow and Shapiro, 2010)? Covariates: political preferences of readers, competitiveness of media markets. There exist dozens, if not more, of other examples in social scientific and humanities research; see references in O’Connor et al. (2011); O’Connor (2014). In this work, I focus on the question: What 2 should be the baseline exploratory tools for textual data, to discover important statistical associations between</context>
</contexts>
<marker>King, Pan, Roberts, 2013</marker>
<rawString>Gary King, Jennifer Pan, and Margaret E. Roberts. How censorship in china allows government criticism but silences collective expression. American Political Science Review, 107:1–18, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT press,</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D Manning and Hinrich Sch¨utze. Foundations of statistical natural language processing. MIT press, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allen R Martin</author>
<author>Matthew O Ward</author>
</authors>
<title>High dimensional brushing for interactive exploration of multivariate data.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Conference on Visualization’95,</booktitle>
<pages>271</pages>
<publisher>IEEE Computer Society,</publisher>
<contexts>
<context position="8316" citStr="Martin and Ward (1995)" startWordPosition="1261" endWordPosition="1264">xt relationships (Mimno, 2012; Roberts et al., 2013), there is no dimension reduction of the terms. Instead, interactivity allows a user to explore more of the high-dimensional space, by specifying a document selection (Q) and/or a term selection (T). We are inspired by the linking and brushing family of techniques in interactive data visualization, in which an analyst can select a group of data points under a query in one covariate space, and see the same data selection in a different covariate space (Figure 3; see Buja et al. (1996), and e.g. Becker and Cleveland (1987); Buja et al. (1991); Martin and Ward (1995); Cook and Swayne (2007)). In our case, one of the variables is text. The interface consists of several linked views, which contain: (A) a view of the documents in a two-dimensional covariate space (e.g. scatterplot), (B) an optional list of pinned terms, (C) document-associated terms: a view of the relatively most frequent terms for the current document selection, (D) term-associated terms: a view of terms that relatively frequently co-occur with the current term selection; and (E) a keyword-in-context (KWIC) display of textual passages for the current term selection. Figure 1 shows the inter</context>
</contexts>
<marker>Martin, Ward, 1995</marker>
<rawString>Allen R. Martin and Matthew O. Ward. High dimensional brushing for interactive exploration of multivariate data. In Proceedings of the 6th Conference on Visualization’95, page 271. IEEE Computer Society, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Melnik</author>
</authors>
<title>Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, and Theo Vassilakis. Dremel: interactive analysis of web-scale datasets.</title>
<date>2010</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<pages>3--1</pages>
<marker>Melnik, 2010</marker>
<rawString>Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, and Theo Vassilakis. Dremel: interactive analysis of web-scale datasets. Proceedings of the VLDB Endowment, 3(1-2):330–339, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
</authors>
<title>Topic regression.</title>
<date>2012</date>
<tech>PhD thesis,</tech>
<institution>University of Massachusetts</institution>
<location>Amherst,</location>
<contexts>
<context position="7723" citStr="Mimno, 2012" startWordPosition="1161" endWordPosition="1162"> and languages. A useful exploratory tool should be able to work with a variety of levels of sophistication in NLP tooling, and allow the user to fall back to manual reading when necessary. 2 MITEXTEXPLORER: linked brushing for text and covariate correlations The analysis tool presented here, MITEXTEXPLORER, is designed for exploratory analysis of relationships between document covariates—such as time, space, or author community—against textual variables—words, or other units of meaning, that can be counted per document. Unlike topic model approaches to analyzing covariate-text relationships (Mimno, 2012; Roberts et al., 2013), there is no dimension reduction of the terms. Instead, interactivity allows a user to explore more of the high-dimensional space, by specifying a document selection (Q) and/or a term selection (T). We are inspired by the linking and brushing family of techniques in interactive data visualization, in which an analyst can select a group of data points under a query in one covariate space, and see the same data selection in a different covariate space (Figure 3; see Buja et al. (1996), and e.g. Becker and Cleveland (1987); Buja et al. (1991); Martin and Ward (1995); Cook </context>
</contexts>
<marker>Mimno, 2012</marker>
<rawString>David Mimno. Topic regression. PhD thesis, University of Massachusetts Amherst, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B L Monroe</author>
<author>M P Colaresi</author>
<author>K M Quinn</author>
</authors>
<title>Fightin’Words: lexical feature selection and evaluation for identifying the content of political conflict.</title>
<date>2008</date>
<journal>Political Analysis,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="22524" citStr="Monroe et al. (2008)" startWordPosition="3648" endWordPosition="3651"> used; for example, by penalizing the log-odds parameterization of term probabilities (Eisenstein et al., 2011; Taddy, 2013). These methods are better in that they incorporate both protection against small count situations, while paying attention to effect size, as well as allowing overlapping covariates and regression control variables; but unfortunately, they are more computationally intensive, as opposed to the above measures which all work directly from sufficient count statistics. An association measure that fulfilled all these desiderata would be very useful. For termcovariate analysis, Monroe et al. (2008) contains a review of many different methods, from both political science as well as computer science; they also propose a hierarchical prior method, and to rank by statistical significance via the asymptotic 3This is apparent as follows, using notation from their section 3.1: � saliency(w) = p(w) T p(w, T) log[p(w, T)/[p(w)p(T)]] This might be called a “half-pointwise” mutual information: between a specific word w and the topic random variable T. Mutual information is Ew saliency(w). standard error of the terms’ odds ratios. Given the large amount of previous work using the significance appro</context>
</contexts>
<marker>Monroe, Colaresi, Quinn, 2008</marker>
<rawString>B. L. Monroe, M. P. Colaresi, and K. M. Quinn. Fightin’Words: lexical feature selection and evaluation for identifying the content of political conflict. Political Analysis, 16(4):372, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newman</author>
<author>T Baldwin</author>
<author>L Cavedon</author>
<author>E Huang</author>
<author>S Karimi</author>
<author>D Martinez</author>
<author>F Scholer</author>
<author>J Zobel</author>
</authors>
<title>Visualizing search results and document collections using topic maps. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2010</date>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="30348" citStr="Newman et al., 2010" startWordPosition="4874" endWordPosition="4877">ntextual information may be useful to integrate; it is worth thinking how to integrate this against the important need of document covariate analysis, while being efficient with the use of space. Many other systems, especially ones designed for literary content analysis, emphasize concordances and keyword searches within a text; for example, Voyeur/Voyant (Rockwell et al., 2010),7 which also features some document covariate analysis through temporal trend analyses for individual terms. Another class of approaches emphasizes the use of document clustering or topic models (Gardner et al., 2010; Newman et al., 2010; 6http://www.cc.gatech.edu/gvu/ii/ jigsaw/ 7http://voyant-tools.org/, http://hermeneuti.ca/voyeur 9 Grimmer and King, 2011; Chaney and Blei, 2013), while Overview8 emphasizes hierarchical document clustering paired with manual tagging. Finally, considerable research has examined exploratory visual interfaces for information retrieval, in which a user specifies an information need in order to find relevant documents or passages from a corpus (Hearst (2009), Ch. 10). Information retrieval problems have some similarities to text-as-data analysis in the need for an exploratory process of iterativ</context>
</contexts>
<marker>Newman, Baldwin, Cavedon, Huang, Karimi, Martinez, Scholer, Zobel, 2010</marker>
<rawString>D. Newman, T. Baldwin, L. Cavedon, E. Huang, S. Karimi, D. Martinez, F. Scholer, and J. Zobel. Visualizing search results and document collections using topic maps. Web Semantics: Science, Services and Agents on the World Wide Web, 8(2):169–175, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
</authors>
<title>Statistical Text Analysis for Social Science.</title>
<date>2014</date>
<tech>PhD thesis,</tech>
<institution>Carnegie Mellon University,</institution>
<marker>O’Connor, 2014</marker>
<rawString>Brendan O’Connor. Statistical Text Analysis for Social Science. PhD thesis, Carnegie Mellon University, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Michel Krieger</author>
<author>David Ahn</author>
</authors>
<title>TweetMotif: Exploratory search and topic summarization for Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media,</booktitle>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>Brendan O’Connor, Michel Krieger, and David Ahn. TweetMotif: Exploratory search and topic summarization for Twitter. In Proceedings of the International AAAI Conference on Weblogs and Social Media, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>David Bamman</author>
<author>Noah A Smith</author>
</authors>
<title>Computational text analysis for social science: Model assumptions and complexity.</title>
<date>2011</date>
<booktitle>In Second Workshop on Comptuational Social Science and the Wisdom of Crowds (NIPS</booktitle>
<marker>O’Connor, Bamman, Smith, 2011</marker>
<rawString>Brendan O’Connor, David Bamman, and Noah A. Smith. Computational text analysis for social science: Model assumptions and complexity. In Second Workshop on Comptuational Social Science and the Wisdom of Crowds (NIPS 2011), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Foster Provost</author>
<author>Tom Fawcett</author>
</authors>
<title>Data Science for Business. O’Reilly Media,</title>
<date>2013</date>
<contexts>
<context position="11696" citStr="Provost and Fawcett, 2013" startWordPosition="1816" endWordPosition="1819">nown weakness of PMI is overemphasis on rare terms; terms that appear only in the queryset, even if they appear only once, will attain the highest PMI value. One way to address this is through a smoothing prior/pseudocounts/regularization, or through statistical significance ranking (see §3). For simplicity, we use a minimum frequency threshold filter. The user interface allows minimums for either local or global term frequencies, and to easily adjust them, which naturally shifts the emphasis between specific and generic language. All methods 1The term “lift” is used in business applications (Provost and Fawcett, 2013), while PMI has been used in many NLP applications to measure word associations. to protect against rare probabilistic events necessarily involve such a tradeoff parameter that the user ought to experiment with; given this situation, we might prefer a transparent mechanism instead of mathematical priors (though see also §3). Figure 1 shows that hella is the highest ranked term for this spatial selection (and freqency threshold), occurring 7.8 times more frequently compared to the overall corpus; this comports with surveyed intuitions of Californian English speakers (Bucholtz et al., 2007). For</context>
</contexts>
<marker>Provost, Fawcett, 2013</marker>
<rawString>Foster Provost and Tom Fawcett. Data Science for Business. O’Reilly Media, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R</author>
</authors>
<title>A Language and Environmentfor Statistical Computing. R Foundation for Statistical Computing,</title>
<date>2013</date>
<pages>3--900051</pages>
<location>Vienna, Austria,</location>
<marker>R, 2013</marker>
<rawString>R Core Team. R: A Language and Environmentfor Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2013. URL http://www.R-project.org/.ISBN3-900051-07-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The ACL anthology network corpus.</title>
<date>2009</date>
<booktitle>In Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries,</booktitle>
<contexts>
<context position="25471" citStr="Radev et al., 2009" startWordPosition="4126" endWordPosition="4129">and are thus not totally meaningful. I’ve experimented with a very strong filtering approach to phrase selection: besides using all unigrams, take all n-grams up to length 5 that have nominal part-of-speech patterns: either the sequence consists of zero or more adjectives followed by one or more noun tokens, or all tokens were classified as names by a named entity recognition system.4 This tends to yield 4For traditional text, the tool currently uses Stanford CoreNLP; for Twitter, CMU ARK TweetNLP. p(T |w) log[p(T|w)/p(T)] �= T 7 Figure 6: MITEXTEXPLORER for paper titles in the ACL Anthology (Radev et al., 2009). Y-axis is venue (conference or journal name), X-axis is year of publication. Unlike the other figures, docvar-associated terms are sorted alphabetically. Figure 7: MITEXTEXPLORER for the King James Bible. Y-axis is book, X-axis is chapter (truncated to 39). 8 (partial) constituents, and nouns tend to be more interesting than other content words (perhaps because they are relatively less reliant on predicateargument structure to express their semantics—as opposed to adjectives or verbs, say—and a bag-ofterms analysis does not allow expression of argument structure.) However, for many corpora, </context>
<context position="31895" citStr="Radev et al. (2009)" startWordPosition="5107" endWordPosition="5110">Several major changes will be necessary for more serious use. First, essential basic capabilities must be added, such as a search box the user can use to search and filter the term list. Second, the document covariate display needs to support more than just scatterplots. When there are hundreds or more documents, summarization is necessary in the form of histograms, kernel density plots, or other tools. For example, for a large corpus of documents over time, a lineplot or temporal histogram is more appropriate, where each timestep has a document count. The ACL Anthology scatterplot (Figure 6, Radev et al. (2009)), which has hundreds of overplotted points at each (year,venue) position, makes clear the limitations of the current approach. Better visual feedback for term selections here could be useful—for example, sizing document points monotonically with the term’s frequency (rather than just presence/absence), or using stacked line plots—though certain visual depictions of frequency may be difficult given the Zipfian distribution of word frequencies. Furthermore, document structures may be thought of as document covariates. A single book has interesting internal variation that could be analyzed itsel</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. The ACL anthology network corpus. In Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret E Roberts</author>
<author>Brandon M Stewart</author>
<author>Edoardo M Airoldi</author>
</authors>
<title>Structural topic models.</title>
<date>2013</date>
<note>URL http://scholar.harvard. edu/bstewart/publications/</note>
<contexts>
<context position="7746" citStr="Roberts et al., 2013" startWordPosition="1163" endWordPosition="1166">s. A useful exploratory tool should be able to work with a variety of levels of sophistication in NLP tooling, and allow the user to fall back to manual reading when necessary. 2 MITEXTEXPLORER: linked brushing for text and covariate correlations The analysis tool presented here, MITEXTEXPLORER, is designed for exploratory analysis of relationships between document covariates—such as time, space, or author community—against textual variables—words, or other units of meaning, that can be counted per document. Unlike topic model approaches to analyzing covariate-text relationships (Mimno, 2012; Roberts et al., 2013), there is no dimension reduction of the terms. Instead, interactivity allows a user to explore more of the high-dimensional space, by specifying a document selection (Q) and/or a term selection (T). We are inspired by the linking and brushing family of techniques in interactive data visualization, in which an analyst can select a group of data points under a query in one covariate space, and see the same data selection in a different covariate space (Figure 3; see Buja et al. (1996), and e.g. Becker and Cleveland (1987); Buja et al. (1991); Martin and Ward (1995); Cook and Swayne (2007)). In </context>
</contexts>
<marker>Roberts, Stewart, Airoldi, 2013</marker>
<rawString>Margaret E. Roberts, Brandon M. Stewart, and Edoardo M. Airoldi. Structural topic models. 2013. URL http://scholar.harvard. edu/bstewart/publications/</rawString>
</citation>
<citation valid="false">
<note>structural-topic-models. Working paper.</note>
<marker></marker>
<rawString>structural-topic-models. Working paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Rockwell</author>
<author>St´efan G Sinclair</author>
<author>Stan Ruecker</author>
<author>Peter Organisciak</author>
</authors>
<title>Ubiquitous text analysis. paj:</title>
<date>2010</date>
<booktitle>The Journal of the Initiative for Digital Humanities, Media, and Culture,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="30110" citStr="Rockwell et al., 2010" startWordPosition="4836" endWordPosition="4839"> a sequential word frequencies as a tree (i.e., what computational linguists might call a trie representation of a high-order Markov model). The “God bless” word sense example from §2 indicates that such statistical summarization of local contextual information may be useful to integrate; it is worth thinking how to integrate this against the important need of document covariate analysis, while being efficient with the use of space. Many other systems, especially ones designed for literary content analysis, emphasize concordances and keyword searches within a text; for example, Voyeur/Voyant (Rockwell et al., 2010),7 which also features some document covariate analysis through temporal trend analyses for individual terms. Another class of approaches emphasizes the use of document clustering or topic models (Gardner et al., 2010; Newman et al., 2010; 6http://www.cc.gatech.edu/gvu/ii/ jigsaw/ 7http://voyant-tools.org/, http://hermeneuti.ca/voyeur 9 Grimmer and King, 2011; Chaney and Blei, 2013), while Overview8 emphasizes hierarchical document clustering paired with manual tagging. Finally, considerable research has examined exploratory visual interfaces for information retrieval, in which a user specifie</context>
</contexts>
<marker>Rockwell, Sinclair, Ruecker, Organisciak, 2010</marker>
<rawString>Geoffrey Rockwell, St´efan G Sinclair, Stan Ruecker, and Peter Organisciak. Ubiquitous text analysis. paj: The Journal of the Initiative for Digital Humanities, Media, and Culture, 2 (1), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Shaw</author>
</authors>
<title>Text-mining as a research tool,</title>
<date>2012</date>
<note>URL http://aeshin.org/ textmining/.</note>
<contexts>
<context position="4129" citStr="Shaw, 2012" startWordPosition="600" endWordPosition="601">r of graphics more extensively systematizes this approach; see also (Wickham, 2010; Bostock et al., 2011).) In the meantime, textual data has emerged as a resource of increasing interest for many scienFigure 3: Linked brushing with the analysis software GGobi. More references at source:http://www.infovis-wiki.net/index. php?title=Linking_and_Brushing tific, business, and government data analysis applications. Consider the use case of automated content analysis (a.k.a. text mining) as a tool for investigating social scientific and humanistic questions (Grimmer and Stewart, 2013; Jockers, 2013; Shaw, 2012; O’Connor et al., 2011). The content of the data is under question: analysts are interested in what/when/how/by-whom different concepts, ideas, or attitudes are expressed in a corpus, and the trends in these factors across time, space, author communities, or other documentlevel covariates (often called metadata). Comparisons of word statistics across covariates are absolutely essential to many interesting questions or social measurement problems, such as • What topics tend to get censored by the Chinese government online, and why (Bamman et al., 2012; King et al., 2013)? Covariates: whether a</context>
</contexts>
<marker>Shaw, 2012</marker>
<rawString>Ryan Shaw. Text-mining as a research tool, 2012. URL http://aeshin.org/ textmining/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditi Shrikumar</author>
</authors>
<title>Designing an Exploratory Text Analysis Tool for Humanities and Social Sciences Research.</title>
<date>2013</date>
<tech>PhD thesis,</tech>
<institution>University of California at Berkeley,</institution>
<contexts>
<context position="20051" citStr="Shrikumar, 2013" startWordPosition="3256" endWordPosition="3257"> of pinned terms (B). The set of terms here does not change as the covariate query is changed; a user can fix a set of terms and see how their PMI scores change while looking at different parts of the covariate space. One possible use of term pinning is to manually build up clusters of terms—for example, topical or synonymous term sets—whose aggregate statistical behavior (i.e. as a disjunctive query) may be interesting to observe. Manually built sets of keywords are a very useful form of text analysis; in fact, the WordSeer corpus analysis tool has explicit support to help users create them (Shrikumar, 2013). 3 Statistical term association measures There exist many measures to measure the statistical strength of an association between a term and a document covariate, or between two terms. A number of methods are based on significance testing, looking for violations of a null hypothesis that term frequencies are independent. For collocation detection, which aims to find meaningful non-compositional lexical items through frequencies of neighboring words, likelihood ratio (Dunning, 1993) and chi-square tests have been used (see review in Manning and Sch¨utze (1999)). For term-covariate associations,</context>
<context position="27695" citStr="Shrikumar, 2013" startWordPosition="4470" endWordPosition="4471">ructures extracted from more sophisticated NLP tools may indeed be better-generalized units of linguistic meaning compared to words and phrases, but they will still bear the same highdimensionality issues for data analysis purposes. 5 Related work: Exploratory text analysis Many systems and techniques have been developed for interactive text analysis. Two such systems, WordSeer and Jigsaw, have been under development for several years, each having had a series of user experiments and feedback. Recent and interesting review papers and theses are available for both of them. The WordSeer system (Shrikumar, 2013)5 contains many different interactive text visualization tools, including syntax-based search, and was initially designed for the needs of text analysis in the humanities; the WordSeer 3.0 system includes a word frequency analysis component that can compare word frequencies along document covari5http://wordseer.berkeley.edu/ ates. Interestingly, Shrikumar found in user studies with literary experts that data comparisons and annotation/note-taking support were very important capabilities to add to the system. Unique to the work in this paper is the emphasis on conditioning on document covariate</context>
</contexts>
<marker>Shrikumar, 2013</marker>
<rawString>Aditi Shrikumar. Designing an Exploratory Text Analysis Tool for Humanities and Social Sciences Research. PhD thesis, University of California at Berkeley, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanchuan Sim</author>
<author>Brice Acree</author>
<author>Justin H Gross</author>
<author>Noah A Smith</author>
</authors>
<title>Measuring ideological proportions in political speeches.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<contexts>
<context position="24440" citStr="Sim et al. (2013)" startWordPosition="3962" endWordPosition="3965">screte variable indicators of word meaning could be used for mutual information and covariate exploratory analysis—for example, a coreference system’s entity ID predictions could be browsed by the system as the term variables. (More complex concepts, of course, would also require more UI support.) At the same time, words can be too coarse compared to the longer phrases they are contained within, which often contain more interesting and distinctive concepts: for example, “death tax” and “social security” are important concepts in U.S. politics that get missed under a unigram analysis. In fact, Sim et al. (2013)’s analysis of U.S. politicians’ speeches found that domain experts had a hard time understanding unigrams out-ofcontext, but bigrams and trigrams worked much better; Gentzkow and Shapiro (2010) similarly focus on partisan political phrases. It sometimes works to simply add overlapping n-grams as more terms, but sometimes odd phrases get selected that cross constituent boundaries from their source sentences, and are thus not totally meaningful. I’ve experimented with a very strong filtering approach to phrase selection: besides using all unigrams, take all n-grams up to length 5 that have nomi</context>
</contexts>
<marker>Sim, Acree, Gross, Smith, 2013</marker>
<rawString>Yanchuan Sim, Brice Acree, Justin H Gross, and Noah A Smith. Measuring ideological proportions in political speeches. In Proceedings of EMNLP, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Taddy</author>
</authors>
<title>Multinomial inverse regression for text analysis.</title>
<date>2013</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>108</volume>
<issue>503</issue>
<contexts>
<context position="22028" citStr="Taddy, 2013" startWordPosition="3576" endWordPosition="3577">o have most of its probability mass in a small set of topics. The measure is a form of mutual information,3 and may be useful for our purposes here if the user wishes to see a report of distinctive terms for a group of several different observed covariate values at once. Blei and Lafferty (2009) ranks words per topic by a measure inspired by TFIDF, which like PMI downweights words that are generically common across all topics. Finally, hierarchical priors and regularizers can also be used; for example, by penalizing the log-odds parameterization of term probabilities (Eisenstein et al., 2011; Taddy, 2013). These methods are better in that they incorporate both protection against small count situations, while paying attention to effect size, as well as allowing overlapping covariates and regression control variables; but unfortunately, they are more computationally intensive, as opposed to the above measures which all work directly from sufficient count statistics. An association measure that fulfilled all these desiderata would be very useful. For termcovariate analysis, Monroe et al. (2008) contains a review of many different methods, from both political science as well as computer science; t</context>
</contexts>
<marker>Taddy, 2013</marker>
<rawString>Matt Taddy. Multinomial inverse regression for text analysis. Journal of the American Statistical Association, 108(503):755–770, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John W Tukey</author>
</authors>
<title>Exploratory data analysis.</title>
<date>1977</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>417424</pages>
<contexts>
<context position="1939" citStr="Tukey (1977)" startWordPosition="279" endWordPosition="280">ociations, and see how which are crucial to help ensure a dataset will be terms are used in context. The goal is to useful. Exploratory techniques can also suggest rapidly compare language usage across in- possible hypotheses or issues for further investiteresting document covariates. gation. 1 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 1–13, Baltimore, Maryland, USA, June 27, 2014. c�2014 Association for Computational Linguistics Figure 2: Anscombe Quartet. (Source: Wikipedia) The classical approach to EDA, as pioneered in works such as Tukey (1977) and Cleveland (1993) (and other work from the Bell Labs statistics group during that period) emphasizes visual analysis under nonparametric, model-free assumptions, in ������������������������������������������������� 1 which visual attributes are a fairly direct reflection of numerical or categorical aspects of data. As a simple example, consider the well-known Anscombe Quartet (1973), a set of four bivariate example datasets. The Pearson correlation, a very widely used measure of dependence that assumes a linear Gaussian model of the data, finds that each dataset has an identical amount of </context>
</contexts>
<marker>Tukey, 1977</marker>
<rawString>John W. Tukey. Exploratory data analysis. 1977. P. D Turney. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, page 417424, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>1411</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<pages>1076--9757</pages>
<marker>Turney, Pantel, 1411</marker>
<rawString>P. D Turney and P. Pantel. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37 (1):141188, 2010. ISSN 1076-9757.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelth European Conference on Machine Learning,</booktitle>
<note>URL http://nparc.cisti-icist. nrc-cnrc.gc.ca/npsi/ctrl? action=rtdoc&amp;an=5765594.</note>
<contexts>
<context position="16446" citStr="Turney (2001" startWordPosition="2623" endWordPosition="2624">eresting to perform grouping and stratified sampling based on local contextual statistics. Summarizing local context by frequencies could be done as a trie visualization (Wattenberg and Vi´egas, 2008); see §5. 2.3 Term-association queries When a term is selected, its interaction with covariates is shown by highlighting documents in (B) that contain the term. This can be thought of as another document query: instead of being specified as a region in the covariate space, is specified as a fragment of the discrete lexical space. As illustrated in much previous work (e.g. Church and Hanks (1990); Turney (2001, 2002)), word-toword PMI scores can find other terms with similar meanings, or having interesting semantic relationships, to the target term.2 This panel ranks terms u by their association with the query term v. The simplest method is to analyze the relative frequencies of terms in documents that contain v, bool-tt-epmi(u, v) = p(wi = u|v E supp(di)) p(wi = u) Here, the subscript i denotes a token position in the entire corpus, for which there is a wordtype wi and a document ID di. In this notation, the covariate PMI in 2.1 would be p(wi = u|di E Q)/p(wi = u). supp(di) denotes the set of term</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. Mining the web for synonyms: Pmi-ir versus lsa on toefl. In Proceedings of the Twelth European Conference on Machine Learning, 2001. URL http://nparc.cisti-icist. nrc-cnrc.gc.ca/npsi/ctrl? action=rtdoc&amp;an=5765594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Wattenberg</author>
<author>Fernanda B Vi´egas</author>
</authors>
<title>The word tree, an interactive visual concordance.</title>
<date>2008</date>
<journal>Visualization and Computer Graphics, IEEE Transactions on,</journal>
<volume>14</volume>
<issue>6</issue>
<marker>Wattenberg, Vi´egas, 2008</marker>
<rawString>Martin Wattenberg and Fernanda B Vi´egas. The word tree, an interactive visual concordance. Visualization and Computer Graphics, IEEE Transactions on, 14(6):1221–1228, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadley Wickham</author>
</authors>
<title>A layered grammar of graphics.</title>
<date>2010</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>10--1198</pages>
<contexts>
<context position="3601" citStr="Wickham, 2010" startWordPosition="527" endWordPosition="528">t least, it is effective under an arguably wider range of data generating assumptions), which is crucial for this example. This nonparametric, visual approach to EDA has been encoded into many data analysis packages, including the now-ubiquitous R language (R Core Team, 2013), which descends from earlier software by the Bell Labs statistics group (Becker and Chambers, 1984). In R, tools such as histograms, boxplots, barplots, dotplots, mosaicplots, etc. are built-in, basic operators in the language. (Wilkinson (2006)’s grammar of graphics more extensively systematizes this approach; see also (Wickham, 2010; Bostock et al., 2011).) In the meantime, textual data has emerged as a resource of increasing interest for many scienFigure 3: Linked brushing with the analysis software GGobi. More references at source:http://www.infovis-wiki.net/index. php?title=Linking_and_Brushing tific, business, and government data analysis applications. Consider the use case of automated content analysis (a.k.a. text mining) as a tool for investigating social scientific and humanistic questions (Grimmer and Stewart, 2013; Jockers, 2013; Shaw, 2012; O’Connor et al., 2011). The content of the data is under question: ana</context>
</contexts>
<marker>Wickham, 2010</marker>
<rawString>Hadley Wickham. A layered grammar of graphics. Journal of Computational and Graphical Statistics, 19(1):328, 2010. doi: 10.1198/jcgs. 2009.07098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leland Wilkinson</author>
</authors>
<title>The grammar of graphics.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<contexts>
<context position="3510" citStr="Wilkinson (2006)" startWordPosition="515" endWordPosition="516">he correlation coefficient is a model-based analysis tool, the scatterplot is model-free (or at least, it is effective under an arguably wider range of data generating assumptions), which is crucial for this example. This nonparametric, visual approach to EDA has been encoded into many data analysis packages, including the now-ubiquitous R language (R Core Team, 2013), which descends from earlier software by the Bell Labs statistics group (Becker and Chambers, 1984). In R, tools such as histograms, boxplots, barplots, dotplots, mosaicplots, etc. are built-in, basic operators in the language. (Wilkinson (2006)’s grammar of graphics more extensively systematizes this approach; see also (Wickham, 2010; Bostock et al., 2011).) In the meantime, textual data has emerged as a resource of increasing interest for many scienFigure 3: Linked brushing with the analysis software GGobi. More references at source:http://www.infovis-wiki.net/index. php?title=Linking_and_Brushing tific, business, and government data analysis applications. Consider the use case of automated content analysis (a.k.a. text mining) as a tool for investigating social scientific and humanistic questions (Grimmer and Stewart, 2013; Jocker</context>
</contexts>
<marker>Wilkinson, 2006</marker>
<rawString>Leland Wilkinson. The grammar of graphics. Springer, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.9973715">
The CMU Submission for the Shared Task on Language Identification in
Code-Switched Data
</title>
<author confidence="0.98734">
Chu-Cheng Lin Waleed Ammar Lori Levin Chris Dyer
</author>
<affiliation confidence="0.983173">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.532023">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.999061">
{chuchenl,wammar,lsl,cdyer}@cs.cmu.edu
</email>
<sectionHeader confidence="0.994802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.978907727272727">
We describe the CMU submission for
the 2014 shared task on language iden-
tification in code-switched data. We
participated in all four language pairs:
Spanish–English, Mandarin–English,
Nepali–English, and Modern Standard
Arabic–Arabic dialects. After describing
our CRF-based baseline system, we
discuss three extensions for learning from
unlabeled data: semi-supervised learning,
word embeddings, and word lists.
</bodyText>
<sectionHeader confidence="0.998344" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975065573771">
Code switching (CS) occurs when a multilingual
speaker uses more than one language in the same
conversation or discourse. Automatic idenefica-
tion of the points at which code switching occurs
is important for two reasons: (1) to help sociolin-
guists analyze the frequency, circumstances and
motivations related to code switching (Gumperz,
1982), and (2) to automatically determine which
language-specific NLP models to use for analyz-
ing segments of text or speech.
CS is pervasive in social media due to its in-
formal nature (Lui and Baldwin, 2014). The first
workshop on computational approaches to code
switching in EMNLP 2014 organized a shared task
(Solorio et al., 2014) on identifying code switch-
ing, providing training data of multilingual tweets
with token-level language-ID annotations. See
§2 for a detailed description of the shared task.
This short paper documents our submission in the
shared task.
We note that constructing a CS data set that is
annotated at the token level requires remarkable
manual effort. However, collecting raw tweets is
easy and fast. We propose leveraging both labeled
and unlabeled data in a unified framework; condi-
tional random field autoencoders (Ammar et al.,
2014). The CRF autoencoder framework consists
of an encoding model and a reconstruction model.
The encoding model is a linear-chain conditional
random field (CRF) (Lafferty et al., 2001) which
generates a sequence of labels, conditional on a
token sequence. Importantly, the parameters of
the encoding model can be interpreted in the same
way a CRF model would. This is in contrary to
generative model parameters which explain both
the observation sequence and the label sequence.
The reconstruction model, on the other hand, inde-
pendently generates the tokens conditional on the
corresponding labels. Both labeled and unlabeled
data can be efficiently used to fit parameters of this
model, minimizing regularized log loss. See §4.1
for more details.
After modeling unlabeled token sequences, we
explore two other ways of leveraging unlabeled
data: word embeddings and word lists. The word
embeddings we use capture monolingual distribu-
tional similarities and therefore may be indicative
of a language (see §4.2). A word list, on the other
hand, is a collection of words which have been
manually or automatically constructed and share
some property (see §4.3). For example, we extract
the set of surface forms in monolingual corpora.
In §5, we describe the experiments and discuss
results. According to the results, modeling unla-
beled data using CRF autoencoders did not im-
prove prediction accuracy. Nevertheless, more ex-
periments need to be run before we can conclude
this setting. On the positive side, word embed-
dings and word lists have been shown to improve
CS prediction accuracy, provided they have decent
coverage of tokens in the test set.
</bodyText>
<sectionHeader confidence="0.975317" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.94301225">
The shared task training data consists of code–
switched tweets with token-level annotations.
The data is organized in four language pairs:
English–Spanish (En-Es), English–Nepali (En-
</bodyText>
<page confidence="0.966294">
80
</page>
<bodyText confidence="0.947569">
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 80–86,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
Ne), Mandarin–English (Zh-En) and Modern
Standard Arabic–Arabic dialects (MSA-ARZ).
Table 1 shows the size of the data sets provided
for the shared task in each language pair.
For each tweet in the data set, the user ID, tweet
ID, and a list of tokens’ start offset and end offset
are provided. Each token is annotated with one
of the following labels: lang1, lang2, ne (i.e.,
named entities), mixed (i.e., mixed parts of lang1
and lang2), ambiguous (i.e., cannot be identified
given context), and other.
Two test sets were used to evaluate each sub-
mission for the shared task in each language pair.
The first test set consists of Tweets, similar to the
training set. The second test set consists of token
sequences from a surprise genre. Since partici-
pants were not given the test sets, we only report
results on a Twitter test set (a subset of the data
provided for shared task participants). Statistics
of our train/test data splits are given in Table 5.
</bodyText>
<table confidence="0.992604615384616">
lang. pair split tweets tokens users
En–Ne all 9,993 146,053 18
train 7,504 109,040 12
test 2,489 37,013 6
En–Es all 11,400 140,738 9
train 7,399 101,451 6
test 4,001 39,287 3
Zh–En all 994 17,408 995
train 662 11,677 663
test 332 5,731 332
MSA–ARZ all 5,862 119,775 7
train 4,800 95,352 6
test 1,062 24,423 1
</table>
<tableCaption confidence="0.997334">
Table 1: Total number of tweets, tokens, and Twit-
</tableCaption>
<bodyText confidence="0.9978791">
ter user IDs for each language pair. For each lan-
guage pair, the first line represents all data pro-
vided to shared task participants. The second and
third lines represent our train/test data split for the
experiments reported in this paper. Since Twit-
ter users are allowed to delete their tweets, the
number of tweets and tokens reported in the third
and fourth columns may be less than the number
of tweets and tokens originally annotated by the
shared task organizers.
</bodyText>
<sectionHeader confidence="0.982519" genericHeader="method">
3 Baseline System
</sectionHeader>
<bodyText confidence="0.9945755">
We model token-level language ID as a sequence
of labels using a linear-chain conditional ran-
dom field (CRF) (Lafferty et al., 2001) described
in §3.1 with the features in §3.2.
</bodyText>
<subsectionHeader confidence="0.998625">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.999953333333333">
A linear-chain CRF models the conditional proba-
bility of a label sequence y given a token sequence
x and given extra context φ, as follows:
</bodyText>
<equation confidence="0.997059666666667">
p(y  |x, φ) = exp λT EIX|1 f(x, yz, yz−1, φ)
Ey, exp λT E|x|
z=1 f(x, yz, y�z−1, φ)
</equation>
<bodyText confidence="0.999943166666667">
where λ is a vector of feature weights, and f is
a vector of local feature functions. We use φ to
explicitly represent context information necessary
to compute the feature functions described below.
In a linear-chain structure, yz only depends on
observed variables x, φ and the neighboring labels
yz−1 and yz+1. Therefore, we can use dynamic
programming to do inference in run time that is
quadratic in the number of unique labels and lin-
ear in the sequence length. We use L-BFGS to
learn the feature weights λ, maximizing the L2-
regularized log-likelihood of labeled examples Z:
</bodyText>
<equation confidence="0.999168666666667">
``supervised(λ) =
cL2||λ||22 + � log p(y  |x, φ)
(x,y)E,C
</equation>
<bodyText confidence="0.999996333333333">
After training the model, we use again use dy-
namic programming to find the most likely label
sequence, for each token sequence in the test set.
</bodyText>
<subsectionHeader confidence="0.99121">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.995379">
We use the following features in the baseline sys-
tem:
</bodyText>
<listItem confidence="0.9953722">
• character n-grams (loweredcased tri- and quad-
grams)
• prefixes and suffixes of lengths 1, 2, 3 and 4
• unicode page of the first character1
• case (first-character-uppercased vs. all-
characters-uppercased vs. all-characters-
alphanumeric)
• tweet-level language ID predictions from two
off-the-shelf language identifiers: cld22 and
ldig3
</listItem>
<footnote confidence="0.999967333333333">
1http://www.unicode.org/charts/
2https://code.google.com/p/cld2/
3https://github.com/shuyo/ldig
</footnote>
<page confidence="0.998335">
81
</page>
<figureCaption confidence="0.999727">
Figure 1: A diagram of the CRF autoencoder
</figureCaption>
<sectionHeader confidence="0.790164" genericHeader="method">
4 Using Unlabeled Data
</sectionHeader>
<bodyText confidence="0.999931125">
In §3, we learn the parameters of the CRF model
parameters in a standard fully supervised fashion,
using labeled examples in the training set. Here,
we attempt to use unlabeled examples to improve
our system’s performance in three ways: model-
ing unlabeled token sequences in the CRF autoen-
coder framework, word embeddings, and word
lists.
</bodyText>
<subsectionHeader confidence="0.99257">
4.1 CRF Autoencoders
</subsectionHeader>
<bodyText confidence="0.999895176470588">
A CRF autoencoder (Ammar et al., 2014) consists
of an input layer, an output layer, and a hidden
layer. Both input and output layer represent the
observed token sequence. The hidden layer rep-
resents the label sequence. Fig. 1 illustrates the
model dependencies for sequence labeling prob-
lems with a first-order Markov assumption. Con-
ditional on an observation sequence x and side in-
formation φ, a traditional linear-chain CRF model
is used to generate the label sequence y. The
model then generates xˆ which represents a recon-
struction of the original observation sequence. El-
ements of this reconstruction (i.e., ˆxi) are then in-
dependently generated conditional on the corre-
sponding label yi using simple categorical distri-
butions.
The parametric form of the model is given by:
</bodyText>
<equation confidence="0.932513">
p(y, xˆ  |x, φ) =
T Ix|
expλEz=1 f(x, yi−1, yi, i, φ)
�
</equation>
<bodyText confidence="0.977869294117647">
Py, expλT Pi|x=|1 f (x, yi−1�, yi i, φ)
where λ is a vector of CRF feature weights, f is a
vector of local feature functions (we use the same
features described in §3.2), and θˆxi|yi are categor-
ical distribution parameters of the reconstruction
model representing p(ˆxi  |yi).
We can think of a label sequence as a low-
cardinality lossy compression of the correspond-
ing token sequence. CRF autoencoders explic-
itly model this intuition by creating an information
bottleneck where label sequences are required to
regenerate the same token sequence despite their
limited capacity. Therefore, when only unlabeled
examples U are available, we train CRF autoen-
coders by maximizing the regularized likelihood
of generating reconstructions ˆx, conditional on x,
marginalizing values of label sequences y:
</bodyText>
<equation confidence="0.9938695">
``unsupervised(λ, θ) = cL2 ||λ|2
|2 + RDirichlet(θ, α)+
X log X p(y, xˆ  |x)
(x,ˆx)EU y:|y|=|x|
</equation>
<bodyText confidence="0.999977285714286">
where RDirichlet is a regularizer based on a vari-
ational approximation of a symmetric Dirichlet
prior with concentration parameter α for the re-
construction parameters θ.
Having access to labeled examples, it is easy to
modify this objective to learn from both labeled
and unlabeled examples as follows:
</bodyText>
<equation confidence="0.9949482">
``semi(λ, θ) = cL2||λ||22 + RDirichlet(θ, α)+
Xcunlabeled X log X p(y, xˆ  |x)+
(x,ˆx)EU y:|y|=|x|
Xclabeled X log p(y  |x)
(x,y)EL
</equation>
<bodyText confidence="0.999844285714286">
We use block coordinate descent to optimize
this objective. First, we use cem iterations of
the expectation maximization algorithm to opti-
mize the θ-block while the λ-block is fixed, then
we optimize the λ-block with clbfgs iterations of
L-BFGS (Liu et al., 1989) while the θ-block is
fixed.4
</bodyText>
<subsectionHeader confidence="0.980405">
4.2 Unsupervised Word Embeddings
</subsectionHeader>
<bodyText confidence="0.875541">
For many NLP tasks, using unsupervised
word representations as features improves
accuracy (Turian et al., 2010). We use
word2vec (Mikolov et al., 2013) to train
100–dimensional word embeddings from a
large Twitter corpus of about 20 million tweets
extracted from the live stream, in multiple lan-
guages. We define an additional feature function
4An open source efficient c++ imple-
mentation of our method can be found at
</bodyText>
<figure confidence="0.753560181818182">
https://github.com/ldmt-muri/alignment-with-openfst
encoding
reconstruction
X
Φ
yi-1 yi yi+1
ΛΛΛ
xi-1 xi xi+1
Y |x|
i=1
θˆxi|yiX
</figure>
<page confidence="0.981857">
82
</page>
<bodyText confidence="0.959301444444444">
in the CRF autoencoder model §4.1 for each of
the 100 dimensions, conjoined with the label yi.
The feature value is the corresponding dimension
for xi. A binary feature indicating the absence of
word embeddings is fired for out-of-vocabulary
words (i.e., words for which we do not have word
embeddings). The token-level coverage of the
word embeddings for each of the languages or
dialects used in the training data is reported in
</bodyText>
<tableCaption confidence="0.768119">
Table 2.
</tableCaption>
<subsectionHeader confidence="0.992853">
4.3 Word List Features
</subsectionHeader>
<bodyText confidence="0.999661466666667">
While some words are ambiguous, many words
frequently occur in only one of the two lan-
guages being considered. An easy way to iden-
tify the label of such unambiguous words is to
check whether they belong to the vocabulary of
either language. Moreover, named entity recog-
nizers typically rely on gazetteers of named enti-
ties to improve their performance. We generalize
the notion of using monolingual vocabularies and
gazetteers of named entities to general word lists.
Using K word lists {l1, ... , lK}, when a token xi
is labeled with yi, we fire a binary feature that con-
joins (yi, δ(xi E l1),... , δ(xi E lK)), where δ is
an indicator boolean function. We use the follow-
ing word lists:
</bodyText>
<listItem confidence="0.996434125">
• Hindi and Nepali Wikipedia article titles
• multilingual named entities from the JRC
dataset5 and CoNLL 2003 shared task
• word types in monolingual corpora in MSA,
ARZ, En and Es.
• set difference between the following pairs of
word lists: MSA-ARZ, ARZ-MSA, En-Es, Es-
En.
</listItem>
<bodyText confidence="0.9984163">
Transliteration from Devanagari The Nepali–
English tweets in the dataset are romanized. This
renders our Nepali word lists, which are based
on the Devanagari script, useless. Therefore, we
transliterate the Hindi and Nepali named entities
lists using a deterministic phonetic mapping. We
romanize the Devanagari words using the IAST
scheme.6 We then drop all accent marks on the
characters to make them fit into the 7–bit ASCII
range.
</bodyText>
<footnote confidence="0.89728975">
5http://datahub.io/dataset/jrc-names
6http://en.wikipedia.org/wiki/
International—Alphabet—of—Sanskrit—
Transliteration
</footnote>
<table confidence="0.999912125">
language embeddings word lists
coverage coverage
ARZ 30.7% 68.8%
En 73.5% 55.7%
MSA 26.6% 76.8%
Ne 14.5% 77.0%
Es 62.9% 78.0%
Zh 16.0% 0.7%
</table>
<tableCaption confidence="0.994789">
Table 2: The type-level coverage of annotated data
</tableCaption>
<bodyText confidence="0.747584333333333">
according to word embeddings (second column)
and according to word lists (third column), per lan-
guage.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999879">
We compare the performance of five models for
each language pair, which correspond to the five
lines in Table 3. The first model, “CRF” is the
baseline model described in §3. The second “CRF
+ Utest” and the third “CRF + Uall” are CRF au-
toencoder models (see §4.1) with two sets of un-
labeled data: (1) Utest which only includes the test
set,7 and (2) Uall which includes the test set as well
as all tweets by the set of users who contributed
any tweets in L. The fourth model “CRF + Uall +
emb.” is a CRF autoencoder which uses word em-
bedding features (see §4.2), as well as the features
described in §3.2. Finally, the fifth model “CRF +
Uall + emb. + lists” further adds word list features
(see §4.3). In all but the “CRF” model, we adopt a
transductive learning setup.
Since the CRF baseline is used as the encoding
part of the CRF autoencoder model, we use the
supervisedly-trained CRF parameters to initialize
the CRF autoencoder models. The categorical dis-
tributions of the reconstruction model are initial-
ized with discrete uniforms. We set the weight
of the labeled data log-likelihood clabeled = 0.5,
the weight of the unlabeled data log-likelihood
cunlabeled = 0.5, the L2 regularization strength
cL2 = 0.3, the concentration parameter of the
Dirichlet prior α = 0.1, the number of L-BFGS
iterations cLBFGS = 4, and the number of EM iter-
ations cEM = 4.8 We stop training after 50 itera-
tions of block coordinate descent.
</bodyText>
<footnote confidence="0.960427714285714">
7Utest is potentially useful when the test set belongs to a
different domain than the labeled examples, which is often
referred to as “domain adaptation”. However we were unable
to test this hypothesis since all the CS annotations we had
access to are from Twitter.
8Hyper-parameters cL, and α were tuned using cross-
validation. The remaining hyper-parameters were not tuned.
</footnote>
<page confidence="0.995478">
83
</page>
<table confidence="0.999910333333333">
config En–Ne MSA–ARZ En–Es Zh–En
CRF 95.2% 80.5% 94.6% 94.9%
+Ttest 95.2% 80.6% 94.6% 94.9%
+Tall 95.2% 80.7% 94.6% 94.9%
+emb. 95.3% 81.3% 95.1% 95.0%
+lists 97.0% 81.2% 96.7% 95.3%
</table>
<tableCaption confidence="0.9482485">
Table 3: Token level accuracy results for each of
the four language pairs.
</tableCaption>
<table confidence="0.597264">
label predicted predicted
MSA ARZ
93.9% 5.3%
32.1% 65.2%
</table>
<tableCaption confidence="0.933437">
Table 4: Confusion between MSA and ARZ in the
Baseline configuration.
</tableCaption>
<bodyText confidence="0.997997393939394">
Results. The CRF baseline results are reported
in the first line in Table 3. For three language
pairs, the overall token-level accuracy ranges be-
tween 94.6% and 95.2%. In the fourth language
pair, MSA-ARZ, the baseline accuracy is 80.5%
which indicates the relative difficulty of this task.
The second and third lines in Table 3 show the
results when we use CRF autoencoders with the
unlabeled test set (Utest), and with all unlabeled
tweets (Uall), respectively. While semi-supervised
learning did not hurt accuracy on any of the lan-
guages, it only resulted in a tiny increase in accu-
racy for the Arabic dialects task.
The fourth line in Table 3 extends the CRF au-
toencoder model (third line) by adding unsuper-
vised word embedding features. This results in
an improvement of 0.6% for MSA-ARZ, 0.5% for
En-Es, 0.1% for En-Ne and Zh-En.
The fifth line builds on the fourth line by adding
word list features. This results in an improvement
of 1.7% in En-Ne, 1.6% in En-Es, 0.4% in Zh-En,
and degradation of 0.1% in MSA-ARZ.
Analysis and Discussion The baseline perfor-
mance in the MSA-ARZ task is considerably
lower than those of the other tasks. Table 4 illus-
trates how the baseline model confuses lang1 and
lang2 in the MSA-ARZ task. While the baseline
system correctly labels 93.9% of MSA tokens, it
only correctly labels 65.2% of ARZ tokens.
Although the reported semi-supervised results
did not significantly improve on the CRF baseline,
more work needs to be done in order to conclude
these results:
</bodyText>
<table confidence="0.996175">
lang. pair |Utest ||Uall ||L|
En–Ne 2489 6230 7504
MSA–ARZ 1062 2520 4800
Zh–En 332 332 663
En–Es 4001 7177 7399
</table>
<tableCaption confidence="0.710182">
Table 5: Number of tweets in L, Utest and Uall used
for semi-supervised learning of CRF autoencoders
models.
</tableCaption>
<listItem confidence="0.985575363636364">
• Use an out-of-domain test set where some adap-
tation to the test set is more promising.
• Vary the number of labeled examples |L |and
the number of unlabeled examples |U|. Table 5
gives the number of labeled and unlabeled ex-
amples used for training the model. It is pos-
sible that semi-supervised learning would have
been more useful with a smaller |L |and a larger
|U|.
• Tune clabeled and cunlabeled.
• Split the parameters A into two subsets: Alabeled
and Aunlabeled; where Alabeled are the parameters
which have a non-zero value for any input x in
L and Aunlabeled are the remaining parameters in
A which only have non-zero values with unla-
beled examples but not with the labeled exam-
ples.
• Use a richer reconstruction model.
• Reconstruct a transformation of the token se-
quences instead of their surface forms.
• Train a token-level language ID model trained
on a large number of languages, as opposed to
</listItem>
<bodyText confidence="0.997776941176471">
disambiguating only two languages at a time.
Word embeddings improve the results for all
language pairs, but the largest improvement is in
MSA-ARZ and En-Es. Looking into the word em-
beddings coverage of those languages (i.e., MSA,
ARZ, Es, En in Table 2), we find that they are bet-
ter covered than the other languages (Ne, Zh). We
conclude that further improvements on En-Ne and
Zh-En may be expected if they are better repre-
sented in the corpus used to learn word embed-
dings.
As for the word lists, the largest improvement
we get is the romanized word lists of Nepali,
which have a 77.0% coverage and improve the
accuracy by 1.7%. This shows that our translit-
erated word lists not only cover a lot of tokens,
and are also useful for language ID. The Spanish
</bodyText>
<figure confidence="0.4666405">
true MSA
true ARZ
</figure>
<page confidence="0.906501">
84
</page>
<table confidence="0.998146">
Config lang1 lang2 ne
+lists 84.1% 76.5% 73.7%
-lists 84.2% 77.1% 71.5%
</table>
<tableCaption confidence="0.925743">
Table 6: F–Measures of two Arabic configura-
</tableCaption>
<bodyText confidence="0.93446625">
tions. lang1 is MSA. lang2 is ARZ.
word lists also have a wide coverage, improving
the overall accuracy by 1.6%. The overall accu-
racy of the Arabic dialects slightly degrades with
the addition of the word lists. Closer inspection
in table 6 reveals that it improves the F–Measure
of the named entities at the expense of both MSA
(lang1) and ARZ (lang2).
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999760944444444">
Previous work on identifying languages in a mul-
tilingual document includes (Singh and Gorla,
2007; King and Abney, 2013; Lui et al., 2014).
Their goal is generally more about identifying the
languages that appear in the document than intra–
sentential CS points.
Previous work on computational models of
code–switching include formalism (Joshi, 1982)
and language models that encode syntactic con-
straints from theories of code–switching, such as
(Li and Fung, 2013; Li and Fung, 2014). These
require the existence of a parser for the languages
under consideration. Other work on prediction
of code–switching points, such as (Elfardy et al.,
2013; Nguyen and Dogruoz, 2013) and ours, do
not depend upon such NLP infrastructure. Both of
the aforementioned use basic character–level fea-
tures and dictionaries on sequence models.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999979611111111">
We have shown that a simple CRF baseline with
a handful of feature templates obtains strong re-
sults for this task. We discussed three methods
to improve over the supervised baseline using un-
labeled data: (1) modeling unlabeleld data using
CRF autoencoders, (2) using pre-trained word em-
beddings, and (3) using word list features.
We show that adding word embedding features
and word lists features is useful when they have
good coverage of words in a data set. While mod-
est improvements are observed due to modeling
unlabeled data with CRF autoenocders, we iden-
tified possible directions to gain further improve-
ments.
While bilingual disambiguation was a good first
step for identifying code switching, we suggest a
reformulation of the task such that each label can
take on one of many languages.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999989555555556">
We thank Brendan O’Connor who helped assem-
ble the Twitter dataset. We also thank the work-
shop organizers for their hard work, and the re-
viewers for their comments. This work was
sponsored by the U.S. Army Research Labora-
tory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533. The
statements made herein are solely the responsibil-
ity of the authors.
</bodyText>
<sectionHeader confidence="0.99832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99942172972973">
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014.
Conditional random field autoencoders for unsuper-
vised structured prediction. In Proc. of NIPS.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412–416. Springer.
John J. Gumperz. 1982. Discourse Strategies. Studies
in Interactional Sociolinguistics. Cambridge Univer-
sity Press.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of
the 9th Conference on Computational Linguistics -
Volume 1, COLING ’82, pages 145–150, Czechoslo-
vakia. Academia Praha.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110–1119. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML.
Ying Li and Pascale Fung. 2013. Improved mixed lan-
guage speech recognition using asymmetric acous-
tic model and language model with code-switch in-
version constraints. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2013 IEEE International
Conference on, pages 7368–7372, May.
Ying Li and Pascale Fung. 2014. Code switch lan-
guage modeling with functional head constraint. In
Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pages
4913–4917, May.
</reference>
<page confidence="0.993124">
85
</page>
<reference confidence="0.99983069047619">
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17–25, Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.
Marco Lui, Han Jey Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation of Computational Linguistics, 2:27–40.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Dong Nguyen and Seza A. Dogruoz. 2013. Word level
language identification in online multilingual com-
munication. Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 857–862. Association for Computational Lin-
guistics.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identi-
fication of languages and encodings in a multilingual
document. In Building and Exploring Web Corpora
(WAC3-2007): Proceedings of the 3rd Web as Cor-
pus Workshop, Incorporating Cleaneval, volume 4,
page 95.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998559">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486327">
<title confidence="0.9912525">The CMU Submission for the Shared Task on Language Identification in Code-Switched Data</title>
<author confidence="0.983557">Chu-Cheng Lin Waleed Ammar Lori Levin Chris Dyer</author>
<affiliation confidence="0.932717">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.998543">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.903176">We describe the CMU submission 2014 shared task on language identification in code-switched data. participated in all four language pairs: Spanish–English, Nepali–English, and Modern Standard Arabic–Arabic dialects. After describing our CRF-based baseline system, we discuss three extensions for learning from unlabeled data: semi-supervised learning, word embeddings, and word lists.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Waleed Ammar</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Conditional random field autoencoders for unsupervised structured prediction.</title>
<date>2014</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="1906" citStr="Ammar et al., 2014" startWordPosition="275" endWordPosition="278">oaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framework; conditional random field autoencoders (Ammar et al., 2014). The CRF autoencoder framework consists of an encoding model and a reconstruction model. The encoding model is a linear-chain conditional random field (CRF) (Lafferty et al., 2001) which generates a sequence of labels, conditional on a token sequence. Importantly, the parameters of the encoding model can be interpreted in the same way a CRF model would. This is in contrary to generative model parameters which explain both the observation sequence and the label sequence. The reconstruction model, on the other hand, independently generates the tokens conditional on the corresponding labels. Bot</context>
<context position="7896" citStr="Ammar et al., 2014" startWordPosition="1255" endWordPosition="1258">two off-the-shelf language identifiers: cld22 and ldig3 1http://www.unicode.org/charts/ 2https://code.google.com/p/cld2/ 3https://github.com/shuyo/ldig 81 Figure 1: A diagram of the CRF autoencoder 4 Using Unlabeled Data In §3, we learn the parameters of the CRF model parameters in a standard fully supervised fashion, using labeled examples in the training set. Here, we attempt to use unlabeled examples to improve our system’s performance in three ways: modeling unlabeled token sequences in the CRF autoencoder framework, word embeddings, and word lists. 4.1 CRF Autoencoders A CRF autoencoder (Ammar et al., 2014) consists of an input layer, an output layer, and a hidden layer. Both input and output layer represent the observed token sequence. The hidden layer represents the label sequence. Fig. 1 illustrates the model dependencies for sequence labeling problems with a first-order Markov assumption. Conditional on an observation sequence x and side information φ, a traditional linear-chain CRF model is used to generate the label sequence y. The model then generates xˆ which represents a reconstruction of the original observation sequence. Elements of this reconstruction (i.e., ˆxi) are then independent</context>
</contexts>
<marker>Ammar, Dyer, Smith, 2014</marker>
<rawString>Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014. Conditional random field autoencoders for unsupervised structured prediction. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mohamed Al-Badrashiny</author>
<author>Mona Diab</author>
</authors>
<title>Code switch point detection in arabic.</title>
<date>2013</date>
<booktitle>In Natural Language Processing and Information Systems,</booktitle>
<pages>412--416</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="19919" citStr="Elfardy et al., 2013" startWordPosition="3265" endWordPosition="3268">ing languages in a multilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend upon such NLP infrastructure. Both of the aforementioned use basic character–level features and dictionaries on sequence models. 7 Conclusion We have shown that a simple CRF baseline with a handful of feature templates obtains strong results for this task. We discussed three methods to improve over the supervised baseline using unlabeled data: (1) modeling unlabeleld data using CRF autoencoders, (2) using pre-trained word embeddings, and (3) using word list features. We show that adding word embedding features and word lists features is usefu</context>
</contexts>
<marker>Elfardy, Al-Badrashiny, Diab, 2013</marker>
<rawString>Heba Elfardy, Mohamed Al-Badrashiny, and Mona Diab. 2013. Code switch point detection in arabic. In Natural Language Processing and Information Systems, pages 412–416. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Gumperz</author>
</authors>
<title>Discourse Strategies. Studies in Interactional Sociolinguistics.</title>
<date>1982</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1042" citStr="Gumperz, 1982" startWordPosition="139" endWordPosition="140">–English, Mandarin–English, Nepali–English, and Modern Standard Arabic–Arabic dialects. After describing our CRF-based baseline system, we discuss three extensions for learning from unlabeled data: semi-supervised learning, word embeddings, and word lists. 1 Introduction Code switching (CS) occurs when a multilingual speaker uses more than one language in the same conversation or discourse. Automatic idenefication of the points at which code switching occurs is important for two reasons: (1) to help sociolinguists analyze the frequency, circumstances and motivations related to code switching (Gumperz, 1982), and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech. CS is pervasive in social media due to its informal nature (Lui and Baldwin, 2014). The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS da</context>
</contexts>
<marker>Gumperz, 1982</marker>
<rawString>John J. Gumperz. 1982. Discourse Strategies. Studies in Interactional Sociolinguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Processing of sentences with intra-sentential code-switching.</title>
<date>1982</date>
<booktitle>In Proceedings of the 9th Conference on Computational Linguistics -Volume 1, COLING ’82,</booktitle>
<pages>145--150</pages>
<location>Czechoslovakia. Academia Praha.</location>
<contexts>
<context position="19624" citStr="Joshi, 1982" startWordPosition="3220" endWordPosition="3221">1.6%. The overall accuracy of the Arabic dialects slightly degrades with the addition of the word lists. Closer inspection in table 6 reveals that it improves the F–Measure of the named entities at the expense of both MSA (lang1) and ARZ (lang2). 6 Related Work Previous work on identifying languages in a multilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend upon such NLP infrastructure. Both of the aforementioned use basic character–level features and dictionaries on sequence models. 7 Conclusion We have shown that a simple CRF baseline with a handful of feature templates obtains strong results for this ta</context>
</contexts>
<marker>Joshi, 1982</marker>
<rawString>Aravind K. Joshi. 1982. Processing of sentences with intra-sentential code-switching. In Proceedings of the 9th Conference on Computational Linguistics -Volume 1, COLING ’82, pages 145–150, Czechoslovakia. Academia Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben King</author>
<author>Steven Abney</author>
</authors>
<title>Labeling the languages of words in mixed-language documents using weakly supervised methods.</title>
<date>2013</date>
<booktitle>Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1110--1119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19393" citStr="King and Abney, 2013" startWordPosition="3184" endWordPosition="3187"> MSA true ARZ 84 Config lang1 lang2 ne +lists 84.1% 76.5% 73.7% -lists 84.2% 77.1% 71.5% Table 6: F–Measures of two Arabic configurations. lang1 is MSA. lang2 is ARZ. word lists also have a wide coverage, improving the overall accuracy by 1.6%. The overall accuracy of the Arabic dialects slightly degrades with the addition of the word lists. Closer inspection in table 6 reveals that it improves the F–Measure of the named entities at the expense of both MSA (lang1) and ARZ (lang2). 6 Related Work Previous work on identifying languages in a multilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend upon such NLP infrastr</context>
</contexts>
<marker>King, Abney, 2013</marker>
<rawString>Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly supervised methods. Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1110–1119. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="2087" citStr="Lafferty et al., 2001" startWordPosition="302" endWordPosition="305">el language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framework; conditional random field autoencoders (Ammar et al., 2014). The CRF autoencoder framework consists of an encoding model and a reconstruction model. The encoding model is a linear-chain conditional random field (CRF) (Lafferty et al., 2001) which generates a sequence of labels, conditional on a token sequence. Importantly, the parameters of the encoding model can be interpreted in the same way a CRF model would. This is in contrary to generative model parameters which explain both the observation sequence and the label sequence. The reconstruction model, on the other hand, independently generates the tokens conditional on the corresponding labels. Both labeled and unlabeled data can be efficiently used to fit parameters of this model, minimizing regularized log loss. See §4.1 for more details. After modeling unlabeled token sequ</context>
<context position="5867" citStr="Lafferty et al., 2001" startWordPosition="925" endWordPosition="928">tter user IDs for each language pair. For each language pair, the first line represents all data provided to shared task participants. The second and third lines represent our train/test data split for the experiments reported in this paper. Since Twitter users are allowed to delete their tweets, the number of tweets and tokens reported in the third and fourth columns may be less than the number of tweets and tokens originally annotated by the shared task organizers. 3 Baseline System We model token-level language ID as a sequence of labels using a linear-chain conditional random field (CRF) (Lafferty et al., 2001) described in §3.1 with the features in §3.2. 3.1 Model A linear-chain CRF models the conditional probability of a label sequence y given a token sequence x and given extra context φ, as follows: p(y |x, φ) = exp λT EIX|1 f(x, yz, yz−1, φ) Ey, exp λT E|x| z=1 f(x, yz, y�z−1, φ) where λ is a vector of feature weights, and f is a vector of local feature functions. We use φ to explicitly represent context information necessary to compute the feature functions described below. In a linear-chain structure, yz only depends on observed variables x, φ and the neighboring labels yz−1 and yz+1. Therefor</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Li</author>
<author>Pascale Fung</author>
</authors>
<title>Improved mixed language speech recognition using asymmetric acoustic model and language model with code-switch inversion constraints.</title>
<date>2013</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,</booktitle>
<pages>7368--7372</pages>
<contexts>
<context position="19738" citStr="Li and Fung, 2013" startWordPosition="3236" endWordPosition="3239">er inspection in table 6 reveals that it improves the F–Measure of the named entities at the expense of both MSA (lang1) and ARZ (lang2). 6 Related Work Previous work on identifying languages in a multilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend upon such NLP infrastructure. Both of the aforementioned use basic character–level features and dictionaries on sequence models. 7 Conclusion We have shown that a simple CRF baseline with a handful of feature templates obtains strong results for this task. We discussed three methods to improve over the supervised baseline using unlabeled data: (1) modeling unlabele</context>
</contexts>
<marker>Li, Fung, 2013</marker>
<rawString>Ying Li and Pascale Fung. 2013. Improved mixed language speech recognition using asymmetric acoustic model and language model with code-switch inversion constraints. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 7368–7372, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Li</author>
<author>Pascale Fung</author>
</authors>
<title>Code switch language modeling with functional head constraint.</title>
<date>2014</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on,</booktitle>
<pages>4913--4917</pages>
<contexts>
<context position="19758" citStr="Li and Fung, 2014" startWordPosition="3240" endWordPosition="3243">ble 6 reveals that it improves the F–Measure of the named entities at the expense of both MSA (lang1) and ARZ (lang2). 6 Related Work Previous work on identifying languages in a multilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend upon such NLP infrastructure. Both of the aforementioned use basic character–level features and dictionaries on sequence models. 7 Conclusion We have shown that a simple CRF baseline with a handful of feature templates obtains strong results for this task. We discussed three methods to improve over the supervised baseline using unlabeled data: (1) modeling unlabeleld data using CRF au</context>
</contexts>
<marker>Li, Fung, 2014</marker>
<rawString>Ying Li and Pascale Fung. 2014. Code switch language modeling with functional head constraint. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 4913–4917, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
<author>C Dong</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming.</booktitle>
<contexts>
<context position="10278" citStr="Liu et al., 1989" startWordPosition="1643" endWordPosition="1646">metric Dirichlet prior with concentration parameter α for the reconstruction parameters θ. Having access to labeled examples, it is easy to modify this objective to learn from both labeled and unlabeled examples as follows: ``semi(λ, θ) = cL2||λ||22 + RDirichlet(θ, α)+ Xcunlabeled X log X p(y, xˆ |x)+ (x,ˆx)EU y:|y|=|x| Xclabeled X log p(y |x) (x,y)EL We use block coordinate descent to optimize this objective. First, we use cem iterations of the expectation maximization algorithm to optimize the θ-block while the λ-block is fixed, then we optimize the λ-block with clbfgs iterations of L-BFGS (Liu et al., 1989) while the θ-block is fixed.4 4.2 Unsupervised Word Embeddings For many NLP tasks, using unsupervised word representations as features improves accuracy (Turian et al., 2010). We use word2vec (Mikolov et al., 2013) to train 100–dimensional word embeddings from a large Twitter corpus of about 20 million tweets extracted from the live stream, in multiple languages. We define an additional feature function 4An open source efficient c++ implementation of our method can be found at https://github.com/ldmt-muri/alignment-with-openfst encoding reconstruction X Φ yi-1 yi yi+1 ΛΛΛ xi-1 xi xi+1 Y |x| i=</context>
</contexts>
<marker>Liu, Nocedal, Dong, 1989</marker>
<rawString>D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Accurate language identification of twitter messages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM),</booktitle>
<pages>17--25</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="1245" citStr="Lui and Baldwin, 2014" startWordPosition="172" endWordPosition="175">a: semi-supervised learning, word embeddings, and word lists. 1 Introduction Code switching (CS) occurs when a multilingual speaker uses more than one language in the same conversation or discourse. Automatic idenefication of the points at which code switching occurs is important for two reasons: (1) to help sociolinguists analyze the frequency, circumstances and motivations related to code switching (Gumperz, 1982), and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech. CS is pervasive in social media due to its informal nature (Lui and Baldwin, 2014). The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framewor</context>
</contexts>
<marker>Lui, Baldwin, 2014</marker>
<rawString>Marco Lui and Timothy Baldwin. 2014. Accurate language identification of twitter messages. In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM), pages 17–25, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Han Jey Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic detection and language identification of multilingual documents.</title>
<date>2014</date>
<journal>Transactions of the Association of Computational Linguistics,</journal>
<pages>2--27</pages>
<contexts>
<context position="19412" citStr="Lui et al., 2014" startWordPosition="3188" endWordPosition="3191">g lang1 lang2 ne +lists 84.1% 76.5% 73.7% -lists 84.2% 77.1% 71.5% Table 6: F–Measures of two Arabic configurations. lang1 is MSA. lang2 is ARZ. word lists also have a wide coverage, improving the overall accuracy by 1.6%. The overall accuracy of the Arabic dialects slightly degrades with the addition of the word lists. Closer inspection in table 6 reveals that it improves the F–Measure of the named entities at the expense of both MSA (lang1) and ARZ (lang2). 6 Related Work Previous work on identifying languages in a multilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend upon such NLP infrastructure. Both of the</context>
</contexts>
<marker>Lui, Lau, Baldwin, 2014</marker>
<rawString>Marco Lui, Han Jey Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multilingual documents. Transactions of the Association of Computational Linguistics, 2:27–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proc. of ICLR.</booktitle>
<contexts>
<context position="10492" citStr="Mikolov et al., 2013" startWordPosition="1675" endWordPosition="1678">s as follows: ``semi(λ, θ) = cL2||λ||22 + RDirichlet(θ, α)+ Xcunlabeled X log X p(y, xˆ |x)+ (x,ˆx)EU y:|y|=|x| Xclabeled X log p(y |x) (x,y)EL We use block coordinate descent to optimize this objective. First, we use cem iterations of the expectation maximization algorithm to optimize the θ-block while the λ-block is fixed, then we optimize the λ-block with clbfgs iterations of L-BFGS (Liu et al., 1989) while the θ-block is fixed.4 4.2 Unsupervised Word Embeddings For many NLP tasks, using unsupervised word representations as features improves accuracy (Turian et al., 2010). We use word2vec (Mikolov et al., 2013) to train 100–dimensional word embeddings from a large Twitter corpus of about 20 million tweets extracted from the live stream, in multiple languages. We define an additional feature function 4An open source efficient c++ implementation of our method can be found at https://github.com/ldmt-muri/alignment-with-openfst encoding reconstruction X Φ yi-1 yi yi+1 ΛΛΛ xi-1 xi xi+1 Y |x| i=1 θˆxi|yiX 82 in the CRF autoencoder model §4.1 for each of the 100 dimensions, conjoined with the label yi. The feature value is the corresponding dimension for xi. A binary feature indicating the absence of word </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proc. of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>Seza A Dogruoz</author>
</authors>
<title>Word level language identification in online multilingual communication.</title>
<date>2013</date>
<booktitle>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>857--862</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19946" citStr="Nguyen and Dogruoz, 2013" startWordPosition="3269" endWordPosition="3272">tilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend upon such NLP infrastructure. Both of the aforementioned use basic character–level features and dictionaries on sequence models. 7 Conclusion We have shown that a simple CRF baseline with a handful of feature templates obtains strong results for this task. We discussed three methods to improve over the supervised baseline using unlabeled data: (1) modeling unlabeleld data using CRF autoencoders, (2) using pre-trained word embeddings, and (3) using word list features. We show that adding word embedding features and word lists features is useful when they have good cover</context>
</contexts>
<marker>Nguyen, Dogruoz, 2013</marker>
<rawString>Dong Nguyen and Seza A. Dogruoz. 2013. Word level language identification in online multilingual communication. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 857–862. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Kumar Singh</author>
<author>Jagadeesh Gorla</author>
</authors>
<title>Identification of languages and encodings in a multilingual document. In Building and Exploring Web Corpora (WAC3-2007):</title>
<date>2007</date>
<booktitle>Proceedings of the 3rd Web as Corpus Workshop, Incorporating Cleaneval,</booktitle>
<volume>4</volume>
<pages>95</pages>
<contexts>
<context position="19371" citStr="Singh and Gorla, 2007" startWordPosition="3180" endWordPosition="3183">ge ID. The Spanish true MSA true ARZ 84 Config lang1 lang2 ne +lists 84.1% 76.5% 73.7% -lists 84.2% 77.1% 71.5% Table 6: F–Measures of two Arabic configurations. lang1 is MSA. lang2 is ARZ. word lists also have a wide coverage, improving the overall accuracy by 1.6%. The overall accuracy of the Arabic dialects slightly degrades with the addition of the word lists. Closer inspection in table 6 reveals that it improves the F–Measure of the named entities at the expense of both MSA (lang1) and ARZ (lang2). 6 Related Work Previous work on identifying languages in a multilingual document includes (Singh and Gorla, 2007; King and Abney, 2013; Lui et al., 2014). Their goal is generally more about identifying the languages that appear in the document than intra– sentential CS points. Previous work on computational models of code–switching include formalism (Joshi, 1982) and language models that encode syntactic constraints from theories of code–switching, such as (Li and Fung, 2013; Li and Fung, 2014). These require the existence of a parser for the languages under consideration. Other work on prediction of code–switching points, such as (Elfardy et al., 2013; Nguyen and Dogruoz, 2013) and ours, do not depend </context>
</contexts>
<marker>Singh, Gorla, 2007</marker>
<rawString>Anil Kumar Singh and Jagadeesh Gorla. 2007. Identification of languages and encodings in a multilingual document. In Building and Exploring Web Corpora (WAC3-2007): Proceedings of the 3rd Web as Corpus Workshop, Incorporating Cleaneval, volume 4, page 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thamar Solorio</author>
<author>Elizabeth Blair</author>
<author>Suraj Maharjan</author>
<author>Steve Bethard</author>
<author>Mona Diab</author>
<author>Mahmoud Gonheim</author>
<author>Abdelati Hawwari</author>
<author>Fahad AlGhamdi</author>
<author>Julia Hirshberg</author>
<author>Alison Chang</author>
<author>Pascale Fung</author>
</authors>
<title>Overview for the first shared task on language identification in codeswitched data.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Computational Approaches to Code-Switching.</booktitle>
<contexts>
<context position="1372" citStr="Solorio et al., 2014" startWordPosition="192" endWordPosition="195">ker uses more than one language in the same conversation or discourse. Automatic idenefication of the points at which code switching occurs is important for two reasons: (1) to help sociolinguists analyze the frequency, circumstances and motivations related to code switching (Gumperz, 1982), and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech. CS is pervasive in social media due to its informal nature (Lui and Baldwin, 2014). The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framework; conditional random field autoencoders (Ammar et al., 2014). The CRF autoencoder framework consists of an encoding model and </context>
</contexts>
<marker>Solorio, Blair, Maharjan, Bethard, Diab, Gonheim, Hawwari, AlGhamdi, Hirshberg, Chang, Fung, 2014</marker>
<rawString>Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve Bethard, Mona Diab, Mahmoud Gonheim, Abdelati Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison Chang, and Pascale Fung. 2014. Overview for the first shared task on language identification in codeswitched data. In Proceedings of the First Workshop on Computational Approaches to Code-Switching.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10452" citStr="Turian et al., 2010" startWordPosition="1668" endWordPosition="1671">from both labeled and unlabeled examples as follows: ``semi(λ, θ) = cL2||λ||22 + RDirichlet(θ, α)+ Xcunlabeled X log X p(y, xˆ |x)+ (x,ˆx)EU y:|y|=|x| Xclabeled X log p(y |x) (x,y)EL We use block coordinate descent to optimize this objective. First, we use cem iterations of the expectation maximization algorithm to optimize the θ-block while the λ-block is fixed, then we optimize the λ-block with clbfgs iterations of L-BFGS (Liu et al., 1989) while the θ-block is fixed.4 4.2 Unsupervised Word Embeddings For many NLP tasks, using unsupervised word representations as features improves accuracy (Turian et al., 2010). We use word2vec (Mikolov et al., 2013) to train 100–dimensional word embeddings from a large Twitter corpus of about 20 million tweets extracted from the live stream, in multiple languages. We define an additional feature function 4An open source efficient c++ implementation of our method can be found at https://github.com/ldmt-muri/alignment-with-openfst encoding reconstruction X Φ yi-1 yi yi+1 ΛΛΛ xi-1 xi xi+1 Y |x| i=1 θˆxi|yiX 82 in the CRF autoencoder model §4.1 for each of the 100 dimensions, conjoined with the label yi. The feature value is the corresponding dimension for xi. A binary</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
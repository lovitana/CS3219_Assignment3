<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000663">
<title confidence="0.962942">
IPA and STOUT: Leveraging Linguistic and Source-based
Features for Machine Translation Evaluation
</title>
<author confidence="0.995343">
Meritxell Gonz`alez, Alberto Barr´on-Cede˜no Lluis M`arquez
</author>
<affiliation confidence="0.9990585">
TALP Research Center, Qatar Computing Research Institute
Technical University of Catalonia Qatar Foundation
</affiliation>
<email confidence="0.99695">
{mgonzalez,albarron}@lsi.upc.edu lmarquez@qf.org.qa
</email>
<sectionHeader confidence="0.997363" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954444444444">
This paper describes the UPC submissions
to the WMT14 Metrics Shared Task: UPC-
IPA and UPC-STOUT. These metrics
use a collection of evaluation measures in-
tegrated in ASIYA, a toolkit for machine
translation evaluation. In addition to some
standard metrics, the two submissions take
advantage of novel metrics that consider
linguistic structures, lexical relationships,
and semantics to compare both source and
reference translation against the candidate
translation. The new metrics are available
for several target languages other than En-
glish. In the the official WMT14 evalua-
tion, UPC-IPA and UPC-STOUT scored
above the average in 7 out of 9 language
pairs at the system level and 8 out of 9 at
the segment level.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999842259259259">
Evaluating Machine Translation (MT) quality is a
difficult task, in which even human experts may
fail to achieve a high degree of agreement when
assessing translations. Conducting manual evalu-
ations is impractical during the development cy-
cle of MT systems or for transation applications
addressed to general users, such as online transla-
tion portals. Automatic evaluation measures bring
valuable benefits in such situations. Compared to
manual evaluation, automatic measures are cheap,
more objective, and reusable across different test
sets and domains.
Nonetheless, automatic metrics are far from
perfection: when used in isolation, they tend to
stress specific aspects of the translation quality and
neglect others (particularly during tuning); they
are often unable to capture little system improve-
ments (enhancements in very specific aspects of
the translation process); and they may make un-
fair comparisons when they are not able to reflect
real differences among the quality of different MT
systems (Gim´enez, 2008).
ASIYA, the core of our approach, is an open-
source suite for automatic machine translation
evaluation and output analysis.1 It provides a rich
set of heterogeneous metrics and tools to evalu-
ate and analyse the quality of automatic transla-
tions. The ASIYA core toolkit was first released
in 2009 (Gim´enez and M`arquez, 2010a) and has
been continuously improved and extended since
then (Gonz`alez et al., 2012; Gonz`alez et al., 2013).
In this paper we first describe the most recent
enhancements to ASIYA: (i) linguistic-based met-
rics for French and German; (ii) an extended set
of source-based metrics for English, Spanish, Ger-
man, French, Russian, and Czech; and (iii) the in-
tegration of mechanisms to exploit the alignments
between sources and translations. These enhance-
ments are all available in ASIYA since version 3.0.
We have used them to prepare the UPC submis-
sions to the WMT14 Metrics Task: UPC-IPA and
UPC-STOUT, which serve the purpose of testing
their usefulness in a real comparative setting.
The rest of the paper is structured as follows.
Section 2 describes the new reference-based met-
rics developed, including syntactic parsers for lan-
guages other than English. Section 3 gives the
details of novel source-based metrics, developed
for almost all the language pairs in this challenge.
Section 4 explains our simple metrics combina-
tion strategy and analyses the results obtained with
both approaches, UPC-IPA and UPC-STOUT,
when applied to the WMT13 dataset. Finally, Sec-
tion 5 summarises our main contributions.
</bodyText>
<sectionHeader confidence="0.999684" genericHeader="method">
2 Reference-based Metrics
</sectionHeader>
<bodyText confidence="0.999243333333333">
We recently added a new set of metrics to ASIYA,
which estimate the similarity between reference
(ref) and candidate (cand) translations. The met-
</bodyText>
<footnote confidence="0.982145">
1http://asiya.lsi.upc.edu
</footnote>
<page confidence="0.951475">
394
</page>
<note confidence="0.7342905">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 394–401,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.861969666666667">
rics rely either on structural linguistic informa-
tion (Section 2.1), on a semantic mapping (Sec-
tion 2.2), or on word n-grams (Section 2.3).
</bodyText>
<subsectionHeader confidence="0.840934">
2.1 Parsing-based Metrics
</subsectionHeader>
<bodyText confidence="0.999967697674419">
Our initial set of parsing-based metrics is a follow-
up of the proposal by Gim´enez and M`arquez
(2010b): it leverages the structural information
provided by linguistic processors to compute sev-
eral similarity cues between two analyzed sen-
tences. ASIYA includes plenty of metrics that cap-
ture syntactic and semantic aspects of a transla-
tion. New metrics based on linguistic structural
information for French and German and upgraded
versions of the parsers for English and Spanish are
available since version 3.0.2
In the WMT14 evaluation, we opt for metrics
based on shallow parsing (SP), constituency pars-
ing (CP), and dependency parsing (DPm)3. Mea-
sures based on named entities (NE) and semantic
roles (SR) were used to analyse translations into
English as well. The nomenclature used below
follows the same patterns as in the ASIYA’s man-
ual (Gonz`alez and Gim´enez, 2014). The manual
describes every family of metrics in detail. Next,
we briefly depict the concrete metrics involved in
our submissions to the WMT14 Shared Task.
The set of SP metrics is available for English,
German, French, Spanish and Catalan. They
measure the lexical overlapping between parts-of-
speech elements in the candidate and reference
translations. For instance, SP-Op(VB) measures
the proportion of correctly translated verbs; and
the coarser SP-Op(*) averages the overlapping be-
tween the words for each part of speech. We also
use NIST (Doddington, 2002) to compute accu-
mulated scores over sequences of n = 1..5 parts
of speech (SP-pNIST).
Similarly, CP metrics analyse similarities be-
tween constituent parse trees associated to can-
didate and reference translations. For instance,
CP-STMi5 and CP-STM4 compute, respectively,
the proportion of (individual) length-5 and accu-
mulated up to length-4 matching sub-paths of the
syntactic tree (Liu and Gildea, 2005). CP-Oc(*)
computes the lexical overlap averaged over all the
phrase constituents. Constituent trees are obtained
using the parsers of Charniak and Johnson (2005),
</bodyText>
<footnote confidence="0.999024">
2Equivalent resources were previously available for En-
glish, Catalan, and Spanish.
3ASIYA includes two dependency parsers; the m identifies
the metrics calculated using the MALT parser.
</footnote>
<bodyText confidence="0.963298055555555">
Bonsai v3.2 (Candito et al., 2010b), and Berke-
ley Parser (Petrov et al., 2006; Petrov and Klein,
2007) for English, French, and German, respec-
tively.
Measures based on dependency parsing (DPm)
— available for English and French thanks to
the MALT parser (Nivre et al., 2007)— capture
the similarities between dependency tree items
(i.e., heads and modifiers). The pre-trained mod-
els for French were obtained from the French
Treebank (Candito et al., 2010a) and used to
train the Bonsai parser, which in turn uses the
MALT parser. For instance, DPm-HWCM w-3 re-
trieves average accumulated proportion of match-
ing word-chains (Liu and Gildea, 2005) up
to length 3; and DPm-HWCMi c-3 computes
the proportion of matching category-chains of
length 3.
</bodyText>
<subsectionHeader confidence="0.98902">
2.2 Explicit-Semantics Metric
</subsectionHeader>
<bodyText confidence="0.999965142857143">
Additionally, we borrowed a metric originally pro-
posed in the field of Information Retrieval: ex-
plicit semantic analysis (ESA) (Gabrilovich and
Markovitch, 2007). ESA is a similarity metric
that relies on a large corpus of general knowl-
edge to represent texts. Our knowledge corpora
are composed of ∼ 100K Wikipedia articles from
2010 for the following target languages: English,
French and German. In this case, ref and cand
translations are both mapped onto the Wikipedia
collection W. The similarities between each text
and every article a E W are computed on the ba-
sis of the cosine measure in order to compose a
similarities vector that represents the text. That is:
</bodyText>
<equation confidence="0.999904">
~ref = {sim(ref, a) ba E W} ,(1)
cand = {sim(cand, a) ba E W} . (2)
</equation>
<bodyText confidence="0.9999685">
As the i-th elements in both ~ref and cand represent
the similarity of ref and cand sentences to a com-
mon article, the similarity between ref and cand
can be estimated by computing sim( ref, cand).
</bodyText>
<subsectionHeader confidence="0.998715">
2.3 Language-Independent Resource-Free
Metric
</subsectionHeader>
<bodyText confidence="0.999993125">
We consider a simple characterisation based on
word n-grams. Texts are broken down into over-
lapping word sequences of length n, with 1-word
shifting. The similarity between cand and ref
is computed on the basis of the Jaccard coeffi-
cient (Jaccard, 1901). We used this metric for the
pairs English–Russian and Russian-English, con-
sidering n = 2 (NGRAM-jacTok2ngram). For the
</bodyText>
<page confidence="0.995179">
395
</page>
<bodyText confidence="0.99993475">
rest of the pairs we opt for the character-n-gram
metrics described in Section 3.1, but they showed
no positive results in the English–Russian pair dur-
ing our tuning experiments.
</bodyText>
<sectionHeader confidence="0.998033" genericHeader="method">
3 Source-based Metrics
</sectionHeader>
<bodyText confidence="0.999976333333333">
We enhance our evaluation module by including
a set of new metrics that compare the source text
against the translations. The metrics can be di-
vided into two subsets: those that do not require
any external resources (Section 3.1) and those that
depend on a parallel corpus (Section 3.2).
</bodyText>
<subsectionHeader confidence="0.997485">
3.1 Language-Independent Resource-Free
Metrics
</subsectionHeader>
<bodyText confidence="0.999836310344827">
We opted for two characterisations that allow for
the comparison of texts across languages without
external resources nor language-related knowl-
edge —as far as the languages use the same writ-
ing system.4
The first characterisation is character n-grams;
proposed by McNamee and Mayfield (2004) for
cross-language information retrieval between Eu-
ropean languages. Texts are broken down into
overlapping character sequences of length n, with
1-character shifting. We opt for case-folded bi-
grams (NGRAM-cosChar2ngrams), as they al-
lowed for the best performance across all the pairs
(except for From/To Russian pairs) during tuning.
The second characterisation (NGRAM-
jacCognates) is based on the concept of
cognateness; originally proposed for bitexts
alignment (Simard et al., 1992). A word is a
pseudo-cognate candidate if (i) it has only letters
and |w |≥ 4, (ii) it contains at least one digit, or
(iii) it is a single punctuation mark. src and cand
sentences are then represented as word vectors,
containing only those words fulfilling one of the
previous conditions. In the case of (i) the word is
cut down to its leading four characters only.
In both cases (character n-grams and cognate-
ness) cand translations are compared against src
sentences on the basis of the cosine similarity
measure.
</bodyText>
<subsectionHeader confidence="0.997837">
3.2 Parallel-Corpus Metrics
</subsectionHeader>
<bodyText confidence="0.999706">
We consider two metrics that make use of parallel
corpora: length factor and alignment.
</bodyText>
<footnote confidence="0.641231">
4Previous research showed that transliteration is a
good short-cut when dealing with different writing sys-
tems (Barr´on-Cede˜no et al., 2014).
</footnote>
<tableCaption confidence="0.8145415">
Table 1: Length factor parameters as estimated on
the WMT13 parallel corpora.
</tableCaption>
<table confidence="0.7627462">
pair µ Q pair µ Q
en–cs 0.972 0.245 cs–en 1.085 0.273
en–de 1.176 0.926 de–en 0.961 0.463
en–fr 1.158 0.411 fr–en 0.914 0.313
en–ru 1.157 0.678 ru–en 1.069 0.668
</table>
<bodyText confidence="0.999911">
The length factor (LeM) is rooted in the fact that
the length of a text and its translation tend to pre-
serve a certain length correlation. For instance,
translations from English into Spanish or French
tend to be longer than their source. Similar mea-
sures were proposed during the statistical machine
translation early days, both considering character-
and word-level lengths (Gale and Church, 1993;
Brown et al., 1991). Pouliquen et al. (2003) de-
fines the length factor as:
</bodyText>
<equation confidence="0.931995">
, (3)
</equation>
<bodyText confidence="0.999972739130435">
where µ and Q represent the mean and standard
deviation of the character lengths between trans-
lations of texts from L into L&apos;. This is a stochas-
tic normal distribution that results in higher values
as the length of the target text approaches the ex-
pected value given the source. Table 1 includes
the values for each language pair, as estimated on
the WMT13 parallel corpora. Note that this metric
was not applied to Hindi–English since this lan-
guage pair was not present in the WMT13 chal-
lenge.
The last of our newly-added measures relies
on the word alignments calculated between the
sentence pairs src–cand and src–ref. We trained
alignment models for each language pair using the
Berkeley Aligner5, and devised three variants of
an ALGN metric, which compute: (i) the propor-
tion of aligned words between src and cand (AL-
GNs); (ii) the proportion of aligned words between
cand and ref, calculated as the combination of the
alignments src–cand and src–ref (ALGNr); and
(iii) the ratio of shared alignments between src-
cand and src–ref (ALGNp).
</bodyText>
<sectionHeader confidence="0.998898" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.938605">
The tuning and selection of the different met-
rics to build UPC-IPA and UPC-STOUT was
</bodyText>
<footnote confidence="0.689924">
5https://code.google.com/p/berkeleyaligner
</footnote>
<equation confidence="0.912118">
−0.5 |dq |µ)
o(d�) = e σ
</equation>
<page confidence="0.988665">
396
</page>
<bodyText confidence="0.996614227272727">
conducted considering the WMT13 Metrics Task
dataset (Mach´aˇcek and Bojar, 2013), and the re-
sources distributed for the WMT13 Translation
Task (Bojar et al., 2013). Table 2 gives a
complete list of these metrics grouped by fami-
lies. First, we calculated the Pearson’s correla-
tion with the human judgements for all the met-
rics in the current version of the ASIYA repos-
itory, including standard MT evaluation metrics,
such as METEOR (Denkowski and Lavie, 2011),
GTM (Melamed et al., 2003), -TERp-A (Snover
et al., 2009) (a variant of TER tuned towards ade-
quacy), WER (Nießen et al., 2000) and PER (Till-
mann et al., 1997). We selected the best perform-
ing metrics (i.e., those resulting in high Pearson
coefficients) in each family across all the From/To
English translation language pairs, including the
newly developed measures —even if they per-
formed poorly compared to others (see This is how
the UPC-STOUT metrics sets for both from En-
glish and To English translation pairs were com-
posed6 (see Table 3).
</bodyText>
<tableCaption confidence="0.577608">
Table 2: Metrics considered in the experiments
separated by families according to the type of
grammatical items they use.
</tableCaption>
<listItem confidence="0.938529444444445">
1. -WER 17. DPm-HWCM r-1
2. -PER 18. DPm-Or(*)
3. -TERp-A 19. SR-Or(*)
4. METEOR-ex 20. SR-Or
5. METEOR-pa 21. SR-Orv(*)
6. GTM-3 22. SR-Orv
7. SP-Op(*) 23. NE-Oe(*)
8. SP-pNIST 24. NE-Oe(**)
9. CP-STMi-5 25. ESA
</listItem>
<table confidence="0.977354428571429">
10. CP-STMi-2 26. NGRAM-jacTok2ngrams
11. CP-STMi-3 27. NGRAM-jacCognates
12. CP-STM-4 28. NGRAM-cosChar2ngrams
13. CP-Oc(*) 29. LeM
14. DPm-HWCM w-3 30. ALGNp
15. DPm-HWCM c-3 31. ALGNs
16. DPm-HWCMi c-3 32. ALGNr
</table>
<tableCaption confidence="0.997513">
Table 3: Metrics considered in each system.7
</tableCaption>
<table confidence="0.815094666666667">
BAS: 1–6 SYN: 7–18
SEM: 19–25 SRC: 26–32
IPA: 1–9, 25–31 STOUT: 1–32
</table>
<tableCaption confidence="0.335073">
6Parser-based measures are not present in Czech nor Rus-
sian as target languages, ALGN is not available for French
pairs, and ESA is not applied to Russian as target.
</tableCaption>
<bodyText confidence="0.999928959183674">
The metric sets included in UPC-IPA are light
versions of the UPC-STOUT ones. They were
composed following different criteria, depending
on the translation direction. Parsing-based mea-
sures were already available in the previous ver-
sion of ASIYA when translating into English —
they are known to be robust across domains and
are usually good indicators of translation qual-
ity (Gim´enez and M`arquez, 2007). So, in order
to assess the gain achieved with these measures
with respect the new ones, UPC-IPA neglects the
measures based on structural information obtained
from parsers. In contrast, this distinction was not
suitable for the From English pairs since the num-
ber of resources and measures varies for each lan-
guage. Hence, in this latter case, UPC-IPA used
only the subset of measures from UPC-STOUT
that required no or little resources.
In summary, when English is the target lan-
guage, UPC-IPA uses the baseline evaluation
metrics along with the length factor, alignments-
based metrics, character n-grams, and ESA. In ad-
dition to the above metrics, UPC-STOUT uses
the linguistic-based metrics over parsing trees,
named entities, and semantic roles. When English
is the source language, UPC-IPA relies on the
basic collection of metrics and character n-grams
only. UPC-STOUT includes the alignment-based
metrics, length factor, ESA, and the syntactic
parsers applied to both German and French.
In all cases (metric sets and language pairs),
the translation quality score is computed as the
uniformly-averaged linear combination (ULC) of
all the individual metrics for each sentence in the
testset. Its calculation implies the normalization
of heterogeneous scores (some of them are neg-
ative or unbounded), into the range [0, 1]. As a
consequence, the scores of UPC-IPA and UPC-
STOUT constitute a natural way of ranking dif-
ferent translations, rather than an overall quality
estimation measure. We opt for this linear combi-
nation for simplicity. The discussion below sug-
gests that a more sophisticated method for weight
tuning (e.g., relying on machine learning methods)
would be required for each language pair, domain
and/or task since different metric families perform
notably different for each subtask.
We complete our experimentation by eval-
uating more configurations: BAS, a baseline
</bodyText>
<footnote confidence="0.858372333333333">
7These are the full sets of measures for each configura-
tion. However, each specific subset for From/To English can
vary slightly depending on the available resources.
</footnote>
<page confidence="0.998087">
397
</page>
<tableCaption confidence="0.996313">
Table 4: System-level Pearson correlation for automatic metrics over translations From/To English.
</tableCaption>
<table confidence="0.99991475">
WMT13 en–fr en–de en–es en–cs en–ru fr–en de–en es–en cs–en ru–en
UPC-IPA 93.079 85.147 88.702 85.259 70.345 96.755 94.660 95.065 94.316 72.083
UPC-STOUT 94.274 90.193 73.314 84.743 70.544 96.916 96.208 96.704 96.666 74.050
BAS 92.502 84.251 90.051 86.584 67.655 95.777 96.506 95.98 96.539 71.536
SYN 95.68 87.297 96.965 n/a n/a 96.291 96.592 96.052 95.238 73.083
BAS+SYN 94.584 87.786 95.162 n/a n/a 96.684 97.057 96.101 96.402 72.800
SEM 89.735 83.647 35.694 95.067 n/a 95.629 96.601 98.021 96.595 76.158
BAS+SEM 92.254 87.005 47.321 89.107 n/a 96.337 97.534 97.568 97.371 74.804
SRC 14.465 -16.796 -22.466 -49.981 39.527 13.405 -51.371 71.64 -73.254 68.766
BAS+SRC 93.637 76.401 83.754 64.742 54.128 95.395 90.889 93.299 89.216 71.882
WMT13-Best 94.745 93.813 96.446 86.036 81.194 98.379 97.789 99.171 83.734 94.768
WMT13-Worst 78.787 -45.461 87.677 69.151 61.075 95.118 92.239 79.957 60.918 82.058
</table>
<tableCaption confidence="0.964732">
Table 5: Segment-level Kendall’s τ correlation for automatic metrics over translations From/To English.
</tableCaption>
<table confidence="0.9990415">
WMT13 en–fr en–de en–es en–cs en–ru fr–en de–en es–en cs–en ru–en
UPC-IPA 18.625 14.901 17.057 7.805 15.132 22.832 25.769 26.907 21.207 19.904
UPC-STOUT 19.488 15.012 17.166 8.545 15.279 23.090 27.117 26.848 21.332 19.100
BAS 19.477 13.589 16.975 8.449 15.599 24.060 28.259 28.381 23.346 20.983
SYN 16.554 14.970 16.444 n/a n/a 22.365 24.289 23.889 20.232 17.679
BAS+SYN 19.112 16.016 18.122 n/a n/a 23.940 28.068 27.988 23.180 19.659
SEM 12.184 9.249 10.871 3.808 n/a 17.282 19.083 20.859 15.186 14.971
BAS+SEM 19.167 13.291 15.857 7.732 n/a 22.024 25.788 26.360 21.427 19.117
SRC 2.745 2.481 1.152 1.992 5.247 2.181 1.154 8.700 -4.023 16.267
BAS+SRC 18.32 13.017 15.698 7.666 13.619 22.292 24.948 26.780 17.603 20.707
WMT13-Best 21.897 19.459 20.699 11.283 18.899 26.836 29.565 24.271 21.665 25.584
WMT13-Worst 16.753 13.910 3.024 4.431 13.166 14.008 14.542 14.494 9.667 13.178
</table>
<bodyText confidence="0.99994428">
with standard and commonly used MT metrics;
SYN, the reference-based syntactic metrics; SEM,
the reference-based semantic metrics; SRC, the
source-based metrics; and the combination of
BAS with every other configuration: BAS+SYN,
BAS+SEM, and BAS+SRC. Their purpose is to
evaluate the contribution of the newly developed
sets of metrics with respect to the baseline. The
composition of the different configurations is sum-
marised in Tables 2 and 3.
Evaluation results are shown in Tables 4 and 5.
For each configuration and language pair, we show
the correlation coefficients obtained at the system-
and the segment-level, respectively. As customary
with the WMT13 dataset, Pearson correlation was
computed at the system-level, whereas Kendall’s
τ was used to estimate segment-level rank correla-
tions. Additionally to the two submitted and seven
extra configurations, we include the coefficients
obtained with the Best and Worst systems reported
in the official WMT13 evaluation for each lan-
guage pair.
Although the results of our two submitted sys-
tems are not radically different to each other,
UPC-STOUT consistently outperforms UPC-
IPA. The currently available version of ASIYA, in-
cluding the new metrics, allows for a performance
close to the top-performing evaluation measures in
last year’s challenge, even with our naive combi-
nation strategy.
It is worth noting that no configuration be-
haves the same way throughout the different lan-
guages. In some cases (e.g., with the SRC config-
uration), the bad performance can be explained by
the weaknesses of the necessary resources when
computing certain metrics. When analysed in de-
tail, the cause can be ascribed to different metric
families in each case. As a result, it is clear that
specific configurations are necessary for evaluat-
ing different languages and domains. We plan to
approach these issues as part of our future work.
When looking at the system-level figures, one
can observe that the SEM set allows for a con-
siderable improvement over the baseline system.
The further inclusion of the SYN set —when
available—, tends to increase the quality of the
estimations, mainly when English is the source
language. These properties impact on some of
the UPC-STOUT configurations. In contrast,
when looking at the segment-level scores, while
</bodyText>
<page confidence="0.998932">
398
</page>
<tableCaption confidence="0.89713">
Table 6: System-level Pearson correlation results
in the WMT14 Metrics shared task
</tableCaption>
<table confidence="0.9999446">
en–fr en–de en–cs en–ru
UPC-IPA 93.7 13.0 96.8 92.2
UPC-STOUT 93.8 14.8 93.8 92.1
WMT14-Best 95.9 19.8 98.8 94.2
WMT14-Worst 88.8 1.1 93.8 90.3
fr–en de–en hi–en cs–en ru–en
UPC-IPA 96.6 89.4 91.5 82.4 80.0
UPC-STOUT 96.8 91.4 89.8 94.7 82.5
WMT14-Best 98.1 94.2 97.6 99.3 86.1
WMT14-Worst 94.5 76.0 41.1 74.1 -41.7
</table>
<tableCaption confidence="0.9511365">
Table 7: Segment-level Kendall’s τ correlation re-
sults in the WMT14 Metrics shared task
</tableCaption>
<table confidence="0.999746">
en–fr en–de en–cs en–ru
UPC-IPA 26.3 21.7 29.7 42.6
UPC-STOUT 27.8 22.4 28.1 42.5
WMT14-Best 29.7 25.8 34.4 44.0
WMT14-Worst 25.4 18.5 28.1 38.1
fr–en de–en hi–en cs–en ru–en
UPC-IPA 41.2 34.1 36.7 27.4 32.4
UPC-STOUT 40.3 34.5 35.1 27.5 32.4
WMT14-Best 43.3 38.1 43.8 32.8 36.4
WMT14-Worst 31.1 22.5 23.7 18.7 21.2
</table>
<bodyText confidence="0.990693071428571">
the SYN measures still tend to provide some gain
over the baseline, the SEM ones do not. Finally, it
merits some attention the good results achieved by
the baseline for translations into English. We may
remark here that our baseline included also the
best performing state-of-the-art metrics, including
all the variants of METEOR, that reported good
results in the WMT13 challenge.
Tables 6 and 7 show the official results obtained
by UPC-IPA and UPC-STOUT in WMT14.8
The best and worst figures for each language pair
are included for comparison —the worst perform-
ing submission at segment level is neglected as it
seems to be a dummy (Mach´aˇcek and Bojar, 2014
to appear). Both UPC-IPA and UPC-STOUT
configurations resulted in different performances
depending on the language pair. UPC-STOUT
scored above the average for all the language pairs
except for en–cs at both system and segment level,
and en–ru at system level. Although the evaluation
results are not directly comparable to the WMT13
ones, one can note that the results were notably
better for pairs that involved Czech and Russian,
and worse for those that involved French and Ger-
man at system level. Analysing the impact of the
evaluation methods and building comparable re-
sults in order to address a study on configurations
for different languages is part of our future work.
</bodyText>
<sectionHeader confidence="0.997775" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999840333333333">
This paper describes the UPC submission to the
WMT14 metrics for automatic machine transla-
tion evaluation task. The core of our evaluation
system is ASIYA, a toolkit for MT evaluation. Be-
sides the formerly available metrics in ASIYA, we
experimented with new metrics for machine trans-
</bodyText>
<footnote confidence="0.4991665">
8At the time of submitting this paper, the evaluation re-
sults for WMT14 Metrics Task were provisional.
</footnote>
<bodyText confidence="0.999737242424242">
lation evaluation, with especial focus on transla-
tion from English into other languages.
As previous work on English as target language
has proven, syntactic and semantic analysis can
contribute positively to the evaluation of automatic
translations. For this reason, we integrated a set of
new metrics for different languages, aimed at eval-
uating a translation from different perspectives.
Among the novelties, (i) new shallow metrics, bor-
rowed from Information Retrieval, were included
to compare the candidate translation against both
the reference translation (monolingual compari-
son) and the source sentence (cross-language com-
parison), including explicit semantic analysis and
the lexical-based characterisations character n-
grams and pseudo-cognates; (ii) new parsers for
other languages than English were applied to com-
pare automatic and reference translation at the
syntactic level; (iii) an experimental metric based
on alignments; and (iv) a metric based on the cor-
relation of the translations’ expected lengths was
included as well. Our preliminary experiments
showed that the combination of these and standard
MT evaluation metrics allows for a performance
close to the best in last year’s competition for some
language pairs.
The new set of metrics is already available
in the current version of the toolkit ASIYA
v3.0 (Gonz`alez and Gim´enez, 2014). Our current
efforts are focused on the exploitation of more so-
phisticated methods to combine the contributions
of each metric, and the extension of the list of sup-
ported languages.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999437333333333">
This work was funded by the Spanish Ministry
of Education and Science (TACARDI project,
TIN2012-38523-C02-00).
</bodyText>
<page confidence="0.998549">
399
</page>
<sectionHeader confidence="0.996332" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828133928571">
Alberto Barr´on-Cede˜no, Monica Lestari Paramita, Paul
Clough, and Paolo Rosso. 2014. A Comparison
of Approaches for Measuring Cross-Lingual Sim-
ilarity of Wikipedia Articles. Advances in Infor-
mation Retrieval. Proceedings of the 36th European
Conference on IR Research, LNCS (8416):424–429.
Springer-Verlag.
Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mer-
cer. 1991. Aligning Sentences in Parallel Corpora.
In Douglas E. Appelt, editor, Proceedings of the
29th Annual Meeting of the Association for Com-
putational Linguistics (ACL 1991), pages 169–176,
Berkeley, CA, USA. Association for Computational
Linguistics.
Marie Candito, Benot Crabb, and Pascal Denis. 2010a.
Statistical French dependency parsing: treebank
conversion and first results. In The seventh interna-
tional conference on Language Resources and Eval-
uation (LREC), Valletta, Malta.
Marie Candito, Joakim Nivre, Pascal Denis, and En-
rique Henestroza Anguiano. 2010b. Benchmark-
ing of Statistical Dependency Parsers for French. In
Proc. 23rd Intl. COLING Conference on Computa-
tional Linguistics: Poster Volume, pages 108–116,
Beijing, China.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine N-best Parsing and MaxEnt Discriminative
Reranking. In Proc. 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 173–180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the 6th Workshop on Statistical Ma-
chine Translation, pages 85–91, Stroudsburg, PA,
USA. Association for Computational Linguistics.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology, pages 138–145, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness Using Wikipedia-based Ex-
plicit Semantic Analysis. In 20th International Joint
Conference on Artificial Intelligence, pages 1606–
1611, San Francisco, CA, USA.
William A. Gale and Kenneth, W. Church. 1993. A
Program for Aligning Sentences in Bilingual Cor-
pora. Computational Linguistics, 19:75–102.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proc. of 2nd Workshop on
statistical Machine Translation (WMT07), ACL’07,
Prague, Czech Republic.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94:77–86.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010b. Linguistic
Measures for Automatic Machine Translation Eval-
uation. Machine Translation, 24(3–4):77–86.
Jes´us Gim´enez. 2008. Empirical Machine Translation
and its Evaluation. Ph.D. thesis, Universitat Politc-
nica de Catalunya, July.
Meritxell Gonz`alez and Jes´us Gim´enez. 2014.
Asiya: An Open Toolkit for Automatic Ma-
chine Translation (Meta-)Evaluation, v3.0, Febru-
ary. http://asiya.lsi.upc.edu.
Meritxell Gonz`alez, Jes´us Gim´enez, and Llu´ıs
M`arquez. 2012. A Graphical Interface for MT Eval-
uation and Error Analysis. In Proc. Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL). System Demonstration, pages 139–144,
Jeju, South Korea, July. Association for Computa-
tional Linguistics.
Meritxell Gonz`alez, Laura Mascarell, and Llu´ıs
M`arquez. 2013. tSearch: Flexible and Fast
Search over Automatic translation for Improved
Quality/Error Analysis. In Proc. 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL). System Demonstration, pages 181–186,
Sofia, Bulgaria, August.
Paul Jaccard. 1901. ´Etude comparative de la distribu-
tion florale dans une portion des Alpes et des Jura.
Bulletin del la Soci´et´e Vaudoise des Sciences Na-
turelles, 37:547–579.
Ding Liu and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 25–32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45–51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2014 (to appear).
Results of the WMT14 metrics shared task. In Pro-
ceedings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimare, US, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.963513">
400
</page>
<reference confidence="0.999576771929825">
Paul McNamee and James Mayfield. 2004. Character
N-Gram Tokenization for European Language Text
Retrieval. Information Retrieval, 7(1-2):73–97.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), pages 61–63, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Sonja Nießen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation (LREC).
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. MaltParser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Slav Petrov and Dan Klein. 2007. Improved Infer-
ence for Unlexicalized Parsing. In Proc. Human
Language Technologies (HLT), pages 404–411. As-
sociation for Computational Linguistics, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 433–440,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat.
2003. Automatic Identification of Document Trans-
lations in Large Multilingual Document Collections.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
(RANLP-2003), pages 401–408, Borovets, Bulgaria.
Michel Simard, George F. Foster, and Pierre Isabelle.
1992. Using Cognates to Align Sentences in Bilin-
gual Corpora. In Proceedings of the Fourth Interna-
tional Conference on Theoretical and Methodologi-
cal Issues in Machine Translation.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the 4th Work-
shop on Statistical Machine Translation, pages 259–
268, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Christoph Tillmann, Stefan Vogel, Hermann Ney,
A. Zubiaga, and H. Sawaf. 1997. Accelerated DP
based Search for Statistical Translation. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology, pages 2667–2670.
</reference>
<page confidence="0.998628">
401
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936106">
<title confidence="0.998745">IPA and STOUT: Leveraging Linguistic and Features for Machine Translation Evaluation</title>
<author confidence="0.994209">Alberto Lluis M`arquez Gonz`alez</author>
<affiliation confidence="0.995405">TALP Research Center, Qatar Computing Research Institute Technical University of Catalonia Qatar Foundation</affiliation>
<email confidence="0.960882">lmarquez@qf.org.qa</email>
<abstract confidence="0.999276368421053">This paper describes the UPC submissions the Metrics Shared UPC- IPA and UPC-STOUT. These metrics use a collection of evaluation measures inin a toolkit for machine translation evaluation. In addition to some standard metrics, the two submissions take advantage of novel metrics that consider linguistic structures, lexical relationships, and semantics to compare both source and reference translation against the candidate translation. The new metrics are available for several target languages other than English. In the the official WMT14 evaluation, UPC-IPA and UPC-STOUT scored above the average in 7 out of 9 language pairs at the system level and 8 out of 9 at the segment level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alberto Barr´on-Cede˜no</author>
<author>Monica Lestari Paramita</author>
<author>Paul Clough</author>
<author>Paolo Rosso</author>
</authors>
<title>A Comparison of Approaches for Measuring Cross-Lingual Similarity of Wikipedia Articles.</title>
<date>2014</date>
<booktitle>Advances in Information Retrieval. Proceedings of the 36th European Conference on IR Research, LNCS</booktitle>
<pages>8416--424</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Barr´on-Cede˜no, Paramita, Clough, Rosso, 2014</marker>
<rawString>Alberto Barr´on-Cede˜no, Monica Lestari Paramita, Paul Clough, and Paolo Rosso. 2014. A Comparison of Approaches for Measuring Cross-Lingual Similarity of Wikipedia Articles. Advances in Information Retrieval. Proceedings of the 36th European Conference on IR Research, LNCS (8416):424–429. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="12681" citStr="Bojar et al., 2013" startWordPosition="1978" endWordPosition="1981">e proportion of aligned words between src and cand (ALGNs); (ii) the proportion of aligned words between cand and ref, calculated as the combination of the alignments src–cand and src–ref (ALGNr); and (iii) the ratio of shared alignments between srccand and src–ref (ALGNp). 4 Experimental Results The tuning and selection of the different metrics to build UPC-IPA and UPC-STOUT was 5https://code.google.com/p/berkeleyaligner −0.5 |dq |µ) o(d�) = e σ 396 conducted considering the WMT13 Metrics Task dataset (Mach´aˇcek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English tran</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Aligning Sentences in Parallel Corpora. In</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>169--176</pages>
<editor>Douglas E. Appelt, editor,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="11231" citStr="Brown et al., 1991" startWordPosition="1738" endWordPosition="1741">stimated on the WMT13 parallel corpora. pair µ Q pair µ Q en–cs 0.972 0.245 cs–en 1.085 0.273 en–de 1.176 0.926 de–en 0.961 0.463 en–fr 1.158 0.411 fr–en 0.914 0.313 en–ru 1.157 0.678 ru–en 1.069 0.668 The length factor (LeM) is rooted in the fact that the length of a text and its translation tend to preserve a certain length correlation. For instance, translations from English into Spanish or French tend to be longer than their source. Similar measures were proposed during the statistical machine translation early days, both considering characterand word-level lengths (Gale and Church, 1993; Brown et al., 1991). Pouliquen et al. (2003) defines the length factor as: , (3) where µ and Q represent the mean and standard deviation of the character lengths between translations of texts from L into L&apos;. This is a stochastic normal distribution that results in higher values as the length of the target text approaches the expected value given the source. Table 1 includes the values for each language pair, as estimated on the WMT13 parallel corpora. Note that this metric was not applied to Hindi–English since this language pair was not present in the WMT13 challenge. The last of our newly-added measures relies</context>
</contexts>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer. 1991. Aligning Sentences in Parallel Corpora. In Douglas E. Appelt, editor, Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics (ACL 1991), pages 169–176, Berkeley, CA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benot Crabb</author>
<author>Pascal Denis</author>
</authors>
<title>Statistical French dependency parsing: treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In The seventh international conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="6380" citStr="Candito et al., 2010" startWordPosition="957" endWordPosition="960">ted to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were previously available for English, Catalan, and Spanish. 3ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gi</context>
</contexts>
<marker>Candito, Crabb, Denis, 2010</marker>
<rawString>Marie Candito, Benot Crabb, and Pascal Denis. 2010a. Statistical French dependency parsing: treebank conversion and first results. In The seventh international conference on Language Resources and Evaluation (LREC), Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Joakim Nivre</author>
<author>Pascal Denis</author>
<author>Enrique Henestroza Anguiano</author>
</authors>
<title>Benchmarking of Statistical Dependency Parsers for French.</title>
<date>2010</date>
<booktitle>In Proc. 23rd Intl. COLING Conference on Computational Linguistics: Poster Volume,</booktitle>
<pages>108--116</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6380" citStr="Candito et al., 2010" startWordPosition="957" endWordPosition="960">ted to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were previously available for English, Catalan, and Spanish. 3ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gi</context>
</contexts>
<marker>Candito, Nivre, Denis, Anguiano, 2010</marker>
<rawString>Marie Candito, Joakim Nivre, Pascal Denis, and Enrique Henestroza Anguiano. 2010b. Benchmarking of Statistical Dependency Parsers for French. In Proc. 23rd Intl. COLING Conference on Computational Linguistics: Poster Volume, pages 108–116, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-Fine N-best Parsing and MaxEnt Discriminative Reranking.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6159" citStr="Charniak and Johnson (2005)" startWordPosition="925" endWordPosition="928">for each part of speech. We also use NIST (Doddington, 2002) to compute accumulated scores over sequences of n = 1..5 parts of speech (SP-pNIST). Similarly, CP metrics analyse similarities between constituent parse trees associated to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were previously available for English, Catalan, and Spanish. 3ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-Fine N-best Parsing and MaxEnt Discriminative Reranking. In Proc. 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on Statistical Machine Translation,</booktitle>
<pages>85--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12976" citStr="Denkowski and Lavie, 2011" startWordPosition="2028" endWordPosition="2031">l Results The tuning and selection of the different metrics to build UPC-IPA and UPC-STOUT was 5https://code.google.com/p/berkeleyaligner −0.5 |dq |µ) o(d�) = e σ 396 conducted considering the WMT13 Metrics Task dataset (Mach´aˇcek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). Table 2: Metrics considered in the experiments separa</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the 6th Workshop on Statistical Machine Translation, pages 85–91, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5592" citStr="Doddington, 2002" startWordPosition="844" endWordPosition="845">s in the ASIYA’s manual (Gonz`alez and Gim´enez, 2014). The manual describes every family of metrics in detail. Next, we briefly depict the concrete metrics involved in our submissions to the WMT14 Shared Task. The set of SP metrics is available for English, German, French, Spanish and Catalan. They measure the lexical overlapping between parts-ofspeech elements in the candidate and reference translations. For instance, SP-Op(VB) measures the proportion of correctly translated verbs; and the coarser SP-Op(*) averages the overlapping between the words for each part of speech. We also use NIST (Doddington, 2002) to compute accumulated scores over sequences of n = 1..5 parts of speech (SP-pNIST). Similarly, CP metrics analyse similarities between constituent parse trees associated to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were prev</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. In Proceedings of the 2nd International Conference on Human Language Technology, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7282" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1097" endWordPosition="1100">etween dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-3 computes the proportion of matching category-chains of length 3. 2.2 Explicit-Semantics Metric Additionally, we borrowed a metric originally proposed in the field of Information Retrieval: explicit semantic analysis (ESA) (Gabrilovich and Markovitch, 2007). ESA is a similarity metric that relies on a large corpus of general knowledge to represent texts. Our knowledge corpora are composed of ∼ 100K Wikipedia articles from 2010 for the following target languages: English, French and German. In this case, ref and cand translations are both mapped onto the Wikipedia collection W. The similarities between each text and every article a E W are computed on the basis of the cosine measure in order to compose a similarities vector that represents the text. That is: ~ref = {sim(ref, a) ba E W} ,(1) cand = {sim(cand, a) ba E W} . (2) As the i-th elements </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. In 20th International Joint Conference on Artificial Intelligence, pages 1606– 1611, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>W Church Kenneth</author>
</authors>
<title>A Program for Aligning Sentences</title>
<date>1993</date>
<booktitle>in Bilingual Corpora. Computational Linguistics,</booktitle>
<pages>19--75</pages>
<marker>Gale, Kenneth, 1993</marker>
<rawString>William A. Gale and Kenneth, W. Church. 1993. A Program for Aligning Sentences in Bilingual Corpora. Computational Linguistics, 19:75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems.</title>
<date>2007</date>
<booktitle>In Proc. of 2nd Workshop on statistical Machine Translation (WMT07), ACL’07,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proc. of 2nd Workshop on statistical Machine Translation (WMT07), ACL’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<pages>94--77</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010a. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics, 94:77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation,</title>
<date>2010</date>
<pages>24--3</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010b. Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation, 24(3–4):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
</authors>
<title>Empirical Machine Translation and its Evaluation.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Politcnica de Catalunya,</institution>
<marker>Gim´enez, 2008</marker>
<rawString>Jes´us Gim´enez. 2008. Empirical Machine Translation and its Evaluation. Ph.D. thesis, Universitat Politcnica de Catalunya, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meritxell Gonz`alez</author>
<author>Jes´us Gim´enez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation,</title>
<date>2014</date>
<location>v3.0,</location>
<note>http://asiya.lsi.upc.edu.</note>
<marker>Gonz`alez, Gim´enez, 2014</marker>
<rawString>Meritxell Gonz`alez and Jes´us Gim´enez. 2014. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation, v3.0, February. http://asiya.lsi.upc.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meritxell Gonz`alez</author>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>A Graphical Interface for MT Evaluation and Error Analysis. In</title>
<date>2012</date>
<booktitle>Proc. Annual Meeting of the Association for Computational Linguistics (ACL). System Demonstration,</booktitle>
<pages>139--144</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Jeju, South</location>
<marker>Gonz`alez, Gim´enez, M`arquez, 2012</marker>
<rawString>Meritxell Gonz`alez, Jes´us Gim´enez, and Llu´ıs M`arquez. 2012. A Graphical Interface for MT Evaluation and Error Analysis. In Proc. Annual Meeting of the Association for Computational Linguistics (ACL). System Demonstration, pages 139–144, Jeju, South Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meritxell Gonz`alez</author>
<author>Laura Mascarell</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>tSearch: Flexible and Fast Search over Automatic translation for Improved Quality/Error Analysis.</title>
<date>2013</date>
<booktitle>In Proc. 51st Annual Meeting of the Association for Computational Linguistics (ACL). System Demonstration,</booktitle>
<pages>181--186</pages>
<location>Sofia, Bulgaria,</location>
<marker>Gonz`alez, Mascarell, M`arquez, 2013</marker>
<rawString>Meritxell Gonz`alez, Laura Mascarell, and Llu´ıs M`arquez. 2013. tSearch: Flexible and Fast Search over Automatic translation for Improved Quality/Error Analysis. In Proc. 51st Annual Meeting of the Association for Computational Linguistics (ACL). System Demonstration, pages 181–186, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jaccard</author>
</authors>
<title>Etude comparative de la distribution florale dans une portion des Alpes et des Jura.</title>
<date>1901</date>
<booktitle>Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles,</booktitle>
<pages>37--547</pages>
<contexts>
<context position="8358" citStr="Jaccard, 1901" startWordPosition="1286" endWordPosition="1287">rities vector that represents the text. That is: ~ref = {sim(ref, a) ba E W} ,(1) cand = {sim(cand, a) ba E W} . (2) As the i-th elements in both ~ref and cand represent the similarity of ref and cand sentences to a common article, the similarity between ref and cand can be estimated by computing sim( ref, cand). 2.3 Language-Independent Resource-Free Metric We consider a simple characterisation based on word n-grams. Texts are broken down into overlapping word sequences of length n, with 1-word shifting. The similarity between cand and ref is computed on the basis of the Jaccard coefficient (Jaccard, 1901). We used this metric for the pairs English–Russian and Russian-English, considering n = 2 (NGRAM-jacTok2ngram). For the 395 rest of the pairs we opt for the character-n-gram metrics described in Section 3.1, but they showed no positive results in the English–Russian pair during our tuning experiments. 3 Source-based Metrics We enhance our evaluation module by including a set of new metrics that compare the source text against the translations. The metrics can be divided into two subsets: those that do not require any external resources (Section 3.1) and those that depend on a parallel corpus </context>
</contexts>
<marker>Jaccard, 1901</marker>
<rawString>Paul Jaccard. 1901. ´Etude comparative de la distribution florale dans une portion des Alpes et des Jura. Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles, 37:547–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5997" citStr="Liu and Gildea, 2005" startWordPosition="902" endWordPosition="905">lations. For instance, SP-Op(VB) measures the proportion of correctly translated verbs; and the coarser SP-Op(*) averages the overlapping between the words for each part of speech. We also use NIST (Doddington, 2002) to compute accumulated scores over sequences of n = 1..5 parts of speech (SP-pNIST). Similarly, CP metrics analyse similarities between constituent parse trees associated to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were previously available for English, Catalan, and Spanish. 3ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT par</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 25–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Mach´acek, Bojar, 2013</marker>
<rawString>Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of the WMT13 metrics shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>(to appear). Results of the WMT14 metrics shared task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimare, US,</location>
<marker>Mach´acek, Bojar, 2014</marker>
<rawString>Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2014 (to appear). Results of the WMT14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimare, US, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
<author>James Mayfield</author>
</authors>
<title>Character N-Gram Tokenization for European Language Text Retrieval. Information Retrieval,</title>
<date>2004</date>
<pages>7--1</pages>
<contexts>
<context position="9312" citStr="McNamee and Mayfield (2004)" startWordPosition="1432" endWordPosition="1435">We enhance our evaluation module by including a set of new metrics that compare the source text against the translations. The metrics can be divided into two subsets: those that do not require any external resources (Section 3.1) and those that depend on a parallel corpus (Section 3.2). 3.1 Language-Independent Resource-Free Metrics We opted for two characterisations that allow for the comparison of texts across languages without external resources nor language-related knowledge —as far as the languages use the same writing system.4 The first characterisation is character n-grams; proposed by McNamee and Mayfield (2004) for cross-language information retrieval between European languages. Texts are broken down into overlapping character sequences of length n, with 1-character shifting. We opt for case-folded bigrams (NGRAM-cosChar2ngrams), as they allowed for the best performance across all the pairs (except for From/To Russian pairs) during tuning. The second characterisation (NGRAMjacCognates) is based on the concept of cognateness; originally proposed for bitexts alignment (Simard et al., 1992). A word is a pseudo-cognate candidate if (i) it has only letters and |w |≥ 4, (ii) it contains at least one digit</context>
</contexts>
<marker>McNamee, Mayfield, 2004</marker>
<rawString>Paul McNamee and James Mayfield. 2004. Character N-Gram Tokenization for European Language Text Retrieval. Information Retrieval, 7(1-2):73–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>61--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13004" citStr="Melamed et al., 2003" startWordPosition="2033" endWordPosition="2036">n of the different metrics to build UPC-IPA and UPC-STOUT was 5https://code.google.com/p/berkeleyaligner −0.5 |dq |µ) o(d�) = e σ 396 conducted considering the WMT13 Metrics Task dataset (Mach´aˇcek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). Table 2: Metrics considered in the experiments separated by families according to</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 61–63, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="13104" citStr="Nießen et al., 2000" startWordPosition="2051" endWordPosition="2054">ner −0.5 |dq |µ) o(d�) = e σ 396 conducted considering the WMT13 Metrics Task dataset (Mach´aˇcek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). Table 2: Metrics considered in the experiments separated by families according to the type of grammatical items they use. 1. -WER 17. DPm-HWCM r-1 2. -PER 18. DPm-Or(*) 3. -TERp-A 1</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="6621" citStr="Nivre et al., 2007" startWordPosition="997" endWordPosition="1000">Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were previously available for English, Catalan, and Spanish. 3ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-3 computes the proportion of matching category-chains of length 3. 2.2 Explicit-Semantics Metric Additionally, we borrowed a metric originally proposed in the field of Information Retrieval: explic</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. MaltParser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In</title>
<date>2007</date>
<booktitle>Proc. Human Language Technologies (HLT),</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="6449" citStr="Petrov and Klein, 2007" startWordPosition="969" endWordPosition="972">5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were previously available for English, Catalan, and Spanish. 3ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-3 computes the proportion</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Proc. Human Language Technologies (HLT), pages 404–411. Association for Computational Linguistics, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6424" citStr="Petrov et al., 2006" startWordPosition="965" endWordPosition="968">For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree (Liu and Gildea, 2005). CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of Charniak and Johnson (2005), 2Equivalent resources were previously available for English, Catalan, and Spanish. 3ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. Bonsai v3.2 (Candito et al., 2010b), and Berkeley Parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, French, and German, respectively. Measures based on dependency parsing (DPm) — available for English and French thanks to the MALT parser (Nivre et al., 2007)— capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank (Candito et al., 2010a) and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains (Liu and Gildea, 2005) up to length 3; and DPm-HWCMi c-</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 433–440, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Ralf Steinberger</author>
<author>Camelia Ignat</author>
</authors>
<title>Automatic Identification of Document Translations in Large Multilingual Document Collections.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-2003),</booktitle>
<pages>401--408</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="11256" citStr="Pouliquen et al. (2003)" startWordPosition="1742" endWordPosition="1745"> parallel corpora. pair µ Q pair µ Q en–cs 0.972 0.245 cs–en 1.085 0.273 en–de 1.176 0.926 de–en 0.961 0.463 en–fr 1.158 0.411 fr–en 0.914 0.313 en–ru 1.157 0.678 ru–en 1.069 0.668 The length factor (LeM) is rooted in the fact that the length of a text and its translation tend to preserve a certain length correlation. For instance, translations from English into Spanish or French tend to be longer than their source. Similar measures were proposed during the statistical machine translation early days, both considering characterand word-level lengths (Gale and Church, 1993; Brown et al., 1991). Pouliquen et al. (2003) defines the length factor as: , (3) where µ and Q represent the mean and standard deviation of the character lengths between translations of texts from L into L&apos;. This is a stochastic normal distribution that results in higher values as the length of the target text approaches the expected value given the source. Table 1 includes the values for each language pair, as estimated on the WMT13 parallel corpora. Note that this metric was not applied to Hindi–English since this language pair was not present in the WMT13 challenge. The last of our newly-added measures relies on the word alignments c</context>
</contexts>
<marker>Pouliquen, Steinberger, Ignat, 2003</marker>
<rawString>Bruno Pouliquen, Ralf Steinberger, and Camelia Ignat. 2003. Automatic Identification of Document Translations in Large Multilingual Document Collections. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-2003), pages 401–408, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>George F Foster</author>
<author>Pierre Isabelle</author>
</authors>
<title>Using Cognates to Align Sentences in Bilingual Corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation.</booktitle>
<contexts>
<context position="9798" citStr="Simard et al., 1992" startWordPosition="1501" endWordPosition="1504">s the languages use the same writing system.4 The first characterisation is character n-grams; proposed by McNamee and Mayfield (2004) for cross-language information retrieval between European languages. Texts are broken down into overlapping character sequences of length n, with 1-character shifting. We opt for case-folded bigrams (NGRAM-cosChar2ngrams), as they allowed for the best performance across all the pairs (except for From/To Russian pairs) during tuning. The second characterisation (NGRAMjacCognates) is based on the concept of cognateness; originally proposed for bitexts alignment (Simard et al., 1992). A word is a pseudo-cognate candidate if (i) it has only letters and |w |≥ 4, (ii) it contains at least one digit, or (iii) it is a single punctuation mark. src and cand sentences are then represented as word vectors, containing only those words fulfilling one of the previous conditions. In the case of (i) the word is cut down to its leading four characters only. In both cases (character n-grams and cognateness) cand translations are compared against src sentences on the basis of the cosine similarity measure. 3.2 Parallel-Corpus Metrics We consider two metrics that make use of parallel corpo</context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>Michel Simard, George F. Foster, and Pierre Isabelle. 1992. Using Cognates to Align Sentences in Bilingual Corpora. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Machine Translation,</booktitle>
<pages>259--268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13035" citStr="Snover et al., 2009" startWordPosition="2038" endWordPosition="2041">ild UPC-IPA and UPC-STOUT was 5https://code.google.com/p/berkeleyaligner −0.5 |dq |µ) o(d�) = e σ 396 conducted considering the WMT13 Metrics Task dataset (Mach´aˇcek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). Table 2: Metrics considered in the experiments separated by families according to the type of grammatical items </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of the 4th Workshop on Statistical Machine Translation, pages 259– 268, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based Search for Statistical Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology,</booktitle>
<pages>2667--2670</pages>
<contexts>
<context position="13136" citStr="Tillmann et al., 1997" startWordPosition="2057" endWordPosition="2061">96 conducted considering the WMT13 Metrics Task dataset (Mach´aˇcek and Bojar, 2013), and the resources distributed for the WMT13 Translation Task (Bojar et al., 2013). Table 2 gives a complete list of these metrics grouped by families. First, we calculated the Pearson’s correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2011), GTM (Melamed et al., 2003), -TERp-A (Snover et al., 2009) (a variant of TER tuned towards adequacy), WER (Nießen et al., 2000) and PER (Tillmann et al., 1997). We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures —even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed6 (see Table 3). Table 2: Metrics considered in the experiments separated by families according to the type of grammatical items they use. 1. -WER 17. DPm-HWCM r-1 2. -PER 18. DPm-Or(*) 3. -TERp-A 19. SR-Or(*) 4. METEOR-ex 20. SR-</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based Search for Statistical Translation. In Proceedings of European Conference on Speech Communication and Technology, pages 2667–2670.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
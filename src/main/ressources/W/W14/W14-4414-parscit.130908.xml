<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.058046">
<title confidence="0.976193">
Two-Stage Stochastic Email Synthesizer
</title>
<author confidence="0.994161">
Yun-Nung Chen and Alexander I. Rudnicky
</author>
<affiliation confidence="0.8121775">
School of Computer Science, Carnegie Mellon University
5000 Forbes Ave., Pittsburgh, PA 15213-3891, USA
</affiliation>
<email confidence="0.996743">
{yvchen, air}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993831" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998685">
This paper presents the design and im-
plementation details of an email synthe-
sizer using two-stage stochastic natural
language generation, where the first stage
structures the emails according to sender
style and topic structure, and the second
stage synthesizes text content based on the
particulars of an email structure element
and the goals of a given communication
for surface realization. The synthesized
emails reflect sender style and the intent of
communication, which can be further used
as synthetic evidence for developing other
applications.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999159542857143">
This paper focuses on synthesizing emails that re-
flect sender style and the intent of the communica-
tion. Such a process might be used for the gener-
ation of common messages (for example a request
for a meeting without direct intervention from the
sender). It can also be used in situations where nat-
uralistic emails are needed for other applications.
For instance, our email synthesizer was developed
to provide emails to be used as part of synthetic
evidence of insider threats for purposes of train-
ing, prototyping, and evaluating anomaly detec-
tors (Hershkop et al., 2011).
Oh and Rudnicky (2002) showed that stochas-
tic generation benefits from two factors: 1) it
takes advantage of the practical language of a do-
main expert instead of the developer and 2) it re-
states the problem in terms of classification and
labeling, where expertise is not required for de-
veloping a rule-based generation system. In the
present work we investigate the use of stochastic
techniques for generation of a different class of
communications and whether global structures can
be convincingly created. Specifically we inves-
tigate whether stochastic techniques can be used
to acceptably model longer texts and individual
sender characteristics in the email domain, both of
which may require higher cohesion to be accept-
able (Chen and Rudnicky, 2014).
Our proposed system involves two-stage
stochastic generation, shown in Figure 1, in which
the first stage models email structures according
to sender style and topic structure (high-level
generation), and the second stage synthesizes
text content based on the particulars of a given
communication (surface-level generation).
</bodyText>
<sectionHeader confidence="0.935554" genericHeader="method">
2 The Proposed System
</sectionHeader>
<bodyText confidence="0.999952782608696">
The whole architecture of the proposed system is
shown in left part of Figure 1, which is composed
of preprocessing, first-stage generation for email
organization, and second-stage generation for sur-
face realization.
In preprocessing, we perform sentence segmen-
tation for each email, and then manually anno-
tate each sentence with a structure element, which
is used to create a structural label sequence for
each email and then to model sender style and
topic structure for email organization (1st stage in
the figure). The defined structural labels include
greeting, inform, request, suggestion, question,
answer, regard, acknowledgement, sorry, and sig-
nature. We also annotate content slots, including
general classes automatically created by named
entity recognition (NER) (Finkel et al., 2005) and
hand-crafted topic classes, to model text content
for surface realization (2nd stage in the figure).
The content slots include person, organization, lo-
cation, time, money, percent, and date (general
classes), and meeting, issue, and discussion (topic
classes).
</bodyText>
<subsectionHeader confidence="0.997497">
2.1 Modeling Sender Style and Topic
Structure for Email Organization
</subsectionHeader>
<bodyText confidence="0.99975825">
In the first stage, given the sender and the fo-
cused topic from the input, we generate the email
structures by predicted sender-topic-specific mix-
ture models, where the detailed is illustrated as be-
</bodyText>
<page confidence="0.947855">
99
</page>
<bodyText confidence="0.6097675">
Proceedings of the 8th International Natural Language Generation Conference, pages 99–102,
Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics
</bodyText>
<figure confidence="0.998573784313726">
2nd Stage Generation
1st Stage Generation
System Input Preprocessing
Building
Content
LM
Slot-Value Pairs
Sender Topic
Building
Structure
LM
Generating
Text
Content
Sender
Model
Topic
Model
Scoring
Email
Candidates
Structural Label Annotation
Predicting
Mixture
Models
Structural Label Sequences
Synthesized Emails
with Slots
Hi [person]
Today’s ...
Generating
Email
Structures
Email Archive
Filling
Slots
Generated Structural
Label Sequences
Emails w/ Slots
&lt;greeting&gt;
&lt;inform&gt;
Synthesized Emails
Slot Annotation
Hi Peter
Today’s ...
Request is generated at
the narrative level.
Form filling:
• Topic Model
• Sender Model
• Slot fillers
</figure>
<figureCaption confidence="0.999987">
Figure 1: The system architecture (left) and the demo synthesizer (right).
</figureCaption>
<bodyText confidence="0.75882">
low.
</bodyText>
<subsectionHeader confidence="0.974907">
2.1.1 Building Structure Language Models
</subsectionHeader>
<bodyText confidence="0.999940875">
Based on the annotation of structural labels, each
email can be transformed into a structural label
sequence. Then we train a sender-specific struc-
ture model using the emails from each sender and
a topic-specific model using the emails related to
each topic. Here the structure models are tri-
gram models with Good-Turing smoothing (Good,
1953).
</bodyText>
<subsectionHeader confidence="0.907245">
2.1.2 Predicting Mixture Models
</subsectionHeader>
<bodyText confidence="0.999975">
With sender-specific and topic-specific structure
models, we predict the sender-topic-specific mix-
ture models by interpolating the probabilities of
two models.
</bodyText>
<subsectionHeader confidence="0.939774">
2.1.3 Generating Email Structures
</subsectionHeader>
<bodyText confidence="0.999996285714286">
We generate structural label sequences randomly
according to the distribution from sender-topic-
specific models. Smoothed trigram models may
generate any unseen trigrams based on back-off
methods, resulting in more randomness. In ad-
dition, we exclude unreasonable emails that don’t
follow two simple rules.
</bodyText>
<listItem confidence="0.931142">
1. The structural label “greeting” only occurs at
the beginning of the email.
2. The structural label “signature” only occurs
at the end of the email.
</listItem>
<subsectionHeader confidence="0.998749">
2.2 Surface Realization
</subsectionHeader>
<bodyText confidence="0.999877">
In the second stage, our surface realizer consists
of four aspects: building content language models,
generating text content, scoring email candidates,
and filling slots.
</bodyText>
<subsectionHeader confidence="0.82335">
2.2.1 Building Content Language Models
</subsectionHeader>
<bodyText confidence="0.9999704">
After replacing the tokens with the slots, for each
structural label, we train an unsmoothed 5-gram
language model using all sentences belonging to
the structural label. Here we assume that the usage
of within-sentence language is independent across
senders and topics, so generating the text content
only considers the structural labels. Unsmoothed
5-gram language models introduce some variabil-
ity in the output sentences while preventing non-
sense sentences.
</bodyText>
<subsectionHeader confidence="0.881921">
2.2.2 Generating Text Content
</subsectionHeader>
<bodyText confidence="0.999853833333333">
The input to surface realization is the generated
structural label sequences. We use the correspond-
ing content language model for the given struc-
tural label to generate word sequences randomly
according to distribution from the language model.
Using unsmoothed 5-grams will not generate
any unseen 5-grams (or smaller n-grams at the
beginning and end of a sentence), avoiding gen-
eration of nonsense sentences within the 5-word
window. With a structural label sequence, we can
generate multiple sentences to form a synthesized
email.
</bodyText>
<page confidence="0.980118">
100
</page>
<subsectionHeader confidence="0.998933">
2.3 Scoring Email Candidates
</subsectionHeader>
<bodyText confidence="0.999817888888889">
The input to the system contains the required in-
formation that should be included in the synthe-
sized result. For each synthesized email, we penal-
ize it if the email 1) contains slots for which there
is no provided valid value, or 2) does not have
the required slots. The content generation engine
stochastically generates a candidate email, scores
it, and outputs it when the synthesized email with
a zero penalty score.
</bodyText>
<subsectionHeader confidence="0.999415">
2.4 Filling Slots
</subsectionHeader>
<bodyText confidence="0.999976444444445">
The last step is to fill slots with the appropriate
values. For example, the sentence “Tomorrow’s
[meeting] is at [location].” becomes “Tomorrow’s
speech seminar is at Gates building.” The right
part of Figure 1 shows the process of the demo sys-
tem,where based on a specific topic, a sender, and
an interpolation weight, the system synthesizes an
email with structural labels first and then fills slots
with given slot fillers.
</bodyText>
<sectionHeader confidence="0.998916" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999984416666667">
We conduct a preliminary experiment to evaluate
the proposed system. The corpus used for our ex-
periments is the Enron Email Dataset1, which con-
tains a total of about 0.5M messages. We selected
the data related to daily business for our use. This
includes data from about 150 users, and we ran-
domly picked 3 senders, ones who wrote many
emails, and define additional 3 topic classes (meet-
ing, discussion, issue) as topic-specific entities
for the task. Each sender-specific model (across
topics) or topic-specific model (across senders) is
trained on 30 emails.
</bodyText>
<subsectionHeader confidence="0.999944">
3.1 Evaluation of Sender Style Modeling
</subsectionHeader>
<bodyText confidence="0.999941357142857">
To evaluate the performance of sender style, 7 sub-
jects were given 5 real emails from each sender
and then 9 synthesized emails. They were asked
to rate each synthesized email for each sender on
a scale between 1 to 5.
With higher weight for sender-specific model
when predicting mixture models, average normal-
ized scores the corresponding senders receives ac-
count for 45%, which is above chance (33%). This
suggests that sender style can be noticed by sub-
jects. In a follow-up questionnaire, subjects indi-
cated that their ratings were based on greeting us-
age, politeness, the length of email and other char-
acteristics.
</bodyText>
<footnote confidence="0.977532">
1https://www.cs.cmu.edu/˜enron/
</footnote>
<subsectionHeader confidence="0.994281">
3.2 Evaluation of Surface Realization
</subsectionHeader>
<bodyText confidence="0.9999858">
We conduct a comparative evaluation of two
different generation algorithms, template-based
generation and stochastic generation, on the
same email structures. Given a structural label,
template-based generation consisted of randomly
selecting an intact whole sentence with the target
structural label. This could be termed sentence-
level NLG, while stochastic generation is word-
level NLG.
We presented 30 pairs of (sentence-, word-)
synthesized emails, and 7 subjects were asked to
compare the overall coherence of an email, its
sentence fluency and naturalness; then select their
preference. The experiments showed that word-
based stochastic generation outperforms or per-
forms as well as the template-based algorithm
for all criteria (coherence, fluency, naturalness,
and preference). Some subjects noted that nei-
ther email seemed human-written, perhaps an ar-
tifact of our experimental design. Nevertheless,
we believe that this stochastic approach would re-
quire less effort compared to most rule-based or
template-based systems in terms of knowledge en-
gineering.
In the future, we plan to develop an automatic
email structural label annotator in order to build
better language models (structure language mod-
els and content language models) by increasing
training data, and then improve the naturalness of
synthesized emails.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999993857142857">
This paper illustrates a design and implementation
of an email synthesizer with two-stage stochastic
NLG: first a structure is generated, and then text is
generated for each structure element. Here sender
style and topic structure can be modeled. We be-
lieve that this system can be applied to create re-
alistic emails and could be carried out using mix-
tures containing additional models based on other
characteristics. The proposed system shows that
emails can be synthesized using a small corpus of
labeled data, and the performance seems accept-
able; however these models could be used to boot-
strap the labeling of a larger corpus which in turn
could be used to create more robust models.
</bodyText>
<sectionHeader confidence="0.998844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99825675">
The authors wish to thank Brian Lindauer and Kurt
Wallnau from the Software Engineering Institute
of Carnegie Mellon University for their guidance,
advice, and help.
</bodyText>
<page confidence="0.998583">
101
</page>
<sectionHeader confidence="0.993948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.940930681818182">
Yun-Nung Chen and Alexander I. Rudnicky. 2014.
Two-stage stochastic natural language generation for
email synthesis by modeling sender style and topic
structure. In Proceedings of the 8th International
Natural Language Generation Conference.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Irving J Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3-4):237–264.
Shlomo Hershkop, Salvatore J Stolfo, Angelos D
Keromytis, and Hugh Thompson. 2011. Anomaly
detection at multiple scales (ADAMS).
Alice H Oh and Alexander I Rudnicky. 2002.
Stochastic natural language generation for spoken
dialog systems. Computer Speech &amp; Language,
16(3):387–407.
</reference>
<page confidence="0.998611">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978061">
<title confidence="0.998908">Two-Stage Stochastic Email Synthesizer</title>
<author confidence="0.998642">Yun-Nung Chen</author>
<author confidence="0.998642">I Alexander</author>
<affiliation confidence="0.999551">School of Computer Science, Carnegie Mellon</affiliation>
<address confidence="0.999621">5000 Forbes Ave., Pittsburgh, PA 15213-3891,</address>
<abstract confidence="0.998735866666667">This paper presents the design and implementation details of an email synthesizer using two-stage stochastic natural language generation, where the first stage structures the emails according to sender style and topic structure, and the second stage synthesizes text content based on the particulars of an email structure element and the goals of a given communication for surface realization. The synthesized emails reflect sender style and the intent of communication, which can be further used as synthetic evidence for developing other applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Two-stage stochastic natural language generation for email synthesis by modeling sender style and topic structure.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Natural Language Generation Conference.</booktitle>
<contexts>
<context position="2115" citStr="Chen and Rudnicky, 2014" startWordPosition="324" endWordPosition="327">e of a domain expert instead of the developer and 2) it restates the problem in terms of classification and labeling, where expertise is not required for developing a rule-based generation system. In the present work we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual sender characteristics in the email domain, both of which may require higher cohesion to be acceptable (Chen and Rudnicky, 2014). Our proposed system involves two-stage stochastic generation, shown in Figure 1, in which the first stage models email structures according to sender style and topic structure (high-level generation), and the second stage synthesizes text content based on the particulars of a given communication (surface-level generation). 2 The Proposed System The whole architecture of the proposed system is shown in left part of Figure 1, which is composed of preprocessing, first-stage generation for email organization, and second-stage generation for surface realization. In preprocessing, we perform sente</context>
</contexts>
<marker>Chen, Rudnicky, 2014</marker>
<rawString>Yun-Nung Chen and Alexander I. Rudnicky. 2014. Two-stage stochastic natural language generation for email synthesis by modeling sender style and topic structure. In Proceedings of the 8th International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3260" citStr="Finkel et al., 2005" startWordPosition="491" endWordPosition="494">tage generation for surface realization. In preprocessing, we perform sentence segmentation for each email, and then manually annotate each sentence with a structure element, which is used to create a structural label sequence for each email and then to model sender style and topic structure for email organization (1st stage in the figure). The defined structural labels include greeting, inform, request, suggestion, question, answer, regard, acknowledgement, sorry, and signature. We also annotate content slots, including general classes automatically created by named entity recognition (NER) (Finkel et al., 2005) and hand-crafted topic classes, to model text content for surface realization (2nd stage in the figure). The content slots include person, organization, location, time, money, percent, and date (general classes), and meeting, issue, and discussion (topic classes). 2.1 Modeling Sender Style and Topic Structure for Email Organization In the first stage, given the sender and the focused topic from the input, we generate the email structures by predicted sender-topic-specific mixture models, where the detailed is illustrated as be99 Proceedings of the 8th International Natural Language Generation</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irving J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<pages>40--3</pages>
<contexts>
<context position="5083" citStr="Good, 1953" startWordPosition="756" endWordPosition="757">Emails Slot Annotation Hi Peter Today’s ... Request is generated at the narrative level. Form filling: • Topic Model • Sender Model • Slot fillers Figure 1: The system architecture (left) and the demo synthesizer (right). low. 2.1.1 Building Structure Language Models Based on the annotation of structural labels, each email can be transformed into a structural label sequence. Then we train a sender-specific structure model using the emails from each sender and a topic-specific model using the emails related to each topic. Here the structure models are trigram models with Good-Turing smoothing (Good, 1953). 2.1.2 Predicting Mixture Models With sender-specific and topic-specific structure models, we predict the sender-topic-specific mixture models by interpolating the probabilities of two models. 2.1.3 Generating Email Structures We generate structural label sequences randomly according to the distribution from sender-topicspecific models. Smoothed trigram models may generate any unseen trigrams based on back-off methods, resulting in more randomness. In addition, we exclude unreasonable emails that don’t follow two simple rules. 1. The structural label “greeting” only occurs at the beginning of</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Irving J Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3-4):237–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Hershkop</author>
<author>Salvatore J Stolfo</author>
<author>Angelos D Keromytis</author>
<author>Hugh Thompson</author>
</authors>
<title>Anomaly detection at multiple scales (ADAMS).</title>
<date>2011</date>
<contexts>
<context position="1359" citStr="Hershkop et al., 2011" startWordPosition="204" endWordPosition="207">or developing other applications. 1 Introduction This paper focuses on synthesizing emails that reflect sender style and the intent of the communication. Such a process might be used for the generation of common messages (for example a request for a meeting without direct intervention from the sender). It can also be used in situations where naturalistic emails are needed for other applications. For instance, our email synthesizer was developed to provide emails to be used as part of synthetic evidence of insider threats for purposes of training, prototyping, and evaluating anomaly detectors (Hershkop et al., 2011). Oh and Rudnicky (2002) showed that stochastic generation benefits from two factors: 1) it takes advantage of the practical language of a domain expert instead of the developer and 2) it restates the problem in terms of classification and labeling, where expertise is not required for developing a rule-based generation system. In the present work we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created. Specifically we investigate whether stochastic techniques can be used to acceptably model </context>
</contexts>
<marker>Hershkop, Stolfo, Keromytis, Thompson, 2011</marker>
<rawString>Shlomo Hershkop, Salvatore J Stolfo, Angelos D Keromytis, and Hugh Thompson. 2011. Anomaly detection at multiple scales (ADAMS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice H Oh</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Stochastic natural language generation for spoken dialog systems.</title>
<date>2002</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="1383" citStr="Oh and Rudnicky (2002)" startWordPosition="208" endWordPosition="211">ications. 1 Introduction This paper focuses on synthesizing emails that reflect sender style and the intent of the communication. Such a process might be used for the generation of common messages (for example a request for a meeting without direct intervention from the sender). It can also be used in situations where naturalistic emails are needed for other applications. For instance, our email synthesizer was developed to provide emails to be used as part of synthetic evidence of insider threats for purposes of training, prototyping, and evaluating anomaly detectors (Hershkop et al., 2011). Oh and Rudnicky (2002) showed that stochastic generation benefits from two factors: 1) it takes advantage of the practical language of a domain expert instead of the developer and 2) it restates the problem in terms of classification and labeling, where expertise is not required for developing a rule-based generation system. In the present work we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individ</context>
</contexts>
<marker>Oh, Rudnicky, 2002</marker>
<rawString>Alice H Oh and Alexander I Rudnicky. 2002. Stochastic natural language generation for spoken dialog systems. Computer Speech &amp; Language, 16(3):387–407.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
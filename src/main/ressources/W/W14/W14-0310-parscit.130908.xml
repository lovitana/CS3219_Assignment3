<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007428">
<title confidence="0.987144">
The ACCEPT Portal: An Online Framework for the Pre-editing and
Post-editing of User-Generated Content
</title>
<note confidence="0.50128575">
Violeta Seretan Johann Roturier David Silva Pierrette Bouillon
FTI/TIM Symantec Ltd. Symantec Ltd. FTI/TIM
University of Geneva Dublin, Ireland Dublin, Ireland University of Geneva
Switzerland johann roturier@symantec.com David Silva@symantec.com Switzerland
</note>
<email confidence="0.694002">
Violeta.Seretan@unige.ch Pierrette.Bouillon@unige.ch
</email>
<sectionHeader confidence="0.98661" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968444444445">
With the development of Web 2.0, a
lot of content is nowadays generated on-
line by users. Due to its characteristics
(e.g., use of jargon and abbreviations, ty-
pos, grammatical and style errors), the
user-generated content poses specific chal-
lenges to machine translation. This pa-
per presents an online platform devoted to
the pre-editing of user-generated content
and its post-editing, two main types of hu-
man assistance strategies which are com-
bined with domain adaptation and other
techniques in order to improve the trans-
lation of this type of content. The plat-
form has recently been released publicly
and is being tested by two main types of
user communities, namely, technical fo-
rum users and volunteer translators.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999833733333333">
User-generated content – i.e., information posted
by Internet users in social communication chan-
nels like blogs, forum posts, social networks – is
one of the main sources of information available
today. Huge volumes of such content are created
each day, reach a very broad audience instantly.1
The democratisation of content creation due
to the emergence of the Web 2.0 paradigm also
means a diversification of the languages used on
the Internet.2 Despite its availability, the new con-
tent is only accessible to the speakers of the lan-
guage in which it was created. The automatic
translation of user-generated content is therefore
one of the key issues to be addressed in the field of
human language technologies. However, as stated
</bodyText>
<footnote confidence="0.9682222">
1For instance, 58 million tweets are sent on aver-
age per day (http://www.statisticbrain.com/
twitter-statistics/).
2See http://en.wikipedia.org/wiki/
Languages-used-on-the-Internet for statistics.
</footnote>
<bodyText confidence="0.985849785714285">
by Jiang et al. (2012), despite the obvious bene-
fits, there are relatively little attempts at translating
user-generated content.
The reason may lie in the fact that user-ge-
nerated content is very challenging for machine
translation. As shown, among others, by Nagara-
jan and Gamon (2011), there are several charac-
teristics of this content that pose new process-
ing challenges with respect to traditional content:
informal style, slang, abbreviations, specific ter-
minology, irregular grammar and spelling. In-
deed, Internet users are rarely professional writ-
ers.3 They often write in a language which is not
their own, and sacrifice quality for speed, not pay-
ing attention to spelling, punctuation, or grammar
rules.
The ACCEPT project4 addresses these chal-
lenges by developing a technology integrating
modules for automatic and manual content pre-
editing, statistical machine translation, as well
as output evaluation and post-editing. Thus, the
project aims to improve the translation of user-ge-
nerated content by proposing a full workflow, in
which the participation of humans is essential.
The application scenario considered in the
project are user communities sharing specific in-
formation on a given topic. The project focuses,
more specifically, on the following use cases:
</bodyText>
<listItem confidence="0.982345111111111">
1. the commercial use case, in which the tar-
get community is the user community built
around a software company in order for
members to help each other with issues re-
lated to products;
2. the NGO use case, in which non-go-
vernmental organisations such as Doctors
Without Borders produce health-care content
for distributions in areas of need.
</listItem>
<footnote confidence="0.9944655">
3Even when they are, as in the case of government agen-
cies, the type of content produced (e.g., tweets) still poses
“multiple challenges” to translation (Gotti et al., 2013).
4http://www.accept-project.eu/
</footnote>
<page confidence="0.918198">
66
</page>
<bodyText confidence="0.963900107142857">
Workshop on Humans and Computer-assisted Translation, pages 66–71,
Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics
The language pairs considered in the project are
English to French, German and Japanese, as well
as French into English for the first use case (in-
volving technical forum information), and French
to and from English for the second use case (in-
volving healthcare information).
Past halfway into its research program, the
project has accomplished significant progress in
the main areas mentioned above (pre-editing, sta-
tistical machine translation, post-editing, and eval-
uation). The ACCEPT technology has recently
been released to the broad public as an on-
line framework, which demonstrates the different
modules of the workflow and provides access to
associated software components (plug-ins, APIs),
as well as to documentation. The pre-editing tech-
nology has been deployed on the targeted user fo-
rum5, allowing users to check their messages be-
fore posting them. The post-editing technology is
being used by a community of translators, which
provide pro-bono translation services to the NGOs
considered in our second use case.
In this paper, we describe the framework by pre-
senting its architecture and main modules (Sec-
tion 2). We discuss related work in Section 3 and
conclude in Section 4.
</bodyText>
<sectionHeader confidence="0.985793" genericHeader="introduction">
2 The Framework
</sectionHeader>
<bodyText confidence="0.99994775">
The ACCEPT technology has been made acces-
sible to a broad audience in the form of an on-
line framework, i.e., an integrated environment
where registered users can perform pre-editing,
post-editing and evaluation work. The framework
– henceforth, the ACCEPT Portal – is hosted on a
cloud computing infrastructure and is available at
www.accept-portal.eu.
</bodyText>
<subsectionHeader confidence="0.999755">
2.1 Architecture of the Framework
</subsectionHeader>
<bodyText confidence="0.9942815">
As explained in Section 1, the ACCEPT techno-
logy consists of the following main modules:
</bodyText>
<listItem confidence="0.9991045">
1. Pre-editing module;
2. Machine translation module,
3. Post-editing module,
4. Evaluation module.
</listItem>
<bodyText confidence="0.9956375">
The typical workflow is incremental, but the
modules are independent. They can be used both
within and outside the portal, as they are built on a
REST API facilitating integration.
</bodyText>
<footnote confidence="0.783673">
5https://community.norton.com/
</footnote>
<bodyText confidence="0.9997045">
In the remaining of this section, we introduce
each of the framework modules.6
</bodyText>
<subsectionHeader confidence="0.998309">
2.2 Pre-editing Module
</subsectionHeader>
<bodyText confidence="0.999981511111111">
The pre-editing module leverages existing ling-
ware which provides authoring support rules
aimed at language professionals, by relying on
shallow language processing (Bredenkamp et al.,
2000). The existing English checker and the lin-
guistic resources on which it relies have been ex-
tended and adapted to suit the type of data gener-
ated by community users. In particular, the soft-
ware extension consisted of designing a number
of pre-editing rules aimed at source normalisation,
for the purpose of making the input text easier
to handle by the SMT systems. In the case of
French, the pre-editing rules have been designed
from scratch. The pre-editing rules pertain to the
levels of spelling, grammar, style and terminology.
They are defined using the original lingware’s rule
formalism and are incorporated into a server dedi-
cated to the project.
The rule development was corpus-driven and
was performed on data collected for this purpose.
A stable set of pre-edition rules is available in
the portal for each of the domains and source
languages considered (i.e., technical forum and
heathcare data in English and French). The rules
are described in detail in the project deliverable
D 2.2 (2013).
The rules proposed have been evaluated individ-
ually and in combination (Roturier et al., 2012;
Gerlach et al., 2013; Seretan et al., 2014). As
a general observation, it is important to notice
that, for SMT, the improvement of the input text
does not go hand in hand with the improvement of
translation. For example, in French the rule for
correcting verbal forms to the subjunctive tense
had a negative impact since the subjunctive is not
frequent in the training data. Conversely, it was
possible to define lexical reformulations which de-
graded the quality of the input text, but had a po-
sitive impact on translation quality.
The combined impact of the rule applica-
tion was measured in a variety of settings in a
large-scale evaluation campaign involving transla-
tion students (Seretan et al., 2014). As the rules
are divided into two major groups, those automati-
cally applicable and those requiring human inter-
</bodyText>
<footnote confidence="0.995672">
6The MT module will be omitted, as it is not part of the
portal. The interested reader is referred to D 4.2 (2013).
</footnote>
<page confidence="0.998907">
67
</page>
<figureCaption confidence="0.999933">
Figure 1: The ACCEPT Pre-edit plug-in in action (screen capture)
</figureCaption>
<bodyText confidence="0.999610222222222">
vention, the evaluation was carried out for the full
set of rules, as well as for the automatic rules only.
In addition, the evaluation was performed in both
a monolingual and a bilingual setting, i.e., with
the evaluators having or not access to the source
text, and it involved evaluation scales of different
granularities. The evaluation results showed a sys-
tematic statistically significant improvement over
the baseline when pre-editing is performed on the
source content. More details about the evalua-
tion methodology and results can be found in the
project deliverable D 9.2.2 (2013).
A data excerpt illustrating the impact of pre-
editing on translation quality is presented in Ex-
ample 1 below. The simple correction of an ac-
cented letter, du → dfu, leads to the change of seve-
ral target words, and to a much better translation of
the input sentence.
</bodyText>
<listItem confidence="0.702587285714286">
1. a) Source (original):
J’ai du m’absenter hier apr`es midi.
b) Source (pre-edited):
J’ai dfu m’absenter hier apr`es midi.
c) Target (original):
I have the leave me yesterday afternoon.
d) Target (pre-edited):
</listItem>
<bodyText confidence="0.995907058823529">
I had to leave yesterday afternoon.
The pre-editing component of the ACCEPT
technology is available as a JQuery plug-in, which
can be downloaded and installed by Web applica-
tion owners, so that it can be used with text areas
and other text-bearing elements. APIs and ac-
companying documentation have also been made
available, so that the pre-editing rules can be
leveraged in automatic steps, without the plug-in,
across devices and platforms. A demo site illus-
trating the use of the plug-in in a TinyMCE envi-
ronment is available on the portal (see Figure 1).
The latest developments of the pre-editing mo-
dule include the possibility for users to customise
the application of rule sets, in particular, to ignore
specific rules and to manage their own dictionary,
in order to prevent the activation of checking flags.
</bodyText>
<subsectionHeader confidence="0.995501">
2.3 Post-editing Module
</subsectionHeader>
<bodyText confidence="0.999987222222222">
The post-editing module of the framework (see
also Roturier et al., (2013)) is designed to fulfil
the project’s objective of collecting post-editing
data in order to learn correction rules and, through
feedback loops, to integrate them into the SMT
engines (with the goal of automating corrections
whenever possible). The project relies on the par-
ticipation of volunteer community members, who
are subject matter experts, native speakers of the
</bodyText>
<page confidence="0.998008">
68
</page>
<figureCaption confidence="0.999813">
Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture)
</figureCaption>
<bodyText confidence="0.986384392156863">
target language and, possibly, of the source lan-
guage. Accordingly, the post-editing environment
(see Figure 2) provides functionalities for both
monolingual and bilingual post-editing.
The post-editing text is organised in tasks be-
longing to post-editing projects. The latter are
created and managed by project administrators,
by defining the project settings (e.g., source and
target languages, monolingual or bilingual mode,
collaborative or non-collaborative type7), upload-
ing the text for each task8, inviting participants by
e-mail, and monitoring revision progress.
The post-editors edit the target text in a
sentence-by-sentence fashion. They have access
to the task guidelines and to help documentation.
The interface of the post-editing window displays
the whole text, through which they can navigate
with next-previous buttons or by clicking on a
specific sentence. Users can check the text they
are editing by accessing, with a button, the con-
tent checking technology described in Section 2.2.
Their actions – in terms of keystrokes and usage
7In a collaborative editing scenario, users may see edits
from other users and do not have to repeat them when work-
ing on the same project task. Conflicts are avoided by pre-
venting concurrent access.
8Currently, the JSON format is used for the input data.
of translation options – and time spent editing are
recorded in the portal.9 When they are done edi-
ting, they can click on a button marking the com-
pletion of the task. At any time, they can interrupt
their work and save their results for later.
Users can enter a comment on the post-editing
task they have performed. The feedback elicited
from users include the difficulty of the task and
their sentiment (Was it easy to post-edit? Did you
enjoy the post-editing task?). For systematically
collecting user feedback, the project administra-
tors can specify on the project configuration page
a link to a post-task survey, which will be sent to
users after completing their tasks.
The post-editing module includes a JQuery
plug-in for deployment in any Web-based envi-
ronment; a dedicated section of the portal; APIs
enabling the use of the post-editing functionality
outside the portal; and sample evaluation projects
for several language pairs.
The post-editing technology has been exten-
sively used in specific post-editing campaigns in-
volving translator volunteers and Amazon Me-
chanical Turk10 workers. The campaigns, includ-
</bodyText>
<footnote confidence="0.99529">
9The post-editing data is exported in XLIFF format.
10The integration was done via the ACCEPT API.
</footnote>
<page confidence="0.999465">
69
</page>
<bodyText confidence="0.999895666666667">
ing reports on post-task surveys, are documented
inter alia in deliverable D 8.1.2 (2013). A notable
finding was that professional translators, who were
reticent towards MT before the task, had a more
positive sentiment after post-editing and their mo-
tivation to post-edit in the future increased.
</bodyText>
<subsectionHeader confidence="0.986324">
2.4 Evaluation Module
</subsectionHeader>
<bodyText confidence="0.99996015">
The role of the evaluation module is to support the
collection of user ratings for assessing the quality
of source, machine-translated and post-edited con-
tent, and, ultimately, to support the development
of the technology created in the project.
This module groups several software compo-
nents: an evaluation environment available as a
section of the portal; APIs enabling the collection
of user evaluations in-context; and a third com-
ponent which is a customisation of the Appraise
toolkit for the collaborative collection of human
judgements (Federmann, 2012).
As in the case of post-editing module, this mod-
ule provides functionality for creating and man-
aging projects. Using the evaluation environ-
ment/APIs, project creators can define question
categories, add questions and possible answers,
and upload evaluation data (in JSON format). For
traditional evaluation projects, the Appraise sys-
tem is used instead.
</bodyText>
<sectionHeader confidence="0.999907" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999941871794872">
Transforming the source text in order to better
fit the needs of machine translation is a well-
investigated area of research. Strategies like
source control, source re-ordering, or source sim-
plification at the lexical or structural level have
been largely explored; for reviews, see, for in-
stance, Huhn (2013), Kazemi (2013), and Feng
(2008), respectively.
User-generated content has been investigated
in the context of machine translation in recent
work dealing specifically with spelling correc-
tion (Bertoldi et al., 2010; Formiga and Fonol-
losa, 2012); lexical normalisation by substituting
ill-formed words with their correct counterpart,
e.g., makn → making (Han and Baldwin, 2011);
missing word – e.g., zero-pronoun – recovery and
punctuation correction (Wang and Ng, 2013).
Rather than focusing on specific phenomena or
Web genres (i.e., tweets), we adopt a more gen-
eral approach in which we address the problem of
source normalisation at multiple levels – punctua-
tion, spelling, grammar, and style – for any type of
linguistically imperfect text.
Another peculiarity of our approach is that it
is rule-based and does not require parallel data
for learning corrections. In exchange, a limi-
tation of our pre-editing approach is that it is
language-dependent, as the underlying technology
is based on shallow analysis and is therefore time-
expensive to extend to a new language.
The post-editing technology differs from exist-
ing (standalone or Web-based) dedicated tools –
e.g., iOmegaT11 or MateCat12 – in that it is tai-
lored to community users, and, consequently, it
is lighter, it generates more concise reports, and
a simpler interface replaces the grid-like format
for presenting data. Another specificity is that it
is sufficiently flexible to be used in other environ-
ments (e.g., Amazon Mechanical Turk, cf. §2.3).
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987551724138">
The technology outlined in this paper demon-
strates a specific case of human-computer interac-
tion, in which, for the first time, several modules
are integrated in a full process in which human
pre-editors, post-editors and evaluators play a key
role for improving the translation of community
content. The technology is freely accessible in the
online portal, has been deployed on a major user
forum, and can be downloaded for integration in
other Web-based environments. Since it is built on
top of a REST API, it is portable across devices
and platforms. The technology would be useful to
anyone who needs information instantly and relia-
bly translated, despite linguistic imperfections.
One of the main future developments concerns
the further improvement of SMT, by exploring,
in particular, the use of text analytics and senti-
ment detection. In addition, by incorporating post-
editing rules and developing techniques to change
the phrase table and system parameters dynam-
ically, it will be possible to reduce the amount
of error corrections that human post-editors have
to perform repeatedly. Another major develop-
ment (joint work with the CASMACAT European
project) will focus on novel types of assistance for
translators, aimed specifically at helping transla-
tors by identifying problematic parts of the ma-
chine translation output and signalling the para-
phrases that are more likely to be useful.
</bodyText>
<footnote confidence="0.999138">
11http://try-and-see-mt.org/
12http://www.matecat.com/
</footnote>
<page confidence="0.997461">
70
</page>
<sectionHeader confidence="0.99833" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999981">
The research leading to these results has received
funding from the European Community’s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no 288769.
</bodyText>
<sectionHeader confidence="0.998479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994745086538461">
Nicola Bertoldi, Mauro Cettolo, and Marcello Fe-
derico. 2010. Statistical machine translation of texts
with misspelled words. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 412–419, Los Angeles,
California.
Andrew Bredenkamp, Berthold Crysmann, and Mirela
Petrea. 2000. Looking for errors: A declarative for-
malism for resource-adaptive language checking. In
Proceedings of the Second International Conference
on Language Resources and Evaluation, Athens,
Greece.
2013. ACCEPT deliverable D 2.2 Definition of
pre-editing rules for English and French (final
version). http://www.accept.unige.
ch/Products/D2_2_Definition_of_
Pre-Editing_Rules_for_English_and_
French_with_appendixes.pdf.
2013. ACCEPT deliverable D 9.2.2: Survey of
evaluation results. http://www.accept.
unige.ch/Products/D_9_2_Survey_of_
evaluation_results.pdf.
2013. ACCEPT deliverable D 4.2 Report on
robust machine translation: domain adap-
tation and linguistic back-off. http:
//www.accept.unige.ch/Products/
D_4_2_Report_on_robust_machine_
translation_domain_adaptation_and_
linguistic_back-off.pdf.
2013. ACCEPT deliverable D 8.1.2 Data and
report from user studies - Year 2. http:
//www.accept.unige.ch/Products/D_
8_1_2_Data_and_report_from_user_
studies_-_Year_2.pdf.
Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. The Prague Bulletin of Mathe-
matical Linguistics (PBML), 98:25–35.
Lijun Feng. 2008. Text simplification: A survey.
Technical report, CUNY.
Llu´ıs Formiga and Jos´e A. R. Fonollosa. 2012. Dea-
ling with input noise in statistical machine transla-
tion. In Proceedings of COLING 2012: Posters,
pages 319–328, Mumbai, India.
Johanna Gerlach, Victoria Porro, Pierrette Bouillon,
and Sabine Lehmann. 2013. La pr´e´edition avec
des r`egles peu coˆuteuses, utile pour la TA statistique
des forums ? In Actes de la 20e conf´erence sur
le Traitement Automatique des Langues Naturelles
(TALN’2013), pages 539–546, Les Sables d’Olonne,
France.
Fabrizio Gotti, Philippe Langlais, and Atefeh Farzin-
dar. 2013. Translating government agencies’ tweet
feeds: Specificities, problems and (a few) solutions.
In Proceedings of the Workshop on Language Anal-
ysis in Social Media, pages 80–89, Atlanta, Georgia.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368–378, Port-
land, Oregon.
Jie Jiang, Andy Way, and Rejwanul Haque. 2012.
Translating user-generated content in the social net-
working space. In Proceedings of the Tenth Biennial
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-2012), San Diego, Cali-
fornia.
Arefeh Kazemi, Amirhassan Monadjemi, and Moham-
madali Nematbakhsh. 2013. A quick review on re-
ordering approaches in statistical machine transla-
tion systems. IJCER, 2(4).
Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.
Meenakshi Nagarajan and Michael Gamon, editors.
2011. Proceedings of the Workshop on Language
in Social Media (LSM 2011). Portland, Oregon.
Johann Roturier, Linda Mitchell, Robert Grabowski,
and Melanie Siegel. 2012. Using automatic ma-
chine translation metrics to analyze the impact of
source reformulations. In Proceedings of the Con-
ference of the Association for Machine Translation
in the Americas (AMTA), San Diego, California.
Johann Roturier, Linda Mitchell, and David Silva.
2013. The ACCEPT post-editing environment: a
flexible and customisable online tool to perform and
analyse machine translation post-editing. In Pro-
ceedings of MT Summit XIV Workshop on Post-edi-
ting Technology and Practice, Nice, France.
Violeta Seretan, Pierrette Bouillon, and Johanna Ger-
lach. 2014. A large-scale evaluation of pre-edi-
ting strategies for improving user-generated content
translation. In Proceedings of the 9th Edition of the
Language Resources and Evaluation Conference,
Reykjavik, Iceland.
Pidong Wang and Hwee Tou Ng. 2013. A beam-
search decoder for normalization of social media
text with application to machine translation. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
471–481, Atlanta, Georgia.
</reference>
<page confidence="0.999141">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.201878">
<title confidence="0.9991715">The ACCEPT Portal: An Online Framework for the Pre-editing Post-editing of User-Generated Content</title>
<author confidence="0.823498">Symantec Ltd FTITIM</author>
<affiliation confidence="0.635655666666667">University of Geneva Dublin, Ireland Dublin, Ireland University of Geneva Switzerland johann roturier@symantec.com David Silva@symantec.com Switzerland Violeta.Seretan@unige.ch Pierrette.Bouillon@unige.ch</affiliation>
<abstract confidence="0.998956210526316">With the development of Web 2.0, a lot of content is nowadays generated online by users. Due to its characteristics (e.g., use of jargon and abbreviations, typos, grammatical and style errors), the user-generated content poses specific challenges to machine translation. This paper presents an online platform devoted to the pre-editing of user-generated content and its post-editing, two main types of human assistance strategies which are combined with domain adaptation and other techniques in order to improve the translation of this type of content. The platform has recently been released publicly and is being tested by two main types of user communities, namely, technical forum users and volunteer translators.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
<author>Marcello Federico</author>
</authors>
<title>Statistical machine translation of texts with misspelled words.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>412--419</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="15194" citStr="Bertoldi et al., 2010" startWordPosition="2361" endWordPosition="2364"> format). For traditional evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the problem of source normalisation at multiple levels – punctuation, spelling, grammar, and style – for any type of linguistically imperfect text. Another peculiarity of our approach is that it is rule-based and does</context>
</contexts>
<marker>Bertoldi, Cettolo, Federico, 2010</marker>
<rawString>Nicola Bertoldi, Mauro Cettolo, and Marcello Federico. 2010. Statistical machine translation of texts with misspelled words. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 412–419, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Bredenkamp</author>
<author>Berthold Crysmann</author>
<author>Mirela Petrea</author>
</authors>
<title>Looking for errors: A declarative formalism for resource-adaptive language checking.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second International Conference on Language Resources and Evaluation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="6355" citStr="Bredenkamp et al., 2000" startWordPosition="952" endWordPosition="955">owing main modules: 1. Pre-editing module; 2. Machine translation module, 3. Post-editing module, 4. Evaluation module. The typical workflow is incremental, but the modules are independent. They can be used both within and outside the portal, as they are built on a REST API facilitating integration. 5https://community.norton.com/ In the remaining of this section, we introduce each of the framework modules.6 2.2 Pre-editing Module The pre-editing module leverages existing lingware which provides authoring support rules aimed at language professionals, by relying on shallow language processing (Bredenkamp et al., 2000). The existing English checker and the linguistic resources on which it relies have been extended and adapted to suit the type of data generated by community users. In particular, the software extension consisted of designing a number of pre-editing rules aimed at source normalisation, for the purpose of making the input text easier to handle by the SMT systems. In the case of French, the pre-editing rules have been designed from scratch. The pre-editing rules pertain to the levels of spelling, grammar, style and terminology. They are defined using the original lingware’s rule formalism and ar</context>
</contexts>
<marker>Bredenkamp, Crysmann, Petrea, 2000</marker>
<rawString>Andrew Bredenkamp, Berthold Crysmann, and Mirela Petrea. 2000. Looking for errors: A declarative formalism for resource-adaptive language checking. In Proceedings of the Second International Conference on Language Resources and Evaluation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<title>ACCEPT deliverable D 2.2 Definition of pre-editing rules for English and French (final version). http://www.accept.unige.</title>
<date>2013</date>
<tech>ch/Products/D2_2_Definition_of_ Pre-Editing_Rules_for_English_and_ French_with_appendixes.pdf.</tech>
<contexts>
<context position="7359" citStr="(2013)" startWordPosition="1121" endWordPosition="1121">iting rules have been designed from scratch. The pre-editing rules pertain to the levels of spelling, grammar, style and terminology. They are defined using the original lingware’s rule formalism and are incorporated into a server dedicated to the project. The rule development was corpus-driven and was performed on data collected for this purpose. A stable set of pre-edition rules is available in the portal for each of the domains and source languages considered (i.e., technical forum and heathcare data in English and French). The rules are described in detail in the project deliverable D 2.2 (2013). The rules proposed have been evaluated individually and in combination (Roturier et al., 2012; Gerlach et al., 2013; Seretan et al., 2014). As a general observation, it is important to notice that, for SMT, the improvement of the input text does not go hand in hand with the improvement of translation. For example, in French the rule for correcting verbal forms to the subjunctive tense had a negative impact since the subjunctive is not frequent in the training data. Conversely, it was possible to define lexical reformulations which degraded the quality of the input text, but had a positive im</context>
<context position="9046" citStr="(2013)" startWordPosition="1401" endWordPosition="1401">screen capture) vention, the evaluation was carried out for the full set of rules, as well as for the automatic rules only. In addition, the evaluation was performed in both a monolingual and a bilingual setting, i.e., with the evaluators having or not access to the source text, and it involved evaluation scales of different granularities. The evaluation results showed a systematic statistically significant improvement over the baseline when pre-editing is performed on the source content. More details about the evaluation methodology and results can be found in the project deliverable D 9.2.2 (2013). A data excerpt illustrating the impact of preediting on translation quality is presented in Example 1 below. The simple correction of an accented letter, du → dfu, leads to the change of several target words, and to a much better translation of the input sentence. 1. a) Source (original): J’ai du m’absenter hier apr`es midi. b) Source (pre-edited): J’ai dfu m’absenter hier apr`es midi. c) Target (original): I have the leave me yesterday afternoon. d) Target (pre-edited): I had to leave yesterday afternoon. The pre-editing component of the ACCEPT technology is available as a JQuery plug-in, w</context>
<context position="10438" citStr="(2013)" startWordPosition="1632" endWordPosition="1632"> made available, so that the pre-editing rules can be leveraged in automatic steps, without the plug-in, across devices and platforms. A demo site illustrating the use of the plug-in in a TinyMCE environment is available on the portal (see Figure 1). The latest developments of the pre-editing module include the possibility for users to customise the application of rule sets, in particular, to ignore specific rules and to manage their own dictionary, in order to prevent the activation of checking flags. 2.3 Post-editing Module The post-editing module of the framework (see also Roturier et al., (2013)) is designed to fulfil the project’s objective of collecting post-editing data in order to learn correction rules and, through feedback loops, to integrate them into the SMT engines (with the goal of automating corrections whenever possible). The project relies on the participation of volunteer community members, who are subject matter experts, native speakers of the 68 Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture) target language and, possibly, of the source language. Accordingly, the post-editing environment (see Figure 2) provides functionalities for both monol</context>
<context position="13511" citStr="(2013)" startWordPosition="2110" endWordPosition="2110">es a JQuery plug-in for deployment in any Web-based environment; a dedicated section of the portal; APIs enabling the use of the post-editing functionality outside the portal; and sample evaluation projects for several language pairs. The post-editing technology has been extensively used in specific post-editing campaigns involving translator volunteers and Amazon Mechanical Turk10 workers. The campaigns, includ9The post-editing data is exported in XLIFF format. 10The integration was done via the ACCEPT API. 69 ing reports on post-task surveys, are documented inter alia in deliverable D 8.1.2 (2013). A notable finding was that professional translators, who were reticent towards MT before the task, had a more positive sentiment after post-editing and their motivation to post-edit in the future increased. 2.4 Evaluation Module The role of the evaluation module is to support the collection of user ratings for assessing the quality of source, machine-translated and post-edited content, and, ultimately, to support the development of the technology created in the project. This module groups several software components: an evaluation environment available as a section of the portal; APIs enabli</context>
<context position="14980" citStr="(2013)" startWordPosition="2334" endWordPosition="2334">ity for creating and managing projects. Using the evaluation environment/APIs, project creators can define question categories, add questions and possible answers, and upload evaluation data (in JSON format). For traditional evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the</context>
</contexts>
<marker>2013</marker>
<rawString>2013. ACCEPT deliverable D 2.2 Definition of pre-editing rules for English and French (final version). http://www.accept.unige. ch/Products/D2_2_Definition_of_ Pre-Editing_Rules_for_English_and_ French_with_appendixes.pdf.</rawString>
</citation>
<citation valid="true">
<title>ACCEPT deliverable D 9.2.2: Survey of evaluation results.</title>
<date>2013</date>
<note>http://www.accept. unige.ch/Products/D_9_2_Survey_of_ evaluation_results.pdf.</note>
<contexts>
<context position="7359" citStr="(2013)" startWordPosition="1121" endWordPosition="1121">iting rules have been designed from scratch. The pre-editing rules pertain to the levels of spelling, grammar, style and terminology. They are defined using the original lingware’s rule formalism and are incorporated into a server dedicated to the project. The rule development was corpus-driven and was performed on data collected for this purpose. A stable set of pre-edition rules is available in the portal for each of the domains and source languages considered (i.e., technical forum and heathcare data in English and French). The rules are described in detail in the project deliverable D 2.2 (2013). The rules proposed have been evaluated individually and in combination (Roturier et al., 2012; Gerlach et al., 2013; Seretan et al., 2014). As a general observation, it is important to notice that, for SMT, the improvement of the input text does not go hand in hand with the improvement of translation. For example, in French the rule for correcting verbal forms to the subjunctive tense had a negative impact since the subjunctive is not frequent in the training data. Conversely, it was possible to define lexical reformulations which degraded the quality of the input text, but had a positive im</context>
<context position="9046" citStr="(2013)" startWordPosition="1401" endWordPosition="1401">screen capture) vention, the evaluation was carried out for the full set of rules, as well as for the automatic rules only. In addition, the evaluation was performed in both a monolingual and a bilingual setting, i.e., with the evaluators having or not access to the source text, and it involved evaluation scales of different granularities. The evaluation results showed a systematic statistically significant improvement over the baseline when pre-editing is performed on the source content. More details about the evaluation methodology and results can be found in the project deliverable D 9.2.2 (2013). A data excerpt illustrating the impact of preediting on translation quality is presented in Example 1 below. The simple correction of an accented letter, du → dfu, leads to the change of several target words, and to a much better translation of the input sentence. 1. a) Source (original): J’ai du m’absenter hier apr`es midi. b) Source (pre-edited): J’ai dfu m’absenter hier apr`es midi. c) Target (original): I have the leave me yesterday afternoon. d) Target (pre-edited): I had to leave yesterday afternoon. The pre-editing component of the ACCEPT technology is available as a JQuery plug-in, w</context>
<context position="10438" citStr="(2013)" startWordPosition="1632" endWordPosition="1632"> made available, so that the pre-editing rules can be leveraged in automatic steps, without the plug-in, across devices and platforms. A demo site illustrating the use of the plug-in in a TinyMCE environment is available on the portal (see Figure 1). The latest developments of the pre-editing module include the possibility for users to customise the application of rule sets, in particular, to ignore specific rules and to manage their own dictionary, in order to prevent the activation of checking flags. 2.3 Post-editing Module The post-editing module of the framework (see also Roturier et al., (2013)) is designed to fulfil the project’s objective of collecting post-editing data in order to learn correction rules and, through feedback loops, to integrate them into the SMT engines (with the goal of automating corrections whenever possible). The project relies on the participation of volunteer community members, who are subject matter experts, native speakers of the 68 Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture) target language and, possibly, of the source language. Accordingly, the post-editing environment (see Figure 2) provides functionalities for both monol</context>
<context position="13511" citStr="(2013)" startWordPosition="2110" endWordPosition="2110">es a JQuery plug-in for deployment in any Web-based environment; a dedicated section of the portal; APIs enabling the use of the post-editing functionality outside the portal; and sample evaluation projects for several language pairs. The post-editing technology has been extensively used in specific post-editing campaigns involving translator volunteers and Amazon Mechanical Turk10 workers. The campaigns, includ9The post-editing data is exported in XLIFF format. 10The integration was done via the ACCEPT API. 69 ing reports on post-task surveys, are documented inter alia in deliverable D 8.1.2 (2013). A notable finding was that professional translators, who were reticent towards MT before the task, had a more positive sentiment after post-editing and their motivation to post-edit in the future increased. 2.4 Evaluation Module The role of the evaluation module is to support the collection of user ratings for assessing the quality of source, machine-translated and post-edited content, and, ultimately, to support the development of the technology created in the project. This module groups several software components: an evaluation environment available as a section of the portal; APIs enabli</context>
<context position="14980" citStr="(2013)" startWordPosition="2334" endWordPosition="2334">ity for creating and managing projects. Using the evaluation environment/APIs, project creators can define question categories, add questions and possible answers, and upload evaluation data (in JSON format). For traditional evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the</context>
</contexts>
<marker>2013</marker>
<rawString>2013. ACCEPT deliverable D 9.2.2: Survey of evaluation results. http://www.accept. unige.ch/Products/D_9_2_Survey_of_ evaluation_results.pdf.</rawString>
</citation>
<citation valid="true">
<title>ACCEPT deliverable D 4.2 Report on robust machine translation: domain adaptation and linguistic back-off. http: //www.accept.unige.ch/Products/ D_4_2_Report_on_robust_machine_ translation_domain_adaptation_and_ linguistic_back-off.pdf.</title>
<date>2013</date>
<contexts>
<context position="7359" citStr="(2013)" startWordPosition="1121" endWordPosition="1121">iting rules have been designed from scratch. The pre-editing rules pertain to the levels of spelling, grammar, style and terminology. They are defined using the original lingware’s rule formalism and are incorporated into a server dedicated to the project. The rule development was corpus-driven and was performed on data collected for this purpose. A stable set of pre-edition rules is available in the portal for each of the domains and source languages considered (i.e., technical forum and heathcare data in English and French). The rules are described in detail in the project deliverable D 2.2 (2013). The rules proposed have been evaluated individually and in combination (Roturier et al., 2012; Gerlach et al., 2013; Seretan et al., 2014). As a general observation, it is important to notice that, for SMT, the improvement of the input text does not go hand in hand with the improvement of translation. For example, in French the rule for correcting verbal forms to the subjunctive tense had a negative impact since the subjunctive is not frequent in the training data. Conversely, it was possible to define lexical reformulations which degraded the quality of the input text, but had a positive im</context>
<context position="9046" citStr="(2013)" startWordPosition="1401" endWordPosition="1401">screen capture) vention, the evaluation was carried out for the full set of rules, as well as for the automatic rules only. In addition, the evaluation was performed in both a monolingual and a bilingual setting, i.e., with the evaluators having or not access to the source text, and it involved evaluation scales of different granularities. The evaluation results showed a systematic statistically significant improvement over the baseline when pre-editing is performed on the source content. More details about the evaluation methodology and results can be found in the project deliverable D 9.2.2 (2013). A data excerpt illustrating the impact of preediting on translation quality is presented in Example 1 below. The simple correction of an accented letter, du → dfu, leads to the change of several target words, and to a much better translation of the input sentence. 1. a) Source (original): J’ai du m’absenter hier apr`es midi. b) Source (pre-edited): J’ai dfu m’absenter hier apr`es midi. c) Target (original): I have the leave me yesterday afternoon. d) Target (pre-edited): I had to leave yesterday afternoon. The pre-editing component of the ACCEPT technology is available as a JQuery plug-in, w</context>
<context position="10438" citStr="(2013)" startWordPosition="1632" endWordPosition="1632"> made available, so that the pre-editing rules can be leveraged in automatic steps, without the plug-in, across devices and platforms. A demo site illustrating the use of the plug-in in a TinyMCE environment is available on the portal (see Figure 1). The latest developments of the pre-editing module include the possibility for users to customise the application of rule sets, in particular, to ignore specific rules and to manage their own dictionary, in order to prevent the activation of checking flags. 2.3 Post-editing Module The post-editing module of the framework (see also Roturier et al., (2013)) is designed to fulfil the project’s objective of collecting post-editing data in order to learn correction rules and, through feedback loops, to integrate them into the SMT engines (with the goal of automating corrections whenever possible). The project relies on the participation of volunteer community members, who are subject matter experts, native speakers of the 68 Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture) target language and, possibly, of the source language. Accordingly, the post-editing environment (see Figure 2) provides functionalities for both monol</context>
<context position="13511" citStr="(2013)" startWordPosition="2110" endWordPosition="2110">es a JQuery plug-in for deployment in any Web-based environment; a dedicated section of the portal; APIs enabling the use of the post-editing functionality outside the portal; and sample evaluation projects for several language pairs. The post-editing technology has been extensively used in specific post-editing campaigns involving translator volunteers and Amazon Mechanical Turk10 workers. The campaigns, includ9The post-editing data is exported in XLIFF format. 10The integration was done via the ACCEPT API. 69 ing reports on post-task surveys, are documented inter alia in deliverable D 8.1.2 (2013). A notable finding was that professional translators, who were reticent towards MT before the task, had a more positive sentiment after post-editing and their motivation to post-edit in the future increased. 2.4 Evaluation Module The role of the evaluation module is to support the collection of user ratings for assessing the quality of source, machine-translated and post-edited content, and, ultimately, to support the development of the technology created in the project. This module groups several software components: an evaluation environment available as a section of the portal; APIs enabli</context>
<context position="14980" citStr="(2013)" startWordPosition="2334" endWordPosition="2334">ity for creating and managing projects. Using the evaluation environment/APIs, project creators can define question categories, add questions and possible answers, and upload evaluation data (in JSON format). For traditional evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the</context>
</contexts>
<marker>2013</marker>
<rawString>2013. ACCEPT deliverable D 4.2 Report on robust machine translation: domain adaptation and linguistic back-off. http: //www.accept.unige.ch/Products/ D_4_2_Report_on_robust_machine_ translation_domain_adaptation_and_ linguistic_back-off.pdf.</rawString>
</citation>
<citation valid="true">
<title>ACCEPT deliverable D 8.1.2 Data and report from user studies -</title>
<date>2013</date>
<journal>Year</journal>
<volume>2</volume>
<note>http: //www.accept.unige.ch/Products/D_ 8_1_2_Data_and_report_from_user_ studies_-_Year_2.pdf.</note>
<contexts>
<context position="7359" citStr="(2013)" startWordPosition="1121" endWordPosition="1121">iting rules have been designed from scratch. The pre-editing rules pertain to the levels of spelling, grammar, style and terminology. They are defined using the original lingware’s rule formalism and are incorporated into a server dedicated to the project. The rule development was corpus-driven and was performed on data collected for this purpose. A stable set of pre-edition rules is available in the portal for each of the domains and source languages considered (i.e., technical forum and heathcare data in English and French). The rules are described in detail in the project deliverable D 2.2 (2013). The rules proposed have been evaluated individually and in combination (Roturier et al., 2012; Gerlach et al., 2013; Seretan et al., 2014). As a general observation, it is important to notice that, for SMT, the improvement of the input text does not go hand in hand with the improvement of translation. For example, in French the rule for correcting verbal forms to the subjunctive tense had a negative impact since the subjunctive is not frequent in the training data. Conversely, it was possible to define lexical reformulations which degraded the quality of the input text, but had a positive im</context>
<context position="9046" citStr="(2013)" startWordPosition="1401" endWordPosition="1401">screen capture) vention, the evaluation was carried out for the full set of rules, as well as for the automatic rules only. In addition, the evaluation was performed in both a monolingual and a bilingual setting, i.e., with the evaluators having or not access to the source text, and it involved evaluation scales of different granularities. The evaluation results showed a systematic statistically significant improvement over the baseline when pre-editing is performed on the source content. More details about the evaluation methodology and results can be found in the project deliverable D 9.2.2 (2013). A data excerpt illustrating the impact of preediting on translation quality is presented in Example 1 below. The simple correction of an accented letter, du → dfu, leads to the change of several target words, and to a much better translation of the input sentence. 1. a) Source (original): J’ai du m’absenter hier apr`es midi. b) Source (pre-edited): J’ai dfu m’absenter hier apr`es midi. c) Target (original): I have the leave me yesterday afternoon. d) Target (pre-edited): I had to leave yesterday afternoon. The pre-editing component of the ACCEPT technology is available as a JQuery plug-in, w</context>
<context position="10438" citStr="(2013)" startWordPosition="1632" endWordPosition="1632"> made available, so that the pre-editing rules can be leveraged in automatic steps, without the plug-in, across devices and platforms. A demo site illustrating the use of the plug-in in a TinyMCE environment is available on the portal (see Figure 1). The latest developments of the pre-editing module include the possibility for users to customise the application of rule sets, in particular, to ignore specific rules and to manage their own dictionary, in order to prevent the activation of checking flags. 2.3 Post-editing Module The post-editing module of the framework (see also Roturier et al., (2013)) is designed to fulfil the project’s objective of collecting post-editing data in order to learn correction rules and, through feedback loops, to integrate them into the SMT engines (with the goal of automating corrections whenever possible). The project relies on the participation of volunteer community members, who are subject matter experts, native speakers of the 68 Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture) target language and, possibly, of the source language. Accordingly, the post-editing environment (see Figure 2) provides functionalities for both monol</context>
<context position="13511" citStr="(2013)" startWordPosition="2110" endWordPosition="2110">es a JQuery plug-in for deployment in any Web-based environment; a dedicated section of the portal; APIs enabling the use of the post-editing functionality outside the portal; and sample evaluation projects for several language pairs. The post-editing technology has been extensively used in specific post-editing campaigns involving translator volunteers and Amazon Mechanical Turk10 workers. The campaigns, includ9The post-editing data is exported in XLIFF format. 10The integration was done via the ACCEPT API. 69 ing reports on post-task surveys, are documented inter alia in deliverable D 8.1.2 (2013). A notable finding was that professional translators, who were reticent towards MT before the task, had a more positive sentiment after post-editing and their motivation to post-edit in the future increased. 2.4 Evaluation Module The role of the evaluation module is to support the collection of user ratings for assessing the quality of source, machine-translated and post-edited content, and, ultimately, to support the development of the technology created in the project. This module groups several software components: an evaluation environment available as a section of the portal; APIs enabli</context>
<context position="14980" citStr="(2013)" startWordPosition="2334" endWordPosition="2334">ity for creating and managing projects. Using the evaluation environment/APIs, project creators can define question categories, add questions and possible answers, and upload evaluation data (in JSON format). For traditional evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the</context>
</contexts>
<marker>2013</marker>
<rawString>2013. ACCEPT deliverable D 8.1.2 Data and report from user studies - Year 2. http: //www.accept.unige.ch/Products/D_ 8_1_2_Data_and_report_from_user_ studies_-_Year_2.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Federmann</author>
</authors>
<title>Appraise: An opensource toolkit for manual evaluation of machine translation output.</title>
<date>2012</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics (PBML),</booktitle>
<pages>98--25</pages>
<contexts>
<context position="14302" citStr="Federmann, 2012" startWordPosition="2229" endWordPosition="2230">ost-edit in the future increased. 2.4 Evaluation Module The role of the evaluation module is to support the collection of user ratings for assessing the quality of source, machine-translated and post-edited content, and, ultimately, to support the development of the technology created in the project. This module groups several software components: an evaluation environment available as a section of the portal; APIs enabling the collection of user evaluations in-context; and a third component which is a customisation of the Appraise toolkit for the collaborative collection of human judgements (Federmann, 2012). As in the case of post-editing module, this module provides functionality for creating and managing projects. Using the evaluation environment/APIs, project creators can define question categories, add questions and possible answers, and upload evaluation data (in JSON format). For traditional evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural</context>
</contexts>
<marker>Federmann, 2012</marker>
<rawString>Christian Federmann. 2012. Appraise: An opensource toolkit for manual evaluation of machine translation output. The Prague Bulletin of Mathematical Linguistics (PBML), 98:25–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
</authors>
<title>Text simplification: A survey.</title>
<date>2008</date>
<tech>Technical report, CUNY.</tech>
<contexts>
<context position="15012" citStr="Feng (2008)" startWordPosition="2338" endWordPosition="2339">ng projects. Using the evaluation environment/APIs, project creators can define question categories, add questions and possible answers, and upload evaluation data (in JSON format). For traditional evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the problem of source normalisation</context>
</contexts>
<marker>Feng, 2008</marker>
<rawString>Lijun Feng. 2008. Text simplification: A survey. Technical report, CUNY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs Formiga</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Dealing with input noise in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>319--328</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="15224" citStr="Formiga and Fonollosa, 2012" startWordPosition="2365" endWordPosition="2369">al evaluation projects, the Appraise system is used instead. 3 Related Work Transforming the source text in order to better fit the needs of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the problem of source normalisation at multiple levels – punctuation, spelling, grammar, and style – for any type of linguistically imperfect text. Another peculiarity of our approach is that it is rule-based and does not require parallel data for</context>
</contexts>
<marker>Formiga, Fonollosa, 2012</marker>
<rawString>Llu´ıs Formiga and Jos´e A. R. Fonollosa. 2012. Dealing with input noise in statistical machine translation. In Proceedings of COLING 2012: Posters, pages 319–328, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Gerlach</author>
<author>Victoria Porro</author>
<author>Pierrette Bouillon</author>
<author>Sabine Lehmann</author>
</authors>
<title>La pr´e´edition avec des r`egles peu coˆuteuses, utile pour la TA statistique des forums ?</title>
<date>2013</date>
<booktitle>In Actes de la 20e conf´erence sur le Traitement Automatique des Langues Naturelles (TALN’2013),</booktitle>
<pages>539--546</pages>
<location>Les Sables d’Olonne, France.</location>
<contexts>
<context position="7476" citStr="Gerlach et al., 2013" startWordPosition="1137" endWordPosition="1140">rammar, style and terminology. They are defined using the original lingware’s rule formalism and are incorporated into a server dedicated to the project. The rule development was corpus-driven and was performed on data collected for this purpose. A stable set of pre-edition rules is available in the portal for each of the domains and source languages considered (i.e., technical forum and heathcare data in English and French). The rules are described in detail in the project deliverable D 2.2 (2013). The rules proposed have been evaluated individually and in combination (Roturier et al., 2012; Gerlach et al., 2013; Seretan et al., 2014). As a general observation, it is important to notice that, for SMT, the improvement of the input text does not go hand in hand with the improvement of translation. For example, in French the rule for correcting verbal forms to the subjunctive tense had a negative impact since the subjunctive is not frequent in the training data. Conversely, it was possible to define lexical reformulations which degraded the quality of the input text, but had a positive impact on translation quality. The combined impact of the rule application was measured in a variety of settings in a l</context>
</contexts>
<marker>Gerlach, Porro, Bouillon, Lehmann, 2013</marker>
<rawString>Johanna Gerlach, Victoria Porro, Pierrette Bouillon, and Sabine Lehmann. 2013. La pr´e´edition avec des r`egles peu coˆuteuses, utile pour la TA statistique des forums ? In Actes de la 20e conf´erence sur le Traitement Automatique des Langues Naturelles (TALN’2013), pages 539–546, Les Sables d’Olonne, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Gotti</author>
<author>Philippe Langlais</author>
<author>Atefeh Farzindar</author>
</authors>
<title>Translating government agencies’ tweet feeds: Specificities, problems and (a few) solutions.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Language Analysis in Social Media,</booktitle>
<pages>80--89</pages>
<location>Atlanta,</location>
<contexts>
<context position="3880" citStr="Gotti et al., 2013" startWordPosition="581" endWordPosition="584">cific information on a given topic. The project focuses, more specifically, on the following use cases: 1. the commercial use case, in which the target community is the user community built around a software company in order for members to help each other with issues related to products; 2. the NGO use case, in which non-governmental organisations such as Doctors Without Borders produce health-care content for distributions in areas of need. 3Even when they are, as in the case of government agencies, the type of content produced (e.g., tweets) still poses “multiple challenges” to translation (Gotti et al., 2013). 4http://www.accept-project.eu/ 66 Workshop on Humans and Computer-assisted Translation, pages 66–71, Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics The language pairs considered in the project are English to French, German and Japanese, as well as French into English for the first use case (involving technical forum information), and French to and from English for the second use case (involving healthcare information). Past halfway into its research program, the project has accomplished significant progress in the main areas mentioned above (pre-editing, </context>
</contexts>
<marker>Gotti, Langlais, Farzindar, 2013</marker>
<rawString>Fabrizio Gotti, Philippe Langlais, and Atefeh Farzindar. 2013. Translating government agencies’ tweet feeds: Specificities, problems and (a few) solutions. In Proceedings of the Workshop on Language Analysis in Social Media, pages 80–89, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>368--378</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="15356" citStr="Han and Baldwin, 2011" startWordPosition="2384" endWordPosition="2387">of machine translation is a wellinvestigated area of research. Strategies like source control, source re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the problem of source normalisation at multiple levels – punctuation, spelling, grammar, and style – for any type of linguistically imperfect text. Another peculiarity of our approach is that it is rule-based and does not require parallel data for learning corrections. In exchange, a limitation of our pre-editing approach is that it is language-dependent, as the underlying tec</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 368–378, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Jiang</author>
<author>Andy Way</author>
<author>Rejwanul Haque</author>
</authors>
<title>Translating user-generated content in the social networking space.</title>
<date>2012</date>
<booktitle>In Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas (AMTA-2012),</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="2110" citStr="Jiang et al. (2012)" startWordPosition="305" endWordPosition="308"> due to the emergence of the Web 2.0 paradigm also means a diversification of the languages used on the Internet.2 Despite its availability, the new content is only accessible to the speakers of the language in which it was created. The automatic translation of user-generated content is therefore one of the key issues to be addressed in the field of human language technologies. However, as stated 1For instance, 58 million tweets are sent on average per day (http://www.statisticbrain.com/ twitter-statistics/). 2See http://en.wikipedia.org/wiki/ Languages-used-on-the-Internet for statistics. by Jiang et al. (2012), despite the obvious benefits, there are relatively little attempts at translating user-generated content. The reason may lie in the fact that user-generated content is very challenging for machine translation. As shown, among others, by Nagarajan and Gamon (2011), there are several characteristics of this content that pose new processing challenges with respect to traditional content: informal style, slang, abbreviations, specific terminology, irregular grammar and spelling. Indeed, Internet users are rarely professional writers.3 They often write in a language which is not their own, and sa</context>
</contexts>
<marker>Jiang, Way, Haque, 2012</marker>
<rawString>Jie Jiang, Andy Way, and Rejwanul Haque. 2012. Translating user-generated content in the social networking space. In Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas (AMTA-2012), San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arefeh Kazemi</author>
<author>Amirhassan Monadjemi</author>
<author>Mohammadali Nematbakhsh</author>
</authors>
<title>A quick review on reordering approaches in statistical machine translation systems.</title>
<date>2013</date>
<journal>IJCER,</journal>
<volume>2</volume>
<issue>4</issue>
<marker>Kazemi, Monadjemi, Nematbakhsh, 2013</marker>
<rawString>Arefeh Kazemi, Amirhassan Monadjemi, and Mohammadali Nematbakhsh. 2013. A quick review on reordering approaches in statistical machine translation systems. IJCER, 2(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Kuhn</author>
</authors>
<title>A survey and classification of controlled natural languages. Computational Linguistics.</title>
<date>2013</date>
<marker>Kuhn, 2013</marker>
<rawString>Tobias Kuhn. 2013. A survey and classification of controlled natural languages. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meenakshi Nagarajan</author>
<author>Michael Gamon</author>
<author>editors</author>
</authors>
<date>2011</date>
<booktitle>Proceedings of the Workshop on Language in Social Media (LSM 2011).</booktitle>
<location>Portland, Oregon.</location>
<marker>Nagarajan, Gamon, editors, 2011</marker>
<rawString>Meenakshi Nagarajan and Michael Gamon, editors. 2011. Proceedings of the Workshop on Language in Social Media (LSM 2011). Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johann Roturier</author>
<author>Linda Mitchell</author>
<author>Robert Grabowski</author>
<author>Melanie Siegel</author>
</authors>
<title>Using automatic machine translation metrics to analyze the impact of source reformulations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="7454" citStr="Roturier et al., 2012" startWordPosition="1133" endWordPosition="1136">e levels of spelling, grammar, style and terminology. They are defined using the original lingware’s rule formalism and are incorporated into a server dedicated to the project. The rule development was corpus-driven and was performed on data collected for this purpose. A stable set of pre-edition rules is available in the portal for each of the domains and source languages considered (i.e., technical forum and heathcare data in English and French). The rules are described in detail in the project deliverable D 2.2 (2013). The rules proposed have been evaluated individually and in combination (Roturier et al., 2012; Gerlach et al., 2013; Seretan et al., 2014). As a general observation, it is important to notice that, for SMT, the improvement of the input text does not go hand in hand with the improvement of translation. For example, in French the rule for correcting verbal forms to the subjunctive tense had a negative impact since the subjunctive is not frequent in the training data. Conversely, it was possible to define lexical reformulations which degraded the quality of the input text, but had a positive impact on translation quality. The combined impact of the rule application was measured in a vari</context>
</contexts>
<marker>Roturier, Mitchell, Grabowski, Siegel, 2012</marker>
<rawString>Johann Roturier, Linda Mitchell, Robert Grabowski, and Melanie Siegel. 2012. Using automatic machine translation metrics to analyze the impact of source reformulations. In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA), San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johann Roturier</author>
<author>Linda Mitchell</author>
<author>David Silva</author>
</authors>
<title>The ACCEPT post-editing environment: a flexible and customisable online tool to perform and analyse machine translation post-editing.</title>
<date>2013</date>
<booktitle>In Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice,</booktitle>
<location>Nice, France.</location>
<contexts>
<context position="10438" citStr="Roturier et al., (2013)" startWordPosition="1629" endWordPosition="1632">on have also been made available, so that the pre-editing rules can be leveraged in automatic steps, without the plug-in, across devices and platforms. A demo site illustrating the use of the plug-in in a TinyMCE environment is available on the portal (see Figure 1). The latest developments of the pre-editing module include the possibility for users to customise the application of rule sets, in particular, to ignore specific rules and to manage their own dictionary, in order to prevent the activation of checking flags. 2.3 Post-editing Module The post-editing module of the framework (see also Roturier et al., (2013)) is designed to fulfil the project’s objective of collecting post-editing data in order to learn correction rules and, through feedback loops, to integrate them into the SMT engines (with the goal of automating corrections whenever possible). The project relies on the participation of volunteer community members, who are subject matter experts, native speakers of the 68 Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture) target language and, possibly, of the source language. Accordingly, the post-editing environment (see Figure 2) provides functionalities for both monol</context>
</contexts>
<marker>Roturier, Mitchell, Silva, 2013</marker>
<rawString>Johann Roturier, Linda Mitchell, and David Silva. 2013. The ACCEPT post-editing environment: a flexible and customisable online tool to perform and analyse machine translation post-editing. In Proceedings of MT Summit XIV Workshop on Post-editing Technology and Practice, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Violeta Seretan</author>
<author>Pierrette Bouillon</author>
<author>Johanna Gerlach</author>
</authors>
<title>A large-scale evaluation of pre-editing strategies for improving user-generated content translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th Edition of the Language Resources and Evaluation Conference,</booktitle>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="7499" citStr="Seretan et al., 2014" startWordPosition="1141" endWordPosition="1144">inology. They are defined using the original lingware’s rule formalism and are incorporated into a server dedicated to the project. The rule development was corpus-driven and was performed on data collected for this purpose. A stable set of pre-edition rules is available in the portal for each of the domains and source languages considered (i.e., technical forum and heathcare data in English and French). The rules are described in detail in the project deliverable D 2.2 (2013). The rules proposed have been evaluated individually and in combination (Roturier et al., 2012; Gerlach et al., 2013; Seretan et al., 2014). As a general observation, it is important to notice that, for SMT, the improvement of the input text does not go hand in hand with the improvement of translation. For example, in French the rule for correcting verbal forms to the subjunctive tense had a negative impact since the subjunctive is not frequent in the training data. Conversely, it was possible to define lexical reformulations which degraded the quality of the input text, but had a positive impact on translation quality. The combined impact of the rule application was measured in a variety of settings in a large-scale evaluation c</context>
</contexts>
<marker>Seretan, Bouillon, Gerlach, 2014</marker>
<rawString>Violeta Seretan, Pierrette Bouillon, and Johanna Gerlach. 2014. A large-scale evaluation of pre-editing strategies for improving user-generated content translation. In Proceedings of the 9th Edition of the Language Resources and Evaluation Conference, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pidong Wang</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A beamsearch decoder for normalization of social media text with application to machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>471--481</pages>
<location>Atlanta,</location>
<contexts>
<context position="15449" citStr="Wang and Ng, 2013" startWordPosition="2398" endWordPosition="2401">urce re-ordering, or source simplification at the lexical or structural level have been largely explored; for reviews, see, for instance, Huhn (2013), Kazemi (2013), and Feng (2008), respectively. User-generated content has been investigated in the context of machine translation in recent work dealing specifically with spelling correction (Bertoldi et al., 2010; Formiga and Fonollosa, 2012); lexical normalisation by substituting ill-formed words with their correct counterpart, e.g., makn → making (Han and Baldwin, 2011); missing word – e.g., zero-pronoun – recovery and punctuation correction (Wang and Ng, 2013). Rather than focusing on specific phenomena or Web genres (i.e., tweets), we adopt a more general approach in which we address the problem of source normalisation at multiple levels – punctuation, spelling, grammar, and style – for any type of linguistically imperfect text. Another peculiarity of our approach is that it is rule-based and does not require parallel data for learning corrections. In exchange, a limitation of our pre-editing approach is that it is language-dependent, as the underlying technology is based on shallow analysis and is therefore timeexpensive to extend to a new langua</context>
</contexts>
<marker>Wang, Ng, 2013</marker>
<rawString>Pidong Wang and Hwee Tou Ng. 2013. A beamsearch decoder for normalization of social media text with application to machine translation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 471–481, Atlanta, Georgia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
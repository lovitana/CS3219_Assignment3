<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988569">
The SJTU System for Dialog State Tracking Challenge 2
</title>
<author confidence="0.99965">
Kai Sun, Lu Chen, Su Zhu and Kai Yu
</author>
<affiliation confidence="0.924103">
Department of Computer Science and Engineering, Shanghai Jiao Tong University
Shanghai, China
</affiliation>
<email confidence="0.939734">
{accreator, chenlusz, paul2204, kai.yu}@sjtu.edu.cn
</email>
<sectionHeader confidence="0.996913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990725">
Dialog state tracking challenge provides
a common testbed for state tracking al-
gorithms. This paper describes the SJTU
system submitted to the second Dialogue
State Tracking Challenge in detail. In
the system, a statistical semantic parser is
used to generate refined semantic hypothe-
ses. A large number of features are then
derived based on the semantic hypothe-
ses and the dialogue log information. The
final tracker is a combination of a rule-
based model, a maximum entropy and a
deep neural network model. The SJTU
system significantly outperformed all the
baselines and showed competitive perfor-
mance in DSTC 2.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892822580645">
Dialog state tracking is important because spo-
ken dialog systems rely on it to choose proper
actions as spoken dialog systems interact with
users. However, due to automatic speech recog-
nition (ASR) and spoken language understanding
(SLU) errors, it is not easy for the dialog man-
ager to maintain the true state of the dialog. In
recent years, much research has been devoted to
dialog state tracking. Many approaches have been
applied to dialog state tracking, from rule-based
to statistical models, from generative models to
discriminative models (Wang and Lemon, 2013;
Zilka et al., 2013; Henderson et al., 2013; Lee
and Eskenazi, 2013). Recently, shared research
tasks like the first Dialog State Tracking Challenge
(DSTC 1) (Williams et al., 2013) have provided
a common testbed and evaluation suite for dialog
state tracking (Henderson et al., 2013).
Compared with DSTC 1 which is in the bus
timetables domain, DSTC 2 introduces more com-
plicated and dynamic dialog states, which may
change through the dialog, in a new domain, i.e.
restaurants domain (Henderson et al., 2014). For
each turn, a tracker is supposed to output a set
of distributions for each of the three components
of the dialog state: goals, method, and requested
slots. At a given turn, the goals consists of the
user’s true required value having been revealed
for each slot in the dialog up until that turn; the
method is the way the user is trying to interact with
the system which may be by name, by constraints,
by alternatives or finished; and the requested slots
consist of the slots which have been requested by
the user and not replied by the system. For evalua-
tion in DSTC 2, 1-best quality measured by accu-
racy, probability calibration measured by L2, and
discrimination measured by ROC are selected as
featured metrics. Further details can be found in
the DSTC 2 handbook (Henderson et al., 2013).
Previous research has demonstrated the effec-
tiveness of rule-based (Zilka et al., 2013), maxi-
mum entropy (MaxEnt) (Lee and Eskenazi, 2013)
and deep neural network (DNN) (Henderson et al.,
2013) models separately. Motivated by this, the
SJTU system employs a combination of a rule-
based model, a MaxEnt and a DNN model. The
three models were first trained (if necessary) on
the training set and tested for each of the three
components of the dialog state, i.e goals, method,
and requested slots on the development set. Then,
models with the best performance for each of the
three components were selected to form a com-
bined model. Finally, the combined model was
retrained using both training set and development
set. Additionally, as the live SLU was found not
good enough with some information lost com-
pared with the live ASR, a new semantic parser
was implemented which took the live ASR as in-
put and the SJTU system used the result from the
new semantic parser instead of the live SLU.
The remainder of the paper is organized as fol-
lows. Section 2 describes the design of the new
</bodyText>
<page confidence="0.976557">
318
</page>
<note confidence="0.733522">
Proceedings of the SIGDIAL 2014 Conference, pages 318–326,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998704166666667">
semantic parser. Section 3 presents the rule-based
model. Section 4 describes the statistical mod-
els including the maximum entropy model and the
deep neural network model. Section 5 shows and
discusses the performance of the SJTU system. Fi-
nally, section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.976327" genericHeader="method">
2 Semantic Parser
</sectionHeader>
<bodyText confidence="0.99968">
It was found that the live SLU provided by the or-
ganisers has poor quality. Hence, a new statistical
semantic parser is trained to parse the live ASR
hypotheses.
</bodyText>
<subsectionHeader confidence="0.996912">
2.1 Semantic Tuple Classifier
</subsectionHeader>
<bodyText confidence="0.99812075">
The semantics of an utterance is represented in
functor form called dialogue act consisting of a
dialogue act type and a list of slot-value pairs, for
example:
</bodyText>
<equation confidence="0.735767">
request(name,food=chinese)
</equation>
<bodyText confidence="0.999928242424242">
where “request” is the dialogue act type,“name”
is a slot requested and “food=chinese” is a
slot-value pair which provides some informa-
tion to the system. In DSTC 2, there are
many different dialogue act types (e.g. “request”,
“inform”, “deny”, etc) and different slot-value
pairs (e.g. “food=chinese”, “pricerange=cheap”,
“area=center”, etc), which are all called semantic
items.
A semantic tuple (e.g. act type, type-slot pair,
slot-value pair) classifier (STC) approach devel-
oped by Mairesse et al. ( 2009) is used in the SJTU
system. It requires a set of SVMs to be trained on
n-gram features from a given utterance: a multi-
class SVM is used to predict the dialogue act type,
and a binary SVM is used to predict the exis-
tence of each slot-value pair. Henderson et al. (
2012) improved this method with converting the
SVM outputs to probabilities, and approximating
the probability of a dialogue-act d of type d-typej
with a set of slot-value pairs S by:
ploited to constrain the semantic parser. In DSTC
2, the dialogue context available contains the his-
tory information of user’s ASR hypotheses, the
system act and the other output of system (e.g.
whether there is a barge-in from the user or not,
the turn-index) and so on. In the SJTU system,
the context features from the last system act (in-
dicators for all act types and slot-value pairs on
whether they appear), an indicator for “barge-in”
and the reciprocal of turn-index are combined with
the original n-gram feature to be the final feature
vector.
</bodyText>
<subsectionHeader confidence="0.999452">
2.3 Generating Confidence Scores
</subsectionHeader>
<bodyText confidence="0.9998928">
For testing and predicting the dialogue act, the se-
mantic parser is applied to each of the top N ASR
hypotheses hi, and the set of results Di with mi
distinct dialogue act hypotheses would be merged
in following way:
</bodyText>
<equation confidence="0.9936612">
�
N
p(hi|o)p(d|hi) if d E Di
0 otherwise
i=1
</equation>
<bodyText confidence="0.999822625">
where o is the acoustic observation, d runs over
each different dialogue act in Di, i = 1, ..., N,
p(hi|o) denotes the ASR posterior probability of
the i-th hypothesis, p(d|hi) denotes the semantic
posterior probability given the i-th ASR hypoth-
esis as in equation (1). Finally, a normalization
should be done to guarantee the sum of P(d|o) to
be one.
</bodyText>
<subsectionHeader confidence="0.986256">
2.4 Implementation
</subsectionHeader>
<bodyText confidence="0.999963333333333">
The STCs-based semantic parser is implemented
with linear kernel SVMs trained using the Lib-
SVM package (Chang and Lin, 2011). The SVM
misclassification cost parameters are optimised in-
dividually for each SVM classifier by performing
cross-validations on the training data.
</bodyText>
<equation confidence="0.996539666666667">
P(d|o) =
ri P(sv|u) 3 Rule-based Model
P (d|u) = P (d-typej|u)
sv∈S
ri (1 − P(sv|u)) (1)
sv/∈S
</equation>
<bodyText confidence="0.998496">
where u denotes an utterance and sv runs over all
possible slot-value pairs.
</bodyText>
<subsectionHeader confidence="0.984978">
2.2 Dialogue Context Features
</subsectionHeader>
<bodyText confidence="0.999863181818182">
In addition to the n-gram feature used in the orig-
inal STC parser, the dialogue context can be ex-
In this section, the rule-based model which is
slightly different from the focus tracker (Hender-
son et al., 2013) and HWU tracker (Wang, 2013) is
described. The idea of the rule-based model is to
maintain beliefs based on basic probability opera-
tions and a few heuristic rules that can be observed
on the training set. In the following the rule-based
model for joint goals, method and requested slots
are described in detail.
</bodyText>
<page confidence="0.998616">
319
</page>
<subsectionHeader confidence="0.999506">
3.1 Joint Goals
</subsectionHeader>
<bodyText confidence="0.999984">
For slot s, the i-th turn and value v, psi,v (p−s,i,v) is
used to denote the sum of all the confidence scores
assigned by the SLU to the user informing or af-
firming (denying or negating) the value of slot s is
v. The belief of “the value of slot s being v in the
i-th turn” denoted by bs,i,v is defined as follows.
</bodyText>
<listItem confidence="0.9421">
• If v =6 “None”,
</listItem>
<equation confidence="0.955452333333333">
bs,i,v = (bs,i−1,v + psi,v(1 − bs,i−1,v))
�(1 − p−s,i,v − p+s,i,v0)
v06=v
• Otherwise,
�bs,i,v = 1 − bs,i,v0
v06=“None”
</equation>
<bodyText confidence="0.9999635">
In particular, when i = 0, bs,0,v = 1 if v =
“None”, otherwise 0. The motivation here comes
from HWU tracker (Wang, 2013) that only p+ 8,·,v
positively contributes to the belief of slot s being
v, and both ps,·,v0 (v0 =6 v) and p−s,·,v contribute to
the belief negatively.
</bodyText>
<subsectionHeader confidence="0.999824">
3.2 Method
</subsectionHeader>
<bodyText confidence="0.9999318">
For the i-th turn, pi,m is used to denote the sum of
all the confidence scores assigned by the SLU to
method m. Then the belief of “the method being
m in the i-th turn” denoted by bi,m is defined as
follows.
</bodyText>
<listItem confidence="0.96861">
• If m =6 “none”,
</listItem>
<equation confidence="0.871557">
�bi,m = bi−1,m(1 − pi,m0) + pi,m
m06=“none”
• Otherwise,
�bi,m = 1 − bi,m0
m06=“none”
</equation>
<bodyText confidence="0.9998214">
In particular, b0,m = 0 when i = 0 and m =6
“none”. An explanation of the above formula
is given by Zilka et al. (2013). The idea is also
adopted by the focus tracker (Henderson et al.,
2013).
</bodyText>
<subsectionHeader confidence="0.99887">
3.3 Requested Slots
</subsectionHeader>
<bodyText confidence="0.9979362">
For the i-th turn and slot r, pi,r is used to denote
the sum of all the confidence scores assigned by
the SLU to r is one of the requested slots. Then
the belief of “r being one of the requested slots in
the i-th turn” denoted by bi,r is defined as follows.
</bodyText>
<listItem confidence="0.886291333333333">
• If i = 1, or system has at least one of
the following actions: “canthelp”, “offer”,
“reqmore”, “confirm-domain”, “expl-conf”,
“bye”, “request”,
bi,r = pi,r
• Otherwise,
</listItem>
<equation confidence="0.922948">
bi,r = bi−1,r(1 − pi,r) + pi,r
</equation>
<bodyText confidence="0.9981366">
This rule is a combination of the idea of HWU
tracker (Wang, 2013) and an observation from the
labelled data that once system has some certain ac-
tions, the statistics of requested slots from the past
turn should be reset.
</bodyText>
<sectionHeader confidence="0.998831" genericHeader="method">
4 Statistical Model
</sectionHeader>
<bodyText confidence="0.999758666666667">
In this section, two statistical models, one is the
MaxEnt model, the other is the DNN model, are
described.
</bodyText>
<subsectionHeader confidence="0.946743">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.9998485">
The performance of statistical models is highly de-
pendent on the feature functions.
</bodyText>
<subsectionHeader confidence="0.963511">
Joint Goals
</subsectionHeader>
<bodyText confidence="0.9996325">
For slot s, the i-th turn and value v, the feature
functions designed for joint goals are listed below.
</bodyText>
<listItem confidence="0.828095857142857">
• f1 °= inform(s, i, v) = the sum of all the
scores assigned by the SLU to the user in-
forming the value of slot s is v.
• f2 °= affirm(s, i, v) = the sum of all the
scores assigned by the SLU to the user af-
firming the value of slot s is v.
• f3 °= pos(s, i, v) = inform(s, i, v) +
affirm(s, i, v).
• f4 °= deny(s, i, v) = the sum of all the scores
assigned by the SLU to the user denying the
value of slot s is v.
• f5 °= negate(s, i, v) = the sum of all the
scores assigned by the SLU to the user negat-
ing the value of slot s is v.
</listItem>
<page confidence="0.779423">
320
</page>
<listItem confidence="0.945008782608696">
• f6 , neg(s, i, v) = deny(s, i, v) +
negate(s, i, v).
• f7 , acc(s, i, v) = pos(s, i, v) − neg(s, i, v).
• f8 , rule(s, i, v) = the confidence score
given by the rule-based model.
• f9 , rank inform(s, i, v) = the sum of all
the reciprocal rank of the scores assigned by
the SLU to the user informing the value of
slot s is v, or 0 if informing v cannot be found
in the SLU n-best list.
• f10 , rank affirm(s, i, v) = the sum of all
the reciprocal rank of the scores assigned by
the SLU to the user affirming the value of slot
s is v, or 0 if affirming v cannot be found in
the SLU n-best list.
• f11 , rank+(s, i, v) =
rank inform(s, i, v) +
rank affirm(s, i, v).
• f12 , rank deny(s, i, v) = the sum of all the
reciprocal rank of the scores assigned by the
SLU to the user denying the value of slot s
is v, or 0 if denying v cannot be found in the
SLU n-best list.
• f13 , rank negate(s, i, v) = the sum of all
the reciprocal rank of the scores assigned by
the SLU to the user negating the value of slot
s is v, or 0 if negating v cannot be found in
the SLU n-best list.
• f14 , rank−(s, i, v) = rank deny(s, i, v)+
rank negate(s, i, v).
• f15 , rank(s, i, v) = rank+(s, i, v) −
rank−(s, i, v).
• f16 , max(s, i, v) = the largest score given
by SLU to the user informing, affirming,
denying, or negating the value of slot s is v
from the 1-st turn.
• f17 , rest(s, i, v) = 1 if v = “None”, oth-
erwise 0.
° Ek=1 &lt;i pos(s,k,v)
• f18 = pos(s, i, v) = , which
i
is the arithmetic mean of pos(s, &apos;, v) from the
1-st turn to the i-th turn. Similarly, f19 ,
neg(s, i, v), f20 , rank+(s, i, v) and f21 ,
rank−(s, i, v) are defined.
• f22 ,(f22,1, f22,2, &apos;&apos; &apos; ,f22,10), where
</listItem>
<equation confidence="0.837593">
f22,j , bin pos(s, i, v, j) = totpos(s,i,v,j) ,
Z
</equation>
<bodyText confidence="0.999462944444444">
where totpos(s, i, v, j) = the total number
of slot-value pairs from the 1-st turn to the
i-th turn with slot s and value v which
will fall in the j-th bin if the range of
confidence scores is divided into 10 bins,
and Z = Ek≤i,1≤j&apos;≤10,v&apos; totpos(s, k, v0, j0),
which is the normalization factor. Simi-
larly, f23 ,(f23,1, f23,2, &apos;&apos;&apos; ,f23,10) where
f23,j , bin neg(s, i, v, j) is defined.
,
where totrule(s, i, v, j) = the total number
of rule(s, &apos;, v) from the 1-st turn to the i-
th turn which will fall in the j-th bin if the
range of rule(&apos;, &apos;, &apos;) is divided into 10 bins,
and Z = Ek≤i,1≤j&apos;≤10,v&apos; totrule(s, k, v0, j0),
which is the normalization factor. Simi-
larly, f25 ,(f25,1, f25,2, &apos; &apos; &apos; , f25,10) where
f25,j , bin rank(s, i, v, j), and f26 ,
</bodyText>
<construct confidence="0.500855">
(f26,1, f26,2, &apos; &apos; &apos; ,f26,10) where f26,j ,
bin acc(s, i, v, j) are defined.
</construct>
<listItem confidence="0.930181208333333">
• f27 ,(f27,1,f27,2, &apos; &apos; &apos; ,f27,10). Where
f27,j , bin max(s, i, v,j) = 1 if
max(s, i, v) will fall in the j-th bin if
the range of confidence scores is divided into
10 bins, otherwise 0.
• f28 , (f28,1,f28,2, &apos; &apos; &apos; ,f28,17). Where
f28,j , user acttype(s, i, v, uj) = the sum
of all the scores assigned by the SLU to the
user act type being uj(1 &lt; j &lt; 17). There
are a total of 17 different user act types de-
scribed in the handbook of DSTC 2 (Hender-
son et al., 2013).
• f29 , (f29,1, f29,2, &apos; &apos; &apos; ,f29,17). Where
f29,j , machine acttype(s, i, v, mj) = the
number of occurrences of act type mj(1 &lt;
j &lt; 17) in machine act. There are a total of
17 different machine act types described in
the handbook of DSTC 2 (Henderson et al.,
2013).
• f30 , canthelp(s, i, v) = 1 if the system can-
not offer a venue with the constrain s = v,
otherwise 0.
• f31 , slot confirmed(s, i, v) = 1 if the sys-
tem has confirmed s = v, otherwise 0.
</listItem>
<figure confidence="0.338531">
• f24 ,(f24,1, f24,2, &apos; &apos; &apos; ,f24,10). Where
, bin rules v totrule(s,i,v,j)
f24,j , i , , �) = Z
</figure>
<page confidence="0.948237">
321
</page>
<listItem confidence="0.9993212">
• f32 , slot requested(s, i, v) = 1 if the sys-
tem has requested the slot s, otherwise 0.
• f33 , slot informed(s, i, v) = 1 if the sys-
tem has informed s = v, otherwise 0.
• f34 , bias(s, i, v) = 1.
</listItem>
<bodyText confidence="0.792699">
In particular, all above feature function are 0
when i &lt; 0.
</bodyText>
<subsectionHeader confidence="0.496108">
Method
</subsectionHeader>
<bodyText confidence="0.9988805">
For the i-th turn and method m, the feature func-
tions designed for method are listed below.
</bodyText>
<listItem confidence="0.950434961538461">
• f1 , slu(i, m) = the sum of all the scores
assigned by the SLU to the method being m.
• f2 , rank(i, m) = the sum of all the recip-
rocal rank of the scores assigned by the SLU
to the method being m.
• f3 , rule(i, m) = the confidence score given
by the rule-based model.
�i k=1 slu(k,m)
• f4 , slu(i, m) = ,which is
i
the arithmetic mean of slu(·, m) from the
1-st turn to the i-th turn. Similarly, f5 ,
rank(i, m) and f6 , rule(i, m) are defined.
• f7 , score name(i) = the sum of all the
scores assigned by the SLU to the user in-
forming the value of slot name is some
value.
• f8 , venue offered(i) = 1 if at least one
venue has been offered to the user by the sys-
tem from the 1-st turn to the i-th turn, other-
wise 0.
• f9 ,(f9,1, f9,2, · · · , f9,17). Where f9,j ,
user acttype(i, uj) = the sum of all the
scores assigned by the SLU to the user act
type being uj(1 &lt; j &lt; 17).
• f10 , bias(i) = 1.
</listItem>
<bodyText confidence="0.85913">
In particular, all above feature function are 0
when i &lt; 0.
</bodyText>
<subsectionHeader confidence="0.697444">
Requested Slots
</subsectionHeader>
<bodyText confidence="0.9945315">
For the i-th turn and slot r, the feature functions
designed for requested slots are listed below.
</bodyText>
<listItem confidence="0.9791732">
•
f1 , slu(i, r) = the sum of all the scores
assigned by the SLU to r being one of the
requested slots.
• f2 , rank(i, r) = the sum of all the recipro-
cal rank of the scores assigned by the SLU to
r being one of the requested slots.
• f3 , rule(i, r) = the confidence score given
by the rule-based model.
• f4 , bias(i, r) = 1
</listItem>
<bodyText confidence="0.859433">
In particular, all above feature function are 0
when i &lt; 0.
</bodyText>
<subsectionHeader confidence="0.977242">
4.2 Maximum Entropy Model
</subsectionHeader>
<bodyText confidence="0.9999405">
Total 6 MaxEnt models (Bohus and Rudnicky,
2006) are employed, four models for the joint
goals, one for the method and one for the re-
quested slots. The Maximum Entropy model is an
efficient means that models the posterior of class
y given the observations x:
</bodyText>
<equation confidence="0.998666">
P(y|x) = Z(x) exp (AT f(y, x))
1
</equation>
<bodyText confidence="0.998141692307692">
Where Z(x) is the normalization constant. A is
the parameter vector and f(y, x) is the feature
vector.
The models for the joint goals are implemented
for four informable slots (i.e. area, food, name
and pricerange) separately. In the k-th turn, for
every informable slot s and its value v, i.e. slot-
value pair in SLU, the MaxEnt model for the cor-
responding slot is used to determine whether the
value v for the slot s in the user goals is right or
not. The input consists of 160 features 1 which
are selected from the feature functions described
in section 4.1 Joint Goals:
</bodyText>
<equation confidence="0.9819595">
�{f34}i=k U {f1,··· ,f15,f28,··· ,f33}
k−2&lt;i&lt;k
</equation>
<bodyText confidence="0.984371142857143">
Where i is the turn index. The output of the model
is the confidence score that the value v for the slot
s is right.
In the k-th turn, the model for the method is
used to determine which way the user is trying to
interact with the system. The input consists of 97
features which are selected from the feature func-
</bodyText>
<footnote confidence="0.8326165">
1For the feature function whose range is not 1 dimen-
sion, the number of features defined by the feature function
is counted as the number of dimensions rather than 1. For
example, the number of features defined by f28 is 17.
</footnote>
<page confidence="0.98884">
322
</page>
<bodyText confidence="0.583686">
tions described in section 4.1 Method:
</bodyText>
<equation confidence="0.9979472">
U{f10}i=k ∪ {f7, f8, f9}
k−3&lt;i&lt;k
∪ U {f3}
m
k − 3 ≤ i ≤ k
</equation>
<bodyText confidence="0.9999694">
and the output consists of five confidence scores
that the method belongs to every one of the five
ways (i.e. by name, by constraints, by alternatives,
finished and none).
The model for the requested slots is used to de-
termine whether the requestable slot r in the SLU
“request(slot)” is truly requested by the user or not
in the k-th turn. The input consists of 10 features
which are selected from the feature functions de-
scribed in section 4.1 Requested Slots:
</bodyText>
<equation confidence="0.9744215">
U{f4}i=k ∪ {f1, f2, f3}
k−2&lt;i&lt;k
</equation>
<bodyText confidence="0.996500857142857">
and the output is the confidence score that r is truly
requested by the user in this turn.
The parameters of the 6 MaxEnt models are op-
timised separately through maximizing the likeli-
hood of the training data. The training process is
stopped when the likelihood change is less than
10−4.
</bodyText>
<subsectionHeader confidence="0.997677">
4.3 Deep Neural Network Model
</subsectionHeader>
<bodyText confidence="0.99904575">
4 DNNs for joint goals (one for each slot), 1 for
method and 1 for requested slots are employed.
All of them have similar structure with Sigmoid
for hidden layer activation and Softmax for out-
put layer activation. As shown in figure 1, each
DNN has 3 hidden layers and each layer has 64
nodes. DNNs take the feature set (which will be
described in detail later) of a certain value of goal,
method, or requested slots as the input, then out-
put two values (donated by X and Y ), through the
hidden layer processing, and finally the confidence
of the value can be got by eX
</bodyText>
<equation confidence="0.565836">
eX+eY .
</equation>
<bodyText confidence="0.996158">
For slot s, the k-th turn and value v, the feature
set of goal consisting of 108 features is defined as:
</bodyText>
<equation confidence="0.9022905">
U {f3, f6, f7, f8, f11, f14, f15}
k−5&lt;i&lt;k
∪ {f18, · · · , f21}i=k−6
∪ {f16, f17, f22, · · · , f27}i=k
</equation>
<bodyText confidence="0.9990145">
For the k-th turn and method m, the feature set
of method consisting of 15 features is defined as:
</bodyText>
<equation confidence="0.482665">
U {f1, f2, f3} ∪ {f4, f5, f6}i=k−4
k−3&lt;i&lt;k
</equation>
<figureCaption confidence="0.999891">
Figure 1: Structure of the DNN Model
</figureCaption>
<bodyText confidence="0.997347">
For the k-th turn and slot r, the feature set of
requested slots consisting of 12 features is defined
as:
</bodyText>
<equation confidence="0.971762">
U {f1, f2, f3}
k−3&lt;i&lt;k
</equation>
<bodyText confidence="0.999221">
Bernoulli-Bernoulli RBM was applied to pre-
train DNNs and Stochastic Gradient Descent with
cross-entropy criterion to fine-tune DNNs. For the
fine-tuning process, 3/4 of the data was used for
training and 1/4 for validation.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999936583333333">
DSTC 2 provides a training dataset of 1612 dia-
logues (11677 utterances) and a development set
of 506 dialogues (3934 utterances). The training
data was first used to train the semantic parser
and the MaxEnt and the DNN models for internal
system development as shown in section 5.1 and
5.2. These systems were tested on the develop-
ment data. Once the system setup and parameters
were determined, the training and development set
were combined together to train the final submit-
ted system. The final system was then tested on
the final evaluation data as shown in section 5.3.
</bodyText>
<subsectionHeader confidence="0.99776">
5.1 Effect of the STC Semantic Parser
</subsectionHeader>
<bodyText confidence="0.976198888888889">
In DSTC 2, as the live semantic information was
found to be poor, two new semantic parsers were
then trained as described in section 2. One used
the top ASR hypothesis n-gram features and the
other one employed additional system feedback
features (the last system act, “barge-in” and turn-
index).
Table 1 shows the performance of two new se-
mantic parser in terms of the precision, recall,
</bodyText>
<figure confidence="0.863343833333333">
An input layer 3 hidden layers An output layer
with |feature_set |nodes Each has 64 nodes with 2 nodes
...
...
...
...
</figure>
<page confidence="0.997154">
323
</page>
<table confidence="0.9998165">
System Precision Recall F-score ICE
baseline 0.6659 0.8827 0.7591 2.1850
1-best 0.7265 0.8894 0.7997 1.4529
+ sys fb 0.7327 0.8969 0.8065 1.3449
</table>
<tableCaption confidence="0.7760455">
Table 1: Performance of semantic parsers with dif-
ferent features on the development set.
</tableCaption>
<bodyText confidence="0.99892152631579">
F-score of top dialogue act hypothesis and the
Item Cross Entropy (ICE) (Thomson et al., 2008)
which measures the overall quality of the confi-
dences distribution of semantic items (the less the
better). The baseline is the original live seman-
tic hypotheses, “1-best” (row 3) represents the se-
mantic parser trained on the top ASR hypothesis
with n-gram feature, and “sys fb” (row 4) rep-
resents the semantic parser added with the sys-
tem feedback features. The STC semantic parsers
significantly improve the quality of semantic hy-
potheses compared with baseline in the score of
precision, recall, F-score and ICE. And the parser
using context features (row 4) scored better than
the other one (row 3).
The improved semantic parsers are expected to
also yield better performance in dialogue state
tracking. Hence, the parsers were used in focus
baseline provided by the organiser. As shown in
</bodyText>
<table confidence="0.99852275">
Joint Goals Method Requested
baseline 0.6121 0.8303 0.8936
1-best 0.6613 0.8764 0.8987
+ sys fb 0.6765 0.8764 0.9297
</table>
<tableCaption confidence="0.9750205">
Table 2: Results for focus baseline tracker with
different parsers
</tableCaption>
<bodyText confidence="0.9720446">
table 2, the new parsers achieved consistent im-
provement on the accuracy of joint goals, method
and requested slots. So the semantic hypotheses
of parser using the system feedback features was
used for later development.
</bodyText>
<subsectionHeader confidence="0.993287">
5.2 Internal System Development
</subsectionHeader>
<bodyText confidence="0.99519975">
Table 3 shows the the results of rule-based model,
the MaxEnt model and the DNN model on the de-
velopment set. From the table we can see that
the DNN model has the best performance for joint
goals, the MaxEnt model has the best performance
for method and the rule-based model has the best
performance for requested slots. So the combined
model is a combination of those three models, one
for one of the three components where it has the
best performance, that is, the rule-based model
for requested slots, the MaxEnt model for method,
and the DNN model for joint goals.
</bodyText>
<table confidence="0.99916325">
Joint Goals Method Requested
Rule-based 0.6890 0.8955 0.9668
MaxEnt 0.6741 0.9079 0.9665
DNN 0.6906 0.8991 0.9661
</table>
<tableCaption confidence="0.99965">
Table 3: Performance of three tracking models
</tableCaption>
<subsectionHeader confidence="0.989343">
5.3 Evaluation Performance
</subsectionHeader>
<bodyText confidence="0.999366181818182">
The official results of the challenge are publicly
available and the SJTU team is team 7. Entry
0, 1, 2, 3 of team 7 is the combined model, the
rule-based model, the DNN model and the Max-
Ent model respectively. They all used the new se-
mantic parser based on live ASR hypotheses. En-
try 4 of team 7 is also a combined model but it
does not use the new semantic parser and takes the
live SLU as input.
Table 4 shows the results on the final evalua-
tion test set. As expected, the semantic parser does
work, and the combined model has the best perfor-
mance for joint goals and method, however, that
is not true for requested slots. Notice that on the
development set, the difference of the accuracy of
requested slots among the 3 models is significantly
smaller than that of joint goals and method. One
reasonable explanation is that one cannot claim
that the rule-based model has better performance
for requested slots than the MaxEnt model and the
DNN model only with an accuracy advantage less
than 0.1%.
</bodyText>
<table confidence="0.9993732">
Joint Goals Method Requested
Baseline 0.6191 0.8788 0.8842
Focus 0.7193 0.8670 0.8786
HWU 0.7108 0.8971 0.8844
HWU+ 0.6662 0.8846 0.8830
Rule-based 0.7387 0.9207 0.9701
MaxEnt 0.7252 0.9357 0.9717
DNN 0.7503 0.9287 0.9710
Combined+ 0.7503 0.9357 0.9701
Combined- 0.7346 0.9102 0.9458
</table>
<tableCaption confidence="0.995662">
Table 4: Accuracy of the combined model (Com-
</tableCaption>
<bodyText confidence="0.9931691">
bined+) compared with the rule-based model,
the MaxEnt model, the DNN model, the com-
bined model without the new semantic parser
(Combined-) and four baselines on the test set.
Four baselines are the baseline tracker (Base-
line), the focus tracker (Focus), the HWU tracker
(HWU) and the HWU tracker with “original” flag
set to (HWU+) respectively.
Figure 2 summaries the performance of the ap-
proach relative to all 31 entries in the DSTC 2.
</bodyText>
<page confidence="0.989602">
324
</page>
<figure confidence="0.99961175">
0.5
0.4
0.3
Max
Min
Median
Quartile
SJTU
0.9
0.8
0.7
0.6
0.2
0.1
0
1
0.96
0.92
0.88
0.84
0.8
0.76
0.72
0.68
0.64
0.6
0.56
0.52
0.48
Max
Min
Median
Quartile
SJTU
Joint Goals Method Requested Slots Joint Goals Method Requested Slots
(a) Accuracy (b) L2
</figure>
<figureCaption confidence="0.9651575">
Figure 2: Performance of the combined model among 31 trackers. SJTU is the combined model (entry 0
of team 7).
</figureCaption>
<bodyText confidence="0.9999312">
As ROC metric is only comparable between sys-
tems of similar accuracy, only accuracy and L2
are compared. The results of the combined model
is competitive for all the three components, espe-
cially for joint goals.
</bodyText>
<subsectionHeader confidence="0.991494">
5.4 Post Evaluation Analysis
</subsectionHeader>
<bodyText confidence="0.89347978125">
Two strategies and two kinds of features were
added to the MaxEnt model for the requested slots
after DSTC 2 based on some observations on the
training set and development set. The first strategy
is that the output for the requested slots of the first
turn is set to empty by force. The second strategy
is that the output of the confidence is additionally
multiplied by (1−Cf), where Cf denotes the con-
fidence given by the MaxEnt model to the method
of current turn being finished. As for the two kinds
of features, one is the slot indicator and the other
is the acttype-slot tuple. They are defined as 2:
• f5 �(f5,1, f5,2, &apos; &apos; &apos; , f5,8), where f5,j �
slot indicator(i, r, sj) = 1 if the index of the
slot r is j, i.e. sj = r, otherwise 0.
• f6 �(f6,1, f6,2, &apos; &apos; &apos; , f6,33), where f6,j �
user act slot(i, r, tj) = the sum of all the
scores assigned by the SLU to the j-th user
acttype-slot tuple tj. The acttype-slot tuple is
the combination of dialog act type and possi-
ble slot, e.g. inform-food, confirm-area.
There are 33 user acttype-slot tuples.
2The feature number is consistent with that in section 4.1.
• f7 �(f7,1, f7,2, &apos; &apos; &apos; , f7,46), where f7,j �
sys act slot(i, r, tj) =the number of occur-
rences of the j-th machine acttype-slot tuple
tj in the dialog acts. There are 46 machine
acttype-slot tuples.
With those strategies and features, the Max-
Ent model achieved an accuracy of 0.9769 for the
requested slots, which is significantly improved
compared with the submitted system.
</bodyText>
<sectionHeader confidence="0.999734" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993444444444">
This paper describes the SJTU submission for
DSTC 2 in detail. It is a combined system con-
sisting of a rule-based model, a maximum entropy
model and a deep neural network model with a
STC semantic parser. The results show that the
SJTU system is competitive and outperforms most
of the other systems in DSTC 2 on test datasets.
Post evaluation analysis reveal that there is still
room for improvement by refining the features.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99920625">
This work was supported by the Program for Pro-
fessor of Special Appointment (Eastern Scholar)
at Shanghai Institutions of Higher Learning and
the China NSFC project No. 61222208.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9760695">
Blaise Thomson, Kai Yu, Milica Gasic, Simon Keizer,
Francois Mairesse, Jost Schatzmann and Steve
</reference>
<page confidence="0.98736">
325
</page>
<reference confidence="0.999842">
Young. 2008. Evaluating semantic-level confi-
dence scores with multiple hypotheses. In INTER-
SPEECH, pp. 1153-1156.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3), 27.
Dan Bohus and Alex Rudnicky. 2006. A K-hypotheses
+ Other Belief Updating Model. In Proc. of AAAI
Workshop on Statistical and Empirical Approaches
for Spoken Dialogue Systems.
Franc¸ois Mairesse, Milica Gasic, Filip Jurc´ıcek, Simon
Keizer, Blaise Thomson, Kai Yu and Steve Young.
2009. Spoken language understanding from un-
aligned data using discriminative classification mod-
els. Acoustics, Speech and Signal Processing, 2009.
ICASSP 2009. IEEE International Conference on,
pp. 4749-4752. IEEE.
Jason Williams, Antoine Raux, Deepak Ramachandran
and Alan Black. 2013. The Dialog State Tracking
Challenge. In SIGDIAL.
Lukas Zilka, David Marek, Matej Korvas and Filip Ju-
rcicek. 2013. Comparison of Bayesian Discrim-
inative and Generative Models for Dialogue State
Tracking. In SIGDIAL.
Matthew Henderson, Milica Gasic, Blaise Thomson,
Pirros Tsiakoulis, Kai Yu and Steve Young. 2012.
Discriminative spoken language understanding us-
ing word confusion networks. Spoken Language
Technology Workshop (SLT), 2012 IEEE, pp. 176-
181. IEEE.
Matthew Henderson, Blaise Thomson and Steve
Young. 2013. Deep Neural Network Approach for
the Dialog State Tracking Challenge. In SIGDIAL.
Matthew Henderson, Blaise Thomson and Jason
Williams. 2013. Dialog State Tracking Challenge
2 &amp; 3. Technical report, University of Cambridge.
Matthew Henderson, Blaise Thomson and Jason
Williams. 2014. The Second Dialog State Track-
ing Challenge. In SIGDIAL.
Sungjin Lee and Maxine Eskenazi. 2013. Recipe For
Building Robust Spoken Dialog State Trackers: Di-
alog State Tracking Challenge System Description.
In SIGDIAL.
Zhuoran Wang and Oliver Lemon. 2013. A Simple
and Generic Belief Tracking Mechanism for the Di-
alog State Tracking Challenge: On the believability
of observed information. In SIGDIAL.
Zhuoran Wang. 2013. HWU Baseline Belief Tracker
for DSTC 2 &amp; 3. Technical report, Heriot-Watt Uni-
versity.
</reference>
<page confidence="0.999113">
326
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588170">
<title confidence="0.940988">The SJTU System for Dialog State Tracking Challenge 2</title>
<author confidence="0.99836">Kai Sun</author>
<author confidence="0.99836">Lu Chen</author>
<author confidence="0.99836">Su Zhu</author>
<author confidence="0.99836">Kai</author>
<affiliation confidence="0.999507">Department of Computer Science and Engineering, Shanghai Jiao Tong</affiliation>
<address confidence="0.894057">Shanghai,</address>
<email confidence="0.816389">chenlusz,paul2204,</email>
<abstract confidence="0.989996588235294">Dialog state tracking challenge provides a common testbed for state tracking algorithms. This paper describes the SJTU system submitted to the second Dialogue State Tracking Challenge in detail. In the system, a statistical semantic parser is used to generate refined semantic hypotheses. A large number of features are then derived based on the semantic hypotheses and the dialogue log information. The final tracker is a combination of a rulebased model, a maximum entropy and a deep neural network model. The SJTU system significantly outperformed all the baselines and showed competitive performance in DSTC 2.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>Milica Gasic, Simon Keizer, Francois Mairesse, Jost Schatzmann and Steve</title>
<marker>Thomson, Yu, </marker>
<rawString>Blaise Thomson, Kai Yu, Milica Gasic, Simon Keizer, Francois Mairesse, Jost Schatzmann and Steve</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young</author>
</authors>
<title>Evaluating semantic-level confidence scores with multiple hypotheses.</title>
<date>2008</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1153--1156</pages>
<marker>Young, 2008</marker>
<rawString>Young. 2008. Evaluating semantic-level confidence scores with multiple hypotheses. In INTERSPEECH, pp. 1153-1156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology (TIST),</journal>
<volume>2</volume>
<issue>3</issue>
<pages>27</pages>
<contexts>
<context position="6964" citStr="Chang and Lin, 2011" startWordPosition="1152" endWordPosition="1155">s Di with mi distinct dialogue act hypotheses would be merged in following way: � N p(hi|o)p(d|hi) if d E Di 0 otherwise i=1 where o is the acoustic observation, d runs over each different dialogue act in Di, i = 1, ..., N, p(hi|o) denotes the ASR posterior probability of the i-th hypothesis, p(d|hi) denotes the semantic posterior probability given the i-th ASR hypothesis as in equation (1). Finally, a normalization should be done to guarantee the sum of P(d|o) to be one. 2.4 Implementation The STCs-based semantic parser is implemented with linear kernel SVMs trained using the LibSVM package (Chang and Lin, 2011). The SVM misclassification cost parameters are optimised individually for each SVM classifier by performing cross-validations on the training data. P(d|o) = ri P(sv|u) 3 Rule-based Model P (d|u) = P (d-typej|u) sv∈S ri (1 − P(sv|u)) (1) sv/∈S where u denotes an utterance and sv runs over all possible slot-value pairs. 2.2 Dialogue Context Features In addition to the n-gram feature used in the original STC parser, the dialogue context can be exIn this section, the rule-based model which is slightly different from the focus tracker (Henderson et al., 2013) and HWU tracker (Wang, 2013) is descri</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3), 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alex Rudnicky</author>
</authors>
<title>A K-hypotheses + Other Belief Updating Model.</title>
<date>2006</date>
<booktitle>In Proc. of AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems.</booktitle>
<contexts>
<context position="16140" citStr="Bohus and Rudnicky, 2006" startWordPosition="3051" endWordPosition="3054">lar, all above feature function are 0 when i &lt; 0. Requested Slots For the i-th turn and slot r, the feature functions designed for requested slots are listed below. • f1 , slu(i, r) = the sum of all the scores assigned by the SLU to r being one of the requested slots. • f2 , rank(i, r) = the sum of all the reciprocal rank of the scores assigned by the SLU to r being one of the requested slots. • f3 , rule(i, r) = the confidence score given by the rule-based model. • f4 , bias(i, r) = 1 In particular, all above feature function are 0 when i &lt; 0. 4.2 Maximum Entropy Model Total 6 MaxEnt models (Bohus and Rudnicky, 2006) are employed, four models for the joint goals, one for the method and one for the requested slots. The Maximum Entropy model is an efficient means that models the posterior of class y given the observations x: P(y|x) = Z(x) exp (AT f(y, x)) 1 Where Z(x) is the normalization constant. A is the parameter vector and f(y, x) is the feature vector. The models for the joint goals are implemented for four informable slots (i.e. area, food, name and pricerange) separately. In the k-th turn, for every informable slot s and its value v, i.e. slotvalue pair in SLU, the MaxEnt model for the corresponding</context>
</contexts>
<marker>Bohus, Rudnicky, 2006</marker>
<rawString>Dan Bohus and Alex Rudnicky. 2006. A K-hypotheses + Other Belief Updating Model. In Proc. of AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Mairesse</author>
<author>Milica Gasic</author>
<author>Filip Jurc´ıcek</author>
<author>Simon Keizer</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
<author>Steve Young</author>
</authors>
<title>Spoken language understanding from unaligned data using discriminative classification models. Acoustics, Speech and Signal Processing,</title>
<date>2009</date>
<booktitle>ICASSP 2009. IEEE International Conference on,</booktitle>
<pages>4749--4752</pages>
<publisher>IEEE.</publisher>
<marker>Mairesse, Gasic, Jurc´ıcek, Keizer, Thomson, Yu, Young, 2009</marker>
<rawString>Franc¸ois Mairesse, Milica Gasic, Filip Jurc´ıcek, Simon Keizer, Blaise Thomson, Kai Yu and Steve Young. 2009. Spoken language understanding from unaligned data using discriminative classification models. Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on, pp. 4749-4752. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Williams</author>
</authors>
<title>Antoine Raux, Deepak Ramachandran and Alan Black.</title>
<date>2013</date>
<booktitle>In SIGDIAL.</booktitle>
<marker>Williams, 2013</marker>
<rawString>Jason Williams, Antoine Raux, Deepak Ramachandran and Alan Black. 2013. The Dialog State Tracking Challenge. In SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lukas Zilka</author>
<author>David Marek</author>
<author>Matej Korvas</author>
<author>Filip Jurcicek</author>
</authors>
<title>Comparison of Bayesian Discriminative and Generative Models for Dialogue State Tracking.</title>
<date>2013</date>
<booktitle>In SIGDIAL.</booktitle>
<contexts>
<context position="1463" citStr="Zilka et al., 2013" startWordPosition="227" endWordPosition="230">nce in DSTC 2. 1 Introduction Dialog state tracking is important because spoken dialog systems rely on it to choose proper actions as spoken dialog systems interact with users. However, due to automatic speech recognition (ASR) and spoken language understanding (SLU) errors, it is not easy for the dialog manager to maintain the true state of the dialog. In recent years, much research has been devoted to dialog state tracking. Many approaches have been applied to dialog state tracking, from rule-based to statistical models, from generative models to discriminative models (Wang and Lemon, 2013; Zilka et al., 2013; Henderson et al., 2013; Lee and Eskenazi, 2013). Recently, shared research tasks like the first Dialog State Tracking Challenge (DSTC 1) (Williams et al., 2013) have provided a common testbed and evaluation suite for dialog state tracking (Henderson et al., 2013). Compared with DSTC 1 which is in the bus timetables domain, DSTC 2 introduces more complicated and dynamic dialog states, which may change through the dialog, in a new domain, i.e. restaurants domain (Henderson et al., 2014). For each turn, a tracker is supposed to output a set of distributions for each of the three components of t</context>
<context position="2842" citStr="Zilka et al., 2013" startWordPosition="460" endWordPosition="463"> the dialog up until that turn; the method is the way the user is trying to interact with the system which may be by name, by constraints, by alternatives or finished; and the requested slots consist of the slots which have been requested by the user and not replied by the system. For evaluation in DSTC 2, 1-best quality measured by accuracy, probability calibration measured by L2, and discrimination measured by ROC are selected as featured metrics. Further details can be found in the DSTC 2 handbook (Henderson et al., 2013). Previous research has demonstrated the effectiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Lee and Eskenazi, 2013) and deep neural network (DNN) (Henderson et al., 2013) models separately. Motivated by this, the SJTU system employs a combination of a rulebased model, a MaxEnt and a DNN model. The three models were first trained (if necessary) on the training set and tested for each of the three components of the dialog state, i.e goals, method, and requested slots on the development set. Then, models with the best performance for each of the three components were selected to form a combined model. Finally, the combined model was retrained using both train</context>
<context position="9026" citStr="Zilka et al. (2013)" startWordPosition="1535" endWordPosition="1538"> HWU tracker (Wang, 2013) that only p+ 8,·,v positively contributes to the belief of slot s being v, and both ps,·,v0 (v0 =6 v) and p−s,·,v contribute to the belief negatively. 3.2 Method For the i-th turn, pi,m is used to denote the sum of all the confidence scores assigned by the SLU to method m. Then the belief of “the method being m in the i-th turn” denoted by bi,m is defined as follows. • If m =6 “none”, �bi,m = bi−1,m(1 − pi,m0) + pi,m m06=“none” • Otherwise, �bi,m = 1 − bi,m0 m06=“none” In particular, b0,m = 0 when i = 0 and m =6 “none”. An explanation of the above formula is given by Zilka et al. (2013). The idea is also adopted by the focus tracker (Henderson et al., 2013). 3.3 Requested Slots For the i-th turn and slot r, pi,r is used to denote the sum of all the confidence scores assigned by the SLU to r is one of the requested slots. Then the belief of “r being one of the requested slots in the i-th turn” denoted by bi,r is defined as follows. • If i = 1, or system has at least one of the following actions: “canthelp”, “offer”, “reqmore”, “confirm-domain”, “expl-conf”, “bye”, “request”, bi,r = pi,r • Otherwise, bi,r = bi−1,r(1 − pi,r) + pi,r This rule is a combination of the idea of HWU </context>
</contexts>
<marker>Zilka, Marek, Korvas, Jurcicek, 2013</marker>
<rawString>Lukas Zilka, David Marek, Matej Korvas and Filip Jurcicek. 2013. Comparison of Bayesian Discriminative and Generative Models for Dialogue State Tracking. In SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
</authors>
<title>Milica Gasic, Blaise Thomson, Pirros Tsiakoulis, Kai Yu and Steve Young.</title>
<date>2012</date>
<booktitle>Spoken Language Technology Workshop (SLT), 2012 IEEE,</booktitle>
<pages>176--181</pages>
<publisher>IEEE.</publisher>
<marker>Henderson, 2012</marker>
<rawString>Matthew Henderson, Milica Gasic, Blaise Thomson, Pirros Tsiakoulis, Kai Yu and Steve Young. 2012. Discriminative spoken language understanding using word confusion networks. Spoken Language Technology Workshop (SLT), 2012 IEEE, pp. 176-181. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Deep Neural Network Approach for the Dialog State Tracking Challenge.</title>
<date>2013</date>
<booktitle>In SIGDIAL.</booktitle>
<contexts>
<context position="1487" citStr="Henderson et al., 2013" startWordPosition="231" endWordPosition="234">roduction Dialog state tracking is important because spoken dialog systems rely on it to choose proper actions as spoken dialog systems interact with users. However, due to automatic speech recognition (ASR) and spoken language understanding (SLU) errors, it is not easy for the dialog manager to maintain the true state of the dialog. In recent years, much research has been devoted to dialog state tracking. Many approaches have been applied to dialog state tracking, from rule-based to statistical models, from generative models to discriminative models (Wang and Lemon, 2013; Zilka et al., 2013; Henderson et al., 2013; Lee and Eskenazi, 2013). Recently, shared research tasks like the first Dialog State Tracking Challenge (DSTC 1) (Williams et al., 2013) have provided a common testbed and evaluation suite for dialog state tracking (Henderson et al., 2013). Compared with DSTC 1 which is in the bus timetables domain, DSTC 2 introduces more complicated and dynamic dialog states, which may change through the dialog, in a new domain, i.e. restaurants domain (Henderson et al., 2014). For each turn, a tracker is supposed to output a set of distributions for each of the three components of the dialog state: goals, </context>
<context position="2753" citStr="Henderson et al., 2013" startWordPosition="447" endWordPosition="450">n, the goals consists of the user’s true required value having been revealed for each slot in the dialog up until that turn; the method is the way the user is trying to interact with the system which may be by name, by constraints, by alternatives or finished; and the requested slots consist of the slots which have been requested by the user and not replied by the system. For evaluation in DSTC 2, 1-best quality measured by accuracy, probability calibration measured by L2, and discrimination measured by ROC are selected as featured metrics. Further details can be found in the DSTC 2 handbook (Henderson et al., 2013). Previous research has demonstrated the effectiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Lee and Eskenazi, 2013) and deep neural network (DNN) (Henderson et al., 2013) models separately. Motivated by this, the SJTU system employs a combination of a rulebased model, a MaxEnt and a DNN model. The three models were first trained (if necessary) on the training set and tested for each of the three components of the dialog state, i.e goals, method, and requested slots on the development set. Then, models with the best performance for each of the three components were sele</context>
<context position="7525" citStr="Henderson et al., 2013" startWordPosition="1245" endWordPosition="1249">el SVMs trained using the LibSVM package (Chang and Lin, 2011). The SVM misclassification cost parameters are optimised individually for each SVM classifier by performing cross-validations on the training data. P(d|o) = ri P(sv|u) 3 Rule-based Model P (d|u) = P (d-typej|u) sv∈S ri (1 − P(sv|u)) (1) sv/∈S where u denotes an utterance and sv runs over all possible slot-value pairs. 2.2 Dialogue Context Features In addition to the n-gram feature used in the original STC parser, the dialogue context can be exIn this section, the rule-based model which is slightly different from the focus tracker (Henderson et al., 2013) and HWU tracker (Wang, 2013) is described. The idea of the rule-based model is to maintain beliefs based on basic probability operations and a few heuristic rules that can be observed on the training set. In the following the rule-based model for joint goals, method and requested slots are described in detail. 319 3.1 Joint Goals For slot s, the i-th turn and value v, psi,v (p−s,i,v) is used to denote the sum of all the confidence scores assigned by the SLU to the user informing or affirming (denying or negating) the value of slot s is v. The belief of “the value of slot s being v in the i-th</context>
<context position="9098" citStr="Henderson et al., 2013" startWordPosition="1548" endWordPosition="1551">to the belief of slot s being v, and both ps,·,v0 (v0 =6 v) and p−s,·,v contribute to the belief negatively. 3.2 Method For the i-th turn, pi,m is used to denote the sum of all the confidence scores assigned by the SLU to method m. Then the belief of “the method being m in the i-th turn” denoted by bi,m is defined as follows. • If m =6 “none”, �bi,m = bi−1,m(1 − pi,m0) + pi,m m06=“none” • Otherwise, �bi,m = 1 − bi,m0 m06=“none” In particular, b0,m = 0 when i = 0 and m =6 “none”. An explanation of the above formula is given by Zilka et al. (2013). The idea is also adopted by the focus tracker (Henderson et al., 2013). 3.3 Requested Slots For the i-th turn and slot r, pi,r is used to denote the sum of all the confidence scores assigned by the SLU to r is one of the requested slots. Then the belief of “r being one of the requested slots in the i-th turn” denoted by bi,r is defined as follows. • If i = 1, or system has at least one of the following actions: “canthelp”, “offer”, “reqmore”, “confirm-domain”, “expl-conf”, “bye”, “request”, bi,r = pi,r • Otherwise, bi,r = bi−1,r(1 − pi,r) + pi,r This rule is a combination of the idea of HWU tracker (Wang, 2013) and an observation from the labelled data that once</context>
<context position="13690" citStr="Henderson et al., 2013" startWordPosition="2518" endWordPosition="2522">5 ,(f25,1, f25,2, &apos; &apos; &apos; , f25,10) where f25,j , bin rank(s, i, v, j), and f26 , (f26,1, f26,2, &apos; &apos; &apos; ,f26,10) where f26,j , bin acc(s, i, v, j) are defined. • f27 ,(f27,1,f27,2, &apos; &apos; &apos; ,f27,10). Where f27,j , bin max(s, i, v,j) = 1 if max(s, i, v) will fall in the j-th bin if the range of confidence scores is divided into 10 bins, otherwise 0. • f28 , (f28,1,f28,2, &apos; &apos; &apos; ,f28,17). Where f28,j , user acttype(s, i, v, uj) = the sum of all the scores assigned by the SLU to the user act type being uj(1 &lt; j &lt; 17). There are a total of 17 different user act types described in the handbook of DSTC 2 (Henderson et al., 2013). • f29 , (f29,1, f29,2, &apos; &apos; &apos; ,f29,17). Where f29,j , machine acttype(s, i, v, mj) = the number of occurrences of act type mj(1 &lt; j &lt; 17) in machine act. There are a total of 17 different machine act types described in the handbook of DSTC 2 (Henderson et al., 2013). • f30 , canthelp(s, i, v) = 1 if the system cannot offer a venue with the constrain s = v, otherwise 0. • f31 , slot confirmed(s, i, v) = 1 if the system has confirmed s = v, otherwise 0. • f24 ,(f24,1, f24,2, &apos; &apos; &apos; ,f24,10). Where , bin rules v totrule(s,i,v,j) f24,j , i , , �) = Z 321 • f32 , slot requested(s, i, v) = 1 if the </context>
</contexts>
<marker>Henderson, Thomson, Young, 2013</marker>
<rawString>Matthew Henderson, Blaise Thomson and Steve Young. 2013. Deep Neural Network Approach for the Dialog State Tracking Challenge. In SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Jason Williams</author>
</authors>
<date>2013</date>
<journal>Dialog State Tracking Challenge</journal>
<tech>Technical report,</tech>
<volume>2</volume>
<institution>University of Cambridge.</institution>
<contexts>
<context position="1487" citStr="Henderson et al., 2013" startWordPosition="231" endWordPosition="234">roduction Dialog state tracking is important because spoken dialog systems rely on it to choose proper actions as spoken dialog systems interact with users. However, due to automatic speech recognition (ASR) and spoken language understanding (SLU) errors, it is not easy for the dialog manager to maintain the true state of the dialog. In recent years, much research has been devoted to dialog state tracking. Many approaches have been applied to dialog state tracking, from rule-based to statistical models, from generative models to discriminative models (Wang and Lemon, 2013; Zilka et al., 2013; Henderson et al., 2013; Lee and Eskenazi, 2013). Recently, shared research tasks like the first Dialog State Tracking Challenge (DSTC 1) (Williams et al., 2013) have provided a common testbed and evaluation suite for dialog state tracking (Henderson et al., 2013). Compared with DSTC 1 which is in the bus timetables domain, DSTC 2 introduces more complicated and dynamic dialog states, which may change through the dialog, in a new domain, i.e. restaurants domain (Henderson et al., 2014). For each turn, a tracker is supposed to output a set of distributions for each of the three components of the dialog state: goals, </context>
<context position="2753" citStr="Henderson et al., 2013" startWordPosition="447" endWordPosition="450">n, the goals consists of the user’s true required value having been revealed for each slot in the dialog up until that turn; the method is the way the user is trying to interact with the system which may be by name, by constraints, by alternatives or finished; and the requested slots consist of the slots which have been requested by the user and not replied by the system. For evaluation in DSTC 2, 1-best quality measured by accuracy, probability calibration measured by L2, and discrimination measured by ROC are selected as featured metrics. Further details can be found in the DSTC 2 handbook (Henderson et al., 2013). Previous research has demonstrated the effectiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Lee and Eskenazi, 2013) and deep neural network (DNN) (Henderson et al., 2013) models separately. Motivated by this, the SJTU system employs a combination of a rulebased model, a MaxEnt and a DNN model. The three models were first trained (if necessary) on the training set and tested for each of the three components of the dialog state, i.e goals, method, and requested slots on the development set. Then, models with the best performance for each of the three components were sele</context>
<context position="7525" citStr="Henderson et al., 2013" startWordPosition="1245" endWordPosition="1249">el SVMs trained using the LibSVM package (Chang and Lin, 2011). The SVM misclassification cost parameters are optimised individually for each SVM classifier by performing cross-validations on the training data. P(d|o) = ri P(sv|u) 3 Rule-based Model P (d|u) = P (d-typej|u) sv∈S ri (1 − P(sv|u)) (1) sv/∈S where u denotes an utterance and sv runs over all possible slot-value pairs. 2.2 Dialogue Context Features In addition to the n-gram feature used in the original STC parser, the dialogue context can be exIn this section, the rule-based model which is slightly different from the focus tracker (Henderson et al., 2013) and HWU tracker (Wang, 2013) is described. The idea of the rule-based model is to maintain beliefs based on basic probability operations and a few heuristic rules that can be observed on the training set. In the following the rule-based model for joint goals, method and requested slots are described in detail. 319 3.1 Joint Goals For slot s, the i-th turn and value v, psi,v (p−s,i,v) is used to denote the sum of all the confidence scores assigned by the SLU to the user informing or affirming (denying or negating) the value of slot s is v. The belief of “the value of slot s being v in the i-th</context>
<context position="9098" citStr="Henderson et al., 2013" startWordPosition="1548" endWordPosition="1551">to the belief of slot s being v, and both ps,·,v0 (v0 =6 v) and p−s,·,v contribute to the belief negatively. 3.2 Method For the i-th turn, pi,m is used to denote the sum of all the confidence scores assigned by the SLU to method m. Then the belief of “the method being m in the i-th turn” denoted by bi,m is defined as follows. • If m =6 “none”, �bi,m = bi−1,m(1 − pi,m0) + pi,m m06=“none” • Otherwise, �bi,m = 1 − bi,m0 m06=“none” In particular, b0,m = 0 when i = 0 and m =6 “none”. An explanation of the above formula is given by Zilka et al. (2013). The idea is also adopted by the focus tracker (Henderson et al., 2013). 3.3 Requested Slots For the i-th turn and slot r, pi,r is used to denote the sum of all the confidence scores assigned by the SLU to r is one of the requested slots. Then the belief of “r being one of the requested slots in the i-th turn” denoted by bi,r is defined as follows. • If i = 1, or system has at least one of the following actions: “canthelp”, “offer”, “reqmore”, “confirm-domain”, “expl-conf”, “bye”, “request”, bi,r = pi,r • Otherwise, bi,r = bi−1,r(1 − pi,r) + pi,r This rule is a combination of the idea of HWU tracker (Wang, 2013) and an observation from the labelled data that once</context>
<context position="13690" citStr="Henderson et al., 2013" startWordPosition="2518" endWordPosition="2522">5 ,(f25,1, f25,2, &apos; &apos; &apos; , f25,10) where f25,j , bin rank(s, i, v, j), and f26 , (f26,1, f26,2, &apos; &apos; &apos; ,f26,10) where f26,j , bin acc(s, i, v, j) are defined. • f27 ,(f27,1,f27,2, &apos; &apos; &apos; ,f27,10). Where f27,j , bin max(s, i, v,j) = 1 if max(s, i, v) will fall in the j-th bin if the range of confidence scores is divided into 10 bins, otherwise 0. • f28 , (f28,1,f28,2, &apos; &apos; &apos; ,f28,17). Where f28,j , user acttype(s, i, v, uj) = the sum of all the scores assigned by the SLU to the user act type being uj(1 &lt; j &lt; 17). There are a total of 17 different user act types described in the handbook of DSTC 2 (Henderson et al., 2013). • f29 , (f29,1, f29,2, &apos; &apos; &apos; ,f29,17). Where f29,j , machine acttype(s, i, v, mj) = the number of occurrences of act type mj(1 &lt; j &lt; 17) in machine act. There are a total of 17 different machine act types described in the handbook of DSTC 2 (Henderson et al., 2013). • f30 , canthelp(s, i, v) = 1 if the system cannot offer a venue with the constrain s = v, otherwise 0. • f31 , slot confirmed(s, i, v) = 1 if the system has confirmed s = v, otherwise 0. • f24 ,(f24,1, f24,2, &apos; &apos; &apos; ,f24,10). Where , bin rules v totrule(s,i,v,j) f24,j , i , , �) = Z 321 • f32 , slot requested(s, i, v) = 1 if the </context>
</contexts>
<marker>Henderson, Thomson, Williams, 2013</marker>
<rawString>Matthew Henderson, Blaise Thomson and Jason Williams. 2013. Dialog State Tracking Challenge 2 &amp; 3. Technical report, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Jason Williams</author>
</authors>
<title>The Second Dialog State Tracking Challenge.</title>
<date>2014</date>
<booktitle>In SIGDIAL.</booktitle>
<contexts>
<context position="1954" citStr="Henderson et al., 2014" startWordPosition="306" endWordPosition="309">ng, from rule-based to statistical models, from generative models to discriminative models (Wang and Lemon, 2013; Zilka et al., 2013; Henderson et al., 2013; Lee and Eskenazi, 2013). Recently, shared research tasks like the first Dialog State Tracking Challenge (DSTC 1) (Williams et al., 2013) have provided a common testbed and evaluation suite for dialog state tracking (Henderson et al., 2013). Compared with DSTC 1 which is in the bus timetables domain, DSTC 2 introduces more complicated and dynamic dialog states, which may change through the dialog, in a new domain, i.e. restaurants domain (Henderson et al., 2014). For each turn, a tracker is supposed to output a set of distributions for each of the three components of the dialog state: goals, method, and requested slots. At a given turn, the goals consists of the user’s true required value having been revealed for each slot in the dialog up until that turn; the method is the way the user is trying to interact with the system which may be by name, by constraints, by alternatives or finished; and the requested slots consist of the slots which have been requested by the user and not replied by the system. For evaluation in DSTC 2, 1-best quality measured</context>
</contexts>
<marker>Henderson, Thomson, Williams, 2014</marker>
<rawString>Matthew Henderson, Blaise Thomson and Jason Williams. 2014. The Second Dialog State Tracking Challenge. In SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjin Lee</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Recipe For Building Robust Spoken Dialog State Trackers: Dialog State Tracking Challenge System Description.</title>
<date>2013</date>
<booktitle>In SIGDIAL.</booktitle>
<contexts>
<context position="1512" citStr="Lee and Eskenazi, 2013" startWordPosition="235" endWordPosition="238">racking is important because spoken dialog systems rely on it to choose proper actions as spoken dialog systems interact with users. However, due to automatic speech recognition (ASR) and spoken language understanding (SLU) errors, it is not easy for the dialog manager to maintain the true state of the dialog. In recent years, much research has been devoted to dialog state tracking. Many approaches have been applied to dialog state tracking, from rule-based to statistical models, from generative models to discriminative models (Wang and Lemon, 2013; Zilka et al., 2013; Henderson et al., 2013; Lee and Eskenazi, 2013). Recently, shared research tasks like the first Dialog State Tracking Challenge (DSTC 1) (Williams et al., 2013) have provided a common testbed and evaluation suite for dialog state tracking (Henderson et al., 2013). Compared with DSTC 1 which is in the bus timetables domain, DSTC 2 introduces more complicated and dynamic dialog states, which may change through the dialog, in a new domain, i.e. restaurants domain (Henderson et al., 2014). For each turn, a tracker is supposed to output a set of distributions for each of the three components of the dialog state: goals, method, and requested slo</context>
<context position="2893" citStr="Lee and Eskenazi, 2013" startWordPosition="468" endWordPosition="471">he way the user is trying to interact with the system which may be by name, by constraints, by alternatives or finished; and the requested slots consist of the slots which have been requested by the user and not replied by the system. For evaluation in DSTC 2, 1-best quality measured by accuracy, probability calibration measured by L2, and discrimination measured by ROC are selected as featured metrics. Further details can be found in the DSTC 2 handbook (Henderson et al., 2013). Previous research has demonstrated the effectiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Lee and Eskenazi, 2013) and deep neural network (DNN) (Henderson et al., 2013) models separately. Motivated by this, the SJTU system employs a combination of a rulebased model, a MaxEnt and a DNN model. The three models were first trained (if necessary) on the training set and tested for each of the three components of the dialog state, i.e goals, method, and requested slots on the development set. Then, models with the best performance for each of the three components were selected to form a combined model. Finally, the combined model was retrained using both training set and development set. Additionally, as the l</context>
</contexts>
<marker>Lee, Eskenazi, 2013</marker>
<rawString>Sungjin Lee and Maxine Eskenazi. 2013. Recipe For Building Robust Spoken Dialog State Trackers: Dialog State Tracking Challenge System Description. In SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
<author>Oliver Lemon</author>
</authors>
<title>A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information.</title>
<date>2013</date>
<booktitle>In SIGDIAL.</booktitle>
<contexts>
<context position="1443" citStr="Wang and Lemon, 2013" startWordPosition="223" endWordPosition="226">d competitive performance in DSTC 2. 1 Introduction Dialog state tracking is important because spoken dialog systems rely on it to choose proper actions as spoken dialog systems interact with users. However, due to automatic speech recognition (ASR) and spoken language understanding (SLU) errors, it is not easy for the dialog manager to maintain the true state of the dialog. In recent years, much research has been devoted to dialog state tracking. Many approaches have been applied to dialog state tracking, from rule-based to statistical models, from generative models to discriminative models (Wang and Lemon, 2013; Zilka et al., 2013; Henderson et al., 2013; Lee and Eskenazi, 2013). Recently, shared research tasks like the first Dialog State Tracking Challenge (DSTC 1) (Williams et al., 2013) have provided a common testbed and evaluation suite for dialog state tracking (Henderson et al., 2013). Compared with DSTC 1 which is in the bus timetables domain, DSTC 2 introduces more complicated and dynamic dialog states, which may change through the dialog, in a new domain, i.e. restaurants domain (Henderson et al., 2014). For each turn, a tracker is supposed to output a set of distributions for each of the t</context>
</contexts>
<marker>Wang, Lemon, 2013</marker>
<rawString>Zhuoran Wang and Oliver Lemon. 2013. A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information. In SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuoran Wang</author>
</authors>
<date>2013</date>
<journal>HWU Baseline Belief Tracker for DSTC</journal>
<tech>Technical report,</tech>
<volume>2</volume>
<institution>Heriot-Watt University.</institution>
<contexts>
<context position="7554" citStr="Wang, 2013" startWordPosition="1253" endWordPosition="1254">(Chang and Lin, 2011). The SVM misclassification cost parameters are optimised individually for each SVM classifier by performing cross-validations on the training data. P(d|o) = ri P(sv|u) 3 Rule-based Model P (d|u) = P (d-typej|u) sv∈S ri (1 − P(sv|u)) (1) sv/∈S where u denotes an utterance and sv runs over all possible slot-value pairs. 2.2 Dialogue Context Features In addition to the n-gram feature used in the original STC parser, the dialogue context can be exIn this section, the rule-based model which is slightly different from the focus tracker (Henderson et al., 2013) and HWU tracker (Wang, 2013) is described. The idea of the rule-based model is to maintain beliefs based on basic probability operations and a few heuristic rules that can be observed on the training set. In the following the rule-based model for joint goals, method and requested slots are described in detail. 319 3.1 Joint Goals For slot s, the i-th turn and value v, psi,v (p−s,i,v) is used to denote the sum of all the confidence scores assigned by the SLU to the user informing or affirming (denying or negating) the value of slot s is v. The belief of “the value of slot s being v in the i-th turn” denoted by bs,i,v is d</context>
<context position="9646" citStr="Wang, 2013" startWordPosition="1653" endWordPosition="1654">a is also adopted by the focus tracker (Henderson et al., 2013). 3.3 Requested Slots For the i-th turn and slot r, pi,r is used to denote the sum of all the confidence scores assigned by the SLU to r is one of the requested slots. Then the belief of “r being one of the requested slots in the i-th turn” denoted by bi,r is defined as follows. • If i = 1, or system has at least one of the following actions: “canthelp”, “offer”, “reqmore”, “confirm-domain”, “expl-conf”, “bye”, “request”, bi,r = pi,r • Otherwise, bi,r = bi−1,r(1 − pi,r) + pi,r This rule is a combination of the idea of HWU tracker (Wang, 2013) and an observation from the labelled data that once system has some certain actions, the statistics of requested slots from the past turn should be reset. 4 Statistical Model In this section, two statistical models, one is the MaxEnt model, the other is the DNN model, are described. 4.1 Features The performance of statistical models is highly dependent on the feature functions. Joint Goals For slot s, the i-th turn and value v, the feature functions designed for joint goals are listed below. • f1 °= inform(s, i, v) = the sum of all the scores assigned by the SLU to the user informing the valu</context>
</contexts>
<marker>Wang, 2013</marker>
<rawString>Zhuoran Wang. 2013. HWU Baseline Belief Tracker for DSTC 2 &amp; 3. Technical report, Heriot-Watt University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
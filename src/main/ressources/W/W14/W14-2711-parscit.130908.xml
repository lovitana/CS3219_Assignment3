<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009836">
<title confidence="0.998026">
As Long as You Name My Name Right:
Social Circles and Social Sentiment in the Hollywood Hearings
</title>
<author confidence="0.96962">
Oren Tsur†§ Dan Calacci† David Lazer†❜
</author>
<email confidence="0.922207">
orentsur@seas.harvard.edu dcalacci@ccs.neu.edu d.lazer@neu.edu
</email>
<affiliation confidence="0.776679666666667">
†Lazer Laboratory, Northeastern University
§School of Engeneering and Applied Sciences, Harvard University
❜Harvard Kennedy School, Harvard University
</affiliation>
<sectionHeader confidence="0.969652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954">
The Hollywood Blacklist was based on
a series of interviews conducted by the
House Committee on Un-American Activ-
ities (HUAC), trying to identify members
of the communist party. We use various
NLP algorithms in order to automatically
analyze a large corpus of interview tran-
scripts and construct a network of the in-
dustry members and their “naming” rela-
tions. We further use algorithms for Senti-
ment Analysis in order to add a psycholog-
ical dimension to the edges in the network.
In particular, we test how different types
of connections are manifested by different
sentiment types and attitude of the inter-
viewees. Analysis of the language used in
the hearings can shed new light on the mo-
tivation and role of network members.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871425925926">
A growing body of computational research is
focused on how language is used and how it
shapes/is shaped by a community of speakers.
Computational works in the nexus of language
and the social arena deal with various topics such
as language accommodation (Danescu-Niculescu-
Mizil and Lee, 2011; Danescu-Niculescu-Mizil
et al., 2011), demographic language variation
(Eisenstein et al., 2010; O’Connor et al., 2010),
the factors that facilitate the spread of information
in Q&amp;A forums and social networks (Adamic et
al., 2008; Bian et al., 2009; Romero et al., 2011) or
the correlation between words and social actions
(Adali et al., 2012).
All of these works analyze the language and the
social dynamics in online communities, mainly
due to the increasing popularity of online social
networks and greater availability of such data.
However, large scale socio-linguistic analysis
should not be restricted to online communities and
can be applied in many social and political settings
beyond the online world. Two examples are the
study of power structures in arguments before the
U.S. Supreme Court (Danescu-Niculescu-Mizil et
al., 2012) and the evolution of specific words and
phrases over time as reflected in Google Books
(Goldberg and Orwant, 2013).
In this paper we propose using network science
and linguistic analysis in order to understand the
social dynamics in the entertainment industry dur-
ing one of its most controversial periods – the ‘red
scare’ and the witch hunt for Communists in Hol-
lywood during 1950’s.
Historical background The Hollywood hear-
ings (often confused with Senator McCarthy’s
hearings and allegations) were a series of inter-
views conducted by the House Committee on Un-
American Activities (HUAC) in the years 1947–
1956. The purpose of the committee was to
conduct “hearings regarding the communist in-
filtration of the motion picture industry” (from
the HUAC Annual Report). The committee sub-
poenaed witnesses such as Ayn Rand (writer),
Arthur Miller (writer), Walt Disney (producer), fu-
ture U.S. president Ronald Reagan (Screen Actors
Guild), Elia Kazan (writer, actor, director) and Al-
bert Maltz (Screen Writers Guild). Some of the
witnesses were ‘friendly’ while some others were
uncooperative1, refusing to “name names” or self
incriminate2. Those who were named and/or were
uncooperative were often jailed or effectively lost
their job.
Arguably, many friendly witnesses felt they
were complying with their patriotic duty. Many
</bodyText>
<footnote confidence="0.996599625">
1A note about terminology: by using the terms friendly
and uncooperative there is no implied moral judgment – these
are the terms used in the literature.
2It should be noted that being a member of the Communist
party was not illegal, however, some individuals avoided self
“incrimination” either in an effort to protect their job or as
an ideological declaration in favor of privacy protection as a
civil right protected by the constitution.
</footnote>
<page confidence="0.992949">
83
</page>
<bodyText confidence="0.9252818">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 83–87,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
others were threatened or simply manipulated to
name names, and some later admitted to coop-
erating for other reasons such as protecting their
work or out of personal vendettas and professional
jealousies. It is also suspected that some nam-
ing occurred due to increasing professional ten-
sion between some producers and the Screen Writ-
ers Guild or (Navasky, 2003).
</bodyText>
<listItem confidence="0.9880082">
Motivation In this work we analyze a collection
of HUAC hearings. We wish to answer the follow-
ing questions:
1. Do sentiment and other linguistic categories
correlate with naming relations?
2. Can we gain any insight on the social dynam-
ics between the people in the network?
3. Does linguistic and network analysis support
any of the social theories about dynamics at
Hollywood during that time?
</listItem>
<bodyText confidence="0.999918777777778">
In order to answer the questions above we build
a social graph of members of the entertainment in-
dustry based on the hearings and add sentiment la-
bels on the graph edges. Layering linguistic fea-
tures on a the social graph may provide us with
new insights related to the questions at hand. In
this short paper we describe the research frame-
work, the various challenges posed by the data and
present some initial promising results.
</bodyText>
<sectionHeader confidence="0.98092" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.986157454545455">
In this work we used two types of datasets: Hear-
ing Transcripts and Annual Reports. Snippets
from hearings can be found in Figures 1(a) and
1(b), Figure 1(c) shows a snippet from an annual
report. The transcripts data is based on 47 inter-
views conducted by the HUAC in the years 1951–
2. Each interview is either a long statement (1(a) )
or a sequence of questions by the committee mem-
bers and answers by a witness (1(b)). In total, our
hearings corpus consists of 2831 dialogue acts and
half a million words.
</bodyText>
<sectionHeader confidence="0.900835" genericHeader="method">
3 Named Entity Recognition and
Anaphora Resolution
</sectionHeader>
<bodyText confidence="0.996718">
The snippets in Figure 1 illustrates some of the
challenges in processing HUAC data. The first
challenge is introduced by the low quality of the
available documents. Due to the low quality of
</bodyText>
<listItem confidence="0.990652">
(a) A snippet from the testimony of Elia Kazan, (actor, writer and director, 3
times Academy Awards winner), 4.10.1952.
(b) A snippet from the testimony of Harold Ashe’s (journalist) testimony 9.17-
19.1951.
(c) A snippet from 1951 annual report.
</listItem>
<figureCaption confidence="0.9892205">
Figure 1: Snippets from HUAC hearings and an
annual report.
</figureCaption>
<bodyText confidence="0.999847142857143">
the documents the OCR output is noisy, contain-
ing misidentified characters, wrong alignment of
sentences and missing words. These problems in-
troduce complications in tasks like named entity
recognition and properly parsing sentences.
Beyond the low graphic quality of the docu-
ments, the hearings present the researcher with the
typical array of NLP challenges. For example, the
hearing excerpt in 1(b) contains four dialogue acts
that need to be separated and processed. The com-
mittee member (Mr. Tavenner) mentions the name
Stanley Lawrence, later referred to by the witness
(Mr. Ashe) as Mr. Lawrence and he thus corefer-
ence resolution is required before the graph con-
struction and the sentiment analysis phases.
As a preprocessing stage we performed named
entity recognition (NER), disambiguation and uni-
fication. For the NER task we used the Stanford
NER (Finkel et al., 2005) and for disambiguation
and unification we used a number of heuristics
based on edit distance and name distribution.
</bodyText>
<page confidence="0.991784">
84
</page>
<bodyText confidence="0.997668">
We used the Stanford Deterministic Corefer-
ence Resolution System (Lee et al., 2011) to re-
solve anaphoric references.
</bodyText>
<sectionHeader confidence="0.858445" genericHeader="method">
4 Naming Graph vs. Mentions Graph
</sectionHeader>
<bodyText confidence="0.999986257142857">
In building the network graph of the members of
the entertainment industry we distinguish between
mentioning and naming in our data. While many
names may be mentioned in a testimony (either by
a committee member or by the witness, see ex-
ample in Figures 1(a) and 1(b)), not all names are
practically ‘named’ (=identified) as Communists.
We thus use the hearings dataset in order to build
a social graph of mentions (MG) and the annual re-
ports are used to build a naming graph (NG). The
NG is used as a “gold standard” in the analysis
of the sentiment labels in the MG. Graph statistics
are presented in Table 1.
While the hearings are commonly perceived as
an “orgy of informing” (Navasky, 2003), the dif-
ference in network structure of the graphs portrays
a more complex picture. The striking difference in
the average out degree suggests that while many
names were mentioned in the testimonies (either
in a direct question or in an answer) – majority of
the witnesses avoided mass-explicit naming3. The
variance in outdegree suggests that most witnesses
did not cooperate at all or gave only a name or
two, while only a small number of witnesses gave
a long list of names. These results are visually
captured in the intersection graph (Figure 2) and
were also manually verified.
The difference between the MG and the NG
graph in the number of nodes with out-going edges
(214 vs. 66) suggests that the HUAC used other
informers that were not subpoenaed to testify in a
hearing4.
In the remainder of this paper we analyze the the
distribution of the usage of various psychological
categories based on the role the witnesses play.
</bodyText>
<sectionHeader confidence="0.997876" genericHeader="method">
5 Sentiment Analysis and Psychological
Categories
</sectionHeader>
<subsectionHeader confidence="0.999723">
5.1 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.9997745">
We performed the sentiment analysis in two dif-
ferent settings: lexical and statistical. In the lexi-
</bodyText>
<footnote confidence="0.9905062">
3Ayn Rand and Ronald Reagan, two of the most ‘friendly’
witnesses (appeared in front of the HUAC in 1947), did not
name anyone.
4There might be some hearings and testimonies that are
classified or still not publicly accessible.
</footnote>
<table confidence="0.999390818181818">
MG NG Intersection
Num of nodes 1353 631 122
Num of edges 2434 842 113
Nodes / Edges 0.55 0.467 1
Avg. out degree 36.87 3.93 8.7
Avg. in degree 1.82 1.83 1.04
Var(outdegree) 3902.62 120.75 415.59
Var(indegree) 4.0 2.51 1.04
Nodes with out going edges 66 214 13
Nodes with incoming edges 1341 459 109
Reciprocity 0.016 0.012 0
</table>
<tableCaption confidence="0.995851">
Table 1: Network features of the Mentions graph,
</tableCaption>
<figureCaption confidence="0.746442">
the Naming graph and the intersection of the
graphs.
Figure 2: Naming graph based on the intersec-
</figureCaption>
<bodyText confidence="0.96479305">
tion of the mentions and the naming data. Larger
node size indicates a bigger out degree; Color in-
dicates the in degree (darker nodes were named
more times).
cal setting we combine (Ding et al., 2008) and the
LIWC lexicon (Tausczik and Pennebaker, 2010).
In the statistical setting we use NaSent (Socher et
al., 2013).
The motivation to use both methods is twofold:
first – while statistical models are generally more
robust, accurate and sensitive to context, they re-
quire parsing of the processed sentences. Parsing
our data is often problematic due to the noise in-
troduced by the OCR algorithm due to the poor
quality of the documents (see Figure 1). We ex-
pected the lexicon-based method to be more toler-
ant to noisy or ill-structured sentences. We opted
for the LIWC since it offers an array of sentiment
and psychological categories that might be rele-
vant in the analysis of such data.
</bodyText>
<page confidence="0.999432">
85
</page>
<table confidence="0.993473666666667">
Stanford LIWC
Pos 75 292
Neg 254 37
</table>
<tableCaption confidence="0.933052">
Table 2: Confusion matrix for Stanford and LIWC
sentiment algorithms.
</tableCaption>
<bodyText confidence="0.9470672">
Aggregated Sentiment A name may be men-
tioned a number of times in a single hearing, each
time with a different sentiment type or polarity.
The aggregated sentiment weight of a witness i to-
ward a mentioned name j is computed as follows:
</bodyText>
<equation confidence="0.998565142857143">
Ek∈Uij score(ukij, c)
(1)
|Uij|
�
wc, if sentiment found
score(uk ij, c) = (3)
0, if c was not returned
</equation>
<bodyText confidence="0.999970727272727">
Unfortunately, both approaches to sentiment
analysis were not as useful as expected. Most
graph edges did not have any sentiment label, ei-
ther due to the limited sentiment lexicon of the
LIWC or due to the noise induced in the OCR
process, preventing the Stanford Sentiment engine
from parsing many of the sentences. Interestingly,
the two approaches did not agree on most sen-
tences (or dialogue acts). The sentiment confu-
sion matrix is presented in Table 2, illustrating the
challenge posed by the data.
</bodyText>
<subsectionHeader confidence="0.999325">
5.2 Psychological Categories
</subsectionHeader>
<bodyText confidence="0.999030538461538">
The LIWC lexicon contains more than just posi-
tive/negative categories. Table 3 presents a sample
of LIWC categories and associated tokens. Fig-
ure 3 presents the frequencysave in which each
psychological category is used by friendly and un-
cooperative witnesses. While the Pronoun cate-
gory is equally used by both parties, the uncooper-
ative witnesses tend to use the I, Self and You cate-
gories while the friendly witnesses tend to use the
Other and Social. A somewhat surprising result
is that the Tentat category is used more by friendly
witnesses – presumably reflecting their discomfort
with their position as informers.
</bodyText>
<figureCaption confidence="0.9915025">
Figure 3: Frequencies of selected LIWC cate-
gories in friendly vs. uncooperative testimonies.
</figureCaption>
<table confidence="0.999106214285714">
Category Typical Words
Cogmech abandon, accept, avoid, admit, know, question
Excl although, besides, but, except
I I, I’d, I’ll, I’m, I’ve, me, mine, my, myself
Insight accept, acknowledge, conclude, know, rational
job work, position, benefit, duty
Negate no, nope, nothing, neither, never, isn’t , can’t
Other he, him, herself, them
Preps about, against, along, from, outside, since
Pronouns I, anybody, anyone, something, they, you
Self I, mine, ours, myself, us
Social acquaintance, admit, party, comrade, confess, friend, human
Tentat ambiguous, tentative, undecided, depend, hesitant, guess
You thou, thoust, thy, y’all, ya, ye, you, you’d
</table>
<tableCaption confidence="0.9544565">
Table 3: LIWC categories and examples of typical
words
</tableCaption>
<sectionHeader confidence="0.963447" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999992421052632">
In this short paper we take a computational ap-
proach in analyzing a collection of HUAC hear-
ings. We combine Natural Language Process-
ing and Network Science techniques in order to
gain a better understanding of the social dynam-
ics within the entertainment industry in its dark-
est time. While sentiment analysis did not prove
as useful as expected, analysis of network struc-
tures and the language usage in an array of psycho-
logical dimensions reveals differences between
friendly and uncooperative witnesses.
Future work should include a better preprocess-
ing of the data, which is also expected to improve
the sentiment analysis. In future work we will an-
alyze the language use in a finer granularity of wit-
ness categories, such as the ideological informer,
the naive informer and the vindictive informer. We
also hope to expand the hearings corpora to in-
clude testimonies from more years.
</bodyText>
<sectionHeader confidence="0.992694" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7542535">
Sibel Adali, Fred Sisenda, and Malik Magdon-Ismail.
2012. Actions speak as loud as words: Predicting
</reference>
<equation confidence="0.629651">
sentiment(i, j) = max
c∈CAT
</equation>
<bodyText confidence="0.987911166666667">
Where CAT is the set of categories used by
LIWC or Stanford Sentiment and Uij is the set
of all utterances (dialogue acts) in which witness
i mentions the name j. The scoreO function is
defined slightly different for each setting. In the
LIWC setting we define score as:
</bodyText>
<equation confidence="0.971839">
k 1 E ukij|w E c}|
score(uij , c) =  ||(2)
ukij
</equation>
<bodyText confidence="0.994552666666667">
In the statistical setting, Stanford Sentiment re-
turns a sentiment category and a weight, we there-
fore use:
</bodyText>
<page confidence="0.994502">
86
</page>
<reference confidence="0.980126904761905">
relationships from social behavior data. In Proceed-
ings of the 21st international conference on World
Wide Web, pages 689–698. ACM.
Lada A Adamic, Jun Zhang, Eytan Bakshy, and Mark S
Ackerman. 2008. Knowledge sharing and yahoo
answers: everyone knows something. In Proceed-
ings of the 17th international conference on World
Wide Web, pages 665–674. ACM.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World Wide
Web, pages 51–60. ACM.
Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2011. Chameleons in imagined conversations: A
new approach to understanding coordination of lin-
guistic style in dialogs. In Proceedings of the Work-
shop on Cognitive Modeling and Computational
Linguistics, ACL 2011.
Cristian Danescu-Niculescu-Mizil, Michael Gamon,
and Susan Dumais. 2011. Mark my words! Lin-
guistic style accommodation in social media. In
Proceedings of WWW, pages 745–754.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: Language effects and power differences in
social interaction. In Proceedings of WWW, pages
699–708.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the 2008 International Conference
on Web Search and Data Mining, WSDM ’08, pages
231–240, New York, NY, USA. ACM.
Jacob Eisenstein, Brendan O’Connor, Noah A Smith,
and Eric P Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1277–1287. Asso-
ciation for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Yoav Goldberg and Jon Orwant. 2013. Syntactic-
ngrams over time from a very large corpus of english
books. In Second Joint Conference on Lexical and
Computational Semantics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34. Association for Computational
Linguistics.
Victor S Navasky. 2003. Naming Names: With a New
Afterword by the Author. Macmillan.
Brendan O’Connor, Jacob Eisenstein, Eric P Xing, and
Noah A Smith. 2010. A mixture model of demo-
graphic lexical variation. In Proceedings of NIPS
workshop on machine learning in computational so-
cial science, pages 1–7.
Daniel M Romero, Brendan Meeder, and Jon Klein-
berg. 2011. Differences in the mechanics of in-
formation diffusion across topics: idioms, politi-
cal hashtags, and complex contagion on twitter. In
Proceedings of the 20th international conference on
World wide web, pages 695–704. ACM.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Yla R. Tausczik and James W. Pennebaker. 2010. The
Psychological Meaning of Words: LIWC and Com-
puterized Text Analysis Methods. Journal of Lan-
guage and Social Psychology, 29(1):24–54, March.
</reference>
<page confidence="0.999474">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.451022">
<title confidence="0.986673">As Long as You Name My Name Right: Social Circles and Social Sentiment in the Hollywood Hearings</title>
<author confidence="0.999901">Dan David</author>
<email confidence="0.995787">orentsur@seas.harvard.edudcalacci@ccs.neu.edud.lazer@neu.edu</email>
<affiliation confidence="0.779097333333333">Laboratory, Northeastern University of Engeneering and Applied Sciences, Harvard Kennedy School, Harvard University</affiliation>
<abstract confidence="0.998394315789474">The Hollywood Blacklist was based on a series of interviews conducted by the House Committee on Un-American Activities (HUAC), trying to identify members of the communist party. We use various NLP algorithms in order to automatically analyze a large corpus of interview transcripts and construct a network of the industry members and their “naming” relations. We further use algorithms for Sentiment Analysis in order to add a psychological dimension to the edges in the network. In particular, we test how different types of connections are manifested by different sentiment types and attitude of the interviewees. Analysis of the language used in the hearings can shed new light on the motivation and role of network members.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sibel Adali</author>
<author>Fred Sisenda</author>
<author>Malik Magdon-Ismail</author>
</authors>
<title>Actions speak as loud as words: Predicting relationships from social behavior data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>689--698</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1737" citStr="Adali et al., 2012" startWordPosition="265" endWordPosition="268">f computational research is focused on how language is used and how it shapes/is shaped by a community of speakers. Computational works in the nexus of language and the social arena deal with various topics such as language accommodation (Danescu-NiculescuMizil and Lee, 2011; Danescu-Niculescu-Mizil et al., 2011), demographic language variation (Eisenstein et al., 2010; O’Connor et al., 2010), the factors that facilitate the spread of information in Q&amp;A forums and social networks (Adamic et al., 2008; Bian et al., 2009; Romero et al., 2011) or the correlation between words and social actions (Adali et al., 2012). All of these works analyze the language and the social dynamics in online communities, mainly due to the increasing popularity of online social networks and greater availability of such data. However, large scale socio-linguistic analysis should not be restricted to online communities and can be applied in many social and political settings beyond the online world. Two examples are the study of power structures in arguments before the U.S. Supreme Court (Danescu-Niculescu-Mizil et al., 2012) and the evolution of specific words and phrases over time as reflected in Google Books (Goldberg and </context>
</contexts>
<marker>Adali, Sisenda, Magdon-Ismail, 2012</marker>
<rawString>Sibel Adali, Fred Sisenda, and Malik Magdon-Ismail. 2012. Actions speak as loud as words: Predicting relationships from social behavior data. In Proceedings of the 21st international conference on World Wide Web, pages 689–698. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lada A Adamic</author>
<author>Jun Zhang</author>
<author>Eytan Bakshy</author>
<author>Mark S Ackerman</author>
</authors>
<title>Knowledge sharing and yahoo answers: everyone knows something.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web,</booktitle>
<pages>665--674</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1623" citStr="Adamic et al., 2008" startWordPosition="245" endWordPosition="248"> in the hearings can shed new light on the motivation and role of network members. 1 Introduction A growing body of computational research is focused on how language is used and how it shapes/is shaped by a community of speakers. Computational works in the nexus of language and the social arena deal with various topics such as language accommodation (Danescu-NiculescuMizil and Lee, 2011; Danescu-Niculescu-Mizil et al., 2011), demographic language variation (Eisenstein et al., 2010; O’Connor et al., 2010), the factors that facilitate the spread of information in Q&amp;A forums and social networks (Adamic et al., 2008; Bian et al., 2009; Romero et al., 2011) or the correlation between words and social actions (Adali et al., 2012). All of these works analyze the language and the social dynamics in online communities, mainly due to the increasing popularity of online social networks and greater availability of such data. However, large scale socio-linguistic analysis should not be restricted to online communities and can be applied in many social and political settings beyond the online world. Two examples are the study of power structures in arguments before the U.S. Supreme Court (Danescu-Niculescu-Mizil e</context>
</contexts>
<marker>Adamic, Zhang, Bakshy, Ackerman, 2008</marker>
<rawString>Lada A Adamic, Jun Zhang, Eytan Bakshy, and Mark S Ackerman. 2008. Knowledge sharing and yahoo answers: everyone knows something. In Proceedings of the 17th international conference on World Wide Web, pages 665–674. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Bian</author>
<author>Yandong Liu</author>
<author>Ding Zhou</author>
<author>Eugene Agichtein</author>
<author>Hongyuan Zha</author>
</authors>
<title>Learning to recognize reliable users and content in social media with coupled mutual reinforcement.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World Wide Web,</booktitle>
<pages>51--60</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1642" citStr="Bian et al., 2009" startWordPosition="249" endWordPosition="252">shed new light on the motivation and role of network members. 1 Introduction A growing body of computational research is focused on how language is used and how it shapes/is shaped by a community of speakers. Computational works in the nexus of language and the social arena deal with various topics such as language accommodation (Danescu-NiculescuMizil and Lee, 2011; Danescu-Niculescu-Mizil et al., 2011), demographic language variation (Eisenstein et al., 2010; O’Connor et al., 2010), the factors that facilitate the spread of information in Q&amp;A forums and social networks (Adamic et al., 2008; Bian et al., 2009; Romero et al., 2011) or the correlation between words and social actions (Adali et al., 2012). All of these works analyze the language and the social dynamics in online communities, mainly due to the increasing popularity of online social networks and greater availability of such data. However, large scale socio-linguistic analysis should not be restricted to online communities and can be applied in many social and political settings beyond the online world. Two examples are the study of power structures in arguments before the U.S. Supreme Court (Danescu-Niculescu-Mizil et al., 2012) and th</context>
</contexts>
<marker>Bian, Liu, Zhou, Agichtein, Zha, 2009</marker>
<rawString>Jiang Bian, Yandong Liu, Ding Zhou, Eugene Agichtein, and Hongyuan Zha. 2009. Learning to recognize reliable users and content in social media with coupled mutual reinforcement. In Proceedings of the 18th international conference on World Wide Web, pages 51–60. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Lillian Lee</author>
</authors>
<title>Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL</booktitle>
<marker>Danescu-Niculescu-Mizil, Lee, 2011</marker>
<rawString>Cristian Danescu-Niculescu-Mizil and Lillian Lee. 2011. Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Michael Gamon</author>
<author>Susan Dumais</author>
</authors>
<title>Mark my words! Linguistic style accommodation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>745--754</pages>
<contexts>
<context position="1432" citStr="Danescu-Niculescu-Mizil et al., 2011" startWordPosition="216" endWordPosition="219">ical dimension to the edges in the network. In particular, we test how different types of connections are manifested by different sentiment types and attitude of the interviewees. Analysis of the language used in the hearings can shed new light on the motivation and role of network members. 1 Introduction A growing body of computational research is focused on how language is used and how it shapes/is shaped by a community of speakers. Computational works in the nexus of language and the social arena deal with various topics such as language accommodation (Danescu-NiculescuMizil and Lee, 2011; Danescu-Niculescu-Mizil et al., 2011), demographic language variation (Eisenstein et al., 2010; O’Connor et al., 2010), the factors that facilitate the spread of information in Q&amp;A forums and social networks (Adamic et al., 2008; Bian et al., 2009; Romero et al., 2011) or the correlation between words and social actions (Adali et al., 2012). All of these works analyze the language and the social dynamics in online communities, mainly due to the increasing popularity of online social networks and greater availability of such data. However, large scale socio-linguistic analysis should not be restricted to online communities and can</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Gamon, Dumais, 2011</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Michael Gamon, and Susan Dumais. 2011. Mark my words! Linguistic style accommodation in social media. In Proceedings of WWW, pages 745–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Lillian Lee</author>
<author>Bo Pang</author>
<author>Jon Kleinberg</author>
</authors>
<title>Echoes of power: Language effects and power differences in social interaction.</title>
<date>2012</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>699--708</pages>
<contexts>
<context position="2235" citStr="Danescu-Niculescu-Mizil et al., 2012" startWordPosition="340" endWordPosition="343">orks (Adamic et al., 2008; Bian et al., 2009; Romero et al., 2011) or the correlation between words and social actions (Adali et al., 2012). All of these works analyze the language and the social dynamics in online communities, mainly due to the increasing popularity of online social networks and greater availability of such data. However, large scale socio-linguistic analysis should not be restricted to online communities and can be applied in many social and political settings beyond the online world. Two examples are the study of power structures in arguments before the U.S. Supreme Court (Danescu-Niculescu-Mizil et al., 2012) and the evolution of specific words and phrases over time as reflected in Google Books (Goldberg and Orwant, 2013). In this paper we propose using network science and linguistic analysis in order to understand the social dynamics in the entertainment industry during one of its most controversial periods – the ‘red scare’ and the witch hunt for Communists in Hollywood during 1950’s. Historical background The Hollywood hearings (often confused with Senator McCarthy’s hearings and allegations) were a series of interviews conducted by the House Committee on UnAmerican Activities (HUAC) in the yea</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Lee, Pang, Kleinberg, 2012</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang, and Jon Kleinberg. 2012. Echoes of power: Language effects and power differences in social interaction. In Proceedings of WWW, pages 699–708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08,</booktitle>
<pages>231--240</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10310" citStr="Ding et al., 2008" startWordPosition="1680" endWordPosition="1683">ges 2434 842 113 Nodes / Edges 0.55 0.467 1 Avg. out degree 36.87 3.93 8.7 Avg. in degree 1.82 1.83 1.04 Var(outdegree) 3902.62 120.75 415.59 Var(indegree) 4.0 2.51 1.04 Nodes with out going edges 66 214 13 Nodes with incoming edges 1341 459 109 Reciprocity 0.016 0.012 0 Table 1: Network features of the Mentions graph, the Naming graph and the intersection of the graphs. Figure 2: Naming graph based on the intersection of the mentions and the naming data. Larger node size indicates a bigger out degree; Color indicates the in degree (darker nodes were named more times). cal setting we combine (Ding et al., 2008) and the LIWC lexicon (Tausczik and Pennebaker, 2010). In the statistical setting we use NaSent (Socher et al., 2013). The motivation to use both methods is twofold: first – while statistical models are generally more robust, accurate and sensitive to context, they require parsing of the processed sentences. Parsing our data is often problematic due to the noise introduced by the OCR algorithm due to the poor quality of the documents (see Figure 1). We expected the lexicon-based method to be more tolerant to noisy or ill-structured sentences. We opted for the LIWC since it offers an array of s</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08, pages 231–240, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1277--1287</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A Smith, and Eric P Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1277–1287. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7338" citStr="Finkel et al., 2005" startWordPosition="1171" endWordPosition="1174">lity of the documents, the hearings present the researcher with the typical array of NLP challenges. For example, the hearing excerpt in 1(b) contains four dialogue acts that need to be separated and processed. The committee member (Mr. Tavenner) mentions the name Stanley Lawrence, later referred to by the witness (Mr. Ashe) as Mr. Lawrence and he thus coreference resolution is required before the graph construction and the sentiment analysis phases. As a preprocessing stage we performed named entity recognition (NER), disambiguation and unification. For the NER task we used the Stanford NER (Finkel et al., 2005) and for disambiguation and unification we used a number of heuristics based on edit distance and name distribution. 84 We used the Stanford Deterministic Coreference Resolution System (Lee et al., 2011) to resolve anaphoric references. 4 Naming Graph vs. Mentions Graph In building the network graph of the members of the entertainment industry we distinguish between mentioning and naming in our data. While many names may be mentioned in a testimony (either by a committee member or by the witness, see example in Figures 1(a) and 1(b)), not all names are practically ‘named’ (=identified) as Comm</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>Syntacticngrams over time from a very large corpus of english books.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="2350" citStr="Goldberg and Orwant, 2013" startWordPosition="359" endWordPosition="362">et al., 2012). All of these works analyze the language and the social dynamics in online communities, mainly due to the increasing popularity of online social networks and greater availability of such data. However, large scale socio-linguistic analysis should not be restricted to online communities and can be applied in many social and political settings beyond the online world. Two examples are the study of power structures in arguments before the U.S. Supreme Court (Danescu-Niculescu-Mizil et al., 2012) and the evolution of specific words and phrases over time as reflected in Google Books (Goldberg and Orwant, 2013). In this paper we propose using network science and linguistic analysis in order to understand the social dynamics in the entertainment industry during one of its most controversial periods – the ‘red scare’ and the witch hunt for Communists in Hollywood during 1950’s. Historical background The Hollywood hearings (often confused with Senator McCarthy’s hearings and allegations) were a series of interviews conducted by the House Committee on UnAmerican Activities (HUAC) in the years 1947– 1956. The purpose of the committee was to conduct “hearings regarding the communist infiltration of the mo</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant. 2013. Syntacticngrams over time from a very large corpus of english books. In Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7541" citStr="Lee et al., 2011" startWordPosition="1203" endWordPosition="1206">sed. The committee member (Mr. Tavenner) mentions the name Stanley Lawrence, later referred to by the witness (Mr. Ashe) as Mr. Lawrence and he thus coreference resolution is required before the graph construction and the sentiment analysis phases. As a preprocessing stage we performed named entity recognition (NER), disambiguation and unification. For the NER task we used the Stanford NER (Finkel et al., 2005) and for disambiguation and unification we used a number of heuristics based on edit distance and name distribution. 84 We used the Stanford Deterministic Coreference Resolution System (Lee et al., 2011) to resolve anaphoric references. 4 Naming Graph vs. Mentions Graph In building the network graph of the members of the entertainment industry we distinguish between mentioning and naming in our data. While many names may be mentioned in a testimony (either by a committee member or by the witness, see example in Figures 1(a) and 1(b)), not all names are practically ‘named’ (=identified) as Communists. We thus use the hearings dataset in order to build a social graph of mentions (MG) and the annual reports are used to build a naming graph (NG). The NG is used as a “gold standard” in the analysi</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor S Navasky</author>
</authors>
<title>Naming Names: With a New Afterword by the Author.</title>
<date>2003</date>
<publisher>Macmillan.</publisher>
<contexts>
<context position="4563" citStr="Navasky, 2003" startWordPosition="708" endWordPosition="709">ection as a civil right protected by the constitution. 83 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 83–87, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics others were threatened or simply manipulated to name names, and some later admitted to cooperating for other reasons such as protecting their work or out of personal vendettas and professional jealousies. It is also suspected that some naming occurred due to increasing professional tension between some producers and the Screen Writers Guild or (Navasky, 2003). Motivation In this work we analyze a collection of HUAC hearings. We wish to answer the following questions: 1. Do sentiment and other linguistic categories correlate with naming relations? 2. Can we gain any insight on the social dynamics between the people in the network? 3. Does linguistic and network analysis support any of the social theories about dynamics at Hollywood during that time? In order to answer the questions above we build a social graph of members of the entertainment industry based on the hearings and add sentiment labels on the graph edges. Layering linguistic features on</context>
<context position="8304" citStr="Navasky, 2003" startWordPosition="1339" endWordPosition="1340">tinguish between mentioning and naming in our data. While many names may be mentioned in a testimony (either by a committee member or by the witness, see example in Figures 1(a) and 1(b)), not all names are practically ‘named’ (=identified) as Communists. We thus use the hearings dataset in order to build a social graph of mentions (MG) and the annual reports are used to build a naming graph (NG). The NG is used as a “gold standard” in the analysis of the sentiment labels in the MG. Graph statistics are presented in Table 1. While the hearings are commonly perceived as an “orgy of informing” (Navasky, 2003), the difference in network structure of the graphs portrays a more complex picture. The striking difference in the average out degree suggests that while many names were mentioned in the testimonies (either in a direct question or in an answer) – majority of the witnesses avoided mass-explicit naming3. The variance in outdegree suggests that most witnesses did not cooperate at all or gave only a name or two, while only a small number of witnesses gave a long list of names. These results are visually captured in the intersection graph (Figure 2) and were also manually verified. The difference </context>
</contexts>
<marker>Navasky, 2003</marker>
<rawString>Victor S Navasky. 2003. Naming Names: With a New Afterword by the Author. Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Jacob Eisenstein</author>
<author>Eric P Xing</author>
<author>Noah A Smith</author>
</authors>
<title>A mixture model of demographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of NIPS workshop on machine learning in computational social science,</booktitle>
<pages>1--7</pages>
<marker>O’Connor, Eisenstein, Xing, Smith, 2010</marker>
<rawString>Brendan O’Connor, Jacob Eisenstein, Eric P Xing, and Noah A Smith. 2010. A mixture model of demographic lexical variation. In Proceedings of NIPS workshop on machine learning in computational social science, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Romero</author>
<author>Brendan Meeder</author>
<author>Jon Kleinberg</author>
</authors>
<title>Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>695--704</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1664" citStr="Romero et al., 2011" startWordPosition="253" endWordPosition="256">he motivation and role of network members. 1 Introduction A growing body of computational research is focused on how language is used and how it shapes/is shaped by a community of speakers. Computational works in the nexus of language and the social arena deal with various topics such as language accommodation (Danescu-NiculescuMizil and Lee, 2011; Danescu-Niculescu-Mizil et al., 2011), demographic language variation (Eisenstein et al., 2010; O’Connor et al., 2010), the factors that facilitate the spread of information in Q&amp;A forums and social networks (Adamic et al., 2008; Bian et al., 2009; Romero et al., 2011) or the correlation between words and social actions (Adali et al., 2012). All of these works analyze the language and the social dynamics in online communities, mainly due to the increasing popularity of online social networks and greater availability of such data. However, large scale socio-linguistic analysis should not be restricted to online communities and can be applied in many social and political settings beyond the online world. Two examples are the study of power structures in arguments before the U.S. Supreme Court (Danescu-Niculescu-Mizil et al., 2012) and the evolution of specifi</context>
</contexts>
<marker>Romero, Meeder, Kleinberg, 2011</marker>
<rawString>Daniel M Romero, Brendan Meeder, and Jon Kleinberg. 2011. Differences in the mechanics of information diffusion across topics: idioms, political hashtags, and complex contagion on twitter. In Proceedings of the 20th international conference on World wide web, pages 695–704. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA,</location>
<contexts>
<context position="10427" citStr="Socher et al., 2013" startWordPosition="1699" endWordPosition="1702">ree) 3902.62 120.75 415.59 Var(indegree) 4.0 2.51 1.04 Nodes with out going edges 66 214 13 Nodes with incoming edges 1341 459 109 Reciprocity 0.016 0.012 0 Table 1: Network features of the Mentions graph, the Naming graph and the intersection of the graphs. Figure 2: Naming graph based on the intersection of the mentions and the naming data. Larger node size indicates a bigger out degree; Color indicates the in degree (darker nodes were named more times). cal setting we combine (Ding et al., 2008) and the LIWC lexicon (Tausczik and Pennebaker, 2010). In the statistical setting we use NaSent (Socher et al., 2013). The motivation to use both methods is twofold: first – while statistical models are generally more robust, accurate and sensitive to context, they require parsing of the processed sentences. Parsing our data is often problematic due to the noise introduced by the OCR algorithm due to the poor quality of the documents (see Figure 1). We expected the lexicon-based method to be more tolerant to noisy or ill-structured sentences. We opted for the LIWC since it offers an array of sentiment and psychological categories that might be relevant in the analysis of such data. 85 Stanford LIWC Pos 75 29</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Stroudsburg, PA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods.</title>
<date>2010</date>
<journal>Journal of Language and Social Psychology,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10363" citStr="Tausczik and Pennebaker, 2010" startWordPosition="1688" endWordPosition="1691"> 1 Avg. out degree 36.87 3.93 8.7 Avg. in degree 1.82 1.83 1.04 Var(outdegree) 3902.62 120.75 415.59 Var(indegree) 4.0 2.51 1.04 Nodes with out going edges 66 214 13 Nodes with incoming edges 1341 459 109 Reciprocity 0.016 0.012 0 Table 1: Network features of the Mentions graph, the Naming graph and the intersection of the graphs. Figure 2: Naming graph based on the intersection of the mentions and the naming data. Larger node size indicates a bigger out degree; Color indicates the in degree (darker nodes were named more times). cal setting we combine (Ding et al., 2008) and the LIWC lexicon (Tausczik and Pennebaker, 2010). In the statistical setting we use NaSent (Socher et al., 2013). The motivation to use both methods is twofold: first – while statistical models are generally more robust, accurate and sensitive to context, they require parsing of the processed sentences. Parsing our data is often problematic due to the noise introduced by the OCR algorithm due to the poor quality of the documents (see Figure 1). We expected the lexicon-based method to be more tolerant to noisy or ill-structured sentences. We opted for the LIWC since it offers an array of sentiment and psychological categories that might be r</context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R. Tausczik and James W. Pennebaker. 2010. The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of Language and Social Psychology, 29(1):24–54, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
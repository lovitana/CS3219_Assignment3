<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000291">
<title confidence="0.949945">
Generating Music from Literature
</title>
<author confidence="0.659217">
Hannah Davis Saif M. Mohammad
</author>
<affiliation confidence="0.560572">
New York University National Research Council Canada
</affiliation>
<email confidence="0.995363">
hannah.davis@nyu.edu saif.mohammad@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.993789" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999110454545454">
We present a system, TransProse, that au-
tomatically generates musical pieces from
text. TransProse uses known relations be-
tween elements of music such as tempo
and scale, and the emotions they evoke.
Further, it uses a novel mechanism to de-
termine sequences of notes that capture the
emotional activity in text. The work has
applications in information visualization,
in creating audio-visual e-books, and in
developing music apps.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999353055555556">
Music and literature have an intertwined past. It
is believed that they originated together (Brown,
1970), but in time, the two have developed into
separate art forms that continue to influence each
other.1 Music, just as prose, drama, and poetry, is
often used to tell stories.2 Opera and ballet tell sto-
ries through music and words, but even instrumen-
tal music, which is devoid of words, can have a
powerful narrative form (Hatten, 1991). Mahler’s
and Beethoven’s symphonies, for example, are re-
garded as particularly good examples of narrative
and evocative music (Micznik, 2001).
In this paper, for the first time, we present a
method to automatically generate music from liter-
ature. Specifically, we focus on novels and gener-
ate music that captures the change in the distribu-
tion of emotion words. We list below some of the
benefits in pursuing this general line of research:
</bodyText>
<listItem confidence="0.53328975">
• Creating audio-visual e-books that generate
music when certain pages are opened—music
that accentuates the mood conveyed by the
text in those pages.
</listItem>
<footnote confidence="0.998532">
1The term music comes from muses—the nine Greek god-
desses of inspiration for literature, science, and arts.
2Music is especially close to poetry as songs often tend to
be poems set to music.
</footnote>
<listItem confidence="0.9692445625">
• Mapping pieces of literature to musical
pieces according to compatibility of the flow
of emotions in text with the audio character-
istics of the musical piece.
• Finding songs that capture the emotions in
different parts of a novel. This could be use-
ful, for example, to allow an app to find and
play songs that are compatible with the mood
of the chapter being read.
• Generating music for movie scripts.
• Appropriate music can add to good visualiza-
tions to communicate information effectively,
quickly, and artfully.
Example 1: A tweet stream that is accom-
panied by music that captures the aggregated
sentiment towards an entity.
</listItem>
<bodyText confidence="0.962580166666667">
Example 2: Displaying the world map where
clicking on a particular region plays music
that captures the emotions of the tweets ema-
nating from there.
Given a novel (in an electronically readable
form), our system, which we call TransProse, gen-
erates simple piano pieces whose notes are depen-
dent on the emotion words in the text. The chal-
lenge in composing new music, just as in creat-
ing a new story, is the infinite number of choices
and possibilities. We present a number of map-
ping rules to determine various elements of music,
such as tempo, major/minor key, etc. according to
the emotion word density in the text. We intro-
duce a novel method to determine the sequence
of notes (sequences of pitch and duration pairs)
to be played as per the change in emotion word
density in the text. We also list some guidelines
we followed to make the sequence of notes sound
like music as opposed to a cacophonous cascade
of sounds.
Certainly, there is no one right way of capturing
the emotions in text through music, and there is no
one right way to produce good music. Generating
</bodyText>
<page confidence="0.821083">
1
</page>
<note confidence="0.9908515">
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 1–10,
Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999064857142857">
compelling music is an art, and TransProse can be
improved in a number of ways (we list several ad-
vancements in the Future Work section). Our goal
with this project is to present initial ideas in an
area that has not been explored before.
This paper does not assume any prior knowl-
edge of music theory. Section 2 presents all the
terminology and ideas from music theory needed
to understand this paper. We present related work
in Section 3. Sections 4, 5, 6, and 7 describe our
system. In Sections 8 and 9, we present an analysis
of the music generated by our system for various
popular novels. Finally in Section 10 we present
limitations and future work.
</bodyText>
<sectionHeader confidence="0.995599" genericHeader="introduction">
2 Music
</sectionHeader>
<bodyText confidence="0.999961935483871">
In physical terms, music is a series of possibly
overlapping sounds, often intended to be pleasing
to the listener. Sound is what our ears perceive
when there is a mechanical oscillation of pressure
in some medium such as air or water. Thus, differ-
ent sounds are associated with different frequen-
cies. In music, a particular frequency is referred
to as pitch. A note has two aspects: pitch and
relative duration.3 Examples of relative duration
are whole-note, half-note, quarter-note, etc. Each
successive element in this list is of half the dura-
tion as the preceding element. Consider the exam-
ple notes: 400Hz–quarter-note and 760Hz–whole-
note. The first note is the sound corresponding
to 400Hz, whereas the second note is the sound
corresponding to 760Hz. Also, the first note is to
be played for one fourth the duration the second
note is played. It is worth repeating at this point
that note and whole-note do not refer to the same
concept—the former is a combination of pitch and
relative duration, whereas whole-note (and oth-
ers such as quarter-note and half-note) are used to
specify the relative duration of a note. Notes are
defined in terms of relative duration to allow for
the same melody to be played quickly or slowly.
A series of notes can be grouped into a mea-
sure (also called bar). Melody (also called tune) is
a sequence of measures (and therefore a sequence
of notes) that creates the musical piece itself. For
example, a melody could be defined as 620Hz-
half-note, 1200Hz-whole-note, 840Hz-half-note,
</bodyText>
<footnote confidence="0.5774442">
3Confusingly, note is also commonly used to refer to pitch
alone. To avoid misunderstanding, we will not use note in that
sense in this paper. However, some statements, such as play
that pitch may seem odd to those familiar with music, who
may be more used to play that note.
</footnote>
<bodyText confidence="0.999714901960784">
660Hz–quarter-note, and so on. There can be one
melody (for example, in the song Mary Had A
Little Lamb) or multiple melodies; they can last
throughout the piece or appear in specific sections.
A challenge for TransProse is to generate appro-
priate sequences of notes, given the infinite possi-
bilities of pitch, duration, and order of the notes.
Tempo is the speed at which the piece should
be played. It is usually indicated by the num-
ber of beats per minute. A beat is a basic unit
of time. A quarter-note is often used as one
beat. In which case, the tempo can be under-
stood simply as the number of quarter-notes per
minute. Consider an example. Let’s assume it
is decided that the example melody specified in
the earlier paragraph is to be played at a tempo of
120 quarter-notes per minute. The total number of
quarter-notes in the initial sequence (620Hz–half-
note, 1200Hz–whole-note, 840Hz–half-note, and
660Hz–quarter-note) is 2 + 4 + 2 + 1 = 9. Thus the
initial sequence must be played in 9/120 minutes,
or 4.5 seconds.
The time signature of a piece indicates two
things: a) how many beats are in a measure, and
b) which note duration represents one beat. It is
written as one number stacked on another num-
ber. The upper number is the number of beats per
measure, and the lower number is the note dura-
tion that represents one beat. For example, a time
signature of s would mean there are six beats per
measure, and an eighth note represents one beat.
One of the most common time signatures is 4, and
it is referred to as common time.
Sounds associated with frequencies that are
multiples or factors of one another (for exam-
ple, 440Hz, 880Hz, 1760Hz, etc) are perceived
by the human ear as being consonant and pleas-
ing. This is because the pressure waves associ-
ated with these sounds have overlapping peaks and
troughs. Sets of such frequencies or pitches form
pitch classes. The intervals between successive
pitches in a pitch class are called octaves. On a
modern 88-key piano, the keys are laid out in in-
creasing order of pitch. Every successive 12 keys
pertain to an octave. (Thus there are keys pertain-
ing to 7 octaves and four additional keys pertain-
ing to the eighth octave.) Further, each of the 12
keys split the octave such that the difference in fre-
quency between successive keys in an octave is the
same. Thus the corresponding keys in each octave
form a pitch class. For example, the keys at posi-
</bodyText>
<page confidence="0.988275">
2
</page>
<bodyText confidence="0.999961782608695">
tion 1, 13, 25, 37, and so on, form a pitch class.
Similarly keys at position 2, 14, 26, 38, and so on,
form another pitch class. The pitch classes on a
piano are given names C, C#, D, D#, E, F, F#, G,
G#, A, A#, B. (The # is pronounced sharp). The
same names can also be used to refer to a partic-
ular key in an octave. (In an octave, there exists
only one C, only one D#, and so on.) The octaves
are often referred to by a number. On a standard
piano, the octaves in increasing order are 0, 1, 2,
and so on. C2 refers to the key in octave 2 that is
in the pitch class C.4
The difference in frequency between successive
piano keys is called a semitone or Half-Tone (Half
for short). The interval between two keys sep-
arated by exactly one key is called Whole-Tone
(Whole for short). Thus, the interval between
C and C# is half, whereas the interval between
C and D is whole. A scale is any sequence of
pitches ordered by frequency. A major scale is
a sequence of pitches obtained by applying the
ascending pattern: Whole–Whole–Half–Whole-
Whole–Whole–Half. For example, if one starts
with C, then the corresponding C major scale con-
sists of C, D (frequency of C + Whole interval), E
(frequency of D + Whole interval), F (frequency
of E + Half interval), G, A, B, C. Major scales can
begin with any pitch (not just C), and that pitch
is called the base pitch. A major key is the set of
pitches corresponding to the major scale. Playing
in the key of C major means that one is primarily
playing the keys (pitches) from the corresponding
scale, C major scale (although not necessarily in a
particular order).
Minor scales are series of pitches obtained
by applying the ascending pattern: Whole-Half-
Whole–Whole–Half–Whole–Whole. Thus, C mi-
nor is C, D, D#, F, G, G#, A#, C. A minor key
is the set of pitches corresponding to the minor
scale. Playing in major keys generally creates
lighter sounding pieces, whereas playing in minor
keys creates darker sounding pieces.
Consonance is how pleasant or stable one per-
ceives two pitches played simultaneously (or one
after the other). There are many theories on what
makes two pitches consonant, some of which are
</bodyText>
<footnote confidence="0.993340833333333">
4The frequencies of piano keys at a given position across
octaves is in log scale. For example, frequencies of C1,
C2,..., and so on are in log scale. The perception of sound
(frequency) in the human ear is also roughly logarithmic.
Also, the frequency 440Hz (mentioned above) is A4 and it
is the customary tuning standard for musical pitch.
</footnote>
<bodyText confidence="0.999943833333334">
culturally dependent. The most common notion
(attributed to Pythagoras) is that the simpler the
ratio between the two frequencies, the more con-
sonant they are (Roederer, 2008; Tenney, 1988).
Given a particular scale, some have argued that
the order of the pitches in decreasing consonance
is as follows: 1st, 5th, 3rd, 6th, 2nd, 4th, and 7th
(Perricone, 2000). Thus for the C major—C (the
base pitch, or 1st), D (2nd) , E (3rd), F (4th), G
(5th) , A (6th) , B (7th)—the order of the pitches
in decreasing consonance is—C, G, E, A, D, F, B.
Similarly, for C minor—C (the base pitch, or 1st),
D (2nd), D# (3rd), F (4th), G (5th), G# (6th), A#
(7th)—the order of pitches in decreasing conso-
nance is—C, G, D#, G#, D, F, A#. We will use
these orders in TransProse to generate more dis-
cordant and unstable pitches to reflect higher emo-
tion word densities in the novels.
</bodyText>
<sectionHeader confidence="0.999937" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999624129032258">
This work is related to automatic sentiment and
emotion analysis of text (computational linguis-
tics), the generation of music (music theory), as
well as the perception of music (psychology).
Sentiment analysis techniques aim to determine
the evaluative nature of text—positive, negative,
or neutral. They have been applied to many dif-
ferent kinds of texts including customer reviews
(Pang and Lee, 2008), newspaper headlines (Bel-
legarda, 2010), emails (Liu et al., 2003; Moham-
mad and Yang, 2011), blogs (Genereux and Evans,
2006; Mihalcea and Liu, 2006), and tweets (Pak
and Paroubek, 2010; Agarwal et al., 2011; Thel-
wall et al., 2011; Brody and Diakopoulos, 2011;
Aisopos et al., 2012; Bakliwal et al., 2012). Sur-
veys by Pang and Lee (2008) and Liu and Zhang
(2012) give a summary of many of these ap-
proaches. Emotion analysis and affective comput-
ing involve the detection of emotions such as joy,
anger, sadness, and anticipation in text. A num-
ber of approaches for emotion analysis have been
proposed in recent years (Boucouvalas, 2002; Zhe
and Boucouvalas, 2002; Aman and Szpakowicz,
2007; Neviarouskaya et al., 2009; Kim et al.,
2009; Bollen et al., 2009; Tumasjan et al., 2010).
Text-to-speech synthesis employs emotion detec-
tion to produce speech consistent with the emo-
tions in the text (Iida et al., 2000; Pierre-Yves,
2003; Schr¨oder, 2009). See surveys by Picard
(2000) and Tao and Tan (2005) for a broader re-
view of the research in this area.
</bodyText>
<page confidence="0.994954">
3
</page>
<bodyText confidence="0.999943454545455">
Some prior empirical sentiment analysis work
focuses specifically on literary texts. Alm and
Sproat (2005) analyzed twenty two Brothers
Grimm fairy tales to show that fairy tales often
began with a neutral sentence and ended with a
happy sentence. Mohammad (2012) visualized the
emotion word densities in novels and fairy tales.
Volkova et al. (2010) study human annotation of
emotions in fairy tales. However, there exists no
work connecting automatic detection of sentiment
with the automatic generation of music.
Methods for both sentiment and emotion analy-
sis often rely on lexicons of words associated with
various affect categories such as positive and neg-
ative sentiment, and emotions such as joy, sad-
ness, fear, and anger. The WordNet Affect Lexi-
con (WAL) (Strapparava and Valitutti, 2004) has a
few hundred words annotated with associations to
a number of affect categories including the six Ek-
man emotions (joy, sadness, anger, fear, disgust,
and surprise).5 The NRC Emotion Lexicon, com-
piled by Mohammad and Turney (2010; 2013),
has annotations for about 14000 words with eight
emotions (six of Ekman, trust, and anticipation).6
We use this lexicon in our project.
Automatic or semi-automatic generation of mu-
sic through computer algorithms was first popular-
ized by Brian Eno (who coined the term generative
music) and David Cope (Cope, 1996). Lerdahl and
Jackendoff (1983) authored a seminal book on the
generative theory of music. Their work greatly in-
fluenced future work in automatic generation of
music such as that of Collins (2008) and Biles
(1994). However, these pieces did not attempt to
explicitly capture emotions.
Dowling and Harwood (1986) showed that vast
amounts of information are processed when listen-
ing to music, and that the most expressive quality
that one perceives is emotion. The communication
of emotions in non-verbal utterances and in music
show how emotions in music have an evolutionary
basis (Rousseau, 2009; Spencer, 1857; Juslin and
Laukka, 2003). There are many known associa-
tions between music and emotions:
</bodyText>
<listItem confidence="0.98966625">
• Loudness: Loud music is associated with in-
tensity, power, and anger, whereas soft music
is associated with sadness or fear (Gabriels-
son and Lindstr¨om, 2001).
</listItem>
<footnote confidence="0.994127">
5http://wndomains.fbk.eu/wnaffect.html
6http://www.purl.org/net/
NRCemotionlexicon
</footnote>
<listItem confidence="0.999868666666667">
• Melody: A sequence of consonant notes is
associated with joy and calm, whereas a se-
quence of disconsonant notes is associated
with excitement, anger, or unpleasantness
(Gabrielsson and Lindstr¨om, 2001).
• Major and Minor Keys: Major keys are as-
sociated with happiness, whereas minor keys
are associated with sadness (Hunter et al.,
2010; Hunter et al., 2008; Ali and Peynirci-
olu, 2010; Gabrielsson and Lindstr¨om, 2001;
Webster and Weir, 2005).
• Tempo: Fast tempo is associated with hap-
</listItem>
<bodyText confidence="0.793850444444444">
piness or excitement (Hunter et al., 2010;
Hunter et al., 2008; Ali and Peynirciolu,
2010; Gabrielsson and Lindstr¨om, 2001;
Webster and Weir, 2005).
Studies have shown that even though many of
the associations mentioned above are largely uni-
versal, one’s own culture also influences the per-
ception of music (Morrison and Demorest, 2009;
Balkwill and Thompson, 1999).
</bodyText>
<sectionHeader confidence="0.987803" genericHeader="method">
4 Our System: TransProse
</sectionHeader>
<bodyText confidence="0.999984285714286">
Our system, which we call TransProse, generates
music according to the use of emotion words in
a given novel. It does so in three steps: First, it
analyzes the input text and generates an emotion
profile. The emotion profile is simply a collection
of various statistics about the presence of emotion
words in the text. Second, based on the emotion
profile of the text, the system generates values for
tempo, scale, octave, notes, and the sequence of
notes for multiple melodies. Finally, these values
are provided to JFugue, an open-source Java API
for programming music, that generates the appro-
priate audio file. In the sections ahead, we de-
scribe the three steps in more detail.
</bodyText>
<sectionHeader confidence="0.953217" genericHeader="method">
5 Calculating Emotion Word Densities
</sectionHeader>
<bodyText confidence="0.999961181818182">
Given a novel in electronic form, we use the
NRC Emotion Lexicon (Mohammad and Turney,
2010; Mohammad and Turney, 2013) to identify
the number of words in each chapter that are asso-
ciated with an affect category. We generate counts
for eight emotions (anticipation, anger, joy, fear,
disgust, sadness, surprise, and trust) as well as
for positive and negative sentiment. We partition
the novel into four sections representing the begin-
ning, early middle, late middle, and end. Each sec-
tion is further partitioned into four sub-sections.
</bodyText>
<page confidence="0.98795">
4
</page>
<bodyText confidence="0.999792125">
The number of sections, the number of subsec-
tions per section, and the number of notes gener-
ated for each of the subsections together determine
the total number of notes generated for the novel.
Even though we set the number of sections and
number of sub-sections to four each, these settings
can be varied, especially for significantly longer or
shorter pieces of text.
For each section and for each sub-section the ra-
tio of emotion words to the total number of words
is calculated. We will refer to this ratio as the over-
all emotions density. We also calculate densities
of particular emotions, for example, the joy den-
sity, anger density, etc. As described in the section
ahead, the emotion densities are used to generate
sequences of notes for each of the subsections.
</bodyText>
<sectionHeader confidence="0.997861" genericHeader="method">
6 Generating Music Specifications
</sectionHeader>
<bodyText confidence="0.9999658">
Each of the pieces presented in this paper are
for the piano with three simultaneous, but differ-
ent, melodies coming together to form the musical
piece. Two melodies sounded too thin (simple),
and four or more melodies sounded less cohesive.
</bodyText>
<subsectionHeader confidence="0.995669">
6.1 Major and Minor Keys
</subsectionHeader>
<bodyText confidence="0.99988303030303">
Major keys generally create a more positive atmo-
sphere in musical pieces, whereas minor keys tend
to produce pieces with more negative undertones
(Hunter et al., 2010; Hunter et al., 2008; Ali and
Peynirciolu, 2010; Gabrielsson and Lindstr¨om,
2001; Webster and Weir, 2005). No consensus
has been reached on whether particular keys them-
selves (for example, A minor vs E minor) evoke
different emotions, and if so, what emotions are
evoked by which keys. For this reason, the pro-
totype of Transprose does not consider different
keys; the chosen key for the produced musical
pieces is limited to either C major or C minor. (C
major was chosen because it is a popular choice
when teaching people music. It is simple because
it does not have any sharps. C minor was chosen
as it is the minor counterpart of C major.)
Whether the piece is major or minor is de-
termined by the ratio of the number of positive
words to the number of negative words in the en-
tire novel. If the ratio is higher than 1, C major
is used, that is, only pitches pertaining to C major
are played. If the ratio is 1 or lower, C minor is
used.
Experimenting with keys other than C major
and C minor is of interest for future work. Further-
more, the eventual intent is to include mid-piece
key changes for added effect. For example, chang-
ing the key from C major to A minor when the plot
suddenly turns sad. The process of changing key
is called modulation. Certain transitions such as
moving from C major to A minor are commonly
used and musically interesting.
</bodyText>
<subsectionHeader confidence="0.992013">
6.2 Melodies
</subsectionHeader>
<bodyText confidence="0.999982782608696">
We use three melodies to capture the change in
emotion word usage in the text. The notes in
one melody are based on the overall emotion word
density (the emotion words associated with any of
the eight emotions in the NRC Emotion Lexicon).
We will refer to this melody, which is intended to
capture the overarching emotional movement, as
melody o or Mo (the ‘o’ stands for overall emo-
tion). The notes in the two other melodies, melody
e1 (Me1) and melody e2 (Me2), are determined
by the most prevalent and second most prevalent
emotions in the text, respectively. Precisely how
the notes are determined is described in the next
sub-section, but first we describe how the octaves
of the notes is determined.
The octave of melody o is proportional to the
difference between the joy and sadness densities
of the novel. We will refer to this difference by JS.
We calculated the lowest density difference (JSmin)
and highest JS score (JSmax) in a collection of nov-
els. For a novel with density difference, JS, the
score is linearly mapped to octave 4, 5, or 6 of a
standard piano, as per the formula shown below:
</bodyText>
<equation confidence="0.8334865">
Oct(Mo) = 4 + r((JS − JSmin) * (6 − 4)) (1)
JSmax − JSmin
</equation>
<bodyText confidence="0.999838833333333">
The function r rounds the expression to the closest
integer. Thus scores closer to JSmin are mapped
to octave 4, scores closer to JSmax are mapped to
octave 6, and those in the middle are mapped to
octave 5.
The octave of Me1 is calculated as follows:
</bodyText>
<equation confidence="0.726694666666667">
{ Oct(Mo) + 1, if e1 is joy or trust
Oct(Mo) − 1, if e1 is anger, fear,
sadness, or disgust
Oct(Mo), otherwise
(2)
That is, Me1 is set to:
</equation>
<listItem confidence="0.99676">
• an octave higher than the octave of Mo if el
is a positive emotion,
</listItem>
<equation confidence="0.930445">
Oct(Me1) =
</equation>
<page confidence="0.849075">
5
</page>
<listItem confidence="0.99793975">
• an octave lower than the octave of Mo if el is
a negative emotion,
• the same octave as that of Mo if el is surprise
or anticipation.
</listItem>
<bodyText confidence="0.998630166666667">
Recall that higher octaves evoke a sense of positiv-
ity, whereas lower octaves evoke a sense of nega-
tivity. The octave of Me2 is calculated exactly as
that of Me1, except that it is based on the second
most prevalent emotion (and not the most preva-
lent emotion) in the text.
</bodyText>
<subsectionHeader confidence="0.999613">
6.3 Structure and Notes
</subsectionHeader>
<bodyText confidence="0.999981177419355">
As mentioned earlier, TransProse generates three
melodies that together make the musical piece for
a novel. The method for generating each melody
is the same, with the exception that the three
melodies (Mo, Me1, and Me2) are based on the
overall emotion density, predominant emotion’s
density, and second most dominant emotion’s den-
sity, respectively. We describe below the method
common for each melody, and use emotion word
density as a stand in for the appropriate density.
Each melody is made up of four sections, repre-
senting four sections of the novel (the beginning,
early middle, late middle, and end).In turn, each
section is represented by four measures. Thus each
measure corresponds to a quarter of a section (a
sub-section). A measure, as defined earlier, is a
series of notes. The number of notes, the pitch of
each note, and the relative duration of each note
are determined such that they reflect the emotion
word densities in the corresponding part of the
novel.
Number of Notes: In our implementation, we
decided to contain the possible note durations
to whole notes, half notes, quarter notes, eighth
notes, and sixteenth notes. A relatively high emo-
tion density is represented by many notes, whereas
a relatively low emotion density is represented by
fewer notes. We first split the interval between the
maximum and minimum emotion density for the
novel into five equal parts (five being the num-
ber of note duration choices – whole, half, quar-
ter, eighth, or sixteenth). Emotion densities that
fall in the lowest interval are mapped to a single
whole note. Emotion densities in the next interval
are mapped to two half-notes. The next interval
is mapped to four quarter-notes. And so on, until
the densities in the last interval are mapped to six-
teen sixteenth-notes (1/16th). The result is shorter
notes during periods of higher emotional activity
(with shorter notes making the piece sound more
active), and longer notes during periods of lower
emotional activity.
Pitch: If the number of notes for a measure
is n, then the corresponding sub-section is parti-
tioned into n equal parts and the pitch for each
note is based on the emotion density of the cor-
responding sub-section. Lower emotion densities
are mapped to more consonant pitches in the key
(C major or C minor), whereas higher emotion
densities are mapped to less consonant pitches in
the same scale. For example, if the melody is in
the key of C major, then the lowest to highest emo-
tion densities are mapped linearly to the pitches C,
G, E, A, D, F, B. Thus, a low emotion value would
create a pitch that is more consonant and a high
emotion value would create a pitch that is more
dissonant (more interesting and unusual).
Repetition: Once the four measures of a section
are played, the same four measures are repeated
in order to create a more structured and melodic
feeling. Without the repetition, the piece sounds
less cohesive.
</bodyText>
<subsectionHeader confidence="0.965206">
6.4 Tempo
</subsectionHeader>
<bodyText confidence="0.999964">
We use a 4 time signature (common time) be-
cause it is one of the most popular time signatures.
Thus each measure (sub-section) has 4 beats. We
determined tempo (beats per minute) by first de-
termining how active the target novel is. Each
of the eight basic emotions is assigned to be ei-
ther active, passive, or neutral. In TransProse, the
tempo is proportional to the activity score, which
we define to be the difference between the aver-
age density of the active emotions (anger and joy)
and the average density of the passive emotions
(sadness). The other five emotions (anticipation,
disgust, fear, surprise, and trust) were considered
ambiguous or neutral, and did not influence the
tempo.
We subjectively identified upper and lower
bounds for the possible tempo values to be 180 and
40 beats/minute, respectively. We determined ac-
tivity scores for a collection of novels, and identi-
fied the highest activity score (Actmax) and the low-
est activity score (Actmin). For a novel whose ac-
tivity score was Act, we determined tempo as per
the formula shown below:
</bodyText>
<equation confidence="0.8315725">
tempo = 40 + (Act − Actmin) * (180 − 40) (3)
Actmax − Actmin
</equation>
<bodyText confidence="0.922645">
Thus, high activity scores were represented by
</bodyText>
<page confidence="0.997445">
6
</page>
<bodyText confidence="0.9997462">
tempo values closer to 180 and lower activity
scores were represented by tempo values closer to
40. The lowest activity score in our collection of
texts, Actmin, was -0.002 whereas the highest ac-
tivity score, Actmax, was 0.017.
</bodyText>
<sectionHeader confidence="0.84995" genericHeader="method">
7 Converting Specifications to Music
</sectionHeader>
<bodyText confidence="0.999776454545454">
JFugue is an open-source Java API that helps cre-
ate generative music.7 It allows the user to easily
experiment with different notes, instruments, oc-
taves, note durations, etc within a Java program.
JFugue requires a line of specifically-formatted
text that describes the melodies in order to play
them. The initial portion of the string of JFugue to-
kens for the novel Peter Pan is shown below. The
string conveys the overall information of the piece
as well as the first eight measures (or one section)
for each of the three melodies (or voices).
</bodyText>
<equation confidence="0.951844666666667">
KCmaj X[VOLUME]=16383 V0 T180
A6/0.25 D6/0.125 F6/0.25 136/0.25
136/0.125 136/0.25 136/0.25...
</equation>
<bodyText confidence="0.999554833333333">
K stands for key and Cmaj stands for C major.
This indicates that the rest of the piece will be in
the key of C major. The second token controls the
volume, which in this example is at the loudest
value (16383). V0 stands for the first melody (or
voice). The tokens with the letter T indicate the
tempo, which in the case of this example is 180
beats per minute.
The tokens that follow indicate the notes of the
melody. The letter is the pitch class of the note,
and the number immediately following it is the
octave. The number following the slash char-
acter indicates the duration of the note. (0.125
is an eighth-note (1/8th), 0.25 is a quarter note,
0.5 is a half note, and 1.0 is a whole note.) We
used JFugue to convert the specifications of the
melodies into music. JFugue saves the pieces as
a midi files, which we converted to MP3 format.8
</bodyText>
<sectionHeader confidence="0.865667" genericHeader="method">
8 Case Studies
</sectionHeader>
<bodyText confidence="0.98907625">
We created musical pieces for several popular
novels through TransProse. These pieces are
available at: http://transprose.weebly.com/
final-pieces.html. Since these novels are
</bodyText>
<footnote confidence="0.9947305">
7http://www.jfugue.org
8The MP3 format uses a lossy data compression, but the
resulting files are significantly smaller in size. Further, a
wider array of music players support the MP3 format.
</footnote>
<bodyText confidence="0.9971515">
likely to have been read by many people, the read-
ers can compare their understanding of the story
with the music generated by TransProse. Table 1
presents details of some of these novels.
</bodyText>
<subsectionHeader confidence="0.997117">
8.1 Overall Tone
</subsectionHeader>
<bodyText confidence="0.999695181818182">
TransProse captures the overall positive or nega-
tive tone of the novel by assigning an either major
or minor key to the piece. Peter Pan and Anne
of Green Gables, novels with overall happy and
uplifting moods, created pieces in the major key.
On the other hand, novels such as Heart of Dark-
ness, A Clockwork Orange, and The Road, with
dark themes, created pieces in the minor key. The
effect of this is pieces that from the start have a
mood that aligns with the basic mood of the novel
they are based on.
</bodyText>
<subsectionHeader confidence="0.999239">
8.2 Overall Happiness and Sadness
</subsectionHeader>
<bodyText confidence="0.9999938125">
The densities of happiness and sadness in a novel
are represented in the baseline octave of a piece.
This representation instantly conveys whether the
novel has a markedly happy or sad mood. The
overall high happiness densities in Peter Pan and
Anne of Green Gables create pieces in an octave
above the average, resulting in higher tones and a
lighter overall mood. Similarly, the overall high
sadness densities in The Road and Heart of Dark-
ness result in pieces an octave lower than the aver-
age, and a darker overall tone to the music. Nov-
els, such as A Clockwork Orange, and The Little
Prince, where happiness and sadness are not dra-
matically higher or lower than the average novel
remain at the average octave, allowing for the cre-
ation of a more nuanced piece.
</bodyText>
<subsectionHeader confidence="0.999417">
8.3 Activeness of the Novel
</subsectionHeader>
<bodyText confidence="0.997696571428571">
Novels with lots of active emotion words, such
as Peter Pan, Anne of Green Gables, Lord of the
Flies, and A Clockwork Orange, generate fast-
paced pieces with tempos over 170 beats per
minute. On the other hand, The Road, which has
relatively few active emotion words is rather slow
(a tempo of 42 beats per minute).
</bodyText>
<subsectionHeader confidence="0.996112">
8.4 Primary Emotions
</subsectionHeader>
<bodyText confidence="0.999247833333333">
The top two emotions of a novel inform two of the
three melodies in a piece (Me1 and Me2). Recall
that if the melody is based on a positive emotion, it
will be an octave higher than the octave of Mo, and
if it is based on a negative emotion, it will be an oc-
tave lower. For novels where the top two emotions
</bodyText>
<page confidence="0.999824">
7
</page>
<tableCaption confidence="0.997464">
Table 1: Emotion and audio features of a few popular novels that were processed by TransProse. The
musical pieces are available at: http://transprose.weebly.com/final-pieces.html.
</tableCaption>
<table confidence="0.6161476">
Book Title Emotion 1 Emotion 2 Octave Tempo Pos/Neg Key Activity Joy-Sad
A Clockwork Orange Fear Sadness 5 171 Negative C Minor 0.009 -0.0007
Alice in Wonderland Trust Fear 5 150 Positive C Major 0.007 -0.0002
Anne of Green Gables Joy Trust 6 180 Positive C Major 0.010 0.0080
Heart of Darkness Fear Sadness 4 122 Negative C Minor 0.005 -0.0060
Little Prince, The Trust Joy 5 133 Positive C Major 0.006 0.0028
Lord of The Flies Fear Sadness 4 151 Negative C Minor 0.008 -0.0053
Peter Pan Trust Joy 6 180 Positive C Major 0.010 0.0040
Road, The Sadness Fear 4 42 Negative C Minor -0.002 -0.0080
To Kill a Mockingbird Trust Fear 5 132 Positive C Major 0.006 -0.0013
</table>
<bodyText confidence="0.9928944">
are both positive, such as Anne of Green Gables
(trust and joy), the pieces sound especially light
and joyful. For novels where the top two emotions
are both negative, such as The Road (sadness and
fear), the pieces sound especially dark.
</bodyText>
<subsectionHeader confidence="0.987198">
8.5 Emotional Activity
</subsectionHeader>
<bodyText confidence="0.999976666666667">
Unlike the overall pace of the novel, individual
segments of activity were also identified in the
pieces through the number and duration of notes
(with more and shorter notes indicating higher
emotion densities). This can be especially heard
in the third section of A Clockwork Orange, the fi-
nal section of The Adventures of Sherlock Holmes,
the second section of To Kill a Mockingbird, and
the final section of Lord of the Flies. In A Clock-
work Orange, the main portion of the piece is
chaotic and eventful, likely as the main charac-
ters cause havoc; at the end of the novel (as the
main character undergoes therapy) the piece dra-
matically changes and becomes structured. Sim-
ilarly, in Heart of Darkness, the piece starts out
only playing a few notes; as the tension in the
novel builds, the number of notes increases and
their durations decrease.
</bodyText>
<subsectionHeader confidence="0.546756">
9 Comparing Alternative Choices
</subsectionHeader>
<bodyText confidence="0.999849619047619">
We examine choices made in TransProse by com-
paring musical pieces generated with different al-
ternatives. These audio clips are available here:
http://transprose.weebly.com/clips.html.
Pieces with two melodies (based on overall
emotion density and the predominant emotion’s
density) and pieces based on four melodies (based
on the top three emotions and the overall emotion
density) were generated and uploaded in the clips
webpage. Observe that with only two melodies,
the pieces tend to sound thin, whereas with four
melodies the pieces sound less cohesive and some-
times chaotic. The effect of increasing and de-
creasing the total number of sections and sub-
sections is also presented. Additionally, the web-
page displays pieces with tempos and octaves be-
yond the limits chosen in TransProse. We also
show other variations such as pieces for relatively
positive novels generated in C minor (instead of
C major). These alternatives are not necessarily
incorrect, but they tend to often be less effective.
</bodyText>
<sectionHeader confidence="0.79503" genericHeader="conclusions">
10 Limitations and Future work
</sectionHeader>
<bodyText confidence="0.999974592592593">
We presented a system, TransProse, that gener-
ates music according to the use of emotion words
in a given piece of text. A number of avenues
for future work exist such as exploring the use of
mid-piece key changes and intentional harmony
and discord between the melodies. We will fur-
ther explore ways to capture activity in music. For
example, an automatically generated activity lex-
icon (built using the method proposed by Turney
and Littman (2003)) can be used to identify por-
tions of text where the characters are relatively
active (fighting, dancing, conspiring, etc) and ar-
eas where they are relatively passive (calm, inca-
pacitated, sad, etc). One can even capture non-
emotional features of the text in music. For ex-
ample, recurring characters or locations in a novel
could be indicated by recurring motifs. We will
conduct human evaluations asking people to judge
various aspects of the generated music such as the
quality of music and the amount and type of emo-
tion evoked by the music. We will also evaluate
the impact of textual features such as the length of
the novel and the style of writing on the generated
music. Work on capturing note models (analogous
to language models) from existing pieces of music
and using them to improve the music generated by
TransProse seems especially promising.
</bodyText>
<page confidence="0.997363">
8
</page>
<sectionHeader confidence="0.989427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998408601694915">
[Agarwal et al.2011] Apoorv Agarwal, Boyi Xie, Ilia
Vovsha, Owen Rambow, and Rebecca Passonneau.
2011. Sentiment analysis of twitter data. In Pro-
ceedings of the Workshop on Languages in Social
Media, LSM ’11, pages 30–38, Portland, Oregon.
[Aisopos et al.2012] Fotis Aisopos, George Papadakis,
Konstantinos Tserpes, and Theodora Varvarigou.
2012. Textual and contextual patterns for sentiment
analysis over microblogs. In Proceedings of the
21st International Conference on World Wide Web
Companion, WWW ’12 Companion, pages 453–
454, New York, NY, USA.
[Ali and Peynirciolu2010] S Omar Ali and Zehra F
Peynirciolu. 2010. Intensity of emotions con-
veyed and elicited by familiar and unfamiliar mu-
sic. Music Perception: An Interdisciplinary Journal,
27(3):177–182.
[Alm and Sproat2005] Cecilia O. Alm and Richard
Sproat, 2005. Emotional sequencing and develop-
ment in fairy tales, pages 668–674. Springer.
[Aman and Szpakowicz2007] Saima Aman and Stan
Szpakowicz. 2007. Identifying expressions of emo-
tion in text. In Vclav Matouˇsek and Pavel Mautner,
editors, Text, Speech and Dialogue, volume 4629 of
Lecture Notes in Computer Science, pages 196–205.
Springer Berlin / Heidelberg.
[Bakliwal et al.2012] Akshat Bakliwal, Piyush Arora,
Senthil Madhappan, Nikhil Kapre, Mukesh Singh,
and Vasudeva Varma. 2012. Mining sentiments
from tweets. In Proceedings of the 3rd Workshop on
Computational Approaches to Subjectivity and Sen-
timentAnalysis, WASSA ’12, pages 11–18, Jeju, Re-
public of Korea.
[Balkwill and Thompson1999] Laura-Lee Balkwill and
William Forde Thompson. 1999. A cross-cultural
investigation of the perception of emotion in music:
Psychophysical and cultural cues. Music perception,
pages 43–64.
[Bellegarda2010] Jerome Bellegarda. 2010. Emotion
analysis using latent affective folding and embed-
ding. In Proceedings of the NAACL-HLT 2010
Workshop on Computational Approaches to Analy-
sis and Generation of Emotion in Text, Los Angeles,
California.
[Biles1994] John Biles. 1994. Genjam: A genetic al-
gorithm for generating jazz solos. pages 131–137.
[Bollen et al.2009] Johan Bollen, Alberto Pepe, and
Huina Mao. 2009. Modeling public mood and emo-
tion: Twitter sentiment and socio-economic phe-
nomena. CoRR.
[Boucouvalas2002] Anthony C. Boucouvalas. 2002.
Real time text-to-emotion engine for expressive in-
ternet communication. Emerging Communication:
Studies on New Technologies and Practices in Com-
munication, 5:305–318.
[Brody and Diakopoulos2011] Samuel Brody
and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: us-
ing word lengthening to detect sentiment in
microblogs. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 562–570, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Brown1970] Calvin S Brown. 1970. The relations be-
tween music and literature as a field of study. Com-
parative Literature, 22(2):97–107.
[Collins2008] Nick Collins. 2008. The analysis
of generative music programs. Organised Sound,
13(3):237–248.
[Cope1996] David Cope. 1996. Experiments in musi-
cal intelligence, volume 12. AR Editions Madison,
WI.
[Dowling and Harwood1986] W Jay Dowling and
Dane L Harwood. 1986. Music cognition, volume
19986. Academic Press New York.
[Gabrielsson and Lindstr¨om2001] Alf Gabrielsson and
Erik Lindstr¨om. 2001. The influence of musical
structure on emotional expression.
[Genereux and Evans2006] Michel Genereux and
Roger P. Evans. 2006. Distinguishing affective
states in weblogs. In Proceedings of the AAAI
Spring Symposium on Computational Approaches
to Analysing Weblogs, pages 27–29, Stanford,
California.
[Hatten1991] Robert Hatten. 1991. On narrativity in
music: expressive genres and levels of discourse in
beethoven.
[Hunter et al.2008] Patrick G Hunter, E Glenn Schel-
lenberg, and Ulrich Schimmack. 2008. Mixed
affective responses to music with conflicting cues.
Cognition &amp; Emotion, 22(2):327–352.
[Hunter et al.2010] Patrick G Hunter, E Glenn Schel-
lenberg, and Ulrich Schimmack. 2010. Feelings
and perceptions of happiness and sadness induced
by music: Similarities, differences, and mixed emo-
tions. Psychology of Aesthetics, Creativity, and the
Arts, 4(1):47.
[Iida et al.2000] Akemi Iida, Nick Campbell, Soichiro
Iga, Fumito Higuchi, and Michiaki Yasumura. 2000.
A speech synthesis system with emotion for assist-
ing communication. In ISCA Tutorial and Research
Workshop (ITRW) on Speech and Emotion.
[Juslin and Laukka2003] Patrik N Juslin and Petri
Laukka. 2003. Communication of emotions in
vocal expression and music performance: Differ-
ent channels, same code? Psychological bulletin,
129(5):770.
[Kim et al.2009] Elsa Kim, Sam Gilbert, Michael J. Ed-
wards, and Erhardt Graeff. 2009. Detecting sadness
in 140 characters: Sentiment analysis of mourning
michael jackson on twitter.
[Lerdahl and Jackendoff1983] Fred Lerdahl and Ray S
Jackendoff. 1983. A generative theory of tonal mu-
sic. MIT press.
[Liu and Zhang2012] Bing Liu and Lei Zhang. 2012.
A survey of opinion mining and sentiment analysis.
In Charu C. Aggarwal and ChengXiang Zhai, edi-
tors, Mining Text Data, pages 415–463. Springer.
</reference>
<page confidence="0.944639">
9
</page>
<reference confidence="0.999647512605042">
[Liu et al.2003] Hugo Liu, Henry Lieberman, and Ted
Selker. 2003. A model of textual affect sensing us-
ing real-world knowledge. In Proceedings of the 8th
International Conference on Intelligent User Inter-
faces, pages 125–132, New York, NY. ACM.
[Micznik2001] Vera Micznik. 2001. Music and nar-
rative revisited: degrees of narrativity in beethoven
and mahler. Journal of the Royal Musical Associa-
tion, 126(2):193–249.
[Mihalcea and Liu2006] Rada Mihalcea and Hugo Liu.
2006. A corpus-based approach to finding happi-
ness. In Proceedings of the AAAI Spring Sympo-
sium on Computational Approaches to Analysing
Weblogs, pages 139–144. AAAI Press.
[Mohammad and Turney2010] Saif M. Mohammad and
Peter D. Turney. 2010. Emotions evoked by com-
mon words and phrases: Using Mechanical Turk
to create an emotion lexicon. In Proceedings of
the NAACL-HLT Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in
Text, LA, California.
[Mohammad and Turney2013] Saif M. Mohammad and
Peter D. Turney. 2013. Crowdsourcing a word-
emotion association lexicon. 29(3):436–465.
[Mohammad and Yang2011] Saif M. Mohammad and
Tony (Wenda) Yang. 2011. Tracking sentiment in
mail: How genders differ on emotional axes. In Pro-
ceedings of the ACL Workshop on Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ’11, Portland, OR, USA.
[Mohammad2012] Saif M. Mohammad. 2012. From
once upon a time to happily ever after: Tracking
emotions in mail and books. Decision Support Sys-
tems, 53(4):730–741.
[Morrison and Demorest2009] Steven J Morrison and
Steven M Demorest. 2009. Cultural constraints on
music perception and cognition. Progress in brain
research, 178:67–77.
[Neviarouskaya et al.2009] Alena Neviarouskaya, Hel-
mut Prendinger, and Mitsuru Ishizuka. 2009. Com-
positionality principle in recognition of fine-grained
emotions from text. In Proceedings of the Proceed-
ings of the Third International Conference on We-
blogs and Social Media (ICWSM-09), pages 278–
281, San Jose, California.
[Pak and Paroubek2010] Alexander Pak and Patrick
Paroubek. 2010. Twitter as a corpus for senti-
ment analysis and opinion mining. In Proceed-
ings of the 7th Conference on International Lan-
guage Resources and Evaluation, LREC ’10, Val-
letta, Malta, May. European Language Resources
Association (ELRA).
[Pang and Lee2008] Bo Pang and Lillian Lee. 2008.
Opinion mining and sentiment analysis. Founda-
tions and Trends in IR, 2(1–2):1–135.
[Perricone2000] J. Perricone. 2000. Melody in Song-
writing: Tools and Techniques for Writing Hit
Songs. Berklee guide. Berklee Press.
[Picard2000] Rosalind W Picard. 2000. Affective com-
puting. MIT press.
[Pierre-Yves2003] Oudeyer Pierre-Yves. 2003. The
production and recognition of emotions in speech:
features and algorithms. International Journal of
Human-Computer Studies, 59(1):157–183.
[Roederer2008] Juan G Roederer. 2008. The physics
and psychophysics of music: an introduction.
Springer Publishing Company, Incorporated.
[Rousseau2009] Jean-Jacques Rousseau. 2009. Essay
on the origin of languages and writings related to
music, volume 7. UPNE.
[Schr¨oder2009] Marc Schr¨oder. 2009. Expressive
speech synthesis: Past, present, and possible futures.
In Affective information processing, pages 111–126.
Springer.
[Spencer1857] Herbert Spencer. 1857. The origin and
function of music. Frasers Magazine, 56:396–408.
[Strapparava and Valitutti2004] Carlo Strapparava and
Alessandro Valitutti. 2004. WordNet-Affect: An
Affective Extension of WordNet. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1083–
1086, Lisbon, Portugal.
[Tao and Tan2005] Jianhua Tao and Tieniu Tan. 2005.
Affective computing: A review. In Affective com-
puting and intelligent interaction, pages 981–995.
Springer.
[Tenney1988] James Tenney. 1988. A history of conso-
nance and dissonance. Excelsior Music Publishing
Company New York.
[Thelwall et al.2011] Mike Thelwall, Kevan Buckley,
and Georgios Paltoglou. 2011. Sentiment in Twitter
events. Journal of the American Society for Infor-
mation Science and Technology, 62(2):406–418.
[Tumasjan et al.2010] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2010. Predicting elections with twitter : What 140
characters reveal about political sentiment. Word
Journal Of The International Linguistic Association,
pages 178–185.
[Turney and Littman2003] Peter Turney and Michael L
Littman. 2003. Measuring praise and criticism:
Inference of semantic orientation from association.
ACM Transactions on Information Systems, 21(4).
[Volkova et al.2010] Ekaterina P Volkova, Betty J
Mohler, Detmar Meurers, Dale Gerdemann, and
Heinrich H B¨ulthoff. 2010. Emotional perception of
fairy tales: Achieving agreement in emotion annota-
tion of text. In Proceedings of the NAACL HLT 2010
Workshop on Computational Approaches to Analysis
and Generation of Emotion in Text, pages 98–106.
Association for Computational Linguistics.
[Webster and Weir2005] Gregory D Webster and
Catherine G Weir. 2005. Emotional responses to
music: Interactive effects of mode, texture, and
tempo. Motivation and Emotion, 29(1):19–39.
[Zhe and Boucouvalas2002] Xu Zhe and A Boucou-
valas, 2002. Text-to-Emotion Engine for Real Time
Internet CommunicationText-to-Emotion Engine for
Real Time Internet Communication, pages 164–168.
</reference>
<page confidence="0.997794">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.740055">
<title confidence="0.999538">Generating Music from Literature</title>
<author confidence="0.999821">Hannah Davis Saif M Mohammad</author>
<affiliation confidence="0.772844">New York University National Research Council Canada</affiliation>
<email confidence="0.984042">hannah.davis@nyu.edusaif.mohammad@nrc-cnrc.gc.ca</email>
<abstract confidence="0.997868333333333">present a system, that automatically generates musical pieces from text. TransProse uses known relations between elements of music such as tempo and scale, and the emotions they evoke. Further, it uses a novel mechanism to determine sequences of notes that capture the emotional activity in text. The work has applications in information visualization, in creating audio-visual e-books, and in developing music apps.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>30--38</pages>
<location>Portland, Oregon.</location>
<marker>[Agarwal et al.2011]</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 30–38, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fotis Aisopos</author>
<author>George Papadakis</author>
<author>Konstantinos Tserpes</author>
<author>Theodora Varvarigou</author>
</authors>
<title>Textual and contextual patterns for sentiment analysis over microblogs.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st International Conference on World Wide Web Companion, WWW ’12 Companion,</booktitle>
<pages>453--454</pages>
<location>New York, NY, USA.</location>
<marker>[Aisopos et al.2012]</marker>
<rawString>Fotis Aisopos, George Papadakis, Konstantinos Tserpes, and Theodora Varvarigou. 2012. Textual and contextual patterns for sentiment analysis over microblogs. In Proceedings of the 21st International Conference on World Wide Web Companion, WWW ’12 Companion, pages 453– 454, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Omar Ali</author>
<author>Zehra F Peynirciolu</author>
</authors>
<title>Intensity of emotions conveyed and elicited by familiar and unfamiliar music. Music Perception: An Interdisciplinary</title>
<date>2010</date>
<journal>Journal,</journal>
<volume>27</volume>
<issue>3</issue>
<marker>[Ali and Peynirciolu2010]</marker>
<rawString>S Omar Ali and Zehra F Peynirciolu. 2010. Intensity of emotions conveyed and elicited by familiar and unfamiliar music. Music Perception: An Interdisciplinary Journal, 27(3):177–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecilia O Alm</author>
<author>Richard Sproat</author>
</authors>
<title>Emotional sequencing and development in fairy tales,</title>
<date>2005</date>
<pages>668--674</pages>
<publisher>Springer.</publisher>
<marker>[Alm and Sproat2005]</marker>
<rawString>Cecilia O. Alm and Richard Sproat, 2005. Emotional sequencing and development in fairy tales, pages 668–674. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saima Aman</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Identifying expressions of emotion in text.</title>
<date>2007</date>
<booktitle>In Vclav Matouˇsek and Pavel Mautner, editors, Text, Speech and Dialogue,</booktitle>
<volume>4629</volume>
<pages>196--205</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>[Aman and Szpakowicz2007]</marker>
<rawString>Saima Aman and Stan Szpakowicz. 2007. Identifying expressions of emotion in text. In Vclav Matouˇsek and Pavel Mautner, editors, Text, Speech and Dialogue, volume 4629 of Lecture Notes in Computer Science, pages 196–205. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshat Bakliwal</author>
<author>Piyush Arora</author>
<author>Senthil Madhappan</author>
<author>Nikhil Kapre</author>
<author>Mukesh Singh</author>
<author>Vasudeva Varma</author>
</authors>
<title>Mining sentiments from tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and SentimentAnalysis, WASSA ’12,</booktitle>
<pages>11--18</pages>
<location>Jeju, Republic of</location>
<marker>[Bakliwal et al.2012]</marker>
<rawString>Akshat Bakliwal, Piyush Arora, Senthil Madhappan, Nikhil Kapre, Mukesh Singh, and Vasudeva Varma. 2012. Mining sentiments from tweets. In Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and SentimentAnalysis, WASSA ’12, pages 11–18, Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura-Lee Balkwill</author>
<author>William Forde Thompson</author>
</authors>
<title>A cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues. Music perception,</title>
<date>1999</date>
<pages>43--64</pages>
<marker>[Balkwill and Thompson1999]</marker>
<rawString>Laura-Lee Balkwill and William Forde Thompson. 1999. A cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues. Music perception, pages 43–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome Bellegarda</author>
</authors>
<title>Emotion analysis using latent affective folding and embedding.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>Los Angeles, California.</location>
<marker>[Bellegarda2010]</marker>
<rawString>Jerome Bellegarda. 2010. Emotion analysis using latent affective folding and embedding. In Proceedings of the NAACL-HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Biles</author>
</authors>
<title>Genjam: A genetic algorithm for generating jazz solos.</title>
<date>1994</date>
<pages>131--137</pages>
<marker>[Biles1994]</marker>
<rawString>John Biles. 1994. Genjam: A genetic algorithm for generating jazz solos. pages 131–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Alberto Pepe</author>
<author>Huina Mao</author>
</authors>
<title>Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena.</title>
<date>2009</date>
<journal>CoRR.</journal>
<marker>[Bollen et al.2009]</marker>
<rawString>Johan Bollen, Alberto Pepe, and Huina Mao. 2009. Modeling public mood and emotion: Twitter sentiment and socio-economic phenomena. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony C Boucouvalas</author>
</authors>
<title>Real time text-to-emotion engine for expressive internet communication. Emerging Communication:</title>
<date>2002</date>
<booktitle>Studies on New Technologies and Practices in Communication,</booktitle>
<pages>5--305</pages>
<marker>[Boucouvalas2002]</marker>
<rawString>Anthony C. Boucouvalas. 2002. Real time text-to-emotion engine for expressive internet communication. Emerging Communication: Studies on New Technologies and Practices in Communication, 5:305–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>562--570</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Brody and Diakopoulos2011]</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word lengthening to detect sentiment in microblogs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 562–570, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Calvin S Brown</author>
</authors>
<title>The relations between music and literature as a field of study.</title>
<date>1970</date>
<journal>Comparative Literature,</journal>
<volume>22</volume>
<issue>2</issue>
<marker>[Brown1970]</marker>
<rawString>Calvin S Brown. 1970. The relations between music and literature as a field of study. Comparative Literature, 22(2):97–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Collins</author>
</authors>
<title>The analysis of generative music programs.</title>
<date>2008</date>
<journal>Organised Sound,</journal>
<volume>13</volume>
<issue>3</issue>
<marker>[Collins2008]</marker>
<rawString>Nick Collins. 2008. The analysis of generative music programs. Organised Sound, 13(3):237–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cope</author>
</authors>
<title>Experiments in musical intelligence,</title>
<date>1996</date>
<volume>12</volume>
<publisher>AR Editions</publisher>
<location>Madison, WI.</location>
<marker>[Cope1996]</marker>
<rawString>David Cope. 1996. Experiments in musical intelligence, volume 12. AR Editions Madison, WI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jay Dowling</author>
<author>Dane L Harwood</author>
</authors>
<title>Music cognition, volume 19986.</title>
<date>1986</date>
<publisher>Academic Press</publisher>
<location>New York.</location>
<marker>[Dowling and Harwood1986]</marker>
<rawString>W Jay Dowling and Dane L Harwood. 1986. Music cognition, volume 19986. Academic Press New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alf Gabrielsson</author>
<author>Erik Lindstr¨om</author>
</authors>
<title>The influence of musical structure on emotional expression.</title>
<date>2001</date>
<marker>[Gabrielsson and Lindstr¨om2001]</marker>
<rawString>Alf Gabrielsson and Erik Lindstr¨om. 2001. The influence of musical structure on emotional expression.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Genereux</author>
<author>Roger P Evans</author>
</authors>
<title>Distinguishing affective states in weblogs.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing Weblogs,</booktitle>
<pages>27--29</pages>
<location>Stanford, California.</location>
<marker>[Genereux and Evans2006]</marker>
<rawString>Michel Genereux and Roger P. Evans. 2006. Distinguishing affective states in weblogs. In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, pages 27–29, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Hatten</author>
</authors>
<title>On narrativity in music: expressive genres and levels of discourse in beethoven.</title>
<date>1991</date>
<marker>[Hatten1991]</marker>
<rawString>Robert Hatten. 1991. On narrativity in music: expressive genres and levels of discourse in beethoven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick G Hunter</author>
<author>E Glenn Schellenberg</author>
<author>Ulrich Schimmack</author>
</authors>
<title>Mixed affective responses to music with conflicting cues.</title>
<date>2008</date>
<journal>Cognition &amp; Emotion,</journal>
<volume>22</volume>
<issue>2</issue>
<marker>[Hunter et al.2008]</marker>
<rawString>Patrick G Hunter, E Glenn Schellenberg, and Ulrich Schimmack. 2008. Mixed affective responses to music with conflicting cues. Cognition &amp; Emotion, 22(2):327–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick G Hunter</author>
<author>E Glenn Schellenberg</author>
<author>Ulrich Schimmack</author>
</authors>
<title>Feelings and perceptions of happiness and sadness induced by music: Similarities, differences, and mixed emotions.</title>
<date>2010</date>
<journal>Psychology of Aesthetics, Creativity, and the Arts,</journal>
<volume>4</volume>
<issue>1</issue>
<marker>[Hunter et al.2010]</marker>
<rawString>Patrick G Hunter, E Glenn Schellenberg, and Ulrich Schimmack. 2010. Feelings and perceptions of happiness and sadness induced by music: Similarities, differences, and mixed emotions. Psychology of Aesthetics, Creativity, and the Arts, 4(1):47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akemi Iida</author>
<author>Nick Campbell</author>
<author>Soichiro Iga</author>
<author>Fumito Higuchi</author>
<author>Michiaki Yasumura</author>
</authors>
<title>A speech synthesis system with emotion for assisting communication.</title>
<date>2000</date>
<booktitle>In ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion.</booktitle>
<marker>[Iida et al.2000]</marker>
<rawString>Akemi Iida, Nick Campbell, Soichiro Iga, Fumito Higuchi, and Michiaki Yasumura. 2000. A speech synthesis system with emotion for assisting communication. In ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik N Juslin</author>
<author>Petri Laukka</author>
</authors>
<title>Communication of emotions in vocal expression and music performance: Different channels, same code? Psychological bulletin,</title>
<date>2003</date>
<pages>129--5</pages>
<marker>[Juslin and Laukka2003]</marker>
<rawString>Patrik N Juslin and Petri Laukka. 2003. Communication of emotions in vocal expression and music performance: Different channels, same code? Psychological bulletin, 129(5):770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elsa Kim</author>
<author>Sam Gilbert</author>
<author>Michael J Edwards</author>
<author>Erhardt Graeff</author>
</authors>
<title>Detecting sadness in 140 characters: Sentiment analysis of mourning michael jackson on twitter.</title>
<date>2009</date>
<marker>[Kim et al.2009]</marker>
<rawString>Elsa Kim, Sam Gilbert, Michael J. Edwards, and Erhardt Graeff. 2009. Detecting sadness in 140 characters: Sentiment analysis of mourning michael jackson on twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Lerdahl</author>
<author>Ray S Jackendoff</author>
</authors>
<title>A generative theory of tonal music.</title>
<date>1983</date>
<publisher>MIT press.</publisher>
<marker>[Lerdahl and Jackendoff1983]</marker>
<rawString>Fred Lerdahl and Ray S Jackendoff. 1983. A generative theory of tonal music. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Lei Zhang</author>
</authors>
<title>A survey of opinion mining and sentiment analysis.</title>
<date>2012</date>
<booktitle>In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data,</booktitle>
<pages>415--463</pages>
<publisher>Springer.</publisher>
<marker>[Liu and Zhang2012]</marker>
<rawString>Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis. In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data, pages 415–463. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
<author>Henry Lieberman</author>
<author>Ted Selker</author>
</authors>
<title>A model of textual affect sensing using real-world knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Conference on Intelligent User Interfaces,</booktitle>
<pages>125--132</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<marker>[Liu et al.2003]</marker>
<rawString>Hugo Liu, Henry Lieberman, and Ted Selker. 2003. A model of textual affect sensing using real-world knowledge. In Proceedings of the 8th International Conference on Intelligent User Interfaces, pages 125–132, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Micznik</author>
</authors>
<title>Music and narrative revisited: degrees of narrativity in beethoven and mahler.</title>
<date>2001</date>
<journal>Journal of the Royal Musical Association,</journal>
<volume>126</volume>
<issue>2</issue>
<marker>[Micznik2001]</marker>
<rawString>Vera Micznik. 2001. Music and narrative revisited: degrees of narrativity in beethoven and mahler. Journal of the Royal Musical Association, 126(2):193–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Hugo Liu</author>
</authors>
<title>A corpus-based approach to finding happiness.</title>
<date>2006</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing Weblogs,</booktitle>
<pages>139--144</pages>
<publisher>AAAI Press.</publisher>
<marker>[Mihalcea and Liu2006]</marker>
<rawString>Rada Mihalcea and Hugo Liu. 2006. A corpus-based approach to finding happiness. In Proceedings of the AAAI Spring Symposium on Computational Approaches to Analysing Weblogs, pages 139–144. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>LA, California.</location>
<marker>[Mohammad and Turney2010]</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, LA, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a wordemotion association lexicon.</title>
<date>2013</date>
<pages>29--3</pages>
<marker>[Mohammad and Turney2013]</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2013. Crowdsourcing a wordemotion association lexicon. 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Tony Yang</author>
</authors>
<title>Tracking sentiment in mail: How genders differ on emotional axes.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’11,</booktitle>
<location>Portland, OR, USA.</location>
<marker>[Mohammad and Yang2011]</marker>
<rawString>Saif M. Mohammad and Tony (Wenda) Yang. 2011. Tracking sentiment in mail: How genders differ on emotional axes. In Proceedings of the ACL Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’11, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
</authors>
<title>From once upon a time to happily ever after: Tracking emotions in mail and books. Decision Support Systems,</title>
<date>2012</date>
<pages>53--4</pages>
<marker>[Mohammad2012]</marker>
<rawString>Saif M. Mohammad. 2012. From once upon a time to happily ever after: Tracking emotions in mail and books. Decision Support Systems, 53(4):730–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Morrison</author>
<author>Steven M Demorest</author>
</authors>
<title>Cultural constraints on music perception and cognition.</title>
<date>2009</date>
<booktitle>Progress in brain research,</booktitle>
<pages>178--67</pages>
<marker>[Morrison and Demorest2009]</marker>
<rawString>Steven J Morrison and Steven M Demorest. 2009. Cultural constraints on music perception and cognition. Progress in brain research, 178:67–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena Neviarouskaya</author>
<author>Helmut Prendinger</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Compositionality principle in recognition of fine-grained emotions from text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Proceedings of the Third International Conference on Weblogs and Social Media (ICWSM-09),</booktitle>
<pages>278--281</pages>
<location>San Jose, California.</location>
<marker>[Neviarouskaya et al.2009]</marker>
<rawString>Alena Neviarouskaya, Helmut Prendinger, and Mitsuru Ishizuka. 2009. Compositionality principle in recognition of fine-grained emotions from text. In Proceedings of the Proceedings of the Third International Conference on Weblogs and Social Media (ICWSM-09), pages 278– 281, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the 7th Conference on International Language Resources and Evaluation, LREC ’10,</booktitle>
<location>Valletta, Malta,</location>
<marker>[Pak and Paroubek2010]</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the 7th Conference on International Language Resources and Evaluation, LREC ’10, Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in IR,</booktitle>
<pages>2--1</pages>
<marker>[Pang and Lee2008]</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in IR, 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Perricone</author>
</authors>
<title>Melody in Songwriting: Tools and Techniques for Writing Hit Songs. Berklee guide.</title>
<date>2000</date>
<publisher>Berklee Press.</publisher>
<marker>[Perricone2000]</marker>
<rawString>J. Perricone. 2000. Melody in Songwriting: Tools and Techniques for Writing Hit Songs. Berklee guide. Berklee Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosalind W Picard</author>
</authors>
<title>Affective computing.</title>
<date>2000</date>
<publisher>MIT press.</publisher>
<marker>[Picard2000]</marker>
<rawString>Rosalind W Picard. 2000. Affective computing. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oudeyer Pierre-Yves</author>
</authors>
<title>The production and recognition of emotions in speech: features and algorithms.</title>
<date>2003</date>
<journal>International Journal of Human-Computer Studies,</journal>
<volume>59</volume>
<issue>1</issue>
<marker>[Pierre-Yves2003]</marker>
<rawString>Oudeyer Pierre-Yves. 2003. The production and recognition of emotions in speech: features and algorithms. International Journal of Human-Computer Studies, 59(1):157–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan G Roederer</author>
</authors>
<title>The physics and psychophysics of music: an introduction.</title>
<date>2008</date>
<publisher>Springer Publishing Company, Incorporated.</publisher>
<marker>[Roederer2008]</marker>
<rawString>Juan G Roederer. 2008. The physics and psychophysics of music: an introduction. Springer Publishing Company, Incorporated.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Jacques Rousseau</author>
</authors>
<title>Essay on the origin of languages and writings related to music, volume 7.</title>
<date>2009</date>
<publisher>UPNE.</publisher>
<marker>[Rousseau2009]</marker>
<rawString>Jean-Jacques Rousseau. 2009. Essay on the origin of languages and writings related to music, volume 7. UPNE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Schr¨oder</author>
</authors>
<title>Expressive speech synthesis: Past, present, and possible futures.</title>
<date>2009</date>
<booktitle>In Affective information processing,</booktitle>
<pages>111--126</pages>
<publisher>Springer.</publisher>
<marker>[Schr¨oder2009]</marker>
<rawString>Marc Schr¨oder. 2009. Expressive speech synthesis: Past, present, and possible futures. In Affective information processing, pages 111–126. Springer.</rawString>
</citation>
<citation valid="false">
<title>The origin and function of music. Frasers Magazine,</title>
<pages>56--396</pages>
<marker>[Spencer1857]</marker>
<rawString>Herbert Spencer. 1857. The origin and function of music. Frasers Magazine, 56:396–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Alessandro Valitutti</author>
</authors>
<title>WordNet-Affect: An Affective Extension of WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC-2004),</booktitle>
<pages>1083--1086</pages>
<location>Lisbon, Portugal.</location>
<marker>[Strapparava and Valitutti2004]</marker>
<rawString>Carlo Strapparava and Alessandro Valitutti. 2004. WordNet-Affect: An Affective Extension of WordNet. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC-2004), pages 1083– 1086, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Tao</author>
<author>Tieniu Tan</author>
</authors>
<title>Affective computing: A review.</title>
<date>2005</date>
<booktitle>In Affective computing and intelligent interaction,</booktitle>
<pages>981--995</pages>
<publisher>Springer.</publisher>
<marker>[Tao and Tan2005]</marker>
<rawString>Jianhua Tao and Tieniu Tan. 2005. Affective computing: A review. In Affective computing and intelligent interaction, pages 981–995. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Tenney</author>
</authors>
<title>A history of consonance and dissonance.</title>
<date>1988</date>
<publisher>Excelsior Music Publishing Company</publisher>
<location>New York.</location>
<marker>[Tenney1988]</marker>
<rawString>James Tenney. 1988. A history of consonance and dissonance. Excelsior Music Publishing Company New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
</authors>
<title>Sentiment in Twitter events.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>2</issue>
<marker>[Thelwall et al.2011]</marker>
<rawString>Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2011. Sentiment in Twitter events. Journal of the American Society for Information Science and Technology, 62(2):406–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Predicting elections with twitter : What 140 characters reveal about political sentiment. Word Journal Of The International Linguistic Association,</title>
<date>2010</date>
<pages>178--185</pages>
<marker>[Tumasjan et al.2010]</marker>
<rawString>Andranik Tumasjan, Timm O Sprenger, Philipp G Sandner, and Isabell M Welpe. 2010. Predicting elections with twitter : What 140 characters reveal about political sentiment. Word Journal Of The International Linguistic Association, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<marker>[Turney and Littman2003]</marker>
<rawString>Peter Turney and Michael L Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina P Volkova</author>
<author>Betty J Mohler</author>
<author>Detmar Meurers</author>
<author>Dale Gerdemann</author>
<author>Heinrich H B¨ulthoff</author>
</authors>
<title>Emotional perception of fairy tales: Achieving agreement in emotion annotation of text.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>98--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Volkova et al.2010]</marker>
<rawString>Ekaterina P Volkova, Betty J Mohler, Detmar Meurers, Dale Gerdemann, and Heinrich H B¨ulthoff. 2010. Emotional perception of fairy tales: Achieving agreement in emotion annotation of text. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98–106. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory D Webster</author>
<author>Catherine G Weir</author>
</authors>
<title>Emotional responses to music: Interactive effects of mode, texture, and tempo.</title>
<date>2005</date>
<journal>Motivation and Emotion,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>[Webster and Weir2005]</marker>
<rawString>Gregory D Webster and Catherine G Weir. 2005. Emotional responses to music: Interactive effects of mode, texture, and tempo. Motivation and Emotion, 29(1):19–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Zhe</author>
<author>A Boucouvalas</author>
</authors>
<title>Text-to-Emotion Engine for Real Time Internet CommunicationText-to-Emotion Engine for Real Time Internet Communication,</title>
<date>2002</date>
<pages>164--168</pages>
<marker>[Zhe and Boucouvalas2002]</marker>
<rawString>Xu Zhe and A Boucouvalas, 2002. Text-to-Emotion Engine for Real Time Internet CommunicationText-to-Emotion Engine for Real Time Internet Communication, pages 164–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
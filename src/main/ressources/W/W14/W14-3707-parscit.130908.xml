<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006890">
<title confidence="0.989835">
The Modular Community Structure of Linguistic Predication Networks
</title>
<author confidence="0.996824">
Aaron Gerow
</author>
<affiliation confidence="0.9961165">
Computation Institute
University of Chicago
</affiliation>
<address confidence="0.71037">
Chicago, IL, USA
</address>
<email confidence="0.998985">
gerow@uchicago.edu
</email>
<author confidence="0.986052">
James Evans
</author>
<affiliation confidence="0.993887">
Dept. of Sociology &amp; Computation Institute
University of Chicago
</affiliation>
<address confidence="0.710161">
Chicago, IL, USA
</address>
<email confidence="0.999283">
jevans@uchicago.edu
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979846153846">
This paper examines the structure of lin-
guistic predications in English text. Iden-
tified by the copular “is-a” form, predi-
cations assert category membership (hy-
pernymy) or equivalence (synonymy) be-
tween two words. Because predication
expresses ontological structure, we hy-
pothesize that networks of predications
will form modular groups. To measure
this, we introduce a semantically mo-
tivated measure of predication strength
to weight relevant predications observed
in text. Results show that predications
do indeed form modular structures with-
out any weighting (Q ≈ 0.6) and that
using predication strength increases this
modularity (Q ≈ 0.9) without discard-
ing low-frequency items. This high
level of modularity supports the network-
based analysis and the use of predication
strength as a way to extract dense semantic
clusters. Additionally, words’ centrality
within communities exhibits slight corre-
lation with hypernym depths in WordNet,
underscoring the ontological organization
of predication.
</bodyText>
<sectionHeader confidence="0.991794" genericHeader="keywords">
1 Introduction &amp; Background
</sectionHeader>
<bodyText confidence="0.999824722222222">
Statistical patterns in language use are evident at
many levels and have proved useful in an increas-
ingly wide range of computational and cognitive
applications. Statistical regularities offer a way
to quantify and model how people create, encode
and use knowledge about the world. Statements
specifically about “what things are” (ie. onto-
logical statements) offer uniquely transparent evi-
dence about peoples’ knowledge of the world. Our
research adopts a corpus-based approach in which
networks of predications are analyzed to assess the
underlying structure of ontological assertions.
Word-word predications, observed as the copu-
lar is-a form in English, are important because,
unlike most grammatical constructions that have
few semantic constraints, predications tend to im-
ply category membership or equivalence. Take (i)
and (ii) for example:
</bodyText>
<listItem confidence="0.722709">
(i) Safety is always a primary concern.
</listItem>
<bodyText confidence="0.992823060606061">
(ii) This organization is an institution
where [...].
(i) is a category assertion (safety as a type of con-
cern) and (ii) is an equivalence assertion (organi-
zation is an institution). Most predications can be
interpreted as category memberships like (i); ex-
plicit articulation of equivalence is actually quite
rare in language (Cimiano, 2006; Cimiano and
V¨olker, 2005). Although some categorical pred-
ications are metaphorical, many of these are inter-
preted using category matching or analogical map-
ping processes (Glucksberg et al., 1997; Bowdle
and Gentner, 2005). In both semantic interpreta-
tions, predications naturally form a directed net-
work of words. Consisting primarily of category
assertions, the structure of this network should ex-
hibit a degree of natural clustering owing natural
categories of the those things it represents.
Network representations of language have been
used to describe a wide range of structures in lan-
guage, including word-word and word-document
co-occurrences, term collocations, dependency
structure and named entity relations. Networks of
grammatical relations have been found to differ-
entiate word-classes (Ferrer i Cancho et al., 2004)
and semantic networks can be used to model vo-
cabulary growth (Steyvers and Tenenbaum, 2005).
Co-occurrence networks, which are perhaps the
most widely studied natural language network, are
the foundation of many vector-space models (Lan-
dauer and Dumais, 1997; Turney et al., 2010)
and can be used to mine synonyms (Cohen et
al., 2005), disambiguate word senses (Agirre et
</bodyText>
<page confidence="0.975208">
48
</page>
<subsubsectionHeader confidence="0.634477">
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55,
</subsubsectionHeader>
<bodyText confidence="0.991496152173913">
October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
al., 2014; Biemann, 2006) and even help mark
the quality of essays (Foltz et al., 1999). Spec-
tral methods applied to linguistic networks have
been used to differentiate languages (Ferrer i Can-
cho et al., 2004), word-classes (Sun and Korho-
nen, 2009) and genres of text (Ferrer i Cancho et
al., 2007). Using spectral methods, research has
also found that syntactic and semantic distribu-
tional similarity networks have considerably dif-
ferent structure (Biemann et al., 2009). The use
of lexical graphs (networks of words) in particu-
lar, pre-dates modern NLP (Rapoport et al., 1966),
though the approach continues to influence a va-
riety of NLP and information retrieval tasks like
summarization and retrieval (Erkan and Radev,
2004; V´eronis, 2004). Network-based methods
have even used community detection, similar to
the algorithm described in this paper, to extract
specialist terms from sets of multi-theme docu-
ments (Grineva et al., 2009) as well as unstruc-
tured texts (Gerow, 2014).
Because predications naturally form directed
chains of ontological assertions, we hypothesize
that their underlying structure is systematic and
modular, given its representation of naturally or-
ganized things in the world. Our method employs
community detection on networks of noun-noun
predications as a way to assess the overall struc-
ture of predication, but it could be extended to
hypernym and category extraction tasks (Hearst,
1992; Caraballo, 1999). Specifically, we test for
community structure in predications and explore
whether this structure becomes more highly re-
solved when using a semantic measure of predica-
tion strength introduced in the following section.
We also predict that central nodes (i.e. words)
in individual modules will correlate to categori-
cal super-ordinance or hypernymy. Thus, we first
seek to assess the overall community structure
of predication, testing whether or not it is more
resolved using a novel measure of predication
strength. Second, within communities of predi-
cations, we compare the words’ closeness central-
ity to their positions in WordNet’s hypernym tree
(Miller, 1995).
</bodyText>
<sectionHeader confidence="0.970661" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999978814814815">
Unlike co-occurrence networks, where words are
related simply by proximity, predication networks
are built using extracted grammatical relation-
ships. The implied relationship in a co-occurrence
network provides a natural way to weight edges,
but predications have no analogue to a proximity-
based weighting scheme. One option would be to
weight edges by the number of times given pred-
ications were observed. While this is perhaps the
most obvious way to account for important predi-
cations, it risks exaggerating high-frequency items
that are common for reasons other than importance
(perhaps they are idioms, collocates or found in
abnormally strong colligational structures). Fre-
quency weighting would also be susceptible to
noise from the many low-frequency items. To ad-
dress these concerns, we introduce a semantically
informed measure of predication relevance.
Wilks’ (1975; 1978) theory of preference se-
mantics proposes that subject- and object-verb re-
lationships evince “selectional preference”, which
can be thought of as the disposition verbs have
to select certain arguments – particular classes of
subjects or objects. To operationalize selectional
preference, Resnik (1997) introduced selectional
preference strength to measure the disposition or
“preference” of a verb, v:
</bodyText>
<equation confidence="0.995969">
� P (c|v)
SR(v) = P(c|v) log P (c) (1)
c∈C
</equation>
<bodyText confidence="0.9700448">
where C is a set of semantic classes from which
v can select and R is the grammatical relation in
question. Note that SR(v) is effectively the sum
K-L divergence between the probabilities of v and
c for all classes. In a corpus-based setting, the
probability of any word can be estimated by its
relative frequency: P(x) =r f (x). Resnik goes
Ei f (xi)
on to define a measure of selectional association
between a verb and a specific class, c:
</bodyText>
<equation confidence="0.995562">
1 P (c|v)
AR(v, c) = SR(v)P(c|v) log P (c) (2)
</equation>
<bodyText confidence="0.999958545454545">
In the typical form of selectional preference induc-
tion – the task of estimating likelihoods over all
classes – Eq. 2 is used to measure a verb’s pref-
erence for classes of nominal subjects or objects
like vehicles, insects, birds, etc. (Resnik, 1997;
Shutova et al., 2013).
To test our assumptions regarding the modu-
lar structure of predications in English, a mea-
sure like selectional association should account
for predicates’ diversity (or uniformity) of attach-
ment. That is, the preference a predicate has to
</bodyText>
<page confidence="0.996741">
49
</page>
<bodyText confidence="0.9999475">
operate on a wide or narrow range of words. To ac-
count for this, we add a term, U(p), to account for
the relative number of unique words a predicate p
has been observed to predicate. Note that this is
not the total number of predications involving p,
which would produce problematically high values
were p to collocate strongly with the words it pred-
icates. Instead, U(p) addresses and normalizes for
the diversity with which p is applied. Additionally,
instead of using a pre-set collection of semantic
classes on which predication is assumed to oper-
ate, each predicate is treated as its own class. For
a predication consisting of word w predicated by
p, predication strength is defined as follows:
</bodyText>
<equation confidence="0.998734">
1 P(wjp)
PS(w,p) = SR(p) log U(p)P(w) (3)
</equation>
<bodyText confidence="0.999697303030303">
PS thus combines three important properties of
predications: the relative frequency of a given
predication P(wjp), the relative frequency of a
word P(w) and the diversity of a word’s poten-
tial predications U(p). Defined like this, U(p)
helps diminish the contribution of predicates that
are widely applicable, under the assumption that
being widely used, they are in-fact somewhat less
significant. Using this measure to weight edges
in a predication network should help diminish
the contribution of exceptionally frequent predica-
tions as well as that from low-frequency predica-
tions without excluding them.
An example predication network is shown in
Figure 1. In these networks, a network is con-
structed over a set of documents where nodes are
the words in a predication, the direction follow-
ing the is-a link. Thus, example (i) would re-
sult in a link from safety to concern with weight
1. Were another predication involving concern to
be observed, another edge would be added from
that node. Note that circularities are allowed even
though this example is acyclic. To assess predi-
cation strength as a relevance function, we com-
pare the community structure of weighted and
unweighted networks. The example in Figure 1
shows a sample network (top) and the communi-
ties extracted from the unweighted and weighted
versions of the same network (middle and bottom).
Note the changes in community assignments from
the unweighted version to the weighted. In par-
ticular, observe the new clusters in the weighted
network around money, factor and murder. If our
</bodyText>
<figureCaption confidence="0.747954">
Figure 1: Predication network from the enTen-
</figureCaption>
<bodyText confidence="0.995592">
Ten corpus (pruned by frequency ~ 170): the ini-
tial network (top), communities assigned by the
Infomap algorithm for the unweighted network
(middle) and for the network weighted by predi-
cation strength (bottom).
</bodyText>
<page confidence="0.974772">
50
</page>
<bodyText confidence="0.972195117647059">
intuitions about the systematic nature of linguis-
tic predication is correct, there should be at least
a moderate degree of community structure in the
unweighted networks, and if predication displays
semantic preference similar to selectional associa-
tion, this community structure should be stronger
for networks weighted by predication strength.
The school librarian may be the person that controls [...]
You may find Rachel is the one person who may [...]
Neither the state nor its government is a person.
An arbitrator is a person who is appointed [...]
On the other hand, an expert is a person to fix [...]
After all, the vendor is the person best able to [...]
An expert need not be an individual person.
The innocent party is a natural person.
If the indemnifier is a natural person, [...]
consumers who are natural persons under the Directive.
</bodyText>
<tableCaption confidence="0.64401775">
Table 1: Sample predications involving forms of
the word person as the target in the BNC. In each
case an edge would connect the predicate (in ital-
ics) to person (in bold).
</tableCaption>
<sectionHeader confidence="0.999846" genericHeader="background">
3 Results
</sectionHeader>
<bodyText confidence="0.999966772727273">
To explore the structure of predication networks,
we analyzed two corpora using the method de-
scribed above: the British National Corpus (BNC)
(Leech et al., 2001) and the enTenTen web corpus.
Predications were extracted templates over a POS-
tagged version of each corpus using the Sketch En-
gine1 tool (Kilgarriff et al., 2004). The BNC con-
tained about 112 million tokens and the enTenTen
collection has 3.2 billion tokens. For each collec-
tion, the top 1,000 most frequent nouns provided
a seed set from which to extract all predicate
and predicate of relations2 (see examples in
Table 1). For the BNC, this resulted in 40,721
predications (14,319 unique) and 260,555 (20,651
unique) for the enTenTen collection. Predication
strength scores were computed for every pred-
ication using within-corpus relative frequencies.
These scores were used to weight edges in one
version of the predication network, whereas the
edge-weights of the “unweighted” version were
uniformly set to 1.0. No node-weighting was ap-
plied in either case.
</bodyText>
<footnote confidence="0.997968428571429">
1http://www.sketchengine.co.uk/
2&amp;quot;NN.?.?&amp;quot; [tag=&amp;quot;WP&amp;quot;|tag=&amp;quot;PNQ&amp;quot;|tag=&amp;quot;CJT&amp;quot;]
?[tag=&amp;quot;RB.?&amp;quot;|tag=&amp;quot;RB&amp;quot;|tag=&amp;quot;VM&amp;quot;]0,5
[lemma=&amp;quot;be&amp;quot; &amp; tag=&amp;quot;V.*&amp;quot;] &amp;quot;RB.?&amp;quot;0,2
[tag=&amp;quot;DT.?&amp;quot;|tag=&amp;quot;PP$&amp;quot;]0,1 &amp;quot;CD&amp;quot;0,2
[tag=&amp;quot;JJ.?&amp;quot;|tag=&amp;quot;RB.?&amp;quot;|word=&amp;quot;,&amp;quot;]0,3
&amp;quot;NN.?.?&amp;quot;0,2 2:&amp;quot;NN.?.?&amp;quot; [tag!=&amp;quot;NN.?.?&amp;quot;]
</footnote>
<bodyText confidence="0.999954717948718">
Two methods were used to extract communities
from the predication networks: the Infomap and
walktrap algorithms. By using two methods, we
attain some assurance that our findings are not ar-
tifacts of the assumptions underlying either algo-
rithm. The Infomap algorithm is an information-
theoretic method that exploits the analogue be-
tween optimizing a compression dictionary and
simplifying a graph by describing “flow” through
nodes (Rosvall and Bergstrom, 2008). Infomap as-
sumes edges in a network induce such flow and
by deriving a minimum description of this flow,
the algorithm can find multi-level communities
in large networks (Rosvall and Bergstrom, 2011).
The second method, walktrap, operationalizes the
intuition that a large set of short random walks on
a network will leave walkers on some groups of
nodes more often than others (Pons and Latapy,
2005). By setting the walk distance to a small
value, relative to a network’s density, walkers will
tend toward communities if the walker sample is
sufficient. These algorithms are both known to
work well with large, directed networks and nei-
ther imposed intractable computational burdens at
our scale (Fortunato, 2010; Lancichinetti and For-
tunato, 2009). Because both algorithms require a
connected network, our analysis is restricted to the
largest connected component (LCC) for all net-
works, though we have no reason to believe results
would differ significantly for other components.
Community assignments can be assessed by
measuring how self-contained or “modular” the
resulting communities are. Modularity was intro-
duced as a way to choose the level of an optimal
cut for hierarchical partitioning algorithms, analo-
gous to the level in the dendrogram that yields the
best communities (Newman and Girvan, 2004).
For a network with adjacency matrix A and com-
munity assignments c, modularity is defined as:
</bodyText>
<equation confidence="0.831957333333333">
_ 1 kikj
Q 2m Aij − δ(ci, cj) (4)
ij 2m
</equation>
<bodyText confidence="0.999867444444444">
where m is the number of edges and ki is the
degree of node i. δ(ci, cj) is 1 when the com-
munity assignment of node i is the same as that
for node j. Modularity measures how likely it is
that nodes in a community are connected to one
another as opposed to nodes in other communi-
ties. Modularity is defined from -1.0 to 1.0 and
graphs where Q &gt; 0.6 are conventionally said to
have relatively strong community structure (New-
</bodyText>
<page confidence="0.996802">
51
</page>
<bodyText confidence="0.984593185714286">
man, 2010). Here, we use modularity instead of
a measure of semantic similarity or semantic co-
herence because predication is seldom an asser-
tion of equivalence or similarity. This means that
although words in predication communities may
be related in an ontological sense, such an assess-
ment would not expose the level of independence
between the communities.
Weighted and unweighted networks from both
corpora were submitted to each community detec-
tion algorithm, the results of which were assessed
using modularity. We also carried out this anal-
ysis on frequency-weighted networks, the results
of which were similar to the unweighted config-
uration, but are not reported for sake of brevity.
Figure 2 shows the modularity for each config-
uration with varying minimum predication fre-
quency (the number of times a predication had
to occur to be included). Varying the minimum
frequency thresholds helps simulate the effect of
corpus-size on the algorithm. In the BNC, un-
weighted networks with no minimum edge fre-
quency show slight modularity (Q = 0.30),
whereas in weighted networks it is quite strong
(Q = 0.89). The enTenTen corpus exhibits a
gap between the unweighted (Q = 0.61) and
weighted networks (Q = 0.88) at low edge thresh-
olds. This shows that predication strength is help-
ful in weighting relevant items without exclud-
ing low-frequency observations. The lower mod-
ularity scores (Q; Eq. 4) in the unweighted net-
works may be due to more novel, loose or figura-
tive associations found in low-frequency predica-
tions that inappropriately connect unrelated com-
munities. Interestingly, scores for unweighted and
weighted networks converge up to a point as the
minimum frequency increases (reducing the size
of the network). This pruning is helpful for the
unweighted networks, but has little effect on the
weighted versions. In all cases, sparsity takes a
toll as the LCC becomes quite small. The reason
for the eventual decline as the LCC shrinks below
70 nodes is because communities are less likely to
form at all in small networks.
In addition to the highly modular structure, the
communities of predications themselves are likely
to represent some semantic organization. Specifi-
cally, we looked for a categorical structure within
the communities by comparing words to the hy-
pernym tree in WordNet (Miller, 1995). Intu-
itively, one would expect words that are central in
Figure 2: Modularity of predication networks in
the BNC (top) and enTenTen (bottom). Note, as
the minimum frequency increases (bottom axis)
and the LCC contains fewer and fewer nodes (top
axis), the community detection algorithms may
not produce a solution with more than one com-
munity, resulting in undefined modularity.
a community to be members of higher-level cate-
gories. In figure 1, for example, summer, hour and
holiday all point to time, one could infer that time
is a shared hypernym. We use closeness centrality,
a graph-theoretic measure of node’s average prox-
imity to other nodes, as a within-community mea-
sure of super-ordinance (i.e. hypernymy). Though
there a number of network centrality measures,
closeness centrality is a robust measure, though it
tends not to scale well to larger networks because
it requires computing the distance between every
pair of nodes (Friedl et al., 2010).
</bodyText>
<page confidence="0.9965">
52
</page>
<bodyText confidence="0.999974666666667">
The centrality scores in the communities were
compared to WordNet using the first sense-entry
for each node (which is typically the most com-
mon) and words not found in the tree were dis-
carded. For the unweighted networks across both
corpora, we found a mean Spearman correlation of
r=0.35 (p &lt; 0.01; using Fisher’s transformation)
for the Infomap algorithm and r=0.38 (p &lt; 0.01)
for walktrap. In the weighted versions, Infomap
produced r=0.41 (p &lt; 0.01) and walktrap pro-
duced r=0.44 (p &lt; 0.01). This confirms that
predication communities tend to specify categor-
ical knowledge is moderately similar to WordNet.
Note these correlation values are comparable be-
tween the weighted and unweighted networks, im-
plying that relevance, as selectional association, is
not an important marker of the communities’ hy-
pernymic composition.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="discussions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999982085365854">
The analysis in this paper is an attempt to iden-
tify whether or not ontological knowledge ex-
pressed in text consists of meaningful clusters.
With the network representation and our measure
of predication strength, results indicate that pred-
ication forms strong community structures. Over-
all, results point to the highly modular nature of
predication, previously unreported in language.
This confirms our prediction that predication com-
prises systematic clusters of related things and the
higher modularity observed in networks weighted
by predication strength implies that predication
exhibits a form of selectional preference. Predi-
cation’s strong community structure is important
because it supports the use of linguistic patterns
in establishing ontological representations, which
naturally form higher-level groups.
Technically, our measure of predication
strength, which is built on prior assessments of
selectional preference, identifies the modular
semantic structure of predication even when
low frequency predications are included. This
may be because low-frequency predications are
more likely to inscribe novel, loose or figurative
associations that reach between semantic clusters
to inappropriately decrease the overall modularity
if not down-weighted. As a result, more sys-
tematic comparison of weighted and unweighted
networks, and the relative location of predica-
tion within these structures, will reveal where
semantic innovation and figurative assertions are
most likely to occur. The predication networks
analyzed rely on a relatively tight definition of
predication, one that, in other languages, may not
be accessible by the copular form. Additionally,
the two literal interpretations of linguistic predica-
tions, equivalence or category membership, may
also not be common in all languages. To the extent
that parsers or taggers are available, a comparative
analysis would broaden the understanding of
predication in general.
Given their high modularity, predication struc-
tures could be exploited further for a number of
NLP tasks. The correlations between centrality
and hypernym depth mean that predication net-
works could help construct or update categorical
taxonomies. For example, these networks could
help automate the construction of a hypernym tax-
onomy with weighted branches, potentially aug-
menting resources like WordNet (Ruiz-Casado et
al., 2005; Miller, 1995). One could also ex-
amine the growth, combination and bifurcation
of specific communities to help track ontological
commitments, either over time as shifts in lan-
guage structures (Gerow and Ahmad, 2012), or
across genre and domain (Davies, 2010). Fur-
ther, because predication encodes categorical in-
formation, its community structure may also en-
code higher-level relations where strong inter-
community links imply relationships between
classes of objects.
Our study examined the topographical struc-
ture of English predications in general, struc-
ture that consists, in large part, of hypernym re-
lations. Though the relations in the examined
networks are defined by copular is-a predica-
tion structure, within-community hierarchies cor-
related only moderately with the hypernym hierar-
chy in WordNet. This implies that the predications
comprising our networks are either not entirely
hypernymic or that WordNet is not a good base-
line. Indeed, predication is a grammatical rela-
tionship that often asserts synonymy or figurative
hypernymy (perhaps sometimes also metonymy)
and it is not apparent from the surface structure
how these semantic interpretation could be disam-
biguated. One reason this correlation is not higher
is likely to do with the low coverage of the copular
form as evidence of hypernymy (Hearst, 1992).
Further work regarding the structure of predi-
cations could build on the network framework to
evaluate the communities themselves. What prop-
</bodyText>
<page confidence="0.997085">
53
</page>
<bodyText confidence="0.999959823529412">
erties differentiate communities? Are there se-
mantic, lexical or statistical properties that con-
tribute to the formation of communities? Are there
discernible differences between words that typify
communities as opposed to those that bridge com-
munities? Predication communities are primar-
ily semantic in nature, implying that central nodes
would typify meaningful aspects of their commu-
nity. It would also be relatively easy to extend
network representations to address more qualita-
tive aspects such as coherence, word norms and
word associations. Indeed, a variety of corpus-
based research could employ network-based meth-
ods like those exemplified in this paper, capitaliz-
ing on graph-theory, social network analysis and
statistical physics, without departing from rela-
tional structures inherent to language.
</bodyText>
<sectionHeader confidence="0.998579" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982272">
This work was supported by a grant from the
Templeton Foundation to the Metaknowledge Re-
search Network and grant #1158803 from the Na-
tional Science Foundation.
</bodyText>
<sectionHeader confidence="0.996189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9782705375">
Eneko Agirre, Oier Lpez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.
Chris Biemann, Monojit Choudhury, and Animesh
Mukherjee. 2009. Syntax is from mars while se-
mantics from venus!: insights from spectral analysis
of distributional similarity networks. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 245–248.
Chris Biemann. 2006. Chinese whispers: an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of the first workshop on graph based methods for
natural language processing, pages 73–80.
Brian F. Bowdle and Dedre Gentner. 2005. The career
of metaphor. Psychological Review, 112(1):193.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics, pages 120–126.
Philipp Cimiano and Johanna V¨olker. 2005.
Text2onto. In Natural language processing and in-
formation systems, pages 227–238. Springer.
Philipp Cimiano. 2006. Ontology learning from text.
Springer.
Aaron M. Cohen, William R. Hersh, Christopher
Dubay, and K. Spackman. 2005. Using co-
occurrence network structure to extract synonymous
gene and protein names from medline abstracts.
BMC Bioinformatics, 6(1):103.
Mark Davies. 2010. The corpus of contemporary
american english as the first reliable monitor cor-
pus of english. Literary and linguistic computing,
25(4):447–464.
G¨unes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22(1):457–479.
Ramon Ferrer i Cancho, Ricard V Sol´e, and Reinhard
K¨ohler. 2004. Patterns in syntactic dependency net-
works. Physical Review E, 69(5):051915.
Ramon Ferrer i Cancho, Andrea Capocci, and Guido
Caldarelli. 2007. Spectral methods cluster words
of the same class in a syntactic dependency net-
work. International Journal of Bifurcation and
Chaos, 17(07):2453–2463.
Peter W. Foltz, Darrell Laham, and Thomas K. Lan-
dauer. 1999. Automated essay scoring: Applica-
tions to educational technology. In World Confer-
ence on Educational Multimedia, Hypermedia and
Telecommunications, pages 939–944.
Santo Fortunato. 2010. Community detection in
graphs. Physics Reports, 486(3):75–174.
Dipl-Math Bettina Friedl, Julia Heidemann, et al.
2010. A critical review of centrality measures in
social networks. Business &amp; Information Systems
Engineering, 2(6):371–385.
Aaron Gerow and Khurshid Ahmad. 2012. Diachronic
variation in grammatical relationships. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics (COLING 2012).
Aaron Gerow. 2014. Extracting clusters of special-
ist terms from unstructured text. In Proceedings of
2014 Conference on Empirical Methods in Natural
Language Processing (forthcoming).
Sam Glucksberg, Matthew S. McGlone, and Deanna
Manfredi. 1997. Property attribution in metaphor
comprehension. Journal of memory and language,
36(1):50–67.
Maria Grineva, Maxim Grinev, and Dmitry Lizorkin.
2009. Extracting key terms from noisy and multi-
theme documents. In Proceedings of the 18th inter-
national conference on Worldwide web, pages 661–
670.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th conference on computational linguistics-
Volume 2, pages 539–545.
</reference>
<page confidence="0.978378">
54
</page>
<reference confidence="0.999882647887324">
Adam Kilgarriff, Pavel Rychl, Pavel Smr, and David
Tugwell. 2004. The sketch engine. In Proceedings
of EURALEX 2004, pages 105–116.
Andrea Lancichinetti and Santo Fortunato. 2009.
Community detection algorithms: a comparative
analysis. Physical Review E, 80(5):056117.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to plato’s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104(2):211.
Geoffrey Leech, Paul Rayson, and Andrew Wilson.
2001. Word frequencies in written and spoken En-
glish: based on the British National Corpus. Long-
man.
George A. Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Mark E. J. Newman and M. Girvan. 2004. Finding and
evaluating community structure in networks. Physi-
cal Review E, 69(2):026113.
Mark E. J. Newman. 2010. Networks: an introduction.
Oxford University Press.
Pascal Pons and Matthieu Latapy. 2005. Computing
communities in large networks using random walks.
In Computer and Information Sciences-ISCIS 2005,
pages 284–293. Springer.
Anatol Rapoport, Amnon Rapoport, William P Livant,
and John Boyd. 1966. A study of lexical graphs.
Foundations of Language, pages 338–376.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52–57.
Martin Rosvall and Carl T. Bergstrom. 2008. Maps of
random walks on complex networks reveal commu-
nity structure. Proceedings of the National Academy
of Sciences, 105(4):1118–1123.
Martin Rosvall and Carl T. Bergstrom. 2011. Mul-
tilevel compression of random walks on networks
reveals hierarchical organization in large integrated
systems. PloS one, 6(4):e18209.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic extraction of semantic
relationships for wordnet by means of pattern learn-
ing from wikipedia. In Natural Language Process-
ing and Information Systems, pages 67–79. Springer.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301–353.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The
large-scale structure of semantic networks: Statisti-
cal analyses and a model of semantic growth. Cog-
nitive Science, 29(1):41–78.
Lin Sun and Anna Korhonen. 2009. Improving verb
clustering with automatically acquired selectional
preferences. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 638–647.
Peter D. Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.
Jean V´eronis. 2004. Hyperlex: lexical cartography
for information retrieval. Computer Speech &amp; Lan-
guage, 18(3):223–252.
Yorick Wilks. 1975. A preferential, pattern-seeking,
semantics for natural language inference. Artificial
Intelligence, 6(1):53–74.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197–223.
</reference>
<page confidence="0.999064">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.492920">
<title confidence="0.999866">The Modular Community Structure of Linguistic Predication Networks</title>
<author confidence="0.988404">Aaron</author>
<affiliation confidence="0.920208">Computation University of Chicago, IL,</affiliation>
<email confidence="0.999353">gerow@uchicago.edu</email>
<author confidence="0.940469">James</author>
<affiliation confidence="0.902479333333333">Dept. of Sociology &amp; Computation University of Chicago, IL,</affiliation>
<email confidence="0.999663">jevans@uchicago.edu</email>
<abstract confidence="0.999128222222222">This paper examines the structure of linguistic predications in English text. Identified by the copular “is-a” form, predications assert category membership (hypernymy) or equivalence (synonymy) between two words. Because predication expresses ontological structure, we hypothesize that networks of predications will form modular groups. To measure this, we introduce a semantically momeasure of strength to weight relevant predications observed in text. Results show that predications do indeed form modular structures withany weighting and that using predication strength increases this without discarding low-frequency items. This level of modularity supports the networkbased analysis and the use of predication strength as a way to extract dense semantic clusters. Additionally, words’ centrality within communities exhibits slight correlation with hypernym depths in WordNet, underscoring the ontological organization of predication.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lpez de Lacalle, and Aitor Soroa.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Agirre, 2014</marker>
<rawString>Eneko Agirre, Oier Lpez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Monojit Choudhury</author>
<author>Animesh Mukherjee</author>
</authors>
<title>Syntax is from mars while semantics from venus!: insights from spectral analysis of distributional similarity networks.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>245--248</pages>
<contexts>
<context position="4409" citStr="Biemann et al., 2009" startWordPosition="646" endWordPosition="649"> on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarization and retrieval (Erkan and Radev, 2004; V´eronis, 2004). Network-based methods have even used community detection, similar to the algorithm described in this paper, to extract specialist terms from sets of multi-theme documents (Grineva et al., 2009) as well as unstructured texts (Gerow, 2014). Because predications naturally form directed chains of ontological assertions, we hypo</context>
</contexts>
<marker>Biemann, Choudhury, Mukherjee, 2009</marker>
<rawString>Chris Biemann, Monojit Choudhury, and Animesh Mukherjee. 2009. Syntax is from mars while semantics from venus!: insights from spectral analysis of distributional similarity networks. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems.</title>
<date>2006</date>
<booktitle>In Proceedings of the</booktitle>
<pages>73--80</pages>
<contexts>
<context position="3963" citStr="Biemann, 2006" startWordPosition="576" endWordPosition="577">i Cancho et al., 2004) and semantic networks can be used to model vocabulary growth (Steyvers and Tenenbaum, 2005). Co-occurrence networks, which are perhaps the most widely studied natural language network, are the foundation of many vector-space models (Landauer and Dumais, 1997; Turney et al., 2010) and can be used to mine synonyms (Cohen et al., 2005), disambiguate word senses (Agirre et 48 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a </context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese whispers: an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the first workshop on graph based methods for natural language processing, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian F Bowdle</author>
<author>Dedre Gentner</author>
</authors>
<title>The career of metaphor.</title>
<date>2005</date>
<journal>Psychological Review,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="2762" citStr="Bowdle and Gentner, 2005" startWordPosition="398" endWordPosition="401">nd (ii) for example: (i) Safety is always a primary concern. (ii) This organization is an institution where [...]. (i) is a category assertion (safety as a type of concern) and (ii) is an equivalence assertion (organization is an institution). Most predications can be interpreted as category memberships like (i); explicit articulation of equivalence is actually quite rare in language (Cimiano, 2006; Cimiano and V¨olker, 2005). Although some categorical predications are metaphorical, many of these are interpreted using category matching or analogical mapping processes (Glucksberg et al., 1997; Bowdle and Gentner, 2005). In both semantic interpretations, predications naturally form a directed network of words. Consisting primarily of category assertions, the structure of this network should exhibit a degree of natural clustering owing natural categories of the those things it represents. Network representations of language have been used to describe a wide range of structures in language, including word-word and word-document co-occurrences, term collocations, dependency structure and named entity relations. Networks of grammatical relations have been found to differentiate word-classes (Ferrer i Cancho et a</context>
</contexts>
<marker>Bowdle, Gentner, 2005</marker>
<rawString>Brian F. Bowdle and Dedre Gentner. 2005. The career of metaphor. Psychological Review, 112(1):193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>120--126</pages>
<contexts>
<context position="5377" citStr="Caraballo, 1999" startWordPosition="794" endWordPosition="795">rithm described in this paper, to extract specialist terms from sets of multi-theme documents (Grineva et al., 2009) as well as unstructured texts (Gerow, 2014). Because predications naturally form directed chains of ontological assertions, we hypothesize that their underlying structure is systematic and modular, given its representation of naturally organized things in the world. Our method employs community detection on networks of noun-noun predications as a way to assess the overall structure of predication, but it could be extended to hypernym and category extraction tasks (Hearst, 1992; Caraballo, 1999). Specifically, we test for community structure in predications and explore whether this structure becomes more highly resolved when using a semantic measure of predication strength introduced in the following section. We also predict that central nodes (i.e. words) in individual modules will correlate to categorical super-ordinance or hypernymy. Thus, we first seek to assess the overall community structure of predication, testing whether or not it is more resolved using a novel measure of predication strength. Second, within communities of predications, we compare the words’ closeness central</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 120–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Johanna V¨olker</author>
</authors>
<date>2005</date>
<booktitle>Text2onto. In Natural language processing and information systems,</booktitle>
<pages>227--238</pages>
<publisher>Springer.</publisher>
<marker>Cimiano, V¨olker, 2005</marker>
<rawString>Philipp Cimiano and Johanna V¨olker. 2005. Text2onto. In Natural language processing and information systems, pages 227–238. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
</authors>
<title>Ontology learning from text.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2538" citStr="Cimiano, 2006" startWordPosition="367" endWordPosition="368">rved as the copular is-a form in English, are important because, unlike most grammatical constructions that have few semantic constraints, predications tend to imply category membership or equivalence. Take (i) and (ii) for example: (i) Safety is always a primary concern. (ii) This organization is an institution where [...]. (i) is a category assertion (safety as a type of concern) and (ii) is an equivalence assertion (organization is an institution). Most predications can be interpreted as category memberships like (i); explicit articulation of equivalence is actually quite rare in language (Cimiano, 2006; Cimiano and V¨olker, 2005). Although some categorical predications are metaphorical, many of these are interpreted using category matching or analogical mapping processes (Glucksberg et al., 1997; Bowdle and Gentner, 2005). In both semantic interpretations, predications naturally form a directed network of words. Consisting primarily of category assertions, the structure of this network should exhibit a degree of natural clustering owing natural categories of the those things it represents. Network representations of language have been used to describe a wide range of structures in language,</context>
</contexts>
<marker>Cimiano, 2006</marker>
<rawString>Philipp Cimiano. 2006. Ontology learning from text. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron M Cohen</author>
<author>William R Hersh</author>
<author>Christopher Dubay</author>
<author>K Spackman</author>
</authors>
<title>Using cooccurrence network structure to extract synonymous gene and protein names from medline abstracts.</title>
<date>2005</date>
<journal>BMC Bioinformatics,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="3706" citStr="Cohen et al., 2005" startWordPosition="540" endWordPosition="543">to describe a wide range of structures in language, including word-word and word-document co-occurrences, term collocations, dependency structure and named entity relations. Networks of grammatical relations have been found to differentiate word-classes (Ferrer i Cancho et al., 2004) and semantic networks can be used to model vocabulary growth (Steyvers and Tenenbaum, 2005). Co-occurrence networks, which are perhaps the most widely studied natural language network, are the foundation of many vector-space models (Landauer and Dumais, 1997; Turney et al., 2010) and can be used to mine synonyms (Cohen et al., 2005), disambiguate word senses (Agirre et 48 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and s</context>
</contexts>
<marker>Cohen, Hersh, Dubay, Spackman, 2005</marker>
<rawString>Aaron M. Cohen, William R. Hersh, Christopher Dubay, and K. Spackman. 2005. Using cooccurrence network structure to extract synonymous gene and protein names from medline abstracts. BMC Bioinformatics, 6(1):103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davies</author>
</authors>
<title>The corpus of contemporary american english as the first reliable monitor corpus of english. Literary and linguistic computing,</title>
<date>2010</date>
<pages>25--4</pages>
<contexts>
<context position="22333" citStr="Davies, 2010" startWordPosition="3469" endWordPosition="3470">er for a number of NLP tasks. The correlations between centrality and hypernym depth mean that predication networks could help construct or update categorical taxonomies. For example, these networks could help automate the construction of a hypernym taxonomy with weighted branches, potentially augmenting resources like WordNet (Ruiz-Casado et al., 2005; Miller, 1995). One could also examine the growth, combination and bifurcation of specific communities to help track ontological commitments, either over time as shifts in language structures (Gerow and Ahmad, 2012), or across genre and domain (Davies, 2010). Further, because predication encodes categorical information, its community structure may also encode higher-level relations where strong intercommunity links imply relationships between classes of objects. Our study examined the topographical structure of English predications in general, structure that consists, in large part, of hypernym relations. Though the relations in the examined networks are defined by copular is-a predication structure, within-community hierarchies correlated only moderately with the hypernym hierarchy in WordNet. This implies that the predications comprising our ne</context>
</contexts>
<marker>Davies, 2010</marker>
<rawString>Mark Davies. 2010. The corpus of contemporary american english as the first reliable monitor corpus of english. Literary and linguistic computing, 25(4):447–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="4665" citStr="Erkan and Radev, 2004" startWordPosition="687" endWordPosition="690">applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarization and retrieval (Erkan and Radev, 2004; V´eronis, 2004). Network-based methods have even used community detection, similar to the algorithm described in this paper, to extract specialist terms from sets of multi-theme documents (Grineva et al., 2009) as well as unstructured texts (Gerow, 2014). Because predications naturally form directed chains of ontological assertions, we hypothesize that their underlying structure is systematic and modular, given its representation of naturally organized things in the world. Our method employs community detection on networks of noun-noun predications as a way to assess the overall structure of</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ferrer i Cancho</author>
<author>Ricard V Sol´e</author>
<author>Reinhard K¨ohler</author>
</authors>
<title>Patterns in syntactic dependency networks. Physical Review E,</title>
<date>2004</date>
<marker>Cancho, Sol´e, K¨ohler, 2004</marker>
<rawString>Ramon Ferrer i Cancho, Ricard V Sol´e, and Reinhard K¨ohler. 2004. Patterns in syntactic dependency networks. Physical Review E, 69(5):051915.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ferrer i Cancho</author>
<author>Andrea Capocci</author>
<author>Guido Caldarelli</author>
</authors>
<title>Spectral methods cluster words of the same class in a syntactic dependency network.</title>
<date>2007</date>
<journal>International Journal of Bifurcation and Chaos,</journal>
<volume>17</volume>
<issue>07</issue>
<contexts>
<context position="4236" citStr="Cancho et al., 2007" startWordPosition="622" endWordPosition="625">d Dumais, 1997; Turney et al., 2010) and can be used to mine synonyms (Cohen et al., 2005), disambiguate word senses (Agirre et 48 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarization and retrieval (Erkan and Radev, 2004; V´eronis, 2004). Network-based methods have even used community detection, similar to the algorithm described in this paper, to extract specialist terms from sets of mul</context>
</contexts>
<marker>Cancho, Capocci, Caldarelli, 2007</marker>
<rawString>Ramon Ferrer i Cancho, Andrea Capocci, and Guido Caldarelli. 2007. Spectral methods cluster words of the same class in a syntactic dependency network. International Journal of Bifurcation and Chaos, 17(07):2453–2463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automated essay scoring: Applications to educational technology.</title>
<date>1999</date>
<booktitle>In World Conference on Educational Multimedia, Hypermedia and Telecommunications,</booktitle>
<pages>939--944</pages>
<contexts>
<context position="4025" citStr="Foltz et al., 1999" startWordPosition="586" endWordPosition="589">to model vocabulary growth (Steyvers and Tenenbaum, 2005). Co-occurrence networks, which are perhaps the most widely studied natural language network, are the foundation of many vector-space models (Landauer and Dumais, 1997; Turney et al., 2010) and can be used to mine synonyms (Cohen et al., 2005), disambiguate word senses (Agirre et 48 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarizat</context>
</contexts>
<marker>Foltz, Laham, Landauer, 1999</marker>
<rawString>Peter W. Foltz, Darrell Laham, and Thomas K. Landauer. 1999. Automated essay scoring: Applications to educational technology. In World Conference on Educational Multimedia, Hypermedia and Telecommunications, pages 939–944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Santo Fortunato</author>
</authors>
<title>Community detection in graphs.</title>
<date>2010</date>
<journal>Physics Reports,</journal>
<volume>486</volume>
<issue>3</issue>
<contexts>
<context position="14420" citStr="Fortunato, 2010" startWordPosition="2227" endWordPosition="2228">flow, the algorithm can find multi-level communities in large networks (Rosvall and Bergstrom, 2011). The second method, walktrap, operationalizes the intuition that a large set of short random walks on a network will leave walkers on some groups of nodes more often than others (Pons and Latapy, 2005). By setting the walk distance to a small value, relative to a network’s density, walkers will tend toward communities if the walker sample is sufficient. These algorithms are both known to work well with large, directed networks and neither imposed intractable computational burdens at our scale (Fortunato, 2010; Lancichinetti and Fortunato, 2009). Because both algorithms require a connected network, our analysis is restricted to the largest connected component (LCC) for all networks, though we have no reason to believe results would differ significantly for other components. Community assignments can be assessed by measuring how self-contained or “modular” the resulting communities are. Modularity was introduced as a way to choose the level of an optimal cut for hierarchical partitioning algorithms, analogous to the level in the dendrogram that yields the best communities (Newman and Girvan, 2004). </context>
</contexts>
<marker>Fortunato, 2010</marker>
<rawString>Santo Fortunato. 2010. Community detection in graphs. Physics Reports, 486(3):75–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipl-Math Bettina Friedl</author>
<author>Julia Heidemann</author>
</authors>
<title>A critical review of centrality measures in social networks.</title>
<date>2010</date>
<journal>Business &amp; Information Systems Engineering,</journal>
<volume>2</volume>
<issue>6</issue>
<marker>Friedl, Heidemann, 2010</marker>
<rawString>Dipl-Math Bettina Friedl, Julia Heidemann, et al. 2010. A critical review of centrality measures in social networks. Business &amp; Information Systems Engineering, 2(6):371–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Gerow</author>
<author>Khurshid Ahmad</author>
</authors>
<title>Diachronic variation in grammatical relationships.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="22290" citStr="Gerow and Ahmad, 2012" startWordPosition="3460" endWordPosition="3463">ity, predication structures could be exploited further for a number of NLP tasks. The correlations between centrality and hypernym depth mean that predication networks could help construct or update categorical taxonomies. For example, these networks could help automate the construction of a hypernym taxonomy with weighted branches, potentially augmenting resources like WordNet (Ruiz-Casado et al., 2005; Miller, 1995). One could also examine the growth, combination and bifurcation of specific communities to help track ontological commitments, either over time as shifts in language structures (Gerow and Ahmad, 2012), or across genre and domain (Davies, 2010). Further, because predication encodes categorical information, its community structure may also encode higher-level relations where strong intercommunity links imply relationships between classes of objects. Our study examined the topographical structure of English predications in general, structure that consists, in large part, of hypernym relations. Though the relations in the examined networks are defined by copular is-a predication structure, within-community hierarchies correlated only moderately with the hypernym hierarchy in WordNet. This impl</context>
</contexts>
<marker>Gerow, Ahmad, 2012</marker>
<rawString>Aaron Gerow and Khurshid Ahmad. 2012. Diachronic variation in grammatical relationships. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Gerow</author>
</authors>
<title>Extracting clusters of specialist terms from unstructured text.</title>
<date>2014</date>
<booktitle>In Proceedings of 2014 Conference on Empirical Methods in Natural Language Processing (forthcoming).</booktitle>
<contexts>
<context position="4921" citStr="Gerow, 2014" startWordPosition="728" endWordPosition="729">ntic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarization and retrieval (Erkan and Radev, 2004; V´eronis, 2004). Network-based methods have even used community detection, similar to the algorithm described in this paper, to extract specialist terms from sets of multi-theme documents (Grineva et al., 2009) as well as unstructured texts (Gerow, 2014). Because predications naturally form directed chains of ontological assertions, we hypothesize that their underlying structure is systematic and modular, given its representation of naturally organized things in the world. Our method employs community detection on networks of noun-noun predications as a way to assess the overall structure of predication, but it could be extended to hypernym and category extraction tasks (Hearst, 1992; Caraballo, 1999). Specifically, we test for community structure in predications and explore whether this structure becomes more highly resolved when using a sem</context>
</contexts>
<marker>Gerow, 2014</marker>
<rawString>Aaron Gerow. 2014. Extracting clusters of specialist terms from unstructured text. In Proceedings of 2014 Conference on Empirical Methods in Natural Language Processing (forthcoming).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Glucksberg</author>
<author>Matthew S McGlone</author>
<author>Deanna Manfredi</author>
</authors>
<title>Property attribution in metaphor comprehension.</title>
<date>1997</date>
<journal>Journal of memory and language,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="2735" citStr="Glucksberg et al., 1997" startWordPosition="394" endWordPosition="397">r equivalence. Take (i) and (ii) for example: (i) Safety is always a primary concern. (ii) This organization is an institution where [...]. (i) is a category assertion (safety as a type of concern) and (ii) is an equivalence assertion (organization is an institution). Most predications can be interpreted as category memberships like (i); explicit articulation of equivalence is actually quite rare in language (Cimiano, 2006; Cimiano and V¨olker, 2005). Although some categorical predications are metaphorical, many of these are interpreted using category matching or analogical mapping processes (Glucksberg et al., 1997; Bowdle and Gentner, 2005). In both semantic interpretations, predications naturally form a directed network of words. Consisting primarily of category assertions, the structure of this network should exhibit a degree of natural clustering owing natural categories of the those things it represents. Network representations of language have been used to describe a wide range of structures in language, including word-word and word-document co-occurrences, term collocations, dependency structure and named entity relations. Networks of grammatical relations have been found to differentiate word-cl</context>
</contexts>
<marker>Glucksberg, McGlone, Manfredi, 1997</marker>
<rawString>Sam Glucksberg, Matthew S. McGlone, and Deanna Manfredi. 1997. Property attribution in metaphor comprehension. Journal of memory and language, 36(1):50–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Grineva</author>
<author>Maxim Grinev</author>
<author>Dmitry Lizorkin</author>
</authors>
<title>Extracting key terms from noisy and multitheme documents.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on Worldwide web,</booktitle>
<pages>661--670</pages>
<contexts>
<context position="4877" citStr="Grineva et al., 2009" startWordPosition="718" endWordPosition="721">hods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarization and retrieval (Erkan and Radev, 2004; V´eronis, 2004). Network-based methods have even used community detection, similar to the algorithm described in this paper, to extract specialist terms from sets of multi-theme documents (Grineva et al., 2009) as well as unstructured texts (Gerow, 2014). Because predications naturally form directed chains of ontological assertions, we hypothesize that their underlying structure is systematic and modular, given its representation of naturally organized things in the world. Our method employs community detection on networks of noun-noun predications as a way to assess the overall structure of predication, but it could be extended to hypernym and category extraction tasks (Hearst, 1992; Caraballo, 1999). Specifically, we test for community structure in predications and explore whether this structure b</context>
</contexts>
<marker>Grineva, Grinev, Lizorkin, 2009</marker>
<rawString>Maria Grineva, Maxim Grinev, and Dmitry Lizorkin. 2009. Extracting key terms from noisy and multitheme documents. In Proceedings of the 18th international conference on Worldwide web, pages 661– 670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on computational linguisticsVolume 2,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="5359" citStr="Hearst, 1992" startWordPosition="792" endWordPosition="793">ar to the algorithm described in this paper, to extract specialist terms from sets of multi-theme documents (Grineva et al., 2009) as well as unstructured texts (Gerow, 2014). Because predications naturally form directed chains of ontological assertions, we hypothesize that their underlying structure is systematic and modular, given its representation of naturally organized things in the world. Our method employs community detection on networks of noun-noun predications as a way to assess the overall structure of predication, but it could be extended to hypernym and category extraction tasks (Hearst, 1992; Caraballo, 1999). Specifically, we test for community structure in predications and explore whether this structure becomes more highly resolved when using a semantic measure of predication strength introduced in the following section. We also predict that central nodes (i.e. words) in individual modules will correlate to categorical super-ordinance or hypernymy. Thus, we first seek to assess the overall community structure of predication, testing whether or not it is more resolved using a novel measure of predication strength. Second, within communities of predications, we compare the words’</context>
<context position="23398" citStr="Hearst, 1992" startWordPosition="3630" endWordPosition="3631">within-community hierarchies correlated only moderately with the hypernym hierarchy in WordNet. This implies that the predications comprising our networks are either not entirely hypernymic or that WordNet is not a good baseline. Indeed, predication is a grammatical relationship that often asserts synonymy or figurative hypernymy (perhaps sometimes also metonymy) and it is not apparent from the surface structure how these semantic interpretation could be disambiguated. One reason this correlation is not higher is likely to do with the low coverage of the copular form as evidence of hypernymy (Hearst, 1992). Further work regarding the structure of predications could build on the network framework to evaluate the communities themselves. What prop53 erties differentiate communities? Are there semantic, lexical or statistical properties that contribute to the formation of communities? Are there discernible differences between words that typify communities as opposed to those that bridge communities? Predication communities are primarily semantic in nature, implying that central nodes would typify meaningful aspects of their community. It would also be relatively easy to extend network representatio</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on computational linguisticsVolume 2, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Pavel Rychl</author>
<author>Pavel Smr</author>
<author>David Tugwell</author>
</authors>
<title>The sketch engine.</title>
<date>2004</date>
<booktitle>In Proceedings of EURALEX</booktitle>
<pages>105--116</pages>
<contexts>
<context position="12294" citStr="Kilgarriff et al., 2004" startWordPosition="1917" endWordPosition="1920">atural person. If the indemnifier is a natural person, [...] consumers who are natural persons under the Directive. Table 1: Sample predications involving forms of the word person as the target in the BNC. In each case an edge would connect the predicate (in italics) to person (in bold). 3 Results To explore the structure of predication networks, we analyzed two corpora using the method described above: the British National Corpus (BNC) (Leech et al., 2001) and the enTenTen web corpus. Predications were extracted templates over a POStagged version of each corpus using the Sketch Engine1 tool (Kilgarriff et al., 2004). The BNC contained about 112 million tokens and the enTenTen collection has 3.2 billion tokens. For each collection, the top 1,000 most frequent nouns provided a seed set from which to extract all predicate and predicate of relations2 (see examples in Table 1). For the BNC, this resulted in 40,721 predications (14,319 unique) and 260,555 (20,651 unique) for the enTenTen collection. Predication strength scores were computed for every predication using within-corpus relative frequencies. These scores were used to weight edges in one version of the predication network, whereas the edge-weights o</context>
</contexts>
<marker>Kilgarriff, Rychl, Smr, Tugwell, 2004</marker>
<rawString>Adam Kilgarriff, Pavel Rychl, Pavel Smr, and David Tugwell. 2004. The sketch engine. In Proceedings of EURALEX 2004, pages 105–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Lancichinetti</author>
<author>Santo Fortunato</author>
</authors>
<title>Community detection algorithms: a comparative analysis. Physical Review E,</title>
<date>2009</date>
<contexts>
<context position="14456" citStr="Lancichinetti and Fortunato, 2009" startWordPosition="2229" endWordPosition="2233">hm can find multi-level communities in large networks (Rosvall and Bergstrom, 2011). The second method, walktrap, operationalizes the intuition that a large set of short random walks on a network will leave walkers on some groups of nodes more often than others (Pons and Latapy, 2005). By setting the walk distance to a small value, relative to a network’s density, walkers will tend toward communities if the walker sample is sufficient. These algorithms are both known to work well with large, directed networks and neither imposed intractable computational burdens at our scale (Fortunato, 2010; Lancichinetti and Fortunato, 2009). Because both algorithms require a connected network, our analysis is restricted to the largest connected component (LCC) for all networks, though we have no reason to believe results would differ significantly for other components. Community assignments can be assessed by measuring how self-contained or “modular” the resulting communities are. Modularity was introduced as a way to choose the level of an optimal cut for hierarchical partitioning algorithms, analogous to the level in the dendrogram that yields the best communities (Newman and Girvan, 2004). For a network with adjacency matrix </context>
</contexts>
<marker>Lancichinetti, Fortunato, 2009</marker>
<rawString>Andrea Lancichinetti and Santo Fortunato. 2009. Community detection algorithms: a comparative analysis. Physical Review E, 80(5):056117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="3630" citStr="Landauer and Dumais, 1997" startWordPosition="524" endWordPosition="528">he those things it represents. Network representations of language have been used to describe a wide range of structures in language, including word-word and word-document co-occurrences, term collocations, dependency structure and named entity relations. Networks of grammatical relations have been found to differentiate word-classes (Ferrer i Cancho et al., 2004) and semantic networks can be used to model vocabulary growth (Steyvers and Tenenbaum, 2005). Co-occurrence networks, which are perhaps the most widely studied natural language network, are the foundation of many vector-space models (Landauer and Dumais, 1997; Turney et al., 2010) and can be used to mine synonyms (Cohen et al., 2005), disambiguate word senses (Agirre et 48 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al.,</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Paul Rayson</author>
<author>Andrew Wilson</author>
</authors>
<title>Word frequencies in written and spoken English: based on the British National Corpus.</title>
<date>2001</date>
<publisher>Longman.</publisher>
<contexts>
<context position="12131" citStr="Leech et al., 2001" startWordPosition="1890" endWordPosition="1893">n expert is a person to fix [...] After all, the vendor is the person best able to [...] An expert need not be an individual person. The innocent party is a natural person. If the indemnifier is a natural person, [...] consumers who are natural persons under the Directive. Table 1: Sample predications involving forms of the word person as the target in the BNC. In each case an edge would connect the predicate (in italics) to person (in bold). 3 Results To explore the structure of predication networks, we analyzed two corpora using the method described above: the British National Corpus (BNC) (Leech et al., 2001) and the enTenTen web corpus. Predications were extracted templates over a POStagged version of each corpus using the Sketch Engine1 tool (Kilgarriff et al., 2004). The BNC contained about 112 million tokens and the enTenTen collection has 3.2 billion tokens. For each collection, the top 1,000 most frequent nouns provided a seed set from which to extract all predicate and predicate of relations2 (see examples in Table 1). For the BNC, this resulted in 40,721 predications (14,319 unique) and 260,555 (20,651 unique) for the enTenTen collection. Predication strength scores were computed for every</context>
</contexts>
<marker>Leech, Rayson, Wilson, 2001</marker>
<rawString>Geoffrey Leech, Paul Rayson, and Andrew Wilson. 2001. Word frequencies in written and spoken English: based on the British National Corpus. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="6041" citStr="Miller, 1995" startWordPosition="894" endWordPosition="895">predications and explore whether this structure becomes more highly resolved when using a semantic measure of predication strength introduced in the following section. We also predict that central nodes (i.e. words) in individual modules will correlate to categorical super-ordinance or hypernymy. Thus, we first seek to assess the overall community structure of predication, testing whether or not it is more resolved using a novel measure of predication strength. Second, within communities of predications, we compare the words’ closeness centrality to their positions in WordNet’s hypernym tree (Miller, 1995). 2 Method Unlike co-occurrence networks, where words are related simply by proximity, predication networks are built using extracted grammatical relationships. The implied relationship in a co-occurrence network provides a natural way to weight edges, but predications have no analogue to a proximitybased weighting scheme. One option would be to weight edges by the number of times given predications were observed. While this is perhaps the most obvious way to account for important predications, it risks exaggerating high-frequency items that are common for reasons other than importance (perhap</context>
<context position="17896" citStr="Miller, 1995" startWordPosition="2803" endWordPosition="2804">ize of the network). This pruning is helpful for the unweighted networks, but has little effect on the weighted versions. In all cases, sparsity takes a toll as the LCC becomes quite small. The reason for the eventual decline as the LCC shrinks below 70 nodes is because communities are less likely to form at all in small networks. In addition to the highly modular structure, the communities of predications themselves are likely to represent some semantic organization. Specifically, we looked for a categorical structure within the communities by comparing words to the hypernym tree in WordNet (Miller, 1995). Intuitively, one would expect words that are central in Figure 2: Modularity of predication networks in the BNC (top) and enTenTen (bottom). Note, as the minimum frequency increases (bottom axis) and the LCC contains fewer and fewer nodes (top axis), the community detection algorithms may not produce a solution with more than one community, resulting in undefined modularity. a community to be members of higher-level categories. In figure 1, for example, summer, hour and holiday all point to time, one could infer that time is a shared hypernym. We use closeness centrality, a graph-theoretic m</context>
<context position="22089" citStr="Miller, 1995" startWordPosition="3431" endWordPosition="3432"> not be common in all languages. To the extent that parsers or taggers are available, a comparative analysis would broaden the understanding of predication in general. Given their high modularity, predication structures could be exploited further for a number of NLP tasks. The correlations between centrality and hypernym depth mean that predication networks could help construct or update categorical taxonomies. For example, these networks could help automate the construction of a hypernym taxonomy with weighted branches, potentially augmenting resources like WordNet (Ruiz-Casado et al., 2005; Miller, 1995). One could also examine the growth, combination and bifurcation of specific communities to help track ontological commitments, either over time as shifts in language structures (Gerow and Ahmad, 2012), or across genre and domain (Davies, 2010). Further, because predication encodes categorical information, its community structure may also encode higher-level relations where strong intercommunity links imply relationships between classes of objects. Our study examined the topographical structure of English predications in general, structure that consists, in large part, of hypernym relations. T</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E J Newman</author>
<author>M Girvan</author>
</authors>
<title>Finding and evaluating community structure in networks. Physical Review E,</title>
<date>2004</date>
<contexts>
<context position="15018" citStr="Newman and Girvan, 2004" startWordPosition="2316" endWordPosition="2319">our scale (Fortunato, 2010; Lancichinetti and Fortunato, 2009). Because both algorithms require a connected network, our analysis is restricted to the largest connected component (LCC) for all networks, though we have no reason to believe results would differ significantly for other components. Community assignments can be assessed by measuring how self-contained or “modular” the resulting communities are. Modularity was introduced as a way to choose the level of an optimal cut for hierarchical partitioning algorithms, analogous to the level in the dendrogram that yields the best communities (Newman and Girvan, 2004). For a network with adjacency matrix A and community assignments c, modularity is defined as: _ 1 kikj Q 2m Aij − δ(ci, cj) (4) ij 2m where m is the number of edges and ki is the degree of node i. δ(ci, cj) is 1 when the community assignment of node i is the same as that for node j. Modularity measures how likely it is that nodes in a community are connected to one another as opposed to nodes in other communities. Modularity is defined from -1.0 to 1.0 and graphs where Q &gt; 0.6 are conventionally said to have relatively strong community structure (New51 man, 2010). Here, we use modularity inst</context>
</contexts>
<marker>Newman, Girvan, 2004</marker>
<rawString>Mark E. J. Newman and M. Girvan. 2004. Finding and evaluating community structure in networks. Physical Review E, 69(2):026113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E J Newman</author>
</authors>
<title>Networks: an introduction.</title>
<date>2010</date>
<publisher>Oxford University Press.</publisher>
<marker>Newman, 2010</marker>
<rawString>Mark E. J. Newman. 2010. Networks: an introduction. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Pons</author>
<author>Matthieu Latapy</author>
</authors>
<title>Computing communities in large networks using random walks.</title>
<date>2005</date>
<booktitle>In Computer and Information Sciences-ISCIS</booktitle>
<pages>284--293</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="14107" citStr="Pons and Latapy, 2005" startWordPosition="2176" endWordPosition="2179">thm. The Infomap algorithm is an informationtheoretic method that exploits the analogue between optimizing a compression dictionary and simplifying a graph by describing “flow” through nodes (Rosvall and Bergstrom, 2008). Infomap assumes edges in a network induce such flow and by deriving a minimum description of this flow, the algorithm can find multi-level communities in large networks (Rosvall and Bergstrom, 2011). The second method, walktrap, operationalizes the intuition that a large set of short random walks on a network will leave walkers on some groups of nodes more often than others (Pons and Latapy, 2005). By setting the walk distance to a small value, relative to a network’s density, walkers will tend toward communities if the walker sample is sufficient. These algorithms are both known to work well with large, directed networks and neither imposed intractable computational burdens at our scale (Fortunato, 2010; Lancichinetti and Fortunato, 2009). Because both algorithms require a connected network, our analysis is restricted to the largest connected component (LCC) for all networks, though we have no reason to believe results would differ significantly for other components. Community assignm</context>
</contexts>
<marker>Pons, Latapy, 2005</marker>
<rawString>Pascal Pons and Matthieu Latapy. 2005. Computing communities in large networks using random walks. In Computer and Information Sciences-ISCIS 2005, pages 284–293. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anatol Rapoport</author>
<author>Amnon Rapoport</author>
<author>William P Livant</author>
<author>John Boyd</author>
</authors>
<title>A study of lexical graphs. Foundations of Language,</title>
<date>1966</date>
<pages>338--376</pages>
<contexts>
<context position="4516" citStr="Rapoport et al., 1966" startWordPosition="664" endWordPosition="667">4 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarization and retrieval (Erkan and Radev, 2004; V´eronis, 2004). Network-based methods have even used community detection, similar to the algorithm described in this paper, to extract specialist terms from sets of multi-theme documents (Grineva et al., 2009) as well as unstructured texts (Gerow, 2014). Because predications naturally form directed chains of ontological assertions, we hypothesize that their underlying structure is systematic and modular, given its representation of naturally or</context>
</contexts>
<marker>Rapoport, Rapoport, Livant, Boyd, 1966</marker>
<rawString>Anatol Rapoport, Amnon Rapoport, William P Livant, and John Boyd. 1966. A study of lexical graphs. Foundations of Language, pages 338–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional preference and sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How,</booktitle>
<pages>52--57</pages>
<contexts>
<context position="7235" citStr="Resnik (1997)" startWordPosition="1069" endWordPosition="1070"> importance (perhaps they are idioms, collocates or found in abnormally strong colligational structures). Frequency weighting would also be susceptible to noise from the many low-frequency items. To address these concerns, we introduce a semantically informed measure of predication relevance. Wilks’ (1975; 1978) theory of preference semantics proposes that subject- and object-verb relationships evince “selectional preference”, which can be thought of as the disposition verbs have to select certain arguments – particular classes of subjects or objects. To operationalize selectional preference, Resnik (1997) introduced selectional preference strength to measure the disposition or “preference” of a verb, v: � P (c|v) SR(v) = P(c|v) log P (c) (1) c∈C where C is a set of semantic classes from which v can select and R is the grammatical relation in question. Note that SR(v) is effectively the sum K-L divergence between the probabilities of v and c for all classes. In a corpus-based setting, the probability of any word can be estimated by its relative frequency: P(x) =r f (x). Resnik goes Ei f (xi) on to define a measure of selectional association between a verb and a specific class, c: 1 P (c|v) AR(v</context>
</contexts>
<marker>Resnik, 1997</marker>
<rawString>Philip Resnik. 1997. Selectional preference and sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How, pages 52–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Rosvall</author>
<author>Carl T Bergstrom</author>
</authors>
<title>Maps of random walks on complex networks reveal community structure.</title>
<date>2008</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>105</volume>
<issue>4</issue>
<contexts>
<context position="13705" citStr="Rosvall and Bergstrom, 2008" startWordPosition="2110" endWordPosition="2113">?&amp;quot;|tag=&amp;quot;RB&amp;quot;|tag=&amp;quot;VM&amp;quot;]0,5 [lemma=&amp;quot;be&amp;quot; &amp; tag=&amp;quot;V.*&amp;quot;] &amp;quot;RB.?&amp;quot;0,2 [tag=&amp;quot;DT.?&amp;quot;|tag=&amp;quot;PP$&amp;quot;]0,1 &amp;quot;CD&amp;quot;0,2 [tag=&amp;quot;JJ.?&amp;quot;|tag=&amp;quot;RB.?&amp;quot;|word=&amp;quot;,&amp;quot;]0,3 &amp;quot;NN.?.?&amp;quot;0,2 2:&amp;quot;NN.?.?&amp;quot; [tag!=&amp;quot;NN.?.?&amp;quot;] Two methods were used to extract communities from the predication networks: the Infomap and walktrap algorithms. By using two methods, we attain some assurance that our findings are not artifacts of the assumptions underlying either algorithm. The Infomap algorithm is an informationtheoretic method that exploits the analogue between optimizing a compression dictionary and simplifying a graph by describing “flow” through nodes (Rosvall and Bergstrom, 2008). Infomap assumes edges in a network induce such flow and by deriving a minimum description of this flow, the algorithm can find multi-level communities in large networks (Rosvall and Bergstrom, 2011). The second method, walktrap, operationalizes the intuition that a large set of short random walks on a network will leave walkers on some groups of nodes more often than others (Pons and Latapy, 2005). By setting the walk distance to a small value, relative to a network’s density, walkers will tend toward communities if the walker sample is sufficient. These algorithms are both known to work wel</context>
</contexts>
<marker>Rosvall, Bergstrom, 2008</marker>
<rawString>Martin Rosvall and Carl T. Bergstrom. 2008. Maps of random walks on complex networks reveal community structure. Proceedings of the National Academy of Sciences, 105(4):1118–1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Rosvall</author>
<author>Carl T Bergstrom</author>
</authors>
<title>Multilevel compression of random walks on networks reveals hierarchical organization in large integrated systems.</title>
<date>2011</date>
<journal>PloS one,</journal>
<volume>6</volume>
<issue>4</issue>
<contexts>
<context position="13905" citStr="Rosvall and Bergstrom, 2011" startWordPosition="2142" endWordPosition="2145">t communities from the predication networks: the Infomap and walktrap algorithms. By using two methods, we attain some assurance that our findings are not artifacts of the assumptions underlying either algorithm. The Infomap algorithm is an informationtheoretic method that exploits the analogue between optimizing a compression dictionary and simplifying a graph by describing “flow” through nodes (Rosvall and Bergstrom, 2008). Infomap assumes edges in a network induce such flow and by deriving a minimum description of this flow, the algorithm can find multi-level communities in large networks (Rosvall and Bergstrom, 2011). The second method, walktrap, operationalizes the intuition that a large set of short random walks on a network will leave walkers on some groups of nodes more often than others (Pons and Latapy, 2005). By setting the walk distance to a small value, relative to a network’s density, walkers will tend toward communities if the walker sample is sufficient. These algorithms are both known to work well with large, directed networks and neither imposed intractable computational burdens at our scale (Fortunato, 2010; Lancichinetti and Fortunato, 2009). Because both algorithms require a connected net</context>
</contexts>
<marker>Rosvall, Bergstrom, 2011</marker>
<rawString>Martin Rosvall and Carl T. Bergstrom. 2011. Multilevel compression of random walks on networks reveals hierarchical organization in large integrated systems. PloS one, 6(4):e18209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-Casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic extraction of semantic relationships for wordnet by means of pattern learning from wikipedia.</title>
<date>2005</date>
<booktitle>In Natural Language Processing and Information Systems,</booktitle>
<pages>67--79</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="22074" citStr="Ruiz-Casado et al., 2005" startWordPosition="3427" endWordPosition="3430">egory membership, may also not be common in all languages. To the extent that parsers or taggers are available, a comparative analysis would broaden the understanding of predication in general. Given their high modularity, predication structures could be exploited further for a number of NLP tasks. The correlations between centrality and hypernym depth mean that predication networks could help construct or update categorical taxonomies. For example, these networks could help automate the construction of a hypernym taxonomy with weighted branches, potentially augmenting resources like WordNet (Ruiz-Casado et al., 2005; Miller, 1995). One could also examine the growth, combination and bifurcation of specific communities to help track ontological commitments, either over time as shifts in language structures (Gerow and Ahmad, 2012), or across genre and domain (Davies, 2010). Further, because predication encodes categorical information, its community structure may also encode higher-level relations where strong intercommunity links imply relationships between classes of objects. Our study examined the topographical structure of English predications in general, structure that consists, in large part, of hypern</context>
</contexts>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic extraction of semantic relationships for wordnet by means of pattern learning from wikipedia. In Natural Language Processing and Information Systems, pages 67–79. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Simone Teufel</author>
<author>Anna Korhonen</author>
</authors>
<title>Statistical metaphor processing.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="8140" citStr="Shutova et al., 2013" startWordPosition="1231" endWordPosition="1234">he sum K-L divergence between the probabilities of v and c for all classes. In a corpus-based setting, the probability of any word can be estimated by its relative frequency: P(x) =r f (x). Resnik goes Ei f (xi) on to define a measure of selectional association between a verb and a specific class, c: 1 P (c|v) AR(v, c) = SR(v)P(c|v) log P (c) (2) In the typical form of selectional preference induction – the task of estimating likelihoods over all classes – Eq. 2 is used to measure a verb’s preference for classes of nominal subjects or objects like vehicles, insects, birds, etc. (Resnik, 1997; Shutova et al., 2013). To test our assumptions regarding the modular structure of predications in English, a measure like selectional association should account for predicates’ diversity (or uniformity) of attachment. That is, the preference a predicate has to 49 operate on a wide or narrow range of words. To account for this, we add a term, U(p), to account for the relative number of unique words a predicate p has been observed to predicate. Note that this is not the total number of predications involving p, which would produce problematically high values were p to collocate strongly with the words it predicates.</context>
</contexts>
<marker>Shutova, Teufel, Korhonen, 2013</marker>
<rawString>Ekaterina Shutova, Simone Teufel, and Anna Korhonen. 2013. Statistical metaphor processing. Computational Linguistics, 39(2):301–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3463" citStr="Steyvers and Tenenbaum, 2005" startWordPosition="501" endWordPosition="504">ed network of words. Consisting primarily of category assertions, the structure of this network should exhibit a degree of natural clustering owing natural categories of the those things it represents. Network representations of language have been used to describe a wide range of structures in language, including word-word and word-document co-occurrences, term collocations, dependency structure and named entity relations. Networks of grammatical relations have been found to differentiate word-classes (Ferrer i Cancho et al., 2004) and semantic networks can be used to model vocabulary growth (Steyvers and Tenenbaum, 2005). Co-occurrence networks, which are perhaps the most widely studied natural language network, are the foundation of many vector-space models (Landauer and Dumais, 1997; Turney et al., 2010) and can be used to mine synonyms (Cohen et al., 2005), disambiguate word senses (Agirre et 48 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguist</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Mark Steyvers and Joshua B. Tenenbaum. 2005. The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. Cognitive Science, 29(1):41–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preferences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>638--647</pages>
<contexts>
<context position="4186" citStr="Sun and Korhonen, 2009" startWordPosition="611" endWordPosition="615">e foundation of many vector-space models (Landauer and Dumais, 1997; Turney et al., 2010) and can be used to mine synonyms (Cohen et al., 2005), disambiguate word senses (Agirre et 48 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 48–55, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 2014; Biemann, 2006) and even help mark the quality of essays (Foltz et al., 1999). Spectral methods applied to linguistic networks have been used to differentiate languages (Ferrer i Cancho et al., 2004), word-classes (Sun and Korhonen, 2009) and genres of text (Ferrer i Cancho et al., 2007). Using spectral methods, research has also found that syntactic and semantic distributional similarity networks have considerably different structure (Biemann et al., 2009). The use of lexical graphs (networks of words) in particular, pre-dates modern NLP (Rapoport et al., 1966), though the approach continues to influence a variety of NLP and information retrieval tasks like summarization and retrieval (Erkan and Radev, 2004; V´eronis, 2004). Network-based methods have even used community detection, similar to the algorithm described in this p</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>Lin Sun and Anna Korhonen. 2009. Improving verb clustering with automatically acquired selectional preferences. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 638–647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney, Patrick Pantel, et al. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>Hyperlex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>Jean V´eronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3):223–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>A preferential, pattern-seeking, semantics for natural language inference.</title>
<date>1975</date>
<journal>Artificial Intelligence,</journal>
<volume>6</volume>
<issue>1</issue>
<marker>Wilks, 1975</marker>
<rawString>Yorick Wilks. 1975. A preferential, pattern-seeking, semantics for natural language inference. Artificial Intelligence, 6(1):53–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Making preferences more active.</title>
<date>1978</date>
<journal>Artificial Intelligence,</journal>
<volume>11</volume>
<issue>3</issue>
<marker>Wilks, 1978</marker>
<rawString>Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence, 11(3):197–223.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
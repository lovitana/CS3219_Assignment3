<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000605">
<title confidence="0.878985">
RED: DCU-CASICT Participation in WMT2014 Metrics Task
</title>
<author confidence="0.991136">
Xiaofeng Wu†, Hui Yu*, Qun Liu†*
</author>
<affiliation confidence="0.896379333333333">
†CNGL Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
*Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, China
</affiliation>
<email confidence="0.9982">
{xiaofengwu,qliu}@computing.dcu.ie, yuhui@ict.ac.cn
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940375">
Based on the last year’s DCU-CASIST
participation on WMT metrics task, we
further improve our model in the follow-
ing ways: 1) parameter tuning 2) support
languages other than English. We tuned
our system on all the data of WMT 2010,
2012 and 2013. The tuning results as well
as the WMT 2014 test results are reported.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995792">
Automatic evaluation plays a more and more im-
portant role in the evolution of machine transla-
tion. There are roughly two categories can be seen:
namely lexical information based and structural
information based.
</bodyText>
<listItem confidence="0.966347636363636">
1) Lexical information based approaches,
among which, BLEU (?), Translation Error Rate
(TER) (?) and METEOR (?) are the most popular
ones and, with simplicity as their merits, cannot
adequately reflect the structural level similarity.
2) A lot of structural level based methods
have been exploited to overcome the weakness
of the lexical based methods, including Syntactic
Tree Model(STM)(?), a constituent tree based ap-
proach, and Head Word Chain Model(HWCM)(?),
a dependency tree based approach. Both of
the methods compute the similarity between the
sub-trees of the hypothesis and the reference.
Owczarzak et al (?; ?; ?) presented a method
using the Lexical-Functional Grammar (LFG) de-
pendency tree. MAXSIM (?) and the method pro-
posed by Zhu et al (?) also employed the syntac-
tic information in association with lexical infor-
mation. As we know that the hypothesis is poten-
tially noisy, and these errors are enlarged through
the parsing process. Thus the power of syntactic
information could be considerably weakened.
</listItem>
<bodyText confidence="0.999730714285714">
In this paper, based on our attempt on WMT
metrics task 2013 (?), we propose a metrics named
RED ( REference Dependency based automatic
evaluation method). Our metrics employs only the
reference dependency tree which contains both the
lexical and syntactic information, leaving the hy-
pothesis side unparsed to avoid error propagation.
</bodyText>
<sectionHeader confidence="0.978015" genericHeader="method">
2 Parameter Tuning
</sectionHeader>
<bodyText confidence="0.99518">
In RED, we use F-score as our final score.
F-score is calculated by Formula (1), where α is
a value between 0 and 1.
</bodyText>
<equation confidence="0.95944425">
F-score =
precision · recall
α · precision + (1 − α) · recall
(1)
</equation>
<bodyText confidence="0.953654">
The dependency tree of the reference and the
string of the translation are used to calculate the
precision and recall. In order to calculate preci-
sion, the number of the dep-ngrams (the ngrams
obtained from dependency tree1) should be given,
but there is no dependency tree for the transla-
tion in our method. We know that the number
of dep-ngrams has an approximate linear relation-
ship with the length of the sentence, so we use the
length of the translation to replace the number of
the dep-ngrams in the translation dependency tree.
Recall can be calculated directly since we know
the number of the dep-ngrams in the reference.
The precision and recall are computed as follows.
precision n = EdEDn p(d,hyp)
lenh
</bodyText>
<equation confidence="0.990343">
recall = EdEDn p(d,hyp)
n countn(ref)
</equation>
<bodyText confidence="0.999470333333333">
Dn is the set of dep-ngrams with the length of n.
lenh is the length of the translation. countn(ref)
is the number of the dep-ngrams with the length
of n in the reference. p(d,hpy) is 0 if there is no
match and a positive number between 0 and 1 oth-
erwise(?).
</bodyText>
<footnote confidence="0.9926265">
1We define two types of dep-ngrams: 1) the head word
chain(?); 2) fix-floating(?))
</footnote>
<page confidence="0.95965">
420
</page>
<bodyText confidence="0.904767214285714">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 420–425,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
The final score of RED is achieved using For-
mula (2), in which a weighted sum of the dep-
ngrams’ F-score is calculated. wngram (0 ≤
wngram ≤ 1) is the weight of dep-ngram with the
length of n. F-scoren is the F-score for the dep-
ngrams with the length of n.
ment with METEOR Aligner, only consid-
ering paraphrase; Then, the matched para-
phrases are extracted from the alignment and
defined as paraphrase-ngram. The score of
a paraphrase is 1 × wpar, where wpar is the
weight of paraphrase-ngram.
</bodyText>
<listItem confidence="0.857662">
• Function word
</listItem>
<equation confidence="0.995251">
N
RED = (wngram × F-scoren) (2)
n=1
</equation>
<bodyText confidence="0.933734">
Other parameters to be tuned includes:
</bodyText>
<listItem confidence="0.998536">
• Stem and Synonym
</listItem>
<bodyText confidence="0.991079851851852">
Stem(?) and synonym (WordNet2) are intro-
duced with the following three steps. First,
we obtain the alignment with METEOR
Aligner (?) in which not only exact match
but also stem and synonym are considered.
We use stem, synonym and exact match as the
three match modules. Second, the alignment
is used to match for a dep-ngram. We think
the dep-ngram can match with the transla-
tion if the following conditions are satisfied.
1) Each of the words in the dep-ngram has
a matched word in the translation according
to the alignment; 2) The words in dep-ngram
and the matched words in translation appear
in the same order; 3) The matched words
in translation must be continuous if the dep-
ngram is a fixed-floating ngram. At last, the
match module score of a dep-ngram is cal-
culated according to Formula (3). Different
match modules have different effects, so we
give them different weights.
, 0 ≤ wmi ≤ 1 (3)
mi is the match module (exact, stem or syn-
onym) of the ith word in a dep-ngram. wmi
is the match module weight of the ith word in
a dep-ngram. n is the number of words in a
dep-ngram.
</bodyText>
<listItem confidence="0.98868">
• Paraphrase
</listItem>
<bodyText confidence="0.9967908">
When introducing paraphrase, we don’t con-
sider the dependency tree of the reference,
because paraphrases may not be contained in
the head word chain and fixed-floating struc-
tures. Therefore we first obtain the align-
</bodyText>
<footnote confidence="0.73746">
2http://wordnet.princeton.edu/
</footnote>
<bodyText confidence="0.9981865">
We introduce a parameter wfun (0 ≤ wfun ≤
1) to distinguish function words and content
words. wfun is the weight of function words.
The function word score of a dep-ngram or
paraphrase-ngram is computed according to
Formula (4).
</bodyText>
<equation confidence="0.552003">
Cfun × wfun + Ccon × (1 − wfun)
Cfun + Ccon
(4)
</equation>
<bodyText confidence="0.98678675">
Cfun is the number of function words in the
dep-ngram or paraphrase-ngram. Ccon is the
number of content words in the dep-ngram or
paraphrase-ngram.
</bodyText>
<equation confidence="0.996927285714286">
N
REDp = (wngram × F-scorepn) (5)
n=1
precisionp · recallp
F-scorep =
α · precisionp + (1 − α) · recallp
(6)
</equation>
<bodyText confidence="0.9877015">
precisionp and recallP in Formula (6) are cal-
culated as follows.
</bodyText>
<equation confidence="0.9902758">
scoreparn + scoredepn
lenh
scoreparn + scoredepn
recallp =
countn(ref) + countn(par)
</equation>
<bodyText confidence="0.99980225">
lenh is the length of the translation. countn(ref)
is the number of the dep-ngrams with the length
of n in the reference. countn(par) is the num-
ber of paraphrases with length of n in refer-
ence. scoreparn is the match score of paraphrase-
ngrams with the length of n. scoredepn is the
match score of dep-ngrams with the length of n.
scoreparn and scoredepn are calculated as follows.
</bodyText>
<equation confidence="0.998640625">
�scoreparn = (1 × wpar × sfum)
par∈Pn
�scoredepn = (p(d,hyp) × smod × sfun)
d∈Dn
smod = Eni=1 wmi
n
sfun =
precisionp =
</equation>
<page confidence="0.985119">
421
</page>
<table confidence="0.999814652173913">
Metrics BLEU TER HWCM METEOR RED RED-sent RED-syssent
WMT 2010 cs-en 0.255 0.253 0.245 0.319 0.328 0.342 0.342
de-en 0.275 0.291 0.267 0.348 0.361 0.371 0.375
es-en 0.280 0.263 0.259 0.326 0.333 0.344 0.347
fr-en 0.220 0.211 0.244 0.275 0.283 0.329 0.328
ave 0.257 0.254 0.253 0.317 0.326 0.346 0.348
WMT 2012 cs-en 0.157 - 0.158 0.212 0.165 0.218 0.212
de-en 0.191 - 0.207 0.275 0.218 0.283 0.279
es-en 0.189 - 0.203 0.249 0.203 0.255 0.256
fr-en 0.210 - 0.204 0.251 0.221 0.250 0.253
ave 0.186 - 0.193 0.246 0.201 0.251 0.250
WMT 2013 cs-en 0.199 - 0.153 0.265 0.228 0.260 0.256
de-en 0.220 - 0.182 0.293 0.267 0.298 0.297
es-en 0.259 - 0.220 0.324 0.312 0.330 0.326
fr-en 0.224 - 0.194 0.264 0.257 0.267 0.266
ru-en 0.162 - 0.136 0.239 0.200 0.262 0.225
ave 0.212 - 0.177 0.277 0.252 0.283 0.274
WMT 2014 hi-en - - - 0.420 - 0.383 0.386
de-en - - - 0.334 - 0.336 0.338
cs-en - - - 0.282 - 0.283 0.283
fr-en - - - 0.406 - 0.403 0.404
ru-en - - - 0.337 - 0.328 0.329
ave - - - 0.355 - 0.347 0.348
</table>
<tableCaption confidence="0.999009">
Table 1: Sentence level correlations tuned on WMT 2010, 2012 and 2013; tested on WMT 2014. The
</tableCaption>
<bodyText confidence="0.988010652173913">
value in bold is the best result in each raw. ave stands for the average result of the language pairs on each
year. RED stands for our untuned system, RED-sent is G.sent.2, RED-syssent is G.sent.1
P,,, is the set of paraphrase-ngrams with the
length of n. D,,, is the set of dep-ngrams with the
length of n.
There are totally nine parameters in RED. We
tried two parameter tuning strategies: Genetic
search algorithm (?) and a Grid search over two
subsets of parameters. The results of Grid search
is more stable, therefore our final submission is
based upon Grid search. We separate the 9 pa-
rameters into two subsets. When searching Sub-
set 1, the parameters in Subset 2 are fixed, and
vice versa. Several iterations are executed to fin-
ish the parameter tuning process. For system
level coefficient score, we set two optimization
goals: G.sys.1) to maximize the sum of Spear-
man’s p rank correlation coefficient on system
level and Kendall’s T correlation coefficient on
sentence level or G.sys.2) only the former; For
sentence level coefficient score, we also set two
optimization goals: G.sent.1) the same as G.sys.1,
G.sent.2) only the latter part of G.sys.1.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999968">
In this section we report the experimental results
of RED on the tuning set, which is the combi-
nation of WMT2010, WMT2012 and WMT2013
data set, as well as the test results on the
WMT2014. Both the sentence level evaluation and
the system level evaluation are conducted to assess
the performance of our automatic metrics. At the
sentence level evaluation, Kendall’s rank correla-
tion coefficient T is used. At the system level eval-
uation, the Spearman’s rank correlation coefficient
p is used.
</bodyText>
<subsectionHeader confidence="0.992832">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999951">
There are four language pairs in WMT2010 and
WMT2012 including German-English, Czech-
English, French-English and Spanish-English. For
WMT2013, except these 4 language pairs, there is
also Russian-English. As the test set, WMT 2014
has also five language pairs, but the organizer re-
moved Spanish-English and replace it with Hindi-
English. For into-English tasks, we parsed the En-
</bodyText>
<page confidence="0.996443">
422
</page>
<table confidence="0.993435217391304">
Metrics BLEU TER HWCM METEOR RED RED-sys RED-syssent
WMT 2010 cs-en 0.840 0.783 0.881 0.839 0.839 0.937 0.881
de-en 0.881 0.892 0.905 0.927 0.933 0.95 0.948
es-en 0.868 0.903 0.824 0.952 0.969 0.965 0.969
fr-en 0.839 0.833 0.815 0.865 0.873 0.875 0.905
ave 0.857 0.852 0.856 0.895 0.903 0.931 0.925
WMT 2012 cs-en 0.886 0.886 0.943 0.657 1 1 1
de-en 0.671 0.624 0.762 0.885 0.759 0.935 0.956
es-en 0.874 0.916 0.937 0.951 0.951 0.965 0.958
fr-en 0.811 0.821 0.818 0.843 0.818 0.871 0.853
ave 0.810 0.811 0.865 0.834 0.882 0.942 0.941
WMT 2013 cs-en 0.936 0.800 0.818 0.964 0.964 0.982 0.972
de-en 0.895 0.833 0.816 0.961 0.951 0.958 0.978
es-en 0.888 0.825 0.755 0.979 0.930 0.979 0.965
fr-en 0.989 0.951 0.940 0.984 0.989 0.995 0.984
ru-en 0.670 0.581 0.360 0.789 0.725 0.847 0.821
ave 0.875 0.798 0.737 0.834 0.935 0.952 0.944
WMT 2014 hi-en 0.956 0.618 - 0.457 - 0.676 0.644
de-en 0.831 0.774 - 0.926 - 0.897 0.909
cs-en 0.908 0.977 - 0.980 - 0.989 0.993
fr-en 0.952 0.952 - 0.975 - 0.981 0.980
ru-en 0.774 0.796 - 0.792 - 0.803 0.797
ave 0.826 0.740 - 0.784 - 0.784 0.770
</table>
<tableCaption confidence="0.830188333333333">
Table 2: System level correlations tuned on WMT 2010, 2012 and 2013, tested on 2014. The value in
bold is the best result in each raw. ave stands for the average result of the language pairs on each year.
RED stands for our untuned system, RED-sys is G.sys.2, RED-syssent is G.sys.1
</tableCaption>
<table confidence="0.999600555555556">
Metrics BLEU TER METEOR RED RED-sent RED-syssent
WMT 2010 en-fr 0.33 0.31 0.369 0.338 0.390 0.369
en-de 0.15 0.08 0.166 0.141 0.214 0.185
WMT 2012 en-fr - - 0.26 0.171 0.273 0.266
en-de - - 0.180 0.129 0.200 0.196
WMT 2013 en-fr - - 0.236 0.220 0.237 0.239
en-de - - 0.203 0.185 0.215 0.219
WMT 2014 en-fr - - 0.278 - 0.297 0.293
en-de - - 0.233 - 0.236 0.229
</table>
<tableCaption confidence="0.999035">
Table 3: Sentence level correlations tuned on WMT 2010, 2012 and 2013, and tested on 2014. The
</tableCaption>
<bodyText confidence="0.867457375">
value in bold is the best result in each raw. RED stands for our untuned system, RED-sent is G.sent.2,
RED-syssent is G.sent.1
glish reference into constituent tree by Berkeley
parser and then converted the constituent tree into
dependency tree by Penn2Malt 3. We also con-
ducted English-to-French and English-to-German
experiments. The German and French dependency
parser we used is Mate-Tool 4.
</bodyText>
<footnote confidence="0.9998705">
3http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
4https://code.google.com/p/mate-tools/
</footnote>
<bodyText confidence="0.999168">
In the experiments, we compare the perfor-
mance of our metric with the widely used lexi-
cal based metrics BLEU, TER, METEOR and a
dependency based metrics HWCM. The results of
RED are provided with exactly the same external
resources like METEOR. The results of BLEU,
TER and METOER are obtained from official re-
port of WMT 2010, 2012, 2013 and 2014 (if they
</bodyText>
<page confidence="0.997954">
423
</page>
<table confidence="0.999574555555556">
Metrics BLEU TER METEOR RED RED-sys RED-syssent
WMT 2010 en-fr 0.89 0.89 0.912 0.881 0.932 0.928
en-de 0.66 0.65 0.688 0.657 0.734 0.734
WMT 2012 en-fr 0.80 0.69 0.82 0.639 0.914 0.914
en-de 0.22 0.41 0.180 0.143 0.243 0.243
WMT 2013 en-fr 0.897 0.912 0.924 0.914 0.931 0.936
en-de 0.786 0.854 0.879 0.85 0.8 0.8
WMT 2014 en-fr 0.934 0.953 0.940 - 0.942 0.943
en-de 0.065 0.163 0.128 - 0.047 0.047
</table>
<tableCaption confidence="0.6841004">
Table 4: System level correlations for English to Franch and German, tuned on WMT 2010, 2012 and
2013; tested on WMT 2014. The value in bold is the best result in each raw. RED stands for our untuned
system, RED-sys is G.sys.2, RED-syssent is G.sys.1
are available). The experiments of HWCM is per-
formed by us.
</tableCaption>
<subsectionHeader confidence="0.998828">
3.2 Sentence-level Evaluation
</subsectionHeader>
<bodyText confidence="0.999930380952381">
Kendall’s rank correlation coefficient τ is em-
ployed to evaluate the correlation of all the MT
evaluation metrics and human judgements at the
sentence level. A higher value of τ means a bet-
ter ranking similarity with the human judges. The
correlation scores of are shown in Table 1. Our
method performs best when maximum length of
dep-n-gram is set to 3, so we present only the
results when the maximum length of dep-n-gram
equals 3. From Table 1, we can see that: firstly, pa-
rameter tuning improve performance significantly
on all the three tuning sets; secondly, although
the best scores in the column RED-sent are much
more than RED-syssent, but the outperform is
very small, so by setting these two optimization
goals, RED can achieve comparable performance;
thirdly, without parameter tuning, RED does not
perform well on WMT 2012 and 2013, and even
with parameter tuning, RED does not outperform
METEOR as much as WMT 2010; lastly, on the
test set, RED does not outperform METEOR.
</bodyText>
<subsectionHeader confidence="0.998988">
3.3 System-level Evaluation
</subsectionHeader>
<bodyText confidence="0.9995639">
We also evaluated the RED scores with the human
rankings at the system level to further investigate
the effectiveness of our metrics. The matching of
the words in RED is correlated with the position
of the words, so the traditional way of computing
system level score, like what BLEU does, is not
feasible for RED. Therefore, we resort to the way
of adding the sentence level scores together to ob-
tain the system level score. At system level evalu-
ation, we employ Spearman’s rank correlation co-
efficient ρ. The correlations and the average scores
are shown in Table 2.
From Table 2, we can see similar trends as in
Table 1 with the following difference: firstly, with-
out parameter tuning, RED perform comparably
with METEOR on all the three tuning sets; sec-
ondly, on test set, RED also perform comparably
with METEOR. thirdly, RED perform very bad on
Hindi-English, which is a newly introduced task
this year.
</bodyText>
<subsectionHeader confidence="0.9575525">
3.4 Evaluation of English to Other
Languages
</subsectionHeader>
<bodyText confidence="0.99988">
We evaluate both sentence level and system level
score of RED on English to French and German.
The reason we only conduct these two languages
are that the dependency parsers are more reliable
in these two languages. The results are shown in
Table 3 and 4.
From Table 3 and 4 we can see that the tuned
version of RED still perform slightly better than
METEOR with the only exception of system level
en-de.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999960181818182">
In this paper, based on the last year’s DCU-
CASICT submission, we further improved our
method, namely RED. The experiments are car-
ried out at both sentence-level and system-level
using to-English and from-English corpus. The
experiment results indicate that although RED
achieves better correlation than BLEU, HWCM,
TER and comparably performance with METEOR
at both sentence level and system level, the per-
formance is not stable on all language pairs, such
as the sentence level correlation score of Hindi to
</bodyText>
<page confidence="0.996891">
424
</page>
<bodyText confidence="0.999613">
English and the system level score of English to
German. To further study the sudden diving of the
performance is our future work.
</bodyText>
<sectionHeader confidence="0.99548" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998733166666667">
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University and Na-
tional Natural Science Foundation of China (Grant
61379086).
</bodyText>
<sectionHeader confidence="0.998193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999580776315789">
Ergun Bic¸ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272–283. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55–62.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ’11, pages 85–91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ’07, pages 228–231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25–32.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-based automatic eval-
uation for machine translation. In Proceedings of
the NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation, SSST ’07,
pages 80–87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007b. Evaluating machine translation with
lfg dependencies. Machine Translation, 21(2):95–
119, June.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007c. Labelled dependencies in machine
translation evaluation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ’07, pages 104–111, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting on association for computational linguis-
tics, pages 311–318. Association for Computational
Linguistics.
Martin F Porter. 2001. Snowball: A language for stem-
ming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649–671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223–231.
Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. Dcu partic-
ipation in wmt2013 metrics task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics.
H. Yu, X. Wu, Q. Liu, and S. Lin. 2014. RED: A
Reference Dependency Based MT Evaluation Met-
ric. In To be published.
Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in strings: a powerful string-
based automatic mt evaluation metric with multi-
ple granularities. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ’10, pages 1533–1540, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999179">
425
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.487066">
<title confidence="0.984622">RED: DCU-CASICT Participation in WMT2014 Metrics Task</title>
<author confidence="0.997314">Hui Qun</author>
<affiliation confidence="0.908157333333333">Centre for Global Intelligent School of Computing, Dublin City Dublin 9, Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of Beijing,</affiliation>
<email confidence="0.984493">yuhui@ict.ac.cn</email>
<abstract confidence="0.998780666666667">Based on the last year’s DCU-CASIST participation on WMT metrics task, we further improve our model in the following ways: 1) parameter tuning 2) support languages other than English. We tuned our system on all the data of WMT 2010, 2012 and 2013. The tuning results as well as the WMT 2014 test results are reported.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>272--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011. Instance selection for machine translation using feature decay algorithms. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 272–283. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Maxsim: A maximum similarity metric for machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>55--62</pages>
<marker>Chan, Ng, 2008</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A maximum similarity metric for machine translation evaluation. In Proceedings of ACL-08: HLT, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>85--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 85–91, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>228--231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 228–231, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Dependency-based automatic evaluation for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation, SSST ’07,</booktitle>
<pages>80--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007a. Dependency-based automatic evaluation for machine translation. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Statistical Translation, SSST ’07, pages 80–87, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Evaluating machine translation with lfg dependencies.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>119</pages>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007b. Evaluating machine translation with lfg dependencies. Machine Translation, 21(2):95– 119, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Labelled dependencies in machine translation evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007c. Labelled dependencies in machine translation evaluation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 104–111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<marker>Porter, 2001</marker>
<rawString>Martin F Porter. 2001. Snowball: A language for stemming algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>String-to-dependency statistical machine translation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<marker>Shen, Xu, Weischedel, 2010</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Computational Linguistics, 36(4):649–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Wu</author>
<author>Hui Yu</author>
<author>Qun Liu</author>
</authors>
<title>Dcu participation in wmt2013 metrics task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation. Association for Computational Linguistics.</booktitle>
<marker>Wu, Yu, Liu, 2013</marker>
<rawString>Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. Dcu participation in wmt2013 metrics task. In Proceedings of the Eighth Workshop on Statistical Machine Translation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>X Wu</author>
<author>Q Liu</author>
<author>S Lin</author>
</authors>
<title>RED: A Reference Dependency Based MT Evaluation Metric. In</title>
<date>2014</date>
<note>To be published.</note>
<marker>Yu, Wu, Liu, Lin, 2014</marker>
<rawString>H. Yu, X. Wu, Q. Liu, and S. Lin. 2014. RED: A Reference Dependency Based MT Evaluation Metric. In To be published.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junguo Zhu</author>
<author>Muyun Yang</author>
<author>Bo Wang</author>
<author>Sheng Li</author>
<author>Tiejun Zhao</author>
</authors>
<title>All in strings: a powerful stringbased automatic mt evaluation metric with multiple granularities.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>1533--1540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Zhu, Yang, Wang, Li, Zhao, 2010</marker>
<rawString>Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and Tiejun Zhao. 2010. All in strings: a powerful stringbased automatic mt evaluation metric with multiple granularities. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 1533–1540, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
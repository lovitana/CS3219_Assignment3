<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003746">
<title confidence="0.951858">
Proofreading Human Translations with an E-pen
</title>
<author confidence="0.924031">
Vicent Alabau and Luis A. Leiva
</author>
<affiliation confidence="0.860308">
PRHLT Research Center
</affiliation>
<address confidence="0.499856">
Universitat Polit`ecnica de Val`encia
</address>
<email confidence="0.989664">
{valabau,luileito}@prhlt.upv.es
</email>
<sectionHeader confidence="0.99548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910545454545">
Proofreading translated text is a task
aimed at checking for correctness, con-
sistency, and appropriate writing style.
While this has been typically done with
a keyboard and a mouse, pen-based
devices set an opportunity for making
such corrections in a comfortable way,
as if proofreading on physical paper.
Arguably, this way of interacting with
a computer is very appropriate when
a small number of modifications are
required to achieve high-quality stan-
dards. In this paper, we propose a tax-
onomy of pen gestures that is tailored
to machine translation review tasks, af-
ter human translator intervention. In
addition, we evaluate the recognition
accuracy of these gestures using a cou-
ple of popular gesture recognizers. Fi-
nally, we comment on open challenges
and limitations, and discuss possible
avenues for future work.
</bodyText>
<sectionHeader confidence="0.998736" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.898063967741936">
Currently, the workflow of many translation
agencies include a final reviewing or proof-
reading processi where the translators’ work
is checked for correctness, consistency and
appropriate writing style. If the translation
quality is good enough, only a small amount
of changes would be necessary to reach a
high-quality result. However, the required
corrections are often spread sparingly and
unequally among the screen, which renders
1The reviewing process can be seen as a detailed
proofreading process where the target sentence is also
compared against the source sentence for errors such as
mistranslations, etc. However, for the purpose of this
paper, we can use the terms reviewing and proofread-
ing indistinguishably.
mouse/keyboard interaction both inefficient
and unappealing.
As a result of the popularization of touch-
screen and pen-based devices, text-editing ap-
plications can be operated today in a simi-
lar way people interact with pen and paper.
This way of reviewing is arguably more natu-
ral and efficient than a keyboard or a mouse,
since the e-pen can be used both to locate and
correct an erroneous word, all at once. Ad-
ditionally, the expressiveness of e-pen interac-
tion provides an opportunity to integrate use-
ful gestures that are able correct other com-
mon mistakes, such as word reordering or cap-
italization.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999974047619048">
The first attempt that we are aware of to post-
edit text with an e-pen interface dates back to
the early seventies of the past century (Cole-
man, 1969). In that work, Coleman proposed
a set of unistroke gestures for post-editing.
Later on, the same corpus was used by (Ru-
bine, 1991) in his seminal work about gesture
recognition with excellent recognition results.
However, the gesture set is too simplistic to be
used in a real translation task today.
Most of the modern applications to generate
and edit textual content using “digital ink” are
based on ad-hoc interaction protocols and of-
ten do not ship handwriting recognition soft-
ware. To our knowledge, MyScript Notes Mo-
bile• is the closest system to provide a natural
onscreen paper-like interaction style, includ-
ing some text-editing gestures and a powerful
handwriting recognition software. However,
this application relies on spatial relations of
the ink strokes to perform handwriting recog-
</bodyText>
<footnote confidence="0.999972">
2http://appadvice.com/appguides/show/
handwriting-apps-for-ipad
3http://www.visionobjects.com
</footnote>
<page confidence="0.981952">
10
</page>
<bodyText confidence="0.983056824561404">
Workshop on Humans and Computer-assisted Translation, pages 10–15,
Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics
nition. For instance, to insert a new word
in the middle of a sentence the user needs to
make room for space explicitly (i.e., if the word
has N characters, the user needs to perform an
Insert Space gesture N times). Moreover, the
produced text does not flow on the UI, i.e., it is
fixed to the position of the ink, which makes
it difficult to modify. As a result, this sys-
tem does not seem suitable for reviewing trans-
lations. Other comparable work is MiNGES-
TURES (Leiva et al., 2013), which proposes a
simplified set of gestures for interactive text
post-editing. Although MiNGESTURES is very
efficient and accurate, it is also very limited in
expressiveness. Only basic edition capabilities
are allowed (insertion, deletion, and substitu-
tion). Thus, advanced e-pen gestures cannot
be used to improve the efficiency of the re-
viewer.
On the other hand, there are applications
for post-editing text where user interactions
are leveraged to propagate text corrections to
the rest of the sentence. CueTIP (Shilman et
al., 2006), CATTI (Romero et al., 2009) and
IMT (Alabau et al., 2014) are the most ad-
vanced representatives of this kind of applica-
tions. These systems allow the user to cor-
rect text either in the form of unconstrained
cursive handwriting or (limited) pen gestures.
Then, the corrections are leveraged by the sys-
tem to provide smart auto-completion capa-
bilities. This way, user interaction is not only
taken into account to amend the proposed cor-
rection but other mistakes in the surrounding
text are automatically amended as well. How-
ever, user interaction is limited in these cases.
In CueTIP, only one handwritten character
can be submitted at a time and only 4 ges-
tures can be performed (join, split, delete, and
substitution). In CATTI, the user can hand-
write text freely but is still limited to perform
4 gestures as well (substitute, insert, delete,
and reject). Finally, IMT does not support
gestures other than substitution. Although
the auto-completion capability is a very inter-
esting and promising topic, it should not be
considered for reviewing: given the locality of
the small amount of changes that are probably
needed, auto-completion can make more harm
than good.
Thus, in light of the current limitations of
state-of-the-art approaches, in this work we
present an exploratory research of how paper-
like interaction should be approached to allow
proofreading translated texts.
</bodyText>
<sectionHeader confidence="0.928607" genericHeader="method">
3 A Taxonomy of Proofreading
Gestures
</sectionHeader>
<bodyText confidence="0.999934952380953">
Indicating text modifications on a sheet of
paper can be made in many different ways.
However, the lack of a consensus may lead
to misinterpretations. Fortunately, a series
of authoritative proofreading and copy-editing
symbols have been proposed (AMA, 2007;
CMO, 2010), even leading to an eventual stan-
dardization (BS, 2005; ISO, 1983).
We have studied the aforementioned author-
itative sources and have found that there is a
huge overlap in the proposed symbols, with
only minor variations. Moreover, such sym-
bols are meant to ease human-human com-
munication and therefore we need to adapt
them to ease human-computer communica-
tion. This way, we will focus on those sym-
bols that could be used to review using stroke-
based gestures. As such, we will study gestures
that allow to change the content and not the
formatting of the text. We can define the fol-
lowing high-level operations; see Figure 1:
</bodyText>
<listItem confidence="0.988196666666667">
Word change: change text’s written form.
Letter case: change word/character casing.
Punctuation: insert punctuation symbols.
Word combination: separate or join words.
Selection: select words or characters.
Text displacement: move text around.
</listItem>
<bodyText confidence="0.9999093">
It is worth noting that punctuation sym-
bols are represented explicitly in the litera-
ture, probably because of their importance in
copy-editing tasks. In addition, dot and hy-
phen symbols are represented differently from
other insertion symbols. The purpose of this
convention is to reduce visual ambiguity in hu-
man recognition. Finally, the selection opera-
tion is often devoted to spell out numbers or
abbreviations.
</bodyText>
<sectionHeader confidence="0.991053" genericHeader="method">
4 Preliminary Evaluation
</sectionHeader>
<bodyText confidence="0.999781">
The initial taxonomy (Figure 1) aims to be a
complete set of symbols for proofreading and
copy-editing onscreen. Nonetheless, the suc-
cess of these gestures will depend on the accu-
</bodyText>
<page confidence="0.994847">
11
</page>
<table confidence="0.7386145">
Word change Letter case Punctuation
DELETE INSERT TEXT LOWERCASE UPPERCASE CAMELCASE COMMA SEMICOLON COLON APOS QUOT DOT
Word combination Selection Text displacement
REMOVE SPACE INSERT SPACE HYPHEN ENCIRCLE MOVE SELECTION FORWARD MOVE SELECTION BACKWARD SWAP BLOCKS TRANSPOSE TEXT BLOCKS
</table>
<figureCaption confidence="0.99811">
Figure 1: Initial taxonomy, based on de facto proofreading symbols.
</figureCaption>
<bodyText confidence="0.995599">
racy of gesture recognizers, to correctly trans-
late gestures into commands.
As a first approach, we wanted to evalu-
ate these symbols with state-of-the-art gesture
recognizers. The initial taxonomy differs sig-
nificantly from other gesture sets in the liter-
ature (Anthony and Wobbrock, 2012; Vatavu
et al., 2012), in the sense that the symbols we
are researching are not expected to be drawn
in isolation. Instead, reviewers will issue a ges-
ture in a very specific context, and so a proof-
reading symbol may change its meaning. This
is specially true for symbols involving multiple
spans of text or block displacements: depend-
ing of the size of the span or the length of the
displacement, the aspect ratio and proportions
among the different parts of the gesture strokes
may vary. Thus, the final shape of the gesture
can be significantly different. An example is
given in Figure 2.
Lorem ipsum dolor sit amet
</bodyText>
<listItem confidence="0.7713935">
(a) MOVE FORWARD with 1 selected word and 2 word
displacement.
(b) MOVE FORWARD with 4 selected words and 1 word
displacement.
</listItem>
<figureCaption confidence="0.595314">
Figure 2: Examples of the same gesture ex-
ecuted with different proportions. As a re-
sult, the shapes of both gestures significantly
diverge from each other.
</figureCaption>
<subsectionHeader confidence="0.985047">
4.1 Gesture Samples Acquisition
</subsectionHeader>
<bodyText confidence="0.999766368421053">
We carried out a controlled study in a real-
world setup. We developed an application
that requested a set of random challenges to
the users (Figure 3). Then, we asked the
users if they would prefer to do the acquisi-
tion on a digitizer tablet or on a tablet com-
puter. On a 1 to 5 point scale, with 1 mean-
ing ‘I prefer writing with a digitizer pen’ and
5 ‘I prefer writing with a pen-capable tablet’,
users indicated that they would prefer a tablet
computer (M=4.6, SD=0.8). Consequently,
we deployed the application into a Lenovo
ThinkPad tablet, which had to be operated
with an e-pen. To make the paper-like ex-
perience more realistic, the touchscreen func-
tionality was disabled, so that users could rest
their hands on the screen. Eventually, 12 users
aged 24–36 submitted 5 times each gesture fol-
lowing the aforementioned random challenges.
</bodyText>
<figureCaption confidence="0.99915">
Figure 3: Acquisition application.
</figureCaption>
<subsectionHeader confidence="0.98977">
4.2 The Family of $ Recognizers
</subsectionHeader>
<bodyText confidence="0.9966045">
In HCI, there is a popular “dollar series”
of template-matching gesture recognizers, us-
ing a nearest-neighbor classifier with scoring
functions based on Euclidean distance. The
$ recognizers present several advantages over
other classifiers based on more complex pat-
tern recognition algorithms. First, $ recogniz-
ers are easily understandable and fast to in-
tegrate or re-implement in different program-
ming languages. Second, they do not depend
on large amounts of training data to achieve
Lorem ipsum dolor sit amet
</bodyText>
<page confidence="0.996931">
12
</page>
<bodyText confidence="0.999746631578948">
high accuracy, just on a small number of pre-
defined templates.
In particular, $N (Anthony and Wobbrock,
2012) and $P (Vatavu et al., 2012) can be
used to recognize multi-stroke gestures, so
they were the only suitable candidates to rec-
ognize our initial gesture taxonomy. On the
one hand, $N deals with multiple strokes by
recombining in every possible way the strokes
of the templates in order to generate new in-
stances of unistroke templates, and then ap-
ply either the $1 recognizer (Wobbrock et al.,
2007) or Protractor (Li, 2010). On the other
hand, $P considers gesture strokes as a cloud
of points, removing thus information about
stroke sequentiality. Then, the best match
is found using an approximation of the Hun-
garian algorithm, which pairs points from the
template with points of the query gesture.
</bodyText>
<subsectionHeader confidence="0.658447">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999979483870968">
We evaluated three fundamental aspects of
the recognition process: accuracy, recognition
time and memory requirements to store the
whole set of templates. Aiming for a portable
recognizer that could work on most everyday
devices, we decided to use a JavaScript (ro-
tation invariant) version of the $ family rec-
ognizers. Experiments were executed as a
nodejs program on a Ubuntu Linux computer
with a 2.83 GHz Intel QuadCoreTMand 4 GB
of RAM. We followed a leaving-one-out (LOO)
setup, i.e., each user’s set of gestures was used
as templates and tested against the rest of the
user’s gestures. All the values show the aver-
age of the different LOO runs.
Table 1 summarizes the experimental re-
sults. For the $N recognizer we found that,
by resampling to 32 points and 5 templates,
we can achieve very good recognition times
(0.7ms in average) but high recognition error
rate (23.6%). On the other hand, the $P rec-
ognizer behaves even worse, with 27.1% error
rate. Memory requirements are marginal but
recognition times increase more than one order
of magnitude.
It must be noted that the space needed by
$N to store just one template of n strokes is
n! × 2n times the space for the original tem-
plate (Vatavu et al., 2012). This is actually
a huge waste of resources. For instance, one
template of the INSERT SPACE gesture requires
</bodyText>
<table confidence="0.934179">
Recognizer Error Time Mem. usage
$N 23.6% 0.7 ms 102 MB
$P 27.1% 45 ms 1.8 MB
</table>
<tableCaption confidence="0.733395">
Table 1: Results for $N and $P recognizers,
with gestures resampled to 32 points and using
5 templates per gesture.
</tableCaption>
<bodyText confidence="0.9661688">
3840 times the original size, assuming that the
user has introduced the minimum strokes re-
quired. With a resampling 8 points, $N needs
almost 33MB of RAM to store 5 templates per
gesture.
</bodyText>
<subsectionHeader confidence="0.99527">
4.4 Error analysis
</subsectionHeader>
<bodyText confidence="0.99987704">
Surprised by the high error rates we decided
to delve into the results of the most accurate
setup so we could find the source of errors.
We observed that the most difficult gesture
to recognize was REMOVE SPACE, which rep-
resented 12% of the total number of errors;
being confused with COMMA and SEMI COLON
more than 50% of the time, probably because
they are formed by two arcs. It was also con-
fused, though less frequently, with MOVE SE-
LECTION FORWARD/BACKWARD. These ges-
tures, excepting the circle part, are also com-
posed by two arcs.
On the other hand, punctuation symbols ac-
counted for 37% of the errors, being mostly
confused with each other, as they have very
similar shapes. Finally, some errors are harder
to dissect. For instance, UPPERCASE was
confused mainly with both MOVE SELECTION
(4.4% of the errors), and punctuation and dis-
placement operations were also confused with
each other at some time, despite their very dif-
ferent visual shapes and sizes. We suspect it
is because of the internal normalization proce-
dures of the $ recognizers.
</bodyText>
<sectionHeader confidence="0.997106" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999912875">
Our results suggest that the $ family of gesture
recognizers, although popular, are not appro-
priate for proofreading translated texts. Our
assumption is that the normalization proce-
dures of these recognizers—mainly scaling and
resampling— are not appropriate to gestures
for which the proportions of its constituent
parts may vary according to the context. For
</bodyText>
<page confidence="0.998742">
13
</page>
<figureCaption confidence="0.961809">
Figure 4: One proposal for gesture set sim-
plification. A Pop-up menu could assist the
user to disambiguate among perceptually sim-
ilar gestures.
</figureCaption>
<bodyText confidence="0.999796432432433">
example, after resizing a MOVE SELECTION
FORWARD that selects a small word and has
a long arrow, the final shape would be primar-
ily that of the arrow (Figure 2).
In the light of this analysis, several actions
can be taken for future work. Firstly, other
gesture recognizers should be explored that
can deal with stroke sequences without resam-
pling (Myers and Rabiner, 1981; Sezgin and
Davis, 2005; ´Alvaro et al., 2013). However, it
must be remarked that response time is crucial
to ensure an adequate user experience. There-
fore, the underlying algorithms should be im-
plementable on thin clients, such as mobile de-
vices, with reasonable recognition times.
Secondly, it would be also necessary to re-
duce the set of gestures, but not at the expense
of reducing also expressiveness as Leiva et al.
(2013) did. For instance, taking advantage of
the interaction that computers can provide, we
can group punctuation operations, SPACE, and
INSERT HYPHEN all into INSERT ABOVE and
BELOW gestures. Both gestures would pop-
up a menu where the user could select deter-
ministically the symbol to insert; see Figure 4.
In the same manner, letter casing operations
could be grouped into a single SELECTION cat-
egory, which would also provide a contextual
menu to trigger the right command. The re-
sulting set of gestures should be, in principle,
much easier to recognize.
Additionally, the current set of proofread-
ing gestures present further challenges. For
instance, we would need to identify the seman-
tics of the gestures, i.e., which elements in the
text are affected by the gesture and how the
system should proceed to accomplish the task.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999909444444445">
In this work we have defined a set of gestures
that is suitable for the reviewing process of
human-translated text. We have performed
an evaluation on gestures generated by real
users that show that popular recognizers are
not able to achieve a satisfactory accuracy. In
consequence, we have identified a series of ar-
eas for improvement that could make e-pen
devices realizable in the near future.
</bodyText>
<sectionHeader confidence="0.997373" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.987101">
This work is supported by the 7th Frame-
work Program of European Commission un-
der grant agreements 287576 (CasMaCat) and
600707 (tranScriptorium).
</bodyText>
<sectionHeader confidence="0.995362" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880896551724">
V. Alabau, A. Sanchis, and F. Casacuberta. 2014.
Improving on-line handwritten recognition in in-
teractive machine translation. Pattern Recogni-
tion, 47(3):1217–1228.
2007. AMA manual of style: A guide for authors
and editors. 10th ed. Oxford University Press.
L. Anthony and J. O. Wobbrock. 2012. $N-
protractor: a fast and accurate multistroke rec-
ognizer. In Proc. GI, pages 117–120.
2005. BS 5261-2:2005. Copy preparation and proof
correction.
2010. The Chicago manual of style. 16th ed. Uni-
versity Of Chicago Press.
M. L. Coleman. 1969. Text editing on a graphic
display device using hand-drawn proofreader’s
symbols. In Pertinent Concepts in Computer
Graphics, Proc. 2nd Univ. Illinois Conf. on
Computer Graphics, pages 283–290.
1983. ISO 5776:1983. Symbols for text correction.
L. A. Leiva, V. Alabau, and E. Vidal. 2013. Error-
proof, high-performance, and context-aware ges-
tures for interactive text edition. In Proc. CHI
EA, pages 1227–1232.
Y. Li. 2010. Protractor: a fast and accurate ges-
ture recognizer. In Proc. CHI, pages 2169–2172.
C. S. Myers and L. R. Rabiner. 1981. A compar-
ative study of several dynamic time-warping al-
gorithms for connected-word. Bell System Tech-
nical Journal.
</reference>
<page confidence="0.98419">
14
</page>
<reference confidence="0.99996992">
V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.
2009. Interactive multimodal transcription of
text images using a web-based demo system. In
Proc. IUI, pages 477–478.
D. Rubine. 1991. Specifying gestures by example.
In Proc. SIGGRAPH, pages 329–337.
T. M. Sezgin and R. Davis. 2005. HMM-based
efficient sketch recognition. In Proc. IUI, pages
281–283.
M. Shilman, D. S. Tan, and P. Simard. 2006.
CueTIP: a mixed-initiative interface for correct-
ing handwriting errors. In Proc. UIST, pages
323–332.
R. D. Vatavu, L. Anthony, and J. O. Wobbrock.
2012. Gestures as point clouds: A $P recognizer
for user interface prototypes. In Proc. ICMI,
pages 273–280.
J. O. Wobbrock, A. D. Wilson, and Y. Li. 2007.
Gestures without libraries, toolkits or training:
A $1 recognizer for user interface prototypes. In
Proc. UIST, pages 159–168.
F. ´Alvaro, J.-A. S´anchez, and J.-M. Benedi. 2013.
Classification of on-line mathematical symbols
with hybrid features and recurrent neural net-
works. In Proc. ICDAR, pages 1012–1016.
</reference>
<page confidence="0.997936">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.730160">
<title confidence="0.884253">Proofreading Human Translations with an E-pen</title>
<author confidence="0.767739">A Alabau</author>
<affiliation confidence="0.9858225">PRHLT Research Universitat Polit`ecnica de</affiliation>
<abstract confidence="0.999697608695652">Proofreading translated text is a task aimed at checking for correctness, consistency, and appropriate writing style. While this has been typically done with a keyboard and a mouse, pen-based devices set an opportunity for making such corrections in a comfortable way, as if proofreading on physical paper. Arguably, this way of interacting with a computer is very appropriate when a small number of modifications are required to achieve high-quality standards. In this paper, we propose a taxonomy of pen gestures that is tailored to machine translation review tasks, after human translator intervention. In addition, we evaluate the recognition accuracy of these gestures using a couple of popular gesture recognizers. Finally, we comment on open challenges and limitations, and discuss possible avenues for future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Alabau</author>
<author>A Sanchis</author>
<author>F Casacuberta</author>
</authors>
<title>Improving on-line handwritten recognition in interactive machine translation.</title>
<date>2014</date>
<journal>Pattern Recognition,</journal>
<volume>47</volume>
<issue>3</issue>
<contexts>
<context position="4641" citStr="Alabau et al., 2014" startWordPosition="721" endWordPosition="724"> (Leiva et al., 2013), which proposes a simplified set of gestures for interactive text post-editing. Although MiNGESTURES is very efficient and accurate, it is also very limited in expressiveness. Only basic edition capabilities are allowed (insertion, deletion, and substitution). Thus, advanced e-pen gestures cannot be used to improve the efficiency of the reviewer. On the other hand, there are applications for post-editing text where user interactions are leveraged to propagate text corrections to the rest of the sentence. CueTIP (Shilman et al., 2006), CATTI (Romero et al., 2009) and IMT (Alabau et al., 2014) are the most advanced representatives of this kind of applications. These systems allow the user to correct text either in the form of unconstrained cursive handwriting or (limited) pen gestures. Then, the corrections are leveraged by the system to provide smart auto-completion capabilities. This way, user interaction is not only taken into account to amend the proposed correction but other mistakes in the surrounding text are automatically amended as well. However, user interaction is limited in these cases. In CueTIP, only one handwritten character can be submitted at a time and only 4 gest</context>
</contexts>
<marker>Alabau, Sanchis, Casacuberta, 2014</marker>
<rawString>V. Alabau, A. Sanchis, and F. Casacuberta. 2014. Improving on-line handwritten recognition in interactive machine translation. Pattern Recognition, 47(3):1217–1228.</rawString>
</citation>
<citation valid="true">
<title>AMA manual of style: A guide for authors and editors. 10th ed.</title>
<date>2007</date>
<publisher>Oxford University Press.</publisher>
<marker>2007</marker>
<rawString>2007. AMA manual of style: A guide for authors and editors. 10th ed. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Anthony</author>
<author>J O Wobbrock</author>
</authors>
<title>Nprotractor: a fast and accurate multistroke recognizer.</title>
<date>2012</date>
<booktitle>In Proc. GI,</booktitle>
<pages>117--120</pages>
<contexts>
<context position="8412" citStr="Anthony and Wobbrock, 2012" startWordPosition="1312" endWordPosition="1315">case Punctuation DELETE INSERT TEXT LOWERCASE UPPERCASE CAMELCASE COMMA SEMICOLON COLON APOS QUOT DOT Word combination Selection Text displacement REMOVE SPACE INSERT SPACE HYPHEN ENCIRCLE MOVE SELECTION FORWARD MOVE SELECTION BACKWARD SWAP BLOCKS TRANSPOSE TEXT BLOCKS Figure 1: Initial taxonomy, based on de facto proofreading symbols. racy of gesture recognizers, to correctly translate gestures into commands. As a first approach, we wanted to evaluate these symbols with state-of-the-art gesture recognizers. The initial taxonomy differs significantly from other gesture sets in the literature (Anthony and Wobbrock, 2012; Vatavu et al., 2012), in the sense that the symbols we are researching are not expected to be drawn in isolation. Instead, reviewers will issue a gesture in a very specific context, and so a proofreading symbol may change its meaning. This is specially true for symbols involving multiple spans of text or block displacements: depending of the size of the span or the length of the displacement, the aspect ratio and proportions among the different parts of the gesture strokes may vary. Thus, the final shape of the gesture can be significantly different. An example is given in Figure 2. Lorem ip</context>
<context position="10882" citStr="Anthony and Wobbrock, 2012" startWordPosition="1727" endWordPosition="1730">In HCI, there is a popular “dollar series” of template-matching gesture recognizers, using a nearest-neighbor classifier with scoring functions based on Euclidean distance. The $ recognizers present several advantages over other classifiers based on more complex pattern recognition algorithms. First, $ recognizers are easily understandable and fast to integrate or re-implement in different programming languages. Second, they do not depend on large amounts of training data to achieve Lorem ipsum dolor sit amet 12 high accuracy, just on a small number of predefined templates. In particular, $N (Anthony and Wobbrock, 2012) and $P (Vatavu et al., 2012) can be used to recognize multi-stroke gestures, so they were the only suitable candidates to recognize our initial gesture taxonomy. On the one hand, $N deals with multiple strokes by recombining in every possible way the strokes of the templates in order to generate new instances of unistroke templates, and then apply either the $1 recognizer (Wobbrock et al., 2007) or Protractor (Li, 2010). On the other hand, $P considers gesture strokes as a cloud of points, removing thus information about stroke sequentiality. Then, the best match is found using an approximati</context>
</contexts>
<marker>Anthony, Wobbrock, 2012</marker>
<rawString>L. Anthony and J. O. Wobbrock. 2012. $Nprotractor: a fast and accurate multistroke recognizer. In Proc. GI, pages 117–120.</rawString>
</citation>
<citation valid="true">
<title>BS 5261-2:2005. Copy preparation and proof correction.</title>
<date>2005</date>
<marker>2005</marker>
<rawString>2005. BS 5261-2:2005. Copy preparation and proof correction.</rawString>
</citation>
<citation valid="true">
<title>The Chicago manual of style. 16th ed.</title>
<date>2010</date>
<publisher>University Of Chicago Press.</publisher>
<marker>2010</marker>
<rawString>2010. The Chicago manual of style. 16th ed. University Of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Coleman</author>
</authors>
<title>Text editing on a graphic display device using hand-drawn proofreader’s symbols.</title>
<date>1969</date>
<booktitle>In Pertinent Concepts in Computer Graphics, Proc. 2nd Univ. Illinois Conf. on Computer Graphics,</booktitle>
<pages>283--290</pages>
<contexts>
<context position="2510" citStr="Coleman, 1969" startWordPosition="392" endWordPosition="394">ons can be operated today in a similar way people interact with pen and paper. This way of reviewing is arguably more natural and efficient than a keyboard or a mouse, since the e-pen can be used both to locate and correct an erroneous word, all at once. Additionally, the expressiveness of e-pen interaction provides an opportunity to integrate useful gestures that are able correct other common mistakes, such as word reordering or capitalization. 2 Related Work The first attempt that we are aware of to postedit text with an e-pen interface dates back to the early seventies of the past century (Coleman, 1969). In that work, Coleman proposed a set of unistroke gestures for post-editing. Later on, the same corpus was used by (Rubine, 1991) in his seminal work about gesture recognition with excellent recognition results. However, the gesture set is too simplistic to be used in a real translation task today. Most of the modern applications to generate and edit textual content using “digital ink” are based on ad-hoc interaction protocols and often do not ship handwriting recognition software. To our knowledge, MyScript Notes Mobile• is the closest system to provide a natural onscreen paper-like interac</context>
</contexts>
<marker>Coleman, 1969</marker>
<rawString>M. L. Coleman. 1969. Text editing on a graphic display device using hand-drawn proofreader’s symbols. In Pertinent Concepts in Computer Graphics, Proc. 2nd Univ. Illinois Conf. on Computer Graphics, pages 283–290. 1983. ISO 5776:1983. Symbols for text correction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Leiva</author>
<author>V Alabau</author>
<author>E Vidal</author>
</authors>
<title>Errorproof, high-performance, and context-aware gestures for interactive text edition.</title>
<date>2013</date>
<booktitle>In Proc. CHI EA,</booktitle>
<pages>1227--1232</pages>
<contexts>
<context position="4042" citStr="Leiva et al., 2013" startWordPosition="629" endWordPosition="632">s and Computer-assisted Translation, pages 10–15, Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics nition. For instance, to insert a new word in the middle of a sentence the user needs to make room for space explicitly (i.e., if the word has N characters, the user needs to perform an Insert Space gesture N times). Moreover, the produced text does not flow on the UI, i.e., it is fixed to the position of the ink, which makes it difficult to modify. As a result, this system does not seem suitable for reviewing translations. Other comparable work is MiNGESTURES (Leiva et al., 2013), which proposes a simplified set of gestures for interactive text post-editing. Although MiNGESTURES is very efficient and accurate, it is also very limited in expressiveness. Only basic edition capabilities are allowed (insertion, deletion, and substitution). Thus, advanced e-pen gestures cannot be used to improve the efficiency of the reviewer. On the other hand, there are applications for post-editing text where user interactions are leveraged to propagate text corrections to the rest of the sentence. CueTIP (Shilman et al., 2006), CATTI (Romero et al., 2009) and IMT (Alabau et al., 2014) </context>
<context position="15698" citStr="Leiva et al. (2013)" startWordPosition="2545" endWordPosition="2548"> analysis, several actions can be taken for future work. Firstly, other gesture recognizers should be explored that can deal with stroke sequences without resampling (Myers and Rabiner, 1981; Sezgin and Davis, 2005; ´Alvaro et al., 2013). However, it must be remarked that response time is crucial to ensure an adequate user experience. Therefore, the underlying algorithms should be implementable on thin clients, such as mobile devices, with reasonable recognition times. Secondly, it would be also necessary to reduce the set of gestures, but not at the expense of reducing also expressiveness as Leiva et al. (2013) did. For instance, taking advantage of the interaction that computers can provide, we can group punctuation operations, SPACE, and INSERT HYPHEN all into INSERT ABOVE and BELOW gestures. Both gestures would popup a menu where the user could select deterministically the symbol to insert; see Figure 4. In the same manner, letter casing operations could be grouped into a single SELECTION category, which would also provide a contextual menu to trigger the right command. The resulting set of gestures should be, in principle, much easier to recognize. Additionally, the current set of proofreading g</context>
</contexts>
<marker>Leiva, Alabau, Vidal, 2013</marker>
<rawString>L. A. Leiva, V. Alabau, and E. Vidal. 2013. Errorproof, high-performance, and context-aware gestures for interactive text edition. In Proc. CHI EA, pages 1227–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
</authors>
<title>Protractor: a fast and accurate gesture recognizer.</title>
<date>2010</date>
<booktitle>In Proc. CHI,</booktitle>
<pages>2169--2172</pages>
<contexts>
<context position="11306" citStr="Li, 2010" startWordPosition="1802" endWordPosition="1803"> on large amounts of training data to achieve Lorem ipsum dolor sit amet 12 high accuracy, just on a small number of predefined templates. In particular, $N (Anthony and Wobbrock, 2012) and $P (Vatavu et al., 2012) can be used to recognize multi-stroke gestures, so they were the only suitable candidates to recognize our initial gesture taxonomy. On the one hand, $N deals with multiple strokes by recombining in every possible way the strokes of the templates in order to generate new instances of unistroke templates, and then apply either the $1 recognizer (Wobbrock et al., 2007) or Protractor (Li, 2010). On the other hand, $P considers gesture strokes as a cloud of points, removing thus information about stroke sequentiality. Then, the best match is found using an approximation of the Hungarian algorithm, which pairs points from the template with points of the query gesture. 4.3 Results We evaluated three fundamental aspects of the recognition process: accuracy, recognition time and memory requirements to store the whole set of templates. Aiming for a portable recognizer that could work on most everyday devices, we decided to use a JavaScript (rotation invariant) version of the $ family reco</context>
</contexts>
<marker>Li, 2010</marker>
<rawString>Y. Li. 2010. Protractor: a fast and accurate gesture recognizer. In Proc. CHI, pages 2169–2172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Myers</author>
<author>L R Rabiner</author>
</authors>
<title>A comparative study of several dynamic time-warping algorithms for connected-word. Bell System</title>
<date>1981</date>
<tech>Technical Journal.</tech>
<contexts>
<context position="15269" citStr="Myers and Rabiner, 1981" startWordPosition="2473" endWordPosition="2476">e to gestures for which the proportions of its constituent parts may vary according to the context. For 13 Figure 4: One proposal for gesture set simplification. A Pop-up menu could assist the user to disambiguate among perceptually similar gestures. example, after resizing a MOVE SELECTION FORWARD that selects a small word and has a long arrow, the final shape would be primarily that of the arrow (Figure 2). In the light of this analysis, several actions can be taken for future work. Firstly, other gesture recognizers should be explored that can deal with stroke sequences without resampling (Myers and Rabiner, 1981; Sezgin and Davis, 2005; ´Alvaro et al., 2013). However, it must be remarked that response time is crucial to ensure an adequate user experience. Therefore, the underlying algorithms should be implementable on thin clients, such as mobile devices, with reasonable recognition times. Secondly, it would be also necessary to reduce the set of gestures, but not at the expense of reducing also expressiveness as Leiva et al. (2013) did. For instance, taking advantage of the interaction that computers can provide, we can group punctuation operations, SPACE, and INSERT HYPHEN all into INSERT ABOVE and</context>
</contexts>
<marker>Myers, Rabiner, 1981</marker>
<rawString>C. S. Myers and L. R. Rabiner. 1981. A comparative study of several dynamic time-warping algorithms for connected-word. Bell System Technical Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Romero</author>
<author>L A Leiva</author>
<author>A H Toselli</author>
<author>E Vidal</author>
</authors>
<title>Interactive multimodal transcription of text images using a web-based demo system.</title>
<date>2009</date>
<booktitle>In Proc. IUI,</booktitle>
<pages>477--478</pages>
<contexts>
<context position="4611" citStr="Romero et al., 2009" startWordPosition="715" endWordPosition="718">comparable work is MiNGESTURES (Leiva et al., 2013), which proposes a simplified set of gestures for interactive text post-editing. Although MiNGESTURES is very efficient and accurate, it is also very limited in expressiveness. Only basic edition capabilities are allowed (insertion, deletion, and substitution). Thus, advanced e-pen gestures cannot be used to improve the efficiency of the reviewer. On the other hand, there are applications for post-editing text where user interactions are leveraged to propagate text corrections to the rest of the sentence. CueTIP (Shilman et al., 2006), CATTI (Romero et al., 2009) and IMT (Alabau et al., 2014) are the most advanced representatives of this kind of applications. These systems allow the user to correct text either in the form of unconstrained cursive handwriting or (limited) pen gestures. Then, the corrections are leveraged by the system to provide smart auto-completion capabilities. This way, user interaction is not only taken into account to amend the proposed correction but other mistakes in the surrounding text are automatically amended as well. However, user interaction is limited in these cases. In CueTIP, only one handwritten character can be submi</context>
</contexts>
<marker>Romero, Leiva, Toselli, Vidal, 2009</marker>
<rawString>V. Romero, L. A. Leiva, A. H. Toselli, and E. Vidal. 2009. Interactive multimodal transcription of text images using a web-based demo system. In Proc. IUI, pages 477–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Rubine</author>
</authors>
<title>Specifying gestures by example.</title>
<date>1991</date>
<booktitle>In Proc. SIGGRAPH,</booktitle>
<pages>329--337</pages>
<contexts>
<context position="2641" citStr="Rubine, 1991" startWordPosition="415" endWordPosition="417">ficient than a keyboard or a mouse, since the e-pen can be used both to locate and correct an erroneous word, all at once. Additionally, the expressiveness of e-pen interaction provides an opportunity to integrate useful gestures that are able correct other common mistakes, such as word reordering or capitalization. 2 Related Work The first attempt that we are aware of to postedit text with an e-pen interface dates back to the early seventies of the past century (Coleman, 1969). In that work, Coleman proposed a set of unistroke gestures for post-editing. Later on, the same corpus was used by (Rubine, 1991) in his seminal work about gesture recognition with excellent recognition results. However, the gesture set is too simplistic to be used in a real translation task today. Most of the modern applications to generate and edit textual content using “digital ink” are based on ad-hoc interaction protocols and often do not ship handwriting recognition software. To our knowledge, MyScript Notes Mobile• is the closest system to provide a natural onscreen paper-like interaction style, including some text-editing gestures and a powerful handwriting recognition software. However, this application relies </context>
</contexts>
<marker>Rubine, 1991</marker>
<rawString>D. Rubine. 1991. Specifying gestures by example. In Proc. SIGGRAPH, pages 329–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Sezgin</author>
<author>R Davis</author>
</authors>
<title>HMM-based efficient sketch recognition.</title>
<date>2005</date>
<booktitle>In Proc. IUI,</booktitle>
<pages>281--283</pages>
<contexts>
<context position="15293" citStr="Sezgin and Davis, 2005" startWordPosition="2477" endWordPosition="2480">he proportions of its constituent parts may vary according to the context. For 13 Figure 4: One proposal for gesture set simplification. A Pop-up menu could assist the user to disambiguate among perceptually similar gestures. example, after resizing a MOVE SELECTION FORWARD that selects a small word and has a long arrow, the final shape would be primarily that of the arrow (Figure 2). In the light of this analysis, several actions can be taken for future work. Firstly, other gesture recognizers should be explored that can deal with stroke sequences without resampling (Myers and Rabiner, 1981; Sezgin and Davis, 2005; ´Alvaro et al., 2013). However, it must be remarked that response time is crucial to ensure an adequate user experience. Therefore, the underlying algorithms should be implementable on thin clients, such as mobile devices, with reasonable recognition times. Secondly, it would be also necessary to reduce the set of gestures, but not at the expense of reducing also expressiveness as Leiva et al. (2013) did. For instance, taking advantage of the interaction that computers can provide, we can group punctuation operations, SPACE, and INSERT HYPHEN all into INSERT ABOVE and BELOW gestures. Both ge</context>
</contexts>
<marker>Sezgin, Davis, 2005</marker>
<rawString>T. M. Sezgin and R. Davis. 2005. HMM-based efficient sketch recognition. In Proc. IUI, pages 281–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Shilman</author>
<author>D S Tan</author>
<author>P Simard</author>
</authors>
<title>CueTIP: a mixed-initiative interface for correcting handwriting errors.</title>
<date>2006</date>
<booktitle>In Proc. UIST,</booktitle>
<pages>323--332</pages>
<contexts>
<context position="4582" citStr="Shilman et al., 2006" startWordPosition="710" endWordPosition="713">reviewing translations. Other comparable work is MiNGESTURES (Leiva et al., 2013), which proposes a simplified set of gestures for interactive text post-editing. Although MiNGESTURES is very efficient and accurate, it is also very limited in expressiveness. Only basic edition capabilities are allowed (insertion, deletion, and substitution). Thus, advanced e-pen gestures cannot be used to improve the efficiency of the reviewer. On the other hand, there are applications for post-editing text where user interactions are leveraged to propagate text corrections to the rest of the sentence. CueTIP (Shilman et al., 2006), CATTI (Romero et al., 2009) and IMT (Alabau et al., 2014) are the most advanced representatives of this kind of applications. These systems allow the user to correct text either in the form of unconstrained cursive handwriting or (limited) pen gestures. Then, the corrections are leveraged by the system to provide smart auto-completion capabilities. This way, user interaction is not only taken into account to amend the proposed correction but other mistakes in the surrounding text are automatically amended as well. However, user interaction is limited in these cases. In CueTIP, only one handw</context>
</contexts>
<marker>Shilman, Tan, Simard, 2006</marker>
<rawString>M. Shilman, D. S. Tan, and P. Simard. 2006. CueTIP: a mixed-initiative interface for correcting handwriting errors. In Proc. UIST, pages 323–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Vatavu</author>
<author>L Anthony</author>
<author>J O Wobbrock</author>
</authors>
<title>Gestures as point clouds: A $P recognizer for user interface prototypes.</title>
<date>2012</date>
<booktitle>In Proc. ICMI,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="8434" citStr="Vatavu et al., 2012" startWordPosition="1316" endWordPosition="1319">RT TEXT LOWERCASE UPPERCASE CAMELCASE COMMA SEMICOLON COLON APOS QUOT DOT Word combination Selection Text displacement REMOVE SPACE INSERT SPACE HYPHEN ENCIRCLE MOVE SELECTION FORWARD MOVE SELECTION BACKWARD SWAP BLOCKS TRANSPOSE TEXT BLOCKS Figure 1: Initial taxonomy, based on de facto proofreading symbols. racy of gesture recognizers, to correctly translate gestures into commands. As a first approach, we wanted to evaluate these symbols with state-of-the-art gesture recognizers. The initial taxonomy differs significantly from other gesture sets in the literature (Anthony and Wobbrock, 2012; Vatavu et al., 2012), in the sense that the symbols we are researching are not expected to be drawn in isolation. Instead, reviewers will issue a gesture in a very specific context, and so a proofreading symbol may change its meaning. This is specially true for symbols involving multiple spans of text or block displacements: depending of the size of the span or the length of the displacement, the aspect ratio and proportions among the different parts of the gesture strokes may vary. Thus, the final shape of the gesture can be significantly different. An example is given in Figure 2. Lorem ipsum dolor sit amet (a)</context>
<context position="10911" citStr="Vatavu et al., 2012" startWordPosition="1733" endWordPosition="1736">eries” of template-matching gesture recognizers, using a nearest-neighbor classifier with scoring functions based on Euclidean distance. The $ recognizers present several advantages over other classifiers based on more complex pattern recognition algorithms. First, $ recognizers are easily understandable and fast to integrate or re-implement in different programming languages. Second, they do not depend on large amounts of training data to achieve Lorem ipsum dolor sit amet 12 high accuracy, just on a small number of predefined templates. In particular, $N (Anthony and Wobbrock, 2012) and $P (Vatavu et al., 2012) can be used to recognize multi-stroke gestures, so they were the only suitable candidates to recognize our initial gesture taxonomy. On the one hand, $N deals with multiple strokes by recombining in every possible way the strokes of the templates in order to generate new instances of unistroke templates, and then apply either the $1 recognizer (Wobbrock et al., 2007) or Protractor (Li, 2010). On the other hand, $P considers gesture strokes as a cloud of points, removing thus information about stroke sequentiality. Then, the best match is found using an approximation of the Hungarian algorithm</context>
<context position="12813" citStr="Vatavu et al., 2012" startWordPosition="2057" endWordPosition="2060">he values show the average of the different LOO runs. Table 1 summarizes the experimental results. For the $N recognizer we found that, by resampling to 32 points and 5 templates, we can achieve very good recognition times (0.7ms in average) but high recognition error rate (23.6%). On the other hand, the $P recognizer behaves even worse, with 27.1% error rate. Memory requirements are marginal but recognition times increase more than one order of magnitude. It must be noted that the space needed by $N to store just one template of n strokes is n! × 2n times the space for the original template (Vatavu et al., 2012). This is actually a huge waste of resources. For instance, one template of the INSERT SPACE gesture requires Recognizer Error Time Mem. usage $N 23.6% 0.7 ms 102 MB $P 27.1% 45 ms 1.8 MB Table 1: Results for $N and $P recognizers, with gestures resampled to 32 points and using 5 templates per gesture. 3840 times the original size, assuming that the user has introduced the minimum strokes required. With a resampling 8 points, $N needs almost 33MB of RAM to store 5 templates per gesture. 4.4 Error analysis Surprised by the high error rates we decided to delve into the results of the most accura</context>
</contexts>
<marker>Vatavu, Anthony, Wobbrock, 2012</marker>
<rawString>R. D. Vatavu, L. Anthony, and J. O. Wobbrock. 2012. Gestures as point clouds: A $P recognizer for user interface prototypes. In Proc. ICMI, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J O Wobbrock</author>
<author>A D Wilson</author>
<author>Y Li</author>
</authors>
<title>Gestures without libraries, toolkits or training: A $1 recognizer for user interface prototypes.</title>
<date>2007</date>
<booktitle>In Proc. UIST,</booktitle>
<pages>159--168</pages>
<contexts>
<context position="11281" citStr="Wobbrock et al., 2007" startWordPosition="1796" endWordPosition="1799"> languages. Second, they do not depend on large amounts of training data to achieve Lorem ipsum dolor sit amet 12 high accuracy, just on a small number of predefined templates. In particular, $N (Anthony and Wobbrock, 2012) and $P (Vatavu et al., 2012) can be used to recognize multi-stroke gestures, so they were the only suitable candidates to recognize our initial gesture taxonomy. On the one hand, $N deals with multiple strokes by recombining in every possible way the strokes of the templates in order to generate new instances of unistroke templates, and then apply either the $1 recognizer (Wobbrock et al., 2007) or Protractor (Li, 2010). On the other hand, $P considers gesture strokes as a cloud of points, removing thus information about stroke sequentiality. Then, the best match is found using an approximation of the Hungarian algorithm, which pairs points from the template with points of the query gesture. 4.3 Results We evaluated three fundamental aspects of the recognition process: accuracy, recognition time and memory requirements to store the whole set of templates. Aiming for a portable recognizer that could work on most everyday devices, we decided to use a JavaScript (rotation invariant) ver</context>
</contexts>
<marker>Wobbrock, Wilson, Li, 2007</marker>
<rawString>J. O. Wobbrock, A. D. Wilson, and Y. Li. 2007. Gestures without libraries, toolkits or training: A $1 recognizer for user interface prototypes. In Proc. UIST, pages 159–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F ´Alvaro</author>
<author>J-A S´anchez</author>
<author>J-M Benedi</author>
</authors>
<title>Classification of on-line mathematical symbols with hybrid features and recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proc. ICDAR,</booktitle>
<pages>1012--1016</pages>
<marker>´Alvaro, S´anchez, Benedi, 2013</marker>
<rawString>F. ´Alvaro, J.-A. S´anchez, and J.-M. Benedi. 2013. Classification of on-line mathematical symbols with hybrid features and recurrent neural networks. In Proc. ICDAR, pages 1012–1016.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
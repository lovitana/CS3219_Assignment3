<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001178">
<title confidence="0.997387">
Weakly-Supervised Bayesian Learning of a CCG Supertagger
</title>
<author confidence="0.999557">
Dan Garrette* Chris Dyer† Jason Baldridge$ Noah A. Smith†
</author>
<affiliation confidence="0.998971666666667">
*Department of Computer Science, The University of Texas at Austin
†School of Computer Science, Carnegie Mellon University
$Department of Linguistics, The University of Texas at Austin
</affiliation>
<email confidence="0.956233">
*Corresponding author: dhg@cs.utexas.edu
</email>
<sectionHeader confidence="0.994046" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999816">
We present a Bayesian formulation for
weakly-supervised learning of a Combina-
tory Categorial Grammar (CCG) supertag-
ger with an HMM. We assume supervi-
sion in the form of a tag dictionary, and
our prior encourages the use of cross-
linguistically common category structures
as well as transitions between tags that
can combine locally according to CCG’s
combinators. Our prior is theoretically ap-
pealing since it is motivated by language-
independent, universal properties of the
CCG formalism. Empirically, we show
that it yields substantial improvements
over previous work that used similar bi-
ases to initialize an EM-based learner. Ad-
ditional gains are obtained by further shap-
ing the prior with corpus-specific informa-
tion that is extracted automatically from
raw text and a tag dictionary.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932189655172">
Unsupervised part-of-speech (POS) induction is a
classic problem in NLP. Many proposed solutions
are based on Hidden Markov models (HMMs), with
various improvements obtainable through: induc-
tive bias in the form of tag dictionaries (Kupiec,
1992; Merialdo, 1994), sparsity constraints (Lee
et al., 2010), careful initialization of parameters
(Goldberg et al., 2008), feature based represen-
tations (Berg-Kirkpatrick et al., 2010; Smith and
Eisner, 2005), and priors on model parameters
(Johnson, 2007; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011, inter alia).
When tag dictionaries are available, a situa-
tion we will call type-supervision, POS induc-
tion from unlabeled corpora can be relatively suc-
cessful; however, as the number of possible tags
increases, performance drops (Ravi and Knight,
2009). In such cases, there are a large number
of possible labels for each token, so picking the
right one simply by chance is unlikely; the pa-
rameter space tends to be large; and devising good
initial parameters is difficult. Therefore, it is un-
surprising that the unsupervised (or even weakly-
supervised) learning of a Combinatory Categorial
Grammar (CCG) supertagger, which labels each
word with one of a large (possibly unbounded)
number of structured categories called supertags,
is a considerable challenge.
Despite the apparent complexity of the task, su-
pertag sequences have regularities due to univer-
sal properties of the CCG formalism (§2) that can
be used to reduce the complexity of the problem;
previous work showed promising results by using
these regularities to initialize an HMM that is then
refined with EM (Baldridge, 2008). Here, we ex-
ploit CCG’s category structure to motivate a novel
prior over HMM parameters for use in Bayesian
learning (§3). This prior encourages (i) cross-
linguistically common tag types, (ii) tag bigrams
that can combine using CCG’s combinators, and
(iii) sparse transition distributions. We also go be-
yond the use of these universals to show how ad-
ditional, corpus-specific information can be auto-
matically extracted from a combination of the tag
dictionary and raw data, and how that information
can be combined with the universal knowledge for
integration into the model to improve the prior.
We use a blocked sampling algorithm to sam-
ple supertag sequences for the sentences in the
training data, proportional to their posterior prob-
ability (§4). We experimentally verify that
our Bayesian formulation is effective and sub-
stantially outperforms the state-of-the-art base-
line initialization/EM strategy in several languages
(§5). We also evaluate using tag dictionaries that
are unpruned and have only partial word coverage,
finding even greater improvements in these more
realistic scenarios.
</bodyText>
<page confidence="0.978382">
141
</page>
<note confidence="0.7412795">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.555754" genericHeader="introduction">
2 CCG and Supertagging
</sectionHeader>
<bodyText confidence="0.9902302">
CCG (Steedman, 2000; Steedman and Baldridge,
2011) is a grammar formalism in which each lex-
ical token is associated with a structured category,
often referred to as a supertag. CCG categories are
defined by the following recursive definition:
</bodyText>
<equation confidence="0.947111">
C → {S, N, NP, PP, ...}
C → {C/C, C\C}
</equation>
<bodyText confidence="0.996743095238095">
A CCG category can either be an atomic cate-
gory indicating a particular type of basic gram-
matical phrase (S for a sentence, N for a noun,
NP for a noun phrase, etc), or a complex category
formed from the combination of two categories
by one of two slash operators. In CCG, complex
categories indicate a grammatical relationship be-
tween the two operands. For example, the cate-
gory (S\NP)/NP might describe a transitive verb,
looking first to its right (indicated by /) for an ob-
ject, then to its left (\) for a subject, to produce a
sentence. Further, atomic categories may be aug-
mented with features, such as Sdcl, to restrict the
set of atoms with which they may unify. The task
of assigning a category to each word in a text is
called supertagging (Bangalore and Joshi, 1999).
Because they are recursively defined, there is
an infinite number of potential CCG categories
(though in practice it is limited by the number
of actual grammatical contexts). As a result, the
number of supertags appearing in a corpus far ex-
ceeds the number of POS tags (see Table 1). Since
supertags specify the grammatical context of a to-
ken, and high frequency words appear in many
contexts, CCG grammars tend to have very high
lexical ambiguity, with frequent word types asso-
ciating with a large number of categories. This
ambiguity has made type-supervised supertagger
learning very difficult because the typical ap-
proaches to initializing parameters for EM become
much less effective.
Grammar-informed supertagger learning.
Baldridge (2008) was successful in extending the
standard type-supervised tagger learning to the
task of CCG supertagging by setting the initial
parameters for EM training of an HMM using
two intrinsic properties of the CCG formalism:
the tendency for adjacent tags to combine, and
the tendency to use less complex tags. These
properties are explained in detail in the original
work, but we restate the ideas briefly throughout
this paper for completeness.
</bodyText>
<equation confidence="0.5712432">
X/Y Y X (&gt;)
Y X\Y X (&lt;)
X/Y Y/Z X/Z (&gt;B)
Y \Z X\Y X\Z (&lt;B)
Y/Z X\Y X/Z (&lt;B×)
</equation>
<figureCaption confidence="0.939274333333333">
Figure 1: Combination rules used by CCGBank.
The man walks a dog
Figure 2: CCG parse for “The man walks a dog.”
</figureCaption>
<bodyText confidence="0.997755461538462">
Tag combinability. A CCG parse of a sentence is
derived by recursively combining the categories of
sub-phrases. Category combination is performed
using only a small set of generic rules (see Fig-
ure 1). In the tree in Figure 2, we can see that
a and dog can combine via Forward Application
(&gt;), with NP/N and N combining to produce NP.
The associativity engendered by CCG’s compo-
sition rules means that most adjacent lexical cate-
gories may be combined. In the Figure 2 tree, we
can see that instead of combining (walks·(a·dog)),
we could have combined ((walks·a)·dog) since
(S\NP)/NP and NP/N can combine using &gt;B.
</bodyText>
<sectionHeader confidence="0.99218" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999895052631579">
In this section we define the generative process
we use to model a corpus of sentences. We begin
by generating the model parameters: for each
supertag type t in the tag set T, the transition
probabilities to the next state (7rt) and the emis-
sion probabilities (Ot) are generated by draws
from Dirichlet distributions parameterized with
per-tag mean distributions (7r0t and O0t, respec-
tively) and concentration parameters (απ and
αφ). By setting απ close to zero, we can encode
our prior expectation that transition distributions
should be relatively peaked (i.e., that each tag
type should be followed by relatively few tag
types). The prior means, discussed below, encode
both linguistic intuitions about expected tag-tag
transition behavior and automatically-extracted
corpus information. Given these parameters, we
next generate the sentences of the corpus. This
process is summarized as follows:
</bodyText>
<figure confidence="0.817393428571428">
NP
NP/N N
(S\NP)/NP
NP
NP/N N
S\NP
S
</figure>
<page confidence="0.841318">
142
</page>
<equation confidence="0.9873455">
Parameters:
φt ∼ Dirichlet(αo, φ0t) ∀t ∈ T
7rt ∼ Dirichlet(απ, 7r0t) ∀t ∈ T
Sentence:
y1 ∼ Categorical(7r(S))
for i ∈ {1, 2, ...}, until yi = hEi
xi  |yi ∼ Categorical(φy.)
yi+1  |yi ∼ Categorical(7ry.)
</equation>
<bodyText confidence="0.9994075">
This model can be understood as a Bayesian
HMM (Goldwater and Griffiths, 2007). We next
discuss how the prior distributions are constructed
to build in additional inductive bias.
</bodyText>
<subsectionHeader confidence="0.994803">
3.1 Transition Prior Means (7r0t)
</subsectionHeader>
<bodyText confidence="0.999975642857143">
We use the prior mean for each tag’s transition dis-
tribution to build in two kinds of bias. First, we
want to favor linguistically probable tags. Second,
we want to favor transitions that result in a tag
pair that combines according to CCG’s combina-
tors. For simplicity, we will define 7r0t as a mixture
of two components, the first, Pπ(u) is an (uncon-
ditional) distribution over category types u that fa-
vors cross-linguistically probable categories. The
second component, Pπ(u  |t), conditions on the
previous tag type, t, and assigns higher probabil-
ity to pairs of tags that can be combined. That is,
the probability of transitioning from t to u in the
Dirichlet mean distribution is given by1
</bodyText>
<equation confidence="0.941641">
7r0t(u) = λ · Pπ(u) + (1 − λ) · Pπ(u  |t).
</equation>
<bodyText confidence="0.939898777777778">
We discuss the two mixture components in turn.
3.1.1 Unigram Category Generator (Pπ(u))
In this section, we define a CCG category gener-
ator that generates cross-linguistically likely cat-
egory types. Baldridge’s approach estimated the
likelihood of a category using the inverse number
of sub-categories: PCPLX(u) ∝ 1/complexity(u).
We propose an improvement, PG, expressed as a
probabilistic grammar:2
</bodyText>
<figure confidence="0.8809568">
C → a pterm · patom(a)
C → A/A pterm · pfw · pmod · PG(A)
C → A/B, A6=B pterm ·pfw · pmod · PG(A)·PG(B)
C → A\A pterm · pfw · pmod · PG(A)
C → A\B, A6=B pterm ·pfw · pmod · PG(A)·PG(B)
</figure>
<footnote confidence="0.99755">
1Following Baldridge (2008), we fix A = 0.5 for our ex-
periments.
2For readability, we use the notation p = (1 − p).
</footnote>
<bodyText confidence="0.998769714285714">
where A, B, C are categories and a is an atomic
category (and terminal): a ∈ {S, N, NP, ...}.3
We have designed this grammar to capture sev-
eral important CCG characteristics. In particular
we encode four main ideas, each captured through
a different parameter of the grammar and dis-
cussed in greater detail below:
</bodyText>
<listItem confidence="0.999137857142857">
1. Simpler categories are more likely: e.g. N/N is
a priori more likely than (N/N)/(N/N).
2. Some atoms are more likely than others: e.g.
NP is more likely than S, much more than NPnb.
3. Modifiers are more likely: e.g. (S\NP)/(S\NP)
is more likely than (S\NP)/(NP\NP).
4. Operators occur with different frequencies.
</listItem>
<bodyText confidence="0.982421605263158">
The first idea subsumes the complexity measure
used by Baldridge, but accomplishes the goal nat-
urally by letting the probabilities decrease as the
category grows. The rate of decay is governed
by the pterm parameter: the marginal probability
of generating a terminal (atomic) category in each
expansion. A higher pterm means a stronger em-
phasis on simplicity. The probability distribution
over categories is guaranteed to be proper so long
as pterm &gt; 21 since the probability of the depth of a
tree will decrease geometrically (Chi, 1999).
The second idea is a natural extension of the
complexity concept and is particularly relevant
when features are used. The original complex-
ity measure treated all atoms uniformly, but e.g.
we would expect NPexpl/N to be less likely than
NP/N since it contains the more specialized, and
thus rarer, atom NPexpl. We define the distribution
patom(a) as the prior over atomic categories.
Due to our weak, type-only supervision, we
have to estimate patom from just the tag dictionary
and raw corpus, without frequency data. Our goal
is to estimate the number of each atom in the su-
pertags that should appear on the raw corpus to-
kens. Since we don’t know what the correct su-
pertags are, we first estimate counts of supertags,
from which we can extract estimated atom counts.
Our strategy is to uniformly distribute each raw
corpus token’s counts over all of its possible su-
pertags, as specified in the tag dictionary. Word
types not appearing in the tag dictionary are ig-
3While very similar to standard probabilistic context-free
grammars seen in NLP work, this grammar is not context-free
because modifier categories must have matching operands.
However, this is not a problem for our approach since the
grammar is unambiguous, defines a proper probability distri-
bution, and is only used for modeling the relative likelihoods
of categories (not parsing categories).
</bodyText>
<page confidence="0.995449">
143
</page>
<bodyText confidence="0.9999805">
nored for the purposes of these estimates. Assum-
ing that C(w) is the number of times that word
type w is seen in the raw corpus, atoms(a, t) is the
number of times atom a appears in t, TD(w) is the
set of tags associated with w, and TD(t) is the set
of word types associated with t:
</bodyText>
<equation confidence="0.999972">
Csupertag(t) = PwETD(t)( C(w)+δ)/|TD(w)|
Catom(a) = PtET atoms(a,t) · Csupertag(t)
patom(a) ∝ Catom(a) + δ
</equation>
<bodyText confidence="0.998488304347826">
Adding δ smooths the estimates.
Using the raw corpus and tag dictionary data to
set patom allows us to move beyond Baldridge’s
work in another direction: it provides us with a
natural way to combine CCG’s universal assump-
tions with corpus-specific data.
The third and fourth ideas pertain only to com-
plex categories. If the category is complex, then
we consider two additional parameters. The pa-
rameter pfw is the marginal probability that the
complex category’s operator specifies a forward
argument. The parameter pmod gives the amount
of marginal probability mass that is allocated for
modifier categories. Note that it is not necessary
for pmod to be greater than 21 to achieve the de-
sired result of making modifier categories more
likely than non-modifier categories: the number
of potential modifiers make up only a tiny fraction
of the space of possible categories, so allocating
more than that mass as pmod will result in a cate-
gory grammar that gives disproportionate weight
to modifiers, increasing the likelihood of any par-
ticular modifier from what it would otherwise be.
</bodyText>
<subsubsectionHeader confidence="0.810506">
3.1.2 Bigram Category Generator (Pπ(u  |t))
</subsubsectionHeader>
<bodyText confidence="0.999960416666667">
While the above processes encode important prop-
erties of the distribution over categories, the in-
ternal structure of categories is not the full story:
cross-linguistically, the categories of adjacent to-
kens are much more likely to be combinable via
some CCG rule. This is the second component of
our mixture model.
Baldridge derives this bias by allocating the ma-
jority of the transition probability mass from each
tag t to tags that can follow t according to some
combination rule. Let κ(t, u) be an indicator of
whether t connects to u; for σ ∈ [0, 1]:4
</bodyText>
<equation confidence="0.9961065">
� _ σ · uniform (u) if κ(t, u)
Pκ(u  |t) (1 − σ) · uniform(u) otherwise
</equation>
<bodyText confidence="0.997737097560976">
4Again, following Baldridge (2008), we fix σ = 0.95 for
our experiments.
There are a few additional considerations that
must be made in defining κ, however. In assum-
ing the special tags hSi and hEi for the start and
end of the sentence, respectively, we can define
κ(hSi, u) = 1 when u seeks no left-side argu-
ments (since there are no tags to the left with
which to combine) and κ(t, hEi) = 1 when t seeks
no right-side arguments. So κ(hSi, NP/N) = 1, but
κ(hSi, S\NP) = 0. If atoms have features asso-
ciated, then the atoms are allowed to unify if the
features match, or if at least one of them does
not have a feature. So κ(NPnb, S\NP) = 1, but
κ(NPnb, S\NPconj) = 0. In defining κ, it is also im-
portant to ignore possible arguments on the wrong
side of the combination since they can be con-
sumed without affecting the connection between
the two. To achieve this for κ(t, u), it is assumed
that it is possible to consume all preceding argu-
ments of t and all following arguments of u. So
κ(NP, (S\NP)/NP) = 1. This helps to ensure the
associativity discussed earlier. Finally, the atom
NP is allowed to unify with N if N is the argument.
So κ(N, S\NP) = 1, but κ(NP/N, NP) = 0. This is
due to the fact that CCGBank assumes that N can
be rewritten as NP.
Type-supervised initialization. As above, we
want to improve upon Baldridge’s ideas by en-
coding not just universal CCG knowledge, but
also automatically-induced corpus-specific infor-
mation where possible. To that end, we can de-
fine a conditional distribution Ptr(u  |t) based on
statistics from the raw corpus and tag dictionary.
We use the same approach as we did above for set-
ting patom (and the definition of φtot below): we esti-
mate by evenly distributing raw corpus counts over
the tag dictionary entries. Assume that C(w1, w2)
is the (δ-smoothed) count of times word type w1
was directly followed by w2 in the raw corpus, and
ignoring any words not found in the tag dictionary:
</bodyText>
<equation confidence="0.99900425">
X
C(t, u) = δ +
w1ETD(t), w2ETD(u)
Ptr(u  |t) = C(t, u)/ Pu, C(t, ut)
</equation>
<bodyText confidence="0.9806955">
Then the alternative definition of the compatibility
distribution is as follows:
</bodyText>
<equation confidence="0.94764675">
Pκ t) = S (1–σ) ( Ptr(u|t) otherwise )
(u
C(w1, w2)
|TD(w1)|·|TD(w2)|
</equation>
<page confidence="0.97685">
144
</page>
<bodyText confidence="0.9982292">
Our experiments compare performance when
π0t is set using Pπ(u)=PCPLX (experiment 3) ver-
sus our category grammar PG (4–6), and using
Pπ(u  |t) = Pκ as the compatibility distribution
(3–4) versus Pκtr (5–6).
</bodyText>
<subsectionHeader confidence="0.990634">
3.2 Emission Prior Means (φ0t)
</subsectionHeader>
<bodyText confidence="0.998927444444445">
For each supertag type t, φ0t is the mean distri-
bution over words it emits. While Baldridge’s
approach used a uniform emission initialization,
treating all words as equally likely, we can,
again, induce token-level corpus-specific informa-
tion:5 To set φ0t, we use a variant and simplifica-
tion of the procedure introduced by Garrette and
Baldridge (2012) that takes advantage of our prior
over categories PG.
Assuming that C(w) is the count of word type
w in the raw corpus, TD(w) is the set of supertags
associated with word type w in the tag dictionary,
and TD(t) is the set of known word types associ-
ated with supertag t, the count of word/tag pairs
for known words (words appearing in the tag dic-
tionary) is estimated by uniformly distributing a
word’s (δ-smoothed) raw counts over its tag dic-
tionary entries:
</bodyText>
<equation confidence="0.902125">
� C(w( )i if t E TD(w)
Cknown(t, w) =
TD w
0 otherwise
</equation>
<bodyText confidence="0.9996302">
For unknown words, we first use the idea of tag
“openness” to estimate the likelihood of a partic-
ular tag t applying to an unknown word: if a tag
applies to many word types, it is likely to apply to
some new word type.
</bodyText>
<equation confidence="0.973756">
P(unk  |t) a |known words w s.t. t E TD(w)|
</equation>
<bodyText confidence="0.996382666666667">
Then, we apply Bayes’ rule to get P(t  |unk), and
use that to estimate word/tag counts for unknown
words:
</bodyText>
<equation confidence="0.921992666666667">
P(t  |unk) a P(unk  |t) · PG(t)
Cunk(t, w) = C(w) · P(t  |unk)
Thus, with the estimated counts for all words:
Cknown(t, w) + Cunk(t, w)
Pem(w  |t)
=
</equation>
<bodyText confidence="0.941943">
We perform experiments comparing perfor-
mance when φ0t is uniform (3–5) and when
φ0t(w) = Pem(w  |t) (6).
5Again, without gold tag frequencies.
</bodyText>
<sectionHeader confidence="0.976062" genericHeader="method">
4 Posterior Inference
</sectionHeader>
<bodyText confidence="0.999991333333333">
We wish to find the most likely supertag of each
word, given the model we just described and a cor-
pus of training data. Since there is exact inference
with these models is intractable, we resort to Gibbs
sampling to find an approximate solution. At a
high level, we alternate between resampling model
parameters (φt, πt) given the current tag sequence
and resampling tag sequences given the current
model parameters and observed word sequences.
It is possible to sample a new tagging from the
posterior distribution over tag sequences for a sen-
tence, given the sentence and the HMM parameters
using the forward-filter backward-sample (FFBS)
algorithm (Carter and Kohn, 1996). To effi-
ciently sample new HMM parameters, we exploit
Dirichlet-multinomial conjugacy. By repeating
these alternating steps and accumulating the num-
ber of times each supertag is used in each position,
we obtain an approximation of the required poste-
rior quantities.
Our inference procedure takes as input the tran-
sition prior means π0t, the emission prior means
φ0t, and concentration parameters απ and αφ,
along with the raw corpus and tag dictionary. The
set of supertags associated with a word w will be
known as TD(w). We will refer to the set of word
types included in the tag dictionary as “known”
words and others as “unknown” words. For sim-
plicity, we will assume that TD(w), for any un-
known word w, is the full set of CCG categories.
During sampling, we always restrict the possible
tag choices for a word w to the categories found in
TD(w). We refer to the sequence of word tokens
as x and tags as y.
We initialize the sampler by setting πt = π0 t
and φt = φ0t and then sampling tagging sequences
using FFBS.
To sample a tagging for a sentence x, the strat-
egy is to inductively compute, for each token xi
starting with i = 0 and going “forward”, the prob-
ability of generating x0, x1,... , xi via any tag se-
quence that ends with yi = u:
</bodyText>
<equation confidence="0.985441">
πt(u) · p(yi−1 = t  |x0:i−1)
</equation>
<bodyText confidence="0.992959">
We then pass through the sequence again, this time
“backward” starting at i = |x |— 1 and sampling
</bodyText>
<equation confidence="0.995026833333333">
yi  |yi+1 — p(yi = t  |x0:i) · πt(yi+1).
Ew, Cknown(t, w&apos;) + Cunk(t, w&apos;)
p(yi = u  |x0:i) =
�
φu(xi) ·
tET
</equation>
<page confidence="0.996945">
145
</page>
<table confidence="0.980717333333333">
Corpus num. raw TD TD ambiguity dev test
tags tokens tokens entries type token tokens tokens
English CCGBank POS 50 158k 735k 45k 3.75 13.11 — —
CCGBank 1,171 65k 56.98 296.18 128k 127k
Chinese CTB-CCG 829 99k 439k 60k 96.58 323.37 59k 85k
Italian CCG-TUT 955 6k 27k 9k 178.88 426.13 5k 5k
</table>
<tableCaption confidence="0.986822">
Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT
</tableCaption>
<bodyText confidence="0.987335740740741">
is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates
are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English
POS statistics are shown only for comparison; only CCG experiments were run.
The block-sampling approach of choosing new
tags for a sentence all at once is particularly ben-
eficial given the sequential nature of the model of
the HMM. In an HMM, a token’s adjacent tags tend
to hold onto its current tag due to the relation-
ships between the three. Resampling all tags at
once allows for more drastic changes at each it-
eration, providing better opportunities for mixing
during inference. The FFBS approach has the ad-
ditional advantage that, by resampling the distri-
butions only once per iteration, we are able to re-
sample all sentences in parallel. This is not strictly
true of all HMM problems with FFBS, but because
our data is divided by sentence, and each sentence
has a known start and end tag, the tags chosen dur-
ing the sampling of one sentence cannot affect the
sampling of another sentence in the same iteration.
Once we have sampled tags for the entire cor-
pus, we resample 7r and O. The newly-sampled
tags y are used to compute C(w, t), the count of
tokens with word type w and tag t, and C(t, u),
the number of times tag t is directly followed by
tag u. We then sample, for each t E T where T is
the full set of valid CCG categories:
</bodyText>
<equation confidence="0.963168">
)
7rt — Dir ((απ · 7r0 t(u) + C(t, u))u∈T
)
Ot — Dir ((αo · O0 t(w) + C(w, t))w∈V
</equation>
<bodyText confidence="0.999980272727273">
It is important to note that this method of re-
sampling allows the draws to incorporate both the
data, in the form of counts, and the prior mean,
which includes all of our carefully-constructed bi-
ases derived from both the intrinsic, universal CCG
properties as well as the information we induced
from the raw corpus and tag dictionary.
With the distributions resampled, we can con-
tinue the procedure by resampling tags as above,
and then resampling distributions again, until a
maximum number of iterations is reached.
</bodyText>
<sectionHeader confidence="0.873769" genericHeader="method">
5 Experiments6
</sectionHeader>
<bodyText confidence="0.998380742857142">
To evaluate our approach, we used CCGBank
(Hockenmaier and Steedman, 2007), which is
a transformation of the English Penn Treebank
(Marcus et al., 1993); the CTB-CCG (Tse and
Curran, 2010) transformation of the Penn Chinese
Treebank (Xue et al., 2005); and the CCG-TUT
corpus (Bos et al., 2009), built from the TUT cor-
pus of Italian text (Bosco et al., 2000). Statistics
on the size and ambiguity of these datasets are
shown in Table 1.
For CCGBank, sections 00–15 were used for
extracting the tag dictionary, 16–18 for the raw
corpus, 19–21 for development data, and 22–24
for test data. For TUT, the first 150 sentences of
each of the CIVIL LAW and NEWSPAPER sections
were used for raw data, the next sentences 150–
249 of each was used for development, and the
sentences 250–349 were used for test; the remain-
ing data, 457 sentences from CIVIL LAW and 548
from NEWSPAPER, plus the much smaller 132-
sentence JRC ACQUIS data, was used for the tag
dictionary. For CTB-CCG, sections 00–11 were
used for the tag dictionary, 20–24 for raw, 25–27
for dev, and 28–31 for test.
Because we are interested in showing the rel-
ative gains that our ideas provide over Baldridge
(2008), we reimplemented the initialization pro-
cedure from that paper, allowing us to evaluate
all approaches consistently. For each dataset, we
ran a series of experiments in which we made fur-
ther changes from the original work. We first ran
a baseline experiment with uniform transition and
emission initialization of EM (indicated as “1.” in
Table 2) followed by our reimplementation of the
initialization procedure by Baldridge (2). We then
</bodyText>
<footnote confidence="0.990623">
6All code and experimental scripts are available
at http://www.github.com/dhgarrette/
2014-ccg-supertagging
</footnote>
<page confidence="0.991289">
146
</page>
<table confidence="0.968858111111111">
Corpus 0.1 English no 0.1 Chinese no 0.1 Italian no
TD cutoff 0.01 0.001 0.01 0.001 0.01 0.001
1. uniform EM 77 62 47 38 64 39 30 26 51 32 30 30
2. init (Baldridge) EM 78 67 55 41 66 43 33 28 54 36 33 32
3. init Bayes 74 68 56 42 65 56 47 37 52 46 40 40
4. PG Bayes 74 70 59 42 64 57 47 36 52 40 39 40
5. PG, Ptr Bayes 75 72 61 50 66 58 49 44 52 44 41 43
,,
6. PG, P,,tr , Pem Bayes 80 80 73 51 69 62 56 49 53 47 45 46
</table>
<tableCaption confidence="0.995765">
Table 2: Experimental results: test-set per-token supertag accuracies. “TD cutoff” indicates the level of
</tableCaption>
<bodyText confidence="0.978218569444445">
tag dictionary pruning; see text. (1) is uniform EM initialization. (2) is a reimplementation of (Baldridge,
2008). (3) is Bayesian formulation using only the ideas from Baldridge: PCPLX, P,,, and uniform emis-
sions. (4–6) are our enhancements to the prior: using our category grammar in PG instead of PCPLX, using
P,,tr instead of P,,, and using Pem instead of uniform.
experimented with the Bayesian formulation, first
using the same information used by Baldridge, and
then adding our enhancements: using our category
grammar in PG, using P,,tr as the transition com-
patability distribution, and using Pem as O0t(w).
For each dataset, we ran experiments using four
different levels of tag dictionary pruning. Prun-
ing is the process of artificially removing noise
from the tag dictionary by using token-level anno-
tation counts to discard low-probability tags; for
each word, for cutoff x, any tag with probability
less than x is excluded. Tag dictionary pruning
is a standard procedure in type-supervised train-
ing, but because it requires information that does
not truly conform to the type-supervised scenario,
we felt that it was critical to demonstrate the per-
formance of our approach under situations of less
pruning, including no artificial pruning at all.
We emphasize that unlike in most previous
work, we use incomplete tag dictionaries. Most
previous work makes the unrealistic assumption
that the tag dictionary contains an entry for ev-
ery word that appears in either the training or test-
ing data. This is a poor approximation of a real
tagging system, which will never have complete
lexical knowledge about the test data. Even work
that only assumes complete knowledge of the tag-
ging possibilities for the lexical items in the train-
ing corpus is problematic (Baldridge, 2008; Ravi
et al., 2010). This still makes learning unrealisti-
cally easy since it dramatically reduces the ambi-
guity of words that would have been unseen, and,
in the case of CCG, introduces additional tags that
would not have otherwise been known. To ensure
that our experiments are more realistic, we draw
our tag dictionary entries from data that is totally
disjoint from both the raw and test corpora. Dur-
ing learning, any unknown words (words not ap-
pearing in the tag dictionary) are unconstrained so
that they may take any tag, and are, thus, maxi-
mally ambiguous.
We only performed minimal parameter tuning,
choosing instead to stay consistent with Baldridge
(2008) and simply pick reasonable-seeming val-
ues for any additional parameters. Any tuning that
was performed was done with simple hill-climbing
on the development data of English CCGBank.
All parameters were held consistent across exper-
iments, including across languages. For EM, we
used 50 iterations; for FFBS we used 100 burn-
in iterations and 200 sampling iterations.7 For
all experiments, we used σ = 0.95 for P(tr) ,,and
A = 0.5 for π0t to be consistent with previous
work, α, = 3000, αo = 7000, pterm = 0.6,
pfw = 0.5, pmod = 0.8, and δ = 1000 for patom.
Test data was run only once, for the final figures.
The final results reported were achieved by us-
ing the following training sequence: initialize pa-
rameters according to the scenario, train an HMM
using EM or FFBS starting with that set of parame-
ters, tag the raw corpus with the trained HMM, add-
0.1 smooth counts from the now-tagged raw cor-
pus, and train a maximum entropy Markov model
(MEMM) from this “auto-supervised” data.8
Results are shown in Table 2. Most notably, the
contributions described in this paper improve re-
sults in nearly every experimental scenario. We
can see immediate, often sizable, gains in most
</bodyText>
<footnote confidence="0.9706068">
7Final counts are averaged across the sampling iterations.
8Auto-supervised training of an MEMM increases accu-
racy by 1–3% on average (Garrette and Baldridge, 2013). We
use the OpenNLP MEMM implementation with its standard
set of features: http://opennlp.apache.org
</footnote>
<page confidence="0.996675">
147
</page>
<bodyText confidence="0.999932666666667">
cases simply by using the Bayesian formulation.
Further gains are seen from adding each of the
other various contributions of this paper. Perhaps
most interestingly, the gains are only minimal with
maximum pruning, but the gains increase as the
pruning becomes less aggressive — as the scenar-
ios become more realistic. This indicates that our
improvements make the overall procedure more
robust.
Error Analysis Like POS-taggers, the learned
supertagger frequently confuses nouns (N) and
their modifiers (N/N), but the most frequent er-
ror made by the English (6) experiment was
(((S\NP)\(S\NP))/N) instead of (NPnb/N). How-
ever, these are both determiner types, indicating an
interesting problem for the supertagger: it often
predicts an object type-raised determiner instead
of the vanilla NP/N, but in many contexts, both cat-
egories are equally valid. (In fact, for parsers that
use type-raising as a rule, this distinction in lexical
categories does not exist.)
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999972243902439">
Ravi et al. (2010) also improved upon the work by
Baldridge (2008) by using integer linear program-
ming to find a minimal model of supertag transi-
tions, thereby generating a better starting point for
EM than the grammatical constraints alone could
provide. This approach is complementary to the
work presented here, and because we have shown
that our work yields gains under tag dictionaries
of various levels of cleanliness, it is probable that
employing minimization to set the base distribu-
tion for sampling could lead to still higher gains.
On the Bayesian side, Van Gael et al. (2009)
used a non-parametric, infinite HMM for truly un-
supervised POS-tagger learning (Van Gael et al.,
2008; Beal et al., 2001). While their model is not
restricted to the standard set of POS tags, and may
learn a more fine-grained set of labels, the induced
labels are arbitrary and not grounded in any gram-
matical formalism.
Bisk and Hockenmaier (2013) developed an ap-
proach to CCG grammar induction that does not
use a tag dictionary. Like ours, their procedure
learns from general properties of the CCG formal-
ism. However, while our work is intended to pro-
duce categories that match those used in a partic-
ular training corpus, however complex they might
be, their work produces categories in a simplified
form of CCG in which N and S are the only atoms
and no atoms have features. Additionally, they as-
sume that their training corpus is annotated with
POS tags, whereas we assume truly raw text.
Finally, we find the task of weakly-supervised
supertagger learning to be particularly relevant
given the recent surge in popularity of CCG.
An array of NLP applications have begun using
CCG, including semantic parsing (Zettlemoyer and
Collins, 2005) and machine translation (Weese et
al., 2012). As CCG finds more applications, and
as these applications move to lower-resource do-
mains and languages, there will be increased need
for the ability to learn without full supervision.
</bodyText>
<sectionHeader confidence="0.992777" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999892055555555">
Standard strategies for type-supervised HMM es-
timation are less effective as the number of cat-
egories increases. In contrast to POS tag sets,
CCG supertags, while quite numerous, have struc-
tural clues that can simplify the learning prob-
lem. Baldridge (2008) used this formalism-
specific structure to inform an initialization pro-
cedure for EM. In this work, we have shown that
CCG structure can instead be used to motivate an
effective prior distribution over the parameters of
an HMM supertagging model, allowing our work
to outperform Baldridge’s previously state-of-the-
art approach, and to do so in a principled manner
that lends itself better to future extensions such as
incorporation in more complex models.
This work also improves on Baldridge’s simple
“complexity” measure, developing instead a prob-
abilistic category grammar over supertags that al-
lows our prior to capture a wider variety of inter-
esting and useful properties of the CCG formalism.
Finally, we were able to achieve further gains
by augmenting the universal CCG knowledge with
corpus-specific information that could be automat-
ically extracted from the weak supervision that is
available: the raw corpus and the tag dictionary.
This allows us to combine the cross-linguistic
properties of the CCG formalism with corpus- or
language-specific information in the data into a
single, unified Bayesian prior.
Our model uses a relatively large number of pa-
rameters, e.g., pter., pfw, p.od, pato., in the prior.
Here, we fixed each to a single value (i.e., a “fully
Bayesian” approach). Future work might explore
sensitivity to these choices, or empirical Bayesian
or maximum a posteriori inference for their values
(Johnson and Goldwater, 2009).
</bodyText>
<page confidence="0.994745">
148
</page>
<bodyText confidence="0.999979333333333">
In this work, as in most type-supervised work,
the tag dictionary was automatically extracted
from an existing tagged corpus. However, a tag
dictionary could instead be automatically induced
via multi-lingual transfer (Das and Petrov, 2011)
or generalized from human-provided information
(Garrette and Baldridge, 2013; Garrette et al.,
2013). Again, since the approach presented here
has been shown to be somewhat robust to tag dic-
tionary noise, it is likely that the model would
perform well even when using an automatically-
induced tag dictionary.
</bodyText>
<sectionHeader confidence="0.994009" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9988006">
This work was supported by the U.S. Department
of Defense through the U.S. Army Research Of-
fice (grant number W911NF-10-1-0533). Exper-
iments were run on the UTCS Mastodon Cluster,
provided by NSF grant EIA-0303609.
</bodyText>
<sectionHeader confidence="0.997115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873148148148">
Jason Baldridge. 2008. Weakly supervised supertag-
ging with grammar-informed initialization. In Pro-
ceedings of COLING.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2).
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The innite hidden Markov
model. In NIPS.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
model for inducing combinatory categorial gram-
mars. Transactions of the Association for Compu-
tational Linguistics, 1.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of ACL.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cat-
egorial grammar treebank for Italian. In M. Pas-
sarotti, Adam Przepi´orkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8).
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank
for Italian: a data-driven annotation schema. In Pro-
ceedings of LREC.
Christopher K. Carter and Robert Kohn. 1996. On
Gibbs sampling for state space models. Biometrika,
81(3):341–553.
Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1).
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings ofACL-HLT.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of POS-
taggers for low-resource languages. In Proceedings
ofACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: Ex-
periments on unsupervised word segmentation with
adaptor grammars. In Proceedings of NAACL.
Mark Johnson. 2007. Why doesn’t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &amp;
Language, 6(3).
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
</reference>
<page confidence="0.989573">
149
</page>
<reference confidence="0.999280447368421">
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010.
Minimized models and grammar-informed initial-
ization for supertagging with highly ambiguous lex-
icons. In Proceedings of ACL, pages 495–503.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Borjars, editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar. Wiley-
Blackwell.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: Extracting CCG derivations from the Penn
Chinese treebank. In Proceedings of COLING.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite hidden Markov model. In Proceedings of
ICML.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of EMNLP.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proceedings of WMT.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
</reference>
<page confidence="0.998313">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.329042">
<title confidence="0.995724">Weakly-Supervised Bayesian Learning of a CCG Supertagger</title>
<author confidence="0.97567">A Chris</author>
<affiliation confidence="0.672458333333333">of Computer Science, The University of Texas at of Computer Science, Carnegie Mellon of Linguistics, The University of Texas at Austin</affiliation>
<email confidence="0.658928">author:</email>
<abstract confidence="0.999268619047619">We present a Bayesian formulation for weakly-supervised learning of a Combinatory Categorial Grammar (CCG) supertagger with an HMM. We assume supervision in the form of a tag dictionary, and our prior encourages the use of crosslinguistically common category structures as well as transitions between tags that can combine locally according to CCG’s combinators. Our prior is theoretically appealing since it is motivated by languageindependent, universal properties of the CCG formalism. Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
</authors>
<title>Weakly supervised supertagging with grammar-informed initialization.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="2798" citStr="Baldridge, 2008" startWordPosition="421" endWordPosition="422">. Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combinatory Categorial Grammar (CCG) supertagger, which labels each word with one of a large (possibly unbounded) number of structured categories called supertags, is a considerable challenge. Despite the apparent complexity of the task, supertag sequences have regularities due to universal properties of the CCG formalism (§2) that can be used to reduce the complexity of the problem; previous work showed promising results by using these regularities to initialize an HMM that is then refined with EM (Baldridge, 2008). Here, we exploit CCG’s category structure to motivate a novel prior over HMM parameters for use in Bayesian learning (§3). This prior encourages (i) crosslinguistically common tag types, (ii) tag bigrams that can combine using CCG’s combinators, and (iii) sparse transition distributions. We also go beyond the use of these universals to show how additional, corpus-specific information can be automatically extracted from a combination of the tag dictionary and raw data, and how that information can be combined with the universal knowledge for integration into the model to improve the prior. We</context>
<context position="5913" citStr="Baldridge (2008)" startWordPosition="921" endWordPosition="922">y the number of actual grammatical contexts). As a result, the number of supertags appearing in a corpus far exceeds the number of POS tags (see Table 1). Since supertags specify the grammatical context of a token, and high frequency words appear in many contexts, CCG grammars tend to have very high lexical ambiguity, with frequent word types associating with a large number of categories. This ambiguity has made type-supervised supertagger learning very difficult because the typical approaches to initializing parameters for EM become much less effective. Grammar-informed supertagger learning. Baldridge (2008) was successful in extending the standard type-supervised tagger learning to the task of CCG supertagging by setting the initial parameters for EM training of an HMM using two intrinsic properties of the CCG formalism: the tendency for adjacent tags to combine, and the tendency to use less complex tags. These properties are explained in detail in the original work, but we restate the ideas briefly throughout this paper for completeness. X/Y Y X (&gt;) Y X\Y X (&lt;) X/Y Y/Z X/Z (&gt;B) Y \Z X\Y X\Z (&lt;B) Y/Z X\Y X/Z (&lt;B×) Figure 1: Combination rules used by CCGBank. The man walks a dog Figure 2: CCG par</context>
<context position="9870" citStr="Baldridge (2008)" startWordPosition="1593" endWordPosition="1594">). We discuss the two mixture components in turn. 3.1.1 Unigram Category Generator (Pπ(u)) In this section, we define a CCG category generator that generates cross-linguistically likely category types. Baldridge’s approach estimated the likelihood of a category using the inverse number of sub-categories: PCPLX(u) ∝ 1/complexity(u). We propose an improvement, PG, expressed as a probabilistic grammar:2 C → a pterm · patom(a) C → A/A pterm · pfw · pmod · PG(A) C → A/B, A6=B pterm ·pfw · pmod · PG(A)·PG(B) C → A\A pterm · pfw · pmod · PG(A) C → A\B, A6=B pterm ·pfw · pmod · PG(A)·PG(B) 1Following Baldridge (2008), we fix A = 0.5 for our experiments. 2For readability, we use the notation p = (1 − p). where A, B, C are categories and a is an atomic category (and terminal): a ∈ {S, N, NP, ...}.3 We have designed this grammar to capture several important CCG characteristics. In particular we encode four main ideas, each captured through a different parameter of the grammar and discussed in greater detail below: 1. Simpler categories are more likely: e.g. N/N is a priori more likely than (N/N)/(N/N). 2. Some atoms are more likely than others: e.g. NP is more likely than S, much more than NPnb. 3. Modifiers</context>
<context position="14672" citStr="Baldridge (2008)" startWordPosition="2405" endWordPosition="2406">operties of the distribution over categories, the internal structure of categories is not the full story: cross-linguistically, the categories of adjacent tokens are much more likely to be combinable via some CCG rule. This is the second component of our mixture model. Baldridge derives this bias by allocating the majority of the transition probability mass from each tag t to tags that can follow t according to some combination rule. Let κ(t, u) be an indicator of whether t connects to u; for σ ∈ [0, 1]:4 � _ σ · uniform (u) if κ(t, u) Pκ(u |t) (1 − σ) · uniform(u) otherwise 4Again, following Baldridge (2008), we fix σ = 0.95 for our experiments. There are a few additional considerations that must be made in defining κ, however. In assuming the special tags hSi and hEi for the start and end of the sentence, respectively, we can define κ(hSi, u) = 1 when u seeks no left-side arguments (since there are no tags to the left with which to combine) and κ(t, hEi) = 1 when t seeks no right-side arguments. So κ(hSi, NP/N) = 1, but κ(hSi, S\NP) = 0. If atoms have features associated, then the atoms are allowed to unify if the features match, or if at least one of them does not have a feature. So κ(NPnb, S\N</context>
<context position="24367" citStr="Baldridge (2008)" startWordPosition="4158" endWordPosition="4159"> data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were used for test; the remaining data, 457 sentences from CIVIL LAW and 548 from NEWSPAPER, plus the much smaller 132- sentence JRC ACQUIS data, was used for the tag dictionary. For CTB-CCG, sections 00–11 were used for the tag dictionary, 20–24 for raw, 25–27 for dev, and 28–31 for test. Because we are interested in showing the relative gains that our ideas provide over Baldridge (2008), we reimplemented the initialization procedure from that paper, allowing us to evaluate all approaches consistently. For each dataset, we ran a series of experiments in which we made further changes from the original work. We first ran a baseline experiment with uniform transition and emission initialization of EM (indicated as “1.” in Table 2) followed by our reimplementation of the initialization procedure by Baldridge (2). We then 6All code and experimental scripts are available at http://www.github.com/dhgarrette/ 2014-ccg-supertagging 146 Corpus 0.1 English no 0.1 Chinese no 0.1 Italian </context>
<context position="27227" citStr="Baldridge, 2008" startWordPosition="4652" endWordPosition="4653">e of our approach under situations of less pruning, including no artificial pruning at all. We emphasize that unlike in most previous work, we use incomplete tag dictionaries. Most previous work makes the unrealistic assumption that the tag dictionary contains an entry for every word that appears in either the training or testing data. This is a poor approximation of a real tagging system, which will never have complete lexical knowledge about the test data. Even work that only assumes complete knowledge of the tagging possibilities for the lexical items in the training corpus is problematic (Baldridge, 2008; Ravi et al., 2010). This still makes learning unrealistically easy since it dramatically reduces the ambiguity of words that would have been unseen, and, in the case of CCG, introduces additional tags that would not have otherwise been known. To ensure that our experiments are more realistic, we draw our tag dictionary entries from data that is totally disjoint from both the raw and test corpora. During learning, any unknown words (words not appearing in the tag dictionary) are unconstrained so that they may take any tag, and are, thus, maximally ambiguous. We only performed minimal paramete</context>
<context position="30385" citStr="Baldridge (2008)" startWordPosition="5168" endWordPosition="5169"> the learned supertagger frequently confuses nouns (N) and their modifiers (N/N), but the most frequent error made by the English (6) experiment was (((S\NP)\(S\NP))/N) instead of (NPnb/N). However, these are both determiner types, indicating an interesting problem for the supertagger: it often predicts an object type-raised determiner instead of the vanilla NP/N, but in many contexts, both categories are equally valid. (In fact, for parsers that use type-raising as a rule, this distinction in lexical categories does not exist.) 6 Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infinite HMM for truly unsupervised POS-tagger learnin</context>
<context position="32569" citStr="Baldridge (2008)" startWordPosition="5528" endWordPosition="5529">f NLP applications have begun using CCG, including semantic parsing (Zettlemoyer and Collins, 2005) and machine translation (Weese et al., 2012). As CCG finds more applications, and as these applications move to lower-resource domains and languages, there will be increased need for the ability to learn without full supervision. 7 Conclusion and Future Work Standard strategies for type-supervised HMM estimation are less effective as the number of categories increases. In contrast to POS tag sets, CCG supertags, while quite numerous, have structural clues that can simplify the learning problem. Baldridge (2008) used this formalismspecific structure to inform an initialization procedure for EM. In this work, we have shown that CCG structure can instead be used to motivate an effective prior distribution over the parameters of an HMM supertagging model, allowing our work to outperform Baldridge’s previously state-of-theart approach, and to do so in a principled manner that lends itself better to future extensions such as incorporation in more complex models. This work also improves on Baldridge’s simple “complexity” measure, developing instead a probabilistic category grammar over supertags that allow</context>
</contexts>
<marker>Baldridge, 2008</marker>
<rawString>Jason Baldridge. 2008. Weakly supervised supertagging with grammar-informed initialization. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="5166" citStr="Bangalore and Joshi, 1999" startWordPosition="803" endWordPosition="806"> phrase, etc), or a complex category formed from the combination of two categories by one of two slash operators. In CCG, complex categories indicate a grammatical relationship between the two operands. For example, the category (S\NP)/NP might describe a transitive verb, looking first to its right (indicated by /) for an object, then to its left (\) for a subject, to produce a sentence. Further, atomic categories may be augmented with features, such as Sdcl, to restrict the set of atoms with which they may unify. The task of assigning a category to each word in a text is called supertagging (Bangalore and Joshi, 1999). Because they are recursively defined, there is an infinite number of potential CCG categories (though in practice it is limited by the number of actual grammatical contexts). As a result, the number of supertags appearing in a corpus far exceeds the number of POS tags (see Table 1). Since supertags specify the grammatical context of a token, and high frequency words appear in many contexts, CCG grammars tend to have very high lexical ambiguity, with frequent word types associating with a large number of categories. This ambiguity has made type-supervised supertagger learning very difficult b</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
<author>Zoubin Ghahramani</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>The innite hidden Markov model.</title>
<date>2001</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="31029" citStr="Beal et al., 2001" startWordPosition="5272" endWordPosition="5275">rogramming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infinite HMM for truly unsupervised POS-tagger learning (Van Gael et al., 2008; Beal et al., 2001). While their model is not restricted to the standard set of POS tags, and may learn a more fine-grained set of labels, the induced labels are arbitrary and not grounded in any grammatical formalism. Bisk and Hockenmaier (2013) developed an approach to CCG grammar induction that does not use a tag dictionary. Like ours, their procedure learns from general properties of the CCG formalism. However, while our work is intended to produce categories that match those used in a particular training corpus, however complex they might be, their work produces categories in a simplified form of CCG in whi</context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2001</marker>
<rawString>Matthew J. Beal, Zoubin Ghahramani, and Carl Edward Rasmussen. 2001. The innite hidden Markov model. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Bisk</author>
<author>Julia Hockenmaier</author>
</authors>
<title>An HDP model for inducing combinatory categorial grammars.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<contexts>
<context position="31256" citStr="Bisk and Hockenmaier (2013)" startWordPosition="5311" endWordPosition="5314">ere, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infinite HMM for truly unsupervised POS-tagger learning (Van Gael et al., 2008; Beal et al., 2001). While their model is not restricted to the standard set of POS tags, and may learn a more fine-grained set of labels, the induced labels are arbitrary and not grounded in any grammatical formalism. Bisk and Hockenmaier (2013) developed an approach to CCG grammar induction that does not use a tag dictionary. Like ours, their procedure learns from general properties of the CCG formalism. However, while our work is intended to produce categories that match those used in a particular training corpus, however complex they might be, their work produces categories in a simplified form of CCG in which N and S are the only atoms and no atoms have features. Additionally, they assume that their training corpus is annotated with POS tags, whereas we assume truly raw text. Finally, we find the task of weakly-supervised superta</context>
</contexts>
<marker>Bisk, Hockenmaier, 2013</marker>
<rawString>Yonatan Bisk and Julia Hockenmaier. 2013. An HDP model for inducing combinatory categorial grammars. Transactions of the Association for Computational Linguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>A hierarchical Pitman-Yor process HMM for unsupervised part of speech induction.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1708" citStr="Blunsom and Cohn, 2011" startWordPosition="246" endWordPosition="249">ed automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible labels for each token, so picking the right one simply by chance is unlikely; the parameter space tends to be large; and devising good initial parameters is difficult. Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combinatory Categorial Grammar </context>
</contexts>
<marker>Blunsom, Cohn, 2011</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2011. A hierarchical Pitman-Yor process HMM for unsupervised part of speech induction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Cristina Bosco</author>
<author>Alessandro Mazzei</author>
</authors>
<title>Converting a dependency treebank to a categorial grammar treebank for Italian.</title>
<date>2009</date>
<booktitle>Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8).</booktitle>
<editor>In M. Passarotti, Adam Przepi´orkowski, S. Raynaud, and Frank Van Eynde, editors,</editor>
<contexts>
<context position="23488" citStr="Bos et al., 2009" startWordPosition="4000" endWordPosition="4003">m both the intrinsic, universal CCG properties as well as the information we induced from the raw corpus and tag dictionary. With the distributions resampled, we can continue the procedure by resampling tags as above, and then resampling distributions again, until a maximum number of iterations is reached. 5 Experiments6 To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were used for test; the remaining data, 457 sentences from CIVIL LAW and 548 from NEWSPAPER, plus the much small</context>
</contexts>
<marker>Bos, Bosco, Mazzei, 2009</marker>
<rawString>Johan Bos, Cristina Bosco, and Alessandro Mazzei. 2009. Converting a dependency treebank to a categorial grammar treebank for Italian. In M. Passarotti, Adam Przepi´orkowski, S. Raynaud, and Frank Van Eynde, editors, Proceedings of the Eighth International Workshop on Treebanks and Linguistic Theories (TLT8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Bosco</author>
<author>Vincenzo Lombardo</author>
<author>Daniela Vassallo</author>
<author>Leonardo Lesmo</author>
</authors>
<title>Building a treebank for Italian: a data-driven annotation schema.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="23552" citStr="Bosco et al., 2000" startWordPosition="4013" endWordPosition="4016">information we induced from the raw corpus and tag dictionary. With the distributions resampled, we can continue the procedure by resampling tags as above, and then resampling distributions again, until a maximum number of iterations is reached. 5 Experiments6 To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were used for test; the remaining data, 457 sentences from CIVIL LAW and 548 from NEWSPAPER, plus the much smaller 132- sentence JRC ACQUIS data, was used for the tag dictionar</context>
</contexts>
<marker>Bosco, Lombardo, Vassallo, Lesmo, 2000</marker>
<rawString>Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo, and Leonardo Lesmo. 2000. Building a treebank for Italian: a data-driven annotation schema. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher K Carter</author>
<author>Robert Kohn</author>
</authors>
<title>On Gibbs sampling for state space models.</title>
<date>1996</date>
<journal>Biometrika,</journal>
<volume>81</volume>
<issue>3</issue>
<contexts>
<context position="19254" citStr="Carter and Kohn, 1996" startWordPosition="3227" endWordPosition="3230">d, given the model we just described and a corpus of training data. Since there is exact inference with these models is intractable, we resort to Gibbs sampling to find an approximate solution. At a high level, we alternate between resampling model parameters (φt, πt) given the current tag sequence and resampling tag sequences given the current model parameters and observed word sequences. It is possible to sample a new tagging from the posterior distribution over tag sequences for a sentence, given the sentence and the HMM parameters using the forward-filter backward-sample (FFBS) algorithm (Carter and Kohn, 1996). To efficiently sample new HMM parameters, we exploit Dirichlet-multinomial conjugacy. By repeating these alternating steps and accumulating the number of times each supertag is used in each position, we obtain an approximation of the required posterior quantities. Our inference procedure takes as input the transition prior means π0t, the emission prior means φ0t, and concentration parameters απ and αφ, along with the raw corpus and tag dictionary. The set of supertags associated with a word w will be known as TD(w). We will refer to the set of word types included in the tag dictionary as “kn</context>
</contexts>
<marker>Carter, Kohn, 1996</marker>
<rawString>Christopher K. Carter and Robert Kohn. 1996. On Gibbs sampling for state space models. Biometrika, 81(3):341–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyi Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="11128" citStr="Chi, 1999" startWordPosition="1807" endWordPosition="1808">ely than (S\NP)/(NP\NP). 4. Operators occur with different frequencies. The first idea subsumes the complexity measure used by Baldridge, but accomplishes the goal naturally by letting the probabilities decrease as the category grows. The rate of decay is governed by the pterm parameter: the marginal probability of generating a terminal (atomic) category in each expansion. A higher pterm means a stronger emphasis on simplicity. The probability distribution over categories is guaranteed to be proper so long as pterm &gt; 21 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). The second idea is a natural extension of the complexity concept and is particularly relevant when features are used. The original complexity measure treated all atoms uniformly, but e.g. we would expect NPexpl/N to be less likely than NP/N since it contains the more specialized, and thus rarer, atom NPexpl. We define the distribution patom(a) as the prior over atomic categories. Due to our weak, type-only supervision, we have to estimate patom from just the tag dictionary and raw corpus, without frequency data. Our goal is to estimate the number of each atom in the supertags that should app</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Zhiyi Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL-HLT.</booktitle>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In Proceedings ofACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Typesupervised hidden Markov models for part-ofspeech tagging with incomplete tag dictionaries.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17389" citStr="Garrette and Baldridge (2012)" startWordPosition="2891" endWordPosition="2894">e ) (u C(w1, w2) |TD(w1)|·|TD(w2)| 144 Our experiments compare performance when π0t is set using Pπ(u)=PCPLX (experiment 3) versus our category grammar PG (4–6), and using Pπ(u |t) = Pκ as the compatibility distribution (3–4) versus Pκtr (5–6). 3.2 Emission Prior Means (φ0t) For each supertag type t, φ0t is the mean distribution over words it emits. While Baldridge’s approach used a uniform emission initialization, treating all words as equally likely, we can, again, induce token-level corpus-specific information:5 To set φ0t, we use a variant and simplification of the procedure introduced by Garrette and Baldridge (2012) that takes advantage of our prior over categories PG. Assuming that C(w) is the count of word type w in the raw corpus, TD(w) is the set of supertags associated with word type w in the tag dictionary, and TD(t) is the set of known word types associated with supertag t, the count of word/tag pairs for known words (words appearing in the tag dictionary) is estimated by uniformly distributing a word’s (δ-smoothed) raw counts over its tag dictionary entries: � C(w( )i if t E TD(w) Cknown(t, w) = TD w 0 otherwise For unknown words, we first use the idea of tag “openness” to estimate the likelihood</context>
</contexts>
<marker>Garrette, Baldridge, 2012</marker>
<rawString>Dan Garrette and Jason Baldridge. 2012. Typesupervised hidden Markov models for part-ofspeech tagging with incomplete tag dictionaries. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Baldridge</author>
</authors>
<title>Learning a part-of-speech tagger from two hours of annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="29235" citStr="Garrette and Baldridge, 2013" startWordPosition="4989" endWordPosition="4992"> according to the scenario, train an HMM using EM or FFBS starting with that set of parameters, tag the raw corpus with the trained HMM, add0.1 smooth counts from the now-tagged raw corpus, and train a maximum entropy Markov model (MEMM) from this “auto-supervised” data.8 Results are shown in Table 2. Most notably, the contributions described in this paper improve results in nearly every experimental scenario. We can see immediate, often sizable, gains in most 7Final counts are averaged across the sampling iterations. 8Auto-supervised training of an MEMM increases accuracy by 1–3% on average (Garrette and Baldridge, 2013). We use the OpenNLP MEMM implementation with its standard set of features: http://opennlp.apache.org 147 cases simply by using the Bayesian formulation. Further gains are seen from adding each of the other various contributions of this paper. Perhaps most interestingly, the gains are only minimal with maximum pruning, but the gains increase as the pruning becomes less aggressive — as the scenarios become more realistic. This indicates that our improvements make the overall procedure more robust. Error Analysis Like POS-taggers, the learned supertagger frequently confuses nouns (N) and their m</context>
</contexts>
<marker>Garrette, Baldridge, 2013</marker>
<rawString>Dan Garrette and Jason Baldridge. 2013. Learning a part-of-speech tagger from two hours of annotation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Garrette</author>
<author>Jason Mielens</author>
<author>Jason Baldridge</author>
</authors>
<title>Real-world semi-supervised learning of POStaggers for low-resource languages. In</title>
<date>2013</date>
<booktitle>Proceedings ofACL.</booktitle>
<marker>Garrette, Mielens, Baldridge, 2013</marker>
<rawString>Dan Garrette, Jason Mielens, and Jason Baldridge. 2013. Real-world semi-supervised learning of POStaggers for low-resource languages. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>EM can find pretty good HMM POS-taggers (when given a good start).</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1519" citStr="Goldberg et al., 2008" startWordPosition="219" endWordPosition="222">ts over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible labels for each token, so picking the right one simply by chance is unlikely; the parameter space tends</context>
</contexts>
<marker>Goldberg, Adler, Elhadad, 2008</marker>
<rawString>Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008. EM can find pretty good HMM POS-taggers (when given a good start). In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1684" citStr="Goldwater and Griffiths, 2007" startWordPosition="242" endWordPosition="245">fic information that is extracted automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible labels for each token, so picking the right one simply by chance is unlikely; the parameter space tends to be large; and devising good initial parameters is difficult. Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combina</context>
<context position="8385" citStr="Goldwater and Griffiths, 2007" startWordPosition="1335" endWordPosition="1338">be followed by relatively few tag types). The prior means, discussed below, encode both linguistic intuitions about expected tag-tag transition behavior and automatically-extracted corpus information. Given these parameters, we next generate the sentences of the corpus. This process is summarized as follows: NP NP/N N (S\NP)/NP NP NP/N N S\NP S 142 Parameters: φt ∼ Dirichlet(αo, φ0t) ∀t ∈ T 7rt ∼ Dirichlet(απ, 7r0t) ∀t ∈ T Sentence: y1 ∼ Categorical(7r(S)) for i ∈ {1, 2, ...}, until yi = hEi xi |yi ∼ Categorical(φy.) yi+1 |yi ∼ Categorical(7ry.) This model can be understood as a Bayesian HMM (Goldwater and Griffiths, 2007). We next discuss how the prior distributions are constructed to build in additional inductive bias. 3.1 Transition Prior Means (7r0t) We use the prior mean for each tag’s transition distribution to build in two kinds of bias. First, we want to favor linguistically probable tags. Second, we want to favor transitions that result in a tag pair that combines according to CCG’s combinators. For simplicity, we will define 7r0t as a mixture of two components, the first, Pπ(u) is an (unconditional) distribution over category types u that favors cross-linguistically probable categories. The second com</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="23268" citStr="Hockenmaier and Steedman, 2007" startWordPosition="3963" endWordPosition="3966"> O0 t(w) + C(w, t))w∈V It is important to note that this method of resampling allows the draws to incorporate both the data, in the form of counts, and the prior mean, which includes all of our carefully-constructed biases derived from both the intrinsic, universal CCG properties as well as the information we induced from the raw corpus and tag dictionary. With the distributions resampled, we can continue the procedure by resampling tags as above, and then resampling distributions again, until a maximum number of iterations is reached. 5 Experiments6 To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="34021" citStr="Johnson and Goldwater, 2009" startWordPosition="5753" endWordPosition="5756"> could be automatically extracted from the weak supervision that is available: the raw corpus and the tag dictionary. This allows us to combine the cross-linguistic properties of the CCG formalism with corpus- or language-specific information in the data into a single, unified Bayesian prior. Our model uses a relatively large number of parameters, e.g., pter., pfw, p.od, pato., in the prior. Here, we fixed each to a single value (i.e., a “fully Bayesian” approach). Future work might explore sensitivity to these choices, or empirical Bayesian or maximum a posteriori inference for their values (Johnson and Goldwater, 2009). 148 In this work, as in most type-supervised work, the tag dictionary was automatically extracted from an existing tagged corpus. However, a tag dictionary could instead be automatically induced via multi-lingual transfer (Das and Petrov, 2011) or generalized from human-provided information (Garrette and Baldridge, 2013; Garrette et al., 2013). Again, since the approach presented here has been shown to be somewhat robust to tag dictionary noise, it is likely that the model would perform well even when using an automaticallyinduced tag dictionary. Acknowledgements This work was supported by t</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="1653" citStr="Johnson, 2007" startWordPosition="240" endWordPosition="241">th corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible labels for each token, so picking the right one simply by chance is unlikely; the parameter space tends to be large; and devising good initial parameters is difficult. Therefore, it is unsurprising that the unsupervised (or even weaklysu</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers? In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="1399" citStr="Kupiec, 1992" startWordPosition="205" endWordPosition="206">pendent, universal properties of the CCG formalism. Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large nu</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Julian Kupiec. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech &amp; Language, 6(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised pos tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1457" citStr="Lee et al., 2010" startWordPosition="211" endWordPosition="214">mpirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible labels for each token, so picking the rig</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised pos tagging. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="23346" citStr="Marcus et al., 1993" startWordPosition="3976" endWordPosition="3979">draws to incorporate both the data, in the form of counts, and the prior mean, which includes all of our carefully-constructed biases derived from both the intrinsic, universal CCG properties as well as the information we induced from the raw corpus and tag dictionary. With the distributions resampled, we can continue the procedure by resampling tags as above, and then resampling distributions again, until a maximum number of iterations is reached. 5 Experiments6 To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for developme</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1416" citStr="Merialdo, 1994" startWordPosition="207" endWordPosition="208">rsal properties of the CCG formalism. Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible </context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models for unsupervised part-of-speech tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-AFNLP.</booktitle>
<contexts>
<context position="1962" citStr="Ravi and Knight, 2009" startWordPosition="285" endWordPosition="288">nductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible labels for each token, so picking the right one simply by chance is unlikely; the parameter space tends to be large; and devising good initial parameters is difficult. Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combinatory Categorial Grammar (CCG) supertagger, which labels each word with one of a large (possibly unbounded) number of structured categories called supertags, is a considerable challenge. Despite the apparent complexity of the task, supertag sequences have regularities due to uni</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized models for unsupervised part-of-speech tagging. In Proceedings of ACL-AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Jason Baldridge</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>495--503</pages>
<contexts>
<context position="27247" citStr="Ravi et al., 2010" startWordPosition="4654" endWordPosition="4657"> under situations of less pruning, including no artificial pruning at all. We emphasize that unlike in most previous work, we use incomplete tag dictionaries. Most previous work makes the unrealistic assumption that the tag dictionary contains an entry for every word that appears in either the training or testing data. This is a poor approximation of a real tagging system, which will never have complete lexical knowledge about the test data. Even work that only assumes complete knowledge of the tagging possibilities for the lexical items in the training corpus is problematic (Baldridge, 2008; Ravi et al., 2010). This still makes learning unrealistically easy since it dramatically reduces the ambiguity of words that would have been unseen, and, in the case of CCG, introduces additional tags that would not have otherwise been known. To ensure that our experiments are more realistic, we draw our tag dictionary entries from data that is totally disjoint from both the raw and test corpora. During learning, any unknown words (words not appearing in the tag dictionary) are unconstrained so that they may take any tag, and are, thus, maximally ambiguous. We only performed minimal parameter tuning, choosing i</context>
<context position="30337" citStr="Ravi et al. (2010)" startWordPosition="5158" endWordPosition="5161">dure more robust. Error Analysis Like POS-taggers, the learned supertagger frequently confuses nouns (N) and their modifiers (N/N), but the most frequent error made by the English (6) experiment was (((S\NP)\(S\NP))/N) instead of (NPnb/N). However, these are both determiner types, indicating an interesting problem for the supertagger: it often predicts an object type-raised determiner instead of the vanilla NP/N, but in many contexts, both categories are equally valid. (In fact, for parsers that use type-raising as a rule, this distinction in lexical categories does not exist.) 6 Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infini</context>
</contexts>
<marker>Ravi, Baldridge, Knight, 2010</marker>
<rawString>Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010. Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons. In Proceedings of ACL, pages 495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1606" citStr="Smith and Eisner, 2005" startWordPosition="231" endWordPosition="234">tional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 2009). In such cases, there are a large number of possible labels for each token, so picking the right one simply by chance is unlikely; the parameter space tends to be large; and devising good initial parameters is difficult. Therefore, it is unsur</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Jason Baldridge</author>
</authors>
<title>Combinatory categorial grammar.</title>
<date>2011</date>
<booktitle>In Robert Borsley and Kersti Borjars, editors, Non-Transformational Syntax: Formal and Explicit Models of Grammar.</booktitle>
<publisher>WileyBlackwell.</publisher>
<contexts>
<context position="4157" citStr="Steedman and Baldridge, 2011" startWordPosition="621" endWordPosition="624">rior probability (§4). We experimentally verify that our Bayesian formulation is effective and substantially outperforms the state-of-the-art baseline initialization/EM strategy in several languages (§5). We also evaluate using tag dictionaries that are unpruned and have only partial word coverage, finding even greater improvements in these more realistic scenarios. 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 2 CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical token is associated with a structured category, often referred to as a supertag. CCG categories are defined by the following recursive definition: C → {S, N, NP, PP, ...} C → {C/C, C\C} A CCG category can either be an atomic category indicating a particular type of basic grammatical phrase (S for a sentence, N for a noun, NP for a noun phrase, etc), or a complex category formed from the combination of two categories by one of two slash operators. In CCG, complex categories indicate a grammatical relationship between the two operands. For example, t</context>
</contexts>
<marker>Steedman, Baldridge, 2011</marker>
<rawString>Mark Steedman and Jason Baldridge. 2011. Combinatory categorial grammar. In Robert Borsley and Kersti Borjars, editors, Non-Transformational Syntax: Formal and Explicit Models of Grammar. WileyBlackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4126" citStr="Steedman, 2000" startWordPosition="619" endWordPosition="620">l to their posterior probability (§4). We experimentally verify that our Bayesian formulation is effective and substantially outperforms the state-of-the-art baseline initialization/EM strategy in several languages (§5). We also evaluate using tag dictionaries that are unpruned and have only partial word coverage, finding even greater improvements in these more realistic scenarios. 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 2 CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical token is associated with a structured category, often referred to as a supertag. CCG categories are defined by the following recursive definition: C → {S, N, NP, PP, ...} C → {C/C, C\C} A CCG category can either be an atomic category indicating a particular type of basic grammatical phrase (S for a sentence, N for a noun, NP for a noun phrase, etc), or a complex category formed from the combination of two categories by one of two slash operators. In CCG, complex categories indicate a grammatical relationship between t</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Tse</author>
<author>James R Curran</author>
</authors>
<title>Chinese CCGbank: Extracting CCG derivations from the Penn Chinese treebank.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="23382" citStr="Tse and Curran, 2010" startWordPosition="3982" endWordPosition="3985"> in the form of counts, and the prior mean, which includes all of our carefully-constructed biases derived from both the intrinsic, universal CCG properties as well as the information we induced from the raw corpus and tag dictionary. With the distributions resampled, we can continue the procedure by resampling tags as above, and then resampling distributions again, until a maximum number of iterations is reached. 5 Experiments6 To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were u</context>
</contexts>
<marker>Tse, Curran, 2010</marker>
<rawString>Daniel Tse and James R. Curran. 2010. Chinese CCGbank: Extracting CCG derivations from the Penn Chinese treebank. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
</authors>
<title>Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<marker>Van Gael, 2008</marker>
<rawString>Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam sampling for the infinite hidden Markov model. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite HMM for unsupervised PoS tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Van Gael, Vlachos, Ghahramani, 2009</marker>
<rawString>Jurgen Van Gael, Andreas Vlachos, and Zoubin Ghahramani. 2009. The infinite HMM for unsupervised PoS tagging. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Weese</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Using categorial grammar to label translation rules.</title>
<date>2012</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="32097" citStr="Weese et al., 2012" startWordPosition="5451" endWordPosition="5454">that match those used in a particular training corpus, however complex they might be, their work produces categories in a simplified form of CCG in which N and S are the only atoms and no atoms have features. Additionally, they assume that their training corpus is annotated with POS tags, whereas we assume truly raw text. Finally, we find the task of weakly-supervised supertagger learning to be particularly relevant given the recent surge in popularity of CCG. An array of NLP applications have begun using CCG, including semantic parsing (Zettlemoyer and Collins, 2005) and machine translation (Weese et al., 2012). As CCG finds more applications, and as these applications move to lower-resource domains and languages, there will be increased need for the ability to learn without full supervision. 7 Conclusion and Future Work Standard strategies for type-supervised HMM estimation are less effective as the number of categories increases. In contrast to POS tag sets, CCG supertags, while quite numerous, have structural clues that can simplify the learning problem. Baldridge (2008) used this formalismspecific structure to inform an initialization procedure for EM. In this work, we have shown that CCG struct</context>
</contexts>
<marker>Weese, Callison-Burch, Lopez, 2012</marker>
<rawString>Jonathan Weese, Chris Callison-Burch, and Adam Lopez. 2012. Using categorial grammar to label translation rules. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="23445" citStr="Xue et al., 2005" startWordPosition="3992" endWordPosition="3995">ur carefully-constructed biases derived from both the intrinsic, universal CCG properties as well as the information we induced from the raw corpus and tag dictionary. With the distributions resampled, we can continue the procedure by resampling tags as above, and then resampling distributions again, until a maximum number of iterations is reached. 5 Experiments6 To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were used for test; the remaining data, 457 sentences from CIVIL LAW </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI.</booktitle>
<contexts>
<context position="32052" citStr="Zettlemoyer and Collins, 2005" startWordPosition="5444" endWordPosition="5447">wever, while our work is intended to produce categories that match those used in a particular training corpus, however complex they might be, their work produces categories in a simplified form of CCG in which N and S are the only atoms and no atoms have features. Additionally, they assume that their training corpus is annotated with POS tags, whereas we assume truly raw text. Finally, we find the task of weakly-supervised supertagger learning to be particularly relevant given the recent surge in popularity of CCG. An array of NLP applications have begun using CCG, including semantic parsing (Zettlemoyer and Collins, 2005) and machine translation (Weese et al., 2012). As CCG finds more applications, and as these applications move to lower-resource domains and languages, there will be increased need for the ability to learn without full supervision. 7 Conclusion and Future Work Standard strategies for type-supervised HMM estimation are less effective as the number of categories increases. In contrast to POS tag sets, CCG supertags, while quite numerous, have structural clues that can simplify the learning problem. Baldridge (2008) used this formalismspecific structure to inform an initialization procedure for EM</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
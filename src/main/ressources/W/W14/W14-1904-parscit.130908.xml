<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002041">
<title confidence="0.994964">
Speech recognition in Alzheimer’s disease with personal assistive robots
</title>
<author confidence="0.992219">
Frank Rudzicz1,2,* and Rosalie Wang1 and Momotaz Begum3 and Alex Mihailidis2,1
</author>
<affiliation confidence="0.9757645">
1 Toronto Rehabilitation Institute, Toronto ON; 2 University of Toronto, Toronto ON;
3 University of Massachussetts Lowell
</affiliation>
<email confidence="0.99561">
*frank@cs.toronto.edu
</email>
<sectionHeader confidence="0.993842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999851">
To help individuals with Alzheimer’s dis-
ease live at home for longer, we are de-
veloping a mobile robotic platform, called
ED, intended to be used as a personal care-
giver to help with the performance of ac-
tivities of daily living. In a series of ex-
periments, we study speech-based inter-
actions between each of 10 older adults
with Alzheimers disease and ED as the
former makes tea in a simulated home en-
vironment. Analysis reveals that speech
recognition remains a challenge for this
recording environment, with word-level
accuracies between 5.8% and 19.2% dur-
ing household tasks with individuals with
Alzheimer’s disease. This work provides a
baseline assessment for the types of tech-
nical and communicative challenges that
will need to be overcome in human-robot
interaction for this population.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901051724138">
Alzheimer’s disease (AD) is a progressive neu-
rodegenerative disorder primarily impairing mem-
ory, followed by declines in language, ability to
carry out motor tasks, object recognition, and ex-
ecutive functioning (American Psychiatric Asso-
ciation, 2000; Gauthier et al., 1997). An accu-
rate measure of functional decline comes from
performance in activities of daily living (ADLs),
such as shopping, finances, housework, and self-
care tasks. The deterioration in language com-
prehension and/or production resulting from spe-
cific brain damage, also known as aphasia, is a
common feature of AD and other related con-
ditions. Language changes observed clinically
in older adults with dementia include increasing
word-finding difficulties, loss of ability to verbally
express information in detail, increasing use of 20
generic references (e.g., “it”), and progressing dif-
ficulties understanding information presented ver-
bally (American Psychiatric Association, 2000).
Many nations are facing healthcare crises in the
lack of capacity to support rapidly aging popula-
tions nor the chronic conditions associated with
aging, including dementia. The current healthcare
model of removing older adults from their homes
and placing them into long-term care facilities
is neither financially sustainable in this scenario
(Bharucha et al., 2009), nor is it desirable. Our
team has been developing “smart home” systems
at the Toronto Rehabilitation Institute (TRI, part
of the University Health Network) to help older
adults “age-in-place” by providing different types
of support, such as step-by-step prompts for daily
tasks (Mihailidis et al., 2008), responses to emer-
gency situations (Lee and Mihaildis, 2005), and
means to communicate with family and friends.
These systems are being evaluated within a com-
pletely functional re-creation of a one-bedroom
apartment located within The TRI hospital, called
HomeLab. These smart home technologies use
advanced sensing techniques and machine learn-
ing to autonomously react to their users, but they
are fixed and embedded into the environment, e.g.,
as cameras in the ceiling. Fixing the location of
these technologies carries a tradeoff between util-
ity and feasibility – installing multiple hardware
units at all locations where assistance could be re-
quired (e.g., bathroom, kitchen, and bedroom) can
be expensive and cumbersome, but installing too
few units will present gaps where a user’s activ-
ity will not be detected. Alternatively, integrat-
ing personal mobile robots with smart homes can
overcome some of these tradeoffs. Moreover, as-
sistance provided via a physically embodied robot
is often more acceptable than that provided by an
embedded system (Klemmer et al., 2006).
With these potential advantages in mind, we
conducted a ‘Wizard-of-Oz’ study to explore the
</bodyText>
<note confidence="0.803338">
Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 20–28,
Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999944827586207">
feasibility and usability of a mobile assistive robot
that uses the step-by-step prompting approaches
for daily activities originally applied to our smart
home research (Mihailidis et al., 2008). We con-
ducted the study with older adults with mild or
moderate AD and the tasks of hand washing and
tea making. Our preliminary data analysis showed
that the participants reacted well to the robot itself
and the prompts that it provided, suggesting the
feasibility of using personal robots for this appli-
cation (Begum et al., 2013). One important iden-
tified issue is the need for an automatic speech
recognition system to detect and understand ut-
terances specifically from older adults with AD.
The development of such a system will enable
the assistive robot to better understand the be-
haviours and needs of these users for effective in-
teractions and will further enhance environmental-
based smart home systems.
This paper presents an analysis of the speech
data collected from our participants with AD when
interacting with the robot. In a series of exper-
iments, we measure the performance of modern
speech recognition with this population and with
their younger caregivers with and without signal
preprocessing. This work will serve as the basis
for further studies by identifying some of the de-
velopment needs of a speech-based interface for
robotic caregivers for older adults with AD.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999780015151515">
Research in smart home systems, assistive robots,
and integrated robot/smart home systems for older
adults with cognitive impairments has often fo-
cused on assistance with activities of daily living
(i.e., reminders to do specific activities according
to a schedule or prompts to perform activity steps),
cognitive and social stimulation and emergency
response systems. Archipel (Serna et al., 2007)
recognizes the user’s intended plan and provides
prompts, e.g. with cooking tasks. Autominder,
(Pollack, 2006), provides context-appropriate re-
minders for activity schedules, and the COACH
(Cognitive Orthosis for Assisting with aCtivities
in the Home) system prompts for the task of hand-
washing (Mihailidis et al., 2008) and tea-making
(Olivier et al., 2009). Mynatt et al. (2004) have
been developing technologies to support aging-in-
place such as the Cooks Collage, which uses a se-
ries of photos to remind the user what the last step
completed was if the user is interrupted during21a
cooking task. These interventions tend to be em-
bedded in existing environments (e.g., around the
sink area).
More recent innovations have examined in-
tegrated robot-smart home systems where sys-
tems are embedded into existing environments that
communicate with mobile assistive robots (e.g.,
CompanionAble, (Mouad et al., 2010); Mobiserv
Kompai, (Lucet, 2012); and ROBADOM (Tapus
and Chetouani, 2010)). Many of these projects
are targeted towards older adults with cognitive
impairment, and not specifically those with sig-
nificant cognitive impairment. One of these sys-
tems, CompanionAble, with a fully autonomous
assistive robot, has recently been tested in a simu-
lated home environment for two days each with
four older adults with dementia (AD or Pick’s
disease/frontal lobe dementia) and two with mild
cognitive impairment. The system provides assis-
tance with various activities, including appoint-
ment reminders for activities input by users or
caregivers, video calls, and cognitive exercises.
Participants reported an overall acceptance of the
system and several upgrades were reported, in-
cluding a speech recognition system that had to be
deactivated by the second day due to poor perfor-
mance.
One critical component for the successful use of
these technological interventions is the usability of
the communication interface for the targeted users,
in this case older adults with Alzheimer’s disease.
As in communication between two people, com-
munication between the older adult and the robot
may include natural, freeform speech (as opposed
to simple spoken keyword interaction) and non-
verbal cues (e.g., hand gestures, head pose, eye
gaze, facial feature cues), although speech tends to
be far more effective (Green et al., 2008; Goodrich
and Schultz, 2007). Previous research indicates
that automated communication systems are more
effective if they take into account the affective
and mental states of the user (Saini et al., 2005).
Indeed, speech appears to be the most powerful
mode of communication for an assistive robot to
communicate with its users (Tapus and Chetouani,
2010; Lucet, 2012).
</bodyText>
<subsectionHeader confidence="0.954649">
2.1 Language use in dementia and
Alzheimer’s disease
</subsectionHeader>
<bodyText confidence="0.999970588235294">
In order to design a speech interface for individ-
uals with dementia, and AD in particular, it is
important to understand how their speech differs
from that of the general population. This then can
be integrated into future automatic speech recog-
nition systems. Guinn and Habash (2012) showed,
through an analysis of conversational dialogs, that
repetition, incomplete words, and paraphrasing
were significant indicators of Alzheimer’s dis-
ease relative but several expected measures such
as filler phrases, syllables per minute, and pro-
noun rate were not. Indeed, pauses, fillers, for-
mulaic speech, restarts, and speech disfluencies
are all hallmarks of speech in individuals with
Alzheimer’s (Davis and Maclagan, 2009; Snover
et al., 2004). Effects of Alzheimer’s disease on
syntax remains controversial, with some evidence
that deficits in syntax or of agrammatism could be
due to memory deficits in the disease (Reilly et al.,
2011).
Other studies has applied similar analyses to
related clinical groups. Pakhomov et al. (2010)
identified several different features from the au-
dio and corresponding transcripts of 38 patients
with frontotemporal lobar degeneration (FTLD).
They found that pause-to-word ratio and pronoun-
to-noun ratios were especially discriminative of
FTLD variants and that length, hesitancy, and
agramatism correspond to the phenomenology of
FTLD. Roark et al. (2011) tested the ability of an
automated classifier to distinguish patients with
mild cognitive impairment from healthy controls
that include acoustic features such as pause fre-
quency and duration.
</bodyText>
<subsectionHeader confidence="0.979529">
2.2 Human-robot interaction
</subsectionHeader>
<bodyText confidence="0.999937307692308">
Receiving assistance from an entity with a physi-
cal body (such as a robot) is often psychologically
more acceptable than receiving assistance from an
entity without a physical body (such as an em-
bedded system) (Klemmer et al., 2006). Physical
embodiment also opens up the possibility of hav-
ing more meaningful interaction between the older
adult and the robot, as discussed in Section 5.
Social collaboration between humans and
robots often depends on communication in which
each participant’s intention and goals are clear
(Freedy et al., 2007; Bauer et al., 2008; Green
et al., 2008). It is important that the human
participant is able to construct a useable ‘men-
tal model’ of the robot through bidirectional com-
munication (Burke and Murphy, 1999) which can
include both natural speech and non-verbal cues22
(e.g., hand gestures, gaze, facial cues), although
speech tends to be far more effective (Green et al.,
2008; Goodrich and Schultz, 2007).
Automated communicative systems that are
more sensitive to the emotive and the mental states
of their users are often more successful than more
neutral conversational agents (Saini et al., 2005).
In order to be useful in practice, these commu-
nicative systems need to mimic some of the tech-
niques employed by caregivers of individuals with
AD. Often, these caregivers are employed by lo-
cal clinics or medical institutions and are trained
by those institutions in ideal verbal communica-
tion strategies for use with those having demen-
tia (Hopper, 2001; Goldfarb and Pietro, 2004).
These include (Wilson et al., 2012) but are not
limited to relatively slow rate of speech, verba-
tim repetition of misunderstood prompts, closed-
ended (e.g., ‘yes/no’) questions, and reduced syn-
tactic complexity. However, Tomoeda et al. (1990)
showed that rates of speech that are too slow
may interfere with comprehension if they intro-
duce problems of short-term retention of working
memory. Small et al. (1997) showed that para-
phrased repetition is just as effective as verbatim
repetition (indeed, syntactic variation of common
semantics may assist comprehension). Further-
more, Rochon et al. (2000) suggested that the syn-
tactic complexity of utterances is not necessarily
the only predictor of comprehension in individuals
with AD; rather, correct comprehension of the se-
mantics of sentences is inversely related to the in-
creasing number of propositions used – it is prefer-
able to have as few clauses or core ideas as possi-
ble, i.e., one-at-a-time.
</bodyText>
<sectionHeader confidence="0.977871" genericHeader="method">
3 Data collection
</sectionHeader>
<bodyText confidence="0.9944027">
The data in this paper come from a study to
examine the feasibility and usability of a per-
sonal assistive robot to assist older adults with
AD in the completion of daily activities (Begum
et al., 2013). Ten older adults diagnosed with
AD, aged ≥ 55, and their caregivers were re-
cruited from a local memory clinic in Toronto,
Canada. Ethics approval was received from the
Toronto Rehabilitation Institute and the Univer-
sity of Toronto. Inclusion criteria included fluency
in English, normal hearing, and difficulty com-
pleting common sequences of steps, according to
their caregivers. Caregivers had to be a family
or privately-hired caregiver who provides regular
care (e.g., 7 hours/week) to the older adult partici-
pant. Following informed consent, the older adult
participants were screened using the Mini Mental
State Exam (MMSE) (Folstein et al., 2001) to as-
certain their general level of cognitive impairment.
Table 1 summarizes relevant demographics.
</bodyText>
<table confidence="0.999552636363636">
Sex Age (years) MMSE (/30)
OA1 F 76 9
OA2 M 86 24
OA3 M 88 25
OA4 F 77 25
OA5 F 59 18
OA6 M 63 23
OA7 F 77 25
OA8 F 83 19
OA9 F 84 25
OA10 M 85 15
</table>
<tableCaption confidence="0.998864">
Table 1: Demographics of older adults (OA).
</tableCaption>
<figureCaption confidence="0.773119666666667">
Figure 1: ED and two participants with AD during
the tea-making task in the kitchen of HomeLab at
TRI.
</figureCaption>
<subsectionHeader confidence="0.986599">
3.1 ED, the personal caregiver robot
</subsectionHeader>
<bodyText confidence="0.999980666666667">
The robot was built on an iRobot base (operat-
ing speed: 28 cm/second) and both its internal
construction and external enclosure were designed
and built at TRI. It is 102 cm in height and has
separate body and head components; the latter is
primarily a LCD monitor that shows audiovisual
prompts or displays a simple ‘smiley face’ other-23
wise, as shown in Figure 2. The robot has two
speakers embedded in its ‘chest’, two video cam-
eras (one in the head and one near the floor, for
navigation), and a microphone. For this study,
the built-in microphones were not used in favor of
environmental Kinect microphones, discussed be-
low. This was done to account for situations when
the robot and human participant were not in the
same room simultaneously.
The robot was tele-operated throughout the
task. The tele-operator continuously monitored
the task progress and the overall affective state
of the participants in a video stream sent by the
robot and triggered social conversation, asked
task-related questions, and delivered prompts to
guide the participants towards successful comple-
tion of the tea-making task (Fig. 1).
</bodyText>
<figureCaption confidence="0.994357">
Figure 2: The prototype robotic caregiver, ED.
</figureCaption>
<bodyText confidence="0.9999775">
The robot used the Cepstral commercial text-to-
speech (TTS) system using the U.S. English voice
‘David’ and its default parameters. This system
is based on the Festival text-to-speech platform in
many respects, including its use of linguistic pre-
processing (e.g., part-of-speech tagging) and cer-
tain heuristics (e.g., letter-to-sound rules). Spo-
ken prompts consisted of simple sentences, some-
times accompanied by short video demonstrations
designed to be easy to follow by people with a cog-
nitive impairment.
For efficient prompting, the tea-making task
was broken down into different steps or sub-task.
Audio or audio-video prompts corresponding to
each of these sub-tasks were recorded prior to
data collection. The human-robot interaction pro-
ceeded according to the following script when col-
laborating with the participants:
</bodyText>
<listItem confidence="0.998507368421052">
1. Allow the participant to initiate steps in each
sub-task, if they wish.
2. If a participant asks for directions, deliver the
appropriate prompt.
3. If a participant requests to perform the sub-
task in their own manner, agree if this does
not involve skipping an essential step.
4. If a participant asks about the location of an
item specific to the task, provide a full-body
gesture by physically orienting the robot to-
wards the sought item.
5. During water boiling, ask the participant to
put sugar or milk or tea bag in the cup. Time
permitting, engage in a social conversation,
e.g., about the weather.
6. When no prerecorded prompt sufficiently an-
swers a participant question, respond with the
correct answer (or “I don’t know”) through
the TTS engine.
</listItem>
<subsectionHeader confidence="0.999904">
3.2 Study set-up and procedures
</subsectionHeader>
<bodyText confidence="0.999970083333333">
Consent included recording video, audio, and
depth images with the Microsoft Kinect sensor in
HomeLab for all interviews and interactions with
ED. Following informed consent, older adults and
their caregivers were interviewed to acquire back-
ground information regarding their daily activi-
ties, the set-up of their home environment, and the
types of assistance that the caregiver typically pro-
vided for the older adult.
Participants were asked to observe ED mov-
ing in HomeLab and older adult participants were
asked to have a brief conversation with ED to
become oriented with the robot’s movement and
speech characteristics. The older adults were
then asked to complete the hand-washing and tea-
making tasks in the bathroom and kitchen, respec-
tively, with ED guiding them to the locations and
providing specific step-by-step prompts, as neces-
sary. The tele-operator observed the progress of
the task, and delivered the pre-recorded prompts
corresponding to the task step to guide the older
adult to complete each task. The TTS system
was used to respond to task-related questions and
to engage in social conversation. The caregivers24
were asked to observe the two tasks and to in-
tervene only if necessary (e.g., if the older adult
showed signs of distress or discomfort). The
older adult and caregiver participants were then
interviewed separately to gain their feedback on
the feasibility of using such a robot for assis-
tance with daily activities and usability of the sys-
tem. Each study session lasted approximately 2.5
hours including consent, introduction to the robot,
tea-making interaction with the robot, and post-
interaction interviews. The average duration for
the tea-making task alone was 12 minutes.
</bodyText>
<sectionHeader confidence="0.9923" genericHeader="evaluation">
4 Experiments and analysis
</sectionHeader>
<bodyText confidence="0.999781916666667">
Automatic speech recognition given these data is
complicated by several factors, including a pre-
ponderance of utterances in which human care-
givers speak concurrently with the participants, as
well as inordinately challenging levels of noise.
The estimated signal-to-noise ratio (SNR) across
utterances range from −3.42 dB to 8.14 dB, which
is extremely low compared to typical SNR of 40
dB in clean speech. One cause of this low SNR
is that microphones are placed in the environment,
rather than on the robot (so the distance to the mi-
crophone is variable, but relatively large) and that
the participant often has their back turned to the
microphone, as shown in figure 1.
As in previous work (Rudzicz et al., 2012),
we enhance speech signals with the log-spectral
amplitude estimator (LSAE) which minimizes the
mean squared error of the log spectra given a
model for the source speech Xk = Ake(jWk),
where Ak is the spectral amplitude. The LSAE
method is a modification of the short-time spectral
amplitude estimator that finds an estimate of the
spectral amplitude, ˆAk, that minimizes the distor-
tion
</bodyText>
<equation confidence="0.9892705">
2
E (logAk − log ˆAk) , (1)
</equation>
<bodyText confidence="0.52856">
such that the log-spectral amplitude estimate is
</bodyText>
<equation confidence="0.989607333333333">
ˆAk = exp (E [ln Ak  |Yk])
1 +kξk exp \ 2 1∞ et t dt/ Rk , (2)
k
</equation>
<bodyText confidence="0.998773068181818">
where ξk is the a priori SNR, Rk is the noisy spec-
tral amplitude, vk = ξk
1+ξk γk, and γk is thea pos-
teriori SNR (Erkelens et al., 2007). Often this is
based on a Gaussian model of noise, as it is here
(Ephraim and Malah, 1985).
As mentioned, there are many utterances in
which human caregivers speak concurrently with
the participants. This is partially confounded by
the fact that utterances by individuals with AD
tend to be shorter, so more of their utterance is lost,
proportionally. Examples of this type where the
caregiver’s voice is louder than the participant’s
voice are discarded, amounting to about 10% of
all utterances. In the following analyses, func-
tion words (i.e., prepositions, subordinating con-
junctions, and determiners) are removed from con-
sideration, although interjections are kept. Proper
names are also omitted.
We use the HTK (Young et al., 2006) toolchain,
which provides an implementation of a semi-
continuous hidden Markov model (HMM) that al-
lows state-tying and represents output densities by
mixtures of Gaussians. Features consisted of the
first 13 Mel-frequency cepstral coefficients, their
first (6) and second (66) derivatives, and the log
energy component, for 42 dimensions. Our own
data were z-scaled regardless of whether LSAE
noise reduction was applied.
Two language models (LMs) are used, both tri-
gram models derived from the English Gigaword
corpus, which contains 1200 word tokens (Graff
and Cieri, 2003). The first LM uses the first 5000
most frequent words and the second uses the first
64,000 most frequent words of that corpus. Five
acoustic models (AMs) are used with 1, 2, 4, 8,
and 16 Gaussians per output density respectively.
These are trained with approximately 211 hours
of spoken transcripts of the Wall Street Journal
(WSJ) from over one hundred non-pathological
speakers (Vertanen, 2006).
Table 2 shows, for the small- and large-
vocabulary LMs, the word-level accuracies of the
baseline HTK ASR system, as determined by
the inverse of the Levenshtein edit distance, for
two scenarios (sit-down interviews vs. during
the task), with and without LSAE noise reduc-
tion, for speech from individuals with AD and
for their caregivers. These values are computed
over all complexities of acoustic model and are
consistent with other tasks of this type (i.e., with
the challenges associated with the population and
recording set up), with this type of relatively un-
constrained ASR (Rudzicz et al., 2012). Apply-
ing LSAE results in a significant increase in ac-
curacy for both the small-vocabulary (right-tailed
homoscedastic t(58) = 3.9, p &lt; 0.005, CI =25
[6.19, ∞]) and large-vocabulary (right-tailed ho-
moscedastic t(58) = 2.4,p &lt; 0.01, CI =
[2.58, ∞]) tasks. For the participants with AD,
ASR accuracy is significantly higher in inter-
views (paired t(39) = 8.7,p &lt; 0.0001, CI =
[13.8, ∞]), which is expected due in large part
to the closer proximity of the microphone. Sur-
prisingly, ASR accuracy on participants with ASR
was not significantly different than on caregivers
(two-tailed heteroscedastic t(78) = −0.32,p =
0.75, CI = [−5.54,4.0]).
Figure 3 shows the mean ASR accuracy, with
standard error (Q/√n), for each of the small-
vocabulary and large-vocabulary ASR systems.
The exponential function b0 + b1 exp(b2x) is fit
to these data for each set, where bz are coef-
ficients that are iteratively adjustable via mean
squared error. For the small-vocabulary data,
R2 = 0.277 and F8 = 3.06,p = 0.12 ver-
sus the constant model. For the large-vocabulary
data, R2 = 0.445 and F8 = 2.81, p = 0.13
versus the constant model. Clearly, there is an
increasing trend in ASR accuracy with MMSE
scores, however an n-way ANOVA on ASR ac-
curacy scores reveals that this increase is not sig-
nificant (F1 = 47.07,p = 0.164). Furthermore,
neither the age (F1 = 1.39, p = 0.247) nor the sex
(F1 = 0.98, p = 0.33) of the participant had a sig-
nificant effect on ASR accuracy. An additional n-
way ANOVA reveals no strong interaction effects
between age, sex, and MMSE.
</bodyText>
<figure confidence="0.401651">
MMSE score
</figure>
<figureCaption confidence="0.47891425">
Figure 3: MMSE score versus mean ASR accu-
racy (with std. error bars) and fits of exponential
regression for each of the small-vocabulary and
large-vocabulary ASR systems.
</figureCaption>
<figure confidence="0.956208083333333">
35
Small vocab
Large vocab
30
25
20
15
10
8 10 12 14 16 18 20 22 24 26
ASR accuracy (%)
Scenario Noise reduction AD caregiver
Small vocabulary
</figure>
<equation confidence="0.831170230769231">
None 25.1 (Q = 9.9) 28.8 (Q = 6.0)
Interview
LSAE 40.9 (Q = 5.6) 40.2 (Q = 5.3)
None 13.7 (Q = 3.7) -
LSAE 19.2 (Q = 9.8) -
In task
Large vocabulary
None 23.7 (Q = 12.9) 27.0 (Q = 10.0)
Interview
LSAE 38.2 (Q = 6.3) 35.1 (Q = 11.2)
None 5.8 (Q = 3.7) -
LSAE 14.3 (Q = 12.8) -
In task
</equation>
<tableCaption confidence="0.8386445">
Table 2: ASR accuracy (means, and std. dev.) across speakers, scenario (interviews vs. during the task),
and presence of noise reduction for the small and large language models.
</tableCaption>
<sectionHeader confidence="0.995795" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9998054">
This study examined low-level aspects of speech
recognition among older adults with Alzheimer’s
disease interacting with a robot in a simulated
home environment. The best word-level accura-
cies of 40.9% (Q = 5.6) and 39.2% (Q = 6.3)
achievable with noise reduction and in a quiet in-
terview setting are comparable with the state-of-
the-art in unrestricted large-vocabulary text entry.
These results form the basis for ongoing work in
ASR and interaction design for this domain. The
trigram language model used in this work encap-
sulates the statistics of a large amount of speech
from the general population – it is a speaker-
independent model derived from a combination
of English news agencies that is not necessarily
representative of the type of language used in the
home, or by our target population. The acoustic
models were also derived from newswire data read
by younger adults in quiet environments. We are
currently training and adapting language models
tuned specifically to older adults with Alzheimer’s
disease using data from the Carolina Conversa-
tions database (Pope and Davis, 2011) and the De-
mentiaBank database (Boller and Becker, 1983).
Additionally, to function realistically, a lot of
ambient and background noise will need to be
overcome. We are currently looking into deploy-
ing a sensor network in the HomeLab that will in-
clude microphone arrays. Another method of im-
proving rates of correct word recognition is to aug-
ment the process from redundant information from
a concurrent sensory stream, i.e., in multimodal
interaction (Rudzicz, 2006). Combining gesture
and eye gaze with speech, for example, can be
used to disambiguate speech-only signals.
Although a focus of this paper, verbal infor-
mation is not the only modality in which human-26
robot interaction can take place. Indeed, Wil-
son et al. (2012) showed that experienced human
caregivers employed various non-verbal and semi-
verbal strategies to assist older adults with demen-
tia about 1/3 as often as verbal strategies (see sec-
tion 2.2). These non-verbal and semi-verbal strate-
gies included eye contact, sitting face-to-face, us-
ing hand gestures, a calm tone of voice, instru-
mental touch, exaggerated facial expressions, and
moving slowly. Multi-modal communication can
be extremely important for individuals with de-
mentia, who may require redundant channels for
disambiguating communication problems, espe-
cially if they have a language impairment or a sig-
nificant hearing impairment.
It is vital that our current technological ap-
proaches to caring for the elderly in their homes
progresses quickly, given the demographic shift
in many nations worldwide. This paper provides
a baseline assessment for the types of technical
and communicative challenges that will need to be
overcome in the near future to provide caregiving
assistance to a growing number of older adults.
</bodyText>
<sectionHeader confidence="0.999051" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999412">
The authors would like to thank Rajibul Huq and
Colin Harry, who designed and built the robot,
Jennifer Boger and Goldie Nejat for their assis-
tance in designing the study, and Sharon Cohen
for her consultations during the study.
</bodyText>
<sectionHeader confidence="0.998358" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.929421333333333">
American Psychiatric Association. 2000. Delirium,
dementia, and amnestic and other cognitive disor-
ders. In Diagnostic and Statistical Manual of Men-
tal Disorders, Text Revision (DSM-IV-TR), chap-
ter 2. American Psychiatric Association, Arlington,
VA, fourth edition.
</reference>
<table confidence="0.9584545">
M. A. Goodrich and A. C. Schultz. 2007. Human-
robot interaction: A survey. Foundations and Trends
in Human-Computer Interaction, 1:203–275.
A. Bauer, D. Wollherr, and M. Buss. 2008. Human-
robot collaboration: A survey. International Journal
of Humanoid Robotics, 5:47–66.
</table>
<reference confidence="0.992563584269664">
Momotaz Begum, Rosalie Wang, Rajibul Huq, and
Alex Mihailidis. 2013. Performance of daily ac-
tivities by older adults with dementia: The role of
an assistive robot. In Proceedings of the IEEE In-
ternational Conference on Rehabilitation Robotics,
Washington USA, June.
Ashok J. Bharucha, Vivek Anand, Jodi Forlizzi,
Mary Amanda Dew, Charles F. Reynolds III, Scott
Stevens, and Howard Wactlar. 2009. Intelligent
assistive technology applications to dementia care:
Current capabilities, limitations, and future chal-
lenges. American Journal of Geriatric Psychiatry,
17(2):88–104, February.
Franc¸ois Boller and James Becker. 1983. Dementia-
Bank database.
J.L. Burke and R.R. Murphy. 1999. Situation
awareness, team communication, and task perfor-
mance in robot-assisted technical search: Bujold
goes to bridgeport. CMPSCI Tech. Rep. CRASAR-
TR2004-23, University of South Florida.
B. Davis and M. Maclagan. 2009. Examining
pauses in Alzheimer’s discourse. American jour-
nal of Alzheimer’s Disease and other dementias,
24(2):141–154.
Y. Ephraim and D. Malah. 1985. Speech enhancement
using a minimum mean-square error log-spectral
amplitude estimator. Acoustics, Speech and Signal
Processing, IEEE Transactions on, 33(2):443 – 445,
apr.
Jan Erkelens, Jesper Jensen, and Richard Heusdens.
2007. A data-driven approach to optimizing spec-
tral speech enhancement methods for various error
criteria. Speech Communication, 49:530–541.
M. F. Folstein, S. E. Folstein, T. White, and M. A.
Messer. 2001. Mini-Mental State Examination
user’s guide. Odessa (FL): Psychological Assess-
ment Resources.
A. Freedy, E. de Visser, G. Weltman, and N. Coeyman.
2007. Measurement of trust in human-robot collab-
oration. In Proceedings of International Conference
on Collaborative Technologies and Systems, pages
17 –24.
Serge Gauthier, Michel Panisset, Josephine Nalban-
toglu, and Judes Poirier. 1997. Alzheimer’s dis-
ease: current knowledge, management and research.
Canadian Medical Association Journal, 157:1047–
1052.
R. Goldfarb and M.J.S. Pietro. 2004. Support systems:
Older adults with neurogenic communication dis-
orders. Journal of Ambulatory Care Management,
27(4):356–365. 2
David Graff and Christopher Cieri. 2003. English gi-
gaword. Linguistic Data Consortium.
S. A. Green, M. Billinghurst, X. Chen, and J. G. Chase.
2008. Human-robot collaboration: A literature re-
view and augmented reality approach in design. In-
ternational Journal Advanced Robotic Systems, 5:1–
18.
Curry Guinn and Anthony Habash. 2012. Technical
Report FS-12-01, Association for the Advancement
of Artificial Intelligence.
T Hopper. 2001. Indirect interventions to facilitate
communication in Alzheimers disease. Seminars in
Speech and Language, 22(4):305–315.
S. Klemmer, B. Hartmann, and L. Takayama. 2006.
How bodies matter: five themes for interaction de-
sign. In Proceedings of the conference on Designing
Interactive systems, pages 140–149.
Tracy Lee and Alex Mihaildis. 2005. An intelligent
emergency response system: Preliminary develop-
ment and testing of automated fall detection. Jour-
nal of Telemedicine and Telecare, 11:194–198.
Eric Lucet. 2012. Social Mobiserv Kompai Robot to
Assist People. In euRobotics workshop on Robots in
Healthcare and Welfare.
Alex Mihailidis, Jennifer N Boger, Tammy Craig, and
Jesse Hoey. 2008. The COACH prompting system
to assist older adults with dementia through hand-
washing: An efficacy study. BMC Geriatrics, 8(28).
Mehdi Mouad, Lounis Adouane, Pierre Schmitt,
Djamel Khadraoui, Benjamin Gˆateau, and Philippe
Martinet. 2010. Multi-agents based system to coor-
dinate mobile teamworking robots. In Proceedings
of the 4th Companion Robotics Institute, Brussels.
Elizabeth D. Mynatt, Anne-Sophie Melenhorst,
Arthur D. Fisk, and Wendy A. Rogers. 2004. Aware
technologies for aging in place: Understanding user
needs and attitudes. IEEE Pervasive Computing,
3:36–41.
Patrick Olivier, Andrew Monk, Guangyou Xu, and
Jesse Hoey. 2009. Ambient kitchen: Designing
situation services using a high fidelity prototyping
environment. In Proceedings of the ACM 2nd Inter-
national Conference on Pervasive Technologies Re-
lated to Assistive Environments, Corfu Greece.
S. V. Pakhomov, G. E. Smith, D. Chacon, Y. Feliciano,
N. Graff-Radford, R. Caselli, and D. S. Knopman.
2010. Computerized analysis of speech and lan-
guage to identify psycholinguistic correlates of fron-
totemporal lobar degeneration. Cognitive and Be-
havioral Neurology, 23:165–177.
M. E. Pollack. 2006. Autominder: A case study of as- mild cognitive impairment. In Proceedings of the
sistive technology for elders with cognitive impair- International Symposium on Quality of Life Technol-
ment. Generations, 30:67–69. ogy Intelligent Systems for Better Living, Las Vegas
USA, June.
Charlene Pope and Boyd H. Davis. 2011. Finding
a balance: The Carolinas Conversation Collection.
Corpus Linguistics and Linguistic Theory, 7(1).
J. Reilly, J. Troche, and M. Grossman. 2011. Lan-
guage processing in dementia. In A. E. Budson and
N. W. Kowall, editors, The Handbook ofAlzheimer’s
Disease and Other Dementias. Wiley-Blackwell.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffery Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Au-
dio, Speech, and Language Processing, 19(7):2081–
2090.
Elizabeth Rochon, Gloria S. Waters, and David Ca-
plan. 2000. The Relationship Between Measures
of Working Memory and Sentence Comprehension
in Patients With Alzheimer’s Disease. Journal of
Speech, Language, and Hearing Research, 43:395–
413.
Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Eliz-
abeth Rochon, and Carol Leonard. 2012. Commu-
nication strategies for a computerized caregiver for
individuals with alzheimer’s disease. In Proceed-
ings of the Third Workshop on Speech and Language
Processing for Assistive Technologies (SLPAT2012)
at the 13th Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL 2012), Montreal Canada, June.
Frank Rudzicz. 2006. Clavius: Bi-directional parsing
for generic multimodal interaction. In Proceedings
of the joint meeting of the International Conference
on Computational Linguistics and the Association
for Computational Linguistics, Sydney Australia.
Privender Saini, Boris de Ruyter, Panos Markopoulos,
and Albert van Breemen. 2005. Benefits of social
intelligence in home dialogue systems. In Proceed-
ings of INTERACT 2005, pages 510–521.
A. Serna, H. Pigot, and V. Rialle. 2007. Modeling the
progression of alzheimer’s disease for cognitive as-
sistance in smart homes. User Modelling and User-
Adapted Interaction, 17:415–438.
Jeff A. Small, Elaine S. Andersen, and Daniel Kem-
pler. 1997. Effects of working memory capacity
on understanding rate-altered speech. Aging, Neu-
ropsychology, and Cognition, 4(2):126–139.
M. Snover, B. Dorr, and R. Schwartz. 2004. A
lexically-driven algorithm for disfluency detection.
In ’Proceedings of HLT-NAACL 2004: Short Papers,
pages 157–160.
Adriana Tapus and Mohamed Chetouani. 2010.
ROBADOM: the impact of a domestic robot on the
psychological and cognitive state of the elderly with28
Cheryl K. Tomoeda, Kathryn A. Bayles, Daniel R.
Boone, Alfred W. Kaszniak, and Thomas J. Slau-
son. 1990. Speech rate and syntactic complexity
effects on the auditory comprehension of alzheimer
patients. Journal of Communication Disorders,
23(2):151 – 161.
Keith Vertanen. 2006. Baseline WSJ acoustic models
for HTK and Sphinx: Training recipes and recogni-
tion experiments. Technical report, Cavendish Lab-
oratory, University of Cambridge.
Rozanne Wilson, Elizabeth Rochon, Alex Mihailidis,
and Carol Leonard. 2012. Examining success of
communication strategies used by formal caregivers
assisting individuals with alzheimer’s disease during
an activity of daily living. Journal of Speech, Lan-
guage, and Hearing Research, 55:328–341, April.
Steve Young, Gunnar Evermann, Mark Gales, Thomas
Hain, Dan Kershaw, Xunying (Andrew) Liu, Gareth
Moore, Julian Odell, Dave Ollason and Dan Povey,
Valtcho Valtchev, and Phil Woodland. 2006. The
HTK Book (version 3.4).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.405936">
<title confidence="0.639866">Speech recognition in Alzheimer’s disease with personal assistive robots and and and</title>
<note confidence="0.884936">Rehabilitation Institute, Toronto ON; 2University of Toronto, Toronto of Massachussetts</note>
<abstract confidence="0.99860519047619">To help individuals with Alzheimer’s disease live at home for longer, we are developing a mobile robotic platform, called ED, intended to be used as a personal caregiver to help with the performance of activities of daily living. In a series of experiments, we study speech-based interactions between each of 10 older adults with Alzheimers disease and ED as the former makes tea in a simulated home environment. Analysis reveals that speech recognition remains a challenge for this recording environment, with word-level accuracies between 5.8% and 19.2% during household tasks with individuals with Alzheimer’s disease. This work provides a baseline assessment for the types of technical and communicative challenges that will need to be overcome in human-robot interaction for this population.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>American Psychiatric Association</author>
</authors>
<title>Delirium, dementia, and amnestic and other cognitive disorders.</title>
<date>2000</date>
<booktitle>In Diagnostic and Statistical Manual of Mental Disorders, Text Revision (DSM-IV-TR), chapter 2. American Psychiatric Association,</booktitle>
<location>Arlington, VA,</location>
<note>fourth edition.</note>
<contexts>
<context position="1368" citStr="Association, 2000" startWordPosition="203" endWordPosition="205">ognition remains a challenge for this recording environment, with word-level accuracies between 5.8% and 19.2% during household tasks with individuals with Alzheimer’s disease. This work provides a baseline assessment for the types of technical and communicative challenges that will need to be overcome in human-robot interaction for this population. 1 Introduction Alzheimer’s disease (AD) is a progressive neurodegenerative disorder primarily impairing memory, followed by declines in language, ability to carry out motor tasks, object recognition, and executive functioning (American Psychiatric Association, 2000; Gauthier et al., 1997). An accurate measure of functional decline comes from performance in activities of daily living (ADLs), such as shopping, finances, housework, and selfcare tasks. The deterioration in language comprehension and/or production resulting from specific brain damage, also known as aphasia, is a common feature of AD and other related conditions. Language changes observed clinically in older adults with dementia include increasing word-finding difficulties, loss of ability to verbally express information in detail, increasing use of 20 generic references (e.g., “it”), and pro</context>
</contexts>
<marker>Association, 2000</marker>
<rawString>American Psychiatric Association. 2000. Delirium, dementia, and amnestic and other cognitive disorders. In Diagnostic and Statistical Manual of Mental Disorders, Text Revision (DSM-IV-TR), chapter 2. American Psychiatric Association, Arlington, VA, fourth edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Momotaz Begum</author>
<author>Rosalie Wang</author>
<author>Rajibul Huq</author>
<author>Alex Mihailidis</author>
</authors>
<title>Performance of daily activities by older adults with dementia: The role of an assistive robot.</title>
<date>2013</date>
<booktitle>In Proceedings of the IEEE International Conference on Rehabilitation Robotics,</booktitle>
<location>Washington USA,</location>
<contexts>
<context position="4651" citStr="Begum et al., 2013" startWordPosition="696" endWordPosition="699">, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics feasibility and usability of a mobile assistive robot that uses the step-by-step prompting approaches for daily activities originally applied to our smart home research (Mihailidis et al., 2008). We conducted the study with older adults with mild or moderate AD and the tasks of hand washing and tea making. Our preliminary data analysis showed that the participants reacted well to the robot itself and the prompts that it provided, suggesting the feasibility of using personal robots for this application (Begum et al., 2013). One important identified issue is the need for an automatic speech recognition system to detect and understand utterances specifically from older adults with AD. The development of such a system will enable the assistive robot to better understand the behaviours and needs of these users for effective interactions and will further enhance environmentalbased smart home systems. This paper presents an analysis of the speech data collected from our participants with AD when interacting with the robot. In a series of experiments, we measure the performance of modern speech recognition with this p</context>
<context position="12974" citStr="Begum et al., 2013" startWordPosition="1988" endWordPosition="1991">comprehension). Furthermore, Rochon et al. (2000) suggested that the syntactic complexity of utterances is not necessarily the only predictor of comprehension in individuals with AD; rather, correct comprehension of the semantics of sentences is inversely related to the increasing number of propositions used – it is preferable to have as few clauses or core ideas as possible, i.e., one-at-a-time. 3 Data collection The data in this paper come from a study to examine the feasibility and usability of a personal assistive robot to assist older adults with AD in the completion of daily activities (Begum et al., 2013). Ten older adults diagnosed with AD, aged ≥ 55, and their caregivers were recruited from a local memory clinic in Toronto, Canada. Ethics approval was received from the Toronto Rehabilitation Institute and the University of Toronto. Inclusion criteria included fluency in English, normal hearing, and difficulty completing common sequences of steps, according to their caregivers. Caregivers had to be a family or privately-hired caregiver who provides regular care (e.g., 7 hours/week) to the older adult participant. Following informed consent, the older adult participants were screened using the</context>
</contexts>
<marker>Begum, Wang, Huq, Mihailidis, 2013</marker>
<rawString>Momotaz Begum, Rosalie Wang, Rajibul Huq, and Alex Mihailidis. 2013. Performance of daily activities by older adults with dementia: The role of an assistive robot. In Proceedings of the IEEE International Conference on Rehabilitation Robotics, Washington USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashok J Bharucha</author>
<author>Vivek Anand</author>
<author>Jodi Forlizzi</author>
<author>Mary Amanda Dew</author>
<author>Charles F Reynolds Scott Stevens</author>
<author>Howard Wactlar</author>
</authors>
<title>Intelligent assistive technology applications to dementia care: Current capabilities, limitations, and future challenges.</title>
<date>2009</date>
<journal>American Journal of Geriatric Psychiatry,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="2444" citStr="Bharucha et al., 2009" startWordPosition="358" endWordPosition="361">ord-finding difficulties, loss of ability to verbally express information in detail, increasing use of 20 generic references (e.g., “it”), and progressing difficulties understanding information presented verbally (American Psychiatric Association, 2000). Many nations are facing healthcare crises in the lack of capacity to support rapidly aging populations nor the chronic conditions associated with aging, including dementia. The current healthcare model of removing older adults from their homes and placing them into long-term care facilities is neither financially sustainable in this scenario (Bharucha et al., 2009), nor is it desirable. Our team has been developing “smart home” systems at the Toronto Rehabilitation Institute (TRI, part of the University Health Network) to help older adults “age-in-place” by providing different types of support, such as step-by-step prompts for daily tasks (Mihailidis et al., 2008), responses to emergency situations (Lee and Mihaildis, 2005), and means to communicate with family and friends. These systems are being evaluated within a completely functional re-creation of a one-bedroom apartment located within The TRI hospital, called HomeLab. These smart home technologies</context>
</contexts>
<marker>Bharucha, Anand, Forlizzi, Dew, Stevens, Wactlar, 2009</marker>
<rawString>Ashok J. Bharucha, Vivek Anand, Jodi Forlizzi, Mary Amanda Dew, Charles F. Reynolds III, Scott Stevens, and Howard Wactlar. 2009. Intelligent assistive technology applications to dementia care: Current capabilities, limitations, and future challenges. American Journal of Geriatric Psychiatry, 17(2):88–104, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Boller</author>
<author>James Becker</author>
</authors>
<date>1983</date>
<note>DementiaBank database.</note>
<contexts>
<context position="25771" citStr="Boller and Becker, 1983" startWordPosition="4122" endWordPosition="4125">sulates the statistics of a large amount of speech from the general population – it is a speakerindependent model derived from a combination of English news agencies that is not necessarily representative of the type of language used in the home, or by our target population. The acoustic models were also derived from newswire data read by younger adults in quiet environments. We are currently training and adapting language models tuned specifically to older adults with Alzheimer’s disease using data from the Carolina Conversations database (Pope and Davis, 2011) and the DementiaBank database (Boller and Becker, 1983). Additionally, to function realistically, a lot of ambient and background noise will need to be overcome. We are currently looking into deploying a sensor network in the HomeLab that will include microphone arrays. Another method of improving rates of correct word recognition is to augment the process from redundant information from a concurrent sensory stream, i.e., in multimodal interaction (Rudzicz, 2006). Combining gesture and eye gaze with speech, for example, can be used to disambiguate speech-only signals. Although a focus of this paper, verbal information is not the only modality in w</context>
</contexts>
<marker>Boller, Becker, 1983</marker>
<rawString>Franc¸ois Boller and James Becker. 1983. DementiaBank database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Burke</author>
<author>R R Murphy</author>
</authors>
<title>Situation awareness, team communication, and task performance in robot-assisted technical search: Bujold goes to bridgeport.</title>
<date>1999</date>
<tech>CMPSCI Tech. Rep. CRASARTR2004-23,</tech>
<institution>University of South Florida.</institution>
<contexts>
<context position="11027" citStr="Burke and Murphy, 1999" startWordPosition="1674" endWordPosition="1677">ving assistance from an entity without a physical body (such as an embedded system) (Klemmer et al., 2006). Physical embodiment also opens up the possibility of having more meaningful interaction between the older adult and the robot, as discussed in Section 5. Social collaboration between humans and robots often depends on communication in which each participant’s intention and goals are clear (Freedy et al., 2007; Bauer et al., 2008; Green et al., 2008). It is important that the human participant is able to construct a useable ‘mental model’ of the robot through bidirectional communication (Burke and Murphy, 1999) which can include both natural speech and non-verbal cues22 (e.g., hand gestures, gaze, facial cues), although speech tends to be far more effective (Green et al., 2008; Goodrich and Schultz, 2007). Automated communicative systems that are more sensitive to the emotive and the mental states of their users are often more successful than more neutral conversational agents (Saini et al., 2005). In order to be useful in practice, these communicative systems need to mimic some of the techniques employed by caregivers of individuals with AD. Often, these caregivers are employed by local clinics or </context>
</contexts>
<marker>Burke, Murphy, 1999</marker>
<rawString>J.L. Burke and R.R. Murphy. 1999. Situation awareness, team communication, and task performance in robot-assisted technical search: Bujold goes to bridgeport. CMPSCI Tech. Rep. CRASARTR2004-23, University of South Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Davis</author>
<author>M Maclagan</author>
</authors>
<title>Examining pauses in Alzheimer’s discourse.</title>
<date>2009</date>
<booktitle>American journal of Alzheimer’s Disease and other dementias,</booktitle>
<pages>24--2</pages>
<contexts>
<context position="9386" citStr="Davis and Maclagan, 2009" startWordPosition="1422" endWordPosition="1425"> is important to understand how their speech differs from that of the general population. This then can be integrated into future automatic speech recognition systems. Guinn and Habash (2012) showed, through an analysis of conversational dialogs, that repetition, incomplete words, and paraphrasing were significant indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and pronoun rate were not. Indeed, pauses, fillers, formulaic speech, restarts, and speech disfluencies are all hallmarks of speech in individuals with Alzheimer’s (Davis and Maclagan, 2009; Snover et al., 2004). Effects of Alzheimer’s disease on syntax remains controversial, with some evidence that deficits in syntax or of agrammatism could be due to memory deficits in the disease (Reilly et al., 2011). Other studies has applied similar analyses to related clinical groups. Pakhomov et al. (2010) identified several different features from the audio and corresponding transcripts of 38 patients with frontotemporal lobar degeneration (FTLD). They found that pause-to-word ratio and pronounto-noun ratios were especially discriminative of FTLD variants and that length, hesitancy, and </context>
</contexts>
<marker>Davis, Maclagan, 2009</marker>
<rawString>B. Davis and M. Maclagan. 2009. Examining pauses in Alzheimer’s discourse. American journal of Alzheimer’s Disease and other dementias, 24(2):141–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ephraim</author>
<author>D Malah</author>
</authors>
<title>Speech enhancement using a minimum mean-square error log-spectral amplitude estimator. Acoustics, Speech and Signal Processing,</title>
<date>1985</date>
<journal>IEEE Transactions on, 33(2):443 –</journal>
<volume>445</volume>
<contexts>
<context position="20057" citStr="Ephraim and Malah, 1985" startWordPosition="3163" endWordPosition="3166">ra given a model for the source speech Xk = Ake(jWk), where Ak is the spectral amplitude. The LSAE method is a modification of the short-time spectral amplitude estimator that finds an estimate of the spectral amplitude, ˆAk, that minimizes the distortion 2 E (logAk − log ˆAk) , (1) such that the log-spectral amplitude estimate is ˆAk = exp (E [ln Ak |Yk]) 1 +kξk exp \ 2 1∞ et t dt/ Rk , (2) k where ξk is the a priori SNR, Rk is the noisy spectral amplitude, vk = ξk 1+ξk γk, and γk is thea posteriori SNR (Erkelens et al., 2007). Often this is based on a Gaussian model of noise, as it is here (Ephraim and Malah, 1985). As mentioned, there are many utterances in which human caregivers speak concurrently with the participants. This is partially confounded by the fact that utterances by individuals with AD tend to be shorter, so more of their utterance is lost, proportionally. Examples of this type where the caregiver’s voice is louder than the participant’s voice are discarded, amounting to about 10% of all utterances. In the following analyses, function words (i.e., prepositions, subordinating conjunctions, and determiners) are removed from consideration, although interjections are kept. Proper names are al</context>
</contexts>
<marker>Ephraim, Malah, 1985</marker>
<rawString>Y. Ephraim and D. Malah. 1985. Speech enhancement using a minimum mean-square error log-spectral amplitude estimator. Acoustics, Speech and Signal Processing, IEEE Transactions on, 33(2):443 – 445, apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Erkelens</author>
<author>Jesper Jensen</author>
<author>Richard Heusdens</author>
</authors>
<title>A data-driven approach to optimizing spectral speech enhancement methods for various error criteria.</title>
<date>2007</date>
<journal>Speech Communication,</journal>
<pages>49--530</pages>
<contexts>
<context position="19966" citStr="Erkelens et al., 2007" startWordPosition="3145" endWordPosition="3148">ectral amplitude estimator (LSAE) which minimizes the mean squared error of the log spectra given a model for the source speech Xk = Ake(jWk), where Ak is the spectral amplitude. The LSAE method is a modification of the short-time spectral amplitude estimator that finds an estimate of the spectral amplitude, ˆAk, that minimizes the distortion 2 E (logAk − log ˆAk) , (1) such that the log-spectral amplitude estimate is ˆAk = exp (E [ln Ak |Yk]) 1 +kξk exp \ 2 1∞ et t dt/ Rk , (2) k where ξk is the a priori SNR, Rk is the noisy spectral amplitude, vk = ξk 1+ξk γk, and γk is thea posteriori SNR (Erkelens et al., 2007). Often this is based on a Gaussian model of noise, as it is here (Ephraim and Malah, 1985). As mentioned, there are many utterances in which human caregivers speak concurrently with the participants. This is partially confounded by the fact that utterances by individuals with AD tend to be shorter, so more of their utterance is lost, proportionally. Examples of this type where the caregiver’s voice is louder than the participant’s voice are discarded, amounting to about 10% of all utterances. In the following analyses, function words (i.e., prepositions, subordinating conjunctions, and determ</context>
</contexts>
<marker>Erkelens, Jensen, Heusdens, 2007</marker>
<rawString>Jan Erkelens, Jesper Jensen, and Richard Heusdens. 2007. A data-driven approach to optimizing spectral speech enhancement methods for various error criteria. Speech Communication, 49:530–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Folstein</author>
<author>S E Folstein</author>
<author>T White</author>
<author>M A Messer</author>
</authors>
<title>Mini-Mental State Examination user’s guide. Odessa (FL): Psychological Assessment Resources.</title>
<date>2001</date>
<contexts>
<context position="13628" citStr="Folstein et al., 2001" startWordPosition="2088" endWordPosition="2091">h AD, aged ≥ 55, and their caregivers were recruited from a local memory clinic in Toronto, Canada. Ethics approval was received from the Toronto Rehabilitation Institute and the University of Toronto. Inclusion criteria included fluency in English, normal hearing, and difficulty completing common sequences of steps, according to their caregivers. Caregivers had to be a family or privately-hired caregiver who provides regular care (e.g., 7 hours/week) to the older adult participant. Following informed consent, the older adult participants were screened using the Mini Mental State Exam (MMSE) (Folstein et al., 2001) to ascertain their general level of cognitive impairment. Table 1 summarizes relevant demographics. Sex Age (years) MMSE (/30) OA1 F 76 9 OA2 M 86 24 OA3 M 88 25 OA4 F 77 25 OA5 F 59 18 OA6 M 63 23 OA7 F 77 25 OA8 F 83 19 OA9 F 84 25 OA10 M 85 15 Table 1: Demographics of older adults (OA). Figure 1: ED and two participants with AD during the tea-making task in the kitchen of HomeLab at TRI. 3.1 ED, the personal caregiver robot The robot was built on an iRobot base (operating speed: 28 cm/second) and both its internal construction and external enclosure were designed and built at TRI. It is 10</context>
</contexts>
<marker>Folstein, Folstein, White, Messer, 2001</marker>
<rawString>M. F. Folstein, S. E. Folstein, T. White, and M. A. Messer. 2001. Mini-Mental State Examination user’s guide. Odessa (FL): Psychological Assessment Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Freedy</author>
<author>E de Visser</author>
<author>G Weltman</author>
<author>N Coeyman</author>
</authors>
<title>Measurement of trust in human-robot collaboration.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on Collaborative Technologies and Systems,</booktitle>
<pages>17--24</pages>
<marker>Freedy, de Visser, Weltman, Coeyman, 2007</marker>
<rawString>A. Freedy, E. de Visser, G. Weltman, and N. Coeyman. 2007. Measurement of trust in human-robot collaboration. In Proceedings of International Conference on Collaborative Technologies and Systems, pages 17 –24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Gauthier</author>
<author>Michel Panisset</author>
<author>Josephine Nalbantoglu</author>
<author>Judes Poirier</author>
</authors>
<title>Alzheimer’s disease: current knowledge, management and research.</title>
<date>1997</date>
<journal>Canadian Medical Association Journal,</journal>
<volume>157</volume>
<pages>1052</pages>
<contexts>
<context position="1392" citStr="Gauthier et al., 1997" startWordPosition="206" endWordPosition="209">challenge for this recording environment, with word-level accuracies between 5.8% and 19.2% during household tasks with individuals with Alzheimer’s disease. This work provides a baseline assessment for the types of technical and communicative challenges that will need to be overcome in human-robot interaction for this population. 1 Introduction Alzheimer’s disease (AD) is a progressive neurodegenerative disorder primarily impairing memory, followed by declines in language, ability to carry out motor tasks, object recognition, and executive functioning (American Psychiatric Association, 2000; Gauthier et al., 1997). An accurate measure of functional decline comes from performance in activities of daily living (ADLs), such as shopping, finances, housework, and selfcare tasks. The deterioration in language comprehension and/or production resulting from specific brain damage, also known as aphasia, is a common feature of AD and other related conditions. Language changes observed clinically in older adults with dementia include increasing word-finding difficulties, loss of ability to verbally express information in detail, increasing use of 20 generic references (e.g., “it”), and progressing difficulties un</context>
</contexts>
<marker>Gauthier, Panisset, Nalbantoglu, Poirier, 1997</marker>
<rawString>Serge Gauthier, Michel Panisset, Josephine Nalbantoglu, and Judes Poirier. 1997. Alzheimer’s disease: current knowledge, management and research. Canadian Medical Association Journal, 157:1047– 1052.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Goldfarb</author>
<author>M J S Pietro</author>
</authors>
<title>Support systems: Older adults with neurogenic communication disorders.</title>
<date>2004</date>
<journal>Journal of Ambulatory Care Management,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>2</pages>
<contexts>
<context position="11803" citStr="Goldfarb and Pietro, 2004" startWordPosition="1798" endWordPosition="1801">(Green et al., 2008; Goodrich and Schultz, 2007). Automated communicative systems that are more sensitive to the emotive and the mental states of their users are often more successful than more neutral conversational agents (Saini et al., 2005). In order to be useful in practice, these communicative systems need to mimic some of the techniques employed by caregivers of individuals with AD. Often, these caregivers are employed by local clinics or medical institutions and are trained by those institutions in ideal verbal communication strategies for use with those having dementia (Hopper, 2001; Goldfarb and Pietro, 2004). These include (Wilson et al., 2012) but are not limited to relatively slow rate of speech, verbatim repetition of misunderstood prompts, closedended (e.g., ‘yes/no’) questions, and reduced syntactic complexity. However, Tomoeda et al. (1990) showed that rates of speech that are too slow may interfere with comprehension if they introduce problems of short-term retention of working memory. Small et al. (1997) showed that paraphrased repetition is just as effective as verbatim repetition (indeed, syntactic variation of common semantics may assist comprehension). Furthermore, Rochon et al. (2000</context>
</contexts>
<marker>Goldfarb, Pietro, 2004</marker>
<rawString>R. Goldfarb and M.J.S. Pietro. 2004. Support systems: Older adults with neurogenic communication disorders. Journal of Ambulatory Care Management, 27(4):356–365. 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<title>English gigaword. Linguistic Data Consortium.</title>
<date>2003</date>
<contexts>
<context position="21279" citStr="Graff and Cieri, 2003" startWordPosition="3349" endWordPosition="3352">so omitted. We use the HTK (Young et al., 2006) toolchain, which provides an implementation of a semicontinuous hidden Markov model (HMM) that allows state-tying and represents output densities by mixtures of Gaussians. Features consisted of the first 13 Mel-frequency cepstral coefficients, their first (6) and second (66) derivatives, and the log energy component, for 42 dimensions. Our own data were z-scaled regardless of whether LSAE noise reduction was applied. Two language models (LMs) are used, both trigram models derived from the English Gigaword corpus, which contains 1200 word tokens (Graff and Cieri, 2003). The first LM uses the first 5000 most frequent words and the second uses the first 64,000 most frequent words of that corpus. Five acoustic models (AMs) are used with 1, 2, 4, 8, and 16 Gaussians per output density respectively. These are trained with approximately 211 hours of spoken transcripts of the Wall Street Journal (WSJ) from over one hundred non-pathological speakers (Vertanen, 2006). Table 2 shows, for the small- and largevocabulary LMs, the word-level accuracies of the baseline HTK ASR system, as determined by the inverse of the Levenshtein edit distance, for two scenarios (sit-do</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and Christopher Cieri. 2003. English gigaword. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Green</author>
<author>M Billinghurst</author>
<author>X Chen</author>
<author>J G Chase</author>
</authors>
<title>Human-robot collaboration: A literature review and augmented reality approach in design.</title>
<date>2008</date>
<journal>International Journal Advanced Robotic Systems,</journal>
<volume>5</volume>
<pages>18</pages>
<contexts>
<context position="8245" citStr="Green et al., 2008" startWordPosition="1249" endWordPosition="1252">tion system that had to be deactivated by the second day due to poor performance. One critical component for the successful use of these technological interventions is the usability of the communication interface for the targeted users, in this case older adults with Alzheimer’s disease. As in communication between two people, communication between the older adult and the robot may include natural, freeform speech (as opposed to simple spoken keyword interaction) and nonverbal cues (e.g., hand gestures, head pose, eye gaze, facial feature cues), although speech tends to be far more effective (Green et al., 2008; Goodrich and Schultz, 2007). Previous research indicates that automated communication systems are more effective if they take into account the affective and mental states of the user (Saini et al., 2005). Indeed, speech appears to be the most powerful mode of communication for an assistive robot to communicate with its users (Tapus and Chetouani, 2010; Lucet, 2012). 2.1 Language use in dementia and Alzheimer’s disease In order to design a speech interface for individuals with dementia, and AD in particular, it is important to understand how their speech differs from that of the general popul</context>
<context position="10863" citStr="Green et al., 2008" startWordPosition="1647" endWordPosition="1650">tion. 2.2 Human-robot interaction Receiving assistance from an entity with a physical body (such as a robot) is often psychologically more acceptable than receiving assistance from an entity without a physical body (such as an embedded system) (Klemmer et al., 2006). Physical embodiment also opens up the possibility of having more meaningful interaction between the older adult and the robot, as discussed in Section 5. Social collaboration between humans and robots often depends on communication in which each participant’s intention and goals are clear (Freedy et al., 2007; Bauer et al., 2008; Green et al., 2008). It is important that the human participant is able to construct a useable ‘mental model’ of the robot through bidirectional communication (Burke and Murphy, 1999) which can include both natural speech and non-verbal cues22 (e.g., hand gestures, gaze, facial cues), although speech tends to be far more effective (Green et al., 2008; Goodrich and Schultz, 2007). Automated communicative systems that are more sensitive to the emotive and the mental states of their users are often more successful than more neutral conversational agents (Saini et al., 2005). In order to be useful in practice, these</context>
</contexts>
<marker>Green, Billinghurst, Chen, Chase, 2008</marker>
<rawString>S. A. Green, M. Billinghurst, X. Chen, and J. G. Chase. 2008. Human-robot collaboration: A literature review and augmented reality approach in design. International Journal Advanced Robotic Systems, 5:1– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curry Guinn</author>
<author>Anthony Habash</author>
</authors>
<date>2012</date>
<journal>Association for the Advancement of Artificial Intelligence.</journal>
<tech>Technical Report FS-12-01,</tech>
<contexts>
<context position="8953" citStr="Guinn and Habash (2012)" startWordPosition="1361" endWordPosition="1364">on systems are more effective if they take into account the affective and mental states of the user (Saini et al., 2005). Indeed, speech appears to be the most powerful mode of communication for an assistive robot to communicate with its users (Tapus and Chetouani, 2010; Lucet, 2012). 2.1 Language use in dementia and Alzheimer’s disease In order to design a speech interface for individuals with dementia, and AD in particular, it is important to understand how their speech differs from that of the general population. This then can be integrated into future automatic speech recognition systems. Guinn and Habash (2012) showed, through an analysis of conversational dialogs, that repetition, incomplete words, and paraphrasing were significant indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and pronoun rate were not. Indeed, pauses, fillers, formulaic speech, restarts, and speech disfluencies are all hallmarks of speech in individuals with Alzheimer’s (Davis and Maclagan, 2009; Snover et al., 2004). Effects of Alzheimer’s disease on syntax remains controversial, with some evidence that deficits in syntax or of agrammatism could be due to me</context>
</contexts>
<marker>Guinn, Habash, 2012</marker>
<rawString>Curry Guinn and Anthony Habash. 2012. Technical Report FS-12-01, Association for the Advancement of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hopper</author>
</authors>
<title>Indirect interventions to facilitate communication in Alzheimers disease.</title>
<date>2001</date>
<booktitle>Seminars in Speech and Language,</booktitle>
<pages>22--4</pages>
<contexts>
<context position="11775" citStr="Hopper, 2001" startWordPosition="1796" endWordPosition="1797">ore effective (Green et al., 2008; Goodrich and Schultz, 2007). Automated communicative systems that are more sensitive to the emotive and the mental states of their users are often more successful than more neutral conversational agents (Saini et al., 2005). In order to be useful in practice, these communicative systems need to mimic some of the techniques employed by caregivers of individuals with AD. Often, these caregivers are employed by local clinics or medical institutions and are trained by those institutions in ideal verbal communication strategies for use with those having dementia (Hopper, 2001; Goldfarb and Pietro, 2004). These include (Wilson et al., 2012) but are not limited to relatively slow rate of speech, verbatim repetition of misunderstood prompts, closedended (e.g., ‘yes/no’) questions, and reduced syntactic complexity. However, Tomoeda et al. (1990) showed that rates of speech that are too slow may interfere with comprehension if they introduce problems of short-term retention of working memory. Small et al. (1997) showed that paraphrased repetition is just as effective as verbatim repetition (indeed, syntactic variation of common semantics may assist comprehension). Furt</context>
</contexts>
<marker>Hopper, 2001</marker>
<rawString>T Hopper. 2001. Indirect interventions to facilitate communication in Alzheimers disease. Seminars in Speech and Language, 22(4):305–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Klemmer</author>
<author>B Hartmann</author>
<author>L Takayama</author>
</authors>
<title>How bodies matter: five themes for interaction design.</title>
<date>2006</date>
<booktitle>In Proceedings of the conference on Designing Interactive systems,</booktitle>
<pages>140--149</pages>
<contexts>
<context position="3825" citStr="Klemmer et al., 2006" startWordPosition="570" endWordPosition="573">as in the ceiling. Fixing the location of these technologies carries a tradeoff between utility and feasibility – installing multiple hardware units at all locations where assistance could be required (e.g., bathroom, kitchen, and bedroom) can be expensive and cumbersome, but installing too few units will present gaps where a user’s activity will not be detected. Alternatively, integrating personal mobile robots with smart homes can overcome some of these tradeoffs. Moreover, assistance provided via a physically embodied robot is often more acceptable than that provided by an embedded system (Klemmer et al., 2006). With these potential advantages in mind, we conducted a ‘Wizard-of-Oz’ study to explore the Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 20–28, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics feasibility and usability of a mobile assistive robot that uses the step-by-step prompting approaches for daily activities originally applied to our smart home research (Mihailidis et al., 2008). We conducted the study with older adults with mild or moderate AD and the tasks of hand washing and tea m</context>
<context position="10510" citStr="Klemmer et al., 2006" startWordPosition="1591" endWordPosition="1594">unto-noun ratios were especially discriminative of FTLD variants and that length, hesitancy, and agramatism correspond to the phenomenology of FTLD. Roark et al. (2011) tested the ability of an automated classifier to distinguish patients with mild cognitive impairment from healthy controls that include acoustic features such as pause frequency and duration. 2.2 Human-robot interaction Receiving assistance from an entity with a physical body (such as a robot) is often psychologically more acceptable than receiving assistance from an entity without a physical body (such as an embedded system) (Klemmer et al., 2006). Physical embodiment also opens up the possibility of having more meaningful interaction between the older adult and the robot, as discussed in Section 5. Social collaboration between humans and robots often depends on communication in which each participant’s intention and goals are clear (Freedy et al., 2007; Bauer et al., 2008; Green et al., 2008). It is important that the human participant is able to construct a useable ‘mental model’ of the robot through bidirectional communication (Burke and Murphy, 1999) which can include both natural speech and non-verbal cues22 (e.g., hand gestures, </context>
</contexts>
<marker>Klemmer, Hartmann, Takayama, 2006</marker>
<rawString>S. Klemmer, B. Hartmann, and L. Takayama. 2006. How bodies matter: five themes for interaction design. In Proceedings of the conference on Designing Interactive systems, pages 140–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Lee</author>
<author>Alex Mihaildis</author>
</authors>
<title>An intelligent emergency response system: Preliminary development and testing of automated fall detection.</title>
<date>2005</date>
<journal>Journal of Telemedicine and Telecare,</journal>
<pages>11--194</pages>
<contexts>
<context position="2810" citStr="Lee and Mihaildis, 2005" startWordPosition="413" endWordPosition="416">e chronic conditions associated with aging, including dementia. The current healthcare model of removing older adults from their homes and placing them into long-term care facilities is neither financially sustainable in this scenario (Bharucha et al., 2009), nor is it desirable. Our team has been developing “smart home” systems at the Toronto Rehabilitation Institute (TRI, part of the University Health Network) to help older adults “age-in-place” by providing different types of support, such as step-by-step prompts for daily tasks (Mihailidis et al., 2008), responses to emergency situations (Lee and Mihaildis, 2005), and means to communicate with family and friends. These systems are being evaluated within a completely functional re-creation of a one-bedroom apartment located within The TRI hospital, called HomeLab. These smart home technologies use advanced sensing techniques and machine learning to autonomously react to their users, but they are fixed and embedded into the environment, e.g., as cameras in the ceiling. Fixing the location of these technologies carries a tradeoff between utility and feasibility – installing multiple hardware units at all locations where assistance could be required (e.g.</context>
</contexts>
<marker>Lee, Mihaildis, 2005</marker>
<rawString>Tracy Lee and Alex Mihaildis. 2005. An intelligent emergency response system: Preliminary development and testing of automated fall detection. Journal of Telemedicine and Telecare, 11:194–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Lucet</author>
</authors>
<title>Social Mobiserv Kompai Robot to Assist People.</title>
<date>2012</date>
<booktitle>In euRobotics workshop on Robots in Healthcare and Welfare.</booktitle>
<contexts>
<context position="6867" citStr="Lucet, 2012" startWordPosition="1040" endWordPosition="1041">king (Olivier et al., 2009). Mynatt et al. (2004) have been developing technologies to support aging-inplace such as the Cooks Collage, which uses a series of photos to remind the user what the last step completed was if the user is interrupted during21a cooking task. These interventions tend to be embedded in existing environments (e.g., around the sink area). More recent innovations have examined integrated robot-smart home systems where systems are embedded into existing environments that communicate with mobile assistive robots (e.g., CompanionAble, (Mouad et al., 2010); Mobiserv Kompai, (Lucet, 2012); and ROBADOM (Tapus and Chetouani, 2010)). Many of these projects are targeted towards older adults with cognitive impairment, and not specifically those with significant cognitive impairment. One of these systems, CompanionAble, with a fully autonomous assistive robot, has recently been tested in a simulated home environment for two days each with four older adults with dementia (AD or Pick’s disease/frontal lobe dementia) and two with mild cognitive impairment. The system provides assistance with various activities, including appointment reminders for activities input by users or caregivers</context>
<context position="8614" citStr="Lucet, 2012" startWordPosition="1308" endWordPosition="1309">robot may include natural, freeform speech (as opposed to simple spoken keyword interaction) and nonverbal cues (e.g., hand gestures, head pose, eye gaze, facial feature cues), although speech tends to be far more effective (Green et al., 2008; Goodrich and Schultz, 2007). Previous research indicates that automated communication systems are more effective if they take into account the affective and mental states of the user (Saini et al., 2005). Indeed, speech appears to be the most powerful mode of communication for an assistive robot to communicate with its users (Tapus and Chetouani, 2010; Lucet, 2012). 2.1 Language use in dementia and Alzheimer’s disease In order to design a speech interface for individuals with dementia, and AD in particular, it is important to understand how their speech differs from that of the general population. This then can be integrated into future automatic speech recognition systems. Guinn and Habash (2012) showed, through an analysis of conversational dialogs, that repetition, incomplete words, and paraphrasing were significant indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and pronoun rate </context>
</contexts>
<marker>Lucet, 2012</marker>
<rawString>Eric Lucet. 2012. Social Mobiserv Kompai Robot to Assist People. In euRobotics workshop on Robots in Healthcare and Welfare.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Mihailidis</author>
<author>Jennifer N Boger</author>
<author>Tammy Craig</author>
<author>Jesse Hoey</author>
</authors>
<title>The COACH prompting system to assist older adults with dementia through handwashing: An efficacy study.</title>
<date>2008</date>
<journal>BMC Geriatrics,</journal>
<volume>8</volume>
<issue>28</issue>
<contexts>
<context position="2749" citStr="Mihailidis et al., 2008" startWordPosition="404" endWordPosition="407"> lack of capacity to support rapidly aging populations nor the chronic conditions associated with aging, including dementia. The current healthcare model of removing older adults from their homes and placing them into long-term care facilities is neither financially sustainable in this scenario (Bharucha et al., 2009), nor is it desirable. Our team has been developing “smart home” systems at the Toronto Rehabilitation Institute (TRI, part of the University Health Network) to help older adults “age-in-place” by providing different types of support, such as step-by-step prompts for daily tasks (Mihailidis et al., 2008), responses to emergency situations (Lee and Mihaildis, 2005), and means to communicate with family and friends. These systems are being evaluated within a completely functional re-creation of a one-bedroom apartment located within The TRI hospital, called HomeLab. These smart home technologies use advanced sensing techniques and machine learning to autonomously react to their users, but they are fixed and embedded into the environment, e.g., as cameras in the ceiling. Fixing the location of these technologies carries a tradeoff between utility and feasibility – installing multiple hardware un</context>
<context position="4318" citStr="Mihailidis et al., 2008" startWordPosition="639" endWordPosition="642">e provided via a physically embodied robot is often more acceptable than that provided by an embedded system (Klemmer et al., 2006). With these potential advantages in mind, we conducted a ‘Wizard-of-Oz’ study to explore the Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 20–28, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics feasibility and usability of a mobile assistive robot that uses the step-by-step prompting approaches for daily activities originally applied to our smart home research (Mihailidis et al., 2008). We conducted the study with older adults with mild or moderate AD and the tasks of hand washing and tea making. Our preliminary data analysis showed that the participants reacted well to the robot itself and the prompts that it provided, suggesting the feasibility of using personal robots for this application (Begum et al., 2013). One important identified issue is the need for an automatic speech recognition system to detect and understand utterances specifically from older adults with AD. The development of such a system will enable the assistive robot to better understand the behaviours an</context>
<context position="6244" citStr="Mihailidis et al., 2008" startWordPosition="941" endWordPosition="944">lder adults with cognitive impairments has often focused on assistance with activities of daily living (i.e., reminders to do specific activities according to a schedule or prompts to perform activity steps), cognitive and social stimulation and emergency response systems. Archipel (Serna et al., 2007) recognizes the user’s intended plan and provides prompts, e.g. with cooking tasks. Autominder, (Pollack, 2006), provides context-appropriate reminders for activity schedules, and the COACH (Cognitive Orthosis for Assisting with aCtivities in the Home) system prompts for the task of handwashing (Mihailidis et al., 2008) and tea-making (Olivier et al., 2009). Mynatt et al. (2004) have been developing technologies to support aging-inplace such as the Cooks Collage, which uses a series of photos to remind the user what the last step completed was if the user is interrupted during21a cooking task. These interventions tend to be embedded in existing environments (e.g., around the sink area). More recent innovations have examined integrated robot-smart home systems where systems are embedded into existing environments that communicate with mobile assistive robots (e.g., CompanionAble, (Mouad et al., 2010); Mobiser</context>
</contexts>
<marker>Mihailidis, Boger, Craig, Hoey, 2008</marker>
<rawString>Alex Mihailidis, Jennifer N Boger, Tammy Craig, and Jesse Hoey. 2008. The COACH prompting system to assist older adults with dementia through handwashing: An efficacy study. BMC Geriatrics, 8(28).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Mouad</author>
<author>Lounis Adouane</author>
<author>Pierre Schmitt</author>
<author>Djamel Khadraoui</author>
<author>Benjamin Gˆateau</author>
<author>Philippe Martinet</author>
</authors>
<title>Multi-agents based system to coordinate mobile teamworking robots.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th Companion Robotics Institute,</booktitle>
<location>Brussels.</location>
<marker>Mouad, Adouane, Schmitt, Khadraoui, Gˆateau, Martinet, 2010</marker>
<rawString>Mehdi Mouad, Lounis Adouane, Pierre Schmitt, Djamel Khadraoui, Benjamin Gˆateau, and Philippe Martinet. 2010. Multi-agents based system to coordinate mobile teamworking robots. In Proceedings of the 4th Companion Robotics Institute, Brussels.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth D Mynatt</author>
<author>Anne-Sophie Melenhorst</author>
<author>Arthur D Fisk</author>
<author>Wendy A Rogers</author>
</authors>
<title>Aware technologies for aging in place: Understanding user needs and attitudes.</title>
<date>2004</date>
<journal>IEEE Pervasive Computing,</journal>
<pages>3--36</pages>
<contexts>
<context position="6304" citStr="Mynatt et al. (2004)" startWordPosition="951" endWordPosition="954">stance with activities of daily living (i.e., reminders to do specific activities according to a schedule or prompts to perform activity steps), cognitive and social stimulation and emergency response systems. Archipel (Serna et al., 2007) recognizes the user’s intended plan and provides prompts, e.g. with cooking tasks. Autominder, (Pollack, 2006), provides context-appropriate reminders for activity schedules, and the COACH (Cognitive Orthosis for Assisting with aCtivities in the Home) system prompts for the task of handwashing (Mihailidis et al., 2008) and tea-making (Olivier et al., 2009). Mynatt et al. (2004) have been developing technologies to support aging-inplace such as the Cooks Collage, which uses a series of photos to remind the user what the last step completed was if the user is interrupted during21a cooking task. These interventions tend to be embedded in existing environments (e.g., around the sink area). More recent innovations have examined integrated robot-smart home systems where systems are embedded into existing environments that communicate with mobile assistive robots (e.g., CompanionAble, (Mouad et al., 2010); Mobiserv Kompai, (Lucet, 2012); and ROBADOM (Tapus and Chetouani, 2</context>
</contexts>
<marker>Mynatt, Melenhorst, Fisk, Rogers, 2004</marker>
<rawString>Elizabeth D. Mynatt, Anne-Sophie Melenhorst, Arthur D. Fisk, and Wendy A. Rogers. 2004. Aware technologies for aging in place: Understanding user needs and attitudes. IEEE Pervasive Computing, 3:36–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Olivier</author>
<author>Andrew Monk</author>
<author>Guangyou Xu</author>
<author>Jesse Hoey</author>
</authors>
<title>Ambient kitchen: Designing situation services using a high fidelity prototyping environment.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACM 2nd International Conference on Pervasive Technologies</booktitle>
<contexts>
<context position="6282" citStr="Olivier et al., 2009" startWordPosition="947" endWordPosition="950">s often focused on assistance with activities of daily living (i.e., reminders to do specific activities according to a schedule or prompts to perform activity steps), cognitive and social stimulation and emergency response systems. Archipel (Serna et al., 2007) recognizes the user’s intended plan and provides prompts, e.g. with cooking tasks. Autominder, (Pollack, 2006), provides context-appropriate reminders for activity schedules, and the COACH (Cognitive Orthosis for Assisting with aCtivities in the Home) system prompts for the task of handwashing (Mihailidis et al., 2008) and tea-making (Olivier et al., 2009). Mynatt et al. (2004) have been developing technologies to support aging-inplace such as the Cooks Collage, which uses a series of photos to remind the user what the last step completed was if the user is interrupted during21a cooking task. These interventions tend to be embedded in existing environments (e.g., around the sink area). More recent innovations have examined integrated robot-smart home systems where systems are embedded into existing environments that communicate with mobile assistive robots (e.g., CompanionAble, (Mouad et al., 2010); Mobiserv Kompai, (Lucet, 2012); and ROBADOM (</context>
</contexts>
<marker>Olivier, Monk, Xu, Hoey, 2009</marker>
<rawString>Patrick Olivier, Andrew Monk, Guangyou Xu, and Jesse Hoey. 2009. Ambient kitchen: Designing situation services using a high fidelity prototyping environment. In Proceedings of the ACM 2nd International Conference on Pervasive Technologies Related to Assistive Environments, Corfu Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V Pakhomov</author>
<author>G E Smith</author>
<author>D Chacon</author>
<author>Y Feliciano</author>
<author>N Graff-Radford</author>
<author>R Caselli</author>
<author>D S Knopman</author>
</authors>
<title>Computerized analysis of speech and language to identify psycholinguistic correlates of frontotemporal lobar degeneration. Cognitive and Behavioral Neurology,</title>
<date>2010</date>
<pages>23--165</pages>
<contexts>
<context position="9698" citStr="Pakhomov et al. (2010)" startWordPosition="1471" endWordPosition="1474">t indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and pronoun rate were not. Indeed, pauses, fillers, formulaic speech, restarts, and speech disfluencies are all hallmarks of speech in individuals with Alzheimer’s (Davis and Maclagan, 2009; Snover et al., 2004). Effects of Alzheimer’s disease on syntax remains controversial, with some evidence that deficits in syntax or of agrammatism could be due to memory deficits in the disease (Reilly et al., 2011). Other studies has applied similar analyses to related clinical groups. Pakhomov et al. (2010) identified several different features from the audio and corresponding transcripts of 38 patients with frontotemporal lobar degeneration (FTLD). They found that pause-to-word ratio and pronounto-noun ratios were especially discriminative of FTLD variants and that length, hesitancy, and agramatism correspond to the phenomenology of FTLD. Roark et al. (2011) tested the ability of an automated classifier to distinguish patients with mild cognitive impairment from healthy controls that include acoustic features such as pause frequency and duration. 2.2 Human-robot interaction Receiving assistance</context>
</contexts>
<marker>Pakhomov, Smith, Chacon, Feliciano, Graff-Radford, Caselli, Knopman, 2010</marker>
<rawString>S. V. Pakhomov, G. E. Smith, D. Chacon, Y. Feliciano, N. Graff-Radford, R. Caselli, and D. S. Knopman. 2010. Computerized analysis of speech and language to identify psycholinguistic correlates of frontotemporal lobar degeneration. Cognitive and Behavioral Neurology, 23:165–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pollack</author>
</authors>
<title>Autominder: A case study of as- mild cognitive impairment.</title>
<date>2006</date>
<booktitle>In Proceedings of the</booktitle>
<location>Las Vegas USA,</location>
<contexts>
<context position="6034" citStr="Pollack, 2006" startWordPosition="912" endWordPosition="913">opment needs of a speech-based interface for robotic caregivers for older adults with AD. 2 Related Work Research in smart home systems, assistive robots, and integrated robot/smart home systems for older adults with cognitive impairments has often focused on assistance with activities of daily living (i.e., reminders to do specific activities according to a schedule or prompts to perform activity steps), cognitive and social stimulation and emergency response systems. Archipel (Serna et al., 2007) recognizes the user’s intended plan and provides prompts, e.g. with cooking tasks. Autominder, (Pollack, 2006), provides context-appropriate reminders for activity schedules, and the COACH (Cognitive Orthosis for Assisting with aCtivities in the Home) system prompts for the task of handwashing (Mihailidis et al., 2008) and tea-making (Olivier et al., 2009). Mynatt et al. (2004) have been developing technologies to support aging-inplace such as the Cooks Collage, which uses a series of photos to remind the user what the last step completed was if the user is interrupted during21a cooking task. These interventions tend to be embedded in existing environments (e.g., around the sink area). More recent inn</context>
</contexts>
<marker>Pollack, 2006</marker>
<rawString>M. E. Pollack. 2006. Autominder: A case study of as- mild cognitive impairment. In Proceedings of the sistive technology for elders with cognitive impair- International Symposium on Quality of Life Technolment. Generations, 30:67–69. ogy Intelligent Systems for Better Living, Las Vegas USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlene Pope</author>
<author>Boyd H Davis</author>
</authors>
<title>Finding a balance: The Carolinas Conversation Collection. Corpus Linguistics and Linguistic Theory,</title>
<date>2011</date>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="25715" citStr="Pope and Davis, 2011" startWordPosition="4113" endWordPosition="4116">n. The trigram language model used in this work encapsulates the statistics of a large amount of speech from the general population – it is a speakerindependent model derived from a combination of English news agencies that is not necessarily representative of the type of language used in the home, or by our target population. The acoustic models were also derived from newswire data read by younger adults in quiet environments. We are currently training and adapting language models tuned specifically to older adults with Alzheimer’s disease using data from the Carolina Conversations database (Pope and Davis, 2011) and the DementiaBank database (Boller and Becker, 1983). Additionally, to function realistically, a lot of ambient and background noise will need to be overcome. We are currently looking into deploying a sensor network in the HomeLab that will include microphone arrays. Another method of improving rates of correct word recognition is to augment the process from redundant information from a concurrent sensory stream, i.e., in multimodal interaction (Rudzicz, 2006). Combining gesture and eye gaze with speech, for example, can be used to disambiguate speech-only signals. Although a focus of this</context>
</contexts>
<marker>Pope, Davis, 2011</marker>
<rawString>Charlene Pope and Boyd H. Davis. 2011. Finding a balance: The Carolinas Conversation Collection. Corpus Linguistics and Linguistic Theory, 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reilly</author>
<author>J Troche</author>
<author>M Grossman</author>
</authors>
<title>Language processing in dementia.</title>
<date>2011</date>
<booktitle>The Handbook ofAlzheimer’s Disease and Other Dementias.</booktitle>
<editor>In A. E. Budson and N. W. Kowall, editors,</editor>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="9603" citStr="Reilly et al., 2011" startWordPosition="1457" endWordPosition="1460">f conversational dialogs, that repetition, incomplete words, and paraphrasing were significant indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and pronoun rate were not. Indeed, pauses, fillers, formulaic speech, restarts, and speech disfluencies are all hallmarks of speech in individuals with Alzheimer’s (Davis and Maclagan, 2009; Snover et al., 2004). Effects of Alzheimer’s disease on syntax remains controversial, with some evidence that deficits in syntax or of agrammatism could be due to memory deficits in the disease (Reilly et al., 2011). Other studies has applied similar analyses to related clinical groups. Pakhomov et al. (2010) identified several different features from the audio and corresponding transcripts of 38 patients with frontotemporal lobar degeneration (FTLD). They found that pause-to-word ratio and pronounto-noun ratios were especially discriminative of FTLD variants and that length, hesitancy, and agramatism correspond to the phenomenology of FTLD. Roark et al. (2011) tested the ability of an automated classifier to distinguish patients with mild cognitive impairment from healthy controls that include acoustic </context>
</contexts>
<marker>Reilly, Troche, Grossman, 2011</marker>
<rawString>J. Reilly, J. Troche, and M. Grossman. 2011. Language processing in dementia. In A. E. Budson and N. W. Kowall, editors, The Handbook ofAlzheimer’s Disease and Other Dementias. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Margaret Mitchell</author>
<author>John-Paul Hosom</author>
<author>Kristy Hollingshead</author>
<author>Jeffery Kaye</author>
</authors>
<title>Spoken language derived measures for detecting mild cognitive impairment.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>7</issue>
<contexts>
<context position="10057" citStr="Roark et al. (2011)" startWordPosition="1521" endWordPosition="1524">se on syntax remains controversial, with some evidence that deficits in syntax or of agrammatism could be due to memory deficits in the disease (Reilly et al., 2011). Other studies has applied similar analyses to related clinical groups. Pakhomov et al. (2010) identified several different features from the audio and corresponding transcripts of 38 patients with frontotemporal lobar degeneration (FTLD). They found that pause-to-word ratio and pronounto-noun ratios were especially discriminative of FTLD variants and that length, hesitancy, and agramatism correspond to the phenomenology of FTLD. Roark et al. (2011) tested the ability of an automated classifier to distinguish patients with mild cognitive impairment from healthy controls that include acoustic features such as pause frequency and duration. 2.2 Human-robot interaction Receiving assistance from an entity with a physical body (such as a robot) is often psychologically more acceptable than receiving assistance from an entity without a physical body (such as an embedded system) (Klemmer et al., 2006). Physical embodiment also opens up the possibility of having more meaningful interaction between the older adult and the robot, as discussed in Se</context>
</contexts>
<marker>Roark, Mitchell, Hosom, Hollingshead, Kaye, 2011</marker>
<rawString>Brian Roark, Margaret Mitchell, John-Paul Hosom, Kristy Hollingshead, and Jeffery Kaye. 2011. Spoken language derived measures for detecting mild cognitive impairment. IEEE Transactions on Audio, Speech, and Language Processing, 19(7):2081– 2090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Rochon</author>
<author>Gloria S Waters</author>
<author>David Caplan</author>
</authors>
<title>The Relationship Between Measures of Working Memory and Sentence Comprehension in Patients With Alzheimer’s Disease.</title>
<date>2000</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<volume>43</volume>
<pages>413</pages>
<contexts>
<context position="12404" citStr="Rochon et al. (2000)" startWordPosition="1890" endWordPosition="1893">b and Pietro, 2004). These include (Wilson et al., 2012) but are not limited to relatively slow rate of speech, verbatim repetition of misunderstood prompts, closedended (e.g., ‘yes/no’) questions, and reduced syntactic complexity. However, Tomoeda et al. (1990) showed that rates of speech that are too slow may interfere with comprehension if they introduce problems of short-term retention of working memory. Small et al. (1997) showed that paraphrased repetition is just as effective as verbatim repetition (indeed, syntactic variation of common semantics may assist comprehension). Furthermore, Rochon et al. (2000) suggested that the syntactic complexity of utterances is not necessarily the only predictor of comprehension in individuals with AD; rather, correct comprehension of the semantics of sentences is inversely related to the increasing number of propositions used – it is preferable to have as few clauses or core ideas as possible, i.e., one-at-a-time. 3 Data collection The data in this paper come from a study to examine the feasibility and usability of a personal assistive robot to assist older adults with AD in the completion of daily activities (Begum et al., 2013). Ten older adults diagnosed w</context>
</contexts>
<marker>Rochon, Waters, Caplan, 2000</marker>
<rawString>Elizabeth Rochon, Gloria S. Waters, and David Caplan. 2000. The Relationship Between Measures of Working Memory and Sentence Comprehension in Patients With Alzheimer’s Disease. Journal of Speech, Language, and Hearing Research, 43:395– 413.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Frank Rudzicz</author>
<author>Rozanne Wilson</author>
<author>Alex Mihailidis</author>
<author>Elizabeth Rochon</author>
<author>Carol Leonard</author>
</authors>
<title>Communication strategies for a computerized caregiver for individuals with alzheimer’s disease.</title>
<date>2012</date>
<booktitle>In Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies (SLPAT2012) at the 13th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012),</booktitle>
<location>Montreal Canada,</location>
<contexts>
<context position="19301" citStr="Rudzicz et al., 2012" startWordPosition="3018" endWordPosition="3021">reponderance of utterances in which human caregivers speak concurrently with the participants, as well as inordinately challenging levels of noise. The estimated signal-to-noise ratio (SNR) across utterances range from −3.42 dB to 8.14 dB, which is extremely low compared to typical SNR of 40 dB in clean speech. One cause of this low SNR is that microphones are placed in the environment, rather than on the robot (so the distance to the microphone is variable, but relatively large) and that the participant often has their back turned to the microphone, as shown in figure 1. As in previous work (Rudzicz et al., 2012), we enhance speech signals with the log-spectral amplitude estimator (LSAE) which minimizes the mean squared error of the log spectra given a model for the source speech Xk = Ake(jWk), where Ak is the spectral amplitude. The LSAE method is a modification of the short-time spectral amplitude estimator that finds an estimate of the spectral amplitude, ˆAk, that minimizes the distortion 2 E (logAk − log ˆAk) , (1) such that the log-spectral amplitude estimate is ˆAk = exp (E [ln Ak |Yk]) 1 +kξk exp \ 2 1∞ et t dt/ Rk , (2) k where ξk is the a priori SNR, Rk is the noisy spectral amplitude, vk = </context>
<context position="22281" citStr="Rudzicz et al., 2012" startWordPosition="3512" endWordPosition="3515">(Vertanen, 2006). Table 2 shows, for the small- and largevocabulary LMs, the word-level accuracies of the baseline HTK ASR system, as determined by the inverse of the Levenshtein edit distance, for two scenarios (sit-down interviews vs. during the task), with and without LSAE noise reduction, for speech from individuals with AD and for their caregivers. These values are computed over all complexities of acoustic model and are consistent with other tasks of this type (i.e., with the challenges associated with the population and recording set up), with this type of relatively unconstrained ASR (Rudzicz et al., 2012). Applying LSAE results in a significant increase in accuracy for both the small-vocabulary (right-tailed homoscedastic t(58) = 3.9, p &lt; 0.005, CI =25 [6.19, ∞]) and large-vocabulary (right-tailed homoscedastic t(58) = 2.4,p &lt; 0.01, CI = [2.58, ∞]) tasks. For the participants with AD, ASR accuracy is significantly higher in interviews (paired t(39) = 8.7,p &lt; 0.0001, CI = [13.8, ∞]), which is expected due in large part to the closer proximity of the microphone. Surprisingly, ASR accuracy on participants with ASR was not significantly different than on caregivers (two-tailed heteroscedastic t(78</context>
</contexts>
<marker>Rudzicz, Wilson, Mihailidis, Rochon, Leonard, 2012</marker>
<rawString>Frank Rudzicz, Rozanne Wilson, Alex Mihailidis, Elizabeth Rochon, and Carol Leonard. 2012. Communication strategies for a computerized caregiver for individuals with alzheimer’s disease. In Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies (SLPAT2012) at the 13th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012), Montreal Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rudzicz</author>
</authors>
<title>Clavius: Bi-directional parsing for generic multimodal interaction.</title>
<date>2006</date>
<booktitle>In Proceedings of the joint meeting of the International Conference on Computational Linguistics and the Association for Computational Linguistics,</booktitle>
<location>Sydney</location>
<contexts>
<context position="26183" citStr="Rudzicz, 2006" startWordPosition="4189" endWordPosition="4190">uage models tuned specifically to older adults with Alzheimer’s disease using data from the Carolina Conversations database (Pope and Davis, 2011) and the DementiaBank database (Boller and Becker, 1983). Additionally, to function realistically, a lot of ambient and background noise will need to be overcome. We are currently looking into deploying a sensor network in the HomeLab that will include microphone arrays. Another method of improving rates of correct word recognition is to augment the process from redundant information from a concurrent sensory stream, i.e., in multimodal interaction (Rudzicz, 2006). Combining gesture and eye gaze with speech, for example, can be used to disambiguate speech-only signals. Although a focus of this paper, verbal information is not the only modality in which human-26 robot interaction can take place. Indeed, Wilson et al. (2012) showed that experienced human caregivers employed various non-verbal and semiverbal strategies to assist older adults with dementia about 1/3 as often as verbal strategies (see section 2.2). These non-verbal and semi-verbal strategies included eye contact, sitting face-to-face, using hand gestures, a calm tone of voice, instrumental </context>
</contexts>
<marker>Rudzicz, 2006</marker>
<rawString>Frank Rudzicz. 2006. Clavius: Bi-directional parsing for generic multimodal interaction. In Proceedings of the joint meeting of the International Conference on Computational Linguistics and the Association for Computational Linguistics, Sydney Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Privender Saini</author>
<author>Boris de Ruyter</author>
<author>Panos Markopoulos</author>
<author>Albert van Breemen</author>
</authors>
<title>Benefits of social intelligence in home dialogue systems.</title>
<date>2005</date>
<booktitle>In Proceedings of INTERACT</booktitle>
<pages>510--521</pages>
<marker>Saini, de Ruyter, Markopoulos, van Breemen, 2005</marker>
<rawString>Privender Saini, Boris de Ruyter, Panos Markopoulos, and Albert van Breemen. 2005. Benefits of social intelligence in home dialogue systems. In Proceedings of INTERACT 2005, pages 510–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Serna</author>
<author>H Pigot</author>
<author>V Rialle</author>
</authors>
<title>Modeling the progression of alzheimer’s disease for cognitive assistance in smart homes. User Modelling and UserAdapted Interaction,</title>
<date>2007</date>
<pages>17--415</pages>
<contexts>
<context position="5923" citStr="Serna et al., 2007" startWordPosition="895" endWordPosition="898">without signal preprocessing. This work will serve as the basis for further studies by identifying some of the development needs of a speech-based interface for robotic caregivers for older adults with AD. 2 Related Work Research in smart home systems, assistive robots, and integrated robot/smart home systems for older adults with cognitive impairments has often focused on assistance with activities of daily living (i.e., reminders to do specific activities according to a schedule or prompts to perform activity steps), cognitive and social stimulation and emergency response systems. Archipel (Serna et al., 2007) recognizes the user’s intended plan and provides prompts, e.g. with cooking tasks. Autominder, (Pollack, 2006), provides context-appropriate reminders for activity schedules, and the COACH (Cognitive Orthosis for Assisting with aCtivities in the Home) system prompts for the task of handwashing (Mihailidis et al., 2008) and tea-making (Olivier et al., 2009). Mynatt et al. (2004) have been developing technologies to support aging-inplace such as the Cooks Collage, which uses a series of photos to remind the user what the last step completed was if the user is interrupted during21a cooking task.</context>
</contexts>
<marker>Serna, Pigot, Rialle, 2007</marker>
<rawString>A. Serna, H. Pigot, and V. Rialle. 2007. Modeling the progression of alzheimer’s disease for cognitive assistance in smart homes. User Modelling and UserAdapted Interaction, 17:415–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Small</author>
<author>Elaine S Andersen</author>
<author>Daniel Kempler</author>
</authors>
<title>Effects of working memory capacity on understanding rate-altered speech.</title>
<date>1997</date>
<journal>Aging, Neuropsychology, and Cognition,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="12215" citStr="Small et al. (1997)" startWordPosition="1863" endWordPosition="1866">e employed by local clinics or medical institutions and are trained by those institutions in ideal verbal communication strategies for use with those having dementia (Hopper, 2001; Goldfarb and Pietro, 2004). These include (Wilson et al., 2012) but are not limited to relatively slow rate of speech, verbatim repetition of misunderstood prompts, closedended (e.g., ‘yes/no’) questions, and reduced syntactic complexity. However, Tomoeda et al. (1990) showed that rates of speech that are too slow may interfere with comprehension if they introduce problems of short-term retention of working memory. Small et al. (1997) showed that paraphrased repetition is just as effective as verbatim repetition (indeed, syntactic variation of common semantics may assist comprehension). Furthermore, Rochon et al. (2000) suggested that the syntactic complexity of utterances is not necessarily the only predictor of comprehension in individuals with AD; rather, correct comprehension of the semantics of sentences is inversely related to the increasing number of propositions used – it is preferable to have as few clauses or core ideas as possible, i.e., one-at-a-time. 3 Data collection The data in this paper come from a study t</context>
</contexts>
<marker>Small, Andersen, Kempler, 1997</marker>
<rawString>Jeff A. Small, Elaine S. Andersen, and Daniel Kempler. 1997. Effects of working memory capacity on understanding rate-altered speech. Aging, Neuropsychology, and Cognition, 4(2):126–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>A lexically-driven algorithm for disfluency detection.</title>
<date>2004</date>
<booktitle>In ’Proceedings of HLT-NAACL 2004: Short Papers,</booktitle>
<pages>157--160</pages>
<contexts>
<context position="9408" citStr="Snover et al., 2004" startWordPosition="1426" endWordPosition="1429">d how their speech differs from that of the general population. This then can be integrated into future automatic speech recognition systems. Guinn and Habash (2012) showed, through an analysis of conversational dialogs, that repetition, incomplete words, and paraphrasing were significant indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and pronoun rate were not. Indeed, pauses, fillers, formulaic speech, restarts, and speech disfluencies are all hallmarks of speech in individuals with Alzheimer’s (Davis and Maclagan, 2009; Snover et al., 2004). Effects of Alzheimer’s disease on syntax remains controversial, with some evidence that deficits in syntax or of agrammatism could be due to memory deficits in the disease (Reilly et al., 2011). Other studies has applied similar analyses to related clinical groups. Pakhomov et al. (2010) identified several different features from the audio and corresponding transcripts of 38 patients with frontotemporal lobar degeneration (FTLD). They found that pause-to-word ratio and pronounto-noun ratios were especially discriminative of FTLD variants and that length, hesitancy, and agramatism correspond </context>
</contexts>
<marker>Snover, Dorr, Schwartz, 2004</marker>
<rawString>M. Snover, B. Dorr, and R. Schwartz. 2004. A lexically-driven algorithm for disfluency detection. In ’Proceedings of HLT-NAACL 2004: Short Papers, pages 157–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriana Tapus</author>
<author>Mohamed Chetouani</author>
</authors>
<title>ROBADOM: the impact of a domestic robot on the psychological and cognitive state of the elderly with28</title>
<date>2010</date>
<contexts>
<context position="6908" citStr="Tapus and Chetouani, 2010" startWordPosition="1044" endWordPosition="1047">. Mynatt et al. (2004) have been developing technologies to support aging-inplace such as the Cooks Collage, which uses a series of photos to remind the user what the last step completed was if the user is interrupted during21a cooking task. These interventions tend to be embedded in existing environments (e.g., around the sink area). More recent innovations have examined integrated robot-smart home systems where systems are embedded into existing environments that communicate with mobile assistive robots (e.g., CompanionAble, (Mouad et al., 2010); Mobiserv Kompai, (Lucet, 2012); and ROBADOM (Tapus and Chetouani, 2010)). Many of these projects are targeted towards older adults with cognitive impairment, and not specifically those with significant cognitive impairment. One of these systems, CompanionAble, with a fully autonomous assistive robot, has recently been tested in a simulated home environment for two days each with four older adults with dementia (AD or Pick’s disease/frontal lobe dementia) and two with mild cognitive impairment. The system provides assistance with various activities, including appointment reminders for activities input by users or caregivers, video calls, and cognitive exercises. P</context>
<context position="8600" citStr="Tapus and Chetouani, 2010" startWordPosition="1304" endWordPosition="1307">en the older adult and the robot may include natural, freeform speech (as opposed to simple spoken keyword interaction) and nonverbal cues (e.g., hand gestures, head pose, eye gaze, facial feature cues), although speech tends to be far more effective (Green et al., 2008; Goodrich and Schultz, 2007). Previous research indicates that automated communication systems are more effective if they take into account the affective and mental states of the user (Saini et al., 2005). Indeed, speech appears to be the most powerful mode of communication for an assistive robot to communicate with its users (Tapus and Chetouani, 2010; Lucet, 2012). 2.1 Language use in dementia and Alzheimer’s disease In order to design a speech interface for individuals with dementia, and AD in particular, it is important to understand how their speech differs from that of the general population. This then can be integrated into future automatic speech recognition systems. Guinn and Habash (2012) showed, through an analysis of conversational dialogs, that repetition, incomplete words, and paraphrasing were significant indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and</context>
</contexts>
<marker>Tapus, Chetouani, 2010</marker>
<rawString>Adriana Tapus and Mohamed Chetouani. 2010. ROBADOM: the impact of a domestic robot on the psychological and cognitive state of the elderly with28</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheryl K Tomoeda</author>
<author>Kathryn A Bayles</author>
<author>Daniel R Boone</author>
<author>Alfred W Kaszniak</author>
<author>Thomas J Slauson</author>
</authors>
<title>Speech rate and syntactic complexity effects on the auditory comprehension of alzheimer patients.</title>
<date>1990</date>
<journal>Journal of Communication Disorders,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>161</pages>
<contexts>
<context position="12046" citStr="Tomoeda et al. (1990)" startWordPosition="1835" endWordPosition="1838"> order to be useful in practice, these communicative systems need to mimic some of the techniques employed by caregivers of individuals with AD. Often, these caregivers are employed by local clinics or medical institutions and are trained by those institutions in ideal verbal communication strategies for use with those having dementia (Hopper, 2001; Goldfarb and Pietro, 2004). These include (Wilson et al., 2012) but are not limited to relatively slow rate of speech, verbatim repetition of misunderstood prompts, closedended (e.g., ‘yes/no’) questions, and reduced syntactic complexity. However, Tomoeda et al. (1990) showed that rates of speech that are too slow may interfere with comprehension if they introduce problems of short-term retention of working memory. Small et al. (1997) showed that paraphrased repetition is just as effective as verbatim repetition (indeed, syntactic variation of common semantics may assist comprehension). Furthermore, Rochon et al. (2000) suggested that the syntactic complexity of utterances is not necessarily the only predictor of comprehension in individuals with AD; rather, correct comprehension of the semantics of sentences is inversely related to the increasing number of</context>
</contexts>
<marker>Tomoeda, Bayles, Boone, Kaszniak, Slauson, 1990</marker>
<rawString>Cheryl K. Tomoeda, Kathryn A. Bayles, Daniel R. Boone, Alfred W. Kaszniak, and Thomas J. Slauson. 1990. Speech rate and syntactic complexity effects on the auditory comprehension of alzheimer patients. Journal of Communication Disorders, 23(2):151 – 161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Vertanen</author>
</authors>
<title>Baseline WSJ acoustic models for HTK and Sphinx: Training recipes and recognition experiments.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>Cavendish Laboratory, University of Cambridge.</institution>
<contexts>
<context position="21676" citStr="Vertanen, 2006" startWordPosition="3416" endWordPosition="3417">z-scaled regardless of whether LSAE noise reduction was applied. Two language models (LMs) are used, both trigram models derived from the English Gigaword corpus, which contains 1200 word tokens (Graff and Cieri, 2003). The first LM uses the first 5000 most frequent words and the second uses the first 64,000 most frequent words of that corpus. Five acoustic models (AMs) are used with 1, 2, 4, 8, and 16 Gaussians per output density respectively. These are trained with approximately 211 hours of spoken transcripts of the Wall Street Journal (WSJ) from over one hundred non-pathological speakers (Vertanen, 2006). Table 2 shows, for the small- and largevocabulary LMs, the word-level accuracies of the baseline HTK ASR system, as determined by the inverse of the Levenshtein edit distance, for two scenarios (sit-down interviews vs. during the task), with and without LSAE noise reduction, for speech from individuals with AD and for their caregivers. These values are computed over all complexities of acoustic model and are consistent with other tasks of this type (i.e., with the challenges associated with the population and recording set up), with this type of relatively unconstrained ASR (Rudzicz et al., </context>
</contexts>
<marker>Vertanen, 2006</marker>
<rawString>Keith Vertanen. 2006. Baseline WSJ acoustic models for HTK and Sphinx: Training recipes and recognition experiments. Technical report, Cavendish Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rozanne Wilson</author>
<author>Elizabeth Rochon</author>
<author>Alex Mihailidis</author>
<author>Carol Leonard</author>
</authors>
<title>Examining success of communication strategies used by formal caregivers assisting individuals with alzheimer’s disease during an activity of daily living.</title>
<date>2012</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<pages>55--328</pages>
<contexts>
<context position="11840" citStr="Wilson et al., 2012" startWordPosition="1804" endWordPosition="1807">2007). Automated communicative systems that are more sensitive to the emotive and the mental states of their users are often more successful than more neutral conversational agents (Saini et al., 2005). In order to be useful in practice, these communicative systems need to mimic some of the techniques employed by caregivers of individuals with AD. Often, these caregivers are employed by local clinics or medical institutions and are trained by those institutions in ideal verbal communication strategies for use with those having dementia (Hopper, 2001; Goldfarb and Pietro, 2004). These include (Wilson et al., 2012) but are not limited to relatively slow rate of speech, verbatim repetition of misunderstood prompts, closedended (e.g., ‘yes/no’) questions, and reduced syntactic complexity. However, Tomoeda et al. (1990) showed that rates of speech that are too slow may interfere with comprehension if they introduce problems of short-term retention of working memory. Small et al. (1997) showed that paraphrased repetition is just as effective as verbatim repetition (indeed, syntactic variation of common semantics may assist comprehension). Furthermore, Rochon et al. (2000) suggested that the syntactic comple</context>
<context position="26447" citStr="Wilson et al. (2012)" startWordPosition="4230" endWordPosition="4234">ent and background noise will need to be overcome. We are currently looking into deploying a sensor network in the HomeLab that will include microphone arrays. Another method of improving rates of correct word recognition is to augment the process from redundant information from a concurrent sensory stream, i.e., in multimodal interaction (Rudzicz, 2006). Combining gesture and eye gaze with speech, for example, can be used to disambiguate speech-only signals. Although a focus of this paper, verbal information is not the only modality in which human-26 robot interaction can take place. Indeed, Wilson et al. (2012) showed that experienced human caregivers employed various non-verbal and semiverbal strategies to assist older adults with dementia about 1/3 as often as verbal strategies (see section 2.2). These non-verbal and semi-verbal strategies included eye contact, sitting face-to-face, using hand gestures, a calm tone of voice, instrumental touch, exaggerated facial expressions, and moving slowly. Multi-modal communication can be extremely important for individuals with dementia, who may require redundant channels for disambiguating communication problems, especially if they have a language impairmen</context>
</contexts>
<marker>Wilson, Rochon, Mihailidis, Leonard, 2012</marker>
<rawString>Rozanne Wilson, Elizabeth Rochon, Alex Mihailidis, and Carol Leonard. 2012. Examining success of communication strategies used by formal caregivers assisting individuals with alzheimer’s disease during an activity of daily living. Journal of Speech, Language, and Hearing Research, 55:328–341, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Gunnar Evermann</author>
<author>Mark Gales</author>
<author>Thomas Hain</author>
<author>Dan Kershaw</author>
<author>Xunying Liu</author>
<author>Gareth Moore</author>
<author>Julian Odell</author>
<author>Dave Ollason</author>
<author>Dan Povey</author>
<author>Valtcho Valtchev</author>
<author>Phil Woodland</author>
</authors>
<date>2006</date>
<journal>The HTK Book (version</journal>
<volume>3</volume>
<contexts>
<context position="20704" citStr="Young et al., 2006" startWordPosition="3262" endWordPosition="3265">ny utterances in which human caregivers speak concurrently with the participants. This is partially confounded by the fact that utterances by individuals with AD tend to be shorter, so more of their utterance is lost, proportionally. Examples of this type where the caregiver’s voice is louder than the participant’s voice are discarded, amounting to about 10% of all utterances. In the following analyses, function words (i.e., prepositions, subordinating conjunctions, and determiners) are removed from consideration, although interjections are kept. Proper names are also omitted. We use the HTK (Young et al., 2006) toolchain, which provides an implementation of a semicontinuous hidden Markov model (HMM) that allows state-tying and represents output densities by mixtures of Gaussians. Features consisted of the first 13 Mel-frequency cepstral coefficients, their first (6) and second (66) derivatives, and the log energy component, for 42 dimensions. Our own data were z-scaled regardless of whether LSAE noise reduction was applied. Two language models (LMs) are used, both trigram models derived from the English Gigaword corpus, which contains 1200 word tokens (Graff and Cieri, 2003). The first LM uses the f</context>
</contexts>
<marker>Young, Evermann, Gales, Hain, Kershaw, Liu, Moore, Odell, Ollason, Povey, Valtchev, Woodland, 2006</marker>
<rawString>Steve Young, Gunnar Evermann, Mark Gales, Thomas Hain, Dan Kershaw, Xunying (Andrew) Liu, Gareth Moore, Julian Odell, Dave Ollason and Dan Povey, Valtcho Valtchev, and Phil Woodland. 2006. The HTK Book (version 3.4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
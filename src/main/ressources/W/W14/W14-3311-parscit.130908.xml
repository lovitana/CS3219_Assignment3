<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.996186">
Phrasal: A Toolkit for New Directions in Statistical Machine Translation
</title>
<author confidence="0.989197">
Spence Green, Daniel Cer, and Christopher D. Manning
</author>
<affiliation confidence="0.988697">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.99693">
{spenceg,danielcer,manning}@stanford.edu
</email>
<sectionHeader confidence="0.993861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999282">
We present a new version of Phrasal, an
open-source toolkit for statistical phrase-
based machine translation. This revision
includes features that support emerging re-
search trends such as (a) tuning with large
feature sets, (b) tuning on large datasets like
the bitext, and (c) web-based interactive ma-
chine translation. A direct comparison with
Moses shows favorable results in terms of
decoding speed and tuning time.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991475547169812">
In the early part of the last decade, phrase-based ma-
chine translation (MT) (Koehn et al., 2003) emerged
as the preeminent design of statistical MT systems.
However, most systems were proprietary or closed-
source, so progress was initially constrained by
the high engineering barrier to entry into the field.
Then Moses (Koehn et al., 2007) was released.
What followed was a flowering of work on all as-
pects of the translation problem, from rule extrac-
tion to deployment issues. Other toolkits appeared
including Joshua (Post et al., 2013), Jane (Wuebker
et al., 2012), cdec (Dyer et al., 2010) and the first
version of our package, Phrasal (Cer et al., 2010), a
Java-based, open source package.
This paper presents a completely re-designed
release of Phrasal that lowers the barrier to entry
into several exciting areas of MT research. First,
Phrasal exposes a simple yet flexible feature API for
building large-scale, feature-rich systems. Second,
Phrasal provides multi-threaded decoding and on-
line tuning for learning feature-rich models on very
large datasets, including the bitext. Third, Phrasal
supplies the key ingredients for web-based, inter-
active MT: an asynchronous RESTful JSON web
service implemented as a J2EE servlet, integrated
pre- and post-processing, and fast search.
Revisions to Phrasal were guided by several de-
sign choices. First, we optimized the system for
multi-core architectures, eschewing distributed in-
frastructure like Hadoop and MapReduce. While
“scaling-out” with distributed infrastructure is the
conventional industry and academic choice, we find
that “scaling-up” on a single large-node is an at-
tractive yet overlooked alternative (Appuswamy et
al., 2013). A single “scale-up” node is usually
competitive in terms of cost and performance, and
multi-core code has fewer dependencies in terms
of software and expertise. Second, Phrasal makes
extensive use of Java interfaces and reflection. This
is especially helpful in the feature API. A feature
function can be added to the system by simply im-
plementing an interface and specifying the class
name on the decoder command line. There is no
need to modify or recompile anything other than
the new feature function.
This paper presents a direct comparison of
Phrasal and Moses that shows favorable results
in terms of decoding speed and tuning time. An
indirect comparison via the WMT2014 shared
task (Neidert et al., 2014) showed that Phrasal
compares favorably to Moses in an evaluation
setting. The source code is freely available at:
http://nlp.stanford.edu/software/phrasal/
</bodyText>
<sectionHeader confidence="0.892635" genericHeader="method">
2 Standard System Pipeline
</sectionHeader>
<bodyText confidence="0.9998276">
This section describes the steps required to build
a phrase-based MT system from raw text. Each
step is implemented as a stand-alone executable.
For convenience, the Phrasal distribution includes
a script that coordinates the steps.
</bodyText>
<subsectionHeader confidence="0.884411">
2.1 Prerequisites
</subsectionHeader>
<bodyText confidence="0.998437">
Phrasal assumes offline preparation of word align-
ments and at least one target-side language model.
Word Alignment The rule extractor can accom-
modate either unsymmetrized or symmetrized
alignments. Unsymmetrized alignments can be
produced with either GIZA++ or the Berkeley
Aligner (Liang et al., 2006). Phrasal then applies
symmetrization on-the-fly using heuristics such as
grow-diag or grow-diag-final. If the alignments are
symmetrized separately, then Phrasal accepts align-
</bodyText>
<page confidence="0.980842">
114
</page>
<note confidence="0.4385595">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 114–121,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936523809524">
ments in the i-j Pharaoh format, which indicates
that source token i is aligned to target token j.
Language Modeling Phrasal can load any n-
gram language model saved in the ARPA format.
There are two LM loaders. The Java-based loader is
used by default and is appropriate for small-scale ex-
periments and pure-Java environments. The C++
KenLM (Heafield, 2011) loader1 is best for large-
scale LMs such as the unfiltered models produced
by lmplz (Heafield et al., 2013). Profiling shows
that LM queries often account for more than 50% of
the CPU time in a Phrasal decoding run, so we de-
signed the Phrasal KenLM loader to execute queries
mostly in C++ for efficiency. The KenLM bind-
ing efficiently passes full strings to C++ via JNI.
KenLM then iterates over the string, returning a
score and a state length. Phrasal can load multiple
language models, and includes native support for
the class-based language models that have become
popular in recent evaluations (Wuebker et al., 2012;
Ammar et al., 2013; Durrani et al., 2013).
</bodyText>
<subsectionHeader confidence="0.999433">
2.2 Rule Extraction
</subsectionHeader>
<bodyText confidence="0.999973192307692">
The next step in the pipeline is extraction of a phrase
table. Phrasal includes a multi-threaded version
of the rule extraction algorithm of Och and Ney
(2004). Phrase tables can be filtered to a specific
data set—as is common in research environments.
When filtering, the rule extractor lowers memory
utilization by splitting the data into arbitrary-sized
chunks and extracting rules from each chunk.
The rule extractor includes a feature API that is
independent of the decoder feature API. This al-
lows for storage of static rule feature values in the
phrase table. Static rule features are useful in two
cases. First, if a feature value depends on bitext
statistics, which are not accessible during tuning
or decoding, then that feature should be stored in
the phrase table. Examples are the standard phrase
translation probabilities, and the dense rule count
and rule uniqueness indicators described by Green
et al. (2013). Second, if a feature depends only
on the rule and is unlikely to change, then it may
be more efficient to store that feature value in the
phrase table. An example is a feature template that
indicates inclusion in a specific data domain (Dur-
rani et al., 2013). Rule extractor feature templates
must implement the FeatureExtractor inter-
face and are loaded via reflection.
</bodyText>
<footnote confidence="0.945045">
1Invoked by prefixing the LM path with the “kenlm:”.
</footnote>
<bodyText confidence="0.999863">
The rule extractor can also create lexicalized re-
ordering tables. The standard phrase orientation
model (Tillmann, 2004) and the hierarchical model
of Galley and Manning (2008) are available.
</bodyText>
<subsectionHeader confidence="0.998982">
2.3 Tuning
</subsectionHeader>
<bodyText confidence="0.9992742">
Once a language model has been estimated and a
phrase table has been extracted, the next step is to
estimate model weights. Phrasal supports tuning
over n-best lists, which permits rapid experimenta-
tion with different error metrics and loss functions.
Lattice-based tuning, while in principle more pow-
erful, requires metrics and losses that factor over
lattices, and in practice works no better than n-best
tuning (Cherry and Foster, 2012).
Tuning requires a parallel set {(ft, et)}Tt=1 of
source sentences ft and target references et.2
Phrasal follows the log-linear approach to phrase-
based translation (Och and Ney, 2004) in which
the predictive translation distribution p(e|f; w) is
modeled directly as
</bodyText>
<equation confidence="0.9949595">
p(e |f; w) = Z1f) exp I
wTφ(e, f)] (1)
</equation>
<bodyText confidence="0.99949124">
where w E Rd is the vector of model parameters,
φ(·) E Rd is a feature map, and Z(f) is an appro-
priate normalizing constant.
MT differs from other machine learning settings
in that it is not common to tune to log-likelihood
under (1). Instead, a gold error metric G(e&apos;, e) is
chosen that specifies the similarity between a hy-
pothesis e&apos; and a reference e, and that error is min-
imized over the tuning set. Phrasal includes Java
implementations of BLEU (Papineni et al., 2002),
NIST, and WER, and bindings for TER (Snover et
al., 2006) and METEOR (Denkowski and Lavie,
2011). The error metric is incorporated into a loss
function ` that returns the loss at either the sentence-
or corpus- level.
For conventional corpus-level (batch) tuning,
Phrasal includes multi-threaded implementations
of MERT (Och, 2003) and PRO (Hopkins and
May, 2011). The MERT implementation uses the
line search of Cer et al. (2008) to directly min-
imize corpus-level error. The PRO implementa-
tion uses a pairwise logistic loss to minimize the
number of inversions in the ranked n-best lists.
These batch implementations accumulate n-best
lists across epochs.
</bodyText>
<footnote confidence="0.989749">
2For simplicity, we assume one reference, but the multi-
reference case is analogous.
</footnote>
<page confidence="0.998314">
115
</page>
<bodyText confidence="0.999876833333333">
Online tuning is faster and more scalable than
batch tuning, and sometimes leads to better solu-
tions for non-convex settings like MT (Bottou and
Bousquet, 2011). Weight updates are performed
after each tuning example is decoded, and n-best
lists are not accumulated. Consequently, online tun-
ing is preferable for large tuning sets, or for rapid
iteration during development. Phrasal includes the
AdaGrad-based (Duchi et al., 2011) tuner of Green
et al. (2013). The regularization options are L2,
efficient L1 for feature selection (Duchi and Singer,
2009), or L1 + L2 (elastic net). There are two on-
line loss functions: a pairwise (PRO) objective and
a listwise minimum expected error objective (Och,
2003). These online loss functions require sentence-
level error metrics, several of which are available in
the toolkit: BLEU+1 (Lin and Och, 2004), Nakov
BLEU (Nakov et al., 2012), and TER.
</bodyText>
<subsectionHeader confidence="0.992734">
2.4 Decoding
</subsectionHeader>
<bodyText confidence="0.99998747826087">
The Phrasal decoder can be invoked either program-
matically as a Java object or as a standalone appli-
cation. In both cases the decoder is configured via
options that specify the language model, phrase
table, weight vector w, etc. The decoder is multi-
threaded, with one decoding instance per thread.
Each decoding instance has its own weight vector,
so in the programmatic case, it is possible to decode
simultaneously under different weight vectors.
Two search procedures are included. The default
is the phrase-based variant of cube pruning (Huang
and Chiang, 2007). The standard multi-stack beam
search (Och and Ney, 2004) is also an option. Ei-
ther procedure can be configured in one of several
recombination modes. The “Pharaoh” mode only
considers linear distortion, source coverage, and
target LM history. The “Exact” mode considers
these states in addition to any feature that declares
recombination state (see section 3.3).
The decoder includes several options for deploy-
ment environments such as an unknown word API,
pre-/post-processing APIs, and both full and prefix-
based force decoding.
</bodyText>
<subsectionHeader confidence="0.997404">
2.5 Evaluation and Post-processing
</subsectionHeader>
<bodyText confidence="0.999986333333333">
All of the error metrics available for tuning can
also be invoked for evaluation. For significance
testing, the toolkit includes an implementation of
the permutation test of Riezler and Maxwell (2005),
which was shown to be less susceptible to Type-I
error than bootstrap re-sampling (Koehn, 2004).
</bodyText>
<equation confidence="0.997701666666667">
r : s(r,w) r E R axiom
d&apos; : s(d&apos;,w) r E/ cov(d) item
|cov(d) |= |s |goal
</equation>
<tableCaption confidence="0.769452">
Table 1: Phrase-based MT as deductive inference.
</tableCaption>
<bodyText confidence="0.997836923076923">
This notation can be read as follows: if the an-
tecedents on the top are true, then the consequent
on the bottom is true subject to the conditions on
the right. The new item d&apos; is creating by appending
r to the ordered sequence of rules that define d.
Phrasal also includes two truecasing packages.
The LM-based truecaser (Lita et al., 2003) requires
an LM estimated from cased, tokenized text. A
subsequent detokenization step is thus necessary. A
more convenient alternative is the CRF-based post-
processor that can be trained to invert an arbitrary
pre-processor. This post-processor can perform
truecasing and detokenization in one pass.
</bodyText>
<sectionHeader confidence="0.996313" genericHeader="method">
3 Feature API
</sectionHeader>
<bodyText confidence="0.999969722222222">
Phrasal supports dynamic feature extraction dur-
ing tuning and decoding. In the API, feature tem-
plates are called featurizers. There are two types
with associated interfaces: RuleFeaturizer
and DerivationFeaturizer. One way to il-
lustrate these two featurizers is to consider phrase-
based decoding as a deductive system. Let r =
(f, e) be a rule in a set R, which is conventionally
called the phrase table. Let d = {rijZi=1 be an
ordered sequence of derivation N rules called a
derivation, which specifies a translation for some
source input sequence s (which, by some abuse of
notation, is equivalent to f in Eq. (1)). Finally,
define functions cov(d) as the source coverage set
of d as a bit vector and s(·, w) as the score of a rule
or derivation under w.3 The expression r E/ cov(d)
means that r maps to an empty/uncovered span in
cov(d). Table 1 shows the deductive system.
</bodyText>
<subsectionHeader confidence="0.99303">
3.1 Dynamic Rule Features
</subsectionHeader>
<bodyText confidence="0.997893166666667">
RuleFeaturizers are invoked when scoring axioms,
which do not require any derivation context. The
static rule features described in section 2.2 also
contribute to axiom scoring, and differ only from
RuleFeaturizers in that they are stored permanently
in the phrase table. In contrast, RuleFeaturizers
</bodyText>
<footnote confidence="0.761155">
3Note that s(d, w) = wTφ(d)in the log-linear formulation
of MT (see Eq. (1)).
</footnote>
<note confidence="0.262839">
d : w(d) r : s(r,w)
</note>
<page confidence="0.90607">
116
</page>
<figure confidence="0.837582133333333">
Listing 1: A RuleFeaturizer, which depends
only on a translation rule.
public class WordPenaltyFeaturizer
implements RuleFeaturizer I
@Override
public List&lt;FeatureValue&gt;
ruleFeaturize(Featurizable f) I
List&lt;FeatureValue&gt; features =
Generics.newLinkedList();
// Extract single feature
features.add(new FeatureValue(
&amp;quot;WordPenalty&amp;quot;, f.targetPhrase.size()));
return features;
}
}
</figure>
<bodyText confidence="0.995388181818182">
are extracted during decoding. An example feature
template is the word penalty, which is simply the
dimension of the target side of r (Listing 1).
Featurizable wraps decoder state from
which features can be extracted. RuleFeaturizers
are extracted during each phrase table query and
cached, so they can be simply efficiently retrieved
during decoding.
Once the feature is compiled, it is simply speci-
fied on the command-line when the decoder is exe-
cuted. No other configuration is required.
</bodyText>
<subsectionHeader confidence="0.999038">
3.2 Derivation Features
</subsectionHeader>
<bodyText confidence="0.999917818181818">
DerivationFeaturizers are invoked when scoring
items, and thus depend on some derivation context.
An example is the LM, which requires the n-gram
context from d to score r when creating the new
hypothesis d&apos; (Listing 2).
The LM featurizer first looks up the recombi-
nation state of the derivation, which contains the
n-gram context. Then it queries the LM by passing
the rule and context, and sets the new state as the
result of the LM query. Finally, it returns a feature
“LM” with the value of the LM query.
</bodyText>
<subsectionHeader confidence="0.999709">
3.3 Recombination State
</subsectionHeader>
<bodyText confidence="0.9943166">
Listing 2 shows a state lookup during feature ex-
traction. Phrase-based MT feature design differs
significantly from that of convex classifiers in terms
of the interaction with inference. For example, in
a maximum entropy classifier inference is exact,
so a good optimizer can simply nullify bad fea-
tures to retain baseline accuracy. In contrast, MT
feature templates affect search through both future
cost heuristics and recombination state. Bad fea-
tures can introduce search errors and thus decrease
</bodyText>
<figure confidence="0.887080590909091">
Listing 2: A DerivationFeaturizer, which
must lookup and save recombination state for ex-
traction.
public class NGramLanguageModelFeaturizer
extends DerivationFeaturizer I
@Override
public List&lt;FeatureValue&gt; featurize(
Featurizable f) I
// Get recombination state
LMState priorState = f.prior.getState(this);
// LM query
LMState state = lm.score(f.targetPhrase, priorState);
List&lt;FeatureValue&gt; features =
Generics.newLinkedList();
// Extract single feature
features.add(
new FeatureValue(&amp;quot;LM&amp;quot;, state.getScore()));
// Set new recombination state
f.setState(this, state);
return features;
}
}
</figure>
<bodyText confidence="0.98854925">
accuracy, sometimes catastrophically.
The feature API allows DerivationFeaturizers
to explicitly declare recombination state via the
FeaturizerState interface.4 The interface re-
quires a state equality operator and a hash code
function. Then the search procedure will only re-
combine derivations with equal states. For example,
the state of the n-gram LM DerivationFeaturizer
(Listing 2) is the n-1 gram context, and the hash-
code is a hash of that context string. Only deriva-
tions for which the equality operator of LMState
returns true can be recombined.
</bodyText>
<sectionHeader confidence="0.973463" genericHeader="method">
4 Web Service
</sectionHeader>
<bodyText confidence="0.99992025">
Machine translation output is increasingly uti-
lized in computer-assisted translation (CAT) work-
benches. To support deployment, Phrasal includes
a lightweight J2EE servlet that exposes a REST-
ful JSON API for querying a trained system. The
toolkit includes a standalone servlet container, but
the servlet may also be incorporated into a J2EE
server. The servlet requires just one input param-
eter: the Phrasal configuration file, which is also
used for tuning and decoding. Consequently, after
running the standard pipeline, the trained system
can be deployed with one command.
</bodyText>
<footnote confidence="0.98610875">
4To control future cost estimation, the designer would need
to write a new heuristic that considers perhaps a subset of
the full feature map. There is a separate API for future cost
heuristics.
</footnote>
<page confidence="0.994574">
117
</page>
<subsectionHeader confidence="0.989967">
4.1 Standard Web Service
</subsectionHeader>
<bodyText confidence="0.99774625">
The standard web service supports two types of
requests. The first is TranslationRequest,
which performs full decoding on a source input.
The JSON message structure is:
</bodyText>
<tableCaption confidence="0.552233">
Listing 3: TranslationRequest message.
TranslationRequest {
srcLang :(string),
tgtLang :(string),
srcText :(string),
tgtText :(string),
limit :(integer),
properties :(object)
}
</tableCaption>
<bodyText confidence="0.8603353">
The srcLang and tgtLang fields are ignored by
the servlet, but can be used by a middleware proxy
to route requests to Phrasal servlet instances, one
per language pair. The srcText field is the source
input, and properties is a Javascript associa-
tive array that can contain key/value pairs to pass
to the feature API. For example, we often use the
properties field to pass domain information
with each request.
Phrasal will perform full decoding and respond
with the message:
Listing 4: TranslationReply message,
which is returned upon successful processing of
TranslationRequest.
TranslationReply {
resultList :[
{tgtText :(string),
align :(string),
score :(float)
},...]
}
resultList is a ranked n-best list of transla-
tions, each with target tokens, word alignments,
and a score.
The second request type is RuleRequest,
which enables phrase table queries. These requests
are processed very quickly since decoding is not
required. The JSON message structure is:
Listing 5: RuleRequest message, which
prompts a direct lookup into the phrase table.
</bodyText>
<construct confidence="0.827196571428572">
RuleRequest {
srcLang :(string),
tgtLang :(string),
srcText :(string),
limit :(integer),
properties :(object)
}
</construct>
<bodyText confidence="0.994480666666667">
limit is the maximum number of translations to
return. The response message is analogous to that
for TranslationRequest, so we omit it.
</bodyText>
<subsectionHeader confidence="0.967445">
4.2 Interactive Machine Translation
</subsectionHeader>
<bodyText confidence="0.9999775">
Interactive machine translation (Bisbey and Kay,
1972) pairs human and machine translators in hopes
of increasing the throughput of high quality trans-
lation. It is an old idea that is again in focus. One
challenge is to present relevant machine suggestions
to humans. To that end, Phrasal supports context-
sensitive translation queries via prefix decod-
ing. Consider again the TranslationRequest
message. When the tgtText field is empty, the
source input is decoded from scratch. But when
this field contains a prefix, Phrasal returns transla-
tions that begin with the prefix. The search proce-
dure force decodes the prefix, and then completes
the translation via conventional decoding. Conse-
quently, if the user has typed a partial translation,
Phrasal can suggest completions conditioned on
that prefix. The longer the prefix, the faster the de-
coding, since the user prefix constrains the search
space. This feature allows Phrasal to produce in-
creasingly precise suggestions as the user works.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999735035714286">
We compare Phrasal and Moses by restricting an
existing large-scale system to a set of common fea-
tures. We start with the Arabic–English system of
Green et al. (2014), which is built from 6.6M paral-
lel segments. The system includes a 5-gram English
LM estimated from the target-side of the bitext and
990M English monolingual tokens. The feature set
is their dense baseline, but without lexicalized re-
ordering and the two extended phrase table features.
This leaves the nine baseline features also imple-
mented by Moses. We use the same phrase table,
phrase table query limit (20), and distortion limit
(5) for both decoders. The tuning set (mt023568)
contains 5,604 segments, and the development set
(mt04) contains 1,075 segments.
We ran all experiments on a dedicated server with
16 physical cores and 128GB of memory.
Figure 1 shows single-threaded decoding time
of the dev set as a function of the cube pruning
pop limit. At very low limits Moses is faster than
Phrasal, but then slows sharply. In contrast, Phrasal
scales linearly and is thus faster at higher pop limits.
Figure 2 shows multi-threaded decoding time of
the dev set with the cube pruning pop limit fixed
at 1,200. Here Phrasal is initially faster, but Moses
becomes more efficient at four threads. There are
two possible explanations. First, profiling shows
that LM queries account for approximately 75%
</bodyText>
<page confidence="0.985923">
118
</page>
<table confidence="0.972218416666667">
● ●
●
●
● ● ● ●
● ●
● ●
● ●
●
● ● ● System
● ●
● Phrasal
Moses
</table>
<figure confidence="0.5984605">
500 1000 1500 2000
Pop Limit
</figure>
<figureCaption confidence="0.861916">
Figure 1: Development set decoding time as a
function of the cube pruning pop limit.
</figureCaption>
<bodyText confidence="0.9998942">
of the Phrasal CPU-time. KenLM is written in
C++, and Phrasal queries it via JNI. It appears
as though multi-threading across this boundary is
a source of inefficiency. Second, we observe that
the Java parallel garbage collector (GC) runs up to
seven threads, which become increasingly active
as the number of decoder threads increases. These
and other Java overhead threads must be scheduled,
limiting gains as the number of decoding threads
approaches the number of physical cores.
Finally, Figure 3 shows tuning BLEU as a func-
tion of wallclock time. For Moses we chose the
batch MIRA implementation of Cherry and Fos-
ter (2012), which is popular for tuning feature-rich
systems. Phrasal uses the online tuner with the ex-
pected BLEU objective (Green et al., 2014). Moses
achieves a maximum BLEU score of 47.63 after
143 minutes of tuning, while Phrasal reaches this
level after just 17 minutes, later reaching a maxi-
mum BLEU of 47.75 after 42 minutes. Much of
the speedup can be attributed to phrase table and
LM loading time: the Phrasal tuner loads these data
structures just once, while the Moses tuner loads
them every epoch. Of course, this loading time be-
comes more significant with larger-scale systems.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99936625">
We presented a revised version of Phrasal, an open-
source, phrase-based MT toolkit. The revisions
support new directions in MT research including
feature-rich models, large-scale tuning, and web-
</bodyText>
<figureCaption confidence="0.9730635">
Figure 2: Development set decoding time as a
function of the threadpool size.
</figureCaption>
<figure confidence="0.8829316">
● ●●●●● ● ●●●
●●●●●●
●●● ●● ●
●
●
System
● ● Phrasal
Moses
0 100 200
Time (minutes)
</figure>
<figureCaption confidence="0.997813">
Figure 3: Approximate BLEU-4 during tuning
</figureCaption>
<bodyText confidence="0.982369466666667">
as a function of time over 25 tuning epochs. The
horizontal axis is accumulated time, while each
point indicates BLEU at the end of an epoch.
based interactive MT. A direct comparison with
Moses showed favorable performance on a large-
scale translation system.
Acknowledgments We thank Michel Galley for previous con-
tributions to Phrasal. The first author is supported by a National
Science Foundation Graduate Research Fellowship. This work
was supported by the Defense Advanced Research Projects
Agency (DARPA) Broad Operational Language Translation
(BOLT) program through IBM. Any opinions, findings, and
conclusions or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect the view
of DARPA or the US government.
</bodyText>
<figure confidence="0.991439807692308">
4 8 12 16
Threads
Time (seconds)
200
100
150
50
●
●
●
●
●
●
●
System
●
Phrasal
Moses
00
Time (seconds)
100
48
47
46
Approx. BLEU−4
45
</figure>
<page confidence="0.994975">
119
</page>
<sectionHeader confidence="0.989821" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999770484848485">
W. Ammar, V. Chahuneau, M. Denkowski, G. Hanne-
man, W. Ling, A. Matthews, et al. 2013. The CMU
machine translation systems at WMT 2013: Syntax,
synthetic translation options, and pseudo-references.
In WMT.
R. Appuswamy, C. Gkantsidis, D. Narayanan, O. Hod-
son, and A. Rowstron. 2013. Nobody ever got fired
for buying a cluster. Technical report, Microsoft Cor-
poration, MSR-TR-2013-2.
R. Bisbey and Kay. 1972. The MIND translation sys-
tem: a study in man-machine collaboration. Techni-
cal Report P-4786, Rand Corp., March.
L. Bottou and O. Bousquet. 2011. The tradeoffs of
large scale learning. In Optimization for Machine
Learning, pages 351–368. MIT Press.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regu-
larization and search for minimum error rate training.
In WMT.
D. Cer, M. Galley, D. Jurafsky, and C. D. Manning.
2010. Phrasal: A statistical machine translation
toolkit for exploring new model features. In HLT-
NAACL, Demonstration Session.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In HLT-NAACL.
M. Denkowski and A. Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation
of machine translation systems. In WMT.
J. Duchi and Y. Singer. 2009. Efficient online and batch
learning using forward backward splitting. JMLR,
10:2899–2934.
J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive sub-
gradient methods for online learning and stochastic
optimization. JMLR, 12:2121–2159.
N. Durrani, B. Haddow, K. Heafield, and P. Koehn.
2013. Edinburgh’s machine translation systems for
European language pairs. In WMT.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
et al. 2010. cdec: A decoder, alignment, and learn-
ing framework for finite-state and context-free trans-
lation models. In ACL System Demonstrations.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
EMNLP.
S. Green, S. Wang, D. Cer, and C. D. Manning. 2013.
Fast and adaptive online training of feature-rich trans-
lation models. In ACL.
S. Green, D. Cer, and C. D. Manning. 2014. An em-
pirical comparison of features and tuning for phrase-
based machine translation. In WMT.
K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013. Scalable modified Kneser-Ney language
model estimation. In ACL, Short Papers.
K. Heafield. 2011. KenLM: Faster and smaller lan-
guage model queries. In WMT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
EMNLP.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
ACL.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, et al. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Demonstration Session.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In EMNLP.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In COLING.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In ACL.
P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing
for sentence-level BLEU+1 yields short translations.
In COLING.
J. Neidert, S. Schuster, S. Green, K. Heafield, and C. D.
Manning. 2014. Stanford University’s submissions
to the WMT 2014 translation task. In WMT.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417–449.
F. J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL.
M. Post, J. Ganitkevitch, L. Orland, J. Weese, Y. Cao,
and C. Callison-Burch. 2013. Joshua 5.0: Sparser,
better, faster, server. In WMT.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in
automatic evaluation and significance testing in MT.
In ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
</reference>
<page confidence="0.995605">
120
</page>
<bodyText confidence="0.813229">
Open source phrase-based and hierarchical statisti-
cal machine translation. In COLING: Demonstration
Papers.
C. Tillmann. 2004. A unigram orientation model for
statistical machine translation. In NAACL.
</bodyText>
<reference confidence="0.8846625">
J. Wuebker, M. Huck, S. Peitz, M. Nuhn, M. Freitag,
J. T. Peter, S. Mansour, and H. Ney. 2012. Jane 2:
</reference>
<page confidence="0.997714">
121
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.992368">
<title confidence="0.999769">Phrasal: A Toolkit for New Directions in Statistical Machine Translation</title>
<author confidence="0.99809">Daniel Cer Green</author>
<author confidence="0.99809">D Manning</author>
<affiliation confidence="0.999925">Computer Science Department, Stanford University</affiliation>
<email confidence="0.999064">spenceg@stanford.edu</email>
<email confidence="0.999064">danielcer@stanford.edu</email>
<email confidence="0.999064">manning@stanford.edu</email>
<abstract confidence="0.999568727272727">We present a new version of Phrasal, an open-source toolkit for statistical phrasebased machine translation. This revision includes features that support emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W Ammar</author>
<author>V Chahuneau</author>
<author>M Denkowski</author>
<author>G Hanneman</author>
<author>W Ling</author>
<author>A Matthews</author>
</authors>
<title>The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references.</title>
<date>2013</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="5158" citStr="Ammar et al., 2013" startWordPosition="781" endWordPosition="784">rgescale LMs such as the unfiltered models produced by lmplz (Heafield et al., 2013). Profiling shows that LM queries often account for more than 50% of the CPU time in a Phrasal decoding run, so we designed the Phrasal KenLM loader to execute queries mostly in C++ for efficiency. The KenLM binding efficiently passes full strings to C++ via JNI. KenLM then iterates over the string, returning a score and a state length. Phrasal can load multiple language models, and includes native support for the class-based language models that have become popular in recent evaluations (Wuebker et al., 2012; Ammar et al., 2013; Durrani et al., 2013). 2.2 Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memory utilization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature API. This allows for storage of static rule feature values in the ph</context>
</contexts>
<marker>Ammar, Chahuneau, Denkowski, Hanneman, Ling, Matthews, 2013</marker>
<rawString>W. Ammar, V. Chahuneau, M. Denkowski, G. Hanneman, W. Ling, A. Matthews, et al. 2013. The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Appuswamy</author>
<author>C Gkantsidis</author>
<author>D Narayanan</author>
<author>O Hodson</author>
<author>A Rowstron</author>
</authors>
<title>Nobody ever got fired for buying a cluster.</title>
<date>2013</date>
<tech>Technical report,</tech>
<pages>2013--2</pages>
<institution>Microsoft Corporation,</institution>
<contexts>
<context position="2351" citStr="Appuswamy et al., 2013" startWordPosition="345" endWordPosition="348">uding the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON web service implemented as a J2EE servlet, integrated pre- and post-processing, and fast search. Revisions to Phrasal were guided by several design choices. First, we optimized the system for multi-core architectures, eschewing distributed infrastructure like Hadoop and MapReduce. While “scaling-out” with distributed infrastructure is the conventional industry and academic choice, we find that “scaling-up” on a single large-node is an attractive yet overlooked alternative (Appuswamy et al., 2013). A single “scale-up” node is usually competitive in terms of cost and performance, and multi-core code has fewer dependencies in terms of software and expertise. Second, Phrasal makes extensive use of Java interfaces and reflection. This is especially helpful in the feature API. A feature function can be added to the system by simply implementing an interface and specifying the class name on the decoder command line. There is no need to modify or recompile anything other than the new feature function. This paper presents a direct comparison of Phrasal and Moses that shows favorable results in</context>
</contexts>
<marker>Appuswamy, Gkantsidis, Narayanan, Hodson, Rowstron, 2013</marker>
<rawString>R. Appuswamy, C. Gkantsidis, D. Narayanan, O. Hodson, and A. Rowstron. 2013. Nobody ever got fired for buying a cluster. Technical report, Microsoft Corporation, MSR-TR-2013-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bisbey</author>
<author>Kay</author>
</authors>
<title>The MIND translation system: a study in man-machine collaboration.</title>
<date>1972</date>
<tech>Technical Report P-4786,</tech>
<location>Rand Corp.,</location>
<contexts>
<context position="18747" citStr="Bisbey and Kay, 1972" startWordPosition="2903" endWordPosition="2906">ts, and a score. The second request type is RuleRequest, which enables phrase table queries. These requests are processed very quickly since decoding is not required. The JSON message structure is: Listing 5: RuleRequest message, which prompts a direct lookup into the phrase table. RuleRequest { srcLang :(string), tgtLang :(string), srcText :(string), limit :(integer), properties :(object) } limit is the maximum number of translations to return. The response message is analogous to that for TranslationRequest, so we omit it. 4.2 Interactive Machine Translation Interactive machine translation (Bisbey and Kay, 1972) pairs human and machine translators in hopes of increasing the throughput of high quality translation. It is an old idea that is again in focus. One challenge is to present relevant machine suggestions to humans. To that end, Phrasal supports contextsensitive translation queries via prefix decoding. Consider again the TranslationRequest message. When the tgtText field is empty, the source input is decoded from scratch. But when this field contains a prefix, Phrasal returns translations that begin with the prefix. The search procedure force decodes the prefix, and then completes the translatio</context>
</contexts>
<marker>Bisbey, Kay, 1972</marker>
<rawString>R. Bisbey and Kay. 1972. The MIND translation system: a study in man-machine collaboration. Technical Report P-4786, Rand Corp., March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
<author>O Bousquet</author>
</authors>
<title>The tradeoffs of large scale learning.</title>
<date>2011</date>
<booktitle>In Optimization for Machine Learning,</booktitle>
<pages>351--368</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8880" citStr="Bottou and Bousquet, 2011" startWordPosition="1387" endWordPosition="1390">s multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. 2For simplicity, we assume one reference, but the multireference case is analogous. 115 Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example is decoded, and n-best lists are not accumulated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2, efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel er</context>
</contexts>
<marker>Bottou, Bousquet, 2011</marker>
<rawString>L. Bottou and O. Bousquet. 2011. The tradeoffs of large scale learning. In Optimization for Machine Learning, pages 351–368. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Regularization and search for minimum error rate training.</title>
<date>2008</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="8405" citStr="Cer et al. (2008)" startWordPosition="1313" endWordPosition="1316"> specifies the similarity between a hypothesis e&apos; and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. 2For simplicity, we assume one reference, but the multireference case is analogous. 115 Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example is decoded, and n-best lists are not accumulated. Consequently, onli</context>
</contexts>
<marker>Cer, Jurafsky, Manning, 2008</marker>
<rawString>D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regularization and search for minimum error rate training. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cer</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Phrasal: A statistical machine translation toolkit for exploring new model features. In HLTNAACL, Demonstration Session.</title>
<date>2010</date>
<contexts>
<context position="1317" citStr="Cer et al., 2010" startWordPosition="198" endWordPosition="201">ase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON web service implemented as a J2EE servlet, integrated pre- and post</context>
</contexts>
<marker>Cer, Galley, Jurafsky, Manning, 2010</marker>
<rawString>D. Cer, M. Galley, D. Jurafsky, and C. D. Manning. 2010. Phrasal: A statistical machine translation toolkit for exploring new model features. In HLTNAACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cherry</author>
<author>G Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="7195" citStr="Cherry and Foster, 2012" startWordPosition="1108" endWordPosition="1111"> can also create lexicalized reordering tables. The standard phrase orientation model (Tillmann, 2004) and the hierarchical model of Galley and Manning (2008) are available. 2.3 Tuning Once a language model has been estimated and a phrase table has been extracted, the next step is to estimate model weights. Phrasal supports tuning over n-best lists, which permits rapid experimentation with different error metrics and loss functions. Lattice-based tuning, while in principle more powerful, requires metrics and losses that factor over lattices, and in practice works no better than n-best tuning (Cherry and Foster, 2012). Tuning requires a parallel set {(ft, et)}Tt=1 of source sentences ft and target references et.2 Phrasal follows the log-linear approach to phrasebased translation (Och and Ney, 2004) in which the predictive translation distribution p(e|f; w) is modeled directly as p(e |f; w) = Z1f) exp I wTφ(e, f)] (1) where w E Rd is the vector of model parameters, φ(·) E Rd is a feature map, and Z(f) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e&apos;, e) is chosen that specif</context>
<context position="21883" citStr="Cherry and Foster (2012)" startWordPosition="3429" endWordPosition="3433">e Phrasal CPU-time. KenLM is written in C++, and Phrasal queries it via JNI. It appears as though multi-threading across this boundary is a source of inefficiency. Second, we observe that the Java parallel garbage collector (GC) runs up to seven threads, which become increasingly active as the number of decoder threads increases. These and other Java overhead threads must be scheduled, limiting gains as the number of decoding threads approaches the number of physical cores. Finally, Figure 3 shows tuning BLEU as a function of wallclock time. For Moses we chose the batch MIRA implementation of Cherry and Foster (2012), which is popular for tuning feature-rich systems. Phrasal uses the online tuner with the expected BLEU objective (Green et al., 2014). Moses achieves a maximum BLEU score of 47.63 after 143 minutes of tuning, while Phrasal reaches this level after just 17 minutes, later reaching a maximum BLEU of 47.75 after 42 minutes. Much of the speedup can be attributed to phrase table and LM loading time: the Phrasal tuner loads these data structures just once, while the Moses tuner loads them every epoch. Of course, this loading time becomes more significant with larger-scale systems. 6 Conclusion We p</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>C. Cherry and G. Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Denkowski</author>
<author>A Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="8073" citStr="Denkowski and Lavie, 2011" startWordPosition="1261" endWordPosition="1264">ed directly as p(e |f; w) = Z1f) exp I wTφ(e, f)] (1) where w E Rd is the vector of model parameters, φ(·) E Rd is a feature map, and Z(f) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e&apos;, e) is chosen that specifies the similarity between a hypothesis e&apos; and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. 2For simplicity, we assume one reference,</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>M. Denkowski and A. Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>Y Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>JMLR,</journal>
<pages>10--2899</pages>
<contexts>
<context position="9275" citStr="Duchi and Singer, 2009" startWordPosition="1447" endWordPosition="1450">assume one reference, but the multireference case is analogous. 115 Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example is decoded, and n-best lists are not accumulated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2, efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithr</context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>J. Duchi and Y. Singer. 2009. Efficient online and batch learning using forward backward splitting. JMLR, 10:2899–2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>E Hazan</author>
<author>Y Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="9150" citStr="Duchi et al., 2011" startWordPosition="1427" endWordPosition="1430">sions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. 2For simplicity, we assume one reference, but the multireference case is analogous. 115 Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example is decoded, and n-best lists are not accumulated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2, efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the d</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>J. Duchi, E. Hazan, and Y. Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Durrani</author>
<author>B Haddow</author>
<author>K Heafield</author>
<author>P Koehn</author>
</authors>
<title>Edinburgh’s machine translation systems for European language pairs.</title>
<date>2013</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="5181" citStr="Durrani et al., 2013" startWordPosition="785" endWordPosition="788"> the unfiltered models produced by lmplz (Heafield et al., 2013). Profiling shows that LM queries often account for more than 50% of the CPU time in a Phrasal decoding run, so we designed the Phrasal KenLM loader to execute queries mostly in C++ for efficiency. The KenLM binding efficiently passes full strings to C++ via JNI. KenLM then iterates over the string, returning a score and a state length. Phrasal can load multiple language models, and includes native support for the class-based language models that have become popular in recent evaluations (Wuebker et al., 2012; Ammar et al., 2013; Durrani et al., 2013). 2.2 Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memory utilization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature API. This allows for storage of static rule feature values in the phrase table. Static rule</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>N. Durrani, B. Haddow, K. Heafield, and P. Koehn. 2013. Edinburgh’s machine translation systems for European language pairs. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>A Lopez</author>
<author>J Ganitkevitch</author>
<author>J Weese</author>
<author>F Ture</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL System Demonstrations.</booktitle>
<contexts>
<context position="1252" citStr="Dyer et al., 2010" startWordPosition="186" endWordPosition="189">ing time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON we</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, 2010</marker>
<rawString>C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, et al. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6729" citStr="Galley and Manning (2008)" startWordPosition="1035" endWordPosition="1038">d by Green et al. (2013). Second, if a feature depends only on the rule and is unlikely to change, then it may be more efficient to store that feature value in the phrase table. An example is a feature template that indicates inclusion in a specific data domain (Durrani et al., 2013). Rule extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1Invoked by prefixing the LM path with the “kenlm:”. The rule extractor can also create lexicalized reordering tables. The standard phrase orientation model (Tillmann, 2004) and the hierarchical model of Galley and Manning (2008) are available. 2.3 Tuning Once a language model has been estimated and a phrase table has been extracted, the next step is to estimate model weights. Phrasal supports tuning over n-best lists, which permits rapid experimentation with different error metrics and loss functions. Lattice-based tuning, while in principle more powerful, requires metrics and losses that factor over lattices, and in practice works no better than n-best tuning (Cherry and Foster, 2012). Tuning requires a parallel set {(ft, et)}Tt=1 of source sentences ft and target references et.2 Phrasal follows the log-linear appro</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>S Wang</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Fast and adaptive online training of feature-rich translation models.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6128" citStr="Green et al. (2013)" startWordPosition="937" endWordPosition="940">ization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature API. This allows for storage of static rule feature values in the phrase table. Static rule features are useful in two cases. First, if a feature value depends on bitext statistics, which are not accessible during tuning or decoding, then that feature should be stored in the phrase table. Examples are the standard phrase translation probabilities, and the dense rule count and rule uniqueness indicators described by Green et al. (2013). Second, if a feature depends only on the rule and is unlikely to change, then it may be more efficient to store that feature value in the phrase table. An example is a feature template that indicates inclusion in a specific data domain (Durrani et al., 2013). Rule extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1Invoked by prefixing the LM path with the “kenlm:”. The rule extractor can also create lexicalized reordering tables. The standard phrase orientation model (Tillmann, 2004) and the hierarchical model of Galley and Manning (2008</context>
<context position="9179" citStr="Green et al. (2013)" startWordPosition="1433" endWordPosition="1436">sts. These batch implementations accumulate n-best lists across epochs. 2For simplicity, we assume one reference, but the multireference case is analogous. 115 Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example is decoded, and n-best lists are not accumulated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2, efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via opti</context>
</contexts>
<marker>Green, Wang, Cer, Manning, 2013</marker>
<rawString>S. Green, S. Wang, D. Cer, and C. D. Manning. 2013. Fast and adaptive online training of feature-rich translation models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>An empirical comparison of features and tuning for phrasebased machine translation.</title>
<date>2014</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="19865" citStr="Green et al. (2014)" startWordPosition="3082" endWordPosition="3085">gin with the prefix. The search procedure force decodes the prefix, and then completes the translation via conventional decoding. Consequently, if the user has typed a partial translation, Phrasal can suggest completions conditioned on that prefix. The longer the prefix, the faster the decoding, since the user prefix constrains the search space. This feature allows Phrasal to produce increasingly precise suggestions as the user works. 5 Experiments We compare Phrasal and Moses by restricting an existing large-scale system to a set of common features. We start with the Arabic–English system of Green et al. (2014), which is built from 6.6M parallel segments. The system includes a 5-gram English LM estimated from the target-side of the bitext and 990M English monolingual tokens. The feature set is their dense baseline, but without lexicalized reordering and the two extended phrase table features. This leaves the nine baseline features also implemented by Moses. We use the same phrase table, phrase table query limit (20), and distortion limit (5) for both decoders. The tuning set (mt023568) contains 5,604 segments, and the development set (mt04) contains 1,075 segments. We ran all experiments on a dedica</context>
<context position="22018" citStr="Green et al., 2014" startWordPosition="3452" endWordPosition="3455">source of inefficiency. Second, we observe that the Java parallel garbage collector (GC) runs up to seven threads, which become increasingly active as the number of decoder threads increases. These and other Java overhead threads must be scheduled, limiting gains as the number of decoding threads approaches the number of physical cores. Finally, Figure 3 shows tuning BLEU as a function of wallclock time. For Moses we chose the batch MIRA implementation of Cherry and Foster (2012), which is popular for tuning feature-rich systems. Phrasal uses the online tuner with the expected BLEU objective (Green et al., 2014). Moses achieves a maximum BLEU score of 47.63 after 143 minutes of tuning, while Phrasal reaches this level after just 17 minutes, later reaching a maximum BLEU of 47.75 after 42 minutes. Much of the speedup can be attributed to phrase table and LM loading time: the Phrasal tuner loads these data structures just once, while the Moses tuner loads them every epoch. Of course, this loading time becomes more significant with larger-scale systems. 6 Conclusion We presented a revised version of Phrasal, an opensource, phrase-based MT toolkit. The revisions support new directions in MT research incl</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>S. Green, D. Cer, and C. D. Manning. 2014. An empirical comparison of features and tuning for phrasebased machine translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
<author>I Pouzyrevsky</author>
<author>J H Clark</author>
<author>P Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation. In ACL, Short Papers.</title>
<date>2013</date>
<contexts>
<context position="4624" citStr="Heafield et al., 2013" startWordPosition="690" endWordPosition="693">Ninth Workshop on Statistical Machine Translation, pages 114–121, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics ments in the i-j Pharaoh format, which indicates that source token i is aligned to target token j. Language Modeling Phrasal can load any ngram language model saved in the ARPA format. There are two LM loaders. The Java-based loader is used by default and is appropriate for small-scale experiments and pure-Java environments. The C++ KenLM (Heafield, 2011) loader1 is best for largescale LMs such as the unfiltered models produced by lmplz (Heafield et al., 2013). Profiling shows that LM queries often account for more than 50% of the CPU time in a Phrasal decoding run, so we designed the Phrasal KenLM loader to execute queries mostly in C++ for efficiency. The KenLM binding efficiently passes full strings to C++ via JNI. KenLM then iterates over the string, returning a score and a state length. Phrasal can load multiple language models, and includes native support for the class-based language models that have become popular in recent evaluations (Wuebker et al., 2012; Ammar et al., 2013; Durrani et al., 2013). 2.2 Rule Extraction The next step in the </context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>K. Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In ACL, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="4517" citStr="Heafield, 2011" startWordPosition="673" endWordPosition="674">nal. If the alignments are symmetrized separately, then Phrasal accepts align114 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 114–121, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics ments in the i-j Pharaoh format, which indicates that source token i is aligned to target token j. Language Modeling Phrasal can load any ngram language model saved in the ARPA format. There are two LM loaders. The Java-based loader is used by default and is appropriate for small-scale experiments and pure-Java environments. The C++ KenLM (Heafield, 2011) loader1 is best for largescale LMs such as the unfiltered models produced by lmplz (Heafield et al., 2013). Profiling shows that LM queries often account for more than 50% of the CPU time in a Phrasal decoding run, so we designed the Phrasal KenLM loader to execute queries mostly in C++ for efficiency. The KenLM binding efficiently passes full strings to C++ via JNI. KenLM then iterates over the string, returning a score and a state length. Phrasal can load multiple language models, and includes native support for the class-based language models that have become popular in recent evaluations </context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>K. Heafield. 2011. KenLM: Faster and smaller language model queries. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hopkins</author>
<author>J May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8338" citStr="Hopkins and May, 2011" startWordPosition="1301" endWordPosition="1304">kelihood under (1). Instead, a gold error metric G(e&apos;, e) is chosen that specifies the similarity between a hypothesis e&apos; and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. 2For simplicity, we assume one reference, but the multireference case is analogous. 115 Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example i</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>M. Hopkins and J. May. 2011. Tuning as ranking. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="10188" citStr="Huang and Chiang, 2007" startWordPosition="1596" endWordPosition="1599">), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (Huang and Chiang, 2007). The standard multi-stack beam search (Och and Ney, 2004) is also an option. Either procedure can be configured in one of several recombination modes. The “Pharaoh” mode only considers linear distortion, source coverage, and target LM history. The “Exact” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics availabl</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="755" citStr="Koehn et al., 2003" startWordPosition="105" endWordPosition="108">cience Department, Stanford University {spenceg,danielcer,manning}@stanford.edu Abstract We present a new version of Phrasal, an open-source toolkit for statistical phrasebased machine translation. This revision includes features that support emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. T</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, Demonstration Session.</booktitle>
<contexts>
<context position="998" citStr="Koehn et al., 2007" startWordPosition="143" endWordPosition="146">emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation. In</title>
<date>2004</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="11052" citStr="Koehn, 2004" startWordPosition="1729" endWordPosition="1730">act” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics available for tuning can also be invoked for evaluation. For significance testing, the toolkit includes an implementation of the permutation test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r E R axiom d&apos; : s(d&apos;,w) r E/ cov(d) item |cov(d) |= |s |goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d&apos; is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM estimated from cased, tokenized text. A subsequent detokenization step is thus necessary. A more convenient alternative </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP. P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>ORANGE: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="9566" citStr="Lin and Och, 2004" startWordPosition="1495" endWordPosition="1498"> and n-best lists are not accumulated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2, efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (H</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L V Lita</author>
<author>A Ittycheriah</author>
<author>S Roukos</author>
<author>N Kambhatla</author>
</authors>
<title>tRuEcasIng.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11516" citStr="Lita et al., 2003" startWordPosition="1814" endWordPosition="1817">ion of the permutation test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r E R axiom d&apos; : s(d&apos;,w) r E/ cov(d) item |cov(d) |= |s |goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d&apos; is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM estimated from cased, tokenized text. A subsequent detokenization step is thus necessary. A more convenient alternative is the CRF-based postprocessor that can be trained to invert an arbitrary pre-processor. This post-processor can perform truecasing and detokenization in one pass. 3 Feature API Phrasal supports dynamic feature extraction during tuning and decoding. In the API, feature templates are called featurizers. There are two types with associated interfaces: RuleFeaturizer and DerivationFeaturizer. One way to illustrate these two featurizers is to consider phrasebased </context>
</contexts>
<marker>Lita, Ittycheriah, Roukos, Kambhatla, 2003</marker>
<rawString>L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 2003. tRuEcasIng. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>F Guzman</author>
<author>S Vogel</author>
</authors>
<title>Optimizing for sentence-level BLEU+1 yields short translations.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="9599" citStr="Nakov et al., 2012" startWordPosition="1501" endWordPosition="1504">lated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2, efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (Huang and Chiang, 2007). The stand</context>
</contexts>
<marker>Nakov, Guzman, Vogel, 2012</marker>
<rawString>P. Nakov, F. Guzman, and S. Vogel. 2012. Optimizing for sentence-level BLEU+1 yields short translations. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neidert</author>
<author>S Schuster</author>
<author>S Green</author>
<author>K Heafield</author>
<author>C D Manning</author>
</authors>
<title>Stanford University’s submissions to the WMT 2014 translation task.</title>
<date>2014</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="3066" citStr="Neidert et al., 2014" startWordPosition="462" endWordPosition="465">core code has fewer dependencies in terms of software and expertise. Second, Phrasal makes extensive use of Java interfaces and reflection. This is especially helpful in the feature API. A feature function can be added to the system by simply implementing an interface and specifying the class name on the decoder command line. There is no need to modify or recompile anything other than the new feature function. This paper presents a direct comparison of Phrasal and Moses that shows favorable results in terms of decoding speed and tuning time. An indirect comparison via the WMT2014 shared task (Neidert et al., 2014) showed that Phrasal compares favorably to Moses in an evaluation setting. The source code is freely available at: http://nlp.stanford.edu/software/phrasal/ 2 Standard System Pipeline This section describes the steps required to build a phrase-based MT system from raw text. Each step is implemented as a stand-alone executable. For convenience, the Phrasal distribution includes a script that coordinates the steps. 2.1 Prerequisites Phrasal assumes offline preparation of word alignments and at least one target-side language model. Word Alignment The rule extractor can accommodate either unsymmet</context>
</contexts>
<marker>Neidert, Schuster, Green, Heafield, Manning, 2014</marker>
<rawString>J. Neidert, S. Schuster, S. Green, K. Heafield, and C. D. Manning. 2014. Stanford University’s submissions to the WMT 2014 translation task. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="5362" citStr="Och and Ney (2004)" startWordPosition="815" endWordPosition="818">ned the Phrasal KenLM loader to execute queries mostly in C++ for efficiency. The KenLM binding efficiently passes full strings to C++ via JNI. KenLM then iterates over the string, returning a score and a state length. Phrasal can load multiple language models, and includes native support for the class-based language models that have become popular in recent evaluations (Wuebker et al., 2012; Ammar et al., 2013; Durrani et al., 2013). 2.2 Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memory utilization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature API. This allows for storage of static rule feature values in the phrase table. Static rule features are useful in two cases. First, if a feature value depends on bitext statistics, which are not accessible during tuning or decoding, then that feature should be stored in </context>
<context position="7379" citStr="Och and Ney, 2004" startWordPosition="1136" endWordPosition="1139">a language model has been estimated and a phrase table has been extracted, the next step is to estimate model weights. Phrasal supports tuning over n-best lists, which permits rapid experimentation with different error metrics and loss functions. Lattice-based tuning, while in principle more powerful, requires metrics and losses that factor over lattices, and in practice works no better than n-best tuning (Cherry and Foster, 2012). Tuning requires a parallel set {(ft, et)}Tt=1 of source sentences ft and target references et.2 Phrasal follows the log-linear approach to phrasebased translation (Och and Ney, 2004) in which the predictive translation distribution p(e|f; w) is modeled directly as p(e |f; w) = Z1f) exp I wTφ(e, f)] (1) where w E Rd is the vector of model parameters, φ(·) E Rd is a feature map, and Z(f) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e&apos;, e) is chosen that specifies the similarity between a hypothesis e&apos; and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NI</context>
<context position="10246" citStr="Och and Ney, 2004" startWordPosition="1605" endWordPosition="1608">hrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (Huang and Chiang, 2007). The standard multi-stack beam search (Och and Ney, 2004) is also an option. Either procedure can be configured in one of several recombination modes. The “Pharaoh” mode only considers linear distortion, source coverage, and target LM history. The “Exact” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics available for tuning can also be invoked for evaluation. For signi</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8306" citStr="Och, 2003" startWordPosition="1297" endWordPosition="1298">on to tune to log-likelihood under (1). Instead, a gold error metric G(e&apos;, e) is chosen that specifies the similarity between a hypothesis e&apos; and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. 2For simplicity, we assume one reference, but the multireference case is analogous. 115 Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are perfo</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7975" citStr="Papineni et al., 2002" startWordPosition="1244" endWordPosition="1247">lation (Och and Ney, 2004) in which the predictive translation distribution p(e|f; w) is modeled directly as p(e |f; w) = Z1f) exp I wTφ(e, f)] (1) where w E Rd is the vector of model parameters, φ(·) E Rd is a feature map, and Z(f) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e&apos;, e) is chosen that specifies the similarity between a hypothesis e&apos; and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batc</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Post</author>
<author>J Ganitkevitch</author>
<author>L Orland</author>
<author>J Weese</author>
<author>Y Cao</author>
<author>C Callison-Burch</author>
</authors>
<date>2013</date>
<note>Joshua 5.0: Sparser, better, faster, server. In WMT.</note>
<contexts>
<context position="1197" citStr="Post et al., 2013" startWordPosition="176" endWordPosition="179">ws favorable results in terms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web</context>
</contexts>
<marker>Post, Ganitkevitch, Orland, Weese, Cao, Callison-Burch, 2013</marker>
<rawString>M. Post, J. Ganitkevitch, L. Orland, J. Weese, Y. Cao, and C. Callison-Burch. 2013. Joshua 5.0: Sparser, better, faster, server. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing in MT.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="10955" citStr="Riezler and Maxwell (2005)" startWordPosition="1712" endWordPosition="1715">ion modes. The “Pharaoh” mode only considers linear distortion, source coverage, and target LM history. The “Exact” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics available for tuning can also be invoked for evaluation. For significance testing, the toolkit includes an implementation of the permutation test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r E R axiom d&apos; : s(d&apos;,w) r E/ cov(d) item |cov(d) |= |s |goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d&apos; is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM estimated from cased, t</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>S. Riezler and J. T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing in MT. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="8034" citStr="Snover et al., 2006" startWordPosition="1255" endWordPosition="1258">n distribution p(e|f; w) is modeled directly as p(e |f; w) = Z1f) exp I wTφ(e, f)] (1) where w E Rd is the vector of model parameters, φ(·) E Rd is a feature map, and Z(f) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e&apos;, e) is chosen that specifies the similarity between a hypothesis e&apos; and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. 2F</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wuebker</author>
<author>M Huck</author>
<author>S Peitz</author>
<author>M Nuhn</author>
<author>M Freitag</author>
<author>J T Peter</author>
<author>S Mansour</author>
<author>H Ney</author>
</authors>
<date>2012</date>
<note>Jane 2:</note>
<contexts>
<context position="1226" citStr="Wuebker et al., 2012" startWordPosition="181" endWordPosition="184">rms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an as</context>
<context position="5138" citStr="Wuebker et al., 2012" startWordPosition="777" endWordPosition="780">loader1 is best for largescale LMs such as the unfiltered models produced by lmplz (Heafield et al., 2013). Profiling shows that LM queries often account for more than 50% of the CPU time in a Phrasal decoding run, so we designed the Phrasal KenLM loader to execute queries mostly in C++ for efficiency. The KenLM binding efficiently passes full strings to C++ via JNI. KenLM then iterates over the string, returning a score and a state length. Phrasal can load multiple language models, and includes native support for the class-based language models that have become popular in recent evaluations (Wuebker et al., 2012; Ammar et al., 2013; Durrani et al., 2013). 2.2 Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memory utilization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature API. This allows for storage of static rule feat</context>
</contexts>
<marker>Wuebker, Huck, Peitz, Nuhn, Freitag, Peter, Mansour, Ney, 2012</marker>
<rawString>J. Wuebker, M. Huck, S. Peitz, M. Nuhn, M. Freitag, J. T. Peter, S. Mansour, and H. Ney. 2012. Jane 2:</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
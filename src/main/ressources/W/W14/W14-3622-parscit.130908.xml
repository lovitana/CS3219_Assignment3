<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007313">
<title confidence="0.65665">
The Columbia System in the QALB-2014 Shared Task
on Arabic Error Correction
Alla Rozovskaya Nizar Habash† Ramy Eskander Noura Farra Wael Salloum
Center for Computational Learning Systems, Columbia University
†New York University Abu Dhabi
</title>
<email confidence="0.993533">
{alla,ramy,noura,wael}@ccls.columbia.edu
†nizar.habash@nyu.edu
</email>
<sectionHeader confidence="0.993863" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927533333333">
The QALB-2014 shared task focuses on
correcting errors in texts written in Mod-
ern Standard Arabic. In this paper, we
describe the Columbia University entry in
the shared task. Our system consists of
several components that rely on machine-
learning techniques and linguistic knowl-
edge. We submitted three versions of the
system: these share several core elements
but each version also includes additional
components. We describe our underlying
approach and the special aspects of the dif-
ferent versions of our submission. Our
system ranked first out of nine participat-
ing teams.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906695652174">
The topic of text correction has seen a lot of in-
terest in the past several years, with a focus on
correcting grammatical errors made by learners of
English as a Second Language (ESL). The two
most recent CoNLL shared tasks were devoted to
grammatical error correction for non-native writ-
ers (Ng et al., 2013; Ng et al., 2014).
The QALB-2014 shared task (Mohit et al.,
2014) is the first competition that addresses the
problem of text correction in Modern Standard
Arabic (MSA) texts. The competition makes
use of the recently developed QALB corpus (Za-
ghouani et al., 2014). The shared task covers all
types of mistakes that occur in the data.
Our system consists of statistical models, lin-
guistic resources, and rule-based modules that ad-
dress different types of errors.
We briefly discuss the task in Section 2. Sec-
tion 3 gives an overview of the Columbia system
and describes the system components. In Sec-
tion 4, we evaluate the complete system on the de-
velopment data and show the results obtained on
test. Section 5 concludes.
</bodyText>
<sectionHeader confidence="0.991974" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999943275862069">
The QALB-2014 shared task addresses the prob-
lem of correcting errors in texts written in Modern
Standard Arabic (MSA). The task organizers re-
leased training, development, and test data. All
of the data comes from online commentaries writ-
ten to Aljazeera articles.1 The training data con-
tains 1.2 million words; the development and the
test data contain about 50,000 words each. The
data was annotated and corrected by native Arabic
speakers. For more detail on the QALB corpus, we
refer the reader to Zaghouani et al. (2014). The re-
sults in the subsequent sections are reported on the
development set.
It should be noted that in the annotation process,
the annotators did not assign error categories but
only specified an appropriate correction. In spite
of this, it is possible, to isolate certain error types
automatically, by using the corrections in coordi-
nation with the input words. The first type con-
cerns punctuation errors. Errors involving punc-
tuation account for about 39% of all errors in the
data. In addition to punctuation mistakes, another
very common source of errors refers to subopti-
mal spelling for two groups of letters – Alif (and
its Hamzated versions) and Ya (and its undotted or
Alif Maqsura versions). For more detail on this
and other Arabic phenomena, we refer the reader
to Habash (2010; Buckwalter (2007; El Kholy and
Habash (2012). Mistakes associated with Alif and
</bodyText>
<footnote confidence="0.994942">
1http://www.aljazeera.net/
</footnote>
<page confidence="0.750484">
160
</page>
<note confidence="0.3307635">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 160–164,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.9996853">
Component CLMB-1 System CLMB-3
CLMB-2
MADAMIRA
MLE
Naive Bayes
GSEC
MLE-unigram
Punctuation
Dialectal
Patterns
</table>
<tableCaption confidence="0.994039">
Table 1: The three versions of the Columbia sys-
tem and their components.
</tableCaption>
<bodyText confidence="0.745266">
Ya spelling constitute almost 30% of all errors.
</bodyText>
<sectionHeader confidence="0.986351" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.999068777777778">
The Columbia University system consists of sev-
eral components designed to address different
types of errors. We submitted three versions of the
system. We refer to these as CLMB-1, CLMB-2,
and CLMB-3. Table 1 lists all of the components
and indicates which components are included in
each version. The components are applied in the
order shown in the table. Below we describe each
component in more detail.
</bodyText>
<subsectionHeader confidence="0.997372">
3.1 MADAMIRA Corrector
</subsectionHeader>
<bodyText confidence="0.999911208333333">
MADAMIRA (Pasha et al., 2014) is a tool
designed for morphological analysis and dis-
ambiguation of Modern Standard Arabic.
MADAMIRA performs morphological analysis
in context. This is a knowledge-rich resource
that requires a morphological analyzer and a
large corpus where every word is marked with
its morphological features. The task organizers
provided the shared task data pre-processed
with MADAMIRA, including all of the features
generated by the tool for every word. In addition
to the morphological analysis and contextual
morphological disambiguation, MADAMIRA
also performs Alif and Ya spelling correction
for the phenomena associated with these letters
discussed in Section 2. The corrected form was
included among the features and can be used
for correcting the input. We use the corrections
proposed by MADAMIRA and apply them to the
data. As we show in Section 4, while the form
proposed by MADAMIRA may not necessarily
be correct, MADAMIRA performs at a very high
precision. MADAMIRA corrector is used in the
CLMB-1 and CLMB-2 systems.
</bodyText>
<subsectionHeader confidence="0.999071">
3.2 Maximum Likelihood Model
</subsectionHeader>
<bodyText confidence="0.999972352941176">
The Maximum Likelihood Estimator (MLE) is a
supervised component that is trained on the train-
ing data of the shared task. Given the annotated
training data, a map is defined that specifies for ev-
ery word n-gram in the source text the most likely
n-gram corresponding to it in the target text. The
MLE model considers source n-grams of lengths
between 1 to 3; the MLE-unigram model that is
part of the CLMB-3 version only considers n-
grams of length 1.
The MLE approach performs well on errors that
have been observed in the training data and can
be unambiguously corrected without using the sur-
rounding context, i.e. do not have many alternative
corrections. Consequently, MLE fails on words
that have many possible corrections, as well as
words not seen in training.
</bodyText>
<subsectionHeader confidence="0.99252">
3.3 Naive Bayes for Unseen Words
</subsectionHeader>
<bodyText confidence="0.9999848">
The Naive Bayes component addresses errors for
words that were not seen in training. The system
uses the approach proposed in Rozovskaya and
Roth (2011) that proved to be successful for cor-
recting errors made by English as a Second Lan-
guage learners. The model operates at the word
level and targets word replacement errors that in-
volve single tokens. Candidate corrections are
generated using a character confusion table that is
based on the training data. The model is a Naive
Bayes classifier trained on the Arabic Gigaword
corpus (Parker et al., 2011) with word n-gram fea-
tures in the 4-word window around the word to be
corrected. The Naive Bayes component is used in
the CLMB-1 system.
</bodyText>
<subsectionHeader confidence="0.935207">
3.4 The GSEC Model
</subsectionHeader>
<bodyText confidence="0.999979692307692">
The CLMB-3 system implements a Generalized
Character-Level Error Correction model (GSEC)
proposed in Farra et al. (2014). GSEC is a super-
vised model that operates at the character level.
Because of this, the source and the target side of
the training data need to be aligned at the charac-
ter level. We use the alignment tool Sclite (Fiscus,
1998). The alignment maps each source charac-
ter to itself, a different character, a pair of char-
acters, or an empty string. For the shared task,
punctuation corrections are ignored since punctu-
ation errors are handled by the punctuation correc-
tor described in the following section. It should
</bodyText>
<page confidence="0.992686">
161
</page>
<bodyText confidence="0.999919333333333">
also be noted that the model was not trained to
insert missing characters. The model is a multi-
class SVM classifier (Kudo, 2005) that makes use
of character-level features using a window of four
characters that may occur within the word bound-
aries as well as in the surrounding context. Due
to a long training time, GSEC was trained on a
quarter of the training data. The system is post-
processed with a unigram word-level maximum-
likelihood model described in Section 3.2. For
more detail on the GSEC approach, we refer the
reader to Farra et al. (2014).
</bodyText>
<subsectionHeader confidence="0.979841">
3.5 Punctuation Corrector
</subsectionHeader>
<bodyText confidence="0.973416266666667">
The shared task data contains a large number of
punctuation mistakes. Punctuation errors, such as
missing periods and commas, account for about
30% of all errors in the data. Most of these errors
involve incorrectly omitting a punctuation symbol.
Our punctuation corrector is a statistical model
that inserts periods and commas. The system is
a decision tree model trained on the shared task
training data using WEKA (Hall et al., 2009). For
punctuation insertion, every space that is not fol-
lowed or preceded by a punctuation mark is con-
sidered.
To generate features, we use a window of size
three around the target space. The features are de-
fined as follows:
</bodyText>
<listItem confidence="0.9003092">
• The part-of-speech of the previous word
• The existence of a conjunctive or connective
proclitic in the following word; that is a “w”
or “f” proclitic that is either a conjunction, a
sub-conjunction or a connective particle
</listItem>
<bodyText confidence="0.999859142857143">
The part-of-speech and proclitic information is
obtained by running MADAMIRA on the text.
We also ran experiments where the model is
trained with a complete list of features produced
by MADAMIRA; that is part-of-speech, gender,
number, person, aspect, voice, case, mood, state,
proclitics and enclitics. This was done for two pre-
ceding words and two following words. However,
this model did not perform as well as the one de-
scribed above, which we used in the final system.
Note that the punctuation model predicts pres-
ence or absence of a punctuation mark in a spe-
cific location and is applied to the source data
from which all punctuation marks have been re-
moved. However, when we apply our punctuation
model in the correction pipeline, we find that it
is always better to keep the already existing peri-
ods and commas in the input text instead of over-
writing them with the model prediction. In other
words, we only attempt to add missing punctua-
tion.
</bodyText>
<subsectionHeader confidence="0.984559">
3.6 Dialectal Usage Corrector
</subsectionHeader>
<bodyText confidence="0.999917366666667">
Even though the shared task data is written in
MSA, MSA is not a native language for Arabic
speakers. Typically, an Arabic speaker has a native
proficiency in one of the many Arabic dialects and
learns to write and read MSA in a formal setting.
For this reason, even in MSA texts produced by
native Arabic speakers, one typically finds words
and linguistic features specific to the writer’s na-
tive dialect that are not found in the standard lan-
guage.
To address such errors, we use Elissa (Salloum
and Habash, 2012), which is Dialectal to Standard
Arabic Machine Translation System. Elissa uses
a rule-based approach that relies on the existence
of a dialectal morphological analyzer (Salloum
and Habash, 2011), a list of hand-written trans-
ferrules, and dialectal-to-standard Arabic lexi-
cons. Elissa uses different dialect identification
techniques to select dialectal words and phrases
(dialectal multi-word expressions) that need to be
handled. Then equivalent MSA paraphrases of the
selected words/phrases are generated and an MSA
lattice for each input sentence is constructed. The
paraphrases within the lattice are then ranked us-
ing language models and the n-best sentences are
extracted from lattice. We use 5-gram language
models trained using SRILM (Stolcke, 2002) on
about 200 million untokenized, Alif/Ya normal-
ized words extracted from Arabic GigaWord. This
component is employed in the CLMB-2 system.
</bodyText>
<subsectionHeader confidence="0.967782">
3.7 Pattern-Based Corrector
</subsectionHeader>
<bodyText confidence="0.9998676">
We created a set of rules that account for very
common phenomena involving incorrectly split or
merged tokens. The MADAMIRA corrector de-
scribed above does not handle splits and merges;
however, some of the cases are handled in the
MLE method. Note that the MLE method is re-
strictive since it does not correct words not seen
in training, while the pattern-based corrector is
more general. The rules were created through
analysis of samples of the QALB Shared Task
</bodyText>
<page confidence="0.995193">
162
</page>
<bodyText confidence="0.9994058">
training data. Some of the rules use regular ex-
pressions, while others make use of the rule-
based Standard Arabic Morphological Analyzer
(SAMA) (Maamouri et al., 2010), the same out-
of-context analyzer used inside of MADAMIRA.
</bodyText>
<subsectionHeader confidence="0.813635">
Rules for splitting words
</subsectionHeader>
<listItem confidence="0.880764307692308">
• All digits are separated from words.
• A space is added after all word medial Ta-
Marbuta characters.
• A space is added after the very common
“ElY” ‘at/about/on’ preposition if it is at-
tached to the following word.
• If a word has a morphological analysis that
includes “lmA” (as negation particle, relative
pronoun or pseudo verb), “hA” (a demonstra-
tive pronoun), or “Ebd” and “&gt;bw” in proper
nouns, a space is inserted after those parts of
the analysis.
• If a word has no morphological analysis, but
</listItem>
<bodyText confidence="0.87372925">
starts with a set of commonly mis-attached
words, and the rest of the word has an anal-
ysis, the word is split after the mis-attached
word sequence.
</bodyText>
<subsectionHeader confidence="0.700339">
Rules for merging words
</subsectionHeader>
<listItem confidence="0.9996736">
• All lone occurrences of the conjunction w
‘and’ are attached to the following word.
• All sequences of the punctuation marks (., ?,
!) that occur between two and six times are
merged: e.g ! ! ! → !!!.
</listItem>
<sectionHeader confidence="0.986764" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999179333333334">
In Section 3, we described the individual sys-
tem components that address different types of
errors. In this section, we show how the sys-
tem improves when each component is added into
the system. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012), the official
scorer of the shared task.
Table 2 reports performance results of each ver-
sion of the Columbia system on the development
data. Table 3 shows the performance results for the
best-performing system, CLMB-1, as each system
component is added.
</bodyText>
<table confidence="0.9995605">
System P R F1
CLMB-1 72.22 62.79 67.18
CLMB-2 69.49 61.72 65.38
CLMB-3 69.71 59.42 64.15
</table>
<tableCaption confidence="0.942342">
Table 2: Performance of the Columbia systems
on the development data.
</tableCaption>
<table confidence="0.999972666666667">
System P R F1
MADAMIRA 83.33 32.94 47.21
+ MLE 86.52 42.52 57.02
+ NB 85.80 43.27 57.53
+ Punc. 73.66 59.51 65.83
+ Patterns 72.22 62.79 67.18
</table>
<tableCaption confidence="0.949618333333333">
Table 3: Performance of the CLMB-1 system on
the development data and the contribution of
its components.
</tableCaption>
<table confidence="0.99983525">
System P R F1
CLMB-1 73.34 63.23 67.91
CLMB-2 70.86 62.21 66.25
CLMB-3 71.45 60.00 65.22
</table>
<tableCaption confidence="0.991471">
Table 4: Performance of the Columbia systems
on the test data.
</tableCaption>
<bodyText confidence="0.99992275">
Finally, Table 4 reports results obtained on the
test data. These results are comparable to the per-
formance observed on the development data. In
particular, CLMB-1 achieves the highest score.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999989875">
We have described the Columbia University sys-
tem that participated in the first shared task
on grammatical error correction for Arabic and
ranked first out of nine participating teams. We
have presented three versions of the system; all of
these incorporate several components that target
different types of mistakes, which we presented
and evaluated in this paper.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995499125">
This material is based on research funded by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors. Nizar Habash performed
most of his contribution to this paper while he was
at the Center for Computational Learning Systems
at Columbia University.
</bodyText>
<page confidence="0.998691">
163
</page>
<sectionHeader confidence="0.990184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999455736111111">
T. Buckwalter. 2007. Issues in Arabic Morphological
Analysis. In A. van den Bosch and A. Soudi, editors,
Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
D. Dahlmeier and H. T. Ng. 2012. Better evaluation
for grammatical error correction. In Proceedings of
NAACL.
A. El Kholy and N. Habash. 2012. Orthographic and
morphological processing for English–Arabic sta-
tistical machine translation. Machine Translation,
26(1-2).
N. Farra, N. Tomeh, A. Rozovskaya, and N. Habash.
2014. Generalized character-level spelling error cor-
rection. In Proceedings of ACL.
J. Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
N. Y. Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies 3.1.
M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA data
mining software: an update. SIGKDD Explorations,
11(1):10–18.
T. Kudo. 2005. YamCha: Yet another multipurpose
chunk annotator. http://chasen.org/ taku/software/.
M. Maamouri, D. Graff, B. Bouziri, S. Krouna, A. Bies,
and S. Kulick. 2010. LDC Standard Arabic Mor-
phological Analyzer (SAMA) Version 3.1. Linguistic
Data Consortium.
B. Mohit, A. Rozovskaya, N. Habash, W. Zaghouani,
and O. Obeid. 2014. The first QALB shared task on
automatic text correction for Arabic. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proceedings of
CoNLL: Shared Task.
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of CoNLL: Shared Task.
R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.
2011. Arabic Gigaword Fifth Edition. Linguistic
Data Consortium.
A. Pasha, M. Al-Badrashiny, A. E. Kholy, R. Eskan-
der, M. Diab, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. 2014. MADAMIRA: A fast, compre-
hensive tool for morphological analysis and disam-
biguation of arabic. In Proceedings of LREC.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proceedings ofACL.
W. Salloum and N. Habash. 2011. Dialectal to stan-
dard arabic paraphrasing to improve arabic-english
statistical machine translation. In Proceedings of
the First Workshop on Algorithms and Resources for
Modelling of Dialects and Language Varieties.
W. Salloum and N. Habash. 2012. Elissa: A dialectal
to standard arabic machine translation system. In
Proceedings of COLING (Demos).
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing.
W. Zaghouani, B. Mohit, N. Habash, O. Obeid,
N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,
and K. Oflazer. 2014. Large scale arabic error anno-
tation: Guidelines and framework. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC’14).
</reference>
<page confidence="0.998496">
164
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619140">
<title confidence="0.9855745">The Columbia System in the QALB-2014 Shared on Arabic Error Correction</title>
<author confidence="0.987323">Rozovskaya Nizar Eskander Noura Farra Wael Salloum</author>
<affiliation confidence="0.830227">Center for Computational Learning Systems, Columbia York University Abu Dhabi</affiliation>
<abstract confidence="0.998349875">The QALB-2014 shared task focuses on correcting errors in texts written in Modern Standard Arabic. In this paper, we describe the Columbia University entry in the shared task. Our system consists of several components that rely on machinelearning techniques and linguistic knowledge. We submitted three versions of the system: these share several core elements but each version also includes additional components. We describe our underlying approach and the special aspects of the different versions of our submission. Our system ranked first out of nine participating teams.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Issues in Arabic Morphological Analysis.</title>
<date>2007</date>
<booktitle>Arabic Computational Morphology: Knowledgebased and Empirical Methods.</booktitle>
<editor>In A. van den Bosch and A. Soudi, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3287" citStr="Buckwalter (2007" startWordPosition="530" endWordPosition="531">d an appropriate correction. In spite of this, it is possible, to isolate certain error types automatically, by using the corrections in coordination with the input words. The first type concerns punctuation errors. Errors involving punctuation account for about 39% of all errors in the data. In addition to punctuation mistakes, another very common source of errors refers to suboptimal spelling for two groups of letters – Alif (and its Hamzated versions) and Ya (and its undotted or Alif Maqsura versions). For more detail on this and other Arabic phenomena, we refer the reader to Habash (2010; Buckwalter (2007; El Kholy and Habash (2012). Mistakes associated with Alif and 1http://www.aljazeera.net/ 160 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 160–164, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Component CLMB-1 System CLMB-3 CLMB-2 MADAMIRA MLE Naive Bayes GSEC MLE-unigram Punctuation Dialectal Patterns Table 1: The three versions of the Columbia system and their components. Ya spelling constitute almost 30% of all errors. 3 System Overview The Columbia University system consists of several components designed to </context>
</contexts>
<marker>Buckwalter, 2007</marker>
<rawString>T. Buckwalter. 2007. Issues in Arabic Morphological Analysis. In A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledgebased and Empirical Methods. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="13246" citStr="Dahlmeier and Ng, 2012" startWordPosition="2178" endWordPosition="2181">ached words, and the rest of the word has an analysis, the word is split after the mis-attached word sequence. Rules for merging words • All lone occurrences of the conjunction w ‘and’ are attached to the following word. • All sequences of the punctuation marks (., ?, !) that occur between two and six times are merged: e.g ! ! ! → !!!. 4 Experimental Results In Section 3, we described the individual system components that address different types of errors. In this section, we show how the system improves when each component is added into the system. System output is scored with the M2 scorer (Dahlmeier and Ng, 2012), the official scorer of the shared task. Table 2 reports performance results of each version of the Columbia system on the development data. Table 3 shows the performance results for the best-performing system, CLMB-1, as each system component is added. System P R F1 CLMB-1 72.22 62.79 67.18 CLMB-2 69.49 61.72 65.38 CLMB-3 69.71 59.42 64.15 Table 2: Performance of the Columbia systems on the development data. System P R F1 MADAMIRA 83.33 32.94 47.21 + MLE 86.52 42.52 57.02 + NB 85.80 43.27 57.53 + Punc. 73.66 59.51 65.83 + Patterns 72.22 62.79 67.18 Table 3: Performance of the CLMB-1 system o</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>D. Dahlmeier and H. T. Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A El Kholy</author>
<author>N Habash</author>
</authors>
<title>Orthographic and morphological processing for English–Arabic statistical machine translation.</title>
<date>2012</date>
<journal>Machine Translation,</journal>
<pages>26--1</pages>
<marker>El Kholy, Habash, 2012</marker>
<rawString>A. El Kholy and N. Habash. 2012. Orthographic and morphological processing for English–Arabic statistical machine translation. Machine Translation, 26(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Farra</author>
<author>N Tomeh</author>
<author>A Rozovskaya</author>
<author>N Habash</author>
</authors>
<title>Generalized character-level spelling error correction.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6954" citStr="Farra et al. (2014)" startWordPosition="1116" endWordPosition="1119">English as a Second Language learners. The model operates at the word level and targets word replacement errors that involve single tokens. Candidate corrections are generated using a character confusion table that is based on the training data. The model is a Naive Bayes classifier trained on the Arabic Gigaword corpus (Parker et al., 2011) with word n-gram features in the 4-word window around the word to be corrected. The Naive Bayes component is used in the CLMB-1 system. 3.4 The GSEC Model The CLMB-3 system implements a Generalized Character-Level Error Correction model (GSEC) proposed in Farra et al. (2014). GSEC is a supervised model that operates at the character level. Because of this, the source and the target side of the training data need to be aligned at the character level. We use the alignment tool Sclite (Fiscus, 1998). The alignment maps each source character to itself, a different character, a pair of characters, or an empty string. For the shared task, punctuation corrections are ignored since punctuation errors are handled by the punctuation corrector described in the following section. It should 161 also be noted that the model was not trained to insert missing characters. The mod</context>
</contexts>
<marker>Farra, Tomeh, Rozovskaya, Habash, 2014</marker>
<rawString>N. Farra, N. Tomeh, A. Rozovskaya, and N. Habash. 2014. Generalized character-level spelling error correction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fiscus</author>
</authors>
<title>Sclite scoring package version 1.5.</title>
<date>1998</date>
<institution>US National Institute of Standard</institution>
<note>Technology (NIST), URL http://www. itl. nist. gov/iaui/894.01/tools.</note>
<contexts>
<context position="7180" citStr="Fiscus, 1998" startWordPosition="1160" endWordPosition="1161">raining data. The model is a Naive Bayes classifier trained on the Arabic Gigaword corpus (Parker et al., 2011) with word n-gram features in the 4-word window around the word to be corrected. The Naive Bayes component is used in the CLMB-1 system. 3.4 The GSEC Model The CLMB-3 system implements a Generalized Character-Level Error Correction model (GSEC) proposed in Farra et al. (2014). GSEC is a supervised model that operates at the character level. Because of this, the source and the target side of the training data need to be aligned at the character level. We use the alignment tool Sclite (Fiscus, 1998). The alignment maps each source character to itself, a different character, a pair of characters, or an empty string. For the shared task, punctuation corrections are ignored since punctuation errors are handled by the punctuation corrector described in the following section. It should 161 also be noted that the model was not trained to insert missing characters. The model is a multiclass SVM classifier (Kudo, 2005) that makes use of character-level features using a window of four characters that may occur within the word boundaries as well as in the surrounding context. Due to a long trainin</context>
</contexts>
<marker>Fiscus, 1998</marker>
<rawString>J. Fiscus. 1998. Sclite scoring package version 1.5. US National Institute of Standard Technology (NIST), URL http://www. itl. nist. gov/iaui/894.01/tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Y Habash</author>
</authors>
<title>Introduction to Arabic natural language processing.</title>
<date>2010</date>
<journal>Synthesis Lectures on Human Language Technologies</journal>
<volume>3</volume>
<contexts>
<context position="3269" citStr="Habash (2010" startWordPosition="528" endWordPosition="529"> only specified an appropriate correction. In spite of this, it is possible, to isolate certain error types automatically, by using the corrections in coordination with the input words. The first type concerns punctuation errors. Errors involving punctuation account for about 39% of all errors in the data. In addition to punctuation mistakes, another very common source of errors refers to suboptimal spelling for two groups of letters – Alif (and its Hamzated versions) and Ya (and its undotted or Alif Maqsura versions). For more detail on this and other Arabic phenomena, we refer the reader to Habash (2010; Buckwalter (2007; El Kholy and Habash (2012). Mistakes associated with Alif and 1http://www.aljazeera.net/ 160 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 160–164, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Component CLMB-1 System CLMB-3 CLMB-2 MADAMIRA MLE Naive Bayes GSEC MLE-unigram Punctuation Dialectal Patterns Table 1: The three versions of the Columbia system and their components. Ya spelling constitute almost 30% of all errors. 3 System Overview The Columbia University system consists of several compo</context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>N. Y. Habash. 2010. Introduction to Arabic natural language processing. Synthesis Lectures on Human Language Technologies 3.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>F Eibe</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="8488" citStr="Hall et al., 2009" startWordPosition="1378" endWordPosition="1381">th a unigram word-level maximumlikelihood model described in Section 3.2. For more detail on the GSEC approach, we refer the reader to Farra et al. (2014). 3.5 Punctuation Corrector The shared task data contains a large number of punctuation mistakes. Punctuation errors, such as missing periods and commas, account for about 30% of all errors in the data. Most of these errors involve incorrectly omitting a punctuation symbol. Our punctuation corrector is a statistical model that inserts periods and commas. The system is a decision tree model trained on the shared task training data using WEKA (Hall et al., 2009). For punctuation insertion, every space that is not followed or preceded by a punctuation mark is considered. To generate features, we use a window of size three around the target space. The features are defined as follows: • The part-of-speech of the previous word • The existence of a conjunctive or connective proclitic in the following word; that is a “w” or “f” proclitic that is either a conjunction, a sub-conjunction or a connective particle The part-of-speech and proclitic information is obtained by running MADAMIRA on the text. We also ran experiments where the model is trained with a c</context>
</contexts>
<marker>Hall, Eibe, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
</authors>
<title>YamCha: Yet another multipurpose chunk annotator.</title>
<date>2005</date>
<note>http://chasen.org/ taku/software/.</note>
<contexts>
<context position="7600" citStr="Kudo, 2005" startWordPosition="1231" endWordPosition="1232">perates at the character level. Because of this, the source and the target side of the training data need to be aligned at the character level. We use the alignment tool Sclite (Fiscus, 1998). The alignment maps each source character to itself, a different character, a pair of characters, or an empty string. For the shared task, punctuation corrections are ignored since punctuation errors are handled by the punctuation corrector described in the following section. It should 161 also be noted that the model was not trained to insert missing characters. The model is a multiclass SVM classifier (Kudo, 2005) that makes use of character-level features using a window of four characters that may occur within the word boundaries as well as in the surrounding context. Due to a long training time, GSEC was trained on a quarter of the training data. The system is postprocessed with a unigram word-level maximumlikelihood model described in Section 3.2. For more detail on the GSEC approach, we refer the reader to Farra et al. (2014). 3.5 Punctuation Corrector The shared task data contains a large number of punctuation mistakes. Punctuation errors, such as missing periods and commas, account for about 30% </context>
</contexts>
<marker>Kudo, 2005</marker>
<rawString>T. Kudo. 2005. YamCha: Yet another multipurpose chunk annotator. http://chasen.org/ taku/software/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>D Graff</author>
<author>B Bouziri</author>
<author>S Krouna</author>
<author>A Bies</author>
<author>S Kulick</author>
</authors>
<title>LDC Standard Arabic Morphological Analyzer (SAMA) Version 3.1. Linguistic Data Consortium.</title>
<date>2010</date>
<contexts>
<context position="11997" citStr="Maamouri et al., 2010" startWordPosition="1955" endWordPosition="1958">a set of rules that account for very common phenomena involving incorrectly split or merged tokens. The MADAMIRA corrector described above does not handle splits and merges; however, some of the cases are handled in the MLE method. Note that the MLE method is restrictive since it does not correct words not seen in training, while the pattern-based corrector is more general. The rules were created through analysis of samples of the QALB Shared Task 162 training data. Some of the rules use regular expressions, while others make use of the rulebased Standard Arabic Morphological Analyzer (SAMA) (Maamouri et al., 2010), the same outof-context analyzer used inside of MADAMIRA. Rules for splitting words • All digits are separated from words. • A space is added after all word medial TaMarbuta characters. • A space is added after the very common “ElY” ‘at/about/on’ preposition if it is attached to the following word. • If a word has a morphological analysis that includes “lmA” (as negation particle, relative pronoun or pseudo verb), “hA” (a demonstrative pronoun), or “Ebd” and “&gt;bw” in proper nouns, a space is inserted after those parts of the analysis. • If a word has no morphological analysis, but starts with</context>
</contexts>
<marker>Maamouri, Graff, Bouziri, Krouna, Bies, Kulick, 2010</marker>
<rawString>M. Maamouri, D. Graff, B. Bouziri, S. Krouna, A. Bies, and S. Kulick. 2010. LDC Standard Arabic Morphological Analyzer (SAMA) Version 3.1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Mohit</author>
<author>A Rozovskaya</author>
<author>N Habash</author>
<author>W Zaghouani</author>
<author>O Obeid</author>
</authors>
<title>The first QALB shared task on automatic text correction for Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing.</booktitle>
<contexts>
<context position="1277" citStr="Mohit et al., 2014" startWordPosition="193" endWordPosition="196">core elements but each version also includes additional components. We describe our underlying approach and the special aspects of the different versions of our submission. Our system ranked first out of nine participating teams. 1 Introduction The topic of text correction has seen a lot of interest in the past several years, with a focus on correcting grammatical errors made by learners of English as a Second Language (ESL). The two most recent CoNLL shared tasks were devoted to grammatical error correction for non-native writers (Ng et al., 2013; Ng et al., 2014). The QALB-2014 shared task (Mohit et al., 2014) is the first competition that addresses the problem of text correction in Modern Standard Arabic (MSA) texts. The competition makes use of the recently developed QALB corpus (Zaghouani et al., 2014). The shared task covers all types of mistakes that occur in the data. Our system consists of statistical models, linguistic resources, and rule-based modules that address different types of errors. We briefly discuss the task in Section 2. Section 3 gives an overview of the Columbia system and describes the system components. In Section 4, we evaluate the complete system on the development data an</context>
</contexts>
<marker>Mohit, Rozovskaya, Habash, Zaghouani, Obeid, 2014</marker>
<rawString>B. Mohit, A. Rozovskaya, N. Habash, W. Zaghouani, and O. Obeid. 2014. The first QALB shared task on automatic text correction for Arabic. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadiwinoto</author>
<author>J Tetreault</author>
</authors>
<title>The CoNLL-2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL: Shared Task.</booktitle>
<marker>Hadiwinoto, Tetreault, 2013</marker>
<rawString>H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and J. Tetreault. 2013. The CoNLL-2013 shared task on grammatical error correction. In Proceedings of CoNLL: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>S M Wu</author>
<author>T Briscoe</author>
<author>C Hadiwinoto</author>
<author>R H Susanto</author>
<author>C Bryant</author>
</authors>
<title>The CoNLL-2014 shared task on grammatical error correction.</title>
<date>2014</date>
<booktitle>In Proceedings of CoNLL: Shared Task.</booktitle>
<contexts>
<context position="1229" citStr="Ng et al., 2014" startWordPosition="185" endWordPosition="188"> versions of the system: these share several core elements but each version also includes additional components. We describe our underlying approach and the special aspects of the different versions of our submission. Our system ranked first out of nine participating teams. 1 Introduction The topic of text correction has seen a lot of interest in the past several years, with a focus on correcting grammatical errors made by learners of English as a Second Language (ESL). The two most recent CoNLL shared tasks were devoted to grammatical error correction for non-native writers (Ng et al., 2013; Ng et al., 2014). The QALB-2014 shared task (Mohit et al., 2014) is the first competition that addresses the problem of text correction in Modern Standard Arabic (MSA) texts. The competition makes use of the recently developed QALB corpus (Zaghouani et al., 2014). The shared task covers all types of mistakes that occur in the data. Our system consists of statistical models, linguistic resources, and rule-based modules that address different types of errors. We briefly discuss the task in Section 2. Section 3 gives an overview of the Columbia system and describes the system components. In Section 4, we evaluat</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H. Susanto, and C. Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Proceedings of CoNLL: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Parker</author>
<author>D Graff</author>
<author>K Chen</author>
<author>J Kong</author>
<author>K Maeda</author>
</authors>
<title>Arabic Gigaword Fifth Edition. Linguistic Data Consortium.</title>
<date>2011</date>
<contexts>
<context position="6678" citStr="Parker et al., 2011" startWordPosition="1070" endWordPosition="1073"> as words not seen in training. 3.3 Naive Bayes for Unseen Words The Naive Bayes component addresses errors for words that were not seen in training. The system uses the approach proposed in Rozovskaya and Roth (2011) that proved to be successful for correcting errors made by English as a Second Language learners. The model operates at the word level and targets word replacement errors that involve single tokens. Candidate corrections are generated using a character confusion table that is based on the training data. The model is a Naive Bayes classifier trained on the Arabic Gigaword corpus (Parker et al., 2011) with word n-gram features in the 4-word window around the word to be corrected. The Naive Bayes component is used in the CLMB-1 system. 3.4 The GSEC Model The CLMB-3 system implements a Generalized Character-Level Error Correction model (GSEC) proposed in Farra et al. (2014). GSEC is a supervised model that operates at the character level. Because of this, the source and the target side of the training data need to be aligned at the character level. We use the alignment tool Sclite (Fiscus, 1998). The alignment maps each source character to itself, a different character, a pair of characters,</context>
</contexts>
<marker>Parker, Graff, Chen, Kong, Maeda, 2011</marker>
<rawString>R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda. 2011. Arabic Gigaword Fifth Edition. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pasha</author>
<author>M Al-Badrashiny</author>
<author>A E Kholy</author>
<author>R Eskander</author>
<author>M Diab</author>
<author>N Habash</author>
<author>M Pooleery</author>
<author>O Rambow</author>
<author>R Roth</author>
</authors>
<title>MADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation of arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="4272" citStr="Pasha et al., 2014" startWordPosition="677" endWordPosition="680"> Dialectal Patterns Table 1: The three versions of the Columbia system and their components. Ya spelling constitute almost 30% of all errors. 3 System Overview The Columbia University system consists of several components designed to address different types of errors. We submitted three versions of the system. We refer to these as CLMB-1, CLMB-2, and CLMB-3. Table 1 lists all of the components and indicates which components are included in each version. The components are applied in the order shown in the table. Below we describe each component in more detail. 3.1 MADAMIRA Corrector MADAMIRA (Pasha et al., 2014) is a tool designed for morphological analysis and disambiguation of Modern Standard Arabic. MADAMIRA performs morphological analysis in context. This is a knowledge-rich resource that requires a morphological analyzer and a large corpus where every word is marked with its morphological features. The task organizers provided the shared task data pre-processed with MADAMIRA, including all of the features generated by the tool for every word. In addition to the morphological analysis and contextual morphological disambiguation, MADAMIRA also performs Alif and Ya spelling correction for the pheno</context>
</contexts>
<marker>Pasha, Al-Badrashiny, Kholy, Eskander, Diab, Habash, Pooleery, Rambow, Roth, 2014</marker>
<rawString>A. Pasha, M. Al-Badrashiny, A. E. Kholy, R. Eskander, M. Diab, N. Habash, M. Pooleery, O. Rambow, and R. Roth. 2014. MADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation of arabic. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Algorithm selection and model adaptation for ESL correction tasks.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6275" citStr="Rozovskaya and Roth (2011)" startWordPosition="1002" endWordPosition="1005">grams of lengths between 1 to 3; the MLE-unigram model that is part of the CLMB-3 version only considers ngrams of length 1. The MLE approach performs well on errors that have been observed in the training data and can be unambiguously corrected without using the surrounding context, i.e. do not have many alternative corrections. Consequently, MLE fails on words that have many possible corrections, as well as words not seen in training. 3.3 Naive Bayes for Unseen Words The Naive Bayes component addresses errors for words that were not seen in training. The system uses the approach proposed in Rozovskaya and Roth (2011) that proved to be successful for correcting errors made by English as a Second Language learners. The model operates at the word level and targets word replacement errors that involve single tokens. Candidate corrections are generated using a character confusion table that is based on the training data. The model is a Naive Bayes classifier trained on the Arabic Gigaword corpus (Parker et al., 2011) with word n-gram features in the 4-word window around the word to be corrected. The Naive Bayes component is used in the CLMB-1 system. 3.4 The GSEC Model The CLMB-3 system implements a Generalize</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>A. Rozovskaya and D. Roth. 2011. Algorithm selection and model adaptation for ESL correction tasks. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Salloum</author>
<author>N Habash</author>
</authors>
<title>Dialectal to standard arabic paraphrasing to improve arabic-english statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties.</booktitle>
<contexts>
<context position="10632" citStr="Salloum and Habash, 2011" startWordPosition="1743" endWordPosition="1746">Arabic speakers. Typically, an Arabic speaker has a native proficiency in one of the many Arabic dialects and learns to write and read MSA in a formal setting. For this reason, even in MSA texts produced by native Arabic speakers, one typically finds words and linguistic features specific to the writer’s native dialect that are not found in the standard language. To address such errors, we use Elissa (Salloum and Habash, 2012), which is Dialectal to Standard Arabic Machine Translation System. Elissa uses a rule-based approach that relies on the existence of a dialectal morphological analyzer (Salloum and Habash, 2011), a list of hand-written transferrules, and dialectal-to-standard Arabic lexicons. Elissa uses different dialect identification techniques to select dialectal words and phrases (dialectal multi-word expressions) that need to be handled. Then equivalent MSA paraphrases of the selected words/phrases are generated and an MSA lattice for each input sentence is constructed. The paraphrases within the lattice are then ranked using language models and the n-best sentences are extracted from lattice. We use 5-gram language models trained using SRILM (Stolcke, 2002) on about 200 million untokenized, Al</context>
</contexts>
<marker>Salloum, Habash, 2011</marker>
<rawString>W. Salloum and N. Habash. 2011. Dialectal to standard arabic paraphrasing to improve arabic-english statistical machine translation. In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Salloum</author>
<author>N Habash</author>
</authors>
<title>Elissa: A dialectal to standard arabic machine translation system.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING (Demos).</booktitle>
<contexts>
<context position="10437" citStr="Salloum and Habash, 2012" startWordPosition="1715" endWordPosition="1718">e model prediction. In other words, we only attempt to add missing punctuation. 3.6 Dialectal Usage Corrector Even though the shared task data is written in MSA, MSA is not a native language for Arabic speakers. Typically, an Arabic speaker has a native proficiency in one of the many Arabic dialects and learns to write and read MSA in a formal setting. For this reason, even in MSA texts produced by native Arabic speakers, one typically finds words and linguistic features specific to the writer’s native dialect that are not found in the standard language. To address such errors, we use Elissa (Salloum and Habash, 2012), which is Dialectal to Standard Arabic Machine Translation System. Elissa uses a rule-based approach that relies on the existence of a dialectal morphological analyzer (Salloum and Habash, 2011), a list of hand-written transferrules, and dialectal-to-standard Arabic lexicons. Elissa uses different dialect identification techniques to select dialectal words and phrases (dialectal multi-word expressions) that need to be handled. Then equivalent MSA paraphrases of the selected words/phrases are generated and an MSA lattice for each input sentence is constructed. The paraphrases within the lattic</context>
</contexts>
<marker>Salloum, Habash, 2012</marker>
<rawString>W. Salloum and N. Habash. 2012. Elissa: A dialectal to standard arabic machine translation system. In Proceedings of COLING (Demos).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="11195" citStr="Stolcke, 2002" startWordPosition="1826" endWordPosition="1827">l morphological analyzer (Salloum and Habash, 2011), a list of hand-written transferrules, and dialectal-to-standard Arabic lexicons. Elissa uses different dialect identification techniques to select dialectal words and phrases (dialectal multi-word expressions) that need to be handled. Then equivalent MSA paraphrases of the selected words/phrases are generated and an MSA lattice for each input sentence is constructed. The paraphrases within the lattice are then ranked using language models and the n-best sentences are extracted from lattice. We use 5-gram language models trained using SRILM (Stolcke, 2002) on about 200 million untokenized, Alif/Ya normalized words extracted from Arabic GigaWord. This component is employed in the CLMB-2 system. 3.7 Pattern-Based Corrector We created a set of rules that account for very common phenomena involving incorrectly split or merged tokens. The MADAMIRA corrector described above does not handle splits and merges; however, some of the cases are handled in the MLE method. Note that the MLE method is restrictive since it does not correct words not seen in training, while the pattern-based corrector is more general. The rules were created through analysis of </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zaghouani</author>
<author>B Mohit</author>
<author>N Habash</author>
<author>O Obeid</author>
<author>N Tomeh</author>
<author>A Rozovskaya</author>
<author>N Farra</author>
<author>S Alkuhlani</author>
<author>K Oflazer</author>
</authors>
<title>Large scale arabic error annotation: Guidelines and framework.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14).</booktitle>
<contexts>
<context position="1476" citStr="Zaghouani et al., 2014" startWordPosition="224" endWordPosition="228">t out of nine participating teams. 1 Introduction The topic of text correction has seen a lot of interest in the past several years, with a focus on correcting grammatical errors made by learners of English as a Second Language (ESL). The two most recent CoNLL shared tasks were devoted to grammatical error correction for non-native writers (Ng et al., 2013; Ng et al., 2014). The QALB-2014 shared task (Mohit et al., 2014) is the first competition that addresses the problem of text correction in Modern Standard Arabic (MSA) texts. The competition makes use of the recently developed QALB corpus (Zaghouani et al., 2014). The shared task covers all types of mistakes that occur in the data. Our system consists of statistical models, linguistic resources, and rule-based modules that address different types of errors. We briefly discuss the task in Section 2. Section 3 gives an overview of the Columbia system and describes the system components. In Section 4, we evaluate the complete system on the development data and show the results obtained on test. Section 5 concludes. 2 Task Description The QALB-2014 shared task addresses the problem of correcting errors in texts written in Modern Standard Arabic (MSA). The</context>
</contexts>
<marker>Zaghouani, Mohit, Habash, Obeid, Tomeh, Rozovskaya, Farra, Alkuhlani, Oflazer, 2014</marker>
<rawString>W. Zaghouani, B. Mohit, N. Habash, O. Obeid, N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani, and K. Oflazer. 2014. Large scale arabic error annotation: Guidelines and framework. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
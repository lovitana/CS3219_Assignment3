<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.999466">
Generating Subjective Responses to Opinionated Articles in Social Media:
An Agenda-Driven Architecture and a Turing-Like Test
</title>
<author confidence="0.996135">
Tomer Cagan
</author>
<affiliation confidence="0.8130235">
School of Computer Science
The Interdisciplinary Center
</affiliation>
<address confidence="0.578153">
Herzeliya, Israel
</address>
<email confidence="0.89732">
cagan.tomer@idc.ac.il
</email>
<author confidence="0.980341">
Stefan L. Frank
</author>
<affiliation confidence="0.959123">
Centre for Language Studies
Radboud University
Nijmegen, The Netherlands
</affiliation>
<email confidence="0.922962">
s.frank@let.ru.nl
</email>
<author confidence="0.904558">
Reut Tsarfaty
</author>
<affiliation confidence="0.956545">
Mathematics and Computer Science
Weizmann Institute of Science
</affiliation>
<address confidence="0.531721">
Rehovot, Israel
</address>
<email confidence="0.940128">
tsarfaty@weizmann.ac.il
</email>
<sectionHeader confidence="0.994415" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999890952380952">
Natural language traffic in social media
(blogs, microblogs, talkbacks) enjoys vast
monitoring and analysis efforts. How-
ever, the question whether computer sys-
tems can generate such content in order
to effectively interact with humans has
been only sparsely attended to. This pa-
per presents an architecture for generat-
ing subjective responses to opinionated
articles based on users’ agenda, docu-
ments’ topics, sentiments and a knowledge
graph. We present an empirical evalua-
tion method for quantifying the human-
likeness and relevance of the generated re-
sponses. We show that responses gen-
erated using world knowledge in the in-
put are regarded as more human-like than
those that rely on topic, sentiment and
agenda only, whereas the use of world
knowledge does not affect perceived rel-
evance.
</bodyText>
<sectionHeader confidence="0.998121" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990708472727273">
Digital media, user-generated content and social
networks enable effective human interaction; so
much so that much of our day-to-day interaction
is conducted online (Viswanath et al., 2009). In-
teraction in social media fundamentally changes
the way businesses and consumers behave (Qual-
man, 2012), can be instrumental to the success
of individuals and businesses (Haenlein and Ka-
plan, 2009), and even affects the stability of polit-
ical regimes (Howard et al., 2011; Lamer, 2012).
These facts force organizations (businesses, gov-
ernments, and non-profit organizations) to be con-
stantly involved in the monitoring of, and the inter-
action with, human agents in digital environments
(Langheinrich and Karjoth, 2011).
Automatic analysis of user-generated online
content benefits from extensive research and com-
mercial opportunities. In natural language pro-
cessing, there is ample research on the analysis
of subjectivity and sentiment of content in social
media. The development of tools for sentiment
analysis (Davidov et al., 2010), mood aggregation
(Agichtein et al., 2008), opinion mining (Mishne,
2006), and many more, now enjoys wide inter-
est and exposure, as is also evident by the many
workshops and dedicated tracks at ACL venues.1
Methods are also developed for the analysis of po-
litical texts (O’Connor et al., 2010; O’Connor et
al., 2013) and for text-driven forecasting based on
these data (Yano et al., 2009). A related strand
of research uses computational methods to find
out what kind of published utterances are influ-
ential, and how they affect linguistic communi-
ties (Danescu-Niculescu-Mizil et al., 2009). Such
work complements, and contributes to, studies
from sociology and sociolinguistics that aim to de-
lineate the process of generating meaningful re-
sponses (e.g., Amabile (1981)).
In contrast to these analysis efforts, the topic
of generating responses to content in social me-
dia is only sparsely explored. Commercially, there
is movement towards online response automation
(Owyang, 2012; Mah, 2012).2 Research on user
interfaces is trying to move away from script-
based interaction towards the development of chat
bots that attempt natural human-like interaction
(Mori et al., 2003; Feng et al., 2006). However,
these chat bots are typically designed to provide
an automated one-size-fits-all type of interaction.
A study by Ritter et al. (2011) addresses
the generation of responses to natural language
tweets in a data-driven setup. It applies a
machine-translation approach to response gener-
ation, where moods and sentiments already ex-
x-
</bodyText>
<footnote confidence="0.807379">
&apos;E.g.,
1E.g., the ACL series LASM http://tinyurl.com/
ludyrkz;WASSA http://tinyurl.com/kjjdhax.
2There is a general debate on the efficiency of automated
</footnote>
<note confidence="0.8354282">
tools (Nall, 2013) and whether such tools are desirable in so-
cial media (McConnell (2012); responses to Owyang (2012)).
58
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58–67,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999967833333333">
pressed in the past are replicated or reused. A re-
cent study by Hasegawa et al. (2013) modifies Rit-
ter’s approach to produce responses that elicit an
emotion from the addressee. Yet, these responses
do not target particular topics and are not driven
by a user agenda.
The present paper addresses the problem of
generating novel, subjective, responses to on-
line opinionated articles. We formally define the
document-to-response mapping problem and sug-
gest an end-to-end system to solve it. Our sys-
tem integrates a range of NLP and NLG technolo-
gies (including topic models, sentiment analysis,
and the integration of a knowledge graph) to de-
sign a flexible generation mechanism that allows
us to vary the information in the input to the gen-
eration procedure. We then use a Turing-inspired
test to study the different factors that contribute to
the perceived human-likeness and relevance of the
generated responses, and show how the perception
of responses depends on external knowledge and
the expressed sentiment.
The remainder of this paper is organized as fol-
lows. The next section presents our proposal: Sec-
tion 2.1 describes our approach, Section 2.2 for-
malizes the proposal, and Section 2.3 presents our
end-to-end architecture. This is followed by our
evaluation method and empirical results in Sec-
tion 3. We discuss related and future work in Sec-
tion 4, and in Section 5 we conclude.
</bodyText>
<sectionHeader confidence="0.997159" genericHeader="method">
2 The Proposal: Generating Subjective
Responses
</sectionHeader>
<subsectionHeader confidence="0.999549">
2.1 Our Approach
</subsectionHeader>
<bodyText confidence="0.999900678571429">
Natural language is, above all, a communicative
device that we employ to achieve certain goals.
In social media, the driving force behind generat-
ing responses is a responder’s disposition towards
some topic. This topic could be a political cam-
paign or a candidate, a product, or some abstract
idea, which the responder has a motive to promote.
Let us call this goal our user’s agenda.
User response generation, like any other natu-
ral language utterance generation, is triggered by
a certain event that is related to the communica-
tive goal. In a social media setting, this event
is often a new online document. The document
and the agenda thus form the input to our gener-
ation system. Each document and each agenda
contain (possibly many) topics, each of which is
associated with a (positive or negative) sentiment.
Document sentiments are attributed to the author,
whereas agenda sentiments are attributed to the
user (henceforth: the responder).
For each non-empty intersection of the topics
in the document and in the agenda, our response-
generation system aims to generate utterances that
are fluent, human-like, and effectively engage
readers. The generation is based on three assump-
tions, roughly reflecting the Gricean maxims of
cooperative interaction (Grice, 1967). Online user
responses should then be:
</bodyText>
<listItem confidence="0.968142714285714">
• Economic (Maxim of Quantity): Responses
are brief and concise;
• Relevant (Maxim of Relation): Responses di-
rectly address the documents’ content.
• Opinionated (Maxim of Quality): Responses
express responders beliefs, sentiments, or
dispositions towards the topic(s).
</listItem>
<subsectionHeader confidence="0.998142">
2.2 The Formal Model
</subsectionHeader>
<bodyText confidence="0.999206333333333">
Let D be a set of documents and let A be a set
of user agendas as we define shortly. Let S be a
set of English sentences over a finite vocabulary
S = E∗. Our system implements a function that
maps each (document, agenda) pair to a natural
language response sentence s E S.
</bodyText>
<equation confidence="0.620629">
fresponse : D x A —* S
</equation>
<bodyText confidence="0.966585666666667">
Response generation takes place in two phases,
roughly corresponding to macro and micro plan-
ning in Reiter and Dale (1997):
</bodyText>
<listItem confidence="0.99917">
• Macro Planning (below, the analysis phase):
What are we going to talk about?
• Micro Planning (below, the generation
phase): How are we going to say it?
</listItem>
<bodyText confidence="0.972700615384615">
The analysis function p : D —* C maps a docu-
ment to a subjective representation of its content.3
The generation function g : C x A —* S inter-
sects the content elements in the document and in
the user agenda, and generates a response based
on the content of the intersection. All in all, our
system implements a composition of the analysis
and the generation functions:
fresponse(d, a) = g(p(d), a) = s
3A content element may conceivably encompass a topic,
its sentiment, its objectivity, its evidentiality, its perceived
truthfulness, and so on. In this paper we focus on topic and
sentiment, and leave the rest for future research.
</bodyText>
<page confidence="0.88195">
59
</page>
<bodyText confidence="0.952873875">
Each content element c E C or an agenda item
a E A is composed of a topic t associated with a
sentiment value sentiments E [−n..n] that sig-
nifies the (negative or positive) disposition of the
document’s author (if c E C) or the user’s agenda
(if a E A) towards the topic. We assume here that
a topic is simply a bag of words from our vocabu-
lary E. Thus, we have the following:
</bodyText>
<equation confidence="0.762179">
A, C C P(E) x [−n..n]
</equation>
<bodyText confidence="0.999942571428571">
Our generation component accepts the result of
the intersection as input and relies on a template-
based grammar and a set of functions for generat-
ing referring expressions in order to construct the
output. To make the responses economic, we limit
the content of a response to one statement about
the document or its author, followed by a state-
ment on the relevant topic. To make the response
relevant, the templates that generate the response
make use of topics in the intersection of the docu-
ment and the agenda. To make the response opin-
ionated, the sentiment of the response depends on
the (mis)match between the sentiment values for
the topic in the document and in the agenda. Con-
cretely, the response is positive if the sentiments
for the topic in the document and agenda are the
same (both positive or both negative) and it is neg-
ative otherwise.
We suggest two variants of the generation func-
tion g. The basic variant implements the baseline
function defined above:
</bodyText>
<equation confidence="0.982546">
gbase(c, a) = s
cEC,aEA,sEE*
</equation>
<bodyText confidence="0.997381333333333">
For the other variant we define a knowledge
base (KB) as a directed graph in which words
w E E from the topic models correspond to nodes
in the graph, and relations r E R between the
words are predicates that hold in the real world.
Our second generation function now becomes:
</bodyText>
<equation confidence="0.988018">
gkb(c, a, KB) = s
KB C {(wi, r, wj)|wi, wj E E, r E R}
</equation>
<bodyText confidence="0.5045095">
with c E C, a E A, s E E* as defined in gbase
above.
</bodyText>
<subsectionHeader confidence="0.998742">
2.3 The Architecture
</subsectionHeader>
<bodyText confidence="0.999995846153846">
The system architecture from a bird’s eye view
is presented in Figure 1. In a nutshell, a docu-
ment enters the analysis phase, where topic infer-
ence and sentiment scoring take place, resulting
in (topic, sentiment)-pairs. During the subsequent
generation phase, these are intersected with the
(topic, sentiment)-pairs in the user agenda. This
intersection, possibly augmented with a knowl-
edge graph, forms the input for a template-based
generation component.
Analysis phase For the task of inferring the top-
ics of the document we use topic modeling: a
probabilistic generative modeling technique that
allows for the discovery of abstract topics over
a large body of documents (Papadimitriou et al.,
1998; Hofmann, 1999; Blei et al., 2003). Specif-
ically, we use topic modeling based on Latent
Dirichlet Allocation (LDA) (Blei et al., 2003; Blei,
2012). Given a new document and a trained
model, the inference method provides a weighted
mix of topics for that document, where each topic
is represented as a vector containing keywords as-
sociated with probabilities. For training the topic
model and inferring the topics in new documents
we use Gensim (Rehurek and Sojka, 2010), a fast
and easy-to-use implementation of LDA.
Next, we wish to infer the sentiment that is ex-
pressed in the text with relation to the topic(s)
identified in the document. We use the seman-
tic/lexical method as implemented in Kathuria
(2012). We rely on a WSD sentiment classifier
that uses the SentiWordNet (Baccianella et al.,
2010) database and calculates the positivity and
negativity scores of a document based on the pos-
itivity and negativity of individual words. The re-
sult of the sentiment analysis is a pair of values,
indicating the positive and negative sentiments of
the document-based scores for individual words.
We use the larger of these two values as the senti-
ment value for the whole document.4
Generation phase Our generation function first
intersects the set of topics in the document and the
set of topics in the agenda in order to discover rel-
evant topics to which the system would generate
responses. A response may in principle integrate
content from a range of topics in the topic model
distribution, but, for the sake of generating concise
responses, in the current implementation we focus
on the single most prevalent, topic. We pick the
highest scoring word of the highest scoring topic,
and intersect it with topics in the agenda. The sys-
tem generates a response based on the identified
</bodyText>
<equation confidence="0.322040333333333">
4Clearly, this is a simplifying assumption. We discuss this
assumption further in Section 4.
60
</equation>
<figureCaption confidence="0.977047">
Figure 1: The system architecture from a bird’s eye view. Components on gray background are executed
offline.
</figureCaption>
<bodyText confidence="0.999896628571429">
topic, the sentiment for the topic in the document,
and the sentiment for that topic in the user agenda.
The generation component relies on a template-
based approach similar to Reiter and Dale (1997)
and Van Deemter et al. (2005). Templates are
essentially subtrees with leaves that are place-
holders for other templates or for functions gener-
ating referring expressions (Theune et al., 2001).
These functions receive (relevant parts of) the in-
put and emit the sequence of fine-grained part-of-
speech (POS) tags that realizes the relevant refer-
ring expression. The POS tags in the resulting
sequences are ultimately place holders for words
from a lexicon E. In order to generate a variety of
expression forms — nouns, adjectives and verbs
— these items are selected randomly from a fine-
grained lexicon we defined. The sentiment (posi-
tive or negative) is expressed in a similar fashion
via templates and randomly selected lexical en-
tries for the POS slots, after calculating the over-
all sentiment for the intersection as stated above.
Our generation implementation is based on Sim-
pleNLG (Gatt and Reiter, 2009) which is a surface
realizer API that allows us to create the desired
templates and functions, and aggregates content
into coherent sentences. The templates and func-
tions that we defined are depicted in Figure 2.
In addition, we handcrafted a simple knowledge
graph (termed here KB) containing the words in a
set of pre-defined user agendas. Table 1 shows a
snippet of the constructed knowledge graph. The
knowledge graph can be used to expand the re-
sponse in the following fashion: The topic of the
response is a node in the KB. We randomly se-
lect one of its outgoing edges for creating a related
</bodyText>
<table confidence="0.987208">
Source Relation Target
Apple CompetesWith Samsung
Apple CompetesWith Google
Apple Creates iOS
</table>
<tableCaption confidence="0.999684">
Table 1: A knowledge graph snippet.
</tableCaption>
<bodyText confidence="0.999984714285714">
statement that has the target node of this relation
as its subject. The related sentence generation uses
the same template-based mechanism as before. In
principle, this process may be repeated any num-
ber of times and express larger parts of the KB.
Here we only add one single knowledge-base re-
lation per response, to keep the responses concise.
</bodyText>
<sectionHeader confidence="0.997597" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999999166666667">
We set out to evaluate how computer-generated re-
sponses compare to human responses in their per-
ceived human-likeness and relevance. More in
particular, we compare different system variants
in order to investigate what makes responses seem
more human-like or relevant.
</bodyText>
<subsectionHeader confidence="0.99701">
3.1 Materials
</subsectionHeader>
<bodyText confidence="0.994611916666667">
Our empirical evaluation is restricted to topics re-
lated to mobile telephones, specifically Apple’s
iPhone and devices based on the Android operat-
ing system. We collected 300 articles from lead-
ing technology sites in the domain to train the
topic models on, settling on 10 topics models.
Next, we generated a set of user agendas refer-
ring to the same 10 topics. Each agenda is rep-
resented by a single keyword from a topic model
distribution and a sentiment value sentiments E
{−8,−4,0,4,8}. Finally, we selected 10 new ar-
ticles from similar sites and generated a pool of
</bodyText>
<figure confidence="0.99396971875">
61
Sresponse Sresponse
NP
VP
I
V↓
belief
SBAR
Sarticle Sitem (Srelation)
that (Sresponse)
VP↓
sentia
VP↓
sentir
NP↓
relationRef
NP↓
articleRef
Sitem
NP↓
itemRef
VP↓
sentii
Sarticle
Srelation
articleRef ← ExpressArticle(...)
itemRef ← ExpressItem(...)
relationRef ← ExpressRelation(...)
sentimenta ← ExpressArticleSentiment(...)
sentimenti ← ExpressItemSentiment(...)
sentimentr ← ExpressRelationSentiment(...)
belief ← ExpressBelief(...)
</figure>
<figureCaption confidence="0.924435">
Figure 2: Template-based response generation. The templates are on the left. The Express* functions on
the right uses regular expressions over the arguments and vocabulary items from a closed lexicon.
</figureCaption>
<bodyText confidence="0.999888714285714">
1000 responses for each, comprising 100 unique
responses for each combination of sentiments
and system variant (i.e., with or without a knowl-
edge base). Table 2 presents an example response
for each such combination. In addition, we ran-
domly collected 5 to 10 real, short or medium-
length, online human responses for each article.
</bodyText>
<subsectionHeader confidence="0.996692">
3.2 Surveys
</subsectionHeader>
<bodyText confidence="0.999950489795918">
We collected evaluation data via two online
surveys on Amazon Mechanical Turk (www.
mturk.com). In Survey 1, participants judged
whether responses to articles were written by hu-
man or computer, akin to (a simplified version of)
the Turing test (Turing, 1950). In Survey 2, re-
sponses were rated on their relevance to the ar-
ticle, in effect testing whether they abide by the
Gricean Maxim of Relation. This is comparable
to the study by Ritter et al. (2011) where people
judged which of two responses was ‘best’.
Each survey comprises 10 randomly ordered tri-
als, corresponding to the 10 selected articles. First,
the participant was presented with a snippet from
the article. When clicking a button, the text was
removed and its presentation duration recorded.
Next, a multiple-choice question asked about the
snippet’s topic. Data on a trial was discarded from
analysis if the participant answered incorrectly or
if the snippet was presented for less than 10 msec
per character; we took these to be cases where the
snippet was not properly read. Next, the partic-
ipant was shown a randomly ordered list of re-
sponses to the article.
In Survey 1, four responses were presented for
each article: three randomly selected from the
pool of human responses to that article and one
generated by our system. The task was to cate-
gorize each response on a 7-point scale with la-
bels ‘Certainly human/computer’, ‘Probably hu-
man/computer’, ‘Maybe human/computer’ and
‘Unsure’. In Survey 2, five responses were pre-
sented: three human responses and two computer-
generated. The task was to rate the responses’
relevance on a 7-point scale labeled ‘Completely
(not) relevant’, ‘Mostly (not) relevant’, ‘Some-
what (not) relevant’, and ‘Unsure’. As a con-
trol condition, one of the human responses and
one of the computer responses were actually taken
from another article than the one just presented.
In both surveys, the computer-generated responses
presented to each participant were balanced across
sentiment levels and generation functions (gbase
and gkb). After completing the 10 trials, partic-
ipants provided basic demographic information,
including native language. Data from non-native
English speakers was discarded. Surveys 1 and 2
were completed by 62 and 60 native speakers, re-
spectively.
</bodyText>
<subsectionHeader confidence="0.996496">
3.3 Analysis and Results
</subsectionHeader>
<bodyText confidence="0.99568075">
Survey 1: Computer-Likeness Rating. Table 3
shows the mean ‘computer-likeness’-ratings from
1 (‘Certainly human’) to 7 (‘Certainly computer’)
for each response category. Clearly, the human
responses are rated as more human-like than the
computer-generated ones: our model did not gen-
erally mislead the participants. This may be due
to the template-based response structure: over the
course of the survey, human raters are likely to
notice this structure and infer that such responses
are computer-generated. To investigate whether
such learning indeed occurs, a linear mixed-
effects model was fitted, with predictor variables
IS COMP (+1:computer-generated, −1:human re-
sponses), POS (position of the trial in the survey, 0
to 9), and the interaction between the two. Table 4
</bodyText>
<page confidence="0.650298">
62
</page>
<table confidence="0.997997333333333">
Sent. KB Response
−8 No Android is horrendous so I think that the writer is completely correct!!!
Yes Apple is horrendous so I feel that the author is not really right!!! iOS is horrendous as well.
−4 No I think that the writer is mistaken because apple actually is unexceptional.
Yes I think that the author is wrong because Nokia is mediocre. Apple on the other hand is pretty good ...
0 No The text is accurate. Apple is okay.
Yes Galaxy is okay so I think that the content is accurate. All-in-all samsung makes fantastic gadgets.
4 No Android is pretty good so I feel that the author is right.
Yes Nokia is nice. The article is precise. Samsung on the other hand is fabulous...
8 No Galaxy is great!!! The text is completely precise.
Yes Galaxy is awesome!!! The author is not completely correct. In fact I think that samsung makes
awesome products.
</table>
<tableCaption confidence="0.9751555">
Table 2: Responses generated by the system with or without a knowledge-base (KB), with different
sentiment levels.
</tableCaption>
<table confidence="0.9996422">
Response Type Mean and CI
Factor b t P(b G 0)
Human 3.33 f 0.08
Computer (all) 4.49 f 0.15
Computer (−KB) 4.66 f 0.20
Computer (+KB) 4.32 f 0.22
(intercept) 3.590 2.11 0.015
IS COMP 0.193 4.76 0.000
POS 0.069 6.27 0.000
IS COMP x POS 0.085
</table>
<tableCaption confidence="0.9975585">
Table 3: Mean and 95% confidence interval of
computer-likeness rating per response category.
fKB indicates whether gbage or gkb was used.
Table 4: Computer-likeness rating regression re-
</tableCaption>
<bodyText confidence="0.993845">
sults, comparing human to computer responses.
presents, for each factor in the regression analysis,
the coefficient b and its t-statistic. The coefficient
equals the increase in computer-likeness rating for
each unit increase in the predictor variable. The t-
statistic is indicative of how much variance in the
ratings is accounted for by the predictor. We also
obtained a probability distribution over each co-
efficient by Markov Chain Monte Carlo sampling
using the R package lme4 version 0.99 (Bates,
2005). From each coefficient’s distribution, we es-
timate the posterior probability that b is negative,
which quantifies the reliability of the effect.
The positive b value for POS shows that re-
sponses drift towards the ‘computer’-end of the
scale. More importantly, a positive interaction
with IS COMP indicates that the difference be-
tween human and computer responses becomes
more noticeable as the survey progresses —
the participants did learn to identify computer-
generated responses. However, the positive coef-
ficient for IS COMP means that even at the very
first trial, computer responses are considered to be
more computer-like than human responses.
Factors Affecting Human-Likeness. Our find-
ing that the identifiability of computer-generated
responses cannot be fully attributed to their repet-
itiveness, raises the question: What makes a such
a response more human-like? The results provide
several insights into this matter.
First, the mean scores in Table 3 suggest that in-
cluding a knowledge base increases the responses’
human-likeness. To further investigate this, we
performed a separate regression analysis, using
only the data on computer-generated responses.
This analysis also included predictors KB (+1:
knowledge base included, −1: otherwise), SENT
(sentiments, from −8 to +8), absolute value of
SENT, and the interaction between KB and POS.
As can be seen in Table 5, there is no reliable in-
teraction between KB and POS: the effect of in-
cluding the KB on the human-likeness of responses
remained constant over the course of the survey.
Furthermore, we see evidence that responses
with a more positive sentiment are considered
more computer-like. The (only weakly reliable)
negative effect of the absolute value of senti-
ment suggests that more extreme sentiments are
considered more human-like. Apparently, people
count on computer responses to be mildly positive,
whereas human responses are expected to be more
extreme, and extremely negative in particular.
Survey 2: Relevance Rating. The mean rele-
vance scores in Table 6 reveal that a response is
rated as more relevant to a snippet if it was actu-
ally a response to that snippet, rather than to a dif-
ferent snippet. This reinforces our design choice
</bodyText>
<page confidence="0.906731">
63
</page>
<table confidence="0.997863428571429">
Factor b t P(b G 0)
(intercept) 4.022
KB −0.240 −2.13 0.987
POS 0.144 5.82 0.000
SENT 0.035 2.98 0.002
abs(SENT) −0.041 −1.97 0.967
KB X POS 0.023 1.03 0.121
</table>
<tableCaption confidence="0.9912105">
Table 5: Computer-likeness rating regression re-
sults, comparing systems with and without KB.
</tableCaption>
<table confidence="0.9996298">
Factor b t P(b G 0)
(intercept) 3.861
IS COMP −0.339 −7.10 1.000
SOURCE 0.824 16.80 0.000
IS COMP X PRES 0.179 5.03 0.000
</table>
<tableCaption confidence="0.808957">
Table 7: Relevance ratings regression results,
comparing human to computer responses.
</tableCaption>
<bodyText confidence="0.999892642857143">
to include input items referring specifically to the
topic and sentiment of the author. However, hu-
man responses are considered more relevant than
the computer-generated ones. This is confirmed
by a reliably negative regression coefficient for
IS COMP (see regression results in Table 7).
The analysis included the binary factor SOURCE
(+1 if the response came from the presented snip-
pet, −1 if it came from a random article). We
see a positive interaction between SOURCE and
IS COMP, indicating that presenting a response
from a random article is more detrimental to rel-
evance of computer-generated responses than that
of the human responses. This is not surprising, as
the computer-generated responses (unlike the hu-
man responses) always includes the article’s topic.
When analyzing only data on computer-
generated responses, and including predictors for
agenda sentiment and for presence of the knowl-
edge base, we see that including the KB does not
affect response relevance (see Table 8). Also, there
is no interaction between KB and SOURCE, that
is, the effect of presenting a response from a dif-
ferent article does not differ between the models
with and without the knowledge base. Possibly,
responses are considered as more relevant if they
have more positive sentiment, but the evidence for
this is fairly weak.
</bodyText>
<subsectionHeader confidence="0.591388">
Response Type Source Mean and CI
</subsectionHeader>
<bodyText confidence="0.9257006">
Table 6: Mean and 95% confidence interval of
relevance rating per response category. ‘Source’
indicates whether the response is from the pre-
sented text snippet or a random other snippet.
fKB indicates whether gbm, or gkb was used.
</bodyText>
<table confidence="0.999071571428571">
Factor b t P(b G 0)
(intercept) 3.603
KB 0.026 0.49 0.322
SOURCE 1.003 15.90 0.000
SENT 0.023 1.94 0.029
abs(SENT) −0.017 −0.93 0.819
KB X SOURCE −0.032 −0.61 0.731
</table>
<tableCaption confidence="0.8847415">
Table 8: Relevance ratings regression results,
comparing systems with and without KB.
</tableCaption>
<sectionHeader confidence="0.995027" genericHeader="evaluation">
4 Related and Future Work
</sectionHeader>
<bodyText confidence="0.999618777777778">
In contrast to the vast amount of research on sen-
timent and topic analysis, as well as generation
tasks in which the input is artificial or pre-defined,
our system implements a full end-to-end cycle
from natural language analysis to natural language
generation with applications in social media and
automated interaction in real-world settings.
The only two other studies on response gener-
ation in social media we know of are Ritter et al.
(2011) and Hasegawa et al. (2013). Ritter’s and
Hasegawa’s approaches differ from ours in their
objective and their approach to generation. Specif-
ically, Ritter’s approach is based on machine trans-
lation, creating responses by directly re-using pre-
vious content. Their data-driven approach gener-
ates relevant, but not opinionated responses. In
addition, both Ritter’s and Hasegawa’s systems re-
spond to tweets, while our system analyzes and re-
sponds to complete articles. Hasegawa’s approach
is closer to ours in that it generates responses that
are intended to elicit a specific emotion from the
addressee. However, it still differs considerably in
settings (dialogues versus online posting) and in
the goal itself (eliciting emotion versus expressing
opinion). Thus, we see these studies as comple-
mentary to ours in the realm of response genera-
tion in social media.
</bodyText>
<table confidence="0.997182333333333">
this 4.85 f 0.11
Human
other 3.56 f 0.18
Computer (all) this 4.52 f 0.16
other 2.52 f 0.15
Computer (−KB) this 4.53 f 0.23
other 2.46 f 0.21
this 4.51 f 0.23
Computer (+KB) other 2.58 f 0.22
</table>
<page confidence="0.929818">
64
</page>
<bodyText confidence="0.999966030769231">
A natural contact point of our work with exist-
ing work in social media analysis is the investiga-
tion of how a change in the implementation of in-
dividual components (e.g., topic inference or sen-
timent scoring) would affect the result of the over-
all generation. In particular, it would be interesting
to test whether a novel mechanism for joint infer-
ence of topic/sentiment distributions could lead to
improvement in the human-likeness of the gener-
ated responses.
The syntactic and semantic means of expres-
sion that we use are based on bare bone templates
and fine-grained POS tags (Theune et al., 2001).
These may potentially be expanded with different
ways to express subject/object relations, relations
between phrases, polarity of sentences, and so on.
Additional approaches to generation can factor in
such aspects, e.g., the template-based methods in
Becker (2002) and Narayan et al. (2011), or gram-
mar based methods, as in DeVault et al. (2008).
Using more sophisticated generation methods with
a rich grammatical backbone may combat the sen-
sitivity to computer-generated response patterns as
acquired by our human raters over time.
Furthermore, our result concerning the human-
likeness of gkb clearly demonstrates that semantic
knowledge must be brought in to support better,
and more human-like, response generation. Large-
scale knowledge graphs such as Freebase support
many semantic tasks (Jacobs, 1985), and can be
used for providing richer context for automatically
generating human-like responses.
From a theoretical viewpoint, the system will
clearly benefit from rigorous analysis of human
interaction in online media. Responses to user-
generated content on the Internet share some
linguistic characteristics in structure, length and
manner of expression. Studying these features the-
oretically and then examining them empirically
using a Turing-like evaluation as presented here
can take us a big step in the direction of better gen-
eration, and also better understanding of the pro-
cesses underlying human response generation.
This latter understanding may be complemented
with insights into the causes, motivations and in-
tricacies of human interaction in such environ-
ments, as studied by sociologists and psychol-
ogists. In particular, our preliminary interac-
tion with colleagues from communication stud-
ies suggests that the present endeavor nicely com-
plements that of “persuasive computing” (Fogg,
1998; Fogg, 2002), and we hope that this collabo-
ration will lead to valuable synergies.
Finally, bridging the gap between the technical
and the theoretical, it would be fascinating to test
the responses in the context for which they are
generated – social media. Generated texts may
be posted as a response to the original article, or
shared with a link of the original article, followed
by measuring the responses to, and shares of, that
response. Such real-world evaluation could indi-
cate that generated responses are indeed believable
and engaging, and may better simulate a Turing-
like test in which machine-generated responses
cannot be distinguished from human responses.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995260869565">
We presented a system for generating responses
that are directly tied to responders’ agendas and
document content. To the best of our knowledge,
this is the first system to generate subjective re-
sponses directly reflecting users’ agendas. Our re-
sponse generation architecture provides an easy-
to-use and easy-to-extend solution encompassing
a range of NLP and NLG techniques. We evalu-
ated both the human-likeness and the relevance of
the generated content, thereby empirically quan-
tifying the efficacy of computer-generated re-
sponses compared head-to-head against human re-
sponses.
Generating concise, relevant, and opinionated
responses that are also human-like is hard — it
requires the integration of text-understanding and
sentiment analysis, and it is also contingent on the
expression of the agents’ prior knowledge, reasons
and motives. We suggest our architecture and eval-
uation method as a baseline for future research
on generated content that would effectively pass
a Turing-like test, and successfully convince hu-
mans of the authenticity of generated responses.5
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99993425">
We thank Yoav Francis for his contribution in the
early stages of this research. We further thank
our anonymous reviewers for their insightful com-
ments on an earlier draft.
</bodyText>
<footnote confidence="0.767519">
5Our code, training data, experimental data (computer and
human responses) and analysis scripts are publicly available
via www.tsarfaty.com/nlg-sd/.
</footnote>
<page confidence="0.955531">
65
</page>
<sectionHeader confidence="0.995872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997403946078432">
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In Proceed-
ings of the international conference on Web search
and web data mining, pages 183–194. ACM.
Teresa M. Amabile. 1981. Brilliant but Cruel: Per-
ceptions of Negative Evaluators. Washington, DC:
ERIC Clearinghouse.
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta. European Language Re-
sources Association (ELRA).
Douglas M. Bates. 2005. Fitting linear mixed models
in R. R News, 5:27–30.
Tilman Becker. 2002. Practical, template-based natu-
ral language generation with TAG. In Proceedings
of the 6th International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+6).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. J. Mach. Learn.
Res., 3:993–1022.
David M. Blei. 2012. Probabilistic topic models.
Commun. ACM, 55(4):77–84.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study
on amazon.com helpfulness votes. In Proceedings
of the 18th International Conference on World Wide
Web, WWW ’09, pages 141–150, New York, NY,
USA. ACM.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using Twitter hashtags
and smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 241–249, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Proceedings of the Fifth International Natural Lan-
guage Generation Conference, INLG ’08, pages 77–
85, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard
Hovy. 2006. An intelligent discussion-bot for
answering student queries in threaded discussions.
In Proceedings of Intelligent User Interface (IUI-
2006), pages 171–177.
B. J. Fogg. 1998. Persuasive computers: Perspec-
tives and research directions. In Proceedings of the
SIGCHI Conference on Human Factors in Comput-
ing Systems, CHI ’98, pages 225–232, New York,
NY, USA. ACM Press/Addison-Wesley Publishing
Co.
B. J. Fogg. 2002. Persuasive technology: Using com-
puters to change what we think and do. Ubiquity,
December.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A
realisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation, ENLG ’09, pages 90–93,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
H. P. Grice. 1967. Logic and conversation. In H. P.
Grice, editor, Studies in the ways of words, pages
22–40. Harvard University Press.
Michael Haenlein and Andreas M. Kaplan. 2009.
Flagship brand stores within virtual worlds: The im-
pact of virtual store exposure on real-life attitude
toward the brand and purchase intent. Recherche
et Applications en Marketing (English Edition),
24(3):57–79.
Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,
and Masashi Toyoda. 2013. Predicting and eliciting
addressee’s emotion in online dialogue. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 964–972, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
’99, pages 50–57, New York, NY, USA. ACM.
Philip N. Howard, Aiden Duffy, Deen Freelon, Muza-
mmil Hussain, Will Mari, and Marwa Mazaid.
2011. Opening closed regimes: What was the role
of social media during the Arab spring? Project on
Information Technology and Political Islam.
Paul S Jacobs. 1985. A knowledge-based approach to
language production. Technical report, University
of California at Berkeley, Berkeley, CA, USA.
Pulkit Kathuria. 2012. Sentiment Clas-
sification using WSD, Maximum En-
tropy and Naive Bayes Classifiers.
https://github.com/kevincobain2000/sentiment classifier.
Visited March 2014.
Wiebke Lamer. 2012. Twitter and tyrants: New me-
dia and its effects on sovereignty in the Middle East.
Arab Media and Society.
Marc Langheinrich and G¨unter Karjoth. 2011. Social
networking and the risk to companies and institu-
tions. Information Security Technical Report. Spe-
cial Issue: Identity Reconstruction and Theft, pages
51–56.
66
Paul Mah. 2012. Tools to automate your
customer service response on social me-
dia. http://www.itbusinessedge.com/blogs/smb-
tech/tools-to-automate-your-customer-service-
response-on-social-media.html. Visited August
2013.
Chris McConnell. 2012. When brands auto-
mate Twitter and Facebook responses I’ll re-
volt. http://dailytekk.com/2012/06/07/brands-
automating-social-media/. Visited August 2013.
Gilad Mishne. 2006. Multiple ranking strategies for
opinion retrieval in blogs. In Proceedings of the 15th
Text Retrieval Conference.
Kyoshi Mori, Adam Jatowt, and Mitsuru Ishizuka.
2003. Enhancing conversational flexibility in multi-
modal interactions with embodied lifelike agent. In
Proceedings of the 8th International Conference on
Intelligent User Interfaces, IUI ’03, pages 270–272,
New York, NY, USA. ACM.
Mickey Nall. 2013. You can’t automate so-
cial media engagement, argues PRSA’s Mickey
Nall. http://www.prmoment.com/1359/you-cant-
automate-social-media-engagement-argues-prsas-
mickey-nall.aspx. Visited August 2013.
Karthik Sankaran Narayan, Charles Lee Isbell Jr., and
David L. Roberts. 2011. Dextor: Reduced effort
authoring for template-based natural language gen-
eration. In Vadim Bulitko and Mark O. Riedl, ed-
itors, Proceedings of the Seventh Artificial Intelli-
gence and Interactive Digital Entertainment Confer-
ence. The AAAI Press.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen
and Samuel Gosling, editors, ICWSM. The AAAI
Press.
Brendan O’Connor, Brandon M. Stewart, and Noah A.
Smith. 2013. Learning to extract international rela-
tions from political context. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1094–1104. The Association for Computer Linguis-
tics.
Jeremiah Owyang. 2012. Brands Start Automating
Social Media Responses on Facebook and Twitter.
http://techcrunch.com/2012/06/07/brands-start-
automating-social-media-responses-on-facebook-
and-twitter/. Visited August 2013.
Christos H. Papadimitriou, Hisao Tamaki, Prabhakar
Raghavan, and Santosh Vempala. 1998. La-
tent Semantic Indexing: A probabilistic analy-
sis. In Proceedings of the Seventeenth ACM
SIGACT-SIGMOD-SIGART Symposium on Princi-
ples of Database Systems, PODS ’98, pages 159–
168, New York, NY, USA. ACM.
Erik Qualman. 2012. Socialnomics: How social media
transforms the way we live and do business. John
Wiley &amp; Sons, Hoboken, NJ, USA, 2nd edition.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May. ELRA.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Nat. Lang.
Eng., 3(1):57–87.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 583–593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
M. Theune, E. Klabbers, J. R. De Pijper, E. Krahmer,
and J. Odijk. 2001. From data to speech: A general
approach. Nat. Lang. Eng., 7(1):47–86.
Alan M. Turing. 1950. Computing machinery and in-
telligence. Mind, LIX:433–460.
Kees Van Deemter, Emiel Krahmer, and Mari¨et The-
une. 2005. Real versus template-based natural lan-
guage generation: A false opposition? Comput. Lin-
guist., 31(1):15–24.
Bimal Viswanath, Alan Mislove, Meeyoung Cha, and
Krishna P. Gummadi. 2009. On the evolution of
user interaction in Facebook. In Proceedings of
the 2nd ACM Workshop on Online Social Networks,
WOSN ’09, pages 37–42, New York, NY, USA.
ACM.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ’09, pages
477–485, Stroudsburg, PA, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.967356">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.082865">
<title confidence="0.991348">Generating Subjective Responses to Opinionated Articles in Social Media: An Agenda-Driven Architecture and a Turing-Like Test</title>
<author confidence="0.999671">Tomer Cagan</author>
<affiliation confidence="0.9995695">School of Computer Science The Interdisciplinary Center</affiliation>
<address confidence="0.999409">Herzeliya, Israel</address>
<email confidence="0.998373">cagan.tomer@idc.ac.il</email>
<author confidence="0.998033">L Stefan</author>
<affiliation confidence="0.998814">Centre for Language Studies Radboud University</affiliation>
<address confidence="0.996622">Nijmegen, The Netherlands</address>
<email confidence="0.995503">s.frank@let.ru.nl</email>
<title confidence="0.29002775">Reut Mathematics and Computer Weizmann Institute of Rehovot,</title>
<email confidence="0.986374">tsarfaty@weizmann.ac.il</email>
<abstract confidence="0.990243409090909">Natural language traffic in social media (blogs, microblogs, talkbacks) enjoys vast and However, the question whether computer syscan content in order to effectively interact with humans has been only sparsely attended to. This paper presents an architecture for generating subjective responses to opinionated articles based on users’ agenda, documents’ topics, sentiments and a knowledge graph. We present an empirical evaluation method for quantifying the humanlikeness and relevance of the generated responses. We show that responses generated using world knowledge in the input are regarded as more human-like than those that rely on topic, sentiment and agenda only, whereas the use of world knowledge does not affect perceived relevance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Gilad Mishne</author>
</authors>
<title>Finding high-quality content in social media.</title>
<date>2008</date>
<booktitle>In Proceedings of the international conference on Web</booktitle>
<pages>183--194</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2345" citStr="Agichtein et al., 2008" startWordPosition="336" endWordPosition="339">ard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies</context>
</contexts>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. 2008. Finding high-quality content in social media. In Proceedings of the international conference on Web search and web data mining, pages 183–194. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa M Amabile</author>
</authors>
<title>Brilliant but Cruel: Perceptions of Negative Evaluators.</title>
<date>1981</date>
<publisher>ERIC Clearinghouse.</publisher>
<location>Washington, DC:</location>
<contexts>
<context position="3072" citStr="Amabile (1981)" startWordPosition="453" endWordPosition="454">many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of res</context>
</contexts>
<marker>Amabile, 1981</marker>
<rawString>Teresa M. Amabile. 1981. Brilliant but Cruel: Perceptions of Negative Evaluators. Washington, DC: ERIC Clearinghouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="11887" citStr="Baccianella et al., 2010" startWordPosition="1925" endWordPosition="1928">ined model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation to the topic(s) identified in the document. We use the semantic/lexical method as implemented in Kathuria (2012). We rely on a WSD sentiment classifier that uses the SentiWordNet (Baccianella et al., 2010) database and calculates the positivity and negativity scores of a document based on the positivity and negativity of individual words. The result of the sentiment analysis is a pair of values, indicating the positive and negative sentiments of the document-based scores for individual words. We use the larger of these two values as the sentiment value for the whole document.4 Generation phase Our generation function first intersects the set of topics in the document and the set of topics in the agenda in order to discover relevant topics to which the system would generate responses. A response</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas M Bates</author>
</authors>
<title>Fitting linear mixed models in R.</title>
<date>2005</date>
<journal>R News,</journal>
<pages>5--27</pages>
<contexts>
<context position="22082" citStr="Bates, 2005" startWordPosition="3576" endWordPosition="3577">se category. fKB indicates whether gbage or gkb was used. Table 4: Computer-likeness rating regression results, comparing human to computer responses. presents, for each factor in the regression analysis, the coefficient b and its t-statistic. The coefficient equals the increase in computer-likeness rating for each unit increase in the predictor variable. The tstatistic is indicative of how much variance in the ratings is accounted for by the predictor. We also obtained a probability distribution over each coefficient by Markov Chain Monte Carlo sampling using the R package lme4 version 0.99 (Bates, 2005). From each coefficient’s distribution, we estimate the posterior probability that b is negative, which quantifies the reliability of the effect. The positive b value for POS shows that responses drift towards the ‘computer’-end of the scale. More importantly, a positive interaction with IS COMP indicates that the difference between human and computer responses becomes more noticeable as the survey progresses — the participants did learn to identify computergenerated responses. However, the positive coefficient for IS COMP means that even at the very first trial, computer responses are conside</context>
</contexts>
<marker>Bates, 2005</marker>
<rawString>Douglas M. Bates. 2005. Fitting linear mixed models in R. R News, 5:27–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
</authors>
<title>Practical, template-based natural language generation with TAG.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6).</booktitle>
<contexts>
<context position="28991" citStr="Becker (2002)" startWordPosition="4689" endWordPosition="4690">ion. In particular, it would be interesting to test whether a novel mechanism for joint inference of topic/sentiment distributions could lead to improvement in the human-likeness of the generated responses. The syntactic and semantic means of expression that we use are based on bare bone templates and fine-grained POS tags (Theune et al., 2001). These may potentially be expanded with different ways to express subject/object relations, relations between phrases, polarity of sentences, and so on. Additional approaches to generation can factor in such aspects, e.g., the template-based methods in Becker (2002) and Narayan et al. (2011), or grammar based methods, as in DeVault et al. (2008). Using more sophisticated generation methods with a rich grammatical backbone may combat the sensitivity to computer-generated response patterns as acquired by our human raters over time. Furthermore, our result concerning the humanlikeness of gkb clearly demonstrates that semantic knowledge must be brought in to support better, and more human-like, response generation. Largescale knowledge graphs such as Freebase support many semantic tasks (Jacobs, 1985), and can be used for providing richer context for automat</context>
</contexts>
<marker>Becker, 2002</marker>
<rawString>Tilman Becker. 2002. Practical, template-based natural language generation with TAG. In Proceedings of the 6th International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="11118" citStr="Blei et al., 2003" startWordPosition="1798" endWordPosition="1801">phase, where topic inference and sentiment scoring take place, resulting in (topic, sentiment)-pairs. During the subsequent generation phase, these are intersected with the (topic, sentiment)-pairs in the user agenda. This intersection, possibly augmented with a knowledge graph, forms the input for a template-based generation component. Analysis phase For the task of inferring the topics of the document we use topic modeling: a probabilistic generative modeling technique that allows for the discovery of abstract topics over a large body of documents (Papadimitriou et al., 1998; Hofmann, 1999; Blei et al., 2003). Specifically, we use topic modeling based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012). Given a new document and a trained model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation to the topic(s) identified in the do</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
</authors>
<title>Probabilistic topic models.</title>
<date>2012</date>
<journal>Commun. ACM,</journal>
<volume>55</volume>
<issue>4</issue>
<contexts>
<context position="11230" citStr="Blei, 2012" startWordPosition="1818" endWordPosition="1819">uent generation phase, these are intersected with the (topic, sentiment)-pairs in the user agenda. This intersection, possibly augmented with a knowledge graph, forms the input for a template-based generation component. Analysis phase For the task of inferring the topics of the document we use topic modeling: a probabilistic generative modeling technique that allows for the discovery of abstract topics over a large body of documents (Papadimitriou et al., 1998; Hofmann, 1999; Blei et al., 2003). Specifically, we use topic modeling based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012). Given a new document and a trained model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation to the topic(s) identified in the document. We use the semantic/lexical method as implemented in Kathuria (2012). We rely on a WSD sentiment classif</context>
</contexts>
<marker>Blei, 2012</marker>
<rawString>David M. Blei. 2012. Probabilistic topic models. Commun. ACM, 55(4):77–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Gueorgi Kossinets</author>
<author>Jon Kleinberg</author>
<author>Lillian Lee</author>
</authors>
<title>How opinions are received by online communities: A case study on amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International Conference on World Wide Web, WWW ’09,</booktitle>
<pages>141--150</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2893" citStr="Danescu-Niculescu-Mizil et al., 2009" startWordPosition="425" endWordPosition="428"> for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 20</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Kossinets, Kleinberg, Lee, 2009</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinberg, and Lillian Lee. 2009. How opinions are received by online communities: A case study on amazon.com helpfulness votes. In Proceedings of the 18th International Conference on World Wide Web, WWW ’09, pages 141–150, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using Twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2302" citStr="Davidov et al., 2010" startWordPosition="330" endWordPosition="333">s the stability of political regimes (Howard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such wo</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using Twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 241–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>David Traum</author>
<author>Ron Artstein</author>
</authors>
<title>Practical grammar-based NLG from examples.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08,</booktitle>
<pages>77--85</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29072" citStr="DeVault et al. (2008)" startWordPosition="4703" endWordPosition="4706">nism for joint inference of topic/sentiment distributions could lead to improvement in the human-likeness of the generated responses. The syntactic and semantic means of expression that we use are based on bare bone templates and fine-grained POS tags (Theune et al., 2001). These may potentially be expanded with different ways to express subject/object relations, relations between phrases, polarity of sentences, and so on. Additional approaches to generation can factor in such aspects, e.g., the template-based methods in Becker (2002) and Narayan et al. (2011), or grammar based methods, as in DeVault et al. (2008). Using more sophisticated generation methods with a rich grammatical backbone may combat the sensitivity to computer-generated response patterns as acquired by our human raters over time. Furthermore, our result concerning the humanlikeness of gkb clearly demonstrates that semantic knowledge must be brought in to support better, and more human-like, response generation. Largescale knowledge graphs such as Freebase support many semantic tasks (Jacobs, 1985), and can be used for providing richer context for automatically generating human-like responses. From a theoretical viewpoint, the system </context>
</contexts>
<marker>DeVault, Traum, Artstein, 2008</marker>
<rawString>David DeVault, David Traum, and Ron Artstein. 2008. Practical grammar-based NLG from examples. In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08, pages 77– 85, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghui Feng</author>
<author>Erin Shaw</author>
<author>Jihie Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>An intelligent discussion-bot for answering student queries in threaded discussions.</title>
<date>2006</date>
<booktitle>In Proceedings of Intelligent User Interface (IUI2006),</booktitle>
<pages>171--177</pages>
<contexts>
<context position="3496" citStr="Feng et al., 2006" startWordPosition="516" endWordPosition="519">l et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl.com/ ludyrkz;WASSA http://tinyurl.com/kjjdhax. 2There is a general debate on the efficiency of automated tools (Nall, 2013) and whether such tools are desirable in social media (McConnell (2012); responses to Owya</context>
</contexts>
<marker>Feng, Shaw, Kim, Hovy, 2006</marker>
<rawString>Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy. 2006. An intelligent discussion-bot for answering student queries in threaded discussions. In Proceedings of Intelligent User Interface (IUI2006), pages 171–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Fogg</author>
</authors>
<title>Persuasive computers: Perspectives and research directions.</title>
<date>1998</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’98,</booktitle>
<pages>225--232</pages>
<publisher>ACM Press/Addison-Wesley Publishing Co.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="30536" citStr="Fogg, 1998" startWordPosition="4918" endWordPosition="4919">y and then examining them empirically using a Turing-like evaluation as presented here can take us a big step in the direction of better generation, and also better understanding of the processes underlying human response generation. This latter understanding may be complemented with insights into the causes, motivations and intricacies of human interaction in such environments, as studied by sociologists and psychologists. In particular, our preliminary interaction with colleagues from communication studies suggests that the present endeavor nicely complements that of “persuasive computing” (Fogg, 1998; Fogg, 2002), and we hope that this collaboration will lead to valuable synergies. Finally, bridging the gap between the technical and the theoretical, it would be fascinating to test the responses in the context for which they are generated – social media. Generated texts may be posted as a response to the original article, or shared with a link of the original article, followed by measuring the responses to, and shares of, that response. Such real-world evaluation could indicate that generated responses are indeed believable and engaging, and may better simulate a Turinglike test in which m</context>
</contexts>
<marker>Fogg, 1998</marker>
<rawString>B. J. Fogg. 1998. Persuasive computers: Perspectives and research directions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’98, pages 225–232, New York, NY, USA. ACM Press/Addison-Wesley Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Fogg</author>
</authors>
<title>Persuasive technology: Using computers to change what we think and do.</title>
<date>2002</date>
<tech>Ubiquity,</tech>
<contexts>
<context position="30549" citStr="Fogg, 2002" startWordPosition="4920" endWordPosition="4921">xamining them empirically using a Turing-like evaluation as presented here can take us a big step in the direction of better generation, and also better understanding of the processes underlying human response generation. This latter understanding may be complemented with insights into the causes, motivations and intricacies of human interaction in such environments, as studied by sociologists and psychologists. In particular, our preliminary interaction with colleagues from communication studies suggests that the present endeavor nicely complements that of “persuasive computing” (Fogg, 1998; Fogg, 2002), and we hope that this collaboration will lead to valuable synergies. Finally, bridging the gap between the technical and the theoretical, it would be fascinating to test the responses in the context for which they are generated – social media. Generated texts may be posted as a response to the original article, or shared with a link of the original article, followed by measuring the responses to, and shares of, that response. Such real-world evaluation could indicate that generated responses are indeed believable and engaging, and may better simulate a Turinglike test in which machine-genera</context>
</contexts>
<marker>Fogg, 2002</marker>
<rawString>B. J. Fogg. 2002. Persuasive technology: Using computers to change what we think and do. Ubiquity, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Ehud Reiter</author>
</authors>
<title>SimpleNLG: A realisation engine for practical applications.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG ’09,</booktitle>
<pages>90--93</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14181" citStr="Gatt and Reiter, 2009" startWordPosition="2304" endWordPosition="2307">part-ofspeech (POS) tags that realizes the relevant referring expression. The POS tags in the resulting sequences are ultimately place holders for words from a lexicon E. In order to generate a variety of expression forms — nouns, adjectives and verbs — these items are selected randomly from a finegrained lexicon we defined. The sentiment (positive or negative) is expressed in a similar fashion via templates and randomly selected lexical entries for the POS slots, after calculating the overall sentiment for the intersection as stated above. Our generation implementation is based on SimpleNLG (Gatt and Reiter, 2009) which is a surface realizer API that allows us to create the desired templates and functions, and aggregates content into coherent sentences. The templates and functions that we defined are depicted in Figure 2. In addition, we handcrafted a simple knowledge graph (termed here KB) containing the words in a set of pre-defined user agendas. Table 1 shows a snippet of the constructed knowledge graph. The knowledge graph can be used to expand the response in the following fashion: The topic of the response is a node in the KB. We randomly select one of its outgoing edges for creating a related So</context>
</contexts>
<marker>Gatt, Reiter, 2009</marker>
<rawString>Albert Gatt and Ehud Reiter. 2009. SimpleNLG: A realisation engine for practical applications. In Proceedings of the 12th European Workshop on Natural Language Generation, ENLG ’09, pages 90–93, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1967</date>
<booktitle>Studies in the ways of words,</booktitle>
<pages>22--40</pages>
<editor>In H. P. Grice, editor,</editor>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="7032" citStr="Grice, 1967" startWordPosition="1075" endWordPosition="1076">o our generation system. Each document and each agenda contain (possibly many) topics, each of which is associated with a (positive or negative) sentiment. Document sentiments are attributed to the author, whereas agenda sentiments are attributed to the user (henceforth: the responder). For each non-empty intersection of the topics in the document and in the agenda, our responsegeneration system aims to generate utterances that are fluent, human-like, and effectively engage readers. The generation is based on three assumptions, roughly reflecting the Gricean maxims of cooperative interaction (Grice, 1967). Online user responses should then be: • Economic (Maxim of Quantity): Responses are brief and concise; • Relevant (Maxim of Relation): Responses directly address the documents’ content. • Opinionated (Maxim of Quality): Responses express responders beliefs, sentiments, or dispositions towards the topic(s). 2.2 The Formal Model Let D be a set of documents and let A be a set of user agendas as we define shortly. Let S be a set of English sentences over a finite vocabulary S = E∗. Our system implements a function that maps each (document, agenda) pair to a natural language response sentence s E</context>
</contexts>
<marker>Grice, 1967</marker>
<rawString>H. P. Grice. 1967. Logic and conversation. In H. P. Grice, editor, Studies in the ways of words, pages 22–40. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Haenlein</author>
<author>Andreas M Kaplan</author>
</authors>
<title>Flagship brand stores within virtual worlds: The impact of virtual store exposure on real-life attitude toward the brand and purchase intent.</title>
<date>2009</date>
<journal>Recherche et Applications en Marketing (English Edition),</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="1664" citStr="Haenlein and Kaplan, 2009" startWordPosition="235" endWordPosition="239">hat responses generated using world knowledge in the input are regarded as more human-like than those that rely on topic, sentiment and agenda only, whereas the use of world knowledge does not affect perceived relevance. 1 Introduction Digital media, user-generated content and social networks enable effective human interaction; so much so that much of our day-to-day interaction is conducted online (Viswanath et al., 2009). Interaction in social media fundamentally changes the way businesses and consumers behave (Qualman, 2012), can be instrumental to the success of individuals and businesses (Haenlein and Kaplan, 2009), and even affects the stability of political regimes (Howard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sen</context>
</contexts>
<marker>Haenlein, Kaplan, 2009</marker>
<rawString>Michael Haenlein and Andreas M. Kaplan. 2009. Flagship brand stores within virtual worlds: The impact of virtual store exposure on real-life attitude toward the brand and purchase intent. Recherche et Applications en Marketing (English Edition), 24(3):57–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takayuki Hasegawa</author>
<author>Nobuhiro Kaji</author>
<author>Naoki Yoshinaga</author>
<author>Masashi Toyoda</author>
</authors>
<title>Predicting and eliciting addressee’s emotion in online dialogue.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>964--972</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4392" citStr="Hasegawa et al. (2013)" startWordPosition="650" endWordPosition="653">h to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl.com/ ludyrkz;WASSA http://tinyurl.com/kjjdhax. 2There is a general debate on the efficiency of automated tools (Nall, 2013) and whether such tools are desirable in social media (McConnell (2012); responses to Owyang (2012)). 58 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58–67, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics pressed in the past are replicated or reused. A recent study by Hasegawa et al. (2013) modifies Ritter’s approach to produce responses that elicit an emotion from the addressee. Yet, these responses do not target particular topics and are not driven by a user agenda. The present paper addresses the problem of generating novel, subjective, responses to online opinionated articles. We formally define the document-to-response mapping problem and suggest an end-to-end system to solve it. Our system integrates a range of NLP and NLG technologies (including topic models, sentiment analysis, and the integration of a knowledge graph) to design a flexible generation mechanism that allow</context>
<context position="27095" citStr="Hasegawa et al. (2013)" startWordPosition="4381" endWordPosition="4384">0.819 KB X SOURCE −0.032 −0.61 0.731 Table 8: Relevance ratings regression results, comparing systems with and without KB. 4 Related and Future Work In contrast to the vast amount of research on sentiment and topic analysis, as well as generation tasks in which the input is artificial or pre-defined, our system implements a full end-to-end cycle from natural language analysis to natural language generation with applications in social media and automated interaction in real-world settings. The only two other studies on response generation in social media we know of are Ritter et al. (2011) and Hasegawa et al. (2013). Ritter’s and Hasegawa’s approaches differ from ours in their objective and their approach to generation. Specifically, Ritter’s approach is based on machine translation, creating responses by directly re-using previous content. Their data-driven approach generates relevant, but not opinionated responses. In addition, both Ritter’s and Hasegawa’s systems respond to tweets, while our system analyzes and responds to complete articles. Hasegawa’s approach is closer to ours in that it generates responses that are intended to elicit a specific emotion from the addressee. However, it still differs </context>
</contexts>
<marker>Hasegawa, Kaji, Yoshinaga, Toyoda, 2013</marker>
<rawString>Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga, and Masashi Toyoda. 2013. Predicting and eliciting addressee’s emotion in online dialogue. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 964–972, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’99,</booktitle>
<pages>50--57</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11098" citStr="Hofmann, 1999" startWordPosition="1796" endWordPosition="1797">s the analysis phase, where topic inference and sentiment scoring take place, resulting in (topic, sentiment)-pairs. During the subsequent generation phase, these are intersected with the (topic, sentiment)-pairs in the user agenda. This intersection, possibly augmented with a knowledge graph, forms the input for a template-based generation component. Analysis phase For the task of inferring the topics of the document we use topic modeling: a probabilistic generative modeling technique that allows for the discovery of abstract topics over a large body of documents (Papadimitriou et al., 1998; Hofmann, 1999; Blei et al., 2003). Specifically, we use topic modeling based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012). Given a new document and a trained model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation to the topic(s) </context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’99, pages 50–57, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip N Howard</author>
<author>Aiden Duffy</author>
<author>Deen Freelon</author>
<author>Muzammil Hussain</author>
<author>Will Mari</author>
<author>Marwa Mazaid</author>
</authors>
<title>Opening closed regimes: What was the role of social media during the Arab spring?</title>
<date>2011</date>
<booktitle>Project on Information Technology and Political Islam.</booktitle>
<contexts>
<context position="1738" citStr="Howard et al., 2011" startWordPosition="249" endWordPosition="252">uman-like than those that rely on topic, sentiment and agenda only, whereas the use of world knowledge does not affect perceived relevance. 1 Introduction Digital media, user-generated content and social networks enable effective human interaction; so much so that much of our day-to-day interaction is conducted online (Viswanath et al., 2009). Interaction in social media fundamentally changes the way businesses and consumers behave (Qualman, 2012), can be instrumental to the success of individuals and businesses (Haenlein and Kaplan, 2009), and even affects the stability of political regimes (Howard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al.</context>
</contexts>
<marker>Howard, Duffy, Freelon, Hussain, Mari, Mazaid, 2011</marker>
<rawString>Philip N. Howard, Aiden Duffy, Deen Freelon, Muzammil Hussain, Will Mari, and Marwa Mazaid. 2011. Opening closed regimes: What was the role of social media during the Arab spring? Project on Information Technology and Political Islam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
</authors>
<title>A knowledge-based approach to language production.</title>
<date>1985</date>
<tech>Technical report,</tech>
<institution>University of California at Berkeley,</institution>
<location>Berkeley, CA, USA.</location>
<contexts>
<context position="29533" citStr="Jacobs, 1985" startWordPosition="4771" endWordPosition="4772">ctor in such aspects, e.g., the template-based methods in Becker (2002) and Narayan et al. (2011), or grammar based methods, as in DeVault et al. (2008). Using more sophisticated generation methods with a rich grammatical backbone may combat the sensitivity to computer-generated response patterns as acquired by our human raters over time. Furthermore, our result concerning the humanlikeness of gkb clearly demonstrates that semantic knowledge must be brought in to support better, and more human-like, response generation. Largescale knowledge graphs such as Freebase support many semantic tasks (Jacobs, 1985), and can be used for providing richer context for automatically generating human-like responses. From a theoretical viewpoint, the system will clearly benefit from rigorous analysis of human interaction in online media. Responses to usergenerated content on the Internet share some linguistic characteristics in structure, length and manner of expression. Studying these features theoretically and then examining them empirically using a Turing-like evaluation as presented here can take us a big step in the direction of better generation, and also better understanding of the processes underlying </context>
</contexts>
<marker>Jacobs, 1985</marker>
<rawString>Paul S Jacobs. 1985. A knowledge-based approach to language production. Technical report, University of California at Berkeley, Berkeley, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pulkit Kathuria</author>
</authors>
<title>Sentiment Classification using WSD, Maximum Entropy and Naive Bayes Classifiers. https://github.com/kevincobain2000/sentiment classifier. Visited</title>
<date>2012</date>
<contexts>
<context position="11794" citStr="Kathuria (2012)" startWordPosition="1912" endWordPosition="1913">et Allocation (LDA) (Blei et al., 2003; Blei, 2012). Given a new document and a trained model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation to the topic(s) identified in the document. We use the semantic/lexical method as implemented in Kathuria (2012). We rely on a WSD sentiment classifier that uses the SentiWordNet (Baccianella et al., 2010) database and calculates the positivity and negativity scores of a document based on the positivity and negativity of individual words. The result of the sentiment analysis is a pair of values, indicating the positive and negative sentiments of the document-based scores for individual words. We use the larger of these two values as the sentiment value for the whole document.4 Generation phase Our generation function first intersects the set of topics in the document and the set of topics in the agenda </context>
</contexts>
<marker>Kathuria, 2012</marker>
<rawString>Pulkit Kathuria. 2012. Sentiment Classification using WSD, Maximum Entropy and Naive Bayes Classifiers. https://github.com/kevincobain2000/sentiment classifier. Visited March 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wiebke Lamer</author>
</authors>
<title>Twitter and tyrants: New media and its effects on sovereignty in the Middle East. Arab Media and Society.</title>
<date>2012</date>
<contexts>
<context position="1752" citStr="Lamer, 2012" startWordPosition="253" endWordPosition="254">that rely on topic, sentiment and agenda only, whereas the use of world knowledge does not affect perceived relevance. 1 Introduction Digital media, user-generated content and social networks enable effective human interaction; so much so that much of our day-to-day interaction is conducted online (Viswanath et al., 2009). Interaction in social media fundamentally changes the way businesses and consumers behave (Qualman, 2012), can be instrumental to the success of individuals and businesses (Haenlein and Kaplan, 2009), and even affects the stability of political regimes (Howard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opini</context>
</contexts>
<marker>Lamer, 2012</marker>
<rawString>Wiebke Lamer. 2012. Twitter and tyrants: New media and its effects on sovereignty in the Middle East. Arab Media and Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Langheinrich</author>
<author>G¨unter Karjoth</author>
</authors>
<title>Social networking and the risk to companies and institutions. Information Security Technical Report. Special Issue: Identity Reconstruction and Theft,</title>
<date>2011</date>
<pages>51--56</pages>
<contexts>
<context position="1985" citStr="Langheinrich and Karjoth, 2011" startWordPosition="284" endWordPosition="287"> interaction; so much so that much of our day-to-day interaction is conducted online (Viswanath et al., 2009). Interaction in social media fundamentally changes the way businesses and consumers behave (Qualman, 2012), can be instrumental to the success of individuals and businesses (Haenlein and Kaplan, 2009), and even affects the stability of political regimes (Howard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et</context>
</contexts>
<marker>Langheinrich, Karjoth, 2011</marker>
<rawString>Marc Langheinrich and G¨unter Karjoth. 2011. Social networking and the risk to companies and institutions. Information Security Technical Report. Special Issue: Identity Reconstruction and Theft, pages 51–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Mah</author>
</authors>
<title>Tools to automate your customer service response on social media. http://www.itbusinessedge.com/blogs/smbtech/tools-to-automate-your-customer-serviceresponse-on-social-media.html. Visited</title>
<date>2012</date>
<contexts>
<context position="3294" citStr="Mah, 2012" startWordPosition="486" endWordPosition="487">, 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl.com/ ludyrk</context>
</contexts>
<marker>Mah, 2012</marker>
<rawString>Paul Mah. 2012. Tools to automate your customer service response on social media. http://www.itbusinessedge.com/blogs/smbtech/tools-to-automate-your-customer-serviceresponse-on-social-media.html. Visited August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris McConnell</author>
</authors>
<date>2012</date>
<booktitle>When brands automate Twitter and Facebook responses I’ll revolt. http://dailytekk.com/2012/06/07/brandsautomating-social-media/. Visited</booktitle>
<contexts>
<context position="4077" citStr="McConnell (2012)" startWordPosition="603" endWordPosition="604">ri et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl.com/ ludyrkz;WASSA http://tinyurl.com/kjjdhax. 2There is a general debate on the efficiency of automated tools (Nall, 2013) and whether such tools are desirable in social media (McConnell (2012); responses to Owyang (2012)). 58 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58–67, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics pressed in the past are replicated or reused. A recent study by Hasegawa et al. (2013) modifies Ritter’s approach to produce responses that elicit an emotion from the addressee. Yet, these responses do not target particular topics and are not driven by a user agenda. The present paper addresses the problem of generating novel, subjective, responses to online opinionate</context>
</contexts>
<marker>McConnell, 2012</marker>
<rawString>Chris McConnell. 2012. When brands automate Twitter and Facebook responses I’ll revolt. http://dailytekk.com/2012/06/07/brandsautomating-social-media/. Visited August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilad Mishne</author>
</authors>
<title>Multiple ranking strategies for opinion retrieval in blogs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th Text Retrieval Conference.</booktitle>
<contexts>
<context position="2376" citStr="Mishne, 2006" startWordPosition="342" endWordPosition="343">ts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolingui</context>
</contexts>
<marker>Mishne, 2006</marker>
<rawString>Gilad Mishne. 2006. Multiple ranking strategies for opinion retrieval in blogs. In Proceedings of the 15th Text Retrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyoshi Mori</author>
<author>Adam Jatowt</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Enhancing conversational flexibility in multimodal interactions with embodied lifelike agent.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Conference on Intelligent User Interfaces, IUI ’03,</booktitle>
<pages>270--272</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3476" citStr="Mori et al., 2003" startWordPosition="512" endWordPosition="515">escu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl.com/ ludyrkz;WASSA http://tinyurl.com/kjjdhax. 2There is a general debate on the efficiency of automated tools (Nall, 2013) and whether such tools are desirable in social media (McConnell (2012</context>
</contexts>
<marker>Mori, Jatowt, Ishizuka, 2003</marker>
<rawString>Kyoshi Mori, Adam Jatowt, and Mitsuru Ishizuka. 2003. Enhancing conversational flexibility in multimodal interactions with embodied lifelike agent. In Proceedings of the 8th International Conference on Intelligent User Interfaces, IUI ’03, pages 270–272, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mickey Nall</author>
</authors>
<title>You can’t automate social media engagement, argues PRSA’s Mickey Nall.</title>
<date>2013</date>
<location>http://www.prmoment.com/1359/you-cantautomate-social-media-engagement-argues-prsasmickey-nall.aspx. Visited</location>
<contexts>
<context position="4006" citStr="Nall, 2013" startWordPosition="591" endWordPosition="592">pment of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl.com/ ludyrkz;WASSA http://tinyurl.com/kjjdhax. 2There is a general debate on the efficiency of automated tools (Nall, 2013) and whether such tools are desirable in social media (McConnell (2012); responses to Owyang (2012)). 58 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58–67, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics pressed in the past are replicated or reused. A recent study by Hasegawa et al. (2013) modifies Ritter’s approach to produce responses that elicit an emotion from the addressee. Yet, these responses do not target particular topics and are not driven by a user agenda. The present paper addresses the </context>
</contexts>
<marker>Nall, 2013</marker>
<rawString>Mickey Nall. 2013. You can’t automate social media engagement, argues PRSA’s Mickey Nall. http://www.prmoment.com/1359/you-cantautomate-social-media-engagement-argues-prsasmickey-nall.aspx. Visited August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Sankaran Narayan</author>
<author>Charles Lee Isbell Jr</author>
<author>David L Roberts</author>
</authors>
<title>Dextor: Reduced effort authoring for template-based natural language generation.</title>
<date>2011</date>
<booktitle>In Vadim Bulitko and</booktitle>
<editor>Mark O. Riedl, editors,</editor>
<publisher>The AAAI Press.</publisher>
<contexts>
<context position="29017" citStr="Narayan et al. (2011)" startWordPosition="4692" endWordPosition="4695">, it would be interesting to test whether a novel mechanism for joint inference of topic/sentiment distributions could lead to improvement in the human-likeness of the generated responses. The syntactic and semantic means of expression that we use are based on bare bone templates and fine-grained POS tags (Theune et al., 2001). These may potentially be expanded with different ways to express subject/object relations, relations between phrases, polarity of sentences, and so on. Additional approaches to generation can factor in such aspects, e.g., the template-based methods in Becker (2002) and Narayan et al. (2011), or grammar based methods, as in DeVault et al. (2008). Using more sophisticated generation methods with a rich grammatical backbone may combat the sensitivity to computer-generated response patterns as acquired by our human raters over time. Furthermore, our result concerning the humanlikeness of gkb clearly demonstrates that semantic knowledge must be brought in to support better, and more human-like, response generation. Largescale knowledge graphs such as Freebase support many semantic tasks (Jacobs, 1985), and can be used for providing richer context for automatically generating human-li</context>
</contexts>
<marker>Narayan, Jr, Roberts, 2011</marker>
<rawString>Karthik Sankaran Narayan, Charles Lee Isbell Jr., and David L. Roberts. 2011. Dextor: Reduced effort authoring for template-based natural language generation. In Vadim Bulitko and Mark O. Riedl, editors, Proceedings of the Seventh Artificial Intelligence and Interactive Digital Entertainment Conference. The AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<editor>In William W. Cohen and Samuel Gosling, editors, ICWSM.</editor>
<publisher>The AAAI Press.</publisher>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In William W. Cohen and Samuel Gosling, editors, ICWSM. The AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Brandon M Stewart</author>
<author>Noah A Smith</author>
</authors>
<title>Learning to extract international relations from political context.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1094--1104</pages>
<publisher>The Association</publisher>
<institution>for Computer Linguistics.</institution>
<marker>O’Connor, Stewart, Smith, 2013</marker>
<rawString>Brendan O’Connor, Brandon M. Stewart, and Noah A. Smith. 2013. Learning to extract international relations from political context. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1094–1104. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremiah Owyang</author>
</authors>
<title>Brands Start Automating Social Media Responses</title>
<date>2012</date>
<booktitle>on Facebook and Twitter. http://techcrunch.com/2012/06/07/brands-startautomating-social-media-responses-on-facebookand-twitter/. Visited</booktitle>
<contexts>
<context position="3282" citStr="Owyang, 2012" startWordPosition="484" endWordPosition="485">a (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl</context>
</contexts>
<marker>Owyang, 2012</marker>
<rawString>Jeremiah Owyang. 2012. Brands Start Automating Social Media Responses on Facebook and Twitter. http://techcrunch.com/2012/06/07/brands-startautomating-social-media-responses-on-facebookand-twitter/. Visited August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos H Papadimitriou</author>
<author>Hisao Tamaki</author>
<author>Prabhakar Raghavan</author>
<author>Santosh Vempala</author>
</authors>
<title>Latent Semantic Indexing: A probabilistic analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, PODS ’98,</booktitle>
<pages>159--168</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11083" citStr="Papadimitriou et al., 1998" startWordPosition="1792" endWordPosition="1795">a nutshell, a document enters the analysis phase, where topic inference and sentiment scoring take place, resulting in (topic, sentiment)-pairs. During the subsequent generation phase, these are intersected with the (topic, sentiment)-pairs in the user agenda. This intersection, possibly augmented with a knowledge graph, forms the input for a template-based generation component. Analysis phase For the task of inferring the topics of the document we use topic modeling: a probabilistic generative modeling technique that allows for the discovery of abstract topics over a large body of documents (Papadimitriou et al., 1998; Hofmann, 1999; Blei et al., 2003). Specifically, we use topic modeling based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012). Given a new document and a trained model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation t</context>
</contexts>
<marker>Papadimitriou, Tamaki, Raghavan, Vempala, 1998</marker>
<rawString>Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. 1998. Latent Semantic Indexing: A probabilistic analysis. In Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, PODS ’98, pages 159– 168, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Qualman</author>
</authors>
<title>Socialnomics: How social media transforms the way we live and do business.</title>
<date>2012</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>Hoboken, NJ, USA,</location>
<note>2nd edition.</note>
<contexts>
<context position="1570" citStr="Qualman, 2012" startWordPosition="222" endWordPosition="224"> quantifying the humanlikeness and relevance of the generated responses. We show that responses generated using world knowledge in the input are regarded as more human-like than those that rely on topic, sentiment and agenda only, whereas the use of world knowledge does not affect perceived relevance. 1 Introduction Digital media, user-generated content and social networks enable effective human interaction; so much so that much of our day-to-day interaction is conducted online (Viswanath et al., 2009). Interaction in social media fundamentally changes the way businesses and consumers behave (Qualman, 2012), can be instrumental to the success of individuals and businesses (Haenlein and Kaplan, 2009), and even affects the stability of political regimes (Howard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive research and commercial opportunities. In natural language processing, there is ample research on the analy</context>
</contexts>
<marker>Qualman, 2012</marker>
<rawString>Erik Qualman. 2012. Socialnomics: How social media transforms the way we live and do business. John Wiley &amp; Sons, Hoboken, NJ, USA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim Rehurek</author>
<author>Petr Sojka</author>
</authors>
<title>Software framework for topic modelling with large corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<publisher>ELRA.</publisher>
<location>Valletta, Malta,</location>
<contexts>
<context position="11553" citStr="Rehurek and Sojka, 2010" startWordPosition="1869" endWordPosition="1872"> a probabilistic generative modeling technique that allows for the discovery of abstract topics over a large body of documents (Papadimitriou et al., 1998; Hofmann, 1999; Blei et al., 2003). Specifically, we use topic modeling based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Blei, 2012). Given a new document and a trained model, the inference method provides a weighted mix of topics for that document, where each topic is represented as a vector containing keywords associated with probabilities. For training the topic model and inferring the topics in new documents we use Gensim (Rehurek and Sojka, 2010), a fast and easy-to-use implementation of LDA. Next, we wish to infer the sentiment that is expressed in the text with relation to the topic(s) identified in the document. We use the semantic/lexical method as implemented in Kathuria (2012). We rely on a WSD sentiment classifier that uses the SentiWordNet (Baccianella et al., 2010) database and calculates the positivity and negativity scores of a document based on the positivity and negativity of individual words. The result of the sentiment analysis is a pair of values, indicating the positive and negative sentiments of the document-based sc</context>
</contexts>
<marker>Rehurek, Sojka, 2010</marker>
<rawString>Radim Rehurek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="7781" citStr="Reiter and Dale (1997)" startWordPosition="1202" endWordPosition="1205">elation): Responses directly address the documents’ content. • Opinionated (Maxim of Quality): Responses express responders beliefs, sentiments, or dispositions towards the topic(s). 2.2 The Formal Model Let D be a set of documents and let A be a set of user agendas as we define shortly. Let S be a set of English sentences over a finite vocabulary S = E∗. Our system implements a function that maps each (document, agenda) pair to a natural language response sentence s E S. fresponse : D x A —* S Response generation takes place in two phases, roughly corresponding to macro and micro planning in Reiter and Dale (1997): • Macro Planning (below, the analysis phase): What are we going to talk about? • Micro Planning (below, the generation phase): How are we going to say it? The analysis function p : D —* C maps a document to a subjective representation of its content.3 The generation function g : C x A —* S intersects the content elements in the document and in the user agenda, and generates a response based on the content of the intersection. All in all, our system implements a composition of the analysis and the generation functions: fresponse(d, a) = g(p(d), a) = s 3A content element may conceivably encomp</context>
<context position="13273" citStr="Reiter and Dale (1997)" startWordPosition="2156" endWordPosition="2159">entation we focus on the single most prevalent, topic. We pick the highest scoring word of the highest scoring topic, and intersect it with topics in the agenda. The system generates a response based on the identified 4Clearly, this is a simplifying assumption. We discuss this assumption further in Section 4. 60 Figure 1: The system architecture from a bird’s eye view. Components on gray background are executed offline. topic, the sentiment for the topic in the document, and the sentiment for that topic in the user agenda. The generation component relies on a templatebased approach similar to Reiter and Dale (1997) and Van Deemter et al. (2005). Templates are essentially subtrees with leaves that are placeholders for other templates or for functions generating referring expressions (Theune et al., 2001). These functions receive (relevant parts of) the input and emit the sequence of fine-grained part-ofspeech (POS) tags that realizes the relevant referring expression. The POS tags in the resulting sequences are ultimately place holders for words from a lexicon E. In order to generate a variety of expression forms — nouns, adjectives and verbs — these items are selected randomly from a finegrained lexicon</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>Ehud Reiter and Robert Dale. 1997. Building applied natural language generation systems. Nat. Lang. Eng., 3(1):57–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>William B Dolan</author>
</authors>
<title>Data-driven response generation in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>583--593</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3640" citStr="Ritter et al. (2011)" startWordPosition="538" endWordPosition="541">enerating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 2012).2 Research on user interfaces is trying to move away from scriptbased interaction towards the development of chat bots that attempt natural human-like interaction (Mori et al., 2003; Feng et al., 2006). However, these chat bots are typically designed to provide an automated one-size-fits-all type of interaction. A study by Ritter et al. (2011) addresses the generation of responses to natural language tweets in a data-driven setup. It applies a machine-translation approach to response generation, where moods and sentiments already exx&apos;E.g., 1E.g., the ACL series LASM http://tinyurl.com/ ludyrkz;WASSA http://tinyurl.com/kjjdhax. 2There is a general debate on the efficiency of automated tools (Nall, 2013) and whether such tools are desirable in social media (McConnell (2012); responses to Owyang (2012)). 58 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 58–67, Baltimore, Maryland US</context>
<context position="17557" citStr="Ritter et al. (2011)" startWordPosition="2841" endWordPosition="2844">ple response for each such combination. In addition, we randomly collected 5 to 10 real, short or mediumlength, online human responses for each article. 3.2 Surveys We collected evaluation data via two online surveys on Amazon Mechanical Turk (www. mturk.com). In Survey 1, participants judged whether responses to articles were written by human or computer, akin to (a simplified version of) the Turing test (Turing, 1950). In Survey 2, responses were rated on their relevance to the article, in effect testing whether they abide by the Gricean Maxim of Relation. This is comparable to the study by Ritter et al. (2011) where people judged which of two responses was ‘best’. Each survey comprises 10 randomly ordered trials, corresponding to the 10 selected articles. First, the participant was presented with a snippet from the article. When clicking a button, the text was removed and its presentation duration recorded. Next, a multiple-choice question asked about the snippet’s topic. Data on a trial was discarded from analysis if the participant answered incorrectly or if the snippet was presented for less than 10 msec per character; we took these to be cases where the snippet was not properly read. Next, the </context>
<context position="27068" citStr="Ritter et al. (2011)" startWordPosition="4376" endWordPosition="4379">9 abs(SENT) −0.017 −0.93 0.819 KB X SOURCE −0.032 −0.61 0.731 Table 8: Relevance ratings regression results, comparing systems with and without KB. 4 Related and Future Work In contrast to the vast amount of research on sentiment and topic analysis, as well as generation tasks in which the input is artificial or pre-defined, our system implements a full end-to-end cycle from natural language analysis to natural language generation with applications in social media and automated interaction in real-world settings. The only two other studies on response generation in social media we know of are Ritter et al. (2011) and Hasegawa et al. (2013). Ritter’s and Hasegawa’s approaches differ from ours in their objective and their approach to generation. Specifically, Ritter’s approach is based on machine translation, creating responses by directly re-using previous content. Their data-driven approach generates relevant, but not opinionated responses. In addition, both Ritter’s and Hasegawa’s systems respond to tweets, while our system analyzes and responds to complete articles. Hasegawa’s approach is closer to ours in that it generates responses that are intended to elicit a specific emotion from the addressee.</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2011</marker>
<rawString>Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in social media. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 583–593, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Theune</author>
<author>E Klabbers</author>
<author>J R De Pijper</author>
<author>E Krahmer</author>
<author>J Odijk</author>
</authors>
<title>From data to speech: A general approach.</title>
<date>2001</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>7</volume>
<issue>1</issue>
<marker>Theune, Klabbers, De Pijper, Krahmer, Odijk, 2001</marker>
<rawString>M. Theune, E. Klabbers, J. R. De Pijper, E. Krahmer, and J. Odijk. 2001. From data to speech: A general approach. Nat. Lang. Eng., 7(1):47–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan M Turing</author>
</authors>
<title>Computing machinery and intelligence.</title>
<date>1950</date>
<location>Mind, LIX:433–460.</location>
<contexts>
<context position="17360" citStr="Turing, 1950" startWordPosition="2806" endWordPosition="2807">sed lexicon. 1000 responses for each, comprising 100 unique responses for each combination of sentiments and system variant (i.e., with or without a knowledge base). Table 2 presents an example response for each such combination. In addition, we randomly collected 5 to 10 real, short or mediumlength, online human responses for each article. 3.2 Surveys We collected evaluation data via two online surveys on Amazon Mechanical Turk (www. mturk.com). In Survey 1, participants judged whether responses to articles were written by human or computer, akin to (a simplified version of) the Turing test (Turing, 1950). In Survey 2, responses were rated on their relevance to the article, in effect testing whether they abide by the Gricean Maxim of Relation. This is comparable to the study by Ritter et al. (2011) where people judged which of two responses was ‘best’. Each survey comprises 10 randomly ordered trials, corresponding to the 10 selected articles. First, the participant was presented with a snippet from the article. When clicking a button, the text was removed and its presentation duration recorded. Next, a multiple-choice question asked about the snippet’s topic. Data on a trial was discarded fro</context>
</contexts>
<marker>Turing, 1950</marker>
<rawString>Alan M. Turing. 1950. Computing machinery and intelligence. Mind, LIX:433–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees Van Deemter</author>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>Real versus template-based natural language generation: A false opposition?</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Van Deemter, Krahmer, Theune, 2005</marker>
<rawString>Kees Van Deemter, Emiel Krahmer, and Mari¨et Theune. 2005. Real versus template-based natural language generation: A false opposition? Comput. Linguist., 31(1):15–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bimal Viswanath</author>
<author>Alan Mislove</author>
<author>Meeyoung Cha</author>
<author>Krishna P Gummadi</author>
</authors>
<title>On the evolution of user interaction in Facebook.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2nd ACM Workshop on Online Social Networks, WOSN ’09,</booktitle>
<pages>37--42</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1463" citStr="Viswanath et al., 2009" startWordPosition="205" endWordPosition="208">on users’ agenda, documents’ topics, sentiments and a knowledge graph. We present an empirical evaluation method for quantifying the humanlikeness and relevance of the generated responses. We show that responses generated using world knowledge in the input are regarded as more human-like than those that rely on topic, sentiment and agenda only, whereas the use of world knowledge does not affect perceived relevance. 1 Introduction Digital media, user-generated content and social networks enable effective human interaction; so much so that much of our day-to-day interaction is conducted online (Viswanath et al., 2009). Interaction in social media fundamentally changes the way businesses and consumers behave (Qualman, 2012), can be instrumental to the success of individuals and businesses (Haenlein and Kaplan, 2009), and even affects the stability of political regimes (Howard et al., 2011; Lamer, 2012). These facts force organizations (businesses, governments, and non-profit organizations) to be constantly involved in the monitoring of, and the interaction with, human agents in digital environments (Langheinrich and Karjoth, 2011). Automatic analysis of user-generated online content benefits from extensive </context>
</contexts>
<marker>Viswanath, Mislove, Cha, Gummadi, 2009</marker>
<rawString>Bimal Viswanath, Alan Mislove, Meeyoung Cha, and Krishna P. Gummadi. 2009. On the evolution of user interaction in Facebook. In Proceedings of the 2nd ACM Workshop on Online Social Networks, WOSN ’09, pages 37–42, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>William W Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting response to political blog posts with topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>477--485</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2691" citStr="Yano et al., 2009" startWordPosition="395" endWordPosition="398">nd commercial opportunities. In natural language processing, there is ample research on the analysis of subjectivity and sentiment of content in social media. The development of tools for sentiment analysis (Davidov et al., 2010), mood aggregation (Agichtein et al., 2008), opinion mining (Mishne, 2006), and many more, now enjoys wide interest and exposure, as is also evident by the many workshops and dedicated tracks at ACL venues.1 Methods are also developed for the analysis of political texts (O’Connor et al., 2010; O’Connor et al., 2013) and for text-driven forecasting based on these data (Yano et al., 2009). A related strand of research uses computational methods to find out what kind of published utterances are influential, and how they affect linguistic communities (Danescu-Niculescu-Mizil et al., 2009). Such work complements, and contributes to, studies from sociology and sociolinguistics that aim to delineate the process of generating meaningful responses (e.g., Amabile (1981)). In contrast to these analysis efforts, the topic of generating responses to content in social media is only sparsely explored. Commercially, there is movement towards online response automation (Owyang, 2012; Mah, 20</context>
</contexts>
<marker>Yano, Cohen, Smith, 2009</marker>
<rawString>Tae Yano, William W. Cohen, and Noah A. Smith. 2009. Predicting response to political blog posts with topic models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 477–485, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
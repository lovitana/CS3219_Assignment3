<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000034">
<title confidence="0.996073">
Dialogue Act Modeling for Non-Visual Web Access
</title>
<author confidence="0.957022">
Vikas Ashok Yevgen Borodin Svetlana Stoyanchev I V Ramakrishnan
</author>
<affiliation confidence="0.846526333333333">
Dept of Computer Science Charmtech Labs LLC AT&amp;T Labs Research Charmtech Labs LLC
Stony Brook University CEWIT SBU R &amp; D Park New York City, New York CEWIT SBU R &amp; D Park
Stony Brook, New York Stony Brook, New York (While at Columbia University) Stony Brook, New York
</affiliation>
<email confidence="0.9791945">
vganjiguntea@cs.sunysb.edu, borodin@charmtechlabs.com,
sstoyanchev@cs.columbia.edu, ram@charmtechlabs.com
</email>
<sectionHeader confidence="0.993817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999761578947369">
Speech-enabled dialogue systems have the
potential to enhance the ease with which
blind individuals can interact with the Web
beyond what is possible with screen read-
ers - the currently available assistive tech-
nology which narrates the textual content
on the screen and provides shortcuts to
navigate the content. In this paper, we
present a dialogue act model towards de-
veloping a speech enabled browsing sys-
tem. The model is based on the corpus
data that was collected in a wizard-of-oz
study with 24 blind individuals who were
assigned a gamut of browsing tasks. The
development of the model included exten-
sive experiments with assorted feature sets
and classifiers; the outcomes of the exper-
iments and the analysis of the results are
presented.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977733333334">
The Web is the “go-to” computing infrastructure
for participating in our fast-paced digital society.
It has the potential to provide an even greater ben-
efit to blind people who once required human as-
sistance with many of their activities. According
to the American Federation for the Blind, there
are 21.5 million Americans who have vision loss,
of whom 1.5 million are computer users (AFB,
2013).
Blind users employ screen readers as the as-
sistive technology to interact with digital con-
tent (e.g.., JAWS (Freedom-Scientific, 2014) and
VoiceOver (Apple-Inc., 2013)). Screen readers se-
rially narrate the content of the screen using text-
to-speech engines and enable users to navigate in
the content using keyboard shortcuts and touch-
screen gestures.
Navigating content-rich web pages and con-
ducting online transactions spanning multiple
pages requires using shortcuts and this can get
quite cumbersome and tedious. Specifically, in
online shopping a user typically browses through
product categories, searches for products, adds
products to cart, logs into his/her account, and fi-
nally makes a payment. All these steps require
screen-reader users listen through a lot of content,
fill forms, and find links and buttons that have to be
selected to get through these steps. If users do not
want to go through all content on the page, they
have to remember and use a number of different
shortcuts. Beginner users often use the “Down”
key to go through the page line by line, listening
to all content on the way (Borodin et al., 2010).
Now suppose that blind users were to tell the
web browser what they wanted to accomplish and
let the browsing application automatically deter-
mine what has to be clicked, fill out forms, help
find products, answer questions, breeze through
checkout, and wherever possible, relieve the user
from doing all the mundane and tedious low-level
operations such as clicking, typing, etc. The abil-
ity to carry out a dialogue with the web browser at
a higher level has the potential to overcome the
limitations of shortcut-based screen reading and
thus offers a richer and more productive user ex-
perience for blind people.
The first step toward building a dialogue-based
system is the understanding of what users could
say and dialogue act modeling. Although di-
alogue act modeling is a well-researched topic
(with details provided in related work - Section
2), it has remained unexplored in the context of
web accessibility for blind people. The commer-
cial speech-based applications have been around
for a while and new ones continue to emerge at a
rapid pace; however, these are mainly stand-alone
(e.g.., Apple’s Siri) domain specific systems that
are not connected to web browsers, which pre-
cludes dialogue-based interaction with the Web.
Current spoken input modules integrated with web
</bodyText>
<page confidence="0.985505">
123
</page>
<note confidence="0.731591">
Proceedings of the SIGDIAL 2014 Conference, pages 123–132,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997913772727273">
browsers are limited to certain specific functional-
ities such as search (e.g.., Google’s voice search)
or are used as a measure of last resort (e.g.., Siri
searching for terms online).
In this paper, we made a principal step towards
building a dialogue-based assistive web browsing
system for blind people; specifically, we built a
dialogue act model for non-visual access to the
Web. The contributions of this paper include:
1) a unique dialogue corpus for non-visual web ac-
cess, collected during the wizard-of-oz user study
conducted with 24 blind participants (Section 3);
2) the design of a suitable dialogue act scheme
(Section 3); 3) experimentation with classifiers ca-
pable of identifying the dialogue acts associated
with utterances based on combinations of lexi-
cal/syntactic, contextual, and task-related feature
sets (Section 4); 4) investigation of the impor-
tance of each feature set with respect to classifi-
cation performance to assess whether simple lex-
ical/syntactic features are sufficient for obtaining
an acceptable performance (Section 5).
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99707841025641">
While previous research addressed spoken dia-
logue interfaces for a domain-specific websites,
such as news or movie search (Ferreras and
Carde˜noso-Payo, 2005; Wang et al., 2014), dia-
logue interface to generic web sites is a novel task.
Spoken dialogue systems (SDS) can be classified
by the type of initiative: system, user, or mixed
initiative (Lee et al., 2010). In a system-initiative
SDS, a system guides a user through a series of
information gathering and information presenting
prompts. In a user-initiative system, a user can
initiate and steer the interaction. Mixed-initiative
systems allow both system and user-initiated ac-
tions.
Dialogue systems also differ in the types of di-
alogue manager: finite state based, form based,
or agent based (Lee et al., 2010), (Chotimongkol,
2008). Finite state and form filling systems are
usually system-initiative. These systems have a
fixed set of dialogue states and finite set of possi-
ble user commands that map to system actions. In
contrast, a speech-enabled browsing system pro-
posed in this work is an agent-based system. The
set of actions of this system correspond to user ac-
tions during web browsing. The domain of possi-
ble user commands at each point of the dialogue
depends on the current web page that is viewed by
a user. The dialogue state in a voice browsing sys-
tem is compiled at run-time as the user can visit
any web page.
While a users dialogue acts in a form-based
or finite state system depends primarily on a di-
alogue state, in an agent-based system with user-
initiative, the space of users dialogue acts at each
dialogue state is open. To determine dialogue
manager action, it is essential for the system to
identify users intent or dialogue act. In this
work, we address dialogue act modelling for open-
domain voice web browsing as a proof of concept
for the system.
Dialogue act (DA) annotation schemes for spo-
ken dialogue systems follow theories on speech
acts originally developed by Searle (1975). A
number of DA annotation schemes have been de-
veloped previously (Core and Allen, 1997), (Car-
letta et al., 1997). Several of dialogue tagging
schemes strive to provide domain-independence
(Core and Allen, 1997), (Bunt, 2011).
Bunt (2011) developed a NIST standardized
domain-independent annotation scheme which in-
corporates elements from the previously devel-
oped annotation schemes. It is a hierarchical
multi-dimensional annotation scheme. Each func-
tional segment (part of an utterance correspond-
ing to a DA) can have a general purpose function,
such as Inform, Propositional Question, Yes/No
Question, and a dimension-specific function in any
number of 10 defined dimensions, such as Task,
Feedback, or Time management.
In the analysis of human-computer dialogues, it
is common to adopt DA annotation schemes to suit
specific domains. Generic domain-independent
schemes are geared towards the analysis of nat-
ural human-human dialogue and provide rich an-
notation structure that can cover complexity of
natural dialogue. Domain-specific dialogues use
a subset of the generic dialogue structure. For
example, Ohtake et al. (2009) developed a DA
scheme for tourist-guide domain motivated by a
generic annotation scheme (Ohtake et al., 2010),
and Bangalore and Stent (2009) created a dialogue
scheme for a catalogue product ordering dialogue
system. In our work we design DA scheme for
Web-Browsing domain motivated by the DAMSL
(Core and Allen, 1997) schema for task-oriented
dialogue.
We used a Wizard-of-Oz (WOZ) approach to
collect an initial dataset of spoken voice com-
</bodyText>
<page confidence="0.997845">
124
</page>
<table confidence="0.99874025">
Task τu τd
Shopping 121 16
Email 92 16
Flight 180 16
Hotel 179 16
Job 76 16
Admission 144 16
Overall 792 96
</table>
<tableCaption confidence="0.9451405">
Table 1: Corpus details. Tu - number of utterances,
Td - number of dialogs.
</tableCaption>
<bodyText confidence="0.911453941176471">
mands by both blind and sighted users. WOZ is
commonly used before building a dialogue system
(Chotimongkol, 2008), (Ohtake et al., 2009), (Es-
kenazi et al., 1999).
In previous work on dialogue modelling, Stol-
cke et al. (2000) used HMM approach to predict
dialogue acts in a switchboard human-human di-
alogue corpus achieving 65% accuracy. Rangara-
jan Sridhar et al. (2009) applied a maximum en-
tropy classifier on the Switchbord corpus. Using
a combination of lexical, syntactic, and prosodic
features, the authors achieve accuracy of 72%
on that corpus. Following the work of Rangara-
jan Sridhar et al. (2009), we use supervised classi-
fication approach to determine dialogue act on the
annotated corpus of human-wizard web-browsing
dialogues.
</bodyText>
<sectionHeader confidence="0.962144" genericHeader="method">
3 Corpus and Annotation
</sectionHeader>
<bodyText confidence="0.999978512195122">
In this section, we describe the corpus and the
associated dialogue act scheme. The corpus was
collected using a WOZ user study with 24 blind
participants. Exactly 50% of the participants indi-
cated that they were very comfortable with screen
readers, while the remaining 50% said they were
not comfortable with computers. We will refer to
them as “experts” and “beginners” respectively.
The study required each participant to complete
a set of typical web browsing tasks (shopping,
sending an email, booking a flight, reserving a ho-
tel room, searching for a job and applying for uni-
versity admission) using unrestricted speech com-
mands ranging from simple commands such as
“click the search button”, to complex commands
such as “buy this product”. Unknown to the partic-
ipants, these commands were executed by a wiz-
ard and appropriate responses were narrated using
a screen reader. The dialogs were effective; al-
most every participant was able to complete each
assigned task by engaging in a dialogue with the
wizarded interface.
As shown in Table 1, the corpus consists of a
total of 96 dialogs collected during the execution
of 6 tasks and captures approximately 22 hours of
speech with a total of 792 user utterances and 774
system utterances. There is exactly 1 dialogue per
task for any given participant. Each user turn con-
sists of a single command that is usually a sim-
ple sentence or phrase. Each system turn is either
narration of webpage content or information re-
quest for the purpose of either form filling or dis-
ambiguation. Therefore, each dialogue turn was
treated as a single utterance and every utterance
was identified with a single associated dialogue
act.
The corpus was manually annotated with dia-
logue act labels and the labeling scheme was ver-
ified by measuring the inter-annotator agreement.
The rest of this section describes the annotation
scheme.
</bodyText>
<subsectionHeader confidence="0.997865">
3.1 Dialogue Act Annotation
</subsectionHeader>
<bodyText confidence="0.999929766666667">
The dialogue act annotation scheme was inspired
by the DAMSL scheme (Core and Allen, 1997)
for task oriented dialogue. The proposed scheme
was also influenced by extended DAMSL tagset
(Stolcke et al., 2000) and the DIT++ annotation
scheme (Bunt, 2011). We customized the annota-
tion scheme to suit the non-visual web access do-
main, thereby making it more relevant to our cor-
pus and tasks.
Table 2 lists the dialogue acts for both user
and system utterances. The user dialogue act
tagset consists of labels representing task related
requests (Command-Intention, Command-Task,
Command-Multiple, Command-Navigation), in-
quiries (Question-Task, Help-Task) and informa-
tion input (Information-Task), whereas the system
DA tagset contains labels representing informa-
tion requests (Prompt), answers to user inquiries
(Question-Answer, Help-Response) and other sys-
tem responses (Short-Response, Long-Response,
etc.) to user commands.
Inter-rater agreement values for different tasks
in the corpus are presented in Table 3. The n val-
ues for all tasks are above 0.80, which according
to Fleiss’ guidelines (Fleiss, 1973), indicates ex-
cellent inter-rater reliability on the DA annotation.
Therefore, the DA tagset is generic enough to be
applicable for a wide varity of tasks that can be
performed on the web. Note that the dialogue act
scheme was specially designed for non-visual web
</bodyText>
<page confidence="0.995917">
125
</page>
<table confidence="0.977003777777778">
User dialogue Acts
Dialogue Act Description Frequency
Command-Intention Indication of user’s intention or end goal, e.g. I wish to buy a Bluetooth speaker 0.117
Command-Task Basic action commands like click, select, enter, etc. 0.072
Command-Multiple Complex commands requiring an execution plan comprising a sequence of basic 0.162
commands, e.g. buy this product, book this room, etc.
Command-Navigation Commands directing the movement of cursor like go to, stop, next etc. 0.136
Information-Task Information required for completing a task, e.g. departure date/return date in- 0.442
formation for flight booking task, first name, phone number, etc.
Question-Task Task specific questions like What is the cheapest flight?, What is the basic 0.041
salary?, etc.
Self-Talk Utterances not directed towards the system, e.g. hmmm, what should I do next? 0.002
Help-Task Request for help when the user wishes to speak with the experimenter, e.g. Help, 0.024
what does that mean?
System dialogue Acts
dialogue Act Description Frequency
Prompt Request for information from user to complete a task, e.g. First Name, text box 0.460
blank
Short-Response A short response to a user command, e.g. description of product, brief details of 0.198
flight, acknowledgements, etc.
Long-Response A lengthy response to a user command, e.g. Narration of entire page, list of 0.120
search results, etc.
Keyboard-Response Response to user keyboard actions 0.072
Article-Response Narration of an article 0.034
Question-Answer Response to a user question regarding task (non-help) 0.044
No-Response No response for some navigation commands like Stop 0.041
Help-Response Response to a help request from the user 0.026
</table>
<tableCaption confidence="0.992712">
Table 2: dialogue acts for non-visual Web access
</tableCaption>
<bodyText confidence="0.999541166666667">
access. Insofar as sighted people are concerned,
a more elaborate scheme would be required since
their utterances are dominated by visual cues, a
fact that was confirmed by a parallel user study
with sighted participants on the same set of web
tasks that were used in the wizard-of-oz study.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.956565580645161">
This section describes the different feature sets
that we experimented with for our classification
tasks. The vector representation for training the
DA classifiers integrates several types of features
(Table 4): unigrams (U) and syntactic features
(S), context related features (C), task related fea-
tures (T), presence of words anywhere in an
utterance(P) and presence of words at the begin-
ning of an utterance(13). The last two feature sets
are similar to the ones used in Boyer et al. (2010).
Task κ
Shopping 0.865
Email 0.829
Flight 0.894
Hotel 0.848
Job 0.824
Admission 0.800
Table 3: Inter-rater agreement measured in terms
of Cohen’s κ for all tasks in the corpus.
The feature sets C, P, 13 and S are specific to
the domain of non-visual web access and were
hand-crafted based on the following three factors:
knowledge of the browsing behavior of blind users
reported in previous studies, e.g. (Borodin et al.,
2010); manual analysis of the corpus; mitigate the
effect of noise that is usually present in standard
lexical/syntactic feature sets such as n-grams and
parse tree rules. Each of the features in C, P, 13
and S were crafted to have a close correspondence
to some dialogue act. For example, p,,,a,,, is closely
tied to the Command-Navigation dialogue act.
</bodyText>
<subsectionHeader confidence="0.969706">
4.1 Unigrams
</subsectionHeader>
<bodyText confidence="0.999689285714286">
Unigrams (U in Table 4) are one of the commonly
used lexical features for training dialogue act clas-
sifiers (e.g. (Boyer et al., 2010), (Stolcke et al.,
2000), (Rangarajan Sridhar et al., 2009)). Encod-
ing unigrams as features is based on the obser-
vation that some words appear more frequently
in certain dialogue acts compared to other di-
alogue acts. For example, approximately 73%
of “want” occur in the Command-Intention DA,
100% of “skip” occur in the Command-Navigation
DA and approximately 92% of “select” occur
in the Command-Task DA. Word-DA corrections
can also be automatically identified using SVM
classifers trained on unigram features. Table 5
</bodyText>
<page confidence="0.992181">
126
</page>
<table confidence="0.991025">
Overall Feature Set
UNIGRAMS (U)
Feature Description Binary
U Unigrams N
PRESENCE OF WORDS IN COMMANDS (P)
Piyou The utterance contains either I or you Y
Phelp The utterance contains the word help Y
Phelpq The utterance contains words usually associated with help requests. E.g., how, am I, etc. Y
Pprev The immediately preceding system DA is Prompt and the utterance contains words also Y
present in this immediately preceding system utterance
Pintent The utterance contains words , need, desire, prefer, like and their synonyms Y
Pbrowser The utterance contains words also present in the web browser tab title. E.g., email, job Y
Phtml The utterance contains references to HTML elements. E.g., form, box, link, page, etc. Y
Pbasic The utterance contains a verb representing basic operations on a web page. E.g., click, edit. Y
Pnbasic The utterance contains a verb not related to basic web page operations; a verb usually Y
associated with task or domain related actions. E.g. send, open, compose, etc.
Pnav The utterance contains words related to cursor movement. E.g., go to, continue, next, etc. Y
Pquestion The utterance contains words usually associated with questions. E.g., what, when, why Y
SYNTACTIC STRUCTURE OF COMMANDS (S)
Snp The utterance is a noun phrase with atleast two words Y
Snoun The utterance consists of a single noun Y
Sbasic The utterance consists of a single verb representing basic web page operations. E.g., click, Y
edit, erase, select, etc.
Snbasic The utterance consists of a single verb representing task or domain related actions. e.g. Y
send, open, compose, order, etc.
cfirst CONTEXT RELATED FEATURES (C) Y
cprevious The utterance is the first command to be issued when a new website is loaded in the browser N
dialogue act of the immediately preceding system utterance
POSITION OF WORDS IN COMMANDS (B)
bnav The utterance begins with word(s) related to cursor movement. e.g. go to, continue, etc. Y
bquestion The utterance begins with a word that is usually associated with a question. E.g., what, Y
when, where, why, etc.
bi The utterance begins with the personal pronoun I. Y
bhelpq The utterance begins with word(s) usually associated with help requests. E.g., how, am I Y
TASK RELATED FEATURES (T )
tname Name of the task associated with the utterance N
</table>
<tableCaption confidence="0.988482">
Table 4: Feature set for user dialogue act classification. The complete list of words associated with each
</tableCaption>
<bodyText confidence="0.88067725">
feature in P and B is provided in Appendix A.
presents few such correlations. Note that some of
the words in Table 5 are task-specific (noise); a
consequence of using a small dataset.
</bodyText>
<subsectionHeader confidence="0.99946">
4.2 Presence of Words in Commands
</subsectionHeader>
<bodyText confidence="0.9999715">
In constract to unigram features that take into
account all possible word-DA correlations, the
presence-of-word features (P in Table 4) are lim-
ited to certain specific words that have strong cor-
relations with the DA types. For each feature
p E P, if the presence of certain specific words
associated with p occur in an utterance, then p is
set to true. The set of words for every p that cor-
responds to some dialogue act d was contructed
by determining the discriminatory words for d us-
ing simple statistical analysis of the corpus (e.g.
relative frequencies of words) as well as by an ex-
amination of the weights of different words learnt
by the SVM classifier trained on a development
dataset using unigram features alone. e.g.., the
words continue and skip occur much more fre-
quently in Command-Navigation than in other di-
alogue acts (see Table 5) and hence are included
in p,,,a,,,. Note however that not all discrimina-
tory words in Table 5 were used. Only generic
words, independent of any specific task, were se-
lected (see Appendix A for details).
</bodyText>
<subsectionHeader confidence="0.999314">
4.3 Syntactic Structure of Commands
</subsectionHeader>
<bodyText confidence="0.9998792">
The binary syntactic features (S in Table 4) were
automatically extracted using the Stanford parser
(Klein and Manning, 2003). As in word-DA
correlations, some of the syntactic structure-DA
correlations were also identified by a manual in-
</bodyText>
<page confidence="0.986241">
127
</page>
<table confidence="0.916177">
Dialogue Act Discriminatory Words
Command-Intention want, compose, book, for, look, email, find, an, ac-
counting, Stanford, a, airplane, message, I, music,
get, ticket, positions, need, bluetooth, jobs, new
Command-Task repeat, choose, delete, select, link, edit, enter,
</table>
<tableCaption confidence="0.81959175">
erase, clear, fill, in, click, third, at, body, box,
again, blue, that
Command-Multiple play, read, senior, send, reviews, Harlem, artists,
study, submit, details, law, description, Kitaro,
mornings, availability, apply, construction, pay,
reservations, proceed, it, this, available
Comand-Navigation skip, next, previous, go, page, finish, stop, item,
continue, back, line, before, box, first, second, to,
top, home, part, would
Information-Task JFK, customer, no, August, July, USA, October,
Kahalui, October30th, anytime, coach, today, non-
stop, movies, York
Question-Task price, time, fare, layover, times, is, what’s, any-
thing, cheaper, best, flight, airline, complete, one-
stop, departure, cards, price, much, cost, weekly.
Help-Task help, do, mean, does, say, can, supposed, some-
thing, how, use, voice, have, apply, reservation, by,
address, give, get
Table 5: Top discriminative unigrams based on
weights from SVM classifier.
</tableCaption>
<bodyText confidence="0.9910815">
vestigation of the corpus. For example, 82.1%
of single noun-only utterrances (snoun) have the
DA Information-Task, 76.2% of “basic” verb-only
utterances (sbasic) have the DA Command-Task
and 83.3% of “non-basic” verb-only utterances
(snbasic) have the DA Command-Multiple.
</bodyText>
<subsectionHeader confidence="0.998227">
4.4 Context Related Features
</subsectionHeader>
<bodyText confidence="0.9999898">
The local context (C in Table 4) provides valuable
cues to identify the dialogue act associated with
a user utterance. It was observed during the study
that user utterance is influenced to a large extent by
the immediately preceding system utterance. For
example, 89.95% of all user utterances immedi-
ately following the system Prompt were observed
to be Information-Task. In addition, most of the
time (probability 87.5%), the first utterance issued
for a task was Command-Intention.
</bodyText>
<subsectionHeader confidence="0.994568">
4.5 Position-of-Word in Commands
</subsectionHeader>
<bodyText confidence="0.999945">
Design of feature set B in Table 4 was inspired by
an analysis of the corpus which revealed that cer-
tain dialogue acts are characterized by the pres-
ence of certain words at the beginning of the cor-
responding utterances. For example, 93.4% of
all Command-Navigation utterances begin with a
cursor-movement related word (e.g. next, previ-
ous, etc. see Appendix A for the complete list).
</bodyText>
<subsectionHeader confidence="0.991117">
4.6 Task Related Features
</subsectionHeader>
<bodyText confidence="0.99982725">
Since it is possible for different tasks to exhibit dif-
ferent feature vector patterns for the same dialogue
act, incorporating task name (T in Table 4) as an
additional feature may therefore improve classifi-
</bodyText>
<figure confidence="0.935564111111111">
Group Composition
91 U
92 PUBUS
93 CUBUS
94 CUPUS
95 CUPUB
96 CUPUBUS
97 CUPUBUSUT
98 C U P U B U S U U
</figure>
<tableCaption confidence="0.947466">
Table 6: Feature groups.
</tableCaption>
<bodyText confidence="0.9956895">
cation performance by exploiting these variations
(if any) between tasks.
</bodyText>
<sectionHeader confidence="0.985049" genericHeader="method">
5 Classification Results
</sectionHeader>
<bodyText confidence="0.999929571428572">
All classification tasks were performed using the
WEKA toolkit (Hall et al., 2009). The classifica-
tion experiments were done using Support Vector
Machine (frequently used for benchmarking), J48
Decision Tree (appropriate for a small size mostly
binary feature set) and Random Forest classifiers.
The model parameters for all classifiers were opti-
mized for maximum performance.
In addition, experiments were also performed
to assess the utility of each feature set (Table 4).
Specifically, the performance of classifiers with
different combinations (Groups 1-8 in Table 6) of
feature sets was evaluated to assess the importance
of each individual feature set. We primarily fo-
cussed on domain-specific feature sets (P, B, C
and S). Observe that group !96 differs from any
of !92 − !95 by exactly one feature set. This lets
us to assess the individual utility of P, B, C and
S. In addition, we also extended !96 by including
U (!97) and T (!98) to determine if there was any
noticeable improvement in performance. !91 with
only unigram features serves as a baseline. All re-
ported results (Table 7) are based on 5-fold cross
validation: 632 instances for training and 158 in-
stances for testing. Table 7 presents the classifica-
tion results for different feature groups. The DA
Self-Talk was excluded from classification due to
insufficient number (2) of data points.
</bodyText>
<subsectionHeader confidence="0.992528">
5.1 Classification Performance
</subsectionHeader>
<bodyText confidence="0.990187">
Overall Performance: As seen in Table 7, the
tree-based classifiers (J48 and RF) performed bet-
ter than SVM in a majority of the feature groups
(6 out of 8). The random forest classifier yielded
the best performance (91% Precision, 90% Recall)
for feature group !96, whereas the !93-SVM com-
bination had the lowest performance (69% Preci-
sion, 67% Recall). However, all groups includ-
</bodyText>
<page confidence="0.996218">
128
</page>
<table confidence="0.999940037037037">
DA MODEL Performance of Feature Groups
G1 G2 G3 G4 G5 G6 G7 G8
P R P R P R P R P R P R P R P R
CI SVM .83 .80 .84 .95 .71 .95 .91 .96 .82 .90 .91 .95 .89 .96 .89 .94
J48 .74 .74 .83 .90 .80 .93 .84 .95 .81 .93 .83 .95 .85 .93 .91 .95
RF .76 .74 .81 .90 .85 .94 .88 .90 .80 .87 .84 .93 .88 .89 .87 .95
CT SVM .87 .73 .86 .81 .93 .30 .89 .87 .84 .81 .89 .83 .89 .81 .92 .88
J48 .80 .64 .80 .70 1.0 .28 .88 .79 .80 .70 .85 .75 .83 .87 .86 .67
RF .72 .58 .84 .89 .81 .26 .88 .89 .85 .85 .79 .93 .77 .78 .88 .80
CM SVM .73 .65 .77 .58 .36 .30 .78 .64 .78 .59 .78 .64 .80 .62 .79 .78
J48 .74 .36 .78 .79 .68 .87 .83 .59 .81 .78 .76 .83 .81 .80 .76 .87
RF .79 .56 .80 .81 .68 .83 .80 .59 .82 .79 .81 .83 .80 .82 .76 .89
CN SVM .89 .84 .93 .87 .96 .82 .67 .96 .94 .87 .96 .89 .94 .87 .90 .92
J48 .89 .65 .95 .95 .96 .92 .65 .93 .95 .95 .95 .92 .92 .93 .87 .90
RF .82 .86 .94 .94 .95 .92 .66 .95 .95 .95 .95 .95 .94 .93 .91 .88
IT SVM .70 .89 .82 .93 .70 .81 .81 .79 .82 .93 .82 .93 .82 .94 .85 .90
J48 .54 .93 .96 .97 .94 .97 .80 .82 .96 .97 .97 .96 .96 .97 .94 .94
RF .65 .93 .98 .98 .95 .97 .81 .82 .97 .98 .98 .97 .98 .98 .97 .92
QT SVM .66 .46 .87 .27 .90 .30 .80 .30 .62 .31 .80 .31 .70 .33 .85 .49
J48 .44 .36 .62 .33 .80 .23 .90 .30 .53 .34 .62 .31 .56 .47 .93 .32
RF .63 .36 .65 .31 .61 .39 .78 .27 .54 .35 .83 .39 .68 .51 .87 .33
HT SVM .77 .71 .73 .65 .80 .45 .79 .63 .63 .67 .78 .63 .72 .64 .92 .76
J48 .86 .79 .80 .57 .80 .33 .81 .60 .70 .50 .81 .55 .55 .52 .93 .91
RF .85 .70 .79 .65 .78 .33 .75 .60 .74 .67 .90 .48 .67 .67 .90 .80
Overall SVM .77 .76 .83 .82 .69 .67 .80 .79 .82 .82 .84 .83 .84 .83 .85 .85
J48 .70 .66 .88 .88 .87 .85 .80 .78 .88 .88 .89 .88 .88 .89 .87 .86
RF .74 .73 .90 .90 .86 .85 .80 .79 .89 .89 .91 .90 .90 .89 .88 .87
</table>
<tableCaption confidence="0.8040485">
Table 7: Classification Results. The overall performance is the weighted average over all dialogue acts.
Notation: J48-Decision Tree, RF-Random Forest, SVM-Support Vector Machine, P-Precision, R-Recall,
</tableCaption>
<bodyText confidence="0.985642866666667">
CI-Command-Intention, CT-Command-Task, CM-Command-Multiple, CN-Command-Navigation, IT-
Information-Task, QT-Question-Task, HT-Help-Task. The best performances for each DA are high-
lighted in bold.
ing !93 did better than !91 with tree-based clas-
sifiers. !91 was consistently outperformed by the
other groups.
Performance on dialogue acts: In 6/8 feature
groups, the performance of SVM with respect to
IT dialogue act was significantly worse than that
of tree-based classifiers. However, SVM produced
consistently good results (&gt; 80% in most cases)
for the CI and CT dialogue acts. All classifiers
performed very well in case of CN dialogue act
(&gt; 80% for 7/8 groups). However, none of the
classifiers performed well in case of QT.
</bodyText>
<subsectionHeader confidence="0.999899">
5.2 Importance of feature sets
</subsectionHeader>
<bodyText confidence="0.999878416666666">
From Table 7, it can be inferred that contextual
features (C) do not contribute to improving overall
classification performance. In particular, for each
classifier, the difference in overall performance
between groups !92 (excluding C) and !96 (includ-
ing C) is very small (worst case: 1% difference
in both P and R). However, inclusion of C signifi-
cantly improved the classification performance of
RF for QT and CI dialogue acts (18% improve-
ment in P, 8% improvement in R for QT, 3% im-
provement in both P and R for CI). Even in case of
J48, where group !96 yields the best performance,
</bodyText>
<table confidence="0.998084428571428">
Dialogue Act Discriminatory Rules
Command-Intention • cfirst ∧ ¬bnav ∧ ¬phtml ∧ ¬snoun
• cfirst ∧ ¬bnav ∧ phtml ∧ piyou
• ¬cfirst ∧¬bnav ∧pintent ∧¬pnav ∧¬pquestion
Command-Task • ¬cfirst ∧¬bnav ∧¬pintent ∧¬phelpq ∧pbasic ∧
¬pnbasic
• ¬cfirst ∧¬bnav ∧¬pintent ∧¬phelpq ∧pbasic ∧
pnbasic ∧ phtml
Command-Multiple • ¬cfirst∧¬bnav∧¬pintent∧¬phelpq∧¬pbasic∧
¬pnbasic ∧ cprevious = [h|k|l|n] ∧ ¬phtml ∧
¬pquestion
• ¬cfirst∧¬bnav∧¬pintent∧¬phelpq∧¬pbasic∧
pnbasic ∧ cprevious = [∧p]
Comand-Navigation • cfirst ∧ bnav
• cfirst ∧ ¬bnav ∧ phtml ∧ ¬piyou
• ¬cfirst ∧ bnav ∧ ¬snp
• ¬cfirst ∧ bnav ∧ snp ∧ cprevious = [s|a]
Information-Task • ¬cfirst∧¬bnav∧¬pintent∧¬phelpq∧¬pbasic∧
¬pnbasic ∧ cprevious = [p]
• ¬cfirst∧¬bnav∧¬pintent∧¬phelpq∧¬pbasic∧
pnbasic ∧ cprevious = [p] ∧ ¬piyou
Question-Task • ¬cfirst∧¬bnav∧¬pintent∧¬phelpq∧¬pbasic∧
¬pnbasic ∧ cprevious = [h|k|l|n] ∧ ¬phtml ∧
pquestion
• ¬cfirst∧¬bnav∧¬pintent∧¬phelpq∧¬pbasic∧
¬pnbasic∧cprevious = [q|s|a]∧¬pnav∧¬phtml∧
¬snoun
Help-Task • ¬cfirst∧¬bnav∧¬pintent∧phelpq∧piyou∧¬bi
</table>
<tableCaption confidence="0.989523">
Table 8: A select sample of J48 rules (conf &gt;
</tableCaption>
<footnote confidence="0.778220666666667">
0.75 and descending order of support) for group
!96. Notation: -&apos;cfirst stands for cfirst = false
and cfirst stands for cfirst = true.
</footnote>
<page confidence="0.990055">
129
</page>
<table confidence="0.997633">
Utterance Actual DA Predicted DA Comments
“Continue to booking it” Command-Multiple Command-Navigation This utterance was issued while performing the book a hotel room task. This
command essentially is the same as “book it”. The presence of a navigation
related verb continue at the beginning caused the classifiers to incorrectly classify
it as Command-Navigation.
“I am looking to check in Information-Task Command-Intention This utterance was in response to a system prompt for check-in date while per-
on July 2t rd” forming the book a hotel room task. The presence of first person nominative
pronoun “I” caused the classifiers to categorize it as Command-Intention.
“What does that mean?” Help-Task Question-Task This utterance was directed towards the experimenter and therefore it was anno-
tated as Help-Task. However, the absence of the keyword help and the presence
of a Wh-word what at the beginning of the command caused the classifiers to
incorrectly classify this command as Question-Task.
“Best available price?” Question-Task The absence of Question related words like Wh-words, is, etc. at the beginning
“Ok, return time?” Command-Multiple coupled with the fact that these commands are noun phrases caused the classifiers
“Price?” Information to incorrectly classify them as either Command-Multiple or Information.
“Layover?”
</table>
<tableCaption confidence="0.999698">
Table 9: A few incorrectly classified utterances.
</tableCaption>
<bodyText confidence="0.999856965517241">
contextual features were found to be a component
of some of the high-confidence, high-support J48
rules (Table 8) for CI and QT. Similar claims can
also be made for syntactic features(S), where al-
though there is not much difference in overall per-
formance between groups G5 and G6 (Worst Case:
2% drop in P, 1% drop in R), improvements were
observed in case of RF for QT and CI dialogue
acts (29% improvement in P, 4% improvement in
R for QT, 4% improvement in P, 6% improvement
in R for CI).
Excluding either word-existential features (P)
or word-position related features (13), however,
caused a significant drop in overall performance
(Worst case: 15% drop in P, 16% drop in R with-
out P, 11% drop in both P and R without 13). Ta-
ble 8 further highlights the importance of feature
set P, since over 50% of the high performing J48
rules (Table 8) have at least one feature of type P
with true as their truth values.
It can be seen in Table 7 that adding either un-
igrams or task-name to the existing feature set of
G6 does not affect the overall performance. How-
ever, the use of unigram features improved re-
sults of all the classifiers for the HT DA. No such
DA specific improvements were seen with task-
name as an added feature to G6. This suggests
that the feature values of G6 for all DAs are task-
independent.
</bodyText>
<subsectionHeader confidence="0.999157">
5.3 Prediction Errors
</subsectionHeader>
<bodyText confidence="0.999892857142857">
It is clear from Table 7 that the prediction accu-
racies of CM, QT and HT are not nearly as good
as those of other dialogue acts. Table 9 provides
some insights into this issue via illustrative exam-
ples from the corpus.
Notice that the errors in case of CI, CM and HT
are mostly related to choice of words used in the
utterances, whereas mistakes in the prediction of
QT are mainly due to inadequate information or
the incompleteness of the utterances. Therefore, it
is recommended that the speech enabled web dia-
logue systems enforce a constraint requiring users
to express their complete thoughts in each of their
corresponding utterances.
</bodyText>
<sectionHeader confidence="0.99956" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998195">
Experiments with the dialogue act model de-
scribed in the paper indicate that with a small set
of simple lexical/syntactic features it is possible
to achieve a high overall dialogue act recogni-
tion accuracy (over 90% precision and recall) us-
ing simple and well-known tree-based classifiers
such as decision trees and random forests. It is
hence possible to build speech-enabled dialogue-
based assistive web browsing systems with low
computational overhead that, inturn, can result in
low latency response times - a critical requirement
from a usability perspective for blind users. Fi-
nally, a dialogue model for non-visual web access,
such as the one described in this paper, can be the
key driver of goal-oriented web browsing - a next
generation assistive technology that will empower
blind users to stay focused on high-level browsing
tasks, while the system does all of the low-level
operations such as clicking on links, filling forms,
etc., necessary to accomplish the tasks.
</bodyText>
<sectionHeader confidence="0.996515" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999195">
Research reported in this publication was sup-
ported by the National Eye Institute of the Na-
tional Institutes of Health under award number
1R43EY21962-1A1. We would like to thank
Lighthouse Guild International and Dr. William
Seiple in particular for helping conduct user stud-
ies.
</bodyText>
<page confidence="0.99596">
130
</page>
<sectionHeader confidence="0.981415" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.996997807692308">
AFB. 2013. Facts and figures on american adults
with vision loss. http://www.afb.org/
info/blindness-statistics/adults/
facts-and-figures/235, January.
Apple-Inc. 2013. Voiceover for os x. http:
//www.apple.com/accessibility/osx/
voiceover/.
Srinivas Bangalore and Amanda J Stent. 2009. In-
cremental parsing models for dialog task structure.
In Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 94–102. Association for Compu-
tational Linguistics.
Yevgen Borodin, Jeffrey P Bigham, Glenn Dausch, and
IV Ramakrishnan. 2010. More than meets the eye:
a survey of screen-reader browsing strategies. In
Proceedings of the 2010 International Cross Dis-
ciplinary Conference on Web Accessibility (W4A),
page 13. ACM.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, Michael D Wallis, Mladen A Vouk, and
James C Lester. 2010. Dialogue act modeling in
a complex task-oriented domain. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 297–305.
Association for Computational Linguistics.
Harry Bunt. 2011. Multifunctionality in dialogue.
Computer Speech &amp; Language, 25(2):222–245.
Jean Carletta, Stephen Isard, Gwyneth Doherty-
Sneddon, Amy Isard, Jacqueline C Kowtko, and
Anne H Anderson. 1997. The reliability of a dia-
logue structure coding scheme. Computational lin-
guistics, 23(1):13–31.
Ananlada Chotimongkol. 2008. Learning the structure
of task-oriented conversations from the corpus of in-
domain dialogs. Ph.D. thesis, SRI International.
Mark G Core and James Allen. 1997. Coding dialogs
with the damsl annotation scheme. In AAAIfall sym-
posium on communicative action in humans and ma-
chines, pages 28–35. Boston, MA.
Maxine Eskenazi, Alexander I Rudnicky, Karin Gre-
gory, Paul C Constantinides, Robert Brennan,
Christina L Bennett, and Jwan Allen. 1999. Data
collection and processing in the carnegie mellon
communicator. In EUROSPEECH.
C´esar Gonz´alez Ferreras and Valent´ın Carde˜noso-Payo.
2005. Development and evaluation of a spoken di-
alog system to access a newspaper web site. In IN-
TERSPEECH, pages 857–860.
J.L. Fleiss. 1973. Statistical methods for rates and
proportions Rates and proportions. Wiley.
Freedom-Scientific. 2014. Screen read-
ing software from freedom scientific.
http://www.freedomscientific.com/
products/fs/jaws-product-page.asp.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Cheongjae Lee, Sangkeun Jung, Kyungduk Kim,
Donghyeon Lee, and Gary Geunbae Lee. 2010. Re-
cent approaches to dialog management for spoken
dialog systems. JCSE, 4(1):1–22.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, Hideki
Kashioka, and Satoshi Nakamura. 2009. Annotat-
ing dialogue acts to construct dialogue systems for
consulting. In Proceedings of the 7th Workshop on
Asian Language Resources, pages 32–39. Associa-
tion for Computational Linguistics.
Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, Hideki
Kashioka, and Satoshi Nakamura. 2010. Dialogue
acts annotation for nict kyoto tour dialogue corpus
to construct statistical dialogue systems. In LREC.
Yury Puzis, Yevgen Borodin, Rami Puzis, and IV Ra-
makrishnan. 2013. Predictive web automation as-
sistant for people with vision impairments. In Pro-
ceedings of the 22nd international conference on
World Wide Web, pages 1031–1040. International
World Wide Web Conferences Steering Committee.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
dialog act tagging. Computer Speech &amp; Language,
23(4):407–422.
John R Searle. 1975. Indirect speech acts. Syntax and
semantics, 3:59–82.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339–373.
Lu Wang, Larry Heck, and Dilek Hakkani-Tur. 2014.
Leveraging semantic web search and browse ses-
sions for multi-turn spoken dialog systems.
</reference>
<page confidence="0.998333">
131
</page>
<sectionHeader confidence="0.7330535" genericHeader="method">
A List of Words Predictive of Dialogue
Acts
</sectionHeader>
<bodyText confidence="0.999007363636364">
Table 10 lists all the words associated with
presence-of-word (P) and position-of-word (B)
related features (Table 4) used in this work. No-
tice that all words specified in Table 10 are task-
independent. This ensures that the proposed fea-
ture set is generic enough to be applicable for a
wide variety of tasks on the web. The proposed
list of words can be easily extended by adding syn-
onyms, which can be obtained automatically from
publicly available sources like WordNet (Miller,
1995).
</bodyText>
<sectionHeader confidence="0.750883" genericHeader="method">
Features Predictive Words
</sectionHeader>
<reference confidence="0.893702">
piyou I, you
phelp help
phelpq, bhelpq how, can, do, am I
pprev dynamically determined at runtime
pintent want, like, would, need, prefer
pbrowser dynamically determined at runtime
phtml body, page, form, box, field, search, link, button,
list, dropdown
pbasic clear, select, fill, delete, click, edit, erase, submit,
repeat, choose, enter, check
pnbasic any verb not in the pbasic list above
pnav, bnav skip, go to, next, first, last, back, continue, previ-
ous, stop, go back, finish, home page
pquestion, bquestion what, where, why, when, how
</reference>
<tableCaption confidence="0.777358">
Table 10: Complete list of predictive words for
features in P and B of Table 4.
</tableCaption>
<bodyText confidence="0.9999797">
As explained earlier, the words in Table 10 were
selected by performing simple statistical analysis
of corpus and also by examining the word-weights
produced by the SVM classifier trained on uni-
gram features alone. In other words, some of the
words in Table 10 were borrowed from Table 5
that lists discriminatory unigrams for different di-
alogue acts. Note that the task-dependent words
(e.g. “Stanford”, “airplane”, etc.) in Table 5 were
ignored while constructing Table 10.
</bodyText>
<subsectionHeader confidence="0.877901">
B Sample Dialogue in the Corpus
</subsectionHeader>
<bodyText confidence="0.9979504">
Table 11 presents an example of a dialogue that
was collected during the execution of the Shop-
ping task by a participant in the Wizard-Of-Oz
study. For deeper understanding, the wizard ac-
tions for every user utterance are also listed.
</bodyText>
<reference confidence="0.984557479166666">
Subject: “I want to buy CD”
Wizard: [enter ‘CDs’ in the search box, clicks ‘Go’, and
narrate search results one by one]
Screen reader: Verbatim 97458 700 MB 80 Minute ... Disc CD-R
by Verbatim link ...
Subject: “Search for new age items”
Wizard: [searches for ‘new age items’, clicks on results]
Screen reader: Age of Wushu - Free Amazon ... Game Connect
link by Snail Games USA ...
Subject: “stop”
Wizard: [presses pause shortcut]
Subject: “new age music CDs”
Wizard: [searches for ‘new music CDs’]
Screen reader: The ultimate Most Relaxing New Age ....
Subject: “new age music CDs by Kitaro”
Wizard: [Searches for ‘new music cds by Kitaro’]
Screen reader: Most Relaxing New Age Music link by Kitaro,
List 8 items, $10.87 link, order in the next 3 hours
... Tenku link by Kitaro ...
Subject: “sample recordings”
Wizard: [follows the current link]
Screen reader: Page loading
Wizard: [plays the first sample track]
Screen reader: [audio sample]
Subject: “next”
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: “next”
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: “go back”
Wizard: [presses back button]
Screen reader: page loading, [repeats the visited link]
Subject: “next CD”
Wizard: [clicks the title of the next item in search result]
Screen Reader: Ancient link by Kitaro ... $14.98 link ...
Subject: “listen to audio”
Wizard: [follows link]
Screen Reader: Page loading
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: “next”
Wizard: [plays the next sample track]
Screen reader: [audio sample]
Subject: “buy this cd”
Wizard: [clicks ‘Add to cart’ button, then clicks ‘Proceed
to Checkout’ button]
Screen reader: [reads out all captions]
</reference>
<tableCaption confidence="0.7450095">
Table 11: An example dialogue from corpus along
with associated wizard actions.
</tableCaption>
<page confidence="0.995683">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.436125">
<title confidence="0.998268">Dialogue Act Modeling for Non-Visual Web Access</title>
<author confidence="0.990363">Vikas Ashok Yevgen Borodin Svetlana Stoyanchev I V Ramakrishnan</author>
<affiliation confidence="0.993018">Dept of Computer Science Charmtech Labs LLC AT&amp;T Labs Research Charmtech Labs LLC</affiliation>
<author confidence="0.546714">Stony Brook University CEWIT SBU R</author>
<author confidence="0.546714">D Park New York City</author>
<author confidence="0.546714">New York CEWIT SBU R</author>
<author confidence="0.546714">D Park Stony Brook</author>
<author confidence="0.546714">New York Stony Brook</author>
<author confidence="0.546714">New York Stony Brook</author>
<author confidence="0.546714">New York</author>
<abstract confidence="0.9966668">Speech-enabled dialogue systems have the potential to enhance the ease with which blind individuals can interact with the Web beyond what is possible with screen readers the currently available assistive technology which narrates the textual content on the screen and provides shortcuts to navigate the content. In this paper, we present a dialogue act model towards developing a speech enabled browsing system. The model is based on the corpus data that was collected in a wizard-of-oz with individuals who were assigned a gamut of browsing tasks. The development of the model included extensive experiments with assorted feature sets and classifiers; the outcomes of the experiments and the analysis of the results are presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AFB</author>
</authors>
<title>Facts and figures on american adults with vision loss. http://www.afb.org/ info/blindness-statistics/adults/</title>
<date>2013</date>
<pages>235</pages>
<contexts>
<context position="1655" citStr="AFB, 2013" startWordPosition="258" endWordPosition="259">browsing tasks. The development of the model included extensive experiments with assorted feature sets and classifiers; the outcomes of the experiments and the analysis of the results are presented. 1 Introduction The Web is the “go-to” computing infrastructure for participating in our fast-paced digital society. It has the potential to provide an even greater benefit to blind people who once required human assistance with many of their activities. According to the American Federation for the Blind, there are 21.5 million Americans who have vision loss, of whom 1.5 million are computer users (AFB, 2013). Blind users employ screen readers as the assistive technology to interact with digital content (e.g.., JAWS (Freedom-Scientific, 2014) and VoiceOver (Apple-Inc., 2013)). Screen readers serially narrate the content of the screen using textto-speech engines and enable users to navigate in the content using keyboard shortcuts and touchscreen gestures. Navigating content-rich web pages and conducting online transactions spanning multiple pages requires using shortcuts and this can get quite cumbersome and tedious. Specifically, in online shopping a user typically browses through product categori</context>
</contexts>
<marker>AFB, 2013</marker>
<rawString>AFB. 2013. Facts and figures on american adults with vision loss. http://www.afb.org/ info/blindness-statistics/adults/ facts-and-figures/235, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apple-Inc</author>
</authors>
<title>Voiceover for os x. http: //www.apple.com/accessibility/osx/ voiceover/.</title>
<date>2013</date>
<marker>Apple-Inc, 2013</marker>
<rawString>Apple-Inc. 2013. Voiceover for os x. http: //www.apple.com/accessibility/osx/ voiceover/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Amanda J Stent</author>
</authors>
<title>Incremental parsing models for dialog task structure.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>94--102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8536" citStr="Bangalore and Stent (2009)" startWordPosition="1344" endWordPosition="1347"> number of 10 defined dimensions, such as Task, Feedback, or Time management. In the analysis of human-computer dialogues, it is common to adopt DA annotation schemes to suit specific domains. Generic domain-independent schemes are geared towards the analysis of natural human-human dialogue and provide rich annotation structure that can cover complexity of natural dialogue. Domain-specific dialogues use a subset of the generic dialogue structure. For example, Ohtake et al. (2009) developed a DA scheme for tourist-guide domain motivated by a generic annotation scheme (Ohtake et al., 2010), and Bangalore and Stent (2009) created a dialogue scheme for a catalogue product ordering dialogue system. In our work we design DA scheme for Web-Browsing domain motivated by the DAMSL (Core and Allen, 1997) schema for task-oriented dialogue. We used a Wizard-of-Oz (WOZ) approach to collect an initial dataset of spoken voice com124 Task τu τd Shopping 121 16 Email 92 16 Flight 180 16 Hotel 179 16 Job 76 16 Admission 144 16 Overall 792 96 Table 1: Corpus details. Tu - number of utterances, Td - number of dialogs. mands by both blind and sighted users. WOZ is commonly used before building a dialogue system (Chotimongkol, 20</context>
</contexts>
<marker>Bangalore, Stent, 2009</marker>
<rawString>Srinivas Bangalore and Amanda J Stent. 2009. Incremental parsing models for dialog task structure. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 94–102. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yevgen Borodin</author>
<author>Jeffrey P Bigham</author>
<author>Glenn Dausch</author>
<author>Ramakrishnan</author>
</authors>
<title>More than meets the eye: a survey of screen-reader browsing strategies.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A),</booktitle>
<pages>13</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2788" citStr="Borodin et al., 2010" startWordPosition="439" endWordPosition="442">ious. Specifically, in online shopping a user typically browses through product categories, searches for products, adds products to cart, logs into his/her account, and finally makes a payment. All these steps require screen-reader users listen through a lot of content, fill forms, and find links and buttons that have to be selected to get through these steps. If users do not want to go through all content on the page, they have to remember and use a number of different shortcuts. Beginner users often use the “Down” key to go through the page line by line, listening to all content on the way (Borodin et al., 2010). Now suppose that blind users were to tell the web browser what they wanted to accomplish and let the browsing application automatically determine what has to be clicked, fill out forms, help find products, answer questions, breeze through checkout, and wherever possible, relieve the user from doing all the mundane and tedious low-level operations such as clicking, typing, etc. The ability to carry out a dialogue with the web browser at a higher level has the potential to overcome the limitations of shortcut-based screen reading and thus offers a richer and more productive user experience for</context>
<context position="16031" citStr="Borodin et al., 2010" startWordPosition="2534" endWordPosition="2537">res (T), presence of words anywhere in an utterance(P) and presence of words at the beginning of an utterance(13). The last two feature sets are similar to the ones used in Boyer et al. (2010). Task κ Shopping 0.865 Email 0.829 Flight 0.894 Hotel 0.848 Job 0.824 Admission 0.800 Table 3: Inter-rater agreement measured in terms of Cohen’s κ for all tasks in the corpus. The feature sets C, P, 13 and S are specific to the domain of non-visual web access and were hand-crafted based on the following three factors: knowledge of the browsing behavior of blind users reported in previous studies, e.g. (Borodin et al., 2010); manual analysis of the corpus; mitigate the effect of noise that is usually present in standard lexical/syntactic feature sets such as n-grams and parse tree rules. Each of the features in C, P, 13 and S were crafted to have a close correspondence to some dialogue act. For example, p,,,a,,, is closely tied to the Command-Navigation dialogue act. 4.1 Unigrams Unigrams (U in Table 4) are one of the commonly used lexical features for training dialogue act classifiers (e.g. (Boyer et al., 2010), (Stolcke et al., 2000), (Rangarajan Sridhar et al., 2009)). Encoding unigrams as features is based on</context>
</contexts>
<marker>Borodin, Bigham, Dausch, Ramakrishnan, 2010</marker>
<rawString>Yevgen Borodin, Jeffrey P Bigham, Glenn Dausch, and IV Ramakrishnan. 2010. More than meets the eye: a survey of screen-reader browsing strategies. In Proceedings of the 2010 International Cross Disciplinary Conference on Web Accessibility (W4A), page 13. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristy Elizabeth Boyer</author>
<author>Eun Young Ha</author>
<author>Robert Phillips</author>
<author>Michael D Wallis</author>
<author>Mladen A Vouk</author>
<author>James C Lester</author>
</authors>
<title>Dialogue act modeling in a complex task-oriented domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>297--305</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15602" citStr="Boyer et al. (2010)" startWordPosition="2461" endWordPosition="2464">rallel user study with sighted participants on the same set of web tasks that were used in the wizard-of-oz study. 4 Features This section describes the different feature sets that we experimented with for our classification tasks. The vector representation for training the DA classifiers integrates several types of features (Table 4): unigrams (U) and syntactic features (S), context related features (C), task related features (T), presence of words anywhere in an utterance(P) and presence of words at the beginning of an utterance(13). The last two feature sets are similar to the ones used in Boyer et al. (2010). Task κ Shopping 0.865 Email 0.829 Flight 0.894 Hotel 0.848 Job 0.824 Admission 0.800 Table 3: Inter-rater agreement measured in terms of Cohen’s κ for all tasks in the corpus. The feature sets C, P, 13 and S are specific to the domain of non-visual web access and were hand-crafted based on the following three factors: knowledge of the browsing behavior of blind users reported in previous studies, e.g. (Borodin et al., 2010); manual analysis of the corpus; mitigate the effect of noise that is usually present in standard lexical/syntactic feature sets such as n-grams and parse tree rules. Each</context>
</contexts>
<marker>Boyer, Ha, Phillips, Wallis, Vouk, Lester, 2010</marker>
<rawString>Kristy Elizabeth Boyer, Eun Young Ha, Robert Phillips, Michael D Wallis, Mladen A Vouk, and James C Lester. 2010. Dialogue act modeling in a complex task-oriented domain. In Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297–305. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Bunt</author>
</authors>
<title>Multifunctionality in dialogue.</title>
<date>2011</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="7490" citStr="Bunt, 2011" startWordPosition="1193" endWordPosition="1194">logue state is open. To determine dialogue manager action, it is essential for the system to identify users intent or dialogue act. In this work, we address dialogue act modelling for opendomain voice web browsing as a proof of concept for the system. Dialogue act (DA) annotation schemes for spoken dialogue systems follow theories on speech acts originally developed by Searle (1975). A number of DA annotation schemes have been developed previously (Core and Allen, 1997), (Carletta et al., 1997). Several of dialogue tagging schemes strive to provide domain-independence (Core and Allen, 1997), (Bunt, 2011). Bunt (2011) developed a NIST standardized domain-independent annotation scheme which incorporates elements from the previously developed annotation schemes. It is a hierarchical multi-dimensional annotation scheme. Each functional segment (part of an utterance corresponding to a DA) can have a general purpose function, such as Inform, Propositional Question, Yes/No Question, and a dimension-specific function in any number of 10 defined dimensions, such as Task, Feedback, or Time management. In the analysis of human-computer dialogues, it is common to adopt DA annotation schemes to suit speci</context>
<context position="11940" citStr="Bunt, 2011" startWordPosition="1908" endWordPosition="1909">efore, each dialogue turn was treated as a single utterance and every utterance was identified with a single associated dialogue act. The corpus was manually annotated with dialogue act labels and the labeling scheme was verified by measuring the inter-annotator agreement. The rest of this section describes the annotation scheme. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by the DAMSL scheme (Core and Allen, 1997) for task oriented dialogue. The proposed scheme was also influenced by extended DAMSL tagset (Stolcke et al., 2000) and the DIT++ annotation scheme (Bunt, 2011). We customized the annotation scheme to suit the non-visual web access domain, thereby making it more relevant to our corpus and tasks. Table 2 lists the dialogue acts for both user and system utterances. The user dialogue act tagset consists of labels representing task related requests (Command-Intention, Command-Task, Command-Multiple, Command-Navigation), inquiries (Question-Task, Help-Task) and information input (Information-Task), whereas the system DA tagset contains labels representing information requests (Prompt), answers to user inquiries (Question-Answer, Help-Response) and other s</context>
</contexts>
<marker>Bunt, 2011</marker>
<rawString>Harry Bunt. 2011. Multifunctionality in dialogue. Computer Speech &amp; Language, 25(2):222–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Stephen Isard</author>
<author>Gwyneth DohertySneddon</author>
<author>Amy Isard</author>
<author>Jacqueline C Kowtko</author>
<author>Anne H Anderson</author>
</authors>
<title>The reliability of a dialogue structure coding scheme.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="7378" citStr="Carletta et al., 1997" startWordPosition="1175" endWordPosition="1179">s primarily on a dialogue state, in an agent-based system with userinitiative, the space of users dialogue acts at each dialogue state is open. To determine dialogue manager action, it is essential for the system to identify users intent or dialogue act. In this work, we address dialogue act modelling for opendomain voice web browsing as a proof of concept for the system. Dialogue act (DA) annotation schemes for spoken dialogue systems follow theories on speech acts originally developed by Searle (1975). A number of DA annotation schemes have been developed previously (Core and Allen, 1997), (Carletta et al., 1997). Several of dialogue tagging schemes strive to provide domain-independence (Core and Allen, 1997), (Bunt, 2011). Bunt (2011) developed a NIST standardized domain-independent annotation scheme which incorporates elements from the previously developed annotation schemes. It is a hierarchical multi-dimensional annotation scheme. Each functional segment (part of an utterance corresponding to a DA) can have a general purpose function, such as Inform, Propositional Question, Yes/No Question, and a dimension-specific function in any number of 10 defined dimensions, such as Task, Feedback, or Time ma</context>
</contexts>
<marker>Carletta, Isard, DohertySneddon, Isard, Kowtko, Anderson, 1997</marker>
<rawString>Jean Carletta, Stephen Isard, Gwyneth DohertySneddon, Amy Isard, Jacqueline C Kowtko, and Anne H Anderson. 1997. The reliability of a dialogue structure coding scheme. Computational linguistics, 23(1):13–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananlada Chotimongkol</author>
</authors>
<title>Learning the structure of task-oriented conversations from the corpus of indomain dialogs.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>SRI International.</institution>
<contexts>
<context position="6086" citStr="Chotimongkol, 2008" startWordPosition="953" endWordPosition="954">2014), dialogue interface to generic web sites is a novel task. Spoken dialogue systems (SDS) can be classified by the type of initiative: system, user, or mixed initiative (Lee et al., 2010). In a system-initiative SDS, a system guides a user through a series of information gathering and information presenting prompts. In a user-initiative system, a user can initiate and steer the interaction. Mixed-initiative systems allow both system and user-initiated actions. Dialogue systems also differ in the types of dialogue manager: finite state based, form based, or agent based (Lee et al., 2010), (Chotimongkol, 2008). Finite state and form filling systems are usually system-initiative. These systems have a fixed set of dialogue states and finite set of possible user commands that map to system actions. In contrast, a speech-enabled browsing system proposed in this work is an agent-based system. The set of actions of this system correspond to user actions during web browsing. The domain of possible user commands at each point of the dialogue depends on the current web page that is viewed by a user. The dialogue state in a voice browsing system is compiled at run-time as the user can visit any web page. Whi</context>
<context position="9139" citStr="Chotimongkol, 2008" startWordPosition="1451" endWordPosition="1452">and Stent (2009) created a dialogue scheme for a catalogue product ordering dialogue system. In our work we design DA scheme for Web-Browsing domain motivated by the DAMSL (Core and Allen, 1997) schema for task-oriented dialogue. We used a Wizard-of-Oz (WOZ) approach to collect an initial dataset of spoken voice com124 Task τu τd Shopping 121 16 Email 92 16 Flight 180 16 Hotel 179 16 Job 76 16 Admission 144 16 Overall 792 96 Table 1: Corpus details. Tu - number of utterances, Td - number of dialogs. mands by both blind and sighted users. WOZ is commonly used before building a dialogue system (Chotimongkol, 2008), (Ohtake et al., 2009), (Eskenazi et al., 1999). In previous work on dialogue modelling, Stolcke et al. (2000) used HMM approach to predict dialogue acts in a switchboard human-human dialogue corpus achieving 65% accuracy. Rangarajan Sridhar et al. (2009) applied a maximum entropy classifier on the Switchbord corpus. Using a combination of lexical, syntactic, and prosodic features, the authors achieve accuracy of 72% on that corpus. Following the work of Rangarajan Sridhar et al. (2009), we use supervised classification approach to determine dialogue act on the annotated corpus of human-wizar</context>
</contexts>
<marker>Chotimongkol, 2008</marker>
<rawString>Ananlada Chotimongkol. 2008. Learning the structure of task-oriented conversations from the corpus of indomain dialogs. Ph.D. thesis, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James Allen</author>
</authors>
<title>Coding dialogs with the damsl annotation scheme. In AAAIfall symposium on communicative action in humans and machines,</title>
<date>1997</date>
<pages>28--35</pages>
<location>Boston, MA.</location>
<contexts>
<context position="7353" citStr="Core and Allen, 1997" startWordPosition="1171" endWordPosition="1174">nite state system depends primarily on a dialogue state, in an agent-based system with userinitiative, the space of users dialogue acts at each dialogue state is open. To determine dialogue manager action, it is essential for the system to identify users intent or dialogue act. In this work, we address dialogue act modelling for opendomain voice web browsing as a proof of concept for the system. Dialogue act (DA) annotation schemes for spoken dialogue systems follow theories on speech acts originally developed by Searle (1975). A number of DA annotation schemes have been developed previously (Core and Allen, 1997), (Carletta et al., 1997). Several of dialogue tagging schemes strive to provide domain-independence (Core and Allen, 1997), (Bunt, 2011). Bunt (2011) developed a NIST standardized domain-independent annotation scheme which incorporates elements from the previously developed annotation schemes. It is a hierarchical multi-dimensional annotation scheme. Each functional segment (part of an utterance corresponding to a DA) can have a general purpose function, such as Inform, Propositional Question, Yes/No Question, and a dimension-specific function in any number of 10 defined dimensions, such as T</context>
<context position="8714" citStr="Core and Allen, 1997" startWordPosition="1373" endWordPosition="1376"> domains. Generic domain-independent schemes are geared towards the analysis of natural human-human dialogue and provide rich annotation structure that can cover complexity of natural dialogue. Domain-specific dialogues use a subset of the generic dialogue structure. For example, Ohtake et al. (2009) developed a DA scheme for tourist-guide domain motivated by a generic annotation scheme (Ohtake et al., 2010), and Bangalore and Stent (2009) created a dialogue scheme for a catalogue product ordering dialogue system. In our work we design DA scheme for Web-Browsing domain motivated by the DAMSL (Core and Allen, 1997) schema for task-oriented dialogue. We used a Wizard-of-Oz (WOZ) approach to collect an initial dataset of spoken voice com124 Task τu τd Shopping 121 16 Email 92 16 Flight 180 16 Hotel 179 16 Job 76 16 Admission 144 16 Overall 792 96 Table 1: Corpus details. Tu - number of utterances, Td - number of dialogs. mands by both blind and sighted users. WOZ is commonly used before building a dialogue system (Chotimongkol, 2008), (Ohtake et al., 2009), (Eskenazi et al., 1999). In previous work on dialogue modelling, Stolcke et al. (2000) used HMM approach to predict dialogue acts in a switchboard hum</context>
<context position="11779" citStr="Core and Allen, 1997" startWordPosition="1881" endWordPosition="1884"> a simple sentence or phrase. Each system turn is either narration of webpage content or information request for the purpose of either form filling or disambiguation. Therefore, each dialogue turn was treated as a single utterance and every utterance was identified with a single associated dialogue act. The corpus was manually annotated with dialogue act labels and the labeling scheme was verified by measuring the inter-annotator agreement. The rest of this section describes the annotation scheme. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by the DAMSL scheme (Core and Allen, 1997) for task oriented dialogue. The proposed scheme was also influenced by extended DAMSL tagset (Stolcke et al., 2000) and the DIT++ annotation scheme (Bunt, 2011). We customized the annotation scheme to suit the non-visual web access domain, thereby making it more relevant to our corpus and tasks. Table 2 lists the dialogue acts for both user and system utterances. The user dialogue act tagset consists of labels representing task related requests (Command-Intention, Command-Task, Command-Multiple, Command-Navigation), inquiries (Question-Task, Help-Task) and information input (Information-Task)</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G Core and James Allen. 1997. Coding dialogs with the damsl annotation scheme. In AAAIfall symposium on communicative action in humans and machines, pages 28–35. Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxine Eskenazi</author>
<author>Alexander I Rudnicky</author>
<author>Karin Gregory</author>
<author>Paul C Constantinides</author>
<author>Robert Brennan</author>
<author>Christina L Bennett</author>
<author>Jwan Allen</author>
</authors>
<title>Data collection and processing in the carnegie mellon communicator.</title>
<date>1999</date>
<booktitle>In EUROSPEECH.</booktitle>
<contexts>
<context position="9187" citStr="Eskenazi et al., 1999" startWordPosition="1457" endWordPosition="1461">r a catalogue product ordering dialogue system. In our work we design DA scheme for Web-Browsing domain motivated by the DAMSL (Core and Allen, 1997) schema for task-oriented dialogue. We used a Wizard-of-Oz (WOZ) approach to collect an initial dataset of spoken voice com124 Task τu τd Shopping 121 16 Email 92 16 Flight 180 16 Hotel 179 16 Job 76 16 Admission 144 16 Overall 792 96 Table 1: Corpus details. Tu - number of utterances, Td - number of dialogs. mands by both blind and sighted users. WOZ is commonly used before building a dialogue system (Chotimongkol, 2008), (Ohtake et al., 2009), (Eskenazi et al., 1999). In previous work on dialogue modelling, Stolcke et al. (2000) used HMM approach to predict dialogue acts in a switchboard human-human dialogue corpus achieving 65% accuracy. Rangarajan Sridhar et al. (2009) applied a maximum entropy classifier on the Switchbord corpus. Using a combination of lexical, syntactic, and prosodic features, the authors achieve accuracy of 72% on that corpus. Following the work of Rangarajan Sridhar et al. (2009), we use supervised classification approach to determine dialogue act on the annotated corpus of human-wizard web-browsing dialogues. 3 Corpus and Annotatio</context>
</contexts>
<marker>Eskenazi, Rudnicky, Gregory, Constantinides, Brennan, Bennett, Allen, 1999</marker>
<rawString>Maxine Eskenazi, Alexander I Rudnicky, Karin Gregory, Paul C Constantinides, Robert Brennan, Christina L Bennett, and Jwan Allen. 1999. Data collection and processing in the carnegie mellon communicator. In EUROSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´esar Gonz´alez Ferreras</author>
<author>Valent´ın Carde˜noso-Payo</author>
</authors>
<title>Development and evaluation of a spoken dialog system to access a newspaper web site.</title>
<date>2005</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>857--860</pages>
<marker>Ferreras, Carde˜noso-Payo, 2005</marker>
<rawString>C´esar Gonz´alez Ferreras and Valent´ın Carde˜noso-Payo. 2005. Development and evaluation of a spoken dialog system to access a newspaper web site. In INTERSPEECH, pages 857–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Statistical methods for rates and proportions Rates and proportions.</title>
<date>1973</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="12796" citStr="Fleiss, 1973" startWordPosition="2030" endWordPosition="2031"> of labels representing task related requests (Command-Intention, Command-Task, Command-Multiple, Command-Navigation), inquiries (Question-Task, Help-Task) and information input (Information-Task), whereas the system DA tagset contains labels representing information requests (Prompt), answers to user inquiries (Question-Answer, Help-Response) and other system responses (Short-Response, Long-Response, etc.) to user commands. Inter-rater agreement values for different tasks in the corpus are presented in Table 3. The n values for all tasks are above 0.80, which according to Fleiss’ guidelines (Fleiss, 1973), indicates excellent inter-rater reliability on the DA annotation. Therefore, the DA tagset is generic enough to be applicable for a wide varity of tasks that can be performed on the web. Note that the dialogue act scheme was specially designed for non-visual web 125 User dialogue Acts Dialogue Act Description Frequency Command-Intention Indication of user’s intention or end goal, e.g. I wish to buy a Bluetooth speaker 0.117 Command-Task Basic action commands like click, select, enter, etc. 0.072 Command-Multiple Complex commands requiring an execution plan comprising a sequence of basic 0.16</context>
</contexts>
<marker>Fleiss, 1973</marker>
<rawString>J.L. Fleiss. 1973. Statistical methods for rates and proportions Rates and proportions. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freedom-Scientific</author>
</authors>
<title>Screen reading software from freedom scientific.</title>
<date>2014</date>
<note>http://www.freedomscientific.com/ products/fs/jaws-product-page.asp.</note>
<contexts>
<context position="1791" citStr="Freedom-Scientific, 2014" startWordPosition="278" endWordPosition="279">he outcomes of the experiments and the analysis of the results are presented. 1 Introduction The Web is the “go-to” computing infrastructure for participating in our fast-paced digital society. It has the potential to provide an even greater benefit to blind people who once required human assistance with many of their activities. According to the American Federation for the Blind, there are 21.5 million Americans who have vision loss, of whom 1.5 million are computer users (AFB, 2013). Blind users employ screen readers as the assistive technology to interact with digital content (e.g.., JAWS (Freedom-Scientific, 2014) and VoiceOver (Apple-Inc., 2013)). Screen readers serially narrate the content of the screen using textto-speech engines and enable users to navigate in the content using keyboard shortcuts and touchscreen gestures. Navigating content-rich web pages and conducting online transactions spanning multiple pages requires using shortcuts and this can get quite cumbersome and tedious. Specifically, in online shopping a user typically browses through product categories, searches for products, adds products to cart, logs into his/her account, and finally makes a payment. All these steps require screen</context>
</contexts>
<marker>Freedom-Scientific, 2014</marker>
<rawString>Freedom-Scientific. 2014. Screen reading software from freedom scientific. http://www.freedomscientific.com/ products/fs/jaws-product-page.asp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="23954" citStr="Hall et al., 2009" startWordPosition="3804" endWordPosition="3807">ted word (e.g. next, previous, etc. see Appendix A for the complete list). 4.6 Task Related Features Since it is possible for different tasks to exhibit different feature vector patterns for the same dialogue act, incorporating task name (T in Table 4) as an additional feature may therefore improve classifiGroup Composition 91 U 92 PUBUS 93 CUBUS 94 CUPUS 95 CUPUB 96 CUPUBUS 97 CUPUBUSUT 98 C U P U B U S U U Table 6: Feature groups. cation performance by exploiting these variations (if any) between tasks. 5 Classification Results All classification tasks were performed using the WEKA toolkit (Hall et al., 2009). The classification experiments were done using Support Vector Machine (frequently used for benchmarking), J48 Decision Tree (appropriate for a small size mostly binary feature set) and Random Forest classifiers. The model parameters for all classifiers were optimized for maximum performance. In addition, experiments were also performed to assess the utility of each feature set (Table 4). Specifically, the performance of classifiers with different combinations (Groups 1-8 in Table 6) of feature sets was evaluated to assess the importance of each individual feature set. We primarily focussed o</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20888" citStr="Klein and Manning, 2003" startWordPosition="3342" endWordPosition="3345">ination of the weights of different words learnt by the SVM classifier trained on a development dataset using unigram features alone. e.g.., the words continue and skip occur much more frequently in Command-Navigation than in other dialogue acts (see Table 5) and hence are included in p,,,a,,,. Note however that not all discriminatory words in Table 5 were used. Only generic words, independent of any specific task, were selected (see Appendix A for details). 4.3 Syntactic Structure of Commands The binary syntactic features (S in Table 4) were automatically extracted using the Stanford parser (Klein and Manning, 2003). As in word-DA correlations, some of the syntactic structure-DA correlations were also identified by a manual in127 Dialogue Act Discriminatory Words Command-Intention want, compose, book, for, look, email, find, an, accounting, Stanford, a, airplane, message, I, music, get, ticket, positions, need, bluetooth, jobs, new Command-Task repeat, choose, delete, select, link, edit, enter, erase, clear, fill, in, click, third, at, body, box, again, blue, that Command-Multiple play, read, senior, send, reviews, Harlem, artists, study, submit, details, law, description, Kitaro, mornings, availability,</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheongjae Lee</author>
<author>Sangkeun Jung</author>
<author>Kyungduk Kim</author>
<author>Donghyeon Lee</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Recent approaches to dialog management for spoken dialog systems.</title>
<date>2010</date>
<journal>JCSE,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="5658" citStr="Lee et al., 2010" startWordPosition="886" endWordPosition="889">re sets (Section 4); 4) investigation of the importance of each feature set with respect to classification performance to assess whether simple lexical/syntactic features are sufficient for obtaining an acceptable performance (Section 5). 2 Related Work While previous research addressed spoken dialogue interfaces for a domain-specific websites, such as news or movie search (Ferreras and Carde˜noso-Payo, 2005; Wang et al., 2014), dialogue interface to generic web sites is a novel task. Spoken dialogue systems (SDS) can be classified by the type of initiative: system, user, or mixed initiative (Lee et al., 2010). In a system-initiative SDS, a system guides a user through a series of information gathering and information presenting prompts. In a user-initiative system, a user can initiate and steer the interaction. Mixed-initiative systems allow both system and user-initiated actions. Dialogue systems also differ in the types of dialogue manager: finite state based, form based, or agent based (Lee et al., 2010), (Chotimongkol, 2008). Finite state and form filling systems are usually system-initiative. These systems have a fixed set of dialogue states and finite set of possible user commands that map t</context>
</contexts>
<marker>Lee, Jung, Kim, Lee, Lee, 2010</marker>
<rawString>Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Donghyeon Lee, and Gary Geunbae Lee. 2010. Recent approaches to dialog management for spoken dialog systems. JCSE, 4(1):1–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyonori Ohtake</author>
<author>Teruhisa Misu</author>
<author>Chiori Hori</author>
<author>Hideki Kashioka</author>
<author>Satoshi Nakamura</author>
</authors>
<title>Annotating dialogue acts to construct dialogue systems for consulting.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th Workshop on Asian Language Resources,</booktitle>
<pages>32--39</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8394" citStr="Ohtake et al. (2009)" startWordPosition="1322" endWordPosition="1325">) can have a general purpose function, such as Inform, Propositional Question, Yes/No Question, and a dimension-specific function in any number of 10 defined dimensions, such as Task, Feedback, or Time management. In the analysis of human-computer dialogues, it is common to adopt DA annotation schemes to suit specific domains. Generic domain-independent schemes are geared towards the analysis of natural human-human dialogue and provide rich annotation structure that can cover complexity of natural dialogue. Domain-specific dialogues use a subset of the generic dialogue structure. For example, Ohtake et al. (2009) developed a DA scheme for tourist-guide domain motivated by a generic annotation scheme (Ohtake et al., 2010), and Bangalore and Stent (2009) created a dialogue scheme for a catalogue product ordering dialogue system. In our work we design DA scheme for Web-Browsing domain motivated by the DAMSL (Core and Allen, 1997) schema for task-oriented dialogue. We used a Wizard-of-Oz (WOZ) approach to collect an initial dataset of spoken voice com124 Task τu τd Shopping 121 16 Email 92 16 Flight 180 16 Hotel 179 16 Job 76 16 Admission 144 16 Overall 792 96 Table 1: Corpus details. Tu - number of utter</context>
</contexts>
<marker>Ohtake, Misu, Hori, Kashioka, Nakamura, 2009</marker>
<rawString>Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, Hideki Kashioka, and Satoshi Nakamura. 2009. Annotating dialogue acts to construct dialogue systems for consulting. In Proceedings of the 7th Workshop on Asian Language Resources, pages 32–39. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyonori Ohtake</author>
<author>Teruhisa Misu</author>
<author>Chiori Hori</author>
<author>Hideki Kashioka</author>
<author>Satoshi Nakamura</author>
</authors>
<title>Dialogue acts annotation for nict kyoto tour dialogue corpus to construct statistical dialogue systems.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="8504" citStr="Ohtake et al., 2010" startWordPosition="1339" endWordPosition="1342">n-specific function in any number of 10 defined dimensions, such as Task, Feedback, or Time management. In the analysis of human-computer dialogues, it is common to adopt DA annotation schemes to suit specific domains. Generic domain-independent schemes are geared towards the analysis of natural human-human dialogue and provide rich annotation structure that can cover complexity of natural dialogue. Domain-specific dialogues use a subset of the generic dialogue structure. For example, Ohtake et al. (2009) developed a DA scheme for tourist-guide domain motivated by a generic annotation scheme (Ohtake et al., 2010), and Bangalore and Stent (2009) created a dialogue scheme for a catalogue product ordering dialogue system. In our work we design DA scheme for Web-Browsing domain motivated by the DAMSL (Core and Allen, 1997) schema for task-oriented dialogue. We used a Wizard-of-Oz (WOZ) approach to collect an initial dataset of spoken voice com124 Task τu τd Shopping 121 16 Email 92 16 Flight 180 16 Hotel 179 16 Job 76 16 Admission 144 16 Overall 792 96 Table 1: Corpus details. Tu - number of utterances, Td - number of dialogs. mands by both blind and sighted users. WOZ is commonly used before building a d</context>
</contexts>
<marker>Ohtake, Misu, Hori, Kashioka, Nakamura, 2010</marker>
<rawString>Kiyonori Ohtake, Teruhisa Misu, Chiori Hori, Hideki Kashioka, and Satoshi Nakamura. 2010. Dialogue acts annotation for nict kyoto tour dialogue corpus to construct statistical dialogue systems. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yury Puzis</author>
<author>Yevgen Borodin</author>
<author>Rami Puzis</author>
<author>Ramakrishnan</author>
</authors>
<title>Predictive web automation assistant for people with vision impairments.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web,</booktitle>
<pages>1031--1040</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<marker>Puzis, Borodin, Puzis, Ramakrishnan, 2013</marker>
<rawString>Yury Puzis, Yevgen Borodin, Rami Puzis, and IV Ramakrishnan. 2013. Predictive web automation assistant for people with vision impairments. In Proceedings of the 22nd international conference on World Wide Web, pages 1031–1040. International World Wide Web Conferences Steering Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Kumar Rangarajan Sridhar</author>
<author>Srinivas Bangalore</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Combining lexical, syntactic and prosodic cues for improved online dialog act tagging.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="9395" citStr="Sridhar et al. (2009)" startWordPosition="1492" endWordPosition="1495">WOZ) approach to collect an initial dataset of spoken voice com124 Task τu τd Shopping 121 16 Email 92 16 Flight 180 16 Hotel 179 16 Job 76 16 Admission 144 16 Overall 792 96 Table 1: Corpus details. Tu - number of utterances, Td - number of dialogs. mands by both blind and sighted users. WOZ is commonly used before building a dialogue system (Chotimongkol, 2008), (Ohtake et al., 2009), (Eskenazi et al., 1999). In previous work on dialogue modelling, Stolcke et al. (2000) used HMM approach to predict dialogue acts in a switchboard human-human dialogue corpus achieving 65% accuracy. Rangarajan Sridhar et al. (2009) applied a maximum entropy classifier on the Switchbord corpus. Using a combination of lexical, syntactic, and prosodic features, the authors achieve accuracy of 72% on that corpus. Following the work of Rangarajan Sridhar et al. (2009), we use supervised classification approach to determine dialogue act on the annotated corpus of human-wizard web-browsing dialogues. 3 Corpus and Annotation In this section, we describe the corpus and the associated dialogue act scheme. The corpus was collected using a WOZ user study with 24 blind participants. Exactly 50% of the participants indicated that the</context>
<context position="16587" citStr="Sridhar et al., 2009" startWordPosition="2627" endWordPosition="2630">d users reported in previous studies, e.g. (Borodin et al., 2010); manual analysis of the corpus; mitigate the effect of noise that is usually present in standard lexical/syntactic feature sets such as n-grams and parse tree rules. Each of the features in C, P, 13 and S were crafted to have a close correspondence to some dialogue act. For example, p,,,a,,, is closely tied to the Command-Navigation dialogue act. 4.1 Unigrams Unigrams (U in Table 4) are one of the commonly used lexical features for training dialogue act classifiers (e.g. (Boyer et al., 2010), (Stolcke et al., 2000), (Rangarajan Sridhar et al., 2009)). Encoding unigrams as features is based on the observation that some words appear more frequently in certain dialogue acts compared to other dialogue acts. For example, approximately 73% of “want” occur in the Command-Intention DA, 100% of “skip” occur in the Command-Navigation DA and approximately 92% of “select” occur in the Command-Task DA. Word-DA corrections can also be automatically identified using SVM classifers trained on unigram features. Table 5 126 Overall Feature Set UNIGRAMS (U) Feature Description Binary U Unigrams N PRESENCE OF WORDS IN COMMANDS (P) Piyou The utterance contai</context>
</contexts>
<marker>Sridhar, Bangalore, Narayanan, 2009</marker>
<rawString>Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore, and Shrikanth Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech &amp; Language, 23(4):407–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Searle</author>
</authors>
<title>Indirect speech acts. Syntax and semantics,</title>
<date>1975</date>
<pages>3--59</pages>
<contexts>
<context position="7264" citStr="Searle (1975)" startWordPosition="1158" endWordPosition="1159">he user can visit any web page. While a users dialogue acts in a form-based or finite state system depends primarily on a dialogue state, in an agent-based system with userinitiative, the space of users dialogue acts at each dialogue state is open. To determine dialogue manager action, it is essential for the system to identify users intent or dialogue act. In this work, we address dialogue act modelling for opendomain voice web browsing as a proof of concept for the system. Dialogue act (DA) annotation schemes for spoken dialogue systems follow theories on speech acts originally developed by Searle (1975). A number of DA annotation schemes have been developed previously (Core and Allen, 1997), (Carletta et al., 1997). Several of dialogue tagging schemes strive to provide domain-independence (Core and Allen, 1997), (Bunt, 2011). Bunt (2011) developed a NIST standardized domain-independent annotation scheme which incorporates elements from the previously developed annotation schemes. It is a hierarchical multi-dimensional annotation scheme. Each functional segment (part of an utterance corresponding to a DA) can have a general purpose function, such as Inform, Propositional Question, Yes/No Ques</context>
</contexts>
<marker>Searle, 1975</marker>
<rawString>John R Searle. 1975. Indirect speech acts. Syntax and semantics, 3:59–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational linguistics,</journal>
<pages>26--3</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Larry Heck</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>Leveraging semantic web search and browse sessions for multi-turn spoken dialog systems.</title>
<date>2014</date>
<contexts>
<context position="5472" citStr="Wang et al., 2014" startWordPosition="854" endWordPosition="857"> 3); 3) experimentation with classifiers capable of identifying the dialogue acts associated with utterances based on combinations of lexical/syntactic, contextual, and task-related feature sets (Section 4); 4) investigation of the importance of each feature set with respect to classification performance to assess whether simple lexical/syntactic features are sufficient for obtaining an acceptable performance (Section 5). 2 Related Work While previous research addressed spoken dialogue interfaces for a domain-specific websites, such as news or movie search (Ferreras and Carde˜noso-Payo, 2005; Wang et al., 2014), dialogue interface to generic web sites is a novel task. Spoken dialogue systems (SDS) can be classified by the type of initiative: system, user, or mixed initiative (Lee et al., 2010). In a system-initiative SDS, a system guides a user through a series of information gathering and information presenting prompts. In a user-initiative system, a user can initiate and steer the interaction. Mixed-initiative systems allow both system and user-initiated actions. Dialogue systems also differ in the types of dialogue manager: finite state based, form based, or agent based (Lee et al., 2010), (Choti</context>
</contexts>
<marker>Wang, Heck, Hakkani-Tur, 2014</marker>
<rawString>Lu Wang, Larry Heck, and Dilek Hakkani-Tur. 2014. Leveraging semantic web search and browse sessions for multi-turn spoken dialog systems.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I piyou</author>
</authors>
<title>you phelp help phelpq, bhelpq how, can, do, am I pprev dynamically determined at runtime pintent want, like, would, need, prefer pbrowser dynamically determined at runtime phtml body, page, form, box, field, search, link, button, list, dropdown pbasic clear, select, fill, delete, click, edit, erase, submit, repeat, choose, enter, check pnbasic any verb not in the pbasic list above pnav, bnav skip,</title>
<booktitle>Screen reader: Verbatim 97458 700 MB 80 Minute ... Disc CD-R by Verbatim link ...</booktitle>
<note>go to, next, first, last, back, continue, previous, stop, go back, finish, home page pquestion, bquestion what, where, why, when, how Subject: “I want to buy CD” Wizard: [enter ‘CDs’ in</note>
<marker>piyou, </marker>
<rawString>piyou I, you phelp help phelpq, bhelpq how, can, do, am I pprev dynamically determined at runtime pintent want, like, would, need, prefer pbrowser dynamically determined at runtime phtml body, page, form, box, field, search, link, button, list, dropdown pbasic clear, select, fill, delete, click, edit, erase, submit, repeat, choose, enter, check pnbasic any verb not in the pbasic list above pnav, bnav skip, go to, next, first, last, back, continue, previous, stop, go back, finish, home page pquestion, bquestion what, where, why, when, how Subject: “I want to buy CD” Wizard: [enter ‘CDs’ in the search box, clicks ‘Go’, and narrate search results one by one] Screen reader: Verbatim 97458 700 MB 80 Minute ... Disc CD-R by Verbatim link ...</rawString>
</citation>
<citation valid="false">
<title>Subject: “Search for new age items” Wizard: [searches for ‘new age items’, clicks on results] Screen reader: Age of Wushu - Free Amazon ... Game Connect link by Snail Games USA ... Subject: “stop” Wizard: [presses pause shortcut] Subject: “new age music CDs” Wizard: [searches for ‘new music CDs’] Screen reader: The ultimate Most Relaxing New Age .... Subject: “new age music CDs by Kitaro” Wizard: [Searches for ‘new music cds by Kitaro’] Screen reader: Most Relaxing New Age Music link by Kitaro, List 8 items, $10.87 link, order in the next 3 hours ... Tenku link by Kitaro ...</title>
<marker></marker>
<rawString>Subject: “Search for new age items” Wizard: [searches for ‘new age items’, clicks on results] Screen reader: Age of Wushu - Free Amazon ... Game Connect link by Snail Games USA ... Subject: “stop” Wizard: [presses pause shortcut] Subject: “new age music CDs” Wizard: [searches for ‘new music CDs’] Screen reader: The ultimate Most Relaxing New Age .... Subject: “new age music CDs by Kitaro” Wizard: [Searches for ‘new music cds by Kitaro’] Screen reader: Most Relaxing New Age Music link by Kitaro, List 8 items, $10.87 link, order in the next 3 hours ... Tenku link by Kitaro ...</rawString>
</citation>
<citation valid="false">
<title>Subject: “sample recordings” Wizard: [follows the current link] Screen reader: Page loading Wizard: [plays the first sample track] Screen reader: [audio sample] Subject: “next” Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “next” Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “go back” Wizard: [presses back button] Screen reader: page loading, [repeats the visited link] Subject: “next CD” Wizard: [clicks the title of the next item in search result] Screen Reader: Ancient link by Kitaro ... $14.98 link ...</title>
<marker></marker>
<rawString>Subject: “sample recordings” Wizard: [follows the current link] Screen reader: Page loading Wizard: [plays the first sample track] Screen reader: [audio sample] Subject: “next” Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “next” Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “go back” Wizard: [presses back button] Screen reader: page loading, [repeats the visited link] Subject: “next CD” Wizard: [clicks the title of the next item in search result] Screen Reader: Ancient link by Kitaro ... $14.98 link ...</rawString>
</citation>
<citation valid="false">
<title>Subject: “listen to audio” Wizard: [follows link] Screen Reader: Page loading Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “next” Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “buy this cd” Wizard: [clicks ‘Add to cart’ button, then clicks ‘Proceed to Checkout’ button] Screen reader: [reads out all captions]</title>
<marker></marker>
<rawString>Subject: “listen to audio” Wizard: [follows link] Screen Reader: Page loading Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “next” Wizard: [plays the next sample track] Screen reader: [audio sample] Subject: “buy this cd” Wizard: [clicks ‘Add to cart’ button, then clicks ‘Proceed to Checkout’ button] Screen reader: [reads out all captions]</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
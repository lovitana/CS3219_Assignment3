<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.851242">
FLORS: Fast and Simple Domain Adaptation for Part-of-Speech Tagging
</title>
<author confidence="0.994224">
Tobias Schnabel Hinrich Schütze
</author>
<affiliation confidence="0.999939">
Department of Computer Science Center for Information &amp; Language Processing
Cornell University University of Munich
</affiliation>
<email confidence="0.996108">
tbs49@cornell.edu inquiries@cislmu.org
</email>
<sectionHeader confidence="0.997336" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99979575">
We present FLORS, a new part-of-speech tag-
ger for domain adaptation. FLORS uses ro-
bust representations that work especially well
for unknown words and for known words with
unseen tags. FLORS is simpler and faster than
previous domain adaptation methods, yet it
has significantly better accuracy than several
baselines.
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995306">
In this paper we describe FLORS, a part-of-speech
(POS) tagger that is Fast in training and tagging,
uses LOcal context only (as opposed to finding the
optimal tag sequence for the entire sentence), per-
forms Robustly on target domains (TDs) in unsu-
pervised domain adaptation (DA) and is Simple in
architecture and feature representation.
FLORS constructs a robust representation of the
local context of the word v that is to be tagged.
This representation consists of distributional fea-
tures, suffixes and word shapes of v and its local
neighbors. We show that it has two advantages.
First, since the main predictors used by FLORS
are distributional features (not the word’s identity),
FLORS predicts unseen tags of known words bet-
ter than prior work on DA for POS. Second, since
FLORS uses representations computed from unla-
beled text, representations of unknown words are
in principle of the same type as representations of
known words; this property of FLORS results in
better performance on unknown words compared to
prior work. These two advantages are especially
beneficial for TDs that contain high rates of unseen
tags of known words and high rates of unknown
words. We show that FLORS achieves excellent DA
tagging results on the five domains of the SANCL
2012 shared task (Petrov and McDonald, 2012) and
outperforms three state-of-the-art taggers on Blitzer
et al.’s (2006) biomedical data.
FLORS is also simpler and faster than other POS
DA methods. It is simple in that the input repre-
sentation consists of three simple types of features:
distributional count features and two types of binary
features, suffix and shape features. Many other word
representations that are used for improving general-
ization (e.g., (Brown et al., 1992; Collobert et al.,
2011)) are costly to train or have difficulty han-
dling unknown words. Our representations are fast
to build and can be created on-the-fly for unknown
words that occur during testing.
The learning architecture is simple and fast as
well. We train k binary one-vs-all classifiers that
use local context only and no sequence informa-
tion (where k is the number of tags). Thus, tag-
ging complexity is O(k). Many other learning se-
tups for DA are more complex; e.g., they learn rep-
resentations (as opposed to just counting), they learn
several classifiers for different subclasses of words
(e.g., known vs. unknown) or they combine left-to-
right and right-to-left taggings.
The next two sections describe experimental data,
setup and results. Results are discussed in Section 4.
We compare FLORS to alternative word representa-
tions in Section 5 and to related work in Section 6.
Section 7 presents our conclusions.
</bodyText>
<sectionHeader confidence="0.964394" genericHeader="method">
2 Experimental data and setup
</sectionHeader>
<bodyText confidence="0.492455">
Data. Our source domain is the Penn Treebank
(Marcus et al., 1993) of Wall Street Journal (WSJ)
</bodyText>
<page confidence="0.988632">
15
</page>
<bodyText confidence="0.974403862500001">
Transactions of the Association for Computational Linguistics, 2 (2014) 15–26. Action Editor: Sharon Goldwater.
Submitted 9/2013; Revised 11/2013; Published 2/2014. c�2014 Association for Computational Linguistics.
text. Following Blitzer et al. (2006), we use sections
2-21 for training and 100,000 WSJ sentences from
1988 as unlabeled data in training.
We evaluate on six different TDs. The first
five TDs (newsgroups, weblogs, reviews, answers,
emails) are from the SANCL shared task (Petrov
and McDonald, 2012). Additionally, the SANCL
dataset contains sections 22 and 23 of the WSJ
for in-domain development and testing, respectively.
Each SANCL TD has an unlabeled training set of
100,000 sentences and development and test sets of
about 1000 labeled sentences each. The sixth TD is
BIO, the Penn BioTreebank data set distributed by
Blitzer. It consists of dev and test sets of 500 sen-
tences each and 100,000 unlabeled sentences.
Classification setup. Similar to SVMTool
(Giménez and Màrquez, 2004) and Choi and Palmer
(2012) (henceforth: C&amp;P), we use local context only
for tagging instead of performing sequence classifi-
cation. For a word w occurring as token vi in a sen-
tence, we build a feature vector for a local window
of size 2l + 1 around vi. The representation of the
object to be classified is this feature vector and the
target class is the POS tag of vi.
We use the linear L2-regularized L2-loss SVM
implementation provided by LIBLINEAR (Fan et
al., 2008) to train k one-vs-all classifiers on the train-
ing set where k is the number of POS tags in the
training set (in our case k = 45). We train with un-
tuned default parameters; in particular, C = 1. In
the special case of linear SVMs, the value of C does
not need to be tuned exhaustively as the solution re-
mains constant after C has reached a certain thresh-
old value C∗ (Keerthi and Lin, 2003). Training can
easily be parallelized by giving each binary SVM its
own thread.
Windows. The local context for tagging token
vi is a window of size 2l + 1 centered around vi:
(vi−l, ... , vi, ..., vi+l). We pad sentences on either
side with (BOUNDARY) to ensure sufficient con-
text for all words. Given a mapping f from words
to feature vectors (see below), the representation F
of a token vi is the concatenation of the 2l + 1 word
vectors in its window
F(vi) = f(vi−l) ED ... ED f(vi+l)
where ED is vector concatenation.
Word features. We represent each word w by
four components: (i) counts of left neighbors, (ii)
counts of right neighbors, (iii) binary suffix features
and (iv) binary shape features. These four compo-
nents are concatenated:
f(w) = fleft(w)EDfright(w)EDfsuffix(w)EDfshape(w)
We consider these sources of information equally
important and normalize each of the four compo-
nent vectors to unit length. Normalization also has
a beneficial effect on SVM training time because it
alleviates numerical problems (Fan et al., 2008).
Distributional features. We follow a long tra-
dition of older (Finch and Chater, 1992; Schütze,
1993; Schütze, 1995) and newer (Huang and Yates,
2009) work on creating distributional features for
POS tagging based on local left and right neighbors.
Specifically, the ith entry xi of fleft(w) is the
weighted number of times that the indicator word
ci occurs immediately to the left of w:
xi = tf (freq (bigram(ci, w)))
where ci is the word with frequency rank i in the cor-
pus, freq (bigram(ci, w)) is the number of times the
bigram “ci w” occurs in the corpus and we weight
the non-zero frequencies logarithmically: tf(x) =
1 + log(x). tf-weighting has been used by other re-
searchers (Huang and Yates, 2009) and showed good
performance in our own previous work.
fright(w) is defined analogously. We restrict the
set of indicator words to the n = 500 most fre-
quent words in the corpus. To avoid zero vectors, we
add an entry xn+1 to each vector that counts omitted
contexts:
</bodyText>
<equation confidence="0.659899333333333">
xn+1 = tf ⎛
⎝freq (bigram(cj, w))
j:j&gt;n
</equation>
<bodyText confidence="0.987707">
We compute distributional vectors on the joint
corpus DALL of all labeled and unlabeled text of
source domain and TD. The text is preprocessed by
lowercasing everything – which is often done when
computing word representations, e.g., by Turian
et al. (2010) – and by padding sentences with
(BOUNDARY) tokens.
Suffix features. Suffixes are promising for DA
because basic morphology rules are the same in dif-
ferent domains. In contrast to other work on tagging
</bodyText>
<page confidence="0.995063">
16
</page>
<table confidence="0.646259583333333">
model classifier
1 TnT HMM
2 Stanford bidir. MEMM
3 SVMTool SVM
4 C&amp;P SVM
5 FLORS SVM
features
p−10,1,21, v0, suffixes (for OOVs)
pf10,1,21, vf10,11, affixes, orthography
pf10,1,2,31, vf10,1,2,31, affixes, orthography, word length
pf10,1,2,31, vf10,1,2,31, affixes, orthography
distributions of vf10,1,21, suffixes, orthography
</table>
<tableCaption confidence="0.98944875">
Table 1: Overview of baseline taggers and FLORS. vi: token, pi: POS tag. Positions included in the sets of token
indices are relative to the position i of the word v0 to be tagged; e.g., pf10,1,21 is short for {p−0, p−1, p−2, p0, p1, p21.
To represent tokens vi, models 1–4 use vocabulary indices and FLORS uses distributional representations. Models
2–4 use combinations of features (e.g., tag-word) as well.
</tableCaption>
<bodyText confidence="0.99982025">
(e.g., Ratnaparkhi (1996), Toutanova et al. (2003),
Miller et al. (2007)) we simply use all (lowercase)
suffixes to avoid the need for selecting a subset of
suffixes; and we treat all words equally as opposed
to using suffix features for only a subset of words.
For suffix s, we set the dimension corresponding to
s in fsuffix(w) to 1 if lowercased w ends in s and to
0 otherwise. Note that w is a suffix of itself.1
Shape features. We use the Berkeley parser word
signatures (Petrov and Klein, 2007). Each word
is mapped to a bit string encompassing 16 binary
indicators that correspond to different orthographic
(e.g., does the word contain a digit, hyphen, upper-
case character) and morphological (e.g., does the
word end in -ed or -ing) features. There are 50
unique signatures in WSJ. We set the dimension of
fshape(w) that corresponds to the signature of w to 1
and all other dimensions to 0. We note that the shape
features we use were designed for English and prob-
ably would have to be adjusted for other languages.
Baselines. We address the problem of unsuper-
vised domain adaptation for POS tagging. For this
problem, we consider three types of baselines: (i)
high-performing publicly available systems, (ii) the
taggers used at SANCL and (iii) POS DA results
published for BIO.
Most of our experiments use taggers from cate-
gory (i) because we can ensure that experimental
conditions are directly comparable. The four base-
lines in category (i) are shown in Table 1. Three
have near state-of-the-art performance on WSJ:
SVMTool (Giménez and Màrquez, 2004), Stanford
</bodyText>
<footnote confidence="0.92735575">
1One could also compute these suffixes for _w (w prefixed
by underscore) instead of for w to include words as distinguish-
able special suffixes. We test this alternative in Table 2, line
15.
</footnote>
<bodyText confidence="0.987622466666667">
(Toutanova et al., 2003) (a birectional MEMM) and
C&amp;P. TnT (Brants, 2000) is included as a represen-
tative of fast and simple HMM taggers. In addition,
C&amp;P is a tagger that has been extensively tested in
DA scenarios with excellent results. Unless other-
wise stated, we train all models using their default
configuration files. We use the optimized parameter
configuration published by C&amp;P for the C&amp;P model.
Test set results will be compared with the SANCL
taggers (category (ii)) at the end of Section 3.
As far as category (iii) is concerned, most work
on POS DA has been evaluated on BIO. We discuss
our concerns about the BIO evaluation sets in Sec-
tion 4, but also show that FLORS beats previously
published results on BIO as well (see Table 6).
</bodyText>
<sectionHeader confidence="0.99589" genericHeader="method">
3 Experimental results
</sectionHeader>
<bodyText confidence="0.982513052631579">
We train k binary SVM classifiers on the training
set. A token in the test set is classified by building
its feature vector, running the classifiers on it and
then assigning it to the POS class whose one-vs-all
LIBLINEAR classifier returns the largest score.
Results for ALL accuracy (accuracy for all to-
kens) and OOV accuracy (accuracy for tokens not
occurring in the labeled WSJ data) are reported in
Table 2. Results with an asterisk are significantly
worse than a column’s best result using McNemar’s
test (p &lt; .001). We use the same test and p-value
throughout this paper.
The basic FLORS model (Table 2, line 5) uses
window size 5 (l = 2). Each word in the window
has 1002 distributional features (501 left and right),
91,161 suffix features and 50 shape features. The
final feature vector for a token has a dimensionality
of about 500,000, but is very sparse.
FLORS outperforms all baselines on the five TDs
</bodyText>
<page confidence="0.99613">
17
</page>
<table confidence="0.413339">
newsgroups reviews weblogs answers emails wsj
ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV
1 TnT 88.66* 54.73* 90.40* 56.75* 93.33* 74.17* 88.55* 48.32* 88.14* 58.09* 95.75* 88.30
2 Stanford 89.11* 56.02* 91.43* 58.66* 94.15* 77.13* 88.92* 49.30* 88.68* 58.42* 96.83 90.25
3 SVMTool 89.14* 53.82* 91.30* 54.20* 94.21* 76.44* 88.96* 47.25* 88.64* 56.37* 96.63 87.96
4 C&amp;P 89.51* 57.23* 91.58* 59.67* 94.41* 78.46* 89.08* 48.46* 88.74* 58.62* 96.78 88.65
5 FLORS basic 90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.15 89.44* 62.61 96.59 90.37
6 n = 250 90.93 67.03 92.93 75.45 94.69 83.69 90.29 62.20 89.63 63.43 96.56* 89.45
7 n = 0 89.14* 55.59* 91.80* 66.31* 93.40* 72.55* 89.47* 55.82* 88.21* 57.83* 96.29* 85.55*
8 no suffixes 90.60 65.17 92.74 71.94* 94.77 84.92 89.77* 58.71* 89.30* 62.09 96.28* 88.88
9 vno shapes 89.70* 63.10* 92.24* 68.70* 92.60* 74.72* 89.55* 59.08* 89.63 64.17 95.52* 83.94*
10 N n = 0 90.61 * 65.95 92.76* 75.56 94.62 84.62 90.23 61.87 89.40* 63.82 96.51 * 90.02
11 no suffixes 90.66 64.78* 92.88 75.08 94.83 84.52 90.36 61.92 89.42 62.74 96.64 89.45
12 vno shapes 90.74 67.03 93.02 75.88 94.57 83.83 90.23 61.73 89.41* 63.49 96.57 90.25
13 l = 1 90.44* 63.62* 92.69* 75.72 94.48* 84.03 90.02* 62.66 89.17* 62.71 96.44* 88.65
14 L-to-R 90.56* 66.08 92.97 75.40 94.57 83.79 90.43 62.80 89.43 63.13 96.53* 90.94
15 voc. indices 90.93 66.64 92.91 75.03 94.71 84.08 90.27 61.92 89.37* 62.26 96.63 90.60
</table>
<tableCaption confidence="0.473297571428571">
Table 2: Tagging accuracy of four baselines and FLORS on the dev sets. The table is structured as follows: baselines
(lines 1–4), basic FLORS setup (lines 5–6), effect of omitting one of the three feature types if the word to be tagged
is changed compared to the basic FLORS setup (lines 7–9) and if the word to be tagged is not changed compared to
basic FLORS (lines 10–12), effect of three important configuration choices on tagging accuracy: window size (line
13), inclusion of prior tagging decision (line 14) and vocabulary index (line 15). n: number of indicator words. 2l + 1:
size of the local context window. Lines 10–12: Only the neighbors of vo are modified compared to basic (line 5).
Lines 7–9: All five token representations (including vo) are modified. A column’s best result is bold.
</tableCaption>
<bodyText confidence="0.99989866">
(line 5 vs. lines 1–4). Only in-domain on WSJ, three
baselines are slightly superior. The baselines are
slightly better on ALL accuracy because they were
designed for tagging in-domain data and use feature
sets that have been found to work well on the source
domain. Generally, C&amp;P performs best for DA
among the baselines. On answers and WSJ, how-
ever, Stanford has better overall accuracies. These
results are in line with C&amp;P.
On lines 6–15, we investigate how different mod-
ifications of the basic FLORS model affect perfor-
mance. First, we examine the effect of leaving out
components of the representation: distributional fea-
tures (fleft(w), fright(w)), suffixes (fsuffix(w)) and
shape features (fshape(w)).
Distributional features boost performance in all
domains: ALL and OOV accuracies are consistently
worse for n = 0 (line 7) than for n ∈ {250, 500}
(lines 6&amp;5). FLORS with n = 250 has better OOV
accuracies in 5 of 6 domains. However, ALL accu-
racy for FLORS with n = 500 is better in the major-
ity of domains. The main result of this comparison
is that FLORS does not seem to be very sensitive to
the value of n if n is large enough.
Shape features also improve results in all do-
mains, with one exception: emails (lines 9 vs 5).
For emails, shape features decrease ALL accuracy
by .19 and OOV accuracy by 1.56. This may be due
to the fact that many OOVs are NNP/NN and that
tagging conventions for NNP/NN vary between do-
mains. See Section 4 for discussion.
Performance benefits from suffixes in all domains
but weblogs (lines 8 vs 5). Weblogs contain many
foreign names such as Abdul and Yasim. For these
words, shapes apparently provide better informa-
tion for classification than suffixes. ALL accura-
cies suffer little when leaving out suffixes, but the
feature space is much smaller: about 3000 dimen-
sions. Thus, for domains where we expect few
OOVs, omitting suffix features could be considered.
Lines 7–9 omit one of the components of f(vi)
for all five words in the local context: i ∈
{−2, −1, 0,1, 2}. Lines 10–12 omit the same com-
ponents for the neighbor words only – i.e., i ∈
{−2,−1, 1, 2} – and leave f(v0) unchanged. 14 of
the 6 × 3 ALL accuracies on lines 10–12 are worse
than FLORS basic, 4 are better. The largest differ-
ences are .25 for newsgroups and .19 for reviews
(lines 5 vs 10), but differences for the other domains
are negligible. This shows that the most important
</bodyText>
<page confidence="0.9952">
18
</page>
<bodyText confidence="0.99997532">
feature representation is that of v0 (not surprisingly)
and that the distributional features of the other words
can be omitted at the cost of some loss in accuracy if
a small average number of active features is desired.
Another FLORS parameter is the size of the local
context. Surprisingly, OOV accuracies benefit a bit
in four domains if we reduce l from 2 to 1 (lines 13
vs 5). However, ALL accuracy consistently drops in
all six domains. This argues for using l = 2, i.e., a
window size of 5.
Results for left-to-right (L-to-R) tagging are given
on line 14. Similar to SVMTool and C&amp;P, each sen-
tence is tagged from left to right and previous tag-
ging decisions are used for the current classification.
In this setting, we use the previous tag pi−1 as one
additional feature in the feature vector of vi.
The effect of left-to-right is similar to the effect
of omitting suffixes: OOV accuracies go up in some
domains, but ALL accuracies decrease (except for
an increase of .02 for reviews). This is in line with
the experiments in (Schnabel and Schütze, 2013)
where sequential information in a CRF was not ro-
bust across domains. OOV tagging may benefit from
correct previous tags because the larger left context
that is indirectly made available by left-to-right tag-
ging compensates partially for the lack of informa-
tion about the OOV word.
In contrast to standard approaches to POS tag-
ging, the FLORS basic representation does not con-
tain vocabulary indices. Line 15 shows what hap-
pens if we add them; the dimensionality of the fea-
ture vector is increased by 51V I – where V is the
training set vocabulary – and in training one binary
feature is set to one for each of the five local con-
text words. Performance is almost indistinguishable
from FLORS basic, suggesting that only using suf-
fixes – which can be viewed as “ambiguous” vocab-
ulary indices, e.g., “at” is on for “at”, “mat”, “hat”,
“laundromat” etc – is sufficient.
In summary, we find that distributional features,
word signatures and suffixes all contribute to suc-
cessful POS DA. Factors with only minor impact
on performance are the number of indicator words
used for the distributional representations, the win-
dow size l and the tagging scheme (L-to-R vs. non-
L-to-R). Unknown words and known words behave
differently with respect to certain feature choices.
The different behavior of unknown and known
words suggests that training and optimizing two sep-
arate models – an approach used by SVMTool –
would further increase tagging accuracy. Note that
there has been at least one publication (Schnabel and
Schütze, 2013) on optimizing a separate model for
unknown words that has in some cases better per-
formance on OOV accuracy than what we publish
here.2 However, this would complicate the architec-
ture of FLORS. We opted for a maximally simple
model in this paper, potentially at the cost of some
performance.
Test set results. Table 3 reports results on the test
sets. FLORS again performs significantly better on
all five TDs, both on ALL and OOV. Only in-domain
on WSJ, ALL performance is worse.
Finally, we compare our results to the POS
taggers for which performance was reported at
SANCL 2012 (Petrov and McDonald, 2012, Ta-
ble 4). Constituency-based parsers – which also
tag words as a by-product of deriving complete
parse trees – are excluded from the comparison be-
cause they are trained on a richer representation, the
syntactic structure of sentences.3 FLORS’ results
are better than the best non-parsing-based results
at SANCL 2012, which were accuracies of 92.32
on newsgroups (HIT), 90.65 on reviews (HIT) and
91.07 on answers (IMS-1).
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9998936">
Advantages of FLORS representation. As we can
see in Table 1, the main representational difference
between FLORS and the other taggers is that the
FLORS representation does not include vocabulary
indices of the word to be tagged or its neighbors
– the FLORS vector only consists of distributional,
suffix and shape features.
This is an obvious advantage for OOVs. In other
representational schemes, OOVs have representa-
tions that are fundamentally different from known
</bodyText>
<footnote confidence="0.845755">
2Schnabel and Schütze (2013) report OOV accuracies of
56.62 (newsgroups), 64.61 (reviews), 71.86 (weblogs), 54.28
(answers), 61.05 (emails) and 64.64 (BIO) for their basic model
and even higher OOV accuracies if parameters are optimized on
a per-domain basis.
3DCU-Paris13 is listed in the dependency parser tables, but
DCU-Paris13 results are derived from a constituency parser.
DCU also developed sophisticated preprocessing rules for the
different domains, which can be viewed as a kind of manual
domain adaptation.
</footnote>
<page confidence="0.971988">
19
</page>
<table confidence="0.999538571428571">
newsgroups reviews weblogs answers emails wsj
ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV
1 TnT 90.85* 56.60* 89.67* 50.98* 91.37* 62.65* 89.36* 51.82* 87.38* 55.12* 96.57* 86.27
2 Stanford 91.25* 57.96* 90.30* 51.87* 92.32* 67.85* 89.74* 53.41* 87.77* 57.10* 97.43 88.71
3 SVMTool 91.21* 54.40* 90.01* 45.05* 92.05* 63.59* 89.90* 51.07* 87.74* 53.23* 97.26 86.47
4 C&amp;P 91.68* 60.58* 90.42* 51.12* 92.22* 66.91* 89.90* 53.31* 87.91* 54.47* 97.44 88.20
5 FLORS basic 92.41 66.91 92.25 70.87 93.14 75.32 91.17 67.93 88.67 61.09 97.11* 87.79
</table>
<tableCaption confidence="0.996154">
Table 3: Tagging accuracy of four baselines and FLORS on the test sets.
</tableCaption>
<table confidence="0.9997568">
newsgroups reviews weblogs answers emails wsj bio
unknown tag 0.31 0.06 0.00 0.25 0.80 0.00 0.98
oke OOV 10.34 6.84 8.45 8.53 10.56 2.72 19.86
unseen word+tag 2.44 2.22 1.46 2.91 3.47 0.61 2.50
TnT 0.00 0.00 0.00 0.00 0.00 0.00 0.00
on 3.66 5.74 9.40 5.46 2.77 15.23 4.64
Stanford 0.00 0.16 0.00 0.00 0.10 0.00 0.00
seed+ SVMTool 14.47 14.75 20.51 13.37 10.29 38.07 8.98
cc ow C&amp;P 21.06 21.97 21.65 17.19 15.13 41.12 12.69
FLORS basic
</table>
<tableCaption confidence="0.953215">
Table 4: Top: Percentage of unknown tags, OOVs and unseen word+tag combinations (i.e., known words tagged with
unseen tags) in the dev sets. Bottom: Tagging accuracy on unseen word+tag.
</tableCaption>
<bodyText confidence="0.999915884615385">
words – since their vocabulary index does not oc-
cur in the training set and cannot be used for predic-
tion. In contrast, given enough unlabeled TD data,
FLORS represents known and unknown words in es-
sentially the same way and prediction of the correct
tag is easier. This explanation is supported by the
experiments in Table 2: FLORS beats all other sys-
tems on OOVs – even in-domain on WSJ.
In our analysis we found that apart from better
handling of OOVs there is a second beneficial ef-
fect of distributional representations: they facilitate
the correct tagging of known words occurring with
tags unseen in the training set, which we call un-
seen word+tags. Table 4 gives statistics on this case
and shows that unseen word+tags occur at least two
times as often out-of-domain (e.g., 1.46% for we-
blogs) than in-domain (.61% for WSJ). The bottom
part of the table shows performance of the five tag-
gers on unseen word+tags. FLORS is the top per-
former on all seven domains, with large differences
of more than 5% in some domains.
The explanation is similar to the OOV case:
FLORS does not restrict the set of possible POS’s of
a word. The other taggers in Table 2 use the vocabu-
lary index of the word to be tagged and will therefore
give a strong preference to seen tags. Since FLORS
uses distributional features, it can more easily assign
an unseen tag as long as it is compatible with the
overall pattern of distribution, suffixes and shapes
typical of the tag. C&amp;P also perform relatively well
on unseen word+tag due to the ambiguity classes in
their model, but FLORS representations are better
for every domain. We take these results to mean that
constraints on a word’s possible POS tags may well
be helpful for in-domain data, but for out-of-domain
data an overly strong bias for a word’s observed tags
is harmful.
It is important to stress that representations sim-
ilar to FLORS representations have been used for
a long time; we would expect many of them to
have similar advantages for unseen word+tags. E.g.,
Brown clusters (Brown et al., 1992) and word em-
beddings (Collobert et al., 2011) are similar to
FLORS in this respect. However, FLORS represen-
tations are extracted by simple counting whereas the
computation of Brown clusters or word embeddings
is much more expensive. The speed with which
FLORS representations can be computed is partic-
ularly beneficial when taggers need to be adapted
to new domains. FLORS can easily adapt its rep-
resentations on the fly – as each new occurrence of
a word is encountered, the counts that are the basis
</bodyText>
<page confidence="0.973733">
20
</page>
<bodyText confidence="0.9998335">
for the xi can simply be incremented. We present
a direct comparison of FLORS representations with
other representations in Section 5.
“Local context” vs. sequence classification. The
most common approach to POS tagging is to tag a
sentence with its most likely sequence; in contrast,
independent tagging of local context is not guaran-
teed to find the best sequence. Recent work on En-
glish suggests that window-based tagging can per-
form as well as sequence-based methods (Liang et
al., 2008; Collobert et al., 2011). Toutanova et al.
(2003) report similar results. In our experiments,
we also did not find consistent improvements when
we incorporated sequence constraints (Table 2, line
14). However, there may be languages and appli-
cations involving long-distance relationships where
local-context classification is suboptimal.
Local-context classification has two advantages
compared to sequence classification. (i) It simplifies
the classification and tagging setup: we can use any
existing statistical classifier. Sequence classification
limits the range of methods that can be applied; e.g.,
it is difficult to find a good CRF implementation that
can handle real-valued features – which are of criti-
cal importance for our representation.
(ii) The time complexity of FLORS in tagging is
O(skf) where s is the length of the sentence, k is the
number of tags and f is the number of non-zero fea-
tures per local-context representation. In contrast,
sequence decoding complexity is O(sk2f). This
difference is not of practical importance for stan-
dard English POS sets, but it could be an argument
against sequence classification for tagging problems
with much larger tag sets.
In summary, replacing sequence classification
with local-context classification is attractive for
large-scale, practical tagging.
What DA can and cannot do. Despite the supe-
rior DA tagging results we report for FLORS in this
paper, there is still a gap of 2%–7% (depending on
the domain) between in-domain WSJ accuracy and
DA accuracy on SANCL. In our analysis of this gap,
we found some evidence that DA performance can
be further improved – especially as more unlabeled
TD data becomes available. But we also found two
reasons for low performance that unsupervised DA
cannot do anything about: differences in tag sets – or
unknown tags – and differences in annotation guide-
lines.
Table 4 shows that unknown tags occur in five of
the seven TDs at rates between 0% (weblogs) and
1% (BIO). Each token that is tagged with an un-
known tag is necessarily an error in unsupervised
DA. Furthermore, the unknown tag can also im-
pact tagging accuracy in the local context4 – so the
unknown tag rates in Table 4 are probably lower
bounds for the error that is due to unknown tags.
Based on these considerations, it is not surprising
that tagging accuracy (e.g., of FLORS basic) and
unknown tag rate are correlated as we can see in Ta-
bles 2, 4 and 6; e.g., we get the highest accuracies
in the two domains that do not have unknown tags
(weblogs and WSJ) and the lowest accuracy in the
domain with the highest rate (BIO).
Since unknown tags cannot be predicted correctly,
one could simply report accuracy on known tags.
However, given the negative effect of unknown tags
on tagging accuracy of the local context in which
they occur, excluding unknown tags does not fully
address the problem. For this reason, it is probably
best to keep the common practice of simply report-
ing accuracy on all tokens, including unknown tags.
But the percentages of unknown tags should also be
reported for each dataset as a basis for a more accu-
rate interpretation of results.
Another type of error that cannot be avoided in
unsupervised DA is due to differences in annota-
tion guidelines. There are a few such problems in
SANCL; e.g., file names like “Services.doc” are an-
notated as NN in the email domain. But their dis-
tributional and grammatical behavior is more simi-
lar to NNPs; as a consequence, most file names are
incorrectly tagged. In general, it is difficult to dis-
criminate NNs from NNPs. The Penn Treebank an-
notation guidelines (Santorini, 1990) are compatible
with either tag in many cases and it may simply be
impossible to write annotation guidelines that avoid
these problems (cf. Manning (2011)). NN-NNP in-
consistencies are especially problematic for OOV
tagging since most OOVs are NNs or NNPs.
</bodyText>
<footnote confidence="0.758960333333333">
4For example, there is a special tag ADD in the web do-
main for web addresses. The last two words of the sentence
“I would like to host my upcoming website to/IN Liquid-
web.com/ADD” are mistagged by Stanford tagger as “... to/TO
Liquidweb.com/VB”. So the missing tag in this case also affects
the tagging of surrounding words.
</footnote>
<page confidence="0.996181">
21
</page>
<table confidence="0.999131428571429">
bio dev wsj train
OOV ALL ALL
NN 62.4 25.4 14.4
JJ 15.9 8.9 6.2
NNS 10.2 7.5 6.3
NNP 0.5 0.2 9.5
NNPS 0.0 0.0 0.3
</table>
<tableCaption confidence="0.984113">
Table 5: Frequency of some tags (percent of tokens) for
bio dev and wsj train.
</tableCaption>
<bodyText confidence="0.977655818181818">
While the amount of inconsistent annotation is
limited for SANCL, it is a serious problem for BIO.
Table 5 shows that the proportion of NNPs in BIO
is less than a tenth of that in WSJ (.2 in BIO vs.
9.5 in WSJ). This is due to the fact that many bio-
specific names, in particular genes, are annotated as
NN. In contrast, the distributionally and orthograph-
ically most similar names in WSJ are tagged as NNP.
For example, we find “One cell was teased out, and
its DNA/NNP extracted” in WSJ vs. “DNA/NN was
isolated” in BIO.
</bodyText>
<table confidence="0.955399833333333">
standard setup NNP→NN
ALL OOV ALL OOV
TnT 87.49* 59.08* 91.75* 78.33*
Stanford 88.46* 62.55* 92.36* 79.19*
SVMTool 88.33* 61.30* 92.47 79.46*
C&amp;P 87.82* 60.60* 92.06* 79.30*
basic 88.90 64.74 92.91 82.58
n = 250 88.90 64.51 92.93 82.47
n = 0 87.27* 57.75* 90.91* 73.57*
no suffixes 88.09* 62.20* 91.98* 79.27*
no shapes 87.78* 59.82* 91.81* 77.31*
l = 1 89.12 65.52 92.99 82.90
</table>
<tableCaption confidence="0.9796185">
Table 6: Tagging accuracy on bio dev. NNP→NN results
were obtained by replacing NNPs with NNs.
</tableCaption>
<bodyText confidence="0.99996064516129">
Given this large discrepancy in the frequency of
the tag NNP – which arguably is due to different
annotation guidelines, not due to underlying differ-
ences between the two genres – BIO should proba-
bly not be used for evaluating DA. This is why we
did not include it in our comparison in Table 2.
For sake of completeness, we provide tagging ac-
curacies for BIO in Table 6, “standard setup”. The
results are in line with SANCL results: FLORS
beats the baselines on ALL and OOV accuracies.
However, if we build the NN bias into our model
by simply replacing all NNP tags with NN tags, then
accuracy goes up by 4% on ALL and by almost 20%
on OOV. Even TnT, the most basic tagger, achieves
ALL/OOV accuracy of 91.75/78.33, better than any
method in the standard setup. These accuracies are
well above those in (Blitzer et al., 2006) and (Huang
and Yates, 2010).
Since simply replacing NNPs with NNs has such
a large effect, BIO cannot be used sensibly for eval-
uating DA methods. In practice, it is not possible
to separate “true” improvements due to generic bet-
ter DA from elements of the proposed method that
simply introduce a negative bias for NNP.
In summary, when comparing different DA meth-
ods caution should be exercised in the choice of do-
mains. In particular, the effect of unknown tags
should be made transparent and the gold standards
should be analyzed to determine whether the task
addressed in the TD differs significantly in some as-
pects from that addressed in the source domain.
</bodyText>
<sectionHeader confidence="0.722565" genericHeader="method">
5 Comparison of word representations
</sectionHeader>
<bodyText confidence="0.999862625">
Our approach to DA is an instance of representation
learning: we aim to find representations that are ro-
bust across domains. In this section, we compare
FLORS with two other widely used representation
learning methods: (i) Brown clusters (Brown et al.,
1992) and (ii) C&amp;W embeddings, the word embed-
dings of Collobert et al. (2011). We use fdist(w) =
fleft(w)⊕fright(w) to refer to our own distributional
word representations (see Section 2).
The perhaps oldest and most frequently used low-
dimensional representation of words is based on
Brown clusters. Typically, prefixes of Brown clus-
ters (Brown et al., 1992) are added to increase the
robustness of POS taggers (e.g., Toutanova et al.
(2003)). Computational costs are high (quadratic in
the vocabulary size) although the computation can
be parallelized (Uszkoreit and Brants, 2008).
More recently, general word representations (Col-
lobert et al., 2011; Turian et al., 2010) have been
used for robust POS tagging. These word represen-
tations are typically trained on a large amount of un-
labeled text and fine-tuned for specific NLP tasks.
Similar to Brown clusters, they are low-dimensional
and can be used as features in many NLP tasks, ei-
</bodyText>
<sectionHeader confidence="0.217787" genericHeader="method">
FLORS
</sectionHeader>
<page confidence="0.978057">
22
</page>
<bodyText confidence="0.99993452631579">
ther alone or in combination with other features.
To compare fdist(w) (our distributional repre-
sentations) with Brown clusters, we induced 1000
Brown clusters on the joint corpus data DALL (see
Section 2) using the publicly available implemen-
tation of Liang (2005). We padded sentences with
(BOUNDARY) tokens on each side and used path
prefixes of length 4, 6, 10 and 20 as features for
each word (cf. Ratinov and Roth (2009), Turian et
al. (2010)).
C&amp;W embeddings are provided by Collobert et al.
(2011): 50-dimensional vectors for 130,000 words
from WSJ, trained on Wikipedia. Similar to our dis-
tributional representations fdist(w), the embeddings
also contain a (BOUNDARY) token (which they
call PADDING). Moreover, they have a special em-
bedding for unknown words (called UNKNOWN)
which we use whenever we encounter a word that
is not in their lookup table. We preprocess our raw
tokens the same way they do (lowercase and replace
sequences of digits by “0”) before we look up a rep-
resentation during training and testing.
We replaced the distributional features in our ba-
sic setup by either Brown cluster features or C&amp;W
embeddings. Table 7 repeats lines 5 and 7 of Table 2
and gives results of the modified FLORS setup.
All three representations improve both ALL and
OOV accuracies in all domains. fdist outperforms
Brown in all cases except for OOV on emails.
Brown may suffer from noisy data; cleaning meth-
ods have been used in the literature (Liang, 2005;
Turian et al., 2010), but they are not unproblematic
since a large part of the data available is lost, which
results in more unknown words.
Brown and fdist can be directly compared since
they were trained on exactly the same data. fdist
and C&amp;W are harder to compare directly because
there are many differences. (i) C&amp;W is trained on
a much larger dataset. One consequence of this is
that OOV accuracy on WSJ may be higher because
some words that are unknown for other methods are
actually known to C&amp;W. (ii) C&amp;W vectors are not
trained on the SANCL TD data sets – this gives fdist
an advantage. (iii) C&amp;W vectors are not trained on
the WSJ. Again, this could give fdist an advantage.
(iv) C&amp;W and fdist are fundamentally different in the
way they handle unknown words. C&amp;W has a lim-
ited vocabulary and must replace all words not in
this vocabulary by the token UNKNOWN. In con-
trast, fdist can create a meaningful individual repre-
sentation for any OOV word it encounters.
Our FLORS tagger provides best ALL accuracies
in all domains but WSJ, where C&amp;W has best re-
sults. The good performance of C&amp;W is rather un-
surprising since the embeddings were created for the
130,000 most frequent words of the WSJ and thus
cover the WSJ domain much better. Also, WSJ
was used to tune parameters during development.
As with our previous experiments, OOV results on
emails seem slightly more sensitive to parameter
choices than on other domains (recall the discussion
of this issue in Section 4).
In summary, we have shown that fdist represen-
tations work better for POS DA than Brown clus-
ters. Furthermore, the evidence we have presented
suggests that fdist are comparable in performance to
C&amp;W embeddings if not better for POS DA.
The most important difference between fdist and
Brown / C&amp;W is that fdist are much simpler and
much faster to compute. They are simpler because
they are just slightly transformed counts in contrast
to the other two approaches, which solve complex
optimization problems. fdist can be computed effi-
ciently through simple incrementation in one pass
through the corpus. In contrast, the other two ap-
proaches are an order of magnitude slower.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999573705882353">
Unsupervised DA methods can be broadly put
into four categories: representation learning and
constraint-based frameworks – which require some
tailoring to a task – and instance weighting and boot-
strapping – which can be more generally applied to a
wide range of problems. Since many approaches are
application-specific, we focus on the ones that have
been applied to POS tagging.
Representation learning. We already discussed
two important approaches to representation learning
in Section 5: C&amp;W embeddings and Brown clusters.
Blitzer et al.’s (2006) structural correspondence
learning (SCL) supports DA by creating similar
representations for correlated features in the pivot
feature space. This is a potentially powerful
method. FLORS is simpler in that correlations are
made directly accessible to the supervised learner.
</bodyText>
<page confidence="0.992142">
23
</page>
<table confidence="0.996780666666667">
newsgroups reviews weblogs answers emails wsj
ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV
1 S fdist(w), n=500 90.86 66.42 92.95 75.29 94.71 83.64 90.30 62.15 89.44 62.61 96.59 90.37
2R fdist(w), n=0 89.14* 55.59* 91.80* 66.31* 93.40* 72.55* 89.47* 55.82* 88.21* 57.83* 96.29* 85.55*
3FLO C&amp;W for fdist(w) 90.57 64.57 92.54* 72.48* 94.51 80.58* 90.23 60.99 89.44 63.13 96.72 90.48
4 Brown for fdist(w) 90.34* 62.41* 92.23* 71.47* 94.45 81.76 89.71* 56.28* 89.02* 63.20 96.48* 87.50
</table>
<tableCaption confidence="0.991307">
Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n:
number of indicator words. A column’s best result is bold.
</tableCaption>
<bodyText confidence="0.998491487179487">
Moreover, FLORS representations consist of simple
counts whereas SCL solves a separate optimization
problem for each pivot feature.
Umansky-Pesin et al. (2010) derive distributional
information for OOVs by running web queries. This
approach is slow since it depends on a search engine.
Ganchev et al. (2012) successfully use search
logs. This is a promising enhancement for FLORS.
Huang and Yates (2009) evaluate CRFs with dis-
tributional features. They examine lower dimen-
sional feature representations using SVD or the la-
tent states of an unsupervised HMM. They find bet-
ter accuracies for their HMM method than Blitzer
et al. (2006); however, they do not compare them
against a CRF baseline using distributional features.
In later work, Huang and Yates (2010) add the la-
tent states of multiple, differently trained HMMs as
features to their CRF. Huang and Yates (2012) ar-
gue that finding an optimal feature representation
is computationally intractable and propose a new
framework that allows prior knowledge to be inte-
grated into representation learning.
Latent sequence states are a form of word repre-
sentation. Thus, it would be interesting to compare
them to the non-sequence-based distributional rep-
resentation that FLORS uses.
Constraint-based methods. Rush et al. (2012)
use global constraints on OOVs to improve out-of-
domain tagging. Although constraints ensure con-
sistency, they require careful manual engineering.
Distributional features can also be seen as a form
of constraint since feature weights will be shared
among all words.
Subramanya et al. (2010) construct a graph to en-
courage similar n-grams to be tagged similarly, re-
sulting in moderate gains in one domain, but no
gains on BIO when compared to self-training. The
reason could be an insufficient amount of unsuper-
vised data for BIO (100,000 sentences). Our ap-
proach does not seem to suffer from this problem.
Bootstrapping. Both self-training (McClosky et
al., 2006) – which uses one classification model –
and co-training (Blum and Mitchell, 1998) – which
uses ≥2 models – have been applied to POS tagging.
Self-training usually improves a POS baseline
only slightly if at all (Huang et al., 2009; Huang and
Yates, 2010). Devising features based on labeled in-
stances (instead of training on them) has been more
successful (Florian et al., 2004; Søgaard, 2011).
Chen et al. (2011) use co-training for DA. In each
round of their algorithm, both new training instances
from the unlabeled data and new features are added.
Their model is limited to binary classification. The
co-training method of Kübler and Baucom (2011)
trains several taggers and adds sentences from the
TD to the training set on which they agree. They
report slight, but statistically significant increases in
accuracy for POS tagging of dialogue data.
Instance weighting. Instance weighting formal-
izes DA as the problem of having data from differ-
ent probability distributions in each domain. The
goal is to make these two distributions align by us-
ing instance-specific weights during training. Jiang
and Zhai (2007) propose a framework that integrates
prior knowledge from different data sets into the
learning objective by weights.
In related work, C&amp;P train generalized and
domain-specific models. An input sentence is tagged
by the model that is most similar to the sentence.
FLORS could be easily extended along these lines,
an experiment we plan for the future.
In terms of the basic classification setup, our POS
tagger is most similar to the SVM-based approaches
of Giménez and Màrquez (2004) and C&amp;P. How-
ever, we do not use a left-to-right approach when
tagging sentences. Moreover, SVMTool trains two
separate models, one for OOVs and one for known
words. FLORS only has a single model. In addition,
</bodyText>
<page confidence="0.995434">
24
</page>
<bodyText confidence="0.999942923076923">
we do not make use of ambiguity classes, token-tag
dictionaries and rare feature thresholds. Instead, we
rely only on three types of features: distributional
representations, suffixes and word shapes.
The local-context-only approach of SVMTool,
C&amp;P and FLORS is different from standard se-
quence classification such as MEMMs (e.g., Rat-
naparkhi (1996), Toutanova et al. (2003), Tsuruoka
and Tsujii (2005)) and CRFs (e.g., Collins (2002)).
Sequence models are more powerful in theory, but
this may not be an advantage in DA because the sub-
tle dependencies they exploit may not hold across
domains.
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.94324">
We have presented FLORS, a new POS tagger for
DA. FLORS uses robust representations that work
especially well for unknown words and for known
words with unseen tags. FLORS is simpler and
faster than previous DA methods, yet we were able
to demonstrate that it has significantly better accu-
racy than several baselines.
Acknowledgments. This work was supported by
DFG (Deutsche Forschungsgemeinschaft).
</bodyText>
<sectionHeader confidence="0.997763" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999834710526316">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP, pages 120–128.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In COLT,
pages 92–100.
Thorsten Brants. 2000. TnT: A statistical part-of-speech
tagger. In ANLP, pages 224–231.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479.
Minmin Chen, Kilian Q. Weinberger, and John Blitzer.
2011. Co-training for domain adaptation. In NIPS,
pages 1–9.
Jinho D. Choi and Martha Palmer. 2012. Fast and robust
part-of-speech tagging using dynamic model selection.
In ACL: Short Papers, pages 363–367.
Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In EMNLP, pages 1–8.
Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Steven Finch and Nick Chater. 1992. Bootstrapping syn-
tactic categories using statistical methods. In Back-
ground and Experiments in Machine Learning of Nat-
ural Language, pages 229–235.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A statisti-
cal model for multilingual entity detection and track-
ing. In HLT-NAACL, pages 1–8.
Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav
Petrov. 2012. Using search-logs to improve query tag-
ging. In ACL: Short Papers, pages 238–242.
Jesús Giménez and Lluís Màrquez. 2004. SVMTool: A
general pos tagger generator based on support vector
machines. In LREC, pages 43–46.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL-IJCNLP, pages 495–503.
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In DANLP, pages 23–30.
Fei Huang and Alexander Yates. 2012. Biased repre-
sentation learning for domain adaptation. In EMNLP-
CoNLL, pages 1313–1323.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram HMM part-
of-speech tagger by latent annotation and self-training.
In NAACL-HLT: Short Papers, pages 213–216.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL, pages 264–
271.
S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymptotic
behaviors of support vector machines with Gaussian
kernel. Neural computation, 15(7):1667–1689.
Sandra Kübler and Eric Baucom. 2011. Fast domain
adaptation for part of speech tagging for dialogues. In
RANLP, pages 41–48.
Percy Liang, Hal Daumé III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In ICML, pages 592–599.
Percy Liang. 2005. Semi-supervised learning for natural
language processing. Master’s thesis, Massachusetts
Institute of Technology.
Christopher D. Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics? In
CICLing, pages 171–189.
</reference>
<page confidence="0.964334">
25
</page>
<reference confidence="0.999784806451613">
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL, pages 337–344.
John Miller, Manabu Torii, and Vijay K. Shanker. 2007.
Building domain-specific taggers without annotated
(domain) data. In EMNLP-CoNLL, pages 1103–1111.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL, pages 404–
411.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. Notes of the
1st SANCL Workshop.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
CoNLL, pages 147–155.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP, pages 133–
142.
Alexander M. Rush, Roi Reichart, Michael Collins, and
Amir Globerson. 2012. Improved parsing and POS
tagging using inter-sentence consistency constraints.
In EMNLP-CoNLL, pages 1434–1444.
Beatrice Santorini. 1990. Part-of-speech tagging guide-
lines for the Penn Treebank project (3rd revision, 2nd
printing). Technical report, Department of Linguistics,
University of Pennsylvania.
Tobias Schnabel and Hinrich Schütze. 2013. Towards
robust cross-domain domain adaptation for part-of-
speech tagging. In IJCNLP, pages 198–206.
Hinrich Schütze. 1993. Part-of-speech induction from
scratch. In ACL, pages 251–258.
Hinrich Schütze. 1995. Distributional part-of-speech
tagging. In EACL, pages 141–148.
Anders Søgaard. 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In ACL: Short
papers, pages 48–52.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In EMNLP,
pages 167–176.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL-
HLT, pages 173–180.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tag-
ging sequence data. In EMNLP-HLT, pages 467–474.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In ACL, pages 384–394.
Shulamit Umansky-Pesin, Roi Reichart, and Ari Rap-
poport. 2010. A multi-domain web-based algorithm
for POS tagging of unknown words. In COLING,
pages 1274–1282.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In ACL, pages 755–
762.
</reference>
<page confidence="0.998034">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924504">
<title confidence="0.999781">FLORS: Fast and Simple Domain Adaptation for Part-of-Speech Tagging</title>
<author confidence="0.999481">Tobias Schnabel Hinrich Schütze</author>
<affiliation confidence="0.9994185">Department of Computer Science Center for Information &amp; Language Processing Cornell University University of Munich</affiliation>
<email confidence="0.952117">tbs49@cornell.eduinquiries@cislmu.org</email>
<abstract confidence="0.996959">We present FLORS, a new part-of-speech tagger for domain adaptation. FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags. FLORS is simpler and faster than previous domain adaptation methods, yet it has significantly better accuracy than several baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="3648" citStr="Blitzer et al. (2006)" startWordPosition="566" endWordPosition="569">he next two sections describe experimental data, setup and results. Results are discussed in Section 4. We compare FLORS to alternative word representations in Section 5 and to related work in Section 6. Section 7 presents our conclusions. 2 Experimental data and setup Data. Our source domain is the Penn Treebank (Marcus et al., 1993) of Wall Street Journal (WSJ) 15 Transactions of the Association for Computational Linguistics, 2 (2014) 15–26. Action Editor: Sharon Goldwater. Submitted 9/2013; Revised 11/2013; Published 2/2014. c�2014 Association for Computational Linguistics. text. Following Blitzer et al. (2006), we use sections 2-21 for training and 100,000 WSJ sentences from 1988 as unlabeled data in training. We evaluate on six different TDs. The first five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 22 and 23 of the WSJ for in-domain development and testing, respectively. Each SANCL TD has an unlabeled training set of 100,000 sentences and development and test sets of about 1000 labeled sentences each. The sixth TD is BIO, the Penn BioTreebank data set distributed by Blitzer. It c</context>
<context position="31704" citStr="Blitzer et al., 2006" startWordPosition="5360" endWordPosition="5363">e used for evaluating DA. This is why we did not include it in our comparison in Table 2. For sake of completeness, we provide tagging accuracies for BIO in Table 6, “standard setup”. The results are in line with SANCL results: FLORS beats the baselines on ALL and OOV accuracies. However, if we build the NN bias into our model by simply replacing all NNP tags with NN tags, then accuracy goes up by 4% on ALL and by almost 20% on OOV. Even TnT, the most basic tagger, achieves ALL/OOV accuracy of 91.75/78.33, better than any method in the standard setup. These accuracies are well above those in (Blitzer et al., 2006) and (Huang and Yates, 2010). Since simply replacing NNPs with NNs has such a large effect, BIO cannot be used sensibly for evaluating DA methods. In practice, it is not possible to separate “true” improvements due to generic better DA from elements of the proposed method that simply introduce a negative bias for NNP. In summary, when comparing different DA methods caution should be exercised in the choice of domains. In particular, the effect of unknown tags should be made transparent and the gold standards should be analyzed to determine whether the task addressed in the TD differs significa</context>
<context position="39358" citStr="Blitzer et al. (2006)" startWordPosition="6622" endWordPosition="6625">ORS representations consist of simple counts whereas SCL solves a separate optimization problem for each pivot feature. Umansky-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses.</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP, pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="40748" citStr="Blum and Mitchell, 1998" startWordPosition="6840" endWordPosition="6843">reful manual engineering. Distributional features can also be seen as a form of constraint since feature weights will be shared among all words. Subramanya et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and ad</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT: A statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In ANLP,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="10352" citStr="Brants, 2000" startWordPosition="1704" endWordPosition="1705">ggers used at SANCL and (iii) POS DA results published for BIO. Most of our experiments use taggers from category (i) because we can ensure that experimental conditions are directly comparable. The four baselines in category (i) are shown in Table 1. Three have near state-of-the-art performance on WSJ: SVMTool (Giménez and Màrquez, 2004), Stanford 1One could also compute these suffixes for _w (w prefixed by underscore) instead of for w to include words as distinguishable special suffixes. We test this alternative in Table 2, line 15. (Toutanova et al., 2003) (a birectional MEMM) and C&amp;P. TnT (Brants, 2000) is included as a representative of fast and simple HMM taggers. In addition, C&amp;P is a tagger that has been extensively tested in DA scenarios with excellent results. Unless otherwise stated, we train all models using their default configuration files. We use the optimized parameter configuration published by C&amp;P for the C&amp;P model. Test set results will be compared with the SANCL taggers (category (ii)) at the end of Section 3. As far as category (iii) is concerned, most work on POS DA has been evaluated on BIO. We discuss our concerns about the BIO evaluation sets in Section 4, but also show </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT: A statistical part-of-speech tagger. In ANLP, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="2341" citStr="Brown et al., 1992" startWordPosition="361" endWordPosition="364">s of known words and high rates of unknown words. We show that FLORS achieves excellent DA tagging results on the five domains of the SANCL 2012 shared task (Petrov and McDonald, 2012) and outperforms three state-of-the-art taggers on Blitzer et al.’s (2006) biomedical data. FLORS is also simpler and faster than other POS DA methods. It is simple in that the input representation consists of three simple types of features: distributional count features and two types of binary features, suffix and shape features. Many other word representations that are used for improving generalization (e.g., (Brown et al., 1992; Collobert et al., 2011)) are costly to train or have difficulty handling unknown words. Our representations are fast to build and can be created on-the-fly for unknown words that occur during testing. The learning architecture is simple and fast as well. We train k binary one-vs-all classifiers that use local context only and no sequence information (where k is the number of tags). Thus, tagging complexity is O(k). Many other learning setups for DA are more complex; e.g., they learn representations (as opposed to just counting), they learn several classifiers for different subclasses of word</context>
<context position="24499" citStr="Brown et al., 1992" startWordPosition="4124" endWordPosition="4127">and shapes typical of the tag. C&amp;P also perform relatively well on unseen word+tag due to the ambiguity classes in their model, but FLORS representations are better for every domain. We take these results to mean that constraints on a word’s possible POS tags may well be helpful for in-domain data, but for out-of-domain data an overly strong bias for a word’s observed tags is harmful. It is important to stress that representations similar to FLORS representations have been used for a long time; we would expect many of them to have similar advantages for unseen word+tags. E.g., Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011) are similar to FLORS in this respect. However, FLORS representations are extracted by simple counting whereas the computation of Brown clusters or word embeddings is much more expensive. The speed with which FLORS representations can be computed is particularly beneficial when taggers need to be adapted to new domains. FLORS can easily adapt its representations on the fly – as each new occurrence of a word is encountered, the counts that are the basis 20 for the xi can simply be incremented. We present a direct comparison of FLORS representations w</context>
<context position="32662" citStr="Brown et al., 1992" startWordPosition="5521" endWordPosition="5524">mparing different DA methods caution should be exercised in the choice of domains. In particular, the effect of unknown tags should be made transparent and the gold standards should be analyzed to determine whether the task addressed in the TD differs significantly in some aspects from that addressed in the source domain. 5 Comparison of word representations Our approach to DA is an instance of representation learning: we aim to find representations that are robust across domains. In this section, we compare FLORS with two other widely used representation learning methods: (i) Brown clusters (Brown et al., 1992) and (ii) C&amp;W embeddings, the word embeddings of Collobert et al. (2011). We use fdist(w) = fleft(w)⊕fright(w) to refer to our own distributional word representations (see Section 2). The perhaps oldest and most frequently used lowdimensional representation of words is based on Brown clusters. Typically, prefixes of Brown clusters (Brown et al., 1992) are added to increase the robustness of POS taggers (e.g., Toutanova et al. (2003)). Computational costs are high (quadratic in the vocabulary size) although the computation can be parallelized (Uszkoreit and Brants, 2008). More recently, general</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Kilian Q Weinberger</author>
<author>John Blitzer</author>
</authors>
<title>Co-training for domain adaptation. In</title>
<date>2011</date>
<booktitle>NIPS,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="41080" citStr="Chen et al. (2011)" startWordPosition="6897" endWordPosition="6900">e reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions in each domain. The goal is to make </context>
</contexts>
<marker>Chen, Weinberger, Blitzer, 2011</marker>
<rawString>Minmin Chen, Kilian Q. Weinberger, and John Blitzer. 2011. Co-training for domain adaptation. In NIPS, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
</authors>
<title>Fast and robust part-of-speech tagging using dynamic model selection. In ACL: Short Papers,</title>
<date>2012</date>
<pages>363--367</pages>
<contexts>
<context position="4427" citStr="Choi and Palmer (2012)" startWordPosition="691" endWordPosition="694">newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 22 and 23 of the WSJ for in-domain development and testing, respectively. Each SANCL TD has an unlabeled training set of 100,000 sentences and development and test sets of about 1000 labeled sentences each. The sixth TD is BIO, the Penn BioTreebank data set distributed by Blitzer. It consists of dev and test sets of 500 sentences each and 100,000 unlabeled sentences. Classification setup. Similar to SVMTool (Giménez and Màrquez, 2004) and Choi and Palmer (2012) (henceforth: C&amp;P), we use local context only for tagging instead of performing sequence classification. For a word w occurring as token vi in a sentence, we build a feature vector for a local window of size 2l + 1 around vi. The representation of the object to be classified is this feature vector and the target class is the POS tag of vi. We use the linear L2-regularized L2-loss SVM implementation provided by LIBLINEAR (Fan et al., 2008) to train k one-vs-all classifiers on the training set where k is the number of POS tags in the training set (in our case k = 45). We train with untuned defau</context>
</contexts>
<marker>Choi, Palmer, 2012</marker>
<rawString>Jinho D. Choi and Martha Palmer. 2012. Fast and robust part-of-speech tagging using dynamic model selection. In ACL: Short Papers, pages 363–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Léon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<date>2011</date>
<contexts>
<context position="2366" citStr="Collobert et al., 2011" startWordPosition="365" endWordPosition="368"> high rates of unknown words. We show that FLORS achieves excellent DA tagging results on the five domains of the SANCL 2012 shared task (Petrov and McDonald, 2012) and outperforms three state-of-the-art taggers on Blitzer et al.’s (2006) biomedical data. FLORS is also simpler and faster than other POS DA methods. It is simple in that the input representation consists of three simple types of features: distributional count features and two types of binary features, suffix and shape features. Many other word representations that are used for improving generalization (e.g., (Brown et al., 1992; Collobert et al., 2011)) are costly to train or have difficulty handling unknown words. Our representations are fast to build and can be created on-the-fly for unknown words that occur during testing. The learning architecture is simple and fast as well. We train k binary one-vs-all classifiers that use local context only and no sequence information (where k is the number of tags). Thus, tagging complexity is O(k). Many other learning setups for DA are more complex; e.g., they learn representations (as opposed to just counting), they learn several classifiers for different subclasses of words (e.g., known vs. unknow</context>
<context position="24544" citStr="Collobert et al., 2011" startWordPosition="4132" endWordPosition="4135">erform relatively well on unseen word+tag due to the ambiguity classes in their model, but FLORS representations are better for every domain. We take these results to mean that constraints on a word’s possible POS tags may well be helpful for in-domain data, but for out-of-domain data an overly strong bias for a word’s observed tags is harmful. It is important to stress that representations similar to FLORS representations have been used for a long time; we would expect many of them to have similar advantages for unseen word+tags. E.g., Brown clusters (Brown et al., 1992) and word embeddings (Collobert et al., 2011) are similar to FLORS in this respect. However, FLORS representations are extracted by simple counting whereas the computation of Brown clusters or word embeddings is much more expensive. The speed with which FLORS representations can be computed is particularly beneficial when taggers need to be adapted to new domains. FLORS can easily adapt its representations on the fly – as each new occurrence of a word is encountered, the counts that are the basis 20 for the xi can simply be incremented. We present a direct comparison of FLORS representations with other representations in Section 5. “Loca</context>
<context position="32734" citStr="Collobert et al. (2011)" startWordPosition="5534" endWordPosition="5537">ce of domains. In particular, the effect of unknown tags should be made transparent and the gold standards should be analyzed to determine whether the task addressed in the TD differs significantly in some aspects from that addressed in the source domain. 5 Comparison of word representations Our approach to DA is an instance of representation learning: we aim to find representations that are robust across domains. In this section, we compare FLORS with two other widely used representation learning methods: (i) Brown clusters (Brown et al., 1992) and (ii) C&amp;W embeddings, the word embeddings of Collobert et al. (2011). We use fdist(w) = fleft(w)⊕fright(w) to refer to our own distributional word representations (see Section 2). The perhaps oldest and most frequently used lowdimensional representation of words is based on Brown clusters. Typically, prefixes of Brown clusters (Brown et al., 1992) are added to increase the robustness of POS taggers (e.g., Toutanova et al. (2003)). Computational costs are high (quadratic in the vocabulary size) although the computation can be parallelized (Uszkoreit and Brants, 2008). More recently, general word representations (Collobert et al., 2011; Turian et al., 2010) have</context>
<context position="34107" citStr="Collobert et al. (2011)" startWordPosition="5756" endWordPosition="5759">P tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, eiFLORS 22 ther alone or in combination with other features. To compare fdist(w) (our distributional representations) with Brown clusters, we induced 1000 Brown clusters on the joint corpus data DALL (see Section 2) using the publicly available implementation of Liang (2005). We padded sentences with (BOUNDARY) tokens on each side and used path prefixes of length 4, 6, 10 and 20 as features for each word (cf. Ratinov and Roth (2009), Turian et al. (2010)). C&amp;W embeddings are provided by Collobert et al. (2011): 50-dimensional vectors for 130,000 words from WSJ, trained on Wikipedia. Similar to our distributional representations fdist(w), the embeddings also contain a (BOUNDARY) token (which they call PADDING). Moreover, they have a special embedding for unknown words (called UNKNOWN) which we use whenever we encounter a word that is not in their lookup table. We preprocess our raw tokens the same way they do (lowercase and replace sequences of digits by “0”) before we look up a representation during training and testing. We replaced the distributional features in our basic setup by either Brown clu</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.</rawString>
</citation>
<citation valid="false">
<title>Natural language processing (almost) from scratch.</title>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<marker></marker>
<rawString>Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="4869" citStr="Fan et al., 2008" startWordPosition="771" endWordPosition="774">sts of dev and test sets of 500 sentences each and 100,000 unlabeled sentences. Classification setup. Similar to SVMTool (Giménez and Màrquez, 2004) and Choi and Palmer (2012) (henceforth: C&amp;P), we use local context only for tagging instead of performing sequence classification. For a word w occurring as token vi in a sentence, we build a feature vector for a local window of size 2l + 1 around vi. The representation of the object to be classified is this feature vector and the target class is the POS tag of vi. We use the linear L2-regularized L2-loss SVM implementation provided by LIBLINEAR (Fan et al., 2008) to train k one-vs-all classifiers on the training set where k is the number of POS tags in the training set (in our case k = 45). We train with untuned default parameters; in particular, C = 1. In the special case of linear SVMs, the value of C does not need to be tuned exhaustively as the solution remains constant after C has reached a certain threshold value C∗ (Keerthi and Lin, 2003). Training can easily be parallelized by giving each binary SVM its own thread. Windows. The local context for tagging token vi is a window of size 2l + 1 centered around vi: (vi−l, ... , vi, ..., vi+l). We pad</context>
<context position="6301" citStr="Fan et al., 2008" startWordPosition="1023" endWordPosition="1026">1 word vectors in its window F(vi) = f(vi−l) ED ... ED f(vi+l) where ED is vector concatenation. Word features. We represent each word w by four components: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f(w) = fleft(w)EDfright(w)EDfsuffix(w)EDfshape(w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of fleft(w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci, w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci, w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Finch</author>
<author>Nick Chater</author>
</authors>
<title>Bootstrapping syntactic categories using statistical methods.</title>
<date>1992</date>
<booktitle>In Background and Experiments in Machine Learning of Natural Language,</booktitle>
<pages>229--235</pages>
<contexts>
<context position="6387" citStr="Finch and Chater, 1992" startWordPosition="1037" endWordPosition="1040"> concatenation. Word features. We represent each word w by four components: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f(w) = fleft(w)EDfright(w)EDfsuffix(w)EDfshape(w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of fleft(w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci, w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci, w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x). tf-weighting has been used by other researchers (</context>
</contexts>
<marker>Finch, Chater, 1992</marker>
<rawString>Steven Finch and Nick Chater. 1992. Bootstrapping syntactic categories using statistical methods. In Background and Experiments in Machine Learning of Natural Language, pages 229–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hany Hassan</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaoqiang Luo</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="41044" citStr="Florian et al., 2004" startWordPosition="6891" endWordPosition="6894">BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions </context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In HLT-NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Keith Hall</author>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
</authors>
<title>Using search-logs to improve query tagging.</title>
<date>2012</date>
<booktitle>In ACL: Short Papers,</booktitle>
<pages>238--242</pages>
<contexts>
<context position="39032" citStr="Ganchev et al. (2012)" startWordPosition="6569" endWordPosition="6572">8* 90.23 60.99 89.44 63.13 96.72 90.48 4 Brown for fdist(w) 90.34* 62.41* 92.23* 71.47* 94.45 81.76 89.71* 56.28* 89.02* 63.20 96.48* 87.50 Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n: number of indicator words. A column’s best result is bold. Moreover, FLORS representations consist of simple counts whereas SCL solves a separate optimization problem for each pivot feature. Umansky-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature</context>
</contexts>
<marker>Ganchev, Hall, McDonald, Petrov, 2012</marker>
<rawString>Kuzman Ganchev, Keith Hall, Ryan McDonald, and Slav Petrov. 2012. Using search-logs to improve query tagging. In ACL: Short Papers, pages 238–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesús Giménez</author>
<author>Lluís Màrquez</author>
</authors>
<title>SVMTool: A general pos tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In LREC,</booktitle>
<pages>43--46</pages>
<contexts>
<context position="4400" citStr="Giménez and Màrquez, 2004" startWordPosition="686" endWordPosition="689">erent TDs. The first five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 22 and 23 of the WSJ for in-domain development and testing, respectively. Each SANCL TD has an unlabeled training set of 100,000 sentences and development and test sets of about 1000 labeled sentences each. The sixth TD is BIO, the Penn BioTreebank data set distributed by Blitzer. It consists of dev and test sets of 500 sentences each and 100,000 unlabeled sentences. Classification setup. Similar to SVMTool (Giménez and Màrquez, 2004) and Choi and Palmer (2012) (henceforth: C&amp;P), we use local context only for tagging instead of performing sequence classification. For a word w occurring as token vi in a sentence, we build a feature vector for a local window of size 2l + 1 around vi. The representation of the object to be classified is this feature vector and the target class is the POS tag of vi. We use the linear L2-regularized L2-loss SVM implementation provided by LIBLINEAR (Fan et al., 2008) to train k one-vs-all classifiers on the training set where k is the number of POS tags in the training set (in our case k = 45). </context>
<context position="10078" citStr="Giménez and Màrquez, 2004" startWordPosition="1656" endWordPosition="1659"> were designed for English and probably would have to be adjusted for other languages. Baselines. We address the problem of unsupervised domain adaptation for POS tagging. For this problem, we consider three types of baselines: (i) high-performing publicly available systems, (ii) the taggers used at SANCL and (iii) POS DA results published for BIO. Most of our experiments use taggers from category (i) because we can ensure that experimental conditions are directly comparable. The four baselines in category (i) are shown in Table 1. Three have near state-of-the-art performance on WSJ: SVMTool (Giménez and Màrquez, 2004), Stanford 1One could also compute these suffixes for _w (w prefixed by underscore) instead of for w to include words as distinguishable special suffixes. We test this alternative in Table 2, line 15. (Toutanova et al., 2003) (a birectional MEMM) and C&amp;P. TnT (Brants, 2000) is included as a representative of fast and simple HMM taggers. In addition, C&amp;P is a tagger that has been extensively tested in DA scenarios with excellent results. Unless otherwise stated, we train all models using their default configuration files. We use the optimized parameter configuration published by C&amp;P for the C&amp;P</context>
<context position="42267" citStr="Giménez and Màrquez (2004)" startWordPosition="7086" endWordPosition="7089">ns in each domain. The goal is to make these two distributions align by using instance-specific weights during training. Jiang and Zhai (2007) propose a framework that integrates prior knowledge from different data sets into the learning objective by weights. In related work, C&amp;P train generalized and domain-specific models. An input sentence is tagged by the model that is most similar to the sentence. FLORS could be easily extended along these lines, an experiment we plan for the future. In terms of the basic classification setup, our POS tagger is most similar to the SVM-based approaches of Giménez and Màrquez (2004) and C&amp;P. However, we do not use a left-to-right approach when tagging sentences. Moreover, SVMTool trains two separate models, one for OOVs and one for known words. FLORS only has a single model. In addition, 24 we do not make use of ambiguity classes, token-tag dictionaries and rare feature thresholds. Instead, we rely only on three types of features: distributional representations, suffixes and word shapes. The local-context-only approach of SVMTool, C&amp;P and FLORS is different from standard sequence classification such as MEMMs (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Tsuruoka an</context>
</contexts>
<marker>Giménez, Màrquez, 2004</marker>
<rawString>Jesús Giménez and Lluís Màrquez. 2004. SVMTool: A general pos tagger generator based on support vector machines. In LREC, pages 43–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence-labeling.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP,</booktitle>
<pages>495--503</pages>
<contexts>
<context position="6452" citStr="Huang and Yates, 2009" startWordPosition="1047" endWordPosition="1050">omponents: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f(w) = fleft(w)EDfright(w)EDfsuffix(w)EDfshape(w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of fleft(w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci, w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci, w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x). tf-weighting has been used by other researchers (Huang and Yates, 2009) and showed good performance in our own pre</context>
<context position="39128" citStr="Huang and Yates (2009)" startWordPosition="6584" endWordPosition="6587">1.76 89.71* 56.28* 89.02* 63.20 96.48* 87.50 Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n: number of indicator words. A column’s best result is bold. Moreover, FLORS representations consist of simple counts whereas SCL solves a separate optimization problem for each pivot feature. Umansky-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior kno</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence-labeling. In ACL-IJCNLP, pages 495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Exploring representation-learning approaches to domain adaptation.</title>
<date>2010</date>
<booktitle>In DANLP,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="31732" citStr="Huang and Yates, 2010" startWordPosition="5365" endWordPosition="5368">his is why we did not include it in our comparison in Table 2. For sake of completeness, we provide tagging accuracies for BIO in Table 6, “standard setup”. The results are in line with SANCL results: FLORS beats the baselines on ALL and OOV accuracies. However, if we build the NN bias into our model by simply replacing all NNP tags with NN tags, then accuracy goes up by 4% on ALL and by almost 20% on OOV. Even TnT, the most basic tagger, achieves ALL/OOV accuracy of 91.75/78.33, better than any method in the standard setup. These accuracies are well above those in (Blitzer et al., 2006) and (Huang and Yates, 2010). Since simply replacing NNPs with NNs has such a large effect, BIO cannot be used sensibly for evaluating DA methods. In practice, it is not possible to separate “true” improvements due to generic better DA from elements of the proposed method that simply introduce a negative bias for NNP. In summary, when comparing different DA methods caution should be exercised in the choice of domains. In particular, the effect of unknown tags should be made transparent and the gold standards should be analyzed to determine whether the task addressed in the TD differs significantly in some aspects from th</context>
<context position="39485" citStr="Huang and Yates (2010)" startWordPosition="6642" endWordPosition="6645">y-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constrai</context>
<context position="40921" citStr="Huang and Yates, 2010" startWordPosition="6871" endWordPosition="6874">ruct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. I</context>
</contexts>
<marker>Huang, Yates, 2010</marker>
<rawString>Fei Huang and Alexander Yates. 2010. Exploring representation-learning approaches to domain adaptation. In DANLP, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Biased representation learning for domain adaptation. In EMNLPCoNLL,</title>
<date>2012</date>
<pages>1313--1323</pages>
<contexts>
<context position="39594" citStr="Huang and Yates (2012)" startWordPosition="6661" endWordPosition="6664">w since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as </context>
</contexts>
<marker>Huang, Yates, 2012</marker>
<rawString>Fei Huang and Alexander Yates. 2012. Biased representation learning for domain adaptation. In EMNLPCoNLL, pages 1313–1323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Vladimir Eidelman</author>
<author>Mary Harper</author>
</authors>
<title>Improving a simple bigram HMM partof-speech tagger by latent annotation and self-training.</title>
<date>2009</date>
<booktitle>In NAACL-HLT: Short Papers,</booktitle>
<pages>213--216</pages>
<contexts>
<context position="40897" citStr="Huang et al., 2009" startWordPosition="6867" endWordPosition="6870"> et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tag</context>
</contexts>
<marker>Huang, Eidelman, Harper, 2009</marker>
<rawString>Zhongqiang Huang, Vladimir Eidelman, and Mary Harper. 2009. Improving a simple bigram HMM partof-speech tagger by latent annotation and self-training. In NAACL-HLT: Short Papers, pages 213–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="41783" citStr="Jiang and Zhai (2007)" startWordPosition="7008" endWordPosition="7011">nces from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions in each domain. The goal is to make these two distributions align by using instance-specific weights during training. Jiang and Zhai (2007) propose a framework that integrates prior knowledge from different data sets into the learning objective by weights. In related work, C&amp;P train generalized and domain-specific models. An input sentence is tagged by the model that is most similar to the sentence. FLORS could be easily extended along these lines, an experiment we plan for the future. In terms of the basic classification setup, our POS tagger is most similar to the SVM-based approaches of Giménez and Màrquez (2004) and C&amp;P. However, we do not use a left-to-right approach when tagging sentences. Moreover, SVMTool trains two separ</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL, pages 264– 271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sathiya Keerthi</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Asymptotic behaviors of support vector machines with Gaussian kernel.</title>
<date>2003</date>
<booktitle>Neural computation,</booktitle>
<pages>15--7</pages>
<contexts>
<context position="5259" citStr="Keerthi and Lin, 2003" startWordPosition="849" endWordPosition="852"> + 1 around vi. The representation of the object to be classified is this feature vector and the target class is the POS tag of vi. We use the linear L2-regularized L2-loss SVM implementation provided by LIBLINEAR (Fan et al., 2008) to train k one-vs-all classifiers on the training set where k is the number of POS tags in the training set (in our case k = 45). We train with untuned default parameters; in particular, C = 1. In the special case of linear SVMs, the value of C does not need to be tuned exhaustively as the solution remains constant after C has reached a certain threshold value C∗ (Keerthi and Lin, 2003). Training can easily be parallelized by giving each binary SVM its own thread. Windows. The local context for tagging token vi is a window of size 2l + 1 centered around vi: (vi−l, ... , vi, ..., vi+l). We pad sentences on either side with (BOUNDARY) to ensure sufficient context for all words. Given a mapping f from words to feature vectors (see below), the representation F of a token vi is the concatenation of the 2l + 1 word vectors in its window F(vi) = f(vi−l) ED ... ED f(vi+l) where ED is vector concatenation. Word features. We represent each word w by four components: (i) counts of left</context>
</contexts>
<marker>Keerthi, Lin, 2003</marker>
<rawString>S. Sathiya Keerthi and Chih-Jen Lin. 2003. Asymptotic behaviors of support vector machines with Gaussian kernel. Neural computation, 15(7):1667–1689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kübler</author>
<author>Eric Baucom</author>
</authors>
<title>Fast domain adaptation for part of speech tagging for dialogues.</title>
<date>2011</date>
<booktitle>In RANLP,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="41318" citStr="Kübler and Baucom (2011)" startWordPosition="6935" endWordPosition="6938">tion model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions in each domain. The goal is to make these two distributions align by using instance-specific weights during training. Jiang and Zhai (2007) propose a framework that integrates prior knowledge from different data sets into the learning objective by weights. In related work, </context>
</contexts>
<marker>Kübler, Baucom, 2011</marker>
<rawString>Sandra Kübler and Eric Baucom. 2011. Fast domain adaptation for part of speech tagging for dialogues. In RANLP, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daumé</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>592--599</pages>
<contexts>
<context position="25494" citStr="Liang et al., 2008" startWordPosition="4290" endWordPosition="4293">easily adapt its representations on the fly – as each new occurrence of a word is encountered, the counts that are the basis 20 for the xi can simply be incremented. We present a direct comparison of FLORS representations with other representations in Section 5. “Local context” vs. sequence classification. The most common approach to POS tagging is to tag a sentence with its most likely sequence; in contrast, independent tagging of local context is not guaranteed to find the best sequence. Recent work on English suggests that window-based tagging can perform as well as sequence-based methods (Liang et al., 2008; Collobert et al., 2011). Toutanova et al. (2003) report similar results. In our experiments, we also did not find consistent improvements when we incorporated sequence constraints (Table 2, line 14). However, there may be languages and applications involving long-distance relationships where local-context classification is suboptimal. Local-context classification has two advantages compared to sequence classification. (i) It simplifies the classification and tagging setup: we can use any existing statistical classifier. Sequence classification limits the range of methods that can be applied;</context>
</contexts>
<marker>Liang, Daumé, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daumé III, and Dan Klein. 2008. Structure compilation: trading structure for features. In ICML, pages 592–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language processing. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="33867" citStr="Liang (2005)" startWordPosition="5715" endWordPosition="5716">general word representations (Collobert et al., 2011; Turian et al., 2010) have been used for robust POS tagging. These word representations are typically trained on a large amount of unlabeled text and fine-tuned for specific NLP tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, eiFLORS 22 ther alone or in combination with other features. To compare fdist(w) (our distributional representations) with Brown clusters, we induced 1000 Brown clusters on the joint corpus data DALL (see Section 2) using the publicly available implementation of Liang (2005). We padded sentences with (BOUNDARY) tokens on each side and used path prefixes of length 4, 6, 10 and 20 as features for each word (cf. Ratinov and Roth (2009), Turian et al. (2010)). C&amp;W embeddings are provided by Collobert et al. (2011): 50-dimensional vectors for 130,000 words from WSJ, trained on Wikipedia. Similar to our distributional representations fdist(w), the embeddings also contain a (BOUNDARY) token (which they call PADDING). Moreover, they have a special embedding for unknown words (called UNKNOWN) which we use whenever we encounter a word that is not in their lookup table. We </context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language processing. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In CICLing,</title>
<date>2011</date>
<pages>171--189</pages>
<contexts>
<context position="29260" citStr="Manning (2011)" startWordPosition="4916" endWordPosition="4917">or that cannot be avoided in unsupervised DA is due to differences in annotation guidelines. There are a few such problems in SANCL; e.g., file names like “Services.doc” are annotated as NN in the email domain. But their distributional and grammatical behavior is more similar to NNPs; as a consequence, most file names are incorrectly tagged. In general, it is difficult to discriminate NNs from NNPs. The Penn Treebank annotation guidelines (Santorini, 1990) are compatible with either tag in many cases and it may simply be impossible to write annotation guidelines that avoid these problems (cf. Manning (2011)). NN-NNP inconsistencies are especially problematic for OOV tagging since most OOVs are NNs or NNPs. 4For example, there is a special tag ADD in the web domain for web addresses. The last two words of the sentence “I would like to host my upcoming website to/IN Liquidweb.com/ADD” are mistagged by Stanford tagger as “... to/TO Liquidweb.com/VB”. So the missing tag in this case also affects the tagging of surrounding words. 21 bio dev wsj train OOV ALL ALL NN 62.4 25.4 14.4 JJ 15.9 8.9 6.2 NNS 10.2 7.5 6.3 NNP 0.5 0.2 9.5 NNPS 0.0 0.0 0.3 Table 5: Frequency of some tags (percent of tokens) for </context>
</contexts>
<marker>Manning, 2011</marker>
<rawString>Christopher D. Manning. 2011. Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In CICLing, pages 171–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3363" citStr="Marcus et al., 1993" startWordPosition="529" endWordPosition="532">omplexity is O(k). Many other learning setups for DA are more complex; e.g., they learn representations (as opposed to just counting), they learn several classifiers for different subclasses of words (e.g., known vs. unknown) or they combine left-toright and right-to-left taggings. The next two sections describe experimental data, setup and results. Results are discussed in Section 4. We compare FLORS to alternative word representations in Section 5 and to related work in Section 6. Section 7 presents our conclusions. 2 Experimental data and setup Data. Our source domain is the Penn Treebank (Marcus et al., 1993) of Wall Street Journal (WSJ) 15 Transactions of the Association for Computational Linguistics, 2 (2014) 15–26. Action Editor: Sharon Goldwater. Submitted 9/2013; Revised 11/2013; Published 2/2014. c�2014 Association for Computational Linguistics. text. Following Blitzer et al. (2006), we use sections 2-21 for training and 100,000 WSJ sentences from 1988 as unlabeled data in training. We evaluate on six different TDs. The first five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 2</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation. In</title>
<date>2006</date>
<booktitle>ACL,</booktitle>
<pages>337--344</pages>
<contexts>
<context position="40666" citStr="McClosky et al., 2006" startWordPosition="6827" endWordPosition="6830">e out-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as a form of constraint since feature weights will be shared among all words. Subramanya et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In ACL, pages 337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Miller</author>
<author>Manabu Torii</author>
<author>Vijay K Shanker</author>
</authors>
<title>Building domain-specific taggers without annotated (domain) data. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>1103--1111</pages>
<contexts>
<context position="8588" citStr="Miller et al. (2007)" startWordPosition="1401" endWordPosition="1404">,2,31, affixes, orthography, word length pf10,1,2,31, vf10,1,2,31, affixes, orthography distributions of vf10,1,21, suffixes, orthography Table 1: Overview of baseline taggers and FLORS. vi: token, pi: POS tag. Positions included in the sets of token indices are relative to the position i of the word v0 to be tagged; e.g., pf10,1,21 is short for {p−0, p−1, p−2, p0, p1, p21. To represent tokens vi, models 1–4 use vocabulary indices and FLORS uses distributional representations. Models 2–4 use combinations of features (e.g., tag-word) as well. (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Miller et al. (2007)) we simply use all (lowercase) suffixes to avoid the need for selecting a subset of suffixes; and we treat all words equally as opposed to using suffix features for only a subset of words. For suffix s, we set the dimension corresponding to s in fsuffix(w) to 1 if lowercased w ends in s and to 0 otherwise. Note that w is a suffix of itself.1 Shape features. We use the Berkeley parser word signatures (Petrov and Klein, 2007). Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., does the word contain a digit, hyphen, uppercase ch</context>
</contexts>
<marker>Miller, Torii, Shanker, 2007</marker>
<rawString>John Miller, Manabu Torii, and Vijay K. Shanker. 2007. Building domain-specific taggers without annotated (domain) data. In EMNLP-CoNLL, pages 1103–1111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="9016" citStr="Petrov and Klein, 2007" startWordPosition="1480" endWordPosition="1483">y indices and FLORS uses distributional representations. Models 2–4 use combinations of features (e.g., tag-word) as well. (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Miller et al. (2007)) we simply use all (lowercase) suffixes to avoid the need for selecting a subset of suffixes; and we treat all words equally as opposed to using suffix features for only a subset of words. For suffix s, we set the dimension corresponding to s in fsuffix(w) to 1 if lowercased w ends in s and to 0 otherwise. Note that w is a suffix of itself.1 Shape features. We use the Berkeley parser word signatures (Petrov and Klein, 2007). Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., does the word contain a digit, hyphen, uppercase character) and morphological (e.g., does the word end in -ed or -ing) features. There are 50 unique signatures in WSJ. We set the dimension of fshape(w) that corresponds to the signature of w to 1 and all other dimensions to 0. We note that the shape features we use were designed for English and probably would have to be adjusted for other languages. Baselines. We address the problem of unsupervised domain adaptation for POS t</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404– 411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<date>2012</date>
<booktitle>Overview of the 2012 Shared Task on Parsing the Web. Notes of the 1st SANCL Workshop.</booktitle>
<contexts>
<context position="1907" citStr="Petrov and McDonald, 2012" startWordPosition="293" endWordPosition="296">), FLORS predicts unseen tags of known words better than prior work on DA for POS. Second, since FLORS uses representations computed from unlabeled text, representations of unknown words are in principle of the same type as representations of known words; this property of FLORS results in better performance on unknown words compared to prior work. These two advantages are especially beneficial for TDs that contain high rates of unseen tags of known words and high rates of unknown words. We show that FLORS achieves excellent DA tagging results on the five domains of the SANCL 2012 shared task (Petrov and McDonald, 2012) and outperforms three state-of-the-art taggers on Blitzer et al.’s (2006) biomedical data. FLORS is also simpler and faster than other POS DA methods. It is simple in that the input representation consists of three simple types of features: distributional count features and two types of binary features, suffix and shape features. Many other word representations that are used for improving generalization (e.g., (Brown et al., 1992; Collobert et al., 2011)) are costly to train or have difficulty handling unknown words. Our representations are fast to build and can be created on-the-fly for unkn</context>
<context position="3910" citStr="Petrov and McDonald, 2012" startWordPosition="608" endWordPosition="611">ta and setup Data. Our source domain is the Penn Treebank (Marcus et al., 1993) of Wall Street Journal (WSJ) 15 Transactions of the Association for Computational Linguistics, 2 (2014) 15–26. Action Editor: Sharon Goldwater. Submitted 9/2013; Revised 11/2013; Published 2/2014. c�2014 Association for Computational Linguistics. text. Following Blitzer et al. (2006), we use sections 2-21 for training and 100,000 WSJ sentences from 1988 as unlabeled data in training. We evaluate on six different TDs. The first five TDs (newsgroups, weblogs, reviews, answers, emails) are from the SANCL shared task (Petrov and McDonald, 2012). Additionally, the SANCL dataset contains sections 22 and 23 of the WSJ for in-domain development and testing, respectively. Each SANCL TD has an unlabeled training set of 100,000 sentences and development and test sets of about 1000 labeled sentences each. The sixth TD is BIO, the Penn BioTreebank data set distributed by Blitzer. It consists of dev and test sets of 500 sentences each and 100,000 unlabeled sentences. Classification setup. Similar to SVMTool (Giménez and Màrquez, 2004) and Choi and Palmer (2012) (henceforth: C&amp;P), we use local context only for tagging instead of performing seq</context>
<context position="19788" citStr="Petrov and McDonald, 2012" startWordPosition="3336" endWordPosition="3339">and Schütze, 2013) on optimizing a separate model for unknown words that has in some cases better performance on OOV accuracy than what we publish here.2 However, this would complicate the architecture of FLORS. We opted for a maximally simple model in this paper, potentially at the cost of some performance. Test set results. Table 3 reports results on the test sets. FLORS again performs significantly better on all five TDs, both on ALL and OOV. Only in-domain on WSJ, ALL performance is worse. Finally, we compare our results to the POS taggers for which performance was reported at SANCL 2012 (Petrov and McDonald, 2012, Table 4). Constituency-based parsers – which also tag words as a by-product of deriving complete parse trees – are excluded from the comparison because they are trained on a richer representation, the syntactic structure of sentences.3 FLORS’ results are better than the best non-parsing-based results at SANCL 2012, which were accuracies of 92.32 on newsgroups (HIT), 90.65 on reviews (HIT) and 91.07 on answers (IMS-1). 4 Discussion Advantages of FLORS representation. As we can see in Table 1, the main representational difference between FLORS and the other taggers is that the FLORS representa</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 Shared Task on Parsing the Web. Notes of the 1st SANCL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In CoNLL,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="34028" citStr="Ratinov and Roth (2009)" startWordPosition="5743" endWordPosition="5746">ally trained on a large amount of unlabeled text and fine-tuned for specific NLP tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, eiFLORS 22 ther alone or in combination with other features. To compare fdist(w) (our distributional representations) with Brown clusters, we induced 1000 Brown clusters on the joint corpus data DALL (see Section 2) using the publicly available implementation of Liang (2005). We padded sentences with (BOUNDARY) tokens on each side and used path prefixes of length 4, 6, 10 and 20 as features for each word (cf. Ratinov and Roth (2009), Turian et al. (2010)). C&amp;W embeddings are provided by Collobert et al. (2011): 50-dimensional vectors for 130,000 words from WSJ, trained on Wikipedia. Similar to our distributional representations fdist(w), the embeddings also contain a (BOUNDARY) token (which they call PADDING). Moreover, they have a special embedding for unknown words (called UNKNOWN) which we use whenever we encounter a word that is not in their lookup table. We preprocess our raw tokens the same way they do (lowercase and replace sequences of digits by “0”) before we look up a representation during training and testing.</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In CoNLL, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging. In</title>
<date>1996</date>
<booktitle>EMNLP,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="8541" citStr="Ratnaparkhi (1996)" startWordPosition="1395" endWordPosition="1396">,11, affixes, orthography pf10,1,2,31, vf10,1,2,31, affixes, orthography, word length pf10,1,2,31, vf10,1,2,31, affixes, orthography distributions of vf10,1,21, suffixes, orthography Table 1: Overview of baseline taggers and FLORS. vi: token, pi: POS tag. Positions included in the sets of token indices are relative to the position i of the word v0 to be tagged; e.g., pf10,1,21 is short for {p−0, p−1, p−2, p0, p1, p21. To represent tokens vi, models 1–4 use vocabulary indices and FLORS uses distributional representations. Models 2–4 use combinations of features (e.g., tag-word) as well. (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Miller et al. (2007)) we simply use all (lowercase) suffixes to avoid the need for selecting a subset of suffixes; and we treat all words equally as opposed to using suffix features for only a subset of words. For suffix s, we set the dimension corresponding to s in fsuffix(w) to 1 if lowercased w ends in s and to 0 otherwise. Note that w is a suffix of itself.1 Shape features. We use the Berkeley parser word signatures (Petrov and Klein, 2007). Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., does</context>
<context position="42829" citStr="Ratnaparkhi (1996)" startWordPosition="7175" endWordPosition="7177">o the SVM-based approaches of Giménez and Màrquez (2004) and C&amp;P. However, we do not use a left-to-right approach when tagging sentences. Moreover, SVMTool trains two separate models, one for OOVs and one for known words. FLORS only has a single model. In addition, 24 we do not make use of ambiguity classes, token-tag dictionaries and rare feature thresholds. Instead, we rely only on three types of features: distributional representations, suffixes and word shapes. The local-context-only approach of SVMTool, C&amp;P and FLORS is different from standard sequence classification such as MEMMs (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Tsuruoka and Tsujii (2005)) and CRFs (e.g., Collins (2002)). Sequence models are more powerful in theory, but this may not be an advantage in DA because the subtle dependencies they exploit may not hold across domains. 7 Conclusion We have presented FLORS, a new POS tagger for DA. FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags. FLORS is simpler and faster than previous DA methods, yet we were able to demonstrate that it has significantly better accuracy than several baselines. Acknowledgments. This w</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In EMNLP, pages 133– 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Roi Reichart</author>
<author>Michael Collins</author>
<author>Amir Globerson</author>
</authors>
<title>Improved parsing and POS tagging using inter-sentence consistency constraints. In EMNLP-CoNLL,</title>
<date>2012</date>
<pages>1434--1444</pages>
<contexts>
<context position="40003" citStr="Rush et al. (2012)" startWordPosition="6721" endWordPosition="6724">are them against a CRF baseline using distributional features. In later work, Huang and Yates (2010) add the latent states of multiple, differently trained HMMs as features to their CRF. Huang and Yates (2012) argue that finding an optimal feature representation is computationally intractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as a form of constraint since feature weights will be shared among all words. Subramanya et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this pro</context>
</contexts>
<marker>Rush, Reichart, Collins, Globerson, 2012</marker>
<rawString>Alexander M. Rush, Roi Reichart, Michael Collins, and Amir Globerson. 2012. Improved parsing and POS tagging using inter-sentence consistency constraints. In EMNLP-CoNLL, pages 1434–1444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank project (3rd revision, 2nd printing).</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>Department of Linguistics, University of Pennsylvania.</institution>
<contexts>
<context position="29106" citStr="Santorini, 1990" startWordPosition="4891" endWordPosition="4892">. But the percentages of unknown tags should also be reported for each dataset as a basis for a more accurate interpretation of results. Another type of error that cannot be avoided in unsupervised DA is due to differences in annotation guidelines. There are a few such problems in SANCL; e.g., file names like “Services.doc” are annotated as NN in the email domain. But their distributional and grammatical behavior is more similar to NNPs; as a consequence, most file names are incorrectly tagged. In general, it is difficult to discriminate NNs from NNPs. The Penn Treebank annotation guidelines (Santorini, 1990) are compatible with either tag in many cases and it may simply be impossible to write annotation guidelines that avoid these problems (cf. Manning (2011)). NN-NNP inconsistencies are especially problematic for OOV tagging since most OOVs are NNs or NNPs. 4For example, there is a special tag ADD in the web domain for web addresses. The last two words of the sentence “I would like to host my upcoming website to/IN Liquidweb.com/ADD” are mistagged by Stanford tagger as “... to/TO Liquidweb.com/VB”. So the missing tag in this case also affects the tagging of surrounding words. 21 bio dev wsj trai</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Beatrice Santorini. 1990. Part-of-speech tagging guidelines for the Penn Treebank project (3rd revision, 2nd printing). Technical report, Department of Linguistics, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Schnabel</author>
<author>Hinrich Schütze</author>
</authors>
<title>Towards robust cross-domain domain adaptation for part-ofspeech tagging. In</title>
<date>2013</date>
<booktitle>IJCNLP,</booktitle>
<pages>198--206</pages>
<contexts>
<context position="17659" citStr="Schnabel and Schütze, 2013" startWordPosition="2977" endWordPosition="2980">domains. This argues for using l = 2, i.e., a window size of 5. Results for left-to-right (L-to-R) tagging are given on line 14. Similar to SVMTool and C&amp;P, each sentence is tagged from left to right and previous tagging decisions are used for the current classification. In this setting, we use the previous tag pi−1 as one additional feature in the feature vector of vi. The effect of left-to-right is similar to the effect of omitting suffixes: OOV accuracies go up in some domains, but ALL accuracies decrease (except for an increase of .02 for reviews). This is in line with the experiments in (Schnabel and Schütze, 2013) where sequential information in a CRF was not robust across domains. OOV tagging may benefit from correct previous tags because the larger left context that is indirectly made available by left-to-right tagging compensates partially for the lack of information about the OOV word. In contrast to standard approaches to POS tagging, the FLORS basic representation does not contain vocabulary indices. Line 15 shows what happens if we add them; the dimensionality of the feature vector is increased by 51V I – where V is the training set vocabulary – and in training one binary feature is set to one f</context>
<context position="19181" citStr="Schnabel and Schütze, 2013" startWordPosition="3232" endWordPosition="3235">tributional features, word signatures and suffixes all contribute to successful POS DA. Factors with only minor impact on performance are the number of indicator words used for the distributional representations, the window size l and the tagging scheme (L-to-R vs. nonL-to-R). Unknown words and known words behave differently with respect to certain feature choices. The different behavior of unknown and known words suggests that training and optimizing two separate models – an approach used by SVMTool – would further increase tagging accuracy. Note that there has been at least one publication (Schnabel and Schütze, 2013) on optimizing a separate model for unknown words that has in some cases better performance on OOV accuracy than what we publish here.2 However, this would complicate the architecture of FLORS. We opted for a maximally simple model in this paper, potentially at the cost of some performance. Test set results. Table 3 reports results on the test sets. FLORS again performs significantly better on all five TDs, both on ALL and OOV. Only in-domain on WSJ, ALL performance is worse. Finally, we compare our results to the POS taggers for which performance was reported at SANCL 2012 (Petrov and McDonal</context>
<context position="20722" citStr="Schnabel and Schütze (2013)" startWordPosition="3482" endWordPosition="3485">SANCL 2012, which were accuracies of 92.32 on newsgroups (HIT), 90.65 on reviews (HIT) and 91.07 on answers (IMS-1). 4 Discussion Advantages of FLORS representation. As we can see in Table 1, the main representational difference between FLORS and the other taggers is that the FLORS representation does not include vocabulary indices of the word to be tagged or its neighbors – the FLORS vector only consists of distributional, suffix and shape features. This is an obvious advantage for OOVs. In other representational schemes, OOVs have representations that are fundamentally different from known 2Schnabel and Schütze (2013) report OOV accuracies of 56.62 (newsgroups), 64.61 (reviews), 71.86 (weblogs), 54.28 (answers), 61.05 (emails) and 64.64 (BIO) for their basic model and even higher OOV accuracies if parameters are optimized on a per-domain basis. 3DCU-Paris13 is listed in the dependency parser tables, but DCU-Paris13 results are derived from a constituency parser. DCU also developed sophisticated preprocessing rules for the different domains, which can be viewed as a kind of manual domain adaptation. 19 newsgroups reviews weblogs answers emails wsj ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV ALL OOV 1 TnT 90.85*</context>
</contexts>
<marker>Schnabel, Schütze, 2013</marker>
<rawString>Tobias Schnabel and Hinrich Schütze. 2013. Towards robust cross-domain domain adaptation for part-ofspeech tagging. In IJCNLP, pages 198–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Part-of-speech induction from scratch.</title>
<date>1993</date>
<booktitle>In ACL,</booktitle>
<pages>251--258</pages>
<contexts>
<context position="6402" citStr="Schütze, 1993" startWordPosition="1041" endWordPosition="1042">tures. We represent each word w by four components: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f(w) = fleft(w)EDfright(w)EDfsuffix(w)EDfshape(w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of fleft(w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci, w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci, w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x). tf-weighting has been used by other researchers (Huang and Yates</context>
</contexts>
<marker>Schütze, 1993</marker>
<rawString>Hinrich Schütze. 1993. Part-of-speech induction from scratch. In ACL, pages 251–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In EACL,</booktitle>
<pages>141--148</pages>
<contexts>
<context position="6418" citStr="Schütze, 1995" startWordPosition="1043" endWordPosition="1044">sent each word w by four components: (i) counts of left neighbors, (ii) counts of right neighbors, (iii) binary suffix features and (iv) binary shape features. These four components are concatenated: f(w) = fleft(w)EDfright(w)EDfsuffix(w)EDfshape(w) We consider these sources of information equally important and normalize each of the four component vectors to unit length. Normalization also has a beneficial effect on SVM training time because it alleviates numerical problems (Fan et al., 2008). Distributional features. We follow a long tradition of older (Finch and Chater, 1992; Schütze, 1993; Schütze, 1995) and newer (Huang and Yates, 2009) work on creating distributional features for POS tagging based on local left and right neighbors. Specifically, the ith entry xi of fleft(w) is the weighted number of times that the indicator word ci occurs immediately to the left of w: xi = tf (freq (bigram(ci, w))) where ci is the word with frequency rank i in the corpus, freq (bigram(ci, w)) is the number of times the bigram “ci w” occurs in the corpus and we weight the non-zero frequencies logarithmically: tf(x) = 1 + log(x). tf-weighting has been used by other researchers (Huang and Yates, 2009) and show</context>
</contexts>
<marker>Schütze, 1995</marker>
<rawString>Hinrich Schütze. 1995. Distributional part-of-speech tagging. In EACL, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semisupervised condensed nearest neighbor for part-of-speech tagging. In ACL: Short papers,</title>
<date>2011</date>
<pages>48--52</pages>
<contexts>
<context position="41060" citStr="Søgaard, 2011" startWordPosition="6895" endWordPosition="6896">elf-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., 2009; Huang and Yates, 2010). Devising features based on labeled instances (instead of training on them) has been more successful (Florian et al., 2004; Søgaard, 2011). Chen et al. (2011) use co-training for DA. In each round of their algorithm, both new training instances from the unlabeled data and new features are added. Their model is limited to binary classification. The co-training method of Kübler and Baucom (2011) trains several taggers and adds sentences from the TD to the training set on which they agree. They report slight, but statistically significant increases in accuracy for POS tagging of dialogue data. Instance weighting. Instance weighting formalizes DA as the problem of having data from different probability distributions in each domain. </context>
</contexts>
<marker>Søgaard, 2011</marker>
<rawString>Anders Søgaard. 2011. Semisupervised condensed nearest neighbor for part-of-speech tagging. In ACL: Short papers, pages 48–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amarnag Subramanya</author>
<author>Slav Petrov</author>
<author>Fernando Pereira</author>
</authors>
<title>Efficient graph-based semi-supervised learning of structured tagging models.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>167--176</pages>
<contexts>
<context position="40293" citStr="Subramanya et al. (2010)" startWordPosition="6765" endWordPosition="6768">tractable and propose a new framework that allows prior knowledge to be integrated into representation learning. Latent sequence states are a form of word representation. Thus, it would be interesting to compare them to the non-sequence-based distributional representation that FLORS uses. Constraint-based methods. Rush et al. (2012) use global constraints on OOVs to improve out-ofdomain tagging. Although constraints ensure consistency, they require careful manual engineering. Distributional features can also be seen as a form of constraint since feature weights will be shared among all words. Subramanya et al. (2010) construct a graph to encourage similar n-grams to be tagged similarly, resulting in moderate gains in one domain, but no gains on BIO when compared to self-training. The reason could be an insufficient amount of unsupervised data for BIO (100,000 sentences). Our approach does not seem to suffer from this problem. Bootstrapping. Both self-training (McClosky et al., 2006) – which uses one classification model – and co-training (Blum and Mitchell, 1998) – which uses ≥2 models – have been applied to POS tagging. Self-training usually improves a POS baseline only slightly if at all (Huang et al., </context>
</contexts>
<marker>Subramanya, Petrov, Pereira, 2010</marker>
<rawString>Amarnag Subramanya, Slav Petrov, and Fernando Pereira. 2010. Efficient graph-based semi-supervised learning of structured tagging models. In EMNLP, pages 167–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>NAACLHLT,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="8566" citStr="Toutanova et al. (2003)" startWordPosition="1397" endWordPosition="1400">raphy pf10,1,2,31, vf10,1,2,31, affixes, orthography, word length pf10,1,2,31, vf10,1,2,31, affixes, orthography distributions of vf10,1,21, suffixes, orthography Table 1: Overview of baseline taggers and FLORS. vi: token, pi: POS tag. Positions included in the sets of token indices are relative to the position i of the word v0 to be tagged; e.g., pf10,1,21 is short for {p−0, p−1, p−2, p0, p1, p21. To represent tokens vi, models 1–4 use vocabulary indices and FLORS uses distributional representations. Models 2–4 use combinations of features (e.g., tag-word) as well. (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Miller et al. (2007)) we simply use all (lowercase) suffixes to avoid the need for selecting a subset of suffixes; and we treat all words equally as opposed to using suffix features for only a subset of words. For suffix s, we set the dimension corresponding to s in fsuffix(w) to 1 if lowercased w ends in s and to 0 otherwise. Note that w is a suffix of itself.1 Shape features. We use the Berkeley parser word signatures (Petrov and Klein, 2007). Each word is mapped to a bit string encompassing 16 binary indicators that correspond to different orthographic (e.g., does the word contain a digit</context>
<context position="10303" citStr="Toutanova et al., 2003" startWordPosition="1694" endWordPosition="1697">(i) high-performing publicly available systems, (ii) the taggers used at SANCL and (iii) POS DA results published for BIO. Most of our experiments use taggers from category (i) because we can ensure that experimental conditions are directly comparable. The four baselines in category (i) are shown in Table 1. Three have near state-of-the-art performance on WSJ: SVMTool (Giménez and Màrquez, 2004), Stanford 1One could also compute these suffixes for _w (w prefixed by underscore) instead of for w to include words as distinguishable special suffixes. We test this alternative in Table 2, line 15. (Toutanova et al., 2003) (a birectional MEMM) and C&amp;P. TnT (Brants, 2000) is included as a representative of fast and simple HMM taggers. In addition, C&amp;P is a tagger that has been extensively tested in DA scenarios with excellent results. Unless otherwise stated, we train all models using their default configuration files. We use the optimized parameter configuration published by C&amp;P for the C&amp;P model. Test set results will be compared with the SANCL taggers (category (ii)) at the end of Section 3. As far as category (iii) is concerned, most work on POS DA has been evaluated on BIO. We discuss our concerns about the</context>
<context position="25544" citStr="Toutanova et al. (2003)" startWordPosition="4298" endWordPosition="4301"> as each new occurrence of a word is encountered, the counts that are the basis 20 for the xi can simply be incremented. We present a direct comparison of FLORS representations with other representations in Section 5. “Local context” vs. sequence classification. The most common approach to POS tagging is to tag a sentence with its most likely sequence; in contrast, independent tagging of local context is not guaranteed to find the best sequence. Recent work on English suggests that window-based tagging can perform as well as sequence-based methods (Liang et al., 2008; Collobert et al., 2011). Toutanova et al. (2003) report similar results. In our experiments, we also did not find consistent improvements when we incorporated sequence constraints (Table 2, line 14). However, there may be languages and applications involving long-distance relationships where local-context classification is suboptimal. Local-context classification has two advantages compared to sequence classification. (i) It simplifies the classification and tagging setup: we can use any existing statistical classifier. Sequence classification limits the range of methods that can be applied; e.g., it is difficult to find a good CRF implemen</context>
<context position="33098" citStr="Toutanova et al. (2003)" startWordPosition="5591" endWordPosition="5594"> find representations that are robust across domains. In this section, we compare FLORS with two other widely used representation learning methods: (i) Brown clusters (Brown et al., 1992) and (ii) C&amp;W embeddings, the word embeddings of Collobert et al. (2011). We use fdist(w) = fleft(w)⊕fright(w) to refer to our own distributional word representations (see Section 2). The perhaps oldest and most frequently used lowdimensional representation of words is based on Brown clusters. Typically, prefixes of Brown clusters (Brown et al., 1992) are added to increase the robustness of POS taggers (e.g., Toutanova et al. (2003)). Computational costs are high (quadratic in the vocabulary size) although the computation can be parallelized (Uszkoreit and Brants, 2008). More recently, general word representations (Collobert et al., 2011; Turian et al., 2010) have been used for robust POS tagging. These word representations are typically trained on a large amount of unlabeled text and fine-tuned for specific NLP tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, eiFLORS 22 ther alone or in combination with other features. To compare fdist(w) (our distributional repre</context>
<context position="42854" citStr="Toutanova et al. (2003)" startWordPosition="7178" endWordPosition="7181">oaches of Giménez and Màrquez (2004) and C&amp;P. However, we do not use a left-to-right approach when tagging sentences. Moreover, SVMTool trains two separate models, one for OOVs and one for known words. FLORS only has a single model. In addition, 24 we do not make use of ambiguity classes, token-tag dictionaries and rare feature thresholds. Instead, we rely only on three types of features: distributional representations, suffixes and word shapes. The local-context-only approach of SVMTool, C&amp;P and FLORS is different from standard sequence classification such as MEMMs (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Tsuruoka and Tsujii (2005)) and CRFs (e.g., Collins (2002)). Sequence models are more powerful in theory, but this may not be an advantage in DA because the subtle dependencies they exploit may not hold across domains. 7 Conclusion We have presented FLORS, a new POS tagger for DA. FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags. FLORS is simpler and faster than previous DA methods, yet we were able to demonstrate that it has significantly better accuracy than several baselines. Acknowledgments. This work was supported by DFG </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACLHLT, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data. In</title>
<date>2005</date>
<booktitle>EMNLP-HLT,</booktitle>
<pages>467--474</pages>
<contexts>
<context position="42882" citStr="Tsuruoka and Tsujii (2005)" startWordPosition="7182" endWordPosition="7185">quez (2004) and C&amp;P. However, we do not use a left-to-right approach when tagging sentences. Moreover, SVMTool trains two separate models, one for OOVs and one for known words. FLORS only has a single model. In addition, 24 we do not make use of ambiguity classes, token-tag dictionaries and rare feature thresholds. Instead, we rely only on three types of features: distributional representations, suffixes and word shapes. The local-context-only approach of SVMTool, C&amp;P and FLORS is different from standard sequence classification such as MEMMs (e.g., Ratnaparkhi (1996), Toutanova et al. (2003), Tsuruoka and Tsujii (2005)) and CRFs (e.g., Collins (2002)). Sequence models are more powerful in theory, but this may not be an advantage in DA because the subtle dependencies they exploit may not hold across domains. 7 Conclusion We have presented FLORS, a new POS tagger for DA. FLORS uses robust representations that work especially well for unknown words and for known words with unseen tags. FLORS is simpler and faster than previous DA methods, yet we were able to demonstrate that it has significantly better accuracy than several baselines. Acknowledgments. This work was supported by DFG (Deutsche Forschungsgemeinsc</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In EMNLP-HLT, pages 467–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="7573" citStr="Turian et al. (2010)" startWordPosition="1244" endWordPosition="1247">s been used by other researchers (Huang and Yates, 2009) and showed good performance in our own previous work. fright(w) is defined analogously. We restrict the set of indicator words to the n = 500 most frequent words in the corpus. To avoid zero vectors, we add an entry xn+1 to each vector that counts omitted contexts: xn+1 = tf ⎛ ⎝freq (bigram(cj, w)) j:j&gt;n We compute distributional vectors on the joint corpus DALL of all labeled and unlabeled text of source domain and TD. The text is preprocessed by lowercasing everything – which is often done when computing word representations, e.g., by Turian et al. (2010) – and by padding sentences with (BOUNDARY) tokens. Suffix features. Suffixes are promising for DA because basic morphology rules are the same in different domains. In contrast to other work on tagging 16 model classifier 1 TnT HMM 2 Stanford bidir. MEMM 3 SVMTool SVM 4 C&amp;P SVM 5 FLORS SVM features p−10,1,21, v0, suffixes (for OOVs) pf10,1,21, vf10,11, affixes, orthography pf10,1,2,31, vf10,1,2,31, affixes, orthography, word length pf10,1,2,31, vf10,1,2,31, affixes, orthography distributions of vf10,1,21, suffixes, orthography Table 1: Overview of baseline taggers and FLORS. vi: token, pi: POS</context>
<context position="33329" citStr="Turian et al., 2010" startWordPosition="5624" endWordPosition="5627">f Collobert et al. (2011). We use fdist(w) = fleft(w)⊕fright(w) to refer to our own distributional word representations (see Section 2). The perhaps oldest and most frequently used lowdimensional representation of words is based on Brown clusters. Typically, prefixes of Brown clusters (Brown et al., 1992) are added to increase the robustness of POS taggers (e.g., Toutanova et al. (2003)). Computational costs are high (quadratic in the vocabulary size) although the computation can be parallelized (Uszkoreit and Brants, 2008). More recently, general word representations (Collobert et al., 2011; Turian et al., 2010) have been used for robust POS tagging. These word representations are typically trained on a large amount of unlabeled text and fine-tuned for specific NLP tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, eiFLORS 22 ther alone or in combination with other features. To compare fdist(w) (our distributional representations) with Brown clusters, we induced 1000 Brown clusters on the joint corpus data DALL (see Section 2) using the publicly available implementation of Liang (2005). We padded sentences with (BOUNDARY) tokens on each side and </context>
<context position="35087" citStr="Turian et al., 2010" startWordPosition="5920" endWordPosition="5923">preprocess our raw tokens the same way they do (lowercase and replace sequences of digits by “0”) before we look up a representation during training and testing. We replaced the distributional features in our basic setup by either Brown cluster features or C&amp;W embeddings. Table 7 repeats lines 5 and 7 of Table 2 and gives results of the modified FLORS setup. All three representations improve both ALL and OOV accuracies in all domains. fdist outperforms Brown in all cases except for OOV on emails. Brown may suffer from noisy data; cleaning methods have been used in the literature (Liang, 2005; Turian et al., 2010), but they are not unproblematic since a large part of the data available is lost, which results in more unknown words. Brown and fdist can be directly compared since they were trained on exactly the same data. fdist and C&amp;W are harder to compare directly because there are many differences. (i) C&amp;W is trained on a much larger dataset. One consequence of this is that OOV accuracy on WSJ may be higher because some words that are unknown for other methods are actually known to C&amp;W. (ii) C&amp;W vectors are not trained on the SANCL TD data sets – this gives fdist an advantage. (iii) C&amp;W vectors are no</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shulamit Umansky-Pesin</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>A multi-domain web-based algorithm for POS tagging of unknown words.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>1274--1282</pages>
<contexts>
<context position="38884" citStr="Umansky-Pesin et al. (2010)" startWordPosition="6545" endWordPosition="6548">ist(w), n=0 89.14* 55.59* 91.80* 66.31* 93.40* 72.55* 89.47* 55.82* 88.21* 57.83* 96.29* 85.55* 3FLO C&amp;W for fdist(w) 90.57 64.57 92.54* 72.48* 94.51 80.58* 90.23 60.99 89.44 63.13 96.72 90.48 4 Brown for fdist(w) 90.34* 62.41* 92.23* 71.47* 94.45 81.76 89.71* 56.28* 89.02* 63.20 96.48* 87.50 Table 7: Tagging accuracy of different word representations on the dev sets. Line 1 corresponds to FLORS basic. n: number of indicator words. A column’s best result is bold. Moreover, FLORS representations consist of simple counts whereas SCL solves a separate optimization problem for each pivot feature. Umansky-Pesin et al. (2010) derive distributional information for OOVs by running web queries. This approach is slow since it depends on a search engine. Ganchev et al. (2012) successfully use search logs. This is a promising enhancement for FLORS. Huang and Yates (2009) evaluate CRFs with distributional features. They examine lower dimensional feature representations using SVD or the latent states of an unsupervised HMM. They find better accuracies for their HMM method than Blitzer et al. (2006); however, they do not compare them against a CRF baseline using distributional features. In later work, Huang and Yates (2010</context>
</contexts>
<marker>Umansky-Pesin, Reichart, Rappoport, 2010</marker>
<rawString>Shulamit Umansky-Pesin, Roi Reichart, and Ari Rappoport. 2010. A multi-domain web-based algorithm for POS tagging of unknown words. In COLING, pages 1274–1282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>755--762</pages>
<contexts>
<context position="33238" citStr="Uszkoreit and Brants, 2008" startWordPosition="5610" endWordPosition="5613">ng methods: (i) Brown clusters (Brown et al., 1992) and (ii) C&amp;W embeddings, the word embeddings of Collobert et al. (2011). We use fdist(w) = fleft(w)⊕fright(w) to refer to our own distributional word representations (see Section 2). The perhaps oldest and most frequently used lowdimensional representation of words is based on Brown clusters. Typically, prefixes of Brown clusters (Brown et al., 1992) are added to increase the robustness of POS taggers (e.g., Toutanova et al. (2003)). Computational costs are high (quadratic in the vocabulary size) although the computation can be parallelized (Uszkoreit and Brants, 2008). More recently, general word representations (Collobert et al., 2011; Turian et al., 2010) have been used for robust POS tagging. These word representations are typically trained on a large amount of unlabeled text and fine-tuned for specific NLP tasks. Similar to Brown clusters, they are low-dimensional and can be used as features in many NLP tasks, eiFLORS 22 ther alone or in combination with other features. To compare fdist(w) (our distributional representations) with Brown clusters, we induced 1000 Brown clusters on the joint corpus data DALL (see Section 2) using the publicly available i</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In ACL, pages 755– 762.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
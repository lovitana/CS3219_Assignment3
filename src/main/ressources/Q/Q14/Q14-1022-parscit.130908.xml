<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.970521">
Dense Event Ordering with a Multi-Pass Architecture
</title>
<author confidence="0.849693">
Nathanael Chambers
</author>
<affiliation confidence="0.648529">
United States Naval Academy
</affiliation>
<email confidence="0.97375">
nchamber@usna.edu
</email>
<author confidence="0.997943">
Bill McDowell
</author>
<affiliation confidence="0.991741">
Carnegie Mellon University
</affiliation>
<email confidence="0.994536">
forkunited@gmail.com
</email>
<sectionHeader confidence="0.995588" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966333333333">
The past 10 years of event ordering research
has focused on learning partial orderings over
document events and time expressions. The
most popular corpus, the TimeBank, contains
a small subset of the possible ordering graph.
Many evaluations follow suit by only testing
certain pairs of events (e.g., only main verbs
of neighboring sentences). This has led most
research to focus on specific learners for par-
tial labelings. This paper attempts to nudge
the discussion from identifying some relations
to all relations. We present new experiments
on strongly connected event graphs that con-
tain ∼10 times more relations per document
than the TimeBank. We also describe a shift
away from the single learner to a sieve-based ar-
chitecture that naturally blends multiple learn-
ers into a precision-ranked cascade of sieves.
Each sieve adds labels to the event graph one
at a time, and earlier sieves inform later ones
through transitive closure. This paper thus de-
scribes innovations in both approach and task.
We experiment on the densest event graphs to
date and show a 14% gain over state-of-the-art.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909111111111">
Event ordering in the NLP community usually refers
to a specific labeling task: given pairs of events and
time expressions, choose ordering relations for them.
The TimeBank corpus (Pustejovsky et al., 2003) pro-
vided a corpus of labeled pairs that motivated ma-
chine learning approaches to this task. However,
only a small subset of possible pairs are labeled. The
annotators identified the most central and obvious re-
lations, and left the rest unlabeled. Partly because of
</bodyText>
<note confidence="0.455049">
Taylor Cassidy
Army Research Lab
IBM Research
</note>
<email confidence="0.694989">
taylorcassidy64@gmail.com
</email>
<author confidence="0.994238">
Steven Bethard
</author>
<affiliation confidence="0.992779">
University of Alabama at Birmingham
</affiliation>
<email confidence="0.988243">
bethard@cis.uab.edu
</email>
<bodyText confidence="0.999771441176471">
this, there has been little research into complete event
graphs. This paper describes our recent exploration
into the dramatic changes required of both learning
algorithms and experimental setup when faced with
the task of a more complete graph labeling.
The most obvious shift required of a complete
graph labeling algorithm is that of relation identifica-
tion. Research on the TimeBank and in the TempEval
contests has largely focused on relation classification.
The event pairs are given, and the task is to classify
them. We are now forced to first determine which
events should be paired up before classification. Our
experiments here fully integrate and evaluate both
identification and classification.
We are not the first to approach relation identifi-
cation with classification, but this paper is the first
to directly and comprehensively address it. Most
recently, TempEval 3 (UzZaman et al., 2013a) pro-
posed a labeling of raw text without prior relation
identification, but the challenge ultimately relied on
the TimeBank. Systems were only evaluated on its
subset of labeled event pairs. This meant that relation
identification was largely ignored. The top system op-
timized only relation classification and intentionally
left many pairs unlabeled (Bethard, 2013).
This paper presents CAEVO, a CAscading EVent
Ordering architecture. It is a novel sieve-based ar-
chitecture for temporal event ordering that directly
addresses the interplay between identification and
classification. We shift away from the idea of mono-
lithic learners, and propose smaller specialized clas-
sifiers. Inspiration comes from the recent success in
named entity coreference with sieve-based learning
(Lee et al., 2013). CAEVO contains a host of classi-
</bodyText>
<page confidence="0.992835">
273
</page>
<bodyText confidence="0.955695464285714">
Transactions of the Association for Computational Linguistics, 2 (2014) 273–284. Action Editor: Ellen Riloff.
Submitted 11/2013; Revised 5/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
fiers that each specialize on different types of edges.
The classifiers are ordered by their individual preci-
sion, and run in order starting with the most precise.
Each sieve informs those below it by passing on its
decisions as graph constraints. These more precise
constraints are then used by later sieves to assist their
less precise decisions.
One of the main advantages of this CAEVO archi-
tecture is the natural inclusion of transitive reasoning.
After each sieve proposes its labels, the architecture
infers transitive links from the new labels and adds
them to the graph. This type of expansion has not
been adequately evaluated due to our community’s
sparsely labeled corpora. We are the first to report the
surprising result that edges from transitive closure are
more precise than the original sieve-provided labels.
Finally, CAEVO enables quick experimentation.
Classifiers are pulled in and out with ease, quickly
testing new theories. As in D’Souza and Ng (2013),
we created rule-based classifiers based on linguistic
theory. We naturally integrate these with machine
learned classifiers using the sieve architecture. Each
classifier focuses on its independent decisions, and
the architecture then imposes transitivity constraints.
CAEVO contains 12 sieves ordered by precision, and
it outperforms the top systems in event ordering.
</bodyText>
<sectionHeader confidence="0.996191" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99990575">
Two lines of research are particularly relevant to our
work: research on event ordering annotation, and
research on event ordering models. We briefly review
both here.
</bodyText>
<subsectionHeader confidence="0.982036">
2.1 Event Ordering Annotation
</subsectionHeader>
<bodyText confidence="0.943476076923077">
Most prior annotation efforts constructed corpora
with only a small subset of temporal relations anno-
tated. In the TimeBank (Pustejovsky et al., 2003),
temporal relations were only annotated when the re-
lation was judged to be salient by the annotator. The
TempEval competitions (Verhagen et al., 2007; Ver-
hagen et al., 2010; UzZaman et al., 2013b) aimed to
improve coverage by annotating relations between
all events and times in the same sentence. However,
event tokens that were mentioned fewer than 20 times
were excluded and only one TempEval task consid-
ered relations between events in different sentences.
Events Times Relations R
</bodyText>
<table confidence="0.99921325">
TimeBank 7935 1414 6418 0.7
Bramsen 2006 627 – 615 1.0
TempEval 2007 6832 1249 5790 0.7
TempEval 2010 5688 2117 4907 0.6
TempEval 2013 11145 2078 11098 0.8
Kolomiyets 2012 1233 – 1139 0.9
Do 20121 324 232 3132 5.6
This work 1729 289 12715 6.3
</table>
<tableCaption confidence="0.9967155">
Table 1: Events, times, temporal relations and the ratio of
relations to events + times (R) in various corpora.
</tableCaption>
<bodyText confidence="0.999032">
To avoid such sparse annotations, researchers have
explored schemes that encourage annotators to con-
nect all events. Bramsen et al. (2006) annotated
timelines as directed acyclic graphs, though they an-
notated multi-sentence segments of text rather than
individual events. Kolomiyets et al. (2012) annotated
“temporal dependency structures” (i.e. dependency
trees of temporal relations), though they only focused
on pairs of events. In Do et al. (2012), “the anno-
tator was not required to annotate all pairs of event
mentions, but as many as possible”, and then more
relations were automatically inferred after the anno-
tation was complete. In contrast, in our work we
required annotators to label every possible pair of
events/times in a given window, and our event graphs
are guaranteed to be strongly connected, with every
edge verified by the annotators. Table 1 compares the
density of relation annotation across various corpora.
A major dilemma from this prior work is that unla-
beled event/time pairs are inherently ambiguous. The
unlabeled pair holds 3 possibilities:
</bodyText>
<listItem confidence="0.956670666666667">
1. The annotator looked at the pair of events and
decided that multiple valid relations could apply.
2. The annotator looked at the pair of events and
decided that no temporal relation exists
3. The annotator failed to look at the pair of events,
so a single relation may exist.
</listItem>
<bodyText confidence="0.999736833333333">
This ambiguity makes accurate training and evalu-
ation difficult. In the current work, we adopt the
VAGUE relation, introduced by TempEval 2007, and
force our annotators to indicate pairs for which no
clear temporal relation exists. We are thus the first to
eliminate the 3rd possibility from our corpus.
</bodyText>
<footnote confidence="0.99573">
1Do 2012 reports 6264 relations, but we list half of this
because they include relations and their inverses in the count.
</footnote>
<page confidence="0.995177">
274
</page>
<subsectionHeader confidence="0.996705">
2.2 Event Ordering Models
</subsectionHeader>
<bodyText confidence="0.999988555555556">
In part because of the sparsity of temporal relations in
the available training corpora, most existing models
formulate temporal ordering as a pair-wise classifi-
cation task, where each pair of events and/or times
is examined and classified as having a temporal rela-
tion or not. Early work on the TimeBank took this
approach (Boguraev and Ando, 2005), classifying re-
lations between all events and times within 64 tokens
of each other. Most of the top-performing systems
in the TempEval competitions also took this pair-
wise classification approach for both event-time and
event-event temporal relations (Bethard and Martin,
2007; Cheng et al., 2007; UzZaman and Allen, 2010;
Llorens et al., 2010; Bethard, 2013). These systems
have sometimes even explicitly focused on a small
subset of temporal relations; for example, the top-
ranked ordering system in TempEval 2013 (Bethard,
2013) only classified relations in certain syntactic
constructions and with certain relation types.
Systems have tried to take advantage of global
information to ensure that the pair-wise classifica-
tions satisfy temporal logic transitivity constraints,
using frameworks like integer linear programming
and Markov logic networks (Bramsen et al., 2006;
Chambers and Jurafsky, 2008; Tatu and Srikanth,
2008; Yoshikawa et al., 2009; UzZaman and Allen,
2010). The gains have been small, likely because
of the disconnectedness that is common in sparsely
annotated corpora (Chambers and Jurafsky, 2008).
An approach that has not been leveraged for event
ordering, but that has been successful in the corefer-
ence community is the sieve-based architecture. The
top performer in CoNLL-2011 shared task was one
such system (Lee et al., 2013). The core idea is to
begin with the most reliable classifier first, and in-
form those below it. This idea also appeared in the
early IBM MT models (Brown et al., 1993) and in
the “islands of reliability” approaches to parsing and
speech (Borghesi and Favareto, 1982; Corazza et al.,
1991). D’Souza and Ng (2013) recently combined
a rule-based model with a machine learned model,
but lacked the fine-grained formality of a cascade of
sieves. This paper is inspired by the above and is the
first to apply it to temporal ordering as an extensible,
formal architecture.
</bodyText>
<subsectionHeader confidence="0.994081">
The TimeBank TimeBank-Dense
</subsectionHeader>
<bodyText confidence="0.992917375">
There were four or five people
inside, and they just started firing
Ms. Sanders was hit several times
and was pronounced dead at the
scene.
The other customers fled, and the
police said it did not appear that
anyone else was injured.
</bodyText>
<figureCaption confidence="0.924218333333333">
Figure 1: TimeBank document on the left with TimeBank-
Dense on the right. Solid arrows indicate BEFORE and
dotted INCLUDED IN. Relations with the DCT not shown.
</figureCaption>
<sectionHeader confidence="0.971645" genericHeader="method">
3 TimeBank-Dense: A Dense Ordering
</sectionHeader>
<bodyText confidence="0.999987227272727">
We use a new corpus, called TimeBank-Dense (Cas-
sidy et al., 2014), to motivate and evaluate our archi-
tecture. This section highlights its main features.
The TimeBank-Dense corpus was created to ad-
dress the sparsity in current corpora. It is unique
in that the annotators were forced to label all local
edges, even in ambiguous cases. The corpus is not
a complete graph over events and time expressions,
but it approximates completeness by labeling locally
complete graphs over neighboring sentences. All
pairs of events and time expressions in the same sen-
tence and all pairs of events and time expressions
in the immediately following sentence were labeled.
It also includes edges between every event and the
document creation time (DCT). The document event
graphs in TimeBank-Dense are thus strongly con-
nected2, but they are not complete graphs because
edges that cross 2 or more sentences are not explic-
itly included3. See Figure 1 for an illustration of
the significant difference between a TimeBank docu-
ment and a TimeBank-Dense document. The specific
edges that were labeled are as follows:
</bodyText>
<listItem confidence="0.997536">
1. Event-Event, Event-Time, and Time-Time pairs in
the same sentence
2. Event-Event, Event-Time, and Time-Time pairs be-
tween the current and next sentence
3. Event-DCT pairs for every event in the document
(DCT is the document creation time)
4. Time-DCT pairs for every time expression in the
document
</listItem>
<footnote confidence="0.969869333333333">
2Viewed as an undirected graph: since all events connect the
DCT, there exists a path from every event to every other event.
3Though many are inferred through transitive closure.
</footnote>
<bodyText confidence="0.99863">
The other customers fled, and the
police said it did not appear that
anyone else was injured.
There were four or five people
inside, and they just started firing
Ms. Sanders was hit several times
and was pronounced dead at the
scene.
</bodyText>
<page confidence="0.891008">
275
</page>
<bodyText confidence="0.999652795454545">
TimeBank-Dense contains 12,715 labeled rela-
tions over 36 TimeBank newspaper articles. It uses
TimeBank annotated events and time expressions,
and then annotates the edges between them. The set
of temporal relations are a subset of the 13 original
Allen relations (Allen, 1983). The TempEval con-
tests have used both a smaller set of relations (Temp-
Eval 1) and all 13 relations (TempEval 3). Published
work mirrors this trend, and different groups focus
on different aspects of the semantics. The TimeBank-
Dense set is a middle ground between coarse and fine-
grained distinctions: BEFORE, AFTER, INCLUDES,
IS INCLUDED, SIMULTANEOUS, and VAGUE.
The main reason for not using a more fine-grained
set is because we annotate pairs that are more am-
biguous than those considered in previous efforts.
Decisions between relations like BEFORE and IM-
MEDIATELY BEFORE complicate an already difficult
task. The added benefit of a system that makes fine-
grained distinctions is also not clear. We lean toward
higher annotator agreement by using relations that
have a greater separation between their semantics.4
Annotators of TimeBank-Dense also had to mu-
tually agree on each labeled edge, otherwise it was
labeled VAGUE. Precision between annotators av-
eraged 65.1%, and the kappa ranged from .56 to
.64. For comparison, TimeBank annotators averaged
77% precision with a .71 kappa. The breakdown
of relation counts is shown in Table 2. T3 refers
to TempEval-3. TempEval-3 used TimeBank doc-
uments, but removed a small portion of its events.
This paper evaluates on T3 data to maintain a connec-
tion to the recent competition, but this resulted in the
removal of a portion of the densely annotated edges.
The most significant change in this new corpus is
the inclusion of the VAGUE relation. This allows a
system to separate relation identification from clas-
sification, if desired. The VAGUE relation makes up
46% of the annotated graph. This is the first empiri-
cal measurement of how many edges do not carry a
clear semantics. This annotation eliminates 1 of the 3
ambiguities (discussed in Section 2) that has plagued
ordering research: the annotator failed to look at the
pair of events, so a single relation may exist.
</bodyText>
<footnote confidence="0.99136025">
4For instance, a relation like STARTS is a special case of
INCLUDES if events are viewed as open intervals, and IMME-
DIATELY BEFORE is a special case of BEFORE. We avoid this
overlap and only use INCLUDES and BEFORE.
</footnote>
<table confidence="0.9411335">
TimeBank-Dense Relation Count
b a i ii s v Total
All 2590 2104 836 1060 215 5910 12715
T3 train 1444 1148 473 629 120 3013 6827
T3 dev 242 218 37 73 20 359 949
T3 test 589 428 116 159 39 900 2231
</table>
<tableCaption confidence="0.969198333333333">
Table 2: The number of each relation type in the
TimeBank-Dense corpus. T3 is the TempEval3 version
used in this paper’s experiments.
</tableCaption>
<table confidence="0.998011">
# of Mutual Vague Partial Vague No Vague
1657 (28%) 3234 (55%) 1019 (17%)
</table>
<tableCaption confidence="0.978897333333333">
Table 3: VAGUE relation origins. Partial vague: one anno-
tator does not choose vague. No vague: neither annotator
chooses vague, but they disagree on the specific relation.
</tableCaption>
<bodyText confidence="0.999943607142857">
This is a significant improvement over previous
ordering annotations. Several questions can now be
answered to benefit learning algorithms. Which set
of edges in the graph are unambiguous? Can I know
for certain that I have all BEFORE relations in this
document? These questions are critical to supervised
learning, and we see the removal of this ambiguity as
an important step toward better classification.
Of course, an ambiguity still remains: does VAGUE
mean that no relation exists, or that multiple relations
are possible? This is not unique to our corpus, but
is present in all current ordering corpora. We erred
on the side of caution because it is not clear how this
can be resolved. Though we don’t solve this problem
here, we can more deeply analyze how VAGUE rela-
tions come about during annotation. Table 3 shows
the 3 annotation scenarios that result in a VAGUE rela-
tion, and how often each occurred: (1) mutual vague:
annotators agreed on VAGUE, (2) partial vague: one
annotator chose VAGUE but the other chose a non-
vague relation, and (3) no vague: annotators chose
different non-vague relations. These disagreements
will be released with the corpus, enabling future re-
search into distinguishing these fine-grained seman-
tics.
TimeBank-Dense is split into a 22 document train-
ing set, 5 document dev set, and a 9 document test
set 5. We follow this split in all experiments.
</bodyText>
<footnote confidence="0.99885">
5Available at http://www.usna.edu/Users/cs/nchamber/caevo/
</footnote>
<page confidence="0.997423">
276
</page>
<sectionHeader confidence="0.953577" genericHeader="method">
4 Sieve-Based Temporal Ordering
</sectionHeader>
<bodyText confidence="0.9999625">
The sieve architecture applies a sequence of temporal
relation classifiers, one at a time, to label the edges of
a graph of events and time expressions. The individ-
ual classifiers are called sieves, and each sieve passes
its temporal relation decisions onto the next sieve.
The input to each sieve is the partially labeled graph
from previous sieves. These previous decisions can
help inform a sieve’s own decisions. The sieves are
ordered by precision, running the most precise mod-
els first. Events and time expressions are assumed to
be annotated with their standard attributes (such as
verb tense). This work uses gold event attributes for
consistent analysis of ordering decisions.
One of the benefits of this architecture is the seam-
less enforcement of transitivity constraints. Indepen-
dent pairwise classifiers often produce inconsistent la-
belings. The architecture avoids inconsistent graphs
by inferring all transitive relations from each sieve’s
output before the graph is passed on to the next one.
Specifically, the nth sieve cannot add an inconsistent
edge that is inferrable from the previous n − 1 sieves.
Transitive inference is run after each stage, so it is
not possible to relabel an already inferred edge. In
a similar fashion, the architecture enforces the pre-
cision preference rule: the nth sieve may not label
edge a if one of the previous n − 1 sieves has already
labeled it. Transitive inferences are made with all
possible relations (e.g., A before B and B includes
C infers A before C). The only relation that does not
infer new edges is VAGUE.
This architecture facilitates a seamless integration
of a wide variety of classifiers. We experimented
with both rule-based and machine learned classifiers.
The following subsections describe the final set of 12
temporal classifiers in CAEVO. We begin with the
deterministic rule-based sieves.
</bodyText>
<subsectionHeader confidence="0.989448">
4.1 Deterministic Sieves
</subsectionHeader>
<bodyText confidence="0.999741777777778">
The most precise sieves come from linguistic insight
into syntactic and semantic patterns. Many pairs of
events share common syntactic attributes that clearly
indicate a temporal relation. Machine learning is
not needed for these known linguistic phenomena,
and a trained classifier can incorrectly label these
obvious pairs if other features incorrectly override
the decision. We therefore rely on deterministic rule-
based approaches to capture these phenomena.
</bodyText>
<subsectionHeader confidence="0.420857">
4.1.1 Verb/Timex Adjacency
</subsectionHeader>
<bodyText confidence="0.999805">
Many prepositions in English carry either explicit
(e.g., before, during) or implicit temporal semantics
(e.g., in, over). Several are so reliable that a simple
series of rules can label event-timex edges with very
high precision. This Verb/Timex Adjacency sieve
thus applies to event words and time expressions that
are connected by a direct path in the syntactic parse
tree, as well as all event/time pairs such that the event
is at most 2 tokens before the time expression.
Time expressions under a preposition are handled
with specific rules. For instance, in, on, over, dur-
ing, and within all resolve to IS INCLUDED. The
prepositions for, at, and throughout resolve to SI-
MULTANEOUS. In the absence of a preposition, a
timex might simply be a modifier of the verb. The
following is one such example.
Police confirmed Friday that the body
found along a highway...
The confirmed event IS INCLUDED in Friday. The
edge between an event and a direct time modifier all
default to IS INCLUDED. The precision of this sieve
on the development set is 92%, the top performer.
</bodyText>
<subsubsectionHeader confidence="0.469938">
4.1.2 Timex-Timex Relations
</subsubsectionHeader>
<bodyText confidence="0.999983176470588">
One of the most precise sieves is a small set of
deterministic rules that address relations between
time expressions (including the DCT). This sieve
uses the normalized time values for time expressions
to compare their start/end times. Given two time
spans with defined starting and ending time points,
the temporal relation is unambiguous.
The more difficult cases involve time expressions
without stated start/end points. Examples of these
include references to the abstract future or past, such
as in phrases like ‘last year’ and ‘next quarter’. This
sieve chooses BEFORE if the abstract references are
not the same, but in clear opposition to each other.
For instance, a past reference and a future reference
are labeled BEFORE. If the abstract references are the
same, then VAGUE is typically applied. We defer to
the publicly released code for further details.
</bodyText>
<subsectionHeader confidence="0.652871">
4.1.3 Reporting Event with DCT
</subsectionHeader>
<bodyText confidence="0.9668005">
Newspaper articles contain an abundance of re-
porting events (e.g., said, reply, tell). We crafted a
</bodyText>
<page confidence="0.981743">
277
</page>
<bodyText confidence="0.999966454545455">
targeted sieve to handle this genre-specific phenom-
ena. Since the document creation time is often the
day of the reporting events, edges between report-
ing events and the DCT are labeled as IS INCLUDED.
This is essentially a high baseline for event-DCT
edges. One of the advantages of the sieve-based ar-
chitecture is that this sieve can be placed later in
the cascade of sieves. More sophisticated reasoners
run before this one, making more informed decisions
when appropriate, and this sieve cleans up the remain-
ing reporting-DCT edges that are still unlabeled.
</bodyText>
<subsubsectionHeader confidence="0.479523">
4.1.4 Reporting Event with Dominated Event
</subsubsectionHeader>
<bodyText confidence="0.9998853">
We also created rules for edges between a report-
ing event and another event that is syntactically dom-
inated by the reporting event. Statistical classifiers
can pick up patterns for reporting verbs, but the la-
bels are consistent enough that a few deterministic
rules can handle many of the cases. The following
rules are solely based on the tense and grammatical
aspect of each event.
The following is a sampling of these rules, where
gov is the governing event and dep the dependent.
</bodyText>
<listItem confidence="0.953578272727273">
1. If gov is present, and dep is past
then (gov AFTER dep)
2. If gov is present, and dep is present perfect
then (gov AFTER dep)
3. If gov is present, and dep is future
then (gov BEFORE dep)
4. If gov is past, and dep is past perfective
then (gov AFTER dep)
5. If gov is past, and dep is past progressive
then (gov IS INCLUDED dep)
4.1.5 General Event with Dominated Event
</listItem>
<bodyText confidence="0.9999485">
We generalized the above reporting-event sieve to
apply to all types of events. This ‘backoff’ sieve han-
dles the non-reporting event pairs where one event
immediately dominates another. It operates similar to
the preceding sieve and uses event features like tense
and aspect to make its decisions. However, unlike the
preceding, the decisions are split into different rule
sets based on the specific typed dependency relation
that connects the two events. We have separate rules
nsubj, dobj, xcomp, ccomp, advcl, and conj.
</bodyText>
<subsectionHeader confidence="0.666542">
4.1.6 Reichenbach Rules
</subsectionHeader>
<bodyText confidence="0.9240905">
Reichenbach (1947) defined the role played by the
various tenses of English verbs in conveying tem-
</bodyText>
<note confidence="0.829737">
Simple Past: John left the room
</note>
<figure confidence="0.6306876">
E,R S
Anterior Past: John had left the room
E R S
Posterior Past: (I did not know that) John would leave the room
R E,S
</figure>
<figureCaption confidence="0.982538">
Figure 2: Timelines in which the ordering S &lt; R is fixed
because the verb to leave is in the past tense.
</figureCaption>
<bodyText confidence="0.999391">
poral information in a discourse. Each tense is dis-
tinguished in terms of the relative ordering of the
following time points:
</bodyText>
<listItem confidence="0.994767857142857">
• Point of speech (S) The time at which an act of
speech is performed.
• Point of event (E) The time at which the the event
referred to by the verb occurs.
• Point of reference (R) A single time with respect to
which S is ordered by one dimension of tense, and
to which E is ordered by another.
</listItem>
<bodyText confidence="0.99981036">
Reichenbach’s account maps the set of possi-
ble orderings of S, E, and R, where each pair
of elements can be ordered with &lt; (before), =
(simultaneous), or &gt; (after), onto a set of tense
names given by: {anterior, simple, posterior} x
{past, present, future}. The relative ordering of
E and R is indicated by the first dimension, while
that of S and R is given by the second. Figure 2
depicts examples of the ordering R &lt; S.
Similar to Derczynski and Gaizauskas (2013) we
map a subset of Reichenbach’s tenses onto pairs of
TimeML tense and aspect attribute values, which
are given by {simple, perfect, progressive} x
{past, present, future}, where the first dimension
is grammatical aspect and the second is tense. We
refer to an event’s tense/aspect combination as its
tense-aspect profile. Consider two events e1 and
e2 associated with S1/E1/R1 and S2/E2/R2. Intu-
itively, given R1 = R2 and the time point orderings
for each event (derived from its tense-aspect profile)
we can enumerate the possible interval relations that
might hold between E1 and E2, which we can in-
terpret as a disjunction of the possible orderings of
e1 and e2 (Derczynski and Gaizauskas, 2013). We
identify a subset of tense/aspect profile pairs that
</bodyText>
<page confidence="0.992042">
278
</page>
<listItem confidence="0.602652444444444">
unambiguously yield an interval relation from our
6 relations, and implement them as rules in the Re-
ichenbach Sieve. Three such rules are given here:
1. If e1 is past/simple and e2 is past/perfect
then (e1 AFTER e2)
2. If e1 is future/simple and e2 is present/perfect
then (e1 AFTER e2)
3. If e1 is past/simple and e2 is future/simple
then (e1 BEFORE e2)
</listItem>
<bodyText confidence="0.9997739">
The Reichenbach sieve achieved a precision of
61% on the entire dataset, and 91% when links la-
beled VAGUE were excluded, a notably larger in-
crease over other sieves. The lower 61% is primarily
due to the abundance of non-canonical usages of var-
ious tense/aspect profiles. For example, the present
tense may convey habitual action, an event conveyed
with the future tense in a quotation may have already
occurred, the present perfect may be used to indicate
that an event occurred at least once in the past, etc.
</bodyText>
<subsectionHeader confidence="0.864377">
4.1.7 WordNet Rules
</subsectionHeader>
<bodyText confidence="0.999986727272727">
Determining whether two words are precisely co-
referent is notoriously difficult (Hovy et al., 2013).
Synonymous words, or even those sharing the same
surface form may be only partially co-referent and
thus likely labeled VAGUE by annotators. For exam-
ple, news articles state what someone said multiple
times without revealing temporal ordering between
the events. Event-event edges whose events share
the same lemma, or are each a member of the same
synset are thus labeled VAGUE. Time-time edges with
the same lemma/synset are labeled SIMULTANEOUS.
</bodyText>
<subsectionHeader confidence="0.945147">
4.1.8 All Vague
</subsectionHeader>
<bodyText confidence="0.999881333333333">
The majority class baseline for this task is to label
all edges as VAGUE. This sieve is added to the end of
the gauntlet, labeling any remaining unlabeled edges.
</bodyText>
<subsectionHeader confidence="0.994596">
4.2 Machine Learned Sieves
</subsectionHeader>
<bodyText confidence="0.999966666666667">
Current state-of-the-art models for temporal ordering
are machine learned classifiers. The top systems in
the latest TempEval-3 contest used supervised clas-
sifiers for the different types of edges in the event
graph (Bethard, 2013; Chambers, 2013). In the spirit
of the sieve architecture, rather than training one
large classifier for all types of edges, we create tar-
geted classifiers for each type of edge. The resulting
models are again ranked by precision and mutually
</bodyText>
<sectionHeader confidence="0.696387" genericHeader="method">
Event-Time Features
</sectionHeader>
<bodyText confidence="0.8112872">
Token, lemma, POS tag of event
Tense, aspect, class of event
Bigram of event and time expression words
Token path from event to time
Syntactic parse tree path between the event and time
Typed dependency edge path between the events
Boolean: syntactically dominates or is-dominated
Boolean: time expression concludes sentence?
Boolean: time expression day of week?
Full time expression phrase
</bodyText>
<figureCaption confidence="0.990303">
Figure 3: Features for event-time edges.
</figureCaption>
<bodyText confidence="0.999025">
inform each other through the architecture’s transi-
tive constraints. All models use the MaxEnt classifier
from the Stanford CoreNLP with its default settings.
Below we describe the features for each sieve.
</bodyText>
<subsubsectionHeader confidence="0.51353">
4.2.1 Event-Time Edges
</subsubsectionHeader>
<bodyText confidence="0.999895117647059">
We created two different sieves for labeling edges
between an event and a time expression. Similar to
the event-event distinction, one classifier applies to
intra-sentence edges and the other to inter-sentence.
The features for these classifiers are shown in Fig-
ure 3. The same features from the above event-
event classifier are used where applicable. These
include the parse paths, syntactic dominance, and
event features with POS tags and token context. Fea-
tures specific to event-time edges include the event
word’s tense, aspect, and class (included indepen-
dently from the time expression). Time features in-
cluded a boolean feature if it was a day of the week,
the head word of the expression, and the entire expres-
sion if it is more than one word. A boolean feature
indicating if the time expression ended the sentence
is also included.
</bodyText>
<subsubsectionHeader confidence="0.46077">
4.2.2 Event-Event Edges
</subsubsectionHeader>
<bodyText confidence="0.999924333333333">
We created three different sieves for event-event
edges. They use the same features, but are trained
and applied to different subsets of the graph edges.
</bodyText>
<listItem confidence="0.999446142857143">
1. intra-sentence: Label edges between all event pairs
that occur in the same sentence.
2. intra-sentence dominance: Label edges between all
event pairs that occur in the same sentence and one
event dominates the other in the syntactic parse tree.
3. inter-sentence: Label edges between all event pairs
that occur in neighboring sentences.
</listItem>
<page confidence="0.998417">
279
</page>
<sectionHeader confidence="0.834784" genericHeader="method">
Event-Event Features
</sectionHeader>
<bodyText confidence="0.888020625">
Text order normalized (put the first event first)
Boolean: syntactically dominates or is-dominated
Boolean: is there an event in between these two?
Syntactic parse tree path between the events
Typed dependency edge path between the events
Token n-grams, POS tag n-grams
Pairs of tense, aspect, event class
Prepositions attached to each event, if any
</bodyText>
<figureCaption confidence="0.997098">
Figure 4: Features for intra-sentence event-event edges.
</figureCaption>
<bodyText confidence="0.999996045454545">
The features for these classifiers are summarized
in Figure 4. The POS tag features for each event
include its previous two tags, the event word’s tag,
and the POS bigram ending on the event word. A
POS bigram is also created from the POS tag of
each event. The token word, its lemma, its WordNet
synset, and the token bigram from each event word
are also used. Various syntactic features are used, in-
cluding preposition words that dominate either event,
boolean dominance features if one event is above the
other in the parse tree, the parse path between the
two events (append the non-terminals together), and
the typed dependency path between the two. Finally,
we use event attributes of tense, aspect, and class to
create event pairs of their values (e.g., if both events
are past tense, then the feature’s value is ‘past past’).
The final CAEVO architecture includes both intra-
sentence learners, but not inter-sentence. The latter’s
precision was not high enough to warrant its inclu-
sion. As mirrored in recent TempEval competition
results, inter-sentence event relations continue to be
an area of future research.
</bodyText>
<subsectionHeader confidence="0.747972">
4.2.3 Event-DCT Edges
</subsectionHeader>
<bodyText confidence="0.999968384615385">
Temporal relations between events and the docu-
ment creation time (DCT) are also separately clas-
sified. The features for this machine-learned sieve
focus solely on the event word, including its POS tag,
the two previous POS tags, event lemma, WordNet
synset, token unigrams from a window of size 2, and
event attributes: tense, aspect, and class. Since this
sieve relies entirely on the event’s features with no
DCT specific features, it can be viewed as a single-
event classifier. We also experimented with a number
of rule-based sieves for Event-DCT classification, but
they were all outperformed by the machine-learning
sieve on the training and development sets.
</bodyText>
<subsectionHeader confidence="0.999753">
4.3 The Complete Sieve Gauntlet
</subsectionHeader>
<bodyText confidence="0.99999525">
We now present the sieves used by CAEVO. Al-
though the final system contains 12 sieves, we ex-
perimented with over 25 different classifiers. The
final set of 12 are sorted by precision:
</bodyText>
<equation confidence="0.807802333333333">
#correct
precision(sieve) = (1)
#proposed by sieve
</equation>
<bodyText confidence="0.9983554">
Transitive closure is not included when calculating
precision. Sieves are ranked according to precision,
placed in order from highest to lowest. Sieves with
a precision lower than the All Vague baseline were
removed. Table 4 shows the final sieve order.
</bodyText>
<sectionHeader confidence="0.994498" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999984806451613">
We now present the first experiments on the
TimeBank-Dense corpus, using the set of events used
for TempEval-3. These experiments highlight two
separate contributions. The first is a new approach
to temporal ordering, the CAEVO architecture with
a cascade of classifiers. The second is an evaluation
of an ordering system on the densest event graphs
to date. We provide per-sieve results and compare
against two state-of-the-art systems.
All sieves were developed on and motivated by
the TimeBank-Dense training data. Precision was
initially computed on the development set, and final
sieve ordering is based on individual performance.
Final performance is reported on the test set.
We compare against the All Vague baseline that
labels all edges as VAGUE. We also compare against
the winner of TempEval-3, ClearTK (Bethard, 2013).
We include ClearTK in four configurations: (1) the
base TempEval-3 system, (2) with CAEVO transitiv-
ity, (3) with transitivity and altered to output a VAGUE
relation where it normally would have skipped the de-
cision, and (4) retrained models on TimeBank-Dense
with transitivity and all VAGUE. We also include the
second place TempEval-3 system, NavyTime (Cham-
bers, 2013), retrained and run as (3) and (4). Results
are shown in Table 5. The optimized TempEval-3
systems ultimately achieve ∼44 F1. The transitiv-
ity architecture automatically boosted ClearTK from
.147 to .264 with no additional changes. CAEVO
outperforms at 50.7 F1, improving over the retrained
TempEval-3 systems by 14% relative F1.
</bodyText>
<page confidence="0.992758">
280
</page>
<table confidence="0.987090866666667">
Ordered Sieves
Dev Test
Sieve Brief Description P R P R
Verb/Time Adjacent Rules for edges between one verbal event and one Timex .92 .01 .74 .01
TimeTime Rules for edges between two Timex .88 .02 .90 .02
Reporting Governor Rules for when a reporting event dominates an event .69 .01 .91 .01
Reichenbach Rules following Reichenbach theory for edges between two events .63 .02 .67 .03
General Governor Rules for edges where one event dominates another via typed dependencies .63 .04 .51 .02
WordNet Rules for edges between two events/times based on WordNet synonym lookup .52 .02 .57 .01
Reporting DCT Rules for reporting events and the DCT 1.0 .00 1.0 .00
ML-E-T SameSent Trained classifier for event-time edges .50 .03 .42 .03
ML-E-E SameSent Trained classifier for all intra-sentence event-event edges .45 .11 .44 .10
ML-E-E Dominate Trained classifier for edges where one event syntactically dominates another .41 .05 .44 .05
ML-E-DCT Trained classifier for all event-DCT edges .40 .06 .53 .07
AllVague Labels all edges as VAGUE .38 .38 .40 .40
</table>
<tableCaption confidence="0.999492">
Table 4: The final order of sieves in CAEVO. Precision was computed on the development set. ReportingDCT only
matched one pair in the dev set, so we moved it below the other more common rule-based sieves.
</tableCaption>
<table confidence="0.9614115">
System Comparison
System P R F1
ClearTK .397 .091 .147
ClearTK + trans .390 .199 .264
ClearTK VAGUE + trans .431 .431 .431
ClearTK Dense VAGUE + trans .460 .426 .442
NavyT Dense VAGUE .485 .415 .447
NavyT Dense VAGUE + trans .483 .427 .453
Baseline: All Vague .405 .405 .405
CAEVO .508 .506 .507
</table>
<tableCaption confidence="0.965051666666667">
Table 5: Comparison with top systems. The “Dense” sys-
tems were retrained on TimeBank-Dense. The “Opt” sys-
tems were optimized to label unknown edges as VAGUE.
</tableCaption>
<bodyText confidence="0.990194133333333">
The bottom of Table 6 illustrates the contribution
of transitive closure in the labeling decisions. Prior re-
search has hypothesized that transitive closure should
help individual decisions (Chambers and Jurafsky,
2008; Yoshikawa et al., 2009), but this has not been
adequately tested due to the sparsely labeled graphs
available to the community. We computed the pre-
cision of edges directly labeled by sieves, and the
precision of edges inferred from transitive closure.
Transitive inferences achieve better performance than
directly classified edges (54.5% to 49.8% precision).
Table 7 breaks apart precision by edge type. Since
CAEVO labels all edges, precision and recall are
the same. Edges between an event and the DCT
show 55% precision, while event-event and event-
</bodyText>
<sectionHeader confidence="0.554129" genericHeader="method">
CAEVO Component Ablation
</sectionHeader>
<table confidence="0.999641285714286">
P R F1
CAEVO with only ML Sieves .458 .202 .280
+ Rule-Based Sieves .486 .240 .321
+ Transitivity .505 .328 .398
+ All Vague Sieve .507 .507 .507
Direct Sieve Predictions .498 .398 .442
Predictions from Transitivity .544 .109 .182
</table>
<tableCaption confidence="0.9949595">
Table 6: System precision on the test set with and without
transitivity, and precision of transitive labels.
</tableCaption>
<bodyText confidence="0.99998275">
time edges achieve the lowest performance at 49%.
Linking time expressions to each other is helpful.
While far less numerous, time-time edges are the
easiest to classify (71% precision). Upon removing
the Time-Time sieve, overall results drop almost 5%,
despite the time-time edges only making up a smaller
2.6% of the data. Combined with transitive closure,
they positively influence the other event decisions.
Finally, Table 8 shows CAEVO’s per-relation re-
sults. We also reversed the order of sieves (but with
all vague still last) to see what effect it has. Perfor-
mance on the dev set dropped from .48 F1 to .46.
</bodyText>
<sectionHeader confidence="0.999837" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999989">
These are the first experiments on event ordering
where the document event graphs were annotated so
as to guarantee high density of relations and strong
</bodyText>
<page confidence="0.993773">
281
</page>
<table confidence="0.977239333333333">
Edge Type Performance
Edge Type Total P/R/F1
Event-Event Edges 1427 .494
Event-Time Edges 423 .494
Event-DCT 311 .553
Time-Time Edges 59 .712
</table>
<tableCaption confidence="0.972261333333333">
Table 7: The number of edges for each edge type, and the
system’s overall precision on each. Since CAEVO labels
all possible edges, precision and recall are the same.
</tableCaption>
<table confidence="0.996983375">
Per-Relation Performance
Relation P R F1
BEFORE .52 .45 .49
AFTER .55 .38 .45
INCLUDES .44 .21 .28
IS INCLUDED .57 .43 .49
SIMULTANEOUS .71 .31 .43
VAGUE .48 .66 .56
</table>
<tableCaption confidence="0.999827">
Table 8: Performance on individual relation types.
</tableCaption>
<bodyText confidence="0.999973666666667">
connectivity. Most related to our experiments is the
dense annotation work of Do et al. (2012), but as they
did not force annotators to label every pair, unanno-
tated pairs in their corpus are ambiguous between a
VAGUE relation and a relation that was simply missed
by the annotators. This ambiguity causes problems
for evaluation that are not present in a corpus where
annotators were required to annotate all pairs.
Constructing event ordering systems that will be
evaluated on dense, strongly connected document
event graphs is fundamentally different than optimiz-
ing a system to label a subset of edges. This can
be seen in the performance of the ClearTK-TimeML
system, which achieved the top performance (36.26)
on the sparse relation task of TempEval 2013, but
performed dramatically worse (15.8) on our dense
relation task. This is not to fault ClearTK, but to
illustrate the difference of evaluating only a subset of
an event graph’s edges rather than the entire graph.
We also presented the first sieve-based architec-
ture for event/time ordering. The CAEVO architec-
ture allowed us to focus on specific classifiers for
local decisions, and leave constraint enforcement
to the architecture itself. There are several intan-
gible benefits to this setup that don’t appear in the
results: (1) seamless integration of new/existing clas-
sifiers, (2) extremely quick ablation experiments by
adding/removing sieves, and (3) facilitation of the
development of specific classifiers that are agnostic
to global constraints.
The final results are encouraging. Our multi-pass
system achieved an F1 of 50.7, a full 10 F1 points
over the All-Vague baseline, and a 14% relative in-
crease over the top two systems in TempEval 3. This
increase is based not just on the top systems out of
the box, but after retraining them with TimeBank-
Dense. The largest factor contributing to this gain
is the enforcement of global constraints. Transitive
inference alone boosted performance by about 10%.
Finally, we note that a lot of effort was put into
linguistically motivated rule-based classifiers. The
overall gain from these high precision sieves was 2
F1 points. The Reichenbach sieve’s application of
a more traditional linguistic theory of time helped
increase performance. Error analysis reveals there
are inherent difficulties in relying solely on tense
and aspect for ordering decisions, but we hope to
continue experiments into this and other theories.
CAEVO and its code is now publicly available6.
In addition to temporal classification (the focus of
this paper), it also automatically extracts events and
time expressions. CAEVO is a complete system that
produces dense event/time graphs from raw text. We
hope it provides a new platform for quick experimen-
tation and development. The TimeBank-Dense cor-
pus is also publicly available, as well as the new an-
notation tool that forces annotators to label all edges.
It is a unique tool that helps with transitive inference
as annotators work. We hope it encourages further
dense data creation for temporal research.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99994725">
This work was supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors. It was also supported by Grant
Number R01LM010090 from the National Library
Of Medicine. Finally, thanks to Benjamin Van Durme
for assistance and insight.
</bodyText>
<footnote confidence="0.973659">
6http://www.usna.edu/Users/cs/nchamber/caevo
</footnote>
<page confidence="0.992723">
282
</page>
<sectionHeader confidence="0.989898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852666666667">
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832–843.
Steven Bethard and James H. Martin. 2007. CU-TMP:
Temporal relation classification using syntactic and se-
mantic features. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 129–132, Prague, Czech Republic, June.
Association for Computational Linguistics.
Steven Bethard. 2013. ClearTK-TimeML: A minimalist
approach to tempeval 2013. In Second Joint Confer-
ence on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 10–14, Atlanta, Georgia, USA, June. Association
for Computational Linguistics.
Branimir Boguraev and Rie Kubota Ando. 2005.
TimeBank-driven TimeML analysis. In Annotating,
Extracting and Reasoning about Time and Events.
Springer.
Luigi Borghesi and Chiara Favareto. 1982. Flexible pars-
ing of discretely uttered sentences. In Proceedings
of the 9th conference on Computational linguistics-
Volume 1, pages 37–42. Academia Praha.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee, and
Regina Barzilay. 2006. Inducing temporal graphs.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 189–
198. Association for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estimation.
Computational linguistics, 19(2):263–311.
Taylor Cassidy, Bill McDowell, Nathanael Chambers, and
Steven Bethard. 2014. An annotation framework for
dense event ordering. In The 52nd Annual Meeting of
the Association for Computational Linguistics, Balti-
more, MD, USA, June. Association for Computational
Linguistics.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 698–
706. Association for Computational Linguistics.
Nathanael Chambers. 2013. Navytime: Event and time
ordering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), Atlanta,
Georgia, USA, June. Association for Computational
Linguistics.
Yuchang Cheng, Masayuki Asahara, and Yuji Matsumoto.
2007. NAIST.Japan: Temporal relation identification
using dependency parsed tree. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 245–248, Prague, Czech
Republic, June. Association for Computational Linguis-
tics.
A Corazza, R De Mori, R Gretter, and G Satta. 1991.
Stochastic context-free grammars for island-driven
probabilistic parsing. In Proceedings of Second Inter-
national Workshop on Parsing Technologies (IWPT91),
pages 210–217.
Leon Derczynski and Robert Gaizauskas. 2013. Empir-
ical validation of reichenbach’s tense framework. In
Proceedings of the 10th International Conference on
Computational Semantics (IWCS 2013), pages 71–82.
Quang Do, Wei Lu, and Dan Roth. 2012. Joint inference
for event timeline construction. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 677–687, Jeju Island, Korea,
July. Association for Computational Linguistics.
Jennifer DSouza and Vincent Ng. 2013. Classifying
temporal relations with rich linguistic knowledge. In
Proceedings of NAACL-HLT, pages 918–927.
Eduard Hovy, Teruko Mitamura, Felisa Verdejo, Jun Araki,
and Andrew Philpot. 2013. Events are not simple:
Identity, non-identity, and quasi-identity. NAACL HLT
2013.
Oleksandr Kolomiyets, Steven Bethard, and Marie-
Francine Moens. 2012. Extracting narrative timelines
as temporal dependency structures. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
88–97, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Linguis-
tics.
Hector Llorens, Estela Saquete, and Borja Navarro. 2010.
TIPSem (English and Spanish): Evaluating CRFs and
semantic roles in TempEval-2. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 284–291, Uppsala, Sweden, July. Association for
Computational Linguistics.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro, et al.
2003. The timebank corpus. In Corpus linguistics,
volume 2003, page 40.
Hans Reichenbach. 1947. Elements of Symbolic Logic.
Macmillan.
</reference>
<page confidence="0.981464">
283
</page>
<reference confidence="0.999571799999999">
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In Proceedings of the 22nd International Con-
ference on Computational Linguistics-Volume 1, pages
857–864. Association for Computational Linguistics.
Naushad UzZaman and James Allen. 2010. TRIPS and
TRIOS system for TempEval-2: Extracting temporal
information from text. In Proceedings of the 5th In-
ternational Workshop on Semantic Evaluation, pages
276–283, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013a. Semeval-2013 task 1: Tempeval-3: Evaluat-
ing time expressions, events, and temporal relations. In
Second Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1–9, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
Marc Verhagen, James Allen, and James Pustejovsky.
2013b. Semeval-2013 task 1: Tempeval-3: Evaluat-
ing time expressions, events, and temporal relations.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark
Hepple, Graham Katz, and James Pustejovsky. 2007.
Semeval-2007 task 15: Tempeval temporal relation
identification. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 75–80. As-
sociation for Computational Linguistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and James
Pustejovsky. 2010. Semeval-2010 task 13: Tempeval-
2. In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 57–62. Association for
Computational Linguistics.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
405–413. Association for Computational Linguistics.
</reference>
<page confidence="0.998379">
284
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842936">
<title confidence="0.999263">Dense Event Ordering with a Multi-Pass Architecture</title>
<author confidence="0.989079">Nathanael Chambers</author>
<affiliation confidence="0.951433">United States Naval Academy</affiliation>
<email confidence="0.994993">nchamber@usna.edu</email>
<author confidence="0.999958">Bill McDowell</author>
<affiliation confidence="0.999937">Carnegie Mellon University</affiliation>
<email confidence="0.999541">forkunited@gmail.com</email>
<abstract confidence="0.99551552">The past 10 years of event ordering research has focused on learning partial orderings over document events and time expressions. The most popular corpus, the TimeBank, contains a small subset of the possible ordering graph. Many evaluations follow suit by only testing certain pairs of events (e.g., only main verbs of neighboring sentences). This has led most research to focus on specific learners for partial labelings. This paper attempts to nudge discussion from identifying We present new experiments on strongly connected event graphs that contimes more relations per document than the TimeBank. We also describe a shift away from the single learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14% gain over state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Communications of the ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="13048" citStr="Allen, 1983" startWordPosition="2051" endWordPosition="2052">path from every event to every other event. 3Though many are inferred through transitive closure. The other customers fled, and the police said it did not appear that anyone else was injured. There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. 275 TimeBank-Dense contains 12,715 labeled relations over 36 TimeBank newspaper articles. It uses TimeBank annotated events and time expressions, and then annotates the edges between them. The set of temporal relations are a subset of the 13 original Allen relations (Allen, 1983). The TempEval contests have used both a smaller set of relations (TempEval 1) and all 13 relations (TempEval 3). Published work mirrors this trend, and different groups focus on different aspects of the semantics. The TimeBankDense set is a middle ground between coarse and finegrained distinctions: BEFORE, AFTER, INCLUDES, IS INCLUDED, SIMULTANEOUS, and VAGUE. The main reason for not using a more fine-grained set is because we annotate pairs that are more ambiguous than those considered in previous efforts. Decisions between relations like BEFORE and IMMEDIATELY BEFORE complicate an already d</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>James F Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>CU-TMP: Temporal relation classification using syntactic and semantic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>129--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8854" citStr="Bethard and Martin, 2007" startWordPosition="1372" endWordPosition="1375"> part because of the sparsity of temporal relations in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; </context>
</contexts>
<marker>Bethard, Martin, 2007</marker>
<rawString>Steven Bethard and James H. Martin. 2007. CU-TMP: Temporal relation classification using syntactic and semantic features. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007), pages 129–132, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
</authors>
<title>ClearTK-TimeML: A minimalist approach to tempeval 2013.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>10--14</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3187" citStr="Bethard, 2013" startWordPosition="485" endWordPosition="486"> evaluate both identification and classification. We are not the first to approach relation identification with classification, but this paper is the first to directly and comprehensively address it. Most recently, TempEval 3 (UzZaman et al., 2013a) proposed a labeling of raw text without prior relation identification, but the challenge ultimately relied on the TimeBank. Systems were only evaluated on its subset of labeled event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away from the idea of monolithic learners, and propose smaller specialized classifiers. Inspiration comes from the recent success in named entity coreference with sieve-based learning (Lee et al., 2013). CAEVO contains a host of classi273 Transactions of the Association for Computational Linguistics, 2 (2014) 273–284. Action Editor: Ellen Riloff. Submitted 11/2013; Revis</context>
<context position="8937" citStr="Bethard, 2013" startWordPosition="1388" endWordPosition="1389">xisting models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZam</context>
<context position="27612" citStr="Bethard, 2013" startWordPosition="4481" endWordPosition="4482">ent edges whose events share the same lemma, or are each a member of the same synset are thus labeled VAGUE. Time-time edges with the same lemma/synset are labeled SIMULTANEOUS. 4.1.8 All Vague The majority class baseline for this task is to label all edges as VAGUE. This sieve is added to the end of the gauntlet, labeling any remaining unlabeled edges. 4.2 Machine Learned Sieves Current state-of-the-art models for temporal ordering are machine learned classifiers. The top systems in the latest TempEval-3 contest used supervised classifiers for the different types of edges in the event graph (Bethard, 2013; Chambers, 2013). In the spirit of the sieve architecture, rather than training one large classifier for all types of edges, we create targeted classifiers for each type of edge. The resulting models are again ranked by precision and mutually Event-Time Features Token, lemma, POS tag of event Tense, aspect, class of event Bigram of event and time expression words Token path from event to time Syntactic parse tree path between the event and time Typed dependency edge path between the events Boolean: syntactically dominates or is-dominated Boolean: time expression concludes sentence? Boolean: t</context>
<context position="33513" citStr="Bethard, 2013" startWordPosition="5419" endWordPosition="5420">, the CAEVO architecture with a cascade of classifiers. The second is an evaluation of an ordering system on the densest event graphs to date. We provide per-sieve results and compare against two state-of-the-art systems. All sieves were developed on and motivated by the TimeBank-Dense training data. Precision was initially computed on the development set, and final sieve ordering is based on individual performance. Final performance is reported on the test set. We compare against the All Vague baseline that labels all edges as VAGUE. We also compare against the winner of TempEval-3, ClearTK (Bethard, 2013). We include ClearTK in four configurations: (1) the base TempEval-3 system, (2) with CAEVO transitivity, (3) with transitivity and altered to output a VAGUE relation where it normally would have skipped the decision, and (4) retrained models on TimeBank-Dense with transitivity and all VAGUE. We also include the second place TempEval-3 system, NavyTime (Chambers, 2013), retrained and run as (3) and (4). Results are shown in Table 5. The optimized TempEval-3 systems ultimately achieve ∼44 F1. The transitivity architecture automatically boosted ClearTK from .147 to .264 with no additional change</context>
</contexts>
<marker>Bethard, 2013</marker>
<rawString>Steven Bethard. 2013. ClearTK-TimeML: A minimalist approach to tempeval 2013. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 10–14, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir Boguraev</author>
<author>Rie Kubota Ando</author>
</authors>
<title>TimeBank-driven TimeML analysis.</title>
<date>2005</date>
<booktitle>In Annotating, Extracting and Reasoning about Time and Events.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="8577" citStr="Boguraev and Ando, 2005" startWordPosition="1331" endWordPosition="1334">pairs for which no clear temporal relation exists. We are thus the first to eliminate the 3rd possibility from our corpus. 1Do 2012 reports 6264 relations, but we list half of this because they include relations and their inverses in the count. 274 2.2 Event Ordering Models In part because of the sparsity of temporal relations in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and wit</context>
</contexts>
<marker>Boguraev, Ando, 2005</marker>
<rawString>Branimir Boguraev and Rie Kubota Ando. 2005. TimeBank-driven TimeML analysis. In Annotating, Extracting and Reasoning about Time and Events. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luigi Borghesi</author>
<author>Chiara Favareto</author>
</authors>
<title>Flexible parsing of discretely uttered sentences.</title>
<date>1982</date>
<booktitle>In Proceedings of the 9th conference on Computational linguisticsVolume 1,</booktitle>
<pages>37--42</pages>
<location>Academia Praha.</location>
<contexts>
<context position="10198" citStr="Borghesi and Favareto, 1982" startWordPosition="1582" endWordPosition="1585">e been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliability” approaches to parsing and speech (Borghesi and Favareto, 1982; Corazza et al., 1991). D’Souza and Ng (2013) recently combined a rule-based model with a machine learned model, but lacked the fine-grained formality of a cascade of sieves. This paper is inspired by the above and is the first to apply it to temporal ordering as an extensible, formal architecture. The TimeBank TimeBank-Dense There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. The other customers fled, and the police said it did not appear that anyone else was injured. Figure 1: TimeBank document on the le</context>
</contexts>
<marker>Borghesi, Favareto, 1982</marker>
<rawString>Luigi Borghesi and Chiara Favareto. 1982. Flexible parsing of discretely uttered sentences. In Proceedings of the 9th conference on Computational linguisticsVolume 1, pages 37–42. Academia Praha.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Bramsen</author>
<author>Pawan Deshpande</author>
<author>Yoong Keok Lee</author>
<author>Regina Barzilay</author>
</authors>
<title>Inducing temporal graphs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>189--198</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6558" citStr="Bramsen et al. (2006)" startWordPosition="1004" endWordPosition="1007">s were excluded and only one TempEval task considered relations between events in different sentences. Events Times Relations R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 Do 20121 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relations to events + times (R) in various corpora. To avoid such sparse annotations, researchers have explored schemes that encourage annotators to connect all events. Bramsen et al. (2006) annotated timelines as directed acyclic graphs, though they annotated multi-sentence segments of text rather than individual events. Kolomiyets et al. (2012) annotated “temporal dependency structures” (i.e. dependency trees of temporal relations), though they only focused on pairs of events. In Do et al. (2012), “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, and then more relations were automatically inferred after the annotation was complete. In contrast, in our work we required annotators to label every possible pair of events/times in a g</context>
<context position="9452" citStr="Bramsen et al., 2006" startWordPosition="1459" endWordPosition="1462">thard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared</context>
</contexts>
<marker>Bramsen, Deshpande, Lee, Barzilay, 2006</marker>
<rawString>Philip Bramsen, Pawan Deshpande, Yoong Keok Lee, and Regina Barzilay. 2006. Inducing temporal graphs. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 189– 198. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="10100" citStr="Brown et al., 1993" startWordPosition="1567" endWordPosition="1570">; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliability” approaches to parsing and speech (Borghesi and Favareto, 1982; Corazza et al., 1991). D’Souza and Ng (2013) recently combined a rule-based model with a machine learned model, but lacked the fine-grained formality of a cascade of sieves. This paper is inspired by the above and is the first to apply it to temporal ordering as an extensible, formal architecture. The TimeBank TimeBank-Dense There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. The other customers fled, and the</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Cassidy</author>
<author>Bill McDowell</author>
<author>Nathanael Chambers</author>
<author>Steven Bethard</author>
</authors>
<title>An annotation framework for dense event ordering.</title>
<date>2014</date>
<booktitle>In The 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="11021" citStr="Cassidy et al., 2014" startWordPosition="1720" endWordPosition="1724">the above and is the first to apply it to temporal ordering as an extensible, formal architecture. The TimeBank TimeBank-Dense There were four or five people inside, and they just started firing Ms. Sanders was hit several times and was pronounced dead at the scene. The other customers fled, and the police said it did not appear that anyone else was injured. Figure 1: TimeBank document on the left with TimeBankDense on the right. Solid arrows indicate BEFORE and dotted INCLUDED IN. Relations with the DCT not shown. 3 TimeBank-Dense: A Dense Ordering We use a new corpus, called TimeBank-Dense (Cassidy et al., 2014), to motivate and evaluate our architecture. This section highlights its main features. The TimeBank-Dense corpus was created to address the sparsity in current corpora. It is unique in that the annotators were forced to label all local edges, even in ambiguous cases. The corpus is not a complete graph over events and time expressions, but it approximates completeness by labeling locally complete graphs over neighboring sentences. All pairs of events and time expressions in the same sentence and all pairs of events and time expressions in the immediately following sentence were labeled. It als</context>
</contexts>
<marker>Cassidy, McDowell, Chambers, Bethard, 2014</marker>
<rawString>Taylor Cassidy, Bill McDowell, Nathanael Chambers, and Steven Bethard. 2014. An annotation framework for dense event ordering. In The 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Jointly combining implicit constraints improves temporal ordering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>698--706</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9481" citStr="Chambers and Jurafsky, 2008" startWordPosition="1463" endWordPosition="1466">; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (</context>
<context position="36142" citStr="Chambers and Jurafsky, 2008" startWordPosition="5849" endWordPosition="5852"> .091 .147 ClearTK + trans .390 .199 .264 ClearTK VAGUE + trans .431 .431 .431 ClearTK Dense VAGUE + trans .460 .426 .442 NavyT Dense VAGUE .485 .415 .447 NavyT Dense VAGUE + trans .483 .427 .453 Baseline: All Vague .405 .405 .405 CAEVO .508 .506 .507 Table 5: Comparison with top systems. The “Dense” systems were retrained on TimeBank-Dense. The “Opt” systems were optimized to label unknown edges as VAGUE. The bottom of Table 6 illustrates the contribution of transitive closure in the labeling decisions. Prior research has hypothesized that transitive closure should help individual decisions (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009), but this has not been adequately tested due to the sparsely labeled graphs available to the community. We computed the precision of edges directly labeled by sieves, and the precision of edges inferred from transitive closure. Transitive inferences achieve better performance than directly classified edges (54.5% to 49.8% precision). Table 7 breaks apart precision by edge type. Since CAEVO labels all edges, precision and recall are the same. Edges between an event and the DCT show 55% precision, while event-event and eventCAEVO Component Ablation P R F1 CAEVO with onl</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Jointly combining implicit constraints improves temporal ordering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 698– 706. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
</authors>
<title>Navytime: Event and time ordering from raw text.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="27629" citStr="Chambers, 2013" startWordPosition="4483" endWordPosition="4484"> events share the same lemma, or are each a member of the same synset are thus labeled VAGUE. Time-time edges with the same lemma/synset are labeled SIMULTANEOUS. 4.1.8 All Vague The majority class baseline for this task is to label all edges as VAGUE. This sieve is added to the end of the gauntlet, labeling any remaining unlabeled edges. 4.2 Machine Learned Sieves Current state-of-the-art models for temporal ordering are machine learned classifiers. The top systems in the latest TempEval-3 contest used supervised classifiers for the different types of edges in the event graph (Bethard, 2013; Chambers, 2013). In the spirit of the sieve architecture, rather than training one large classifier for all types of edges, we create targeted classifiers for each type of edge. The resulting models are again ranked by precision and mutually Event-Time Features Token, lemma, POS tag of event Tense, aspect, class of event Bigram of event and time expression words Token path from event to time Syntactic parse tree path between the event and time Typed dependency edge path between the events Boolean: syntactically dominates or is-dominated Boolean: time expression concludes sentence? Boolean: time expression da</context>
<context position="33884" citStr="Chambers, 2013" startWordPosition="5476" endWordPosition="5478">ieve ordering is based on individual performance. Final performance is reported on the test set. We compare against the All Vague baseline that labels all edges as VAGUE. We also compare against the winner of TempEval-3, ClearTK (Bethard, 2013). We include ClearTK in four configurations: (1) the base TempEval-3 system, (2) with CAEVO transitivity, (3) with transitivity and altered to output a VAGUE relation where it normally would have skipped the decision, and (4) retrained models on TimeBank-Dense with transitivity and all VAGUE. We also include the second place TempEval-3 system, NavyTime (Chambers, 2013), retrained and run as (3) and (4). Results are shown in Table 5. The optimized TempEval-3 systems ultimately achieve ∼44 F1. The transitivity architecture automatically boosted ClearTK from .147 to .264 with no additional changes. CAEVO outperforms at 50.7 F1, improving over the retrained TempEval-3 systems by 14% relative F1. 280 Ordered Sieves Dev Test Sieve Brief Description P R P R Verb/Time Adjacent Rules for edges between one verbal event and one Timex .92 .01 .74 .01 TimeTime Rules for edges between two Timex .88 .02 .90 .02 Reporting Governor Rules for when a reporting event dominates</context>
</contexts>
<marker>Chambers, 2013</marker>
<rawString>Nathanael Chambers. 2013. Navytime: Event and time ordering from raw text. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuchang Cheng</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>NAIST.Japan: Temporal relation identification using dependency parsed tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>245--248</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8874" citStr="Cheng et al., 2007" startWordPosition="1376" endWordPosition="1379">ity of temporal relations in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsk</context>
</contexts>
<marker>Cheng, Asahara, Matsumoto, 2007</marker>
<rawString>Yuchang Cheng, Masayuki Asahara, and Yuji Matsumoto. 2007. NAIST.Japan: Temporal relation identification using dependency parsed tree. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 245–248, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corazza</author>
<author>R De Mori</author>
<author>R Gretter</author>
<author>G Satta</author>
</authors>
<title>Stochastic context-free grammars for island-driven probabilistic parsing.</title>
<date>1991</date>
<booktitle>In Proceedings of Second International Workshop on Parsing Technologies (IWPT91),</booktitle>
<pages>210--217</pages>
<marker>Corazza, De Mori, Gretter, Satta, 1991</marker>
<rawString>A Corazza, R De Mori, R Gretter, and G Satta. 1991. Stochastic context-free grammars for island-driven probabilistic parsing. In Proceedings of Second International Workshop on Parsing Technologies (IWPT91), pages 210–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Empirical validation of reichenbach’s tense framework.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS</booktitle>
<pages>71--82</pages>
<contexts>
<context position="24978" citStr="Derczynski and Gaizauskas (2013)" startWordPosition="4045" endWordPosition="4048">red to by the verb occurs. • Point of reference (R) A single time with respect to which S is ordered by one dimension of tense, and to which E is ordered by another. Reichenbach’s account maps the set of possible orderings of S, E, and R, where each pair of elements can be ordered with &lt; (before), = (simultaneous), or &gt; (after), onto a set of tense names given by: {anterior, simple, posterior} x {past, present, future}. The relative ordering of E and R is indicated by the first dimension, while that of S and R is given by the second. Figure 2 depicts examples of the ordering R &lt; S. Similar to Derczynski and Gaizauskas (2013) we map a subset of Reichenbach’s tenses onto pairs of TimeML tense and aspect attribute values, which are given by {simple, perfect, progressive} x {past, present, future}, where the first dimension is grammatical aspect and the second is tense. We refer to an event’s tense/aspect combination as its tense-aspect profile. Consider two events e1 and e2 associated with S1/E1/R1 and S2/E2/R2. Intuitively, given R1 = R2 and the time point orderings for each event (derived from its tense-aspect profile) we can enumerate the possible interval relations that might hold between E1 and E2, which we can</context>
</contexts>
<marker>Derczynski, Gaizauskas, 2013</marker>
<rawString>Leon Derczynski and Robert Gaizauskas. 2013. Empirical validation of reichenbach’s tense framework. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013), pages 71–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Do</author>
<author>Wei Lu</author>
<author>Dan Roth</author>
</authors>
<title>Joint inference for event timeline construction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>677--687</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6871" citStr="Do et al. (2012)" startWordPosition="1049" endWordPosition="1052"> 20121 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relations to events + times (R) in various corpora. To avoid such sparse annotations, researchers have explored schemes that encourage annotators to connect all events. Bramsen et al. (2006) annotated timelines as directed acyclic graphs, though they annotated multi-sentence segments of text rather than individual events. Kolomiyets et al. (2012) annotated “temporal dependency structures” (i.e. dependency trees of temporal relations), though they only focused on pairs of events. In Do et al. (2012), “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, and then more relations were automatically inferred after the annotation was complete. In contrast, in our work we required annotators to label every possible pair of events/times in a given window, and our event graphs are guaranteed to be strongly connected, with every edge verified by the annotators. Table 1 compares the density of relation annotation across various corpora. A major dilemma from this prior work is that unlabeled event/time pairs are inherently ambiguous. The unlabeled pair h</context>
<context position="38467" citStr="Do et al. (2012)" startWordPosition="6232" endWordPosition="6235"> Type Performance Edge Type Total P/R/F1 Event-Event Edges 1427 .494 Event-Time Edges 423 .494 Event-DCT 311 .553 Time-Time Edges 59 .712 Table 7: The number of edges for each edge type, and the system’s overall precision on each. Since CAEVO labels all possible edges, precision and recall are the same. Per-Relation Performance Relation P R F1 BEFORE .52 .45 .49 AFTER .55 .38 .45 INCLUDES .44 .21 .28 IS INCLUDED .57 .43 .49 SIMULTANEOUS .71 .31 .43 VAGUE .48 .66 .56 Table 8: Performance on individual relation types. connectivity. Most related to our experiments is the dense annotation work of Do et al. (2012), but as they did not force annotators to label every pair, unannotated pairs in their corpus are ambiguous between a VAGUE relation and a relation that was simply missed by the annotators. This ambiguity causes problems for evaluation that are not present in a corpus where annotators were required to annotate all pairs. Constructing event ordering systems that will be evaluated on dense, strongly connected document event graphs is fundamentally different than optimizing a system to label a subset of edges. This can be seen in the performance of the ClearTK-TimeML system, which achieved the to</context>
</contexts>
<marker>Do, Lu, Roth, 2012</marker>
<rawString>Quang Do, Wei Lu, and Dan Roth. 2012. Joint inference for event timeline construction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 677–687, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer DSouza</author>
<author>Vincent Ng</author>
</authors>
<title>Classifying temporal relations with rich linguistic knowledge.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>918--927</pages>
<marker>DSouza, Ng, 2013</marker>
<rawString>Jennifer DSouza and Vincent Ng. 2013. Classifying temporal relations with rich linguistic knowledge. In Proceedings of NAACL-HLT, pages 918–927.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Teruko Mitamura</author>
<author>Felisa Verdejo</author>
<author>Jun Araki</author>
<author>Andrew Philpot</author>
</authors>
<title>Events are not simple: Identity, non-identity, and quasi-identity.</title>
<date>2013</date>
<journal>NAACL HLT</journal>
<contexts>
<context position="26726" citStr="Hovy et al., 2013" startWordPosition="4339" endWordPosition="4342">ch sieve achieved a precision of 61% on the entire dataset, and 91% when links labeled VAGUE were excluded, a notably larger increase over other sieves. The lower 61% is primarily due to the abundance of non-canonical usages of various tense/aspect profiles. For example, the present tense may convey habitual action, an event conveyed with the future tense in a quotation may have already occurred, the present perfect may be used to indicate that an event occurred at least once in the past, etc. 4.1.7 WordNet Rules Determining whether two words are precisely coreferent is notoriously difficult (Hovy et al., 2013). Synonymous words, or even those sharing the same surface form may be only partially co-referent and thus likely labeled VAGUE by annotators. For example, news articles state what someone said multiple times without revealing temporal ordering between the events. Event-event edges whose events share the same lemma, or are each a member of the same synset are thus labeled VAGUE. Time-time edges with the same lemma/synset are labeled SIMULTANEOUS. 4.1.8 All Vague The majority class baseline for this task is to label all edges as VAGUE. This sieve is added to the end of the gauntlet, labeling an</context>
</contexts>
<marker>Hovy, Mitamura, Verdejo, Araki, Philpot, 2013</marker>
<rawString>Eduard Hovy, Teruko Mitamura, Felisa Verdejo, Jun Araki, and Andrew Philpot. 2013. Events are not simple: Identity, non-identity, and quasi-identity. NAACL HLT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Kolomiyets</author>
<author>Steven Bethard</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Extracting narrative timelines as temporal dependency structures.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>88--97</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6716" citStr="Kolomiyets et al. (2012)" startWordPosition="1026" endWordPosition="1029"> Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 Do 20121 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relations to events + times (R) in various corpora. To avoid such sparse annotations, researchers have explored schemes that encourage annotators to connect all events. Bramsen et al. (2006) annotated timelines as directed acyclic graphs, though they annotated multi-sentence segments of text rather than individual events. Kolomiyets et al. (2012) annotated “temporal dependency structures” (i.e. dependency trees of temporal relations), though they only focused on pairs of events. In Do et al. (2012), “the annotator was not required to annotate all pairs of event mentions, but as many as possible”, and then more relations were automatically inferred after the annotation was complete. In contrast, in our work we required annotators to label every possible pair of events/times in a given window, and our event graphs are guaranteed to be strongly connected, with every edge verified by the annotators. Table 1 compares the density of relatio</context>
</contexts>
<marker>Kolomiyets, Bethard, Moens, 2012</marker>
<rawString>Oleksandr Kolomiyets, Steven Bethard, and MarieFrancine Moens. 2012. Extracting narrative timelines as temporal dependency structures. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 88–97, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<date>2013</date>
<contexts>
<context position="3616" citStr="Lee et al., 2013" startWordPosition="545" endWordPosition="548"> event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away from the idea of monolithic learners, and propose smaller specialized classifiers. Inspiration comes from the recent success in named entity coreference with sieve-based learning (Lee et al., 2013). CAEVO contains a host of classi273 Transactions of the Association for Computational Linguistics, 2 (2014) 273–284. Action Editor: Ellen Riloff. Submitted 11/2013; Revised 5/2014; Published 10/2014. c�2014 Association for Computational Linguistics. fiers that each specialize on different types of edges. The classifiers are ordered by their individual precision, and run in order starting with the most precise. Each sieve informs those below it by passing on its decisions as graph constraints. These more precise constraints are then used by later sieves to assist their less precise decisions. </context>
<context position="9933" citStr="Lee et al., 2013" startWordPosition="1535" endWordPosition="1538">emporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliability” approaches to parsing and speech (Borghesi and Favareto, 1982; Corazza et al., 1991). D’Souza and Ng (2013) recently combined a rule-based model with a machine learned model, but lacked the fine-grained formality of a cascade of sieves. This paper is inspired by the above and is the first to apply it to temporal ordering as an extensible, formal architecture. The TimeBank TimeBank-Dense There </context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.</rawString>
</citation>
<citation valid="false">
<title>Deterministic coreference resolution based on entitycentric, precision-ranked rules. Computational Linguistics.</title>
<marker></marker>
<rawString>Deterministic coreference resolution based on entitycentric, precision-ranked rules. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector Llorens</author>
<author>Estela Saquete</author>
<author>Borja Navarro</author>
</authors>
<title>TIPSem (English and Spanish): Evaluating CRFs and semantic roles in TempEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>284--291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8921" citStr="Llorens et al., 2010" startWordPosition="1384" endWordPosition="1387">aining corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et </context>
</contexts>
<marker>Llorens, Saquete, Navarro, 2010</marker>
<rawString>Hector Llorens, Estela Saquete, and Borja Navarro. 2010. TIPSem (English and Spanish): Evaluating CRFs and semantic roles in TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 284–291, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
</authors>
<title>The timebank corpus.</title>
<date>2003</date>
<booktitle>In Corpus linguistics,</booktitle>
<volume>volume</volume>
<pages>40</pages>
<contexts>
<context position="1506" citStr="Pustejovsky et al., 2003" startWordPosition="228" endWordPosition="231">le learner to a sieve-based architecture that naturally blends multiple learners into a precision-ranked cascade of sieves. Each sieve adds labels to the event graph one at a time, and earlier sieves inform later ones through transitive closure. This paper thus describes innovations in both approach and task. We experiment on the densest event graphs to date and show a 14% gain over state-of-the-art. 1 Introduction Event ordering in the NLP community usually refers to a specific labeling task: given pairs of events and time expressions, choose ordering relations for them. The TimeBank corpus (Pustejovsky et al., 2003) provided a corpus of labeled pairs that motivated machine learning approaches to this task. However, only a small subset of possible pairs are labeled. The annotators identified the most central and obvious relations, and left the rest unlabeled. Partly because of Taylor Cassidy Army Research Lab IBM Research taylorcassidy64@gmail.com Steven Bethard University of Alabama at Birmingham bethard@cis.uab.edu this, there has been little research into complete event graphs. This paper describes our recent exploration into the dramatic changes required of both learning algorithms and experimental se</context>
<context position="5578" citStr="Pustejovsky et al., 2003" startWordPosition="839" endWordPosition="842">ned classifiers using the sieve architecture. Each classifier focuses on its independent decisions, and the architecture then imposes transitivity constraints. CAEVO contains 12 sieves ordered by precision, and it outperforms the top systems in event ordering. 2 Previous Work Two lines of research are particularly relevant to our work: research on event ordering annotation, and research on event ordering models. We briefly review both here. 2.1 Event Ordering Annotation Most prior annotation efforts constructed corpora with only a small subset of temporal relations annotated. In the TimeBank (Pustejovsky et al., 2003), temporal relations were only annotated when the relation was judged to be salient by the annotator. The TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013b) aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. Events Times Relations R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir 2003. The timebank corpus. In Corpus linguistics, volume 2003, page 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Reichenbach</author>
</authors>
<date>1947</date>
<booktitle>Elements of Symbolic Logic.</booktitle>
<publisher>Macmillan.</publisher>
<contexts>
<context position="23754" citStr="Reichenbach (1947)" startWordPosition="3812" endWordPosition="3813">CLUDED dep) 4.1.5 General Event with Dominated Event We generalized the above reporting-event sieve to apply to all types of events. This ‘backoff’ sieve handles the non-reporting event pairs where one event immediately dominates another. It operates similar to the preceding sieve and uses event features like tense and aspect to make its decisions. However, unlike the preceding, the decisions are split into different rule sets based on the specific typed dependency relation that connects the two events. We have separate rules nsubj, dobj, xcomp, ccomp, advcl, and conj. 4.1.6 Reichenbach Rules Reichenbach (1947) defined the role played by the various tenses of English verbs in conveying temSimple Past: John left the room E,R S Anterior Past: John had left the room E R S Posterior Past: (I did not know that) John would leave the room R E,S Figure 2: Timelines in which the ordering S &lt; R is fixed because the verb to leave is in the past tense. poral information in a discourse. Each tense is distinguished in terms of the relative ordering of the following time points: • Point of speech (S) The time at which an act of speech is performed. • Point of event (E) The time at which the the event referred to b</context>
</contexts>
<marker>Reichenbach, 1947</marker>
<rawString>Hans Reichenbach. 1947. Elements of Symbolic Logic. Macmillan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Munirathnam Srikanth</author>
</authors>
<title>Experiments with reasoning for temporal relations between events.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>857--864</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9506" citStr="Tatu and Srikanth, 2008" startWordPosition="1467" endWordPosition="1470"> and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and i</context>
</contexts>
<marker>Tatu, Srikanth, 2008</marker>
<rawString>Marta Tatu and Munirathnam Srikanth. 2008. Experiments with reasoning for temporal relations between events. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 857–864. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>James Allen</author>
</authors>
<title>TRIPS and TRIOS system for TempEval-2: Extracting temporal information from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>276--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="8899" citStr="UzZaman and Allen, 2010" startWordPosition="1380" endWordPosition="1383">tions in the available training corpora, most existing models formulate temporal ordering as a pair-wise classification task, where each pair of events and/or times is examined and classified as having a temporal relation or not. Early work on the TimeBank took this approach (Boguraev and Ando, 2005), classifying relations between all events and times within 64 tokens of each other. Most of the top-performing systems in the TempEval competitions also took this pairwise classification approach for both event-time and event-event temporal relations (Bethard and Martin, 2007; Cheng et al., 2007; UzZaman and Allen, 2010; Llorens et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikant</context>
</contexts>
<marker>UzZaman, Allen, 2010</marker>
<rawString>Naushad UzZaman and James Allen. 2010. TRIPS and TRIOS system for TempEval-2: Extracting temporal information from text. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 276–283, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>Leon Derczynski</author>
<author>James Allen</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="2820" citStr="UzZaman et al., 2013" startWordPosition="429" endWordPosition="432">d of a complete graph labeling algorithm is that of relation identification. Research on the TimeBank and in the TempEval contests has largely focused on relation classification. The event pairs are given, and the task is to classify them. We are now forced to first determine which events should be paired up before classification. Our experiments here fully integrate and evaluate both identification and classification. We are not the first to approach relation identification with classification, but this paper is the first to directly and comprehensively address it. Most recently, TempEval 3 (UzZaman et al., 2013a) proposed a labeling of raw text without prior relation identification, but the challenge ultimately relied on the TimeBank. Systems were only evaluated on its subset of labeled event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away f</context>
<context position="5773" citStr="UzZaman et al., 2013" startWordPosition="872" endWordPosition="875">ision, and it outperforms the top systems in event ordering. 2 Previous Work Two lines of research are particularly relevant to our work: research on event ordering annotation, and research on event ordering models. We briefly review both here. 2.1 Event Ordering Annotation Most prior annotation efforts constructed corpora with only a small subset of temporal relations annotated. In the TimeBank (Pustejovsky et al., 2003), temporal relations were only annotated when the relation was judged to be salient by the annotator. The TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013b) aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. Events Times Relations R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 Do 20121 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relat</context>
</contexts>
<marker>UzZaman, Llorens, Derczynski, Allen, Verhagen, Pustejovsky, 2013</marker>
<rawString>Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013a. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 1–9, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>Leon Derczynski</author>
<author>Marc Verhagen</author>
<author>James Allen</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2820" citStr="UzZaman et al., 2013" startWordPosition="429" endWordPosition="432">d of a complete graph labeling algorithm is that of relation identification. Research on the TimeBank and in the TempEval contests has largely focused on relation classification. The event pairs are given, and the task is to classify them. We are now forced to first determine which events should be paired up before classification. Our experiments here fully integrate and evaluate both identification and classification. We are not the first to approach relation identification with classification, but this paper is the first to directly and comprehensively address it. Most recently, TempEval 3 (UzZaman et al., 2013a) proposed a labeling of raw text without prior relation identification, but the challenge ultimately relied on the TimeBank. Systems were only evaluated on its subset of labeled event pairs. This meant that relation identification was largely ignored. The top system optimized only relation classification and intentionally left many pairs unlabeled (Bethard, 2013). This paper presents CAEVO, a CAscading EVent Ordering architecture. It is a novel sieve-based architecture for temporal event ordering that directly addresses the interplay between identification and classification. We shift away f</context>
<context position="5773" citStr="UzZaman et al., 2013" startWordPosition="872" endWordPosition="875">ision, and it outperforms the top systems in event ordering. 2 Previous Work Two lines of research are particularly relevant to our work: research on event ordering annotation, and research on event ordering models. We briefly review both here. 2.1 Event Ordering Annotation Most prior annotation efforts constructed corpora with only a small subset of temporal relations annotated. In the TimeBank (Pustejovsky et al., 2003), temporal relations were only annotated when the relation was judged to be salient by the annotator. The TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013b) aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. Events Times Relations R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 Do 20121 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations and the ratio of relat</context>
</contexts>
<marker>UzZaman, Llorens, Derczynski, Verhagen, Allen, Pustejovsky, 2013</marker>
<rawString>Naushad UzZaman, Hector Llorens, Leon Derczynski, Marc Verhagen, James Allen, and James Pustejovsky. 2013b. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>75--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5728" citStr="Verhagen et al., 2007" startWordPosition="863" endWordPosition="866">ints. CAEVO contains 12 sieves ordered by precision, and it outperforms the top systems in event ordering. 2 Previous Work Two lines of research are particularly relevant to our work: research on event ordering annotation, and research on event ordering models. We briefly review both here. 2.1 Event Ordering Annotation Most prior annotation efforts constructed corpora with only a small subset of temporal relations annotated. In the TimeBank (Pustejovsky et al., 2003), temporal relations were only annotated when the relation was judged to be salient by the annotator. The TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013b) aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. Events Times Relations R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 Do 20121 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, tim</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75–80. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 13: Tempeval2. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>57--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5751" citStr="Verhagen et al., 2010" startWordPosition="867" endWordPosition="871"> sieves ordered by precision, and it outperforms the top systems in event ordering. 2 Previous Work Two lines of research are particularly relevant to our work: research on event ordering annotation, and research on event ordering models. We briefly review both here. 2.1 Event Ordering Annotation Most prior annotation efforts constructed corpora with only a small subset of temporal relations annotated. In the TimeBank (Pustejovsky et al., 2003), temporal relations were only annotated when the relation was judged to be salient by the annotator. The TempEval competitions (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013b) aimed to improve coverage by annotating relations between all events and times in the same sentence. However, event tokens that were mentioned fewer than 20 times were excluded and only one TempEval task considered relations between events in different sentences. Events Times Relations R TimeBank 7935 1414 6418 0.7 Bramsen 2006 627 – 615 1.0 TempEval 2007 6832 1249 5790 0.7 TempEval 2010 5688 2117 4907 0.6 TempEval 2013 11145 2078 11098 0.8 Kolomiyets 2012 1233 – 1139 0.9 Do 20121 324 232 3132 5.6 This work 1729 289 12715 6.3 Table 1: Events, times, temporal relations </context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. Semeval-2010 task 13: Tempeval2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 57–62. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Sebastian Riedel</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Jointly identifying temporal relations with Markov Logic.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>405--413</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9530" citStr="Yoshikawa et al., 2009" startWordPosition="1471" endWordPosition="1474"> et al., 2010; Bethard, 2013). These systems have sometimes even explicitly focused on a small subset of temporal relations; for example, the topranked ordering system in TempEval 2013 (Bethard, 2013) only classified relations in certain syntactic constructions and with certain relation types. Systems have tried to take advantage of global information to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks like integer linear programming and Markov logic networks (Bramsen et al., 2006; Chambers and Jurafsky, 2008; Tatu and Srikanth, 2008; Yoshikawa et al., 2009; UzZaman and Allen, 2010). The gains have been small, likely because of the disconnectedness that is common in sparsely annotated corpora (Chambers and Jurafsky, 2008). An approach that has not been leveraged for event ordering, but that has been successful in the coreference community is the sieve-based architecture. The top performer in CoNLL-2011 shared task was one such system (Lee et al., 2013). The core idea is to begin with the most reliable classifier first, and inform those below it. This idea also appeared in the early IBM MT models (Brown et al., 1993) and in the “islands of reliab</context>
<context position="36167" citStr="Yoshikawa et al., 2009" startWordPosition="5853" endWordPosition="5856">90 .199 .264 ClearTK VAGUE + trans .431 .431 .431 ClearTK Dense VAGUE + trans .460 .426 .442 NavyT Dense VAGUE .485 .415 .447 NavyT Dense VAGUE + trans .483 .427 .453 Baseline: All Vague .405 .405 .405 CAEVO .508 .506 .507 Table 5: Comparison with top systems. The “Dense” systems were retrained on TimeBank-Dense. The “Opt” systems were optimized to label unknown edges as VAGUE. The bottom of Table 6 illustrates the contribution of transitive closure in the labeling decisions. Prior research has hypothesized that transitive closure should help individual decisions (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009), but this has not been adequately tested due to the sparsely labeled graphs available to the community. We computed the precision of edges directly labeled by sieves, and the precision of edges inferred from transitive closure. Transitive inferences achieve better performance than directly classified edges (54.5% to 49.8% precision). Table 7 breaks apart precision by edge type. Since CAEVO labels all edges, precision and recall are the same. Edges between an event and the DCT show 55% precision, while event-event and eventCAEVO Component Ablation P R F1 CAEVO with only ML Sieves .458 .202 .28</context>
</contexts>
<marker>Yoshikawa, Riedel, Asahara, Matsumoto, 2009</marker>
<rawString>Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asahara, and Yuji Matsumoto. 2009. Jointly identifying temporal relations with Markov Logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 405–413. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
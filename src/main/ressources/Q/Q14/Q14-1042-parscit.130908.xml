<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017811">
<title confidence="0.9977505">
A New Corpus and Imitation Learning Framework for Context-Dependent
Semantic Parsing
</title>
<author confidence="0.995545">
Andreas Vlachos Stephen Clark
</author>
<affiliation confidence="0.999208">
Computer Science Department Computer Laboratory
University College London University of Cambridge
</affiliation>
<email confidence="0.997726">
a.vlachos@cs.ucl.ac.uk sc609@cam.ac.uk
</email>
<sectionHeader confidence="0.993854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920578947369">
Semantic parsing is the task of translating
natural language utterances into a machine-
interpretable meaning representation. Most
approaches to this task have been evaluated
on a small number of existing corpora which
assume that all utterances must be interpreted
according to a database and typically ignore
context. In this paper we present a new, pub-
licly available corpus for context-dependent
semantic parsing. The MRL used for the an-
notation was designed to support a portable,
interactive tourist information system. We
develop a semantic parser for this corpus
by adapting the imitation learning algorithm
DAGGER without requiring alignment infor-
mation during training. DAGGER improves
upon independently trained classifiers by 9.0
and 4.8 points in F-score on the development
and test sets respectively.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983511111111">
Semantic parsing is the task of translating natu-
ral language utterances into a machine-interpretable
meaning representation (MR). Progress in semantic
parsing has been facilitated by the existence of cor-
pora containing utterances annotated with MRs, the
most commonly used being ATIS (Dahl et al., 1994)
and GeoQuery (Zelle, 1995). As these corpora cover
rather narrow application domains, recent work has
developed corpora to support natural language in-
terfaces to the Freebase database (Cai and Yates,
2013), as well as the development of MT systems
(Banarescu et al., 2013).
However, these existing corpora have some im-
portant limitations. The MRs accompanying the
utterances are typically restricted to some form of
database query. Furthermore, in most cases each
utterance is interpreted in isolation; thus utterances
that use coreference or whose semantics are context-
dependent are typically ignored. In this paper we
present a new corpus for context-dependent seman-
tic parsing to support the development of an interac-
tive navigation and exploration system for tourism-
related activities. The new corpus was annotated
with MRs that can handle dialog context such as
coreference and can accommodate utterances that
are not interpretable according to a database, e.g.
repetition requests. The utterances were collected in
experiments with human subjects, and contain phe-
nomena such as ellipsis and disfluency. We devel-
oped guidelines and annotated 17 dialogs containing
2,374 utterances, with 82.9% exact match agreement
between two annotators.
We also develop a semantic parser for this cor-
pus. As the output MRs are rather complex, in-
stead of adopting an approach that searches the out-
put space exhaustively, we use the imitation learning
algorithm DAGGER (Ross et al., 2011) that converts
learning a structured prediction model into learning
a set of classification models. We take advantage of
its ability to learn with non-decomposable loss func-
tions and extend it to handle the absence of align-
ment information during training by developing a
randomized expert policy. Our approach improves
upon independently trained classifiers by 9.0 and 4.8
F-score on the development and test sets.
</bodyText>
<sectionHeader confidence="0.918311" genericHeader="introduction">
2 Meaning Representation Language
</sectionHeader>
<bodyText confidence="0.9968295">
Our proposed MR language (MRL) was designed
in the context of the portable, interactive naviga-
</bodyText>
<page confidence="0.985059">
547
</page>
<bodyText confidence="0.98867175">
Transactions of the Association for Computational Linguistics, vol. 2, pp. 547–559, 2014. Action Editor: Sharon Goldwater, Alexander Koller.
Submission batch: 3/2014; Revision batch 8/2014; Published 12/2014. c�2014 Association for Computational Linguistics.
tion and exploration system of Janarthanam et al.
(2013), through which users can obtain information
about places and objects of interest, such as mon-
uments and restaurants, as well as directions (see
dialog in Fig. 1). The system is aware of the po-
sition of the user (through the use of GPS technol-
ogy) and is designed to be interactive; hence it can
initiate the dialog by offering information on nearby
points of interest and correcting the route taken by
the user if needed. The MRs returned by the se-
mantic parser must represent the user utterances ad-
equately so that the system can generate the appro-
priate response. The system was developed in the
context of the SPACEBOOK project.1
The MRL uses a flat syntax composed of elemen-
tary predications, based loosely on minimal recur-
sion semantics (Copestake et al., 2005), but with-
out an explicit treatment of scope. Each MR con-
sists of a dialog act representing the overall function
of the utterance, followed for some dialog acts by
an unordered set of predicates. All predicates are
implicitly conjoined and the names of their argu-
ments specified to improve readability and to allow
for some of the arguments to be optional. The ar-
gument values can be either constants from the con-
trolled vocabulary, verbatim string extracts from the
utterance (enclosed in quotes) or variables (Xno).
Negation is denoted by a tilde (˜) in front of predi-
cates. The variables are used to bind together the ar-
guments of different predicates within an utterance,
as well as to denote coreference across utterances.
The goals in designing the MRL were to remain
close to existing semantic formalisms, whilst at the
same time producing an MRL that is particularly
suited to the application at hand (Janarthanam et al.,
2013). We also wanted an MRL that could be com-
puted with efficiently and accurately, given the na-
ture of the NL input. Hence we developed an MRL
that is able to express the relevant semantics for the
majority of the utterances in our data, without mov-
ing to the full expressive power of, e.g., DRT.
Dialog acts The dialog acts are utterance-level la-
bels which capture the overall function of the utter-
ance in the dialog, for example whether an utterance
is a question seeking a list as an answer, a statement
of information, an acknowledgement, an instruction
</bodyText>
<footnote confidence="0.922406">
1www.spacebook-project.eu
</footnote>
<figureCaption confidence="0.999463">
Figure 1: Sample dialog annotated with MRs
</figureCaption>
<figure confidence="0.94486096">
WIZARD keep walking straight down clerk street.
dialogAct(instruct)
*walk(agent:@USER, along_location:X1,
direction:forward)
isA(id:X1, type:street)
isNamed(id:X1, name:&amp;quot;clerk street&amp;quot;)
WIZARD certainly.
dialogAct(acknowledge)
USER take me to vapiano!
dialogAct(set_question)
*route(from_location:@USER,
to_location:X4)
isA(id:X4, type:restaurant)
isNamed(id:X4, name:&amp;quot;vapiano&amp;quot;)
USER what’s the nearest italian, em, for a meal?
dialogAct(set_question)
*isA(id:X1, type:restaurant)
def(id:X1)
hasProperty(id:X1, property:cuisine,
value:&amp;quot;italian&amp;quot;)
distance(location:@USER,
location:X1, value:X2)
argmin(argument:X1, value:X2)
WIZARD vapiano’s.
dialogAct(inform)
</figure>
<equation confidence="0.972678555555556">
isA(id:X4, type:restaurant)
*isNamed(id:X4, name:&amp;quot;vapiano’s&amp;quot;)
equivalent(id:X1, id:X4)
USER yes.
dialogAct(acknowledge)
USER what is this church?
dialogAct(set_question)
*isA(id:X2, type:church)
index(id:X2)
WIZARD sorry, can you say this again?
dialogAct(repeat)
USER i said what is this church on my left!
dialogAct(set_question)
*isA(id:X2, type:church)
index(id:X2)
position(id:X2, ref:@USER,
location:left)
WIZARD it is saint john’s.
dialogAct(inform)
isA(id:X3, type:church)
*isNamed(id:X3, name:&amp;quot;saint john’s&amp;quot;)
equivalent(id:X2, id:X3)
USER A sign here says it is saint mark’s.
dialogAct(inform)
isA(id:X4, type:church)
*isNamed(id:X4, name:&amp;quot;saint mark’s&amp;quot;)
equivalent(id:X2, id:X4)
</equation>
<page confidence="0.97905">
548
</page>
<bodyText confidence="0.99222325">
or a repetition request (set_question, inform,
acknowledge, instruct and repeat in Fig-
ure 1). The focal point together with the act provide
similar information to the intent annotation in ATIS
(Tur et al., 2010). The acts defined in the proposed
MRL follow the guidelines proposed by Allen and
Core (1997), Stolcke et al. (2000) and Bunt et al.
(2012).
The dialog acts are divided into two categories.
The first category contains those that are accompa-
nied by a set of predicates to represent the seman-
tics of the sentence, such as set_question and
inform. For these acts we denote their focal points
— for example the piece of information requested in
a set_question — with an asterisk (*) in front
of the relevant predicate. The second category con-
tains dialog acts that are not accompanied by predi-
cates, such as acknowledge and repeat.
Predicates The MRL contains predicates to de-
note entities, properties and their relations:
</bodyText>
<listItem confidence="0.994916307692308">
• Predicates introducing entities and their proper-
ties: isA, isNamed and hasProperty.
• Predicates describing user actions, such as walk
and turn, with arguments such as direction
and along_location.
• Predicates describing geographic relations, such
as distance, route and position, using
ref to denote relative positioning.
• Predicates denoting whether an entity is intro-
duced using a definite article (def), an indefi-
nite (indef) or an indexical (index).
• Predicates expressing numerical relations such
as argmin and argmax.
</listItem>
<bodyText confidence="0.998590291666667">
Coreference In order to model coreference we
adopt the notion of discourse referents (DRs) and
discourse entities (DEs) from Discourse Representa-
tion Theory (DRT) (Webber, 1978; Kamp and Reyle,
1993). DRs are referential expressions appearing
in utterances which denote DEs, which are mental
entities in the speaker’s model of discourse. Mul-
tiple DEs can refer to the same real-world entity;
for example, in Fig. 1 “vapiano’s” refers to a dif-
ferent DE from the restaurant in the previous sen-
tence (“the nearest italian”), even though they are
likely to be the same real-world entity. We con-
sidered DEs instead of actual entities in the MRL
because they allow us to capture the semantics of
interactions such as the last exchange between the
wizard and user. The MRL represents multiple DEs
referring to the same real-world entity through the
predicate equivalent.
Coreference is indicated by using identical vari-
ables across predicate arguments within an utterance
or across utterances. The main principle in deter-
mining whether DRs corefer is that it must be possi-
ble to infer this from the dialog context alone, with-
out using world knowledge.
</bodyText>
<sectionHeader confidence="0.899338" genericHeader="method">
3 Data Collection and Annotation
</sectionHeader>
<bodyText confidence="0.9995247">
The NL utterances were collected using Wizard-of-
Oz experiments (Kelley, 1983) with pairs of hu-
man subjects. In each experiment, one human pre-
tended to be a tourist visiting Edinburgh (by physi-
cally walking around the city), while the other per-
formed the role of the system responding through a
suitable interface using a text-to-speech system.
Each user-wizard pair was given one of two sce-
narios involving requests for directions to different
points of interest. The first scenario involves seeking
directions to the national museum of Scotland, then
going to a nearby coffee shop, followed by a pub
via a cash machine and finally looking for a park.
The second scenario involves looking for a Japanese
restaurant and the university gym, requesting infor-
mation about the Flodden Wall monument, visiting
the Scottish parliament and the Dynamic Earth sci-
ence centre, and going to the Royal Mile and the
Surgeon’s Hall museum. Each experiment formed
one dialog which was manually transcribed from
recorded audio files. 17 dialogs were collected in
total, 7 from the first scenario and 10 from the sec-
ond. More details are reported in Hill et al. (2013).
Given the varied nature of the dialogs, some of the
user requests were not within the scope of the sys-
tem. Furthermore, the proposed MRL has its own
limitations; for example it does not have predicates
to express temporal relationships. Thus, it was nec-
essary to filter the utterances collected and decide
which ones to annotate with MRs.2 In particular, we
</bodyText>
<footnote confidence="0.994671666666667">
2A similar filtering process was used for GeoQuery (Sec-
tion 7.5.1 in Zelle (1995)) and ATIS (principles of interpretation
document (/atis3/doc/pofi.doc) in the NIST CDs).
</footnote>
<page confidence="0.995594">
549
</page>
<table confidence="0.729397285714286">
vocabulary type number of terms
dialog acts 15
predicates 19
arguments 41
constants 9
entity types 26
properties 4
</table>
<tableCaption confidence="0.998848">
Table 1: MRL vocabulary used in the annotation
</tableCaption>
<bodyText confidence="0.9574315">
did not annotate utterances falling into one or more
of the following categories:
</bodyText>
<listItem confidence="0.965892333333333">
• Utterances that are not human-interpretable, e.g.
utterances that were interrupted too early to be
interpretable. In such cases, the system is likely
to respond with a repetition request.
• Utterances that are human-interpretable but out-
side the scope of the system, e.g. questions about
historical events which are not included in the
database of the application considered.
• Utterances that are within the scope of the sys-
tem but too complex to be represented by the
proposed MRL, e.g. an utterance requiring rep-
resentation of time to be interpreted.
</listItem>
<bodyText confidence="0.999711023809524">
Note that we still annotate an utterance if the core
of its semantics can be captured by the MRL. For
example, “take me to vapiano now!” would be an-
notated, even though the MRL cannot represent the
meaning of “now”. Broad information requests such
as “tell me more about this church” are also anno-
tated using the predicate extraInfo(id:Xno).
We argue that determining which utterances should
be translated into MRs, and which should be ig-
nored, is an important subtask for real-world appli-
cations of semantic parsing.
The annotation was performed by one of the au-
thors and a freelance linguist with no experience in
semantic parsing. As well as annotating the user
utterances, we also annotated the wizard utterances
with dialog acts and the entities mentioned, as they
provide the necessary context to perform context-
dependent interpretation. In practice, though, we
expect this information to be used by a natural lan-
guage generation system to produce the system’s re-
sponse and thus be available to the semantic parser.
The total number of user utterances annotated
was 2374, out of which 1906 were annotated with
MRs, the remaining not translated due to the rea-
sons discussed earlier in this section. The number
and types of the MRL vocabulary terms used ap-
pear in Tbl. 1. The annotated dialogs, the guide-
lines and the lists of the vocabulary terms are
available from http://sites.google.com/
site/andreasvlachos/resources.
In order to assess the quality of the guidelines
and the annotation, we conducted an inter-annotator
agreement study. For this purpose, the two anno-
tators annotated one dialog consisting of 510 utter-
ances. Exact match agreement at the utterance level,
which requires that the MRs by the annotators agree
on dialog act, predicates and within-utterance vari-
able assignment, was 0.829, which is a strong re-
sult given the complexity of the annotation task, and
which suggests that the proposed guidelines can be
applied consistently. We also assessed the agree-
ment on predicates using F-score, which was 0.914.
</bodyText>
<sectionHeader confidence="0.961796" genericHeader="method">
4 Comparison to Existing Corpora
</sectionHeader>
<bodyText confidence="0.99999412">
The most closely related corpus to the one presented
in this paper (herein SPACEBOOK) is the airline
travel information system (ATIS) corpus (Dahl et al.,
1994) which consists of dialogs between a user and
a flight booking system collected in Wizard-of-Oz
experiments. Each utterance is annotated with the
SQL statement that would return the requested piece
of information from the flights database. The utter-
ance interpretation is context-dependent. For exam-
ple, when the user follows up an initial flight request
— e.g. “find me flights to Boston” — with utterances
containing additional preferences — e.g. “on Mon-
day” — the interpretation of the additional prefer-
ences extends the MR for the initial request.
Compared to ATIS, the dialogs in the SPACE-
BOOK corpus are substantially longer (8.8 vs. 139.7
utterances on average respectively) and cover a
broader domain due to the longer scenarios used in
data collection. Furthermore, allowing the wizards
to answer in natural language instead of restricting
them to responding via database queries as in ATIS
led to more varied dialogs. Finally, our approach
to annotating coreference avoids repeating the MR
of previous utterances, thus resulting in shorter ex-
pressions that are closer to the semantics of the NL
</bodyText>
<page confidence="0.971694">
550
</page>
<bodyText confidence="0.999859928571428">
utterances.
The datasets developed in the recent dialog state
tracking challenge (Henderson et al., 2014) also con-
sist of dialogs between a user and a tourism informa-
tion system. However the task is easier since only
three entity types are considered (restaurant, cof-
feeshop and pub), a slot-filling MRL is used and the
argument slots take values from fixed lists.
The abstract meaning representation (AMR) de-
scribed by Banarescu et al. (2013) was developed to
provide a semantic interpretation layer to improve
machine translation (MT) systems. It has similar
predicate argument structure to the MRL proposed
here, including a lack of cover for temporal relations
and scoping. However, due to the different appli-
cation domains (MT vs. tourism-related activities),
there are some differences. Since MT systems oper-
ate at the sentence-level, each sentence is interpreted
in isolation in AMR, whilst our proposed MRL takes
context into account. Also, AMR tries to account
for all the words in a sentence, whilst our MRL only
tries to capture the semantics of those words that are
relevant to the application at hand.
Other popular semantic parsing corpora include
GeoQuery (Zelle, 1995) and Free-917 (Cai and
Yates, 2013). Both consist exclusively of questions
to be answered with a database query, the former
considering a small American geography database
and the latter the much wider Freebase database
(Bollacker et al., 2008). Unlike SPACEBOOK and
ATIS, there is no notion of context in either of these
corpora. Furthermore, the NL utterances in these
corpora are compiled to be interpreted as database
queries, which is equivalent to only one of the dialog
acts (set_question)in the SPACEBOOK corpus.
Thus the latter allows the exploration of the applica-
tion of dialog act tagging as a first step in semantic
parsing. Finally, MacMahon et al. (2006) developed
a corpus of natural language instructions paired with
sequences of actions; however the domain is limited
to simple navigation instructions and there is no no-
tion of dialog in this corpus.
</bodyText>
<sectionHeader confidence="0.90111" genericHeader="method">
5 Semantic Parsing for the New Corpus
</sectionHeader>
<bodyText confidence="0.999981048780488">
The MRL in Fig. 1 is readable and easy to annotate
with. However, it is not ideal for experiments, as it
is difficult to compare MR expressions beyond exact
match. For these reasons, we converted the MR ex-
pressions into a node-argument form. In particular,
all predicates introducing entities (isA) and most
predicates introducing relations among entities (e.g.
distance) become nodes, while all other predi-
cates (e.g. isNamed, def) are converted into argu-
ments. For example, the MR for the first utterance in
Fig. 1 is converted into the form in Fig. 2g. Entities
appearing in MR expressions without a type (e.g.
X2 in the last utterance of Fig. 1) are denoted with a
node of type empty. Each node has a unique id (e.g.
X1) and each argument can take as value a constant
(e.g. det), a node id, or a verbatim string extract from
the utterance. Arguments that are absent (e.g. the
name of restaurant) are set to the constant null.
This conversion results in 16 utterance-level labels
(15 dialog acts plus one for the non-interpretable ut-
terances), 35 node types and 32 arguments.
The comparison between a predicted and a gold
standard node-argument form is performed in three
stages. First we map the ids of the predicted nodes
to those of the gold standard. While ids do not
carry any semantics, they are needed to differenti-
ate between multiple nodes of the same type; e.g.
if a second restaurant had been predicted in
Fig. 2h then it would have a different id and would
not be matched to a gold standard node. Second,
we decompose the node-argument forms into a set
of atomic predictions (Fig. 2h). This decomposi-
tion allows the awarding of partial credit, e.g. when
the node type is correct but some of the arguments
are not. Using these atomic predictions we calculate
precision, recall and F-score.
The mapping between predicted and gold stan-
dard ids is performed by evaluating all mappings
(with mappings between nodes of different types not
allowed), and choosing the one resulting in the low-
est sum of false positives and negatives.
</bodyText>
<subsectionHeader confidence="0.991146">
5.1 Task decomposition
</subsectionHeader>
<bodyText confidence="0.999315142857143">
Fig. 2 shows the decomposition of the semantic
parsing task in stages, which are described below.
Dialog act prediction We first assign an
utterance-level label using a classifier that ex-
ploits features based on the textual content of the
utterance and on the utterance preceding it. The fea-
tures extracted from the utterance are all unigrams,
</bodyText>
<page confidence="0.981159">
551
</page>
<figure confidence="0.994612693333333">
SET QUESTION
what ’s the nearest italian for a meal ?
(a) Dialog act prediction
what ’s the nearest italian for a meal ?
SET QUESTION
distance restaurant
what ’s the nearest italian for a meal ?
(b) Node prediction
what ’s the nearest italian for a meal ?
USER
distance
restaurant
det
def
SET QUESTION
location
number
singular
restaurant
distance
cuisine
USER
det
def
SET QUESTION
location
number
singular
OUT OUT OUT OUT IN OUT OUT OUT
number
argmin
USER
distance restaurant
USER
distance restaurant
(c) Constant argument prediction
SET QUESTION
location
singular
location
(d) String argument prediction
SET QUESTION
focus
number
singular
argmin
location
location
what ’s the nearest italian for a meal ?
(e) Node argument prediction
what ’s the nearest italian for a meal ?
(f) Focus/negation prediction
cuisine
det
def
cuisine
det
def
dialogAct:SET QUESTION
X1:restaurant
X1:restaurant(num:singular)
X1:restaurant(det:def)
X1:restaurant(cuisine:“italian”)
X2:distance
X2:distance(location:USER)
X2:distance(location:X1-restaurant (num:singular, det:def))
X2:distance(argmin:X1)
focus:X1-restaurant(num:singular, det:def)
SET QUESTION
X1:restaurant(num:singular,
det:def, cuisine:“italian”)
X2:distance(location:USER,
location:X1, argmin:X1)
focus:X1
(g) Node-argument form (h) Atomic predictions
</figure>
<figureCaption confidence="0.999739">
Figure 2: Semantic parsing decomposition.
</figureCaption>
<bodyText confidence="0.999827941176471">
bigrams and trigrams and the final punctuation
mark. Unlike in typical text classification tasks,
content words are not always helpful in dialog act
tagging; e.g. the token “meal” in Fig. 2a is not
indicative of set_question, while n-grams of
words typically considered as stopwords, such as
“what ’s the”, can be more helpful. If the dialog act
predicted is to be accompanied by other predicates
according to the guidelines (Sec. 2) we proceed to
the following stages, otherwise stop.
The features based on the preceding utterance in-
dicate whether it was by the user or the wizard and,
in the latter case, its dialog act. Such features are
useful in determining the act of short, ambiguous
utterances such as “yes”, which is tagged as yes
when following a prop_question utterance, but
as acknowledge otherwise.
</bodyText>
<page confidence="0.988249">
552
</page>
<bodyText confidence="0.99988384375">
Node prediction In node prediction we use a clas-
sifier to predict whether each of the tokens in the ut-
terance denotes a node of a particular type or empty
(Fig. 2b). The features used include the target to-
ken and its lemma, which are conjoined with the
PoS tag, the previous and following tokens, as well
as the lemmas of the tokens with which it has syn-
tactic dependencies. Further features represent the
dialog act (e.g. route is more likely to appear in
a set question utterance), and the number and
types of the nodes already predicted. Since the
evaluation ignores the alignment between nodes and
tokens, it would have been correct to predict the
correct nodes from any token; e.g. restaurant
could be predicted from “italian” instead. However,
alignment does affect argument prediction, since it
determines its feature extraction.
Constant argument prediction In this stage
(Fig. 2c) we predict, for each argument of each node,
whether its value is an MRL vocabulary term, a ver-
batim string extract, a node, or absent (special val-
ues STRING, NODE, null respectively). If the value
predicted is STRING or NODE it is replaced by the
predictions in subsequent stages. For each argument
different values are possible; thus we use separate
classifiers for each, resulting in 32 classifiers. The
features used include the node type, the token that
predicted the node, and the syntactic dependency
paths from that token to all other tokens in the ut-
terance. We also include as features the values pre-
dicted for other arguments of the node, the dialog
act, and the other node types predicted.
String argument prediction For each argument
predicted to be STRING (e.g. cuisine in Fig-
ure 2d), we predict for each token in left-to-right or-
der whether it should be part of the value for this
argument or not (IN or OUT). Since the strings that
are appropriate for each argument differ (e.g. the
strings for cuisine are unlikely to be appropriate
for name), we use separate classifiers for each of
them, resulting in five classifiers. The features used
include the target token and its lemma, its conjunc-
tion with the PoS tag, the previous and following
tokens, and the lemmas of the tokens with which it
has syntactic dependencies. We also added the label
assigned to the previous token and the syntactic de-
pendency path to the token that predicted the node.
Node argument prediction For each argument
predicted to have NODE as its value, we predict for
every other node whether it should be the value or
not (e.g. argmin in Fig. 2e). As with the string ar-
gument prediction, we use separate binary classifiers
for each argument, resulting in 18 classifiers. The
features extracted are similar to that stage, but we
now consider the tokens that predicted each candi-
date argument node (e.g. “meal” for restaurant)
instead of the tokens in the utterance.
Focus/Negation prediction We predict whether
each node should be focused or negated as two sep-
arate binary tasks. The features used include the to-
ken that predicted the target node, its lemma and PoS
tag and the syntactic dependency paths to all other
tokens in the utterance. Further features include the
type of the node and its arguments.
</bodyText>
<sectionHeader confidence="0.995542" genericHeader="method">
6 Imitation Learning
</sectionHeader>
<bodyText confidence="0.999930363636364">
In order to learn the classifiers for the task de-
composition described, two challenges must be ad-
dressed. The first is the complexity of the struc-
ture to be predicted. The task involves many inter-
dependent predictions made by a variety of clas-
sifiers, and thus cannot be tackled by approaches
that assume a particular type of graph structure, or
restrict structure feature extraction in order to per-
form efficient dynamic programming. The second
challenge is the lack of alignment information dur-
ing training. Imitation learning algorithms such as
SEARN (Daum´e III et al., 2009) and DAGGER (Ross
et al., 2011) have been applied successfully to a vari-
ety of structured prediction tasks including summa-
rization, biomedical event extraction and dynamic
feature selection (Daum´e III et al., 2009; Vlachos,
2012; He et al., 2013) thanks to their ability to han-
dle complex output spaces without exhaustive search
and their flexibility in incorporating features based
on the structured output. In this work we focus on
DAGGER and extend it to handle the missing align-
ments.
</bodyText>
<subsectionHeader confidence="0.983461">
6.1 Structured prediction with DAGGER
</subsectionHeader>
<bodyText confidence="0.999888">
The dataset aggregation (DAGGER) algorithm (Ross
et al., 2011) forms the prediction of an instance s as
a sequence of T actions ˆy1:T predicted by a learned
policy which consists of one or more classifiers.
</bodyText>
<page confidence="0.998452">
553
</page>
<table confidence="0.91666235">
Algorithm 1: Imitation learning with DAG-
GER
Input: training instances S, expert policy π*,
loss function `, learning rate β, CSC learner CSCL
Output: Learned policy HN
1 CSC Examples E = 0
2 for i = 1 to N do
3 p = (1 − β)i−1
4 current policy π = pπ* + (1 − p)Hi−1
5 for s in S do
6 Predict π(s) = ˆy1:T
7 for ˆyt in π(s) do
8 Extract features Φt = f(s, ˆy1:t−1)
9 foreach possible action yjt do
10 Predict a,,�
yft+1:T = π(s; ˆy1:t−1, Jt )
11 Assess
cjt = `(ˆy1:t−1, yjt, yft+1:T)
12 Add (Φt, ct) to E
13 Learn Hi = CSCL(E)
</table>
<bodyText confidence="0.999949734375">
These actions are taken in a greedy fashion, i.e. once
an action has been taken it cannot be changed. Dur-
ing training, it converts the problem of learning how
to predict these sequences of actions into cost sensi-
tive classification (CSC) learning. In CSC learning
each training example has a vector of misclassifica-
tion costs associated with it, thus rendering some
mistakes on some examples to be more expensive
than others (Domingos, 1999).
Algorithm 1 presents the training procedure.
DAGGER requires a set of labeled training instances
S and a loss function ` that compares complete out-
puts for instances in S against the gold standard.
In addition, an expert policy π* must be specified
which is an oracle that returns the optimal action for
the instances in S, akin to an expert demonstrating
the task. π* is typically derived from the gold stan-
dard; e.g. in part of speech tagging π* would return
the correct tag for each token. In addition, the learn-
ing rate β and a CSC learner (CSCL) must be pro-
vided. The algorithm outputs a learned policy HN
that, unlike π*, can generalize to unseen data.
Each training iteration begins by setting the prob-
ability p (line 3) of using π* in the current policy π.
In the first iteration, only π* is used but, in later it-
erations, π becomes stochastic and, for each action,
π* is used with probability p, and the learned pol-
icy from the previous iteration Hz_1 with probability
1 − p (line 4). Then π is used to predict each train-
ing instance s (line 6). For each action ˆyt, a CSC
example is generated (lines 7-12). The features Φt
are extracted from s and all previous actions ˆy1:t_1
(line 8). The cost for each possible action yt is es-
timated by predicting the remaining actions yft+1:T
for s using π (line 10) and calculating the loss in-
curred given yt w.r.t. the gold standard for s using
` (line 11). As π is stochastic, it is common to use
multiple samples of yft+1:T to assess the cost of each
action yt by repeating lines 10-11. The features, to-
gether with the costs for each possible action, form
a CSC example (Φt, ct) (line 12). At the end of each
iteration, the CSC examples obtained from all itera-
tions are used by the CSC learning algorithm to learn
the classifier(s) for Hz (line 13).
When predicting the training instances (line 6),
and when estimating the costs for each possible ac-
tion (lines 10-11), the policy learned in the previous
iteration Hz_1 is used as part of π after the first it-
eration. Thus the CSC examples generated to learn
Hz depend on the predictions of Hz_1 and, by grad-
ually increasing the use of Hz_1 and ignoring π* in
π, the learned policies are adjusted to their own pre-
dictions, thus learning the dependencies among the
actions and how to predict them in order to mini-
mize the loss. The learning rate β determines how
fast π moves away from π*. The use of Hz_1 in
predicting the training instances (line 6) also has the
effect of exploring sub-optimal actions so that the
learned policies are adjusted to recover from their
mistakes. Finally, note that if only one training it-
eration is performed, the learned policy is equiva-
lent to a set of independently trained classifiers since
no training against the predictions of the previously
learned policy takes place.
</bodyText>
<subsectionHeader confidence="0.999872">
6.2 Training with missing alignments
</subsectionHeader>
<bodyText confidence="0.999985625">
The loss function ` in DAGGER is only used to
compare complete outputs against the gold standard.
Therefore, when generating a CSC training example
in DAGGER (lines 7-12), we do not need to know
whether an action yt is correct or not, we only evalu-
ate what the effect of yt is on the loss incurred by the
complete action sequence. Thus, it does not need to
decompose over the actions taken to evaluate them.
</bodyText>
<page confidence="0.99682">
554
</page>
<bodyText confidence="0.99985169047619">
The ability to train against non-decomposable loss
functions is useful when the training data has miss-
ing labels, as is the case with semantic parsing. Fol-
lowing Sec. 5, f is defined as the sum of the false
positive and false negative atomic predictions used
to calculate precision and recall and, since it ignores
the alignment between tokens and nodes, it cannot
assess node prediction actions. However, we can use
it under DAGGER to learn a node prediction classi-
fier together with the classifiers of the other stages.
The only component of DAGGER which assumes
knowledge of the correct actions for training is the
expert policy 7r*. Since these are not available for
the node prediction stage, we replace 7r* with a ran-
domized expert policy 7rrand, in which actions that
are not specified by the annotation are chosen ran-
domly from a set of equally optimal ones. For ex-
ample, in Fig. 2b when predicting the action for
each token, 7rrand chooses randomly among null,
distance, and restaurant, so that by the end
of the stage the correct nodes have been predicted.
Randomizing this choice helps explore the actions
available. In our experiments we placed a uniform
distribution over the available actions, i.e. all op-
timal actions are equally likely to be chosen. The
actions returned by 7rrand will often result in align-
ments that do not incur any loss but are nonsensical,
e.g. predicting restaurant from “what”. How-
ever, since 7rrand is progressively ignored, the effect
of such actions is reduced.
While being able to learn a semantic parser with-
out alignment information is useful, it would help to
use some supervision, e.g. that “street” commonly
predicts the node street. We incorporate such
an alignment dictionary in 7rrand as follows: if the
target token is mapped to a node type in the dictio-
nary, and if a node of this type needs to be predicted
for the utterance, then this type is returned. Other-
wise, the prediction is made with 7rrand. Finally, like
7rrand itself, the dictionary is progressively ignored
and neither constrains the training process, nor is
used during testing.
</bodyText>
<sectionHeader confidence="0.99849" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999793019607843">
We split the annotated dialogs into training and test
sets. The former consists of four dialogs from the
first scenario and seven from the second, and the lat-
ter of three dialogs from each scenario. All devel-
opment and feature engineering was conducted us-
ing cross-validation on the training set, at the dialog
level rather than the utterance level (therefore result-
ing in as many folds as dialogs in the training set),
to ensure that each fold contains utterances from all
parts of the scenario from which the dialog is taken.
To perform cost-sensitive classification learning
we used the adaptive regularization of weight vec-
tors (AROW) algorithm (Crammer et al., 2009).
AROW is an online algorithm for linear predic-
tors that adjusts the per-feature learning rates so
that popular features do not overshadow rare but
useful ones. Given the task decomposition, each
learned hypothesis consists of 59 classifiers. We
restricted the prediction of nodes to content words
since function words are unlikely to provide useful
alignments. All preprocessing was performed us-
ing the Stanford CoreNLP toolkit (Manning et al.,
2014). The implementation of the semantic parser is
available from http://sites.google.com/
site/andreasvlachos/resources. The
DAGGER parameters were set to 12 training itera-
tions, 0 = 0.3 and 3 samples for action cost assess-
ment. We compared our DAGGER-based imitation
learning approach (henceforth Imit) against indepen-
dently trained classifiers using the same classifica-
tion learner and features (henceforth Indep).
For both systems we incorporated an alignment
dictionary (+align versions) as described in Sec. 6.2,
in order to improve node prediction performance.
The dictionary was extracted from the training data
and contains 96 tokens that commonly predict a par-
ticular node type.
The results from the cross-validation experiments
are reported in Tbl. 2. Overall performance eval-
uated as described in Sec. 5 was 53.6 points in F-
score for Imit, 5.7 points higher than Indep and the
difference is greater for the +align versions. These
results demonstrate the advantages of training clas-
sifiers using imitation learning versus independently
trained classifiers. Isolating the performance for
node and argument prediction stages, we observe
that the main bottleneck is the former, which in the
case of Imit is 60.9 points in F-score compared to
78.8 for the latter. Accuracy for dialog acts is 78.9%.
As shown in Tbl. 2, the alignment dictionary im-
proved not only node prediction performance by 6
</bodyText>
<page confidence="0.995022">
555
</page>
<table confidence="0.999885428571429">
Imit Imit+align Indep Indep+align
exact match (accuracy) 58.4% 59.1% 56% 55.9%
dialog act (accuracy) 78.9% 79.3% 78.8% 79%
nodes (Rec/Prec/F) 72.3 52.6 60.9 76.1 59.8 66.9 44.4 61.6 51.6 53.3 64 58.1
arguments (Rec/Prec/F) 77.6 80 78.8 79.6 83 81.3 74.1 67.2 70.1 78.2 66.3 71.8
focus (Rec/Prec/F) 81.8 87.2 84.4 84.4 86.7 85.5 85.9 87 86.5 86.8 8.3 84.7
overall (Rec/Prec/F) 59.3 48.9 53.6 62.2 54.4 59.1 45.3 50.8 47.9 50 50.1 50.1
</table>
<tableCaption confidence="0.99947">
Table 2: Performances using 11-fold cross-validation on the training set.
</tableCaption>
<bodyText confidence="0.999811974358974">
points in F-score, but also argument prediction by
2.5 points, thus demonstrating the benefits of learn-
ing the alignments together with the other compo-
nents of the semantic parser. The overall perfor-
mance improved by 5.5 points in F-score.
Finally, we ran an experiment with oracle node
prediction and found that the overall performance
using cross-validation on the training data improved
to 88.2 and 79.9 points in F-score for the Imit+align
Indep+align systems. This is in agreement with the
results presented by Flanigan et al. (2014) on devel-
oping a semantic parsing parser for the AMR for-
malism who also argue that node prediction is the
main performance bottleneck.
Tbl. 3 gives results on the test set. The overall
performance for Imit is 48.4 F-score and 47.9% for
exact match. As in the cross-validation results on
the training data, training with imitation learning im-
proved upon independently trained classifiers. The
performance was improved further using the align-
ment dictionary, reaching 53.5 points in F-score and
49.1% exact match accuracy.
In the experimental setup above, dialogs from the
same scenarios appear in both training and testing.
While this is a reasonable evaluation approach also
followed in ATIS evaluations, it is likely to be rel-
atively forgiving; in practice, semantic parsers are
likely to encounter entities, activities, etc. unseen in
training. Hence we conducted a second evaluation
in which dialogs from one scenario are used to train
a parser evaluated on the other (still respecting the
train/test split from before). When testing on the di-
alogs from the first scenario and training on the di-
alogs from the second, the overall performance us-
ing Imit+align was 36.9 points in F-score, while in
the reverse experiment it was 41.7. Note that direct
comparisons against the performances in Tbl. 3 are
not meaningful since fewer dialogs are being used
for training and testing in the cross-scenario setup.
</bodyText>
<sectionHeader confidence="0.847512" genericHeader="method">
8 Comparison with Related Work
</sectionHeader>
<bodyText confidence="0.999743121212121">
Previous work on semantic parsing handled the
lack of alignments during training in a variety of
ways. Zettlemoyer and Collins (2009) manually
engineered a CCG lexicon for the ATIS corpus.
Kwiatkowski et al. (2011) used a dedicated algo-
rithm to infer a similar dictionary and used align-
ments from Giza++ (Och and Ney, 2000) to initial-
ize the relevant features. Most recent work on Geo-
Query uses an alignment dictionary that includes for
each geographical entity all noun phrases referring
to it (Jones et al., 2012). More recently, Flanigan
et al. (2014) developed a dedicated alignment model
on top of which they learned a semantic parser for
the AMR formalism. In our approach, we learn the
alignments together with the semantic parser with-
out requiring a dictionary.
In terms of structured prediction frameworks,
most previous work uses hidden variable linear
(Zettlemoyer and Collins, 2007) or log-linear (Liang
et al., 2011) models with beam search. In terms of
direct comparisons with existing work, the goal of
this paper is to introduce the new corpus and pro-
vide a competitive first attempt at the new semantic
parsing task. However, we believe it is non-trivial to
apply existing approaches to the new task, since, as-
suming a decomposition similar to that of Sec. 5.1,
exhaustive search would be too expensive, and ap-
plying vanilla beam search would be difficult since
different predictions result in beams of (sometimes
radically) different lengths that are not comparable.
We have attempted applying the MT-based se-
mantic parsing approach proposed by Andreas et al.
(2013) to our dataset but in initial experiments the
</bodyText>
<page confidence="0.994192">
556
</page>
<table confidence="0.999850285714286">
Imit Imit+align Indep Indep+align
exact match (accuracy) 47.9% 49.1% 47.6% 46.1%
dialog act (accuracy) 77% 80.5% 79.8% 79.5%
nodes (Rec/Prec/F) 68.7 45.7 54.8 75.5 51.7 61.4 41.9 61.1 49.7 54 64.9 58.9
arguments (Rec/Prec/F) 73.9 73.7 73.8 76.8 77.3 77.1 69.5 61.3 65.1 77.3 63.6 69.8
focus (Rec/Prec/F) 87.1 80.7 83.8 86 81.2 83.6 81.6 73.4 77.3 90.6 76.8 83.1
overall (Rec/Prec/F) 56.6 42.3 48.4 63.5 46.2 53.5 41.2 47.8 44.3 50 47.4 48.7
</table>
<tableCaption confidence="0.999925">
Table 3: Performances on the test set.
</tableCaption>
<bodyText confidence="0.999908212765958">
performance was poor. The main reason for this is
that, unlike GeoQuery, the proposed MRL does not
align well with English.
The expert policy in DAGGER is a generalization
of the dynamic oracle of Goldberg and Nivre (2013)
for shift-reduce dependency parsing to any struc-
tured prediction task decomposed into a sequence of
actions. The randomized expert policy proposed ex-
tends DAGGER to learn not only how to avoid error
propagation, but also how to infer latent variables.
The main bottleneck is training data sparsity.
Some node types appear only a few times in rela-
tively long utterances, and thus it is difficult to infer
appropriate alignments for them. Unlike machine
translation between natural languages, it is unreal-
istic to expect large quantities of utterances to be
annotated with MR expressions. An appealing al-
ternative would be to use response-based learning,
i.e. use the response from the system instead of MR
expressions as training signal (Liang et al., 2011;
Kwiatkowski et al., 2013; Berant and Liang, 2014).
However such an approach would not be straight-
forward to implement in our application, since the
response from the system is not always the result
of a database query but, e.g., a navigation instruc-
tion that is context-dependent and thus difficult to
assess its correctness. Furthermore, it would require
the development of a user simulator (Keizer et al.,
2012), a non-trivial task which is beyond the scope
of this work. A different approach is to use dialogs
between a system and its users as proposed by Artzi
and Zettlemoyer (2011) using the DARPA commu-
nicator corpus (Walker et al., 2002). However, in
that work utterances were selected to be shorter than
6 words and to include one noun phrase present
in the lexicon used during learning while ignoring
short but common phrases such as “yes” and “no”;
thus it is unclear whether it would be applicable to
our dataset.
Finally, dialog context is only taken into account
in predicting the dialog act for each utterance. Even
though our corpus contains coreference information,
we did not attempt this task as it is difficult to eval-
uate and our performance on node prediction on
which it relies is relatively low. We leave corefer-
ence resolution on the new corpus as an interesting
and challenging task for future work.
</bodyText>
<sectionHeader confidence="0.997631" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999586894736842">
In this paper we presented a new corpus for context-
dependent semantic parsing in the context of a
portable, interactive navigation and exploration sys-
tem for tourism-related activities. The MRL used
for the annotation can handle dialog context such
as coreference and can accommodate utterances that
are not interpretable according to a database. We
conducted an inter-annotator agreement study and
found 0.829 exact match agreement.
We also developed a semantic parser for the
SPACEBOOK corpus using the imitation learning al-
gorithm DAGGER that, unlike previous approaches,
can infer the missing alignments in the training data
using a randomized expert policy. In experiments
using the new corpus we found that training with im-
itation learning substantially improves performance
compared to independently trained classifiers. Fi-
nally, we showed how to improve performance fur-
ther by incorporating an alignment dictionary.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99951125">
The research reported was conducted while the
first author was at the University of Cambridge
and funded by the European Community’s Sev-
enth Framework Programme (FP7/2007-2013) un-
</bodyText>
<page confidence="0.989009">
557
</page>
<bodyText confidence="0.999883142857143">
der grant agreement no. 270019 (SPACEBOOK
project www.spacebook-project.eu). The
authors would like to acknowledge the work of Di-
ane Nicholls in the annotation; the efforts of Robin
Hill in collecting the dialogs from Wizard-of-Oz ex-
periments; and Tim Vieira for helpful comments on
an earlier version of this manuscript.
</bodyText>
<sectionHeader confidence="0.998481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999351329787234">
James Allen and Mark Core. 1997. Dialogue act markup
in several layers. Technical report, University of
Rochester.
Jacob Andreas, Andreas Vlachos, and Stephen Clark.
2013. Semantic parsing as machine translation. In
Proceedings of the 51st Annual Meeting of the Associ-
ation for Computational Linguistics (short papers).
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping
semantic parsers from conversations. In Proceedings
of the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 421–432, Edinburgh,
UK.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop and Interoperability with Dis-
course, pages 178–186, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Jonathan Berant and Percy Liang. 2014. Semantic pars-
ing via paraphrasing. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management of
Data, pages 1247–1250.
Harry Bunt, Jan Alexandersson, Jae-Woong Choe,
Alex Chengyu Fang, Koiti Hasida, Volha Petukhova,
Andrei Popescu-Belis, and David Traum. 2012. Iso
24617-2: A semantically-based standard for dialogue
annotation. In Proceedings of the Eight International
Conference on Language Resources and Evaluation,
Istanbul, Turkey.
Qingqing Cai and Alexander Yates. 2013. Large-scale
Semantic Parsing via Schema Matching and Lexicon
Extension. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-
lard. 2005. Minimal recursion semantics: An in-
troduction. Research in Language and Computation,
3(2–3):281–332.
Koby Crammer, Alex Kulesza, and Mark Dredze. 2009.
Adaptive regularization of weight vectors. In Ad-
vances in Neural Information Processing Systems 22,
pages 414–422.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In Proceedings of the Work-
shop on Human Language Technology, pages 43–48,
Plainsboro, New Jersey.
Hal Daum´e III, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, 75:297–325.
Pedro Domingos. 1999. Metacost: a general method for
making classifiers cost-sensitive. In Proceedings of
the 5th International Conference on Knowledge Dis-
covery and Data Mining, pages 155–164. Association
for Computing Machinery.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris
Dyer, and Noah A. Smith. 2014. A discriminative
graph-based parser for the abstract meaning represen-
tation. In Proceedings of the 52nd Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1426–1436, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
Transactions of the Association for Computational
Linguistics, 3(1):403–414, October.
He He, Hal Daum´e III, and Jason Eisner. 2013. Dynamic
feature selection for dependency parsing. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1455–1464,
Seattle, October.
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The Third Dialog State Tracking
Challenge. In Proceedings of IEEE Spoken Language
Technology.
Robin Hill, Jana G¨otze, and Bonnie Webber. 2013.
SpaceBook Project: Final Data Release, Wizard-of-
Oz (WoZ) experiments. Technical report, University
of Edinburgh.
Srinivasan Janarthanam, Oliver Lemon, Phil Bartie,
Tiphaine Dalmas, Anna Dickinson, Xingkun Liu,
William Mackaness, and Bonnie Webber. 2013. Eval-
uating a city exploration dialogue system with inte-
grated question-answering and pedestrian navigation.
</reference>
<page confidence="0.976435">
558
</page>
<reference confidence="0.999823269230769">
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1660–1668, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Bevan Keeley Jones, Mark Johnson, and Sharon Goldwa-
ter. 2012. Semantic parsing with Bayesian tree trans-
ducers. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, pages
488–496.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic. Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and Discourse Rep-
resentation Theory. Kluwer, Dordrecht.
Simon Keizer, Stphane Rossignol, Senthilkumar Chan-
dramohan, and Olivier Pietquin. 2012. User simula-
tion in the development of statistical spoken dialogue
systems. In Oliver Lemon and Olivier Pietquin, edi-
tors, Data-Driven Methods for Adaptive Spoken Dia-
logue Systems, pages 39–73. Springer New York.
John F. Kelley. 1983. An empirical methodology for
writing user-friendly natural language computer appli-
cations. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, pages 193–
196.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic parsing.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1512–1523, Edinburgh, UK.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, pages 1545–1556, Seattle, WA.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional semantics.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 590–599, Portland, Ore-
gon.
Matt MacMahon, Brian Stankiewicz, and Benjamin
Kuipers. 2006. Walk the talk: connecting language,
knowledge, and action in route instructions. In Pro-
ceedings of the 21st National Conference on Artificial
Intelligence, pages 1475–1482. AAAI Press.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440–447, Hong Kong, China.
St´ephane Ross, Geoffrey J. Gordon, and Drew Bagnell.
2011. A reduction of imitation learning and structured
prediction to no-regret online learning. In 14th In-
ternational Conference on Artificial Intelligence and
Statistics, pages 627–635.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339–373.
Gokhan Tur, Dilek Hakkani-T¨ur, and Larry Heck. 2010.
What’s left to be understood in ATIS? In IEEE Work-
shop on Spoken Language Technologies.
Andreas Vlachos. 2012. An investigation of imitation
learning algorithms for structured prediction. Journal
of Machine Learning Research Workshop and Confer-
ence Proceedings, Proceedings of the 10th European
Workshop on Reinforcement Learning, 24:143–154.
Marilyn A. Walker, Alexander I. Rudnicky, Rashmi
Prasad, John S. Aberdeen, Elizabeth Owen Bratt,
John S. Garofolo, Helen Wright Hastie, Audrey N.
Le, Bryan L. Pellom, Alexandros Potamianos, Re-
becca J. Passonneau, Salim Roukos, Gregory A.
Sanders, Stephanie Seneff, and David Stallard. 2002.
DARPA communicator: cross-system results for the
2001 evaluation. In Proceedings of the 7th Interna-
tional Conference on Spoken Language Processing.
Bonnie Lynn Webber. 1978. A Formal Approach to Dis-
course Anaphora. Ph.D. thesis, Harvard University.
John M. Zelle. 1995. Using Inductive Logic Program-
ming to Automate the Construction of Natural Lan-
guage Parsers. Ph.D. thesis, Department of Computer
Sciences, The University of Texas at Austin.
Luke S. Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to logi-
cal form. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 678–687.
Luke S. Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint conference
of the 47th Annual Meeting of the Association for
Computational Linguistics and the 4th International
Joint Conference on Natural Language Processing of
the Asian Federation of Natural Language Processing,
pages 976–984, Singapore.
</reference>
<page confidence="0.999176">
559
560
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902024">
<title confidence="0.9995005">A New Corpus and Imitation Learning Framework for Semantic Parsing</title>
<author confidence="0.999801">Andreas Vlachos Stephen Clark</author>
<affiliation confidence="0.99971">Computer Science Department Computer Laboratory University College London University of Cambridge</affiliation>
<email confidence="0.913737">a.vlachos@cs.ucl.ac.uksc609@cam.ac.uk</email>
<abstract confidence="0.99943745">Semantic parsing is the task of translating natural language utterances into a machineinterpretable meaning representation. Most approaches to this task have been evaluated on a small number of existing corpora which assume that all utterances must be interpreted according to a database and typically ignore context. In this paper we present a new, publicly available corpus for context-dependent semantic parsing. The MRL used for the annotation was designed to support a portable, interactive tourist information system. We develop a semantic parser for this corpus by adapting the imitation learning algorithm requiring alignment inforduring training. upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Mark Core</author>
</authors>
<title>Dialogue act markup in several layers.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="7715" citStr="Allen and Core (1997)" startWordPosition="1119" endWordPosition="1122">n(id:X2, ref:@USER, location:left) WIZARD it is saint john’s. dialogAct(inform) isA(id:X3, type:church) *isNamed(id:X3, name:&amp;quot;saint john’s&amp;quot;) equivalent(id:X2, id:X3) USER A sign here says it is saint mark’s. dialogAct(inform) isA(id:X4, type:church) *isNamed(id:X4, name:&amp;quot;saint mark’s&amp;quot;) equivalent(id:X2, id:X4) 548 or a repetition request (set_question, inform, acknowledge, instruct and repeat in Figure 1). The focal point together with the act provide similar information to the intent annotation in ATIS (Tur et al., 2010). The acts defined in the proposed MRL follow the guidelines proposed by Allen and Core (1997), Stolcke et al. (2000) and Bunt et al. (2012). The dialog acts are divided into two categories. The first category contains those that are accompanied by a set of predicates to represent the semantics of the sentence, such as set_question and inform. For these acts we denote their focal points — for example the piece of information requested in a set_question — with an asterisk (*) in front of the relevant predicate. The second category contains dialog acts that are not accompanied by predicates, such as acknowledge and repeat. Predicates The MRL contains predicates to denote entities, proper</context>
</contexts>
<marker>Allen, Core, 1997</marker>
<rawString>James Allen and Mark Core. 1997. Dialogue act markup in several layers. Technical report, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Andreas Vlachos</author>
<author>Stephen Clark</author>
</authors>
<title>Semantic parsing as machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</booktitle>
<contexts>
<context position="39986" citStr="Andreas et al. (2013)" startWordPosition="6435" endWordPosition="6438"> of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe it is non-trivial to apply existing approaches to the new task, since, assuming a decomposition similar to that of Sec. 5.1, exhaustive search would be too expensive, and applying vanilla beam search would be difficult since different predictions result in beams of (sometimes radically) different lengths that are not comparable. We have attempted applying the MT-based semantic parsing approach proposed by Andreas et al. (2013) to our dataset but in initial experiments the 556 Imit Imit+align Indep Indep+align exact match (accuracy) 47.9% 49.1% 47.6% 46.1% dialog act (accuracy) 77% 80.5% 79.8% 79.5% nodes (Rec/Prec/F) 68.7 45.7 54.8 75.5 51.7 61.4 41.9 61.1 49.7 54 64.9 58.9 arguments (Rec/Prec/F) 73.9 73.7 73.8 76.8 77.3 77.1 69.5 61.3 65.1 77.3 63.6 69.8 focus (Rec/Prec/F) 87.1 80.7 83.8 86 81.2 83.6 81.6 73.4 77.3 90.6 76.8 83.1 overall (Rec/Prec/F) 56.6 42.3 48.4 63.5 46.2 53.5 41.2 47.8 44.3 50 47.4 48.7 Table 3: Performances on the test set. performance was poor. The main reason for this is that, unlike GeoQue</context>
</contexts>
<marker>Andreas, Vlachos, Clark, 2013</marker>
<rawString>Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (short papers).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Bootstrapping semantic parsers from conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>421--432</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="42084" citStr="Artzi and Zettlemoyer (2011)" startWordPosition="6782" endWordPosition="6785"> as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). However, in that work utterances were selected to be shorter than 6 words and to include one noun phrase present in the lexicon used during learning while ignoring short but common phrases such as “yes” and “no”; thus it is unclear whether it would be applicable to our dataset. Finally, dialog context is only taken into account in predicting the dialog act for each utterance. Even though our corpus contains coreference information, we did not attempt this task as it is difficult to evaluate and our performance on node prediction on wh</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping semantic parsers from conversations. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 421–432, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>178--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1665" citStr="Banarescu et al., 2013" startWordPosition="236" endWordPosition="239">ent and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignored. In this paper we present a new corpus for context-dependent semantic parsing to support the development of an interactive navigation and exploration system for tourismrelated activities. The new corpus was annotated with MRs that can handle dialog context such as coref</context>
<context position="16313" citStr="Banarescu et al. (2013)" startWordPosition="2510" endWordPosition="2513">inally, our approach to annotating coreference avoids repeating the MR of previous utterances, thus resulting in shorter expressions that are closer to the semantics of the NL 550 utterances. The datasets developed in the recent dialog state tracking challenge (Henderson et al., 2014) also consist of dialogs between a user and a tourism information system. However the task is easier since only three entity types are considered (restaurant, coffeeshop and pub), a slot-filling MRL is used and the argument slots take values from fixed lists. The abstract meaning representation (AMR) described by Banarescu et al. (2013) was developed to provide a semantic interpretation layer to improve machine translation (MT) systems. It has similar predicate argument structure to the MRL proposed here, including a lack of cover for temporal relations and scoping. However, due to the different application domains (MT vs. tourism-related activities), there are some differences. Since MT systems operate at the sentence-level, each sentence is interpreted in isolation in AMR, whilst our proposed MRL takes context into account. Also, AMR tries to account for all the words in a sentence, whilst our MRL only tries to capture the</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="41546" citStr="Berant and Liang, 2014" startWordPosition="6692" endWordPosition="6695"> to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). Ho</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="17298" citStr="Bollacker et al., 2008" startWordPosition="2664" endWordPosition="2667">tems operate at the sentence-level, each sentence is interpreted in isolation in AMR, whilst our proposed MRL takes context into account. Also, AMR tries to account for all the words in a sentence, whilst our MRL only tries to capture the semantics of those words that are relevant to the application at hand. Other popular semantic parsing corpora include GeoQuery (Zelle, 1995) and Free-917 (Cai and Yates, 2013). Both consist exclusively of questions to be answered with a database query, the former considering a small American geography database and the latter the much wider Freebase database (Bollacker et al., 2008). Unlike SPACEBOOK and ATIS, there is no notion of context in either of these corpora. Furthermore, the NL utterances in these corpora are compiled to be interpreted as database queries, which is equivalent to only one of the dialog acts (set_question)in the SPACEBOOK corpus. Thus the latter allows the exploration of the application of dialog act tagging as a first step in semantic parsing. Finally, MacMahon et al. (2006) developed a corpus of natural language instructions paired with sequences of actions; however the domain is limited to simple navigation instructions and there is no notion o</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Bunt</author>
</authors>
<title>Alexandersson, Jae-Woong Choe, Alex Chengyu Fang, Koiti Hasida, Volha Petukhova, Andrei Popescu-Belis, and</title>
<date></date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation,</booktitle>
<location>Istanbul, Turkey.</location>
<marker>Bunt, </marker>
<rawString>Harry Bunt, Jan Alexandersson, Jae-Woong Choe, Alex Chengyu Fang, Koiti Hasida, Volha Petukhova, Andrei Popescu-Belis, and David Traum. 2012. Iso 24617-2: A semantically-based standard for dialogue annotation. In Proceedings of the Eight International Conference on Language Resources and Evaluation, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale Semantic Parsing via Schema Matching and Lexicon Extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1598" citStr="Cai and Yates, 2013" startWordPosition="224" endWordPosition="227">ned classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignored. In this paper we present a new corpus for context-dependent semantic parsing to support the development of an interactive navigation and exploration system for tourismrelated activities. The new corpus </context>
<context position="17089" citStr="Cai and Yates, 2013" startWordPosition="2632" endWordPosition="2635">L proposed here, including a lack of cover for temporal relations and scoping. However, due to the different application domains (MT vs. tourism-related activities), there are some differences. Since MT systems operate at the sentence-level, each sentence is interpreted in isolation in AMR, whilst our proposed MRL takes context into account. Also, AMR tries to account for all the words in a sentence, whilst our MRL only tries to capture the semantics of those words that are relevant to the application at hand. Other popular semantic parsing corpora include GeoQuery (Zelle, 1995) and Free-917 (Cai and Yates, 2013). Both consist exclusively of questions to be answered with a database query, the former considering a small American geography database and the latter the much wider Freebase database (Bollacker et al., 2008). Unlike SPACEBOOK and ATIS, there is no notion of context in either of these corpora. Furthermore, the NL utterances in these corpora are compiled to be interpreted as database queries, which is equivalent to only one of the dialog acts (set_question)in the SPACEBOOK corpus. Thus the latter allows the exploration of the application of dialog act tagging as a first step in semantic parsin</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale Semantic Parsing via Schema Matching and Lexicon Extension. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<booktitle>Research in Language and Computation,</booktitle>
<pages>3--2</pages>
<contexts>
<context position="4499" citStr="Copestake et al., 2005" startWordPosition="677" endWordPosition="680"> dialog in Fig. 1). The system is aware of the position of the user (through the use of GPS technology) and is designed to be interactive; hence it can initiate the dialog by offering information on nearby points of interest and correcting the route taken by the user if needed. The MRs returned by the semantic parser must represent the user utterances adequately so that the system can generate the appropriate response. The system was developed in the context of the SPACEBOOK project.1 The MRL uses a flat syntax composed of elementary predications, based loosely on minimal recursion semantics (Copestake et al., 2005), but without an explicit treatment of scope. Each MR consists of a dialog act representing the overall function of the utterance, followed for some dialog acts by an unordered set of predicates. All predicates are implicitly conjoined and the names of their arguments specified to improve readability and to allow for some of the arguments to be optional. The argument values can be either constants from the controlled vocabulary, verbatim string extracts from the utterance (enclosed in quotes) or variables (Xno). Negation is denoted by a tilde (˜) in front of predicates. The variables are used </context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard. 2005. Minimal recursion semantics: An introduction. Research in Language and Computation, 3(2–3):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Mark Dredze</author>
</authors>
<title>Adaptive regularization of weight vectors.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 22,</booktitle>
<pages>414--422</pages>
<contexts>
<context position="34065" citStr="Crammer et al., 2009" startWordPosition="5492" endWordPosition="5495">est sets. The former consists of four dialogs from the first scenario and seven from the second, and the latter of three dialogs from each scenario. All development and feature engineering was conducted using cross-validation on the training set, at the dialog level rather than the utterance level (therefore resulting in as many folds as dialogs in the training set), to ensure that each fold contains utterances from all parts of the scenario from which the dialog is taken. To perform cost-sensitive classification learning we used the adaptive regularization of weight vectors (AROW) algorithm (Crammer et al., 2009). AROW is an online algorithm for linear predictors that adjusts the per-feature learning rates so that popular features do not overshadow rare but useful ones. Given the task decomposition, each learned hypothesis consists of 59 classifiers. We restricted the prediction of nodes to content words since function words are unlikely to provide useful alignments. All preprocessing was performed using the Stanford CoreNLP toolkit (Manning et al., 2014). The implementation of the semantic parser is available from http://sites.google.com/ site/andreasvlachos/resources. The DAGGER parameters were set </context>
</contexts>
<marker>Crammer, Kulesza, Dredze, 2009</marker>
<rawString>Koby Crammer, Alex Kulesza, and Mark Dredze. 2009. Adaptive regularization of weight vectors. In Advances in Neural Information Processing Systems 22, pages 414–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah A Dahl</author>
<author>Madeleine Bates</author>
<author>Michael Brown</author>
<author>William Fisher</author>
<author>Kate Hunicke-Smith</author>
<author>David Pallett</author>
<author>Christine Pao</author>
<author>Alexander Rudnicky</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Expanding the scope of the ATIS task: the ATIS-3 corpus.</title>
<date>1994</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology,</booktitle>
<pages>43--48</pages>
<location>Plainsboro, New Jersey.</location>
<contexts>
<context position="1392" citStr="Dahl et al., 1994" startWordPosition="193" endWordPosition="196">on system. We develop a semantic parser for this corpus by adapting the imitation learning algorithm DAGGER without requiring alignment information during training. DAGGER improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignor</context>
<context position="14761" citStr="Dahl et al., 1994" startWordPosition="2262" endWordPosition="2265">ting of 510 utterances. Exact match agreement at the utterance level, which requires that the MRs by the annotators agree on dialog act, predicates and within-utterance variable assignment, was 0.829, which is a strong result given the complexity of the annotation task, and which suggests that the proposed guidelines can be applied consistently. We also assessed the agreement on predicates using F-score, which was 0.914. 4 Comparison to Existing Corpora The most closely related corpus to the one presented in this paper (herein SPACEBOOK) is the airline travel information system (ATIS) corpus (Dahl et al., 1994) which consists of dialogs between a user and a flight booking system collected in Wizard-of-Oz experiments. Each utterance is annotated with the SQL statement that would return the requested piece of information from the flights database. The utterance interpretation is context-dependent. For example, when the user follows up an initial flight request — e.g. “find me flights to Boston” — with utterances containing additional preferences — e.g. “on Monday” — the interpretation of the additional preferences extends the MR for the initial request. Compared to ATIS, the dialogs in the SPACEBOOK c</context>
</contexts>
<marker>Dahl, Bates, Brown, Fisher, Hunicke-Smith, Pallett, Pao, Rudnicky, Shriberg, 1994</marker>
<rawString>Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the ATIS task: the ATIS-3 corpus. In Proceedings of the Workshop on Human Language Technology, pages 43–48, Plainsboro, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<pages>75--297</pages>
<marker>Langford, Marcu, 2009</marker>
<rawString>Hal Daum´e III, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning, 75:297–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
</authors>
<title>Metacost: a general method for making classifiers cost-sensitive.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>155--164</pages>
<publisher>Association for Computing Machinery.</publisher>
<contexts>
<context position="28022" citStr="Domingos, 1999" startWordPosition="4440" endWordPosition="4441">s, ˆy1:t−1) 9 foreach possible action yjt do 10 Predict a,,� yft+1:T = π(s; ˆy1:t−1, Jt ) 11 Assess cjt = `(ˆy1:t−1, yjt, yft+1:T) 12 Add (Φt, ct) to E 13 Learn Hi = CSCL(E) These actions are taken in a greedy fashion, i.e. once an action has been taken it cannot be changed. During training, it converts the problem of learning how to predict these sequences of actions into cost sensitive classification (CSC) learning. In CSC learning each training example has a vector of misclassification costs associated with it, thus rendering some mistakes on some examples to be more expensive than others (Domingos, 1999). Algorithm 1 presents the training procedure. DAGGER requires a set of labeled training instances S and a loss function ` that compares complete outputs for instances in S against the gold standard. In addition, an expert policy π* must be specified which is an oracle that returns the optimal action for the instances in S, akin to an expert demonstrating the task. π* is typically derived from the gold standard; e.g. in part of speech tagging π* would return the correct tag for each token. In addition, the learning rate β and a CSC learner (CSCL) must be provided. The algorithm outputs a learn</context>
</contexts>
<marker>Domingos, 1999</marker>
<rawString>Pedro Domingos. 1999. Metacost: a general method for making classifiers cost-sensitive. In Proceedings of the 5th International Conference on Knowledge Discovery and Data Mining, pages 155–164. Association for Computing Machinery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1426--1436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="36964" citStr="Flanigan et al. (2014)" startWordPosition="5946" endWordPosition="5949">50.1 50.1 Table 2: Performances using 11-fold cross-validation on the training set. points in F-score, but also argument prediction by 2.5 points, thus demonstrating the benefits of learning the alignments together with the other components of the semantic parser. The overall performance improved by 5.5 points in F-score. Finally, we ran an experiment with oracle node prediction and found that the overall performance using cross-validation on the training data improved to 88.2 and 79.9 points in F-score for the Imit+align Indep+align systems. This is in agreement with the results presented by Flanigan et al. (2014) on developing a semantic parsing parser for the AMR formalism who also argue that node prediction is the main performance bottleneck. Tbl. 3 gives results on the test set. The overall performance for Imit is 48.4 F-score and 47.9% for exact match. As in the cross-validation results on the training data, training with imitation learning improved upon independently trained classifiers. The performance was improved further using the alignment dictionary, reaching 53.5 points in F-score and 49.1% exact match accuracy. In the experimental setup above, dialogs from the same scenarios appear in both</context>
<context position="38956" citStr="Flanigan et al. (2014)" startWordPosition="6270" endWordPosition="6273">testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe </context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1426–1436, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="40739" citStr="Goldberg and Nivre (2013)" startWordPosition="6563" endWordPosition="6566">.1% dialog act (accuracy) 77% 80.5% 79.8% 79.5% nodes (Rec/Prec/F) 68.7 45.7 54.8 75.5 51.7 61.4 41.9 61.1 49.7 54 64.9 58.9 arguments (Rec/Prec/F) 73.9 73.7 73.8 76.8 77.3 77.1 69.5 61.3 65.1 77.3 63.6 69.8 focus (Rec/Prec/F) 87.1 80.7 83.8 86 81.2 83.6 81.6 73.4 77.3 90.6 76.8 83.1 overall (Rec/Prec/F) 56.6 42.3 48.4 63.5 46.2 53.5 41.2 47.8 44.3 50 47.4 48.7 Table 3: Performances on the test set. performance was poor. The main reason for this is that, unlike GeoQuery, the proposed MRL does not align well with English. The expert policy in DAGGER is a generalization of the dynamic oracle of Goldberg and Nivre (2013) for shift-reduce dependency parsing to any structured prediction task decomposed into a sequence of actions. The randomized expert policy proposed extends DAGGER to learn not only how to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing</context>
</contexts>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. Transactions of the Association for Computational Linguistics, 3(1):403–414, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>He He</author>
<author>Hal Daum´e</author>
<author>Jason Eisner</author>
</authors>
<title>Dynamic feature selection for dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1455--1464</pages>
<location>Seattle,</location>
<marker>He, Daum´e, Eisner, 2013</marker>
<rawString>He He, Hal Daum´e III, and Jason Eisner. 2013. Dynamic feature selection for dependency parsing. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, Seattle, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Jason Williams</author>
</authors>
<title>The Third Dialog State Tracking Challenge.</title>
<date>2014</date>
<booktitle>In Proceedings of IEEE Spoken Language Technology.</booktitle>
<contexts>
<context position="15975" citStr="Henderson et al., 2014" startWordPosition="2453" endWordPosition="2456">ACEBOOK corpus are substantially longer (8.8 vs. 139.7 utterances on average respectively) and cover a broader domain due to the longer scenarios used in data collection. Furthermore, allowing the wizards to answer in natural language instead of restricting them to responding via database queries as in ATIS led to more varied dialogs. Finally, our approach to annotating coreference avoids repeating the MR of previous utterances, thus resulting in shorter expressions that are closer to the semantics of the NL 550 utterances. The datasets developed in the recent dialog state tracking challenge (Henderson et al., 2014) also consist of dialogs between a user and a tourism information system. However the task is easier since only three entity types are considered (restaurant, coffeeshop and pub), a slot-filling MRL is used and the argument slots take values from fixed lists. The abstract meaning representation (AMR) described by Banarescu et al. (2013) was developed to provide a semantic interpretation layer to improve machine translation (MT) systems. It has similar predicate argument structure to the MRL proposed here, including a lack of cover for temporal relations and scoping. However, due to the differe</context>
</contexts>
<marker>Henderson, Thomson, Williams, 2014</marker>
<rawString>Matthew Henderson, Blaise Thomson, and Jason Williams. 2014. The Third Dialog State Tracking Challenge. In Proceedings of IEEE Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Hill</author>
<author>Jana G¨otze</author>
<author>Bonnie Webber</author>
</authors>
<title>SpaceBook Project: Final Data Release, Wizard-ofOz (WoZ) experiments.</title>
<date>2013</date>
<tech>Technical report,</tech>
<institution>University of Edinburgh.</institution>
<marker>Hill, G¨otze, Webber, 2013</marker>
<rawString>Robin Hill, Jana G¨otze, and Bonnie Webber. 2013. SpaceBook Project: Final Data Release, Wizard-ofOz (WoZ) experiments. Technical report, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
<author>Phil Bartie</author>
</authors>
<title>Tiphaine Dalmas,</title>
<date>2013</date>
<location>Anna Dickinson, Xingkun Liu, William</location>
<contexts>
<context position="3732" citStr="Janarthanam et al. (2013)" startWordPosition="544" endWordPosition="547">n during training by developing a randomized expert policy. Our approach improves upon independently trained classifiers by 9.0 and 4.8 F-score on the development and test sets. 2 Meaning Representation Language Our proposed MR language (MRL) was designed in the context of the portable, interactive naviga547 Transactions of the Association for Computational Linguistics, vol. 2, pp. 547–559, 2014. Action Editor: Sharon Goldwater, Alexander Koller. Submission batch: 3/2014; Revision batch 8/2014; Published 12/2014. c�2014 Association for Computational Linguistics. tion and exploration system of Janarthanam et al. (2013), through which users can obtain information about places and objects of interest, such as monuments and restaurants, as well as directions (see dialog in Fig. 1). The system is aware of the position of the user (through the use of GPS technology) and is designed to be interactive; hence it can initiate the dialog by offering information on nearby points of interest and correcting the route taken by the user if needed. The MRs returned by the semantic parser must represent the user utterances adequately so that the system can generate the appropriate response. The system was developed in the c</context>
<context position="5434" citStr="Janarthanam et al., 2013" startWordPosition="834" endWordPosition="837">llow for some of the arguments to be optional. The argument values can be either constants from the controlled vocabulary, verbatim string extracts from the utterance (enclosed in quotes) or variables (Xno). Negation is denoted by a tilde (˜) in front of predicates. The variables are used to bind together the arguments of different predicates within an utterance, as well as to denote coreference across utterances. The goals in designing the MRL were to remain close to existing semantic formalisms, whilst at the same time producing an MRL that is particularly suited to the application at hand (Janarthanam et al., 2013). We also wanted an MRL that could be computed with efficiently and accurately, given the nature of the NL input. Hence we developed an MRL that is able to express the relevant semantics for the majority of the utterances in our data, without moving to the full expressive power of, e.g., DRT. Dialog acts The dialog acts are utterance-level labels which capture the overall function of the utterance in the dialog, for example whether an utterance is a question seeking a list as an answer, a statement of information, an acknowledgement, an instruction 1www.spacebook-project.eu Figure 1: Sample di</context>
</contexts>
<marker>Janarthanam, Lemon, Bartie, 2013</marker>
<rawString>Srinivasan Janarthanam, Oliver Lemon, Phil Bartie, Tiphaine Dalmas, Anna Dickinson, Xingkun Liu, William Mackaness, and Bonnie Webber. 2013. Evaluating a city exploration dialogue system with integrated question-answering and pedestrian navigation.</rawString>
</citation>
<citation valid="true">
<title>Association for Computational Linguistics.</title>
<date></date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1660--1668</pages>
<location>Sofia, Bulgaria,</location>
<marker></marker>
<rawString>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1660–1668, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Keeley Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Semantic parsing with Bayesian tree transducers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>488--496</pages>
<contexts>
<context position="38917" citStr="Jones et al., 2012" startWordPosition="6264" endWordPosition="6267">ogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new sema</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Bevan Keeley Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with Bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic. Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="9070" citStr="Kamp and Reyle, 1993" startWordPosition="1333" endWordPosition="1336">g user actions, such as walk and turn, with arguments such as direction and along_location. • Predicates describing geographic relations, such as distance, route and position, using ref to denote relative positioning. • Predicates denoting whether an entity is introduced using a definite article (def), an indefinite (indef) or an indexical (index). • Predicates expressing numerical relations such as argmin and argmax. Coreference In order to model coreference we adopt the notion of discourse referents (DRs) and discourse entities (DEs) from Discourse Representation Theory (DRT) (Webber, 1978; Kamp and Reyle, 1993). DRs are referential expressions appearing in utterances which denote DEs, which are mental entities in the speaker’s model of discourse. Multiple DEs can refer to the same real-world entity; for example, in Fig. 1 “vapiano’s” refers to a different DE from the restaurant in the previous sentence (“the nearest italian”), even though they are likely to be the same real-world entity. We considered DEs instead of actual entities in the MRL because they allow us to capture the semantics of interactions such as the last exchange between the wizard and user. The MRL represents multiple DEs referring</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic. Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Keizer</author>
<author>Stphane Rossignol</author>
<author>Senthilkumar Chandramohan</author>
<author>Olivier Pietquin</author>
</authors>
<title>User simulation in the development of statistical spoken dialogue systems.</title>
<date>2012</date>
<booktitle>In Oliver Lemon and Olivier Pietquin, editors, Data-Driven Methods for Adaptive Spoken Dialogue Systems,</booktitle>
<pages>39--73</pages>
<publisher>Springer</publisher>
<location>New York.</location>
<contexts>
<context position="41910" citStr="Keizer et al., 2012" startWordPosition="6751" endWordPosition="6754">to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). However, in that work utterances were selected to be shorter than 6 words and to include one noun phrase present in the lexicon used during learning while ignoring short but common phrases such as “yes” and “no”; thus it is unclear whether it would be applicable to our dataset. Finally, dialog context is only taken into account in predicting the dialog act for eac</context>
</contexts>
<marker>Keizer, Rossignol, Chandramohan, Pietquin, 2012</marker>
<rawString>Simon Keizer, Stphane Rossignol, Senthilkumar Chandramohan, and Olivier Pietquin. 2012. User simulation in the development of statistical spoken dialogue systems. In Oliver Lemon and Olivier Pietquin, editors, Data-Driven Methods for Adaptive Spoken Dialogue Systems, pages 39–73. Springer New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John F Kelley</author>
</authors>
<title>An empirical methodology for writing user-friendly natural language computer applications.</title>
<date>1983</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>193--196</pages>
<contexts>
<context position="10126" citStr="Kelley, 1983" startWordPosition="1507" endWordPosition="1508">cause they allow us to capture the semantics of interactions such as the last exchange between the wizard and user. The MRL represents multiple DEs referring to the same real-world entity through the predicate equivalent. Coreference is indicated by using identical variables across predicate arguments within an utterance or across utterances. The main principle in determining whether DRs corefer is that it must be possible to infer this from the dialog context alone, without using world knowledge. 3 Data Collection and Annotation The NL utterances were collected using Wizard-ofOz experiments (Kelley, 1983) with pairs of human subjects. In each experiment, one human pretended to be a tourist visiting Edinburgh (by physically walking around the city), while the other performed the role of the system responding through a suitable interface using a text-to-speech system. Each user-wizard pair was given one of two scenarios involving requests for directions to different points of interest. The first scenario involves seeking directions to the national museum of Scotland, then going to a nearby coffee shop, followed by a pub via a cash machine and finally looking for a park. The second scenario invol</context>
</contexts>
<marker>Kelley, 1983</marker>
<rawString>John F. Kelley. 1983. An empirical methodology for writing user-friendly natural language computer applications. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 193– 196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1512--1523</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="38616" citStr="Kwiatkowski et al. (2011)" startWordPosition="6212" endWordPosition="6215">n testing on the dialogs from the first scenario and training on the dialogs from the second, the overall performance using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1512–1523, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1545--1556</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="41521" citStr="Kwiatkowski et al., 2013" startWordPosition="6688" endWordPosition="6691">GGER to learn not only how to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus </context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>590--599</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="39331" citStr="Liang et al., 2011" startWordPosition="6328" endWordPosition="6331">nd Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe it is non-trivial to apply existing approaches to the new task, since, assuming a decomposition similar to that of Sec. 5.1, exhaustive search would be too expensive, and applying vanilla beam search would be difficult since different predictions result in beams of (sometimes radically) different lengths that are not comparable. We have attempted applying the MT-based sema</context>
<context position="41495" citStr="Liang et al., 2011" startWordPosition="6684" endWordPosition="6687"> proposed extends DAGGER to learn not only how to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the </context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 590–599, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt MacMahon</author>
<author>Brian Stankiewicz</author>
<author>Benjamin Kuipers</author>
</authors>
<title>Walk the talk: connecting language, knowledge, and action in route instructions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>1475--1482</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="17723" citStr="MacMahon et al. (2006)" startWordPosition="2734" endWordPosition="2737">st exclusively of questions to be answered with a database query, the former considering a small American geography database and the latter the much wider Freebase database (Bollacker et al., 2008). Unlike SPACEBOOK and ATIS, there is no notion of context in either of these corpora. Furthermore, the NL utterances in these corpora are compiled to be interpreted as database queries, which is equivalent to only one of the dialog acts (set_question)in the SPACEBOOK corpus. Thus the latter allows the exploration of the application of dialog act tagging as a first step in semantic parsing. Finally, MacMahon et al. (2006) developed a corpus of natural language instructions paired with sequences of actions; however the domain is limited to simple navigation instructions and there is no notion of dialog in this corpus. 5 Semantic Parsing for the New Corpus The MRL in Fig. 1 is readable and easy to annotate with. However, it is not ideal for experiments, as it is difficult to compare MR expressions beyond exact match. For these reasons, we converted the MR expressions into a node-argument form. In particular, all predicates introducing entities (isA) and most predicates introducing relations among entities (e.g. </context>
</contexts>
<marker>MacMahon, Stankiewicz, Kuipers, 2006</marker>
<rawString>Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. 2006. Walk the talk: connecting language, knowledge, and action in route instructions. In Proceedings of the 21st National Conference on Artificial Intelligence, pages 1475–1482. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="34516" citStr="Manning et al., 2014" startWordPosition="5561" endWordPosition="5564">which the dialog is taken. To perform cost-sensitive classification learning we used the adaptive regularization of weight vectors (AROW) algorithm (Crammer et al., 2009). AROW is an online algorithm for linear predictors that adjusts the per-feature learning rates so that popular features do not overshadow rare but useful ones. Given the task decomposition, each learned hypothesis consists of 59 classifiers. We restricted the prediction of nodes to content words since function words are unlikely to provide useful alignments. All preprocessing was performed using the Stanford CoreNLP toolkit (Manning et al., 2014). The implementation of the semantic parser is available from http://sites.google.com/ site/andreasvlachos/resources. The DAGGER parameters were set to 12 training iterations, 0 = 0.3 and 3 samples for action cost assessment. We compared our DAGGER-based imitation learning approach (henceforth Imit) against independently trained classifiers using the same classification learner and features (henceforth Indep). For both systems we incorporated an alignment dictionary (+align versions) as described in Sec. 6.2, in order to improve node prediction performance. The dictionary was extracted from th</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="38725" citStr="Och and Ney, 2000" startWordPosition="6232" endWordPosition="6235">e using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al.,</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Ross</author>
<author>Geoffrey J Gordon</author>
<author>Drew Bagnell</author>
</authors>
<title>A reduction of imitation learning and structured prediction to no-regret online learning.</title>
<date>2011</date>
<booktitle>In 14th International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>627--635</pages>
<contexts>
<context position="2869" citStr="Ross et al., 2011" startWordPosition="420" endWordPosition="423"> such as coreference and can accommodate utterances that are not interpretable according to a database, e.g. repetition requests. The utterances were collected in experiments with human subjects, and contain phenomena such as ellipsis and disfluency. We developed guidelines and annotated 17 dialogs containing 2,374 utterances, with 82.9% exact match agreement between two annotators. We also develop a semantic parser for this corpus. As the output MRs are rather complex, instead of adopting an approach that searches the output space exhaustively, we use the imitation learning algorithm DAGGER (Ross et al., 2011) that converts learning a structured prediction model into learning a set of classification models. We take advantage of its ability to learn with non-decomposable loss functions and extend it to handle the absence of alignment information during training by developing a randomized expert policy. Our approach improves upon independently trained classifiers by 9.0 and 4.8 F-score on the development and test sets. 2 Meaning Representation Language Our proposed MR language (MRL) was designed in the context of the portable, interactive naviga547 Transactions of the Association for Computational Li</context>
<context position="26348" citStr="Ross et al., 2011" startWordPosition="4137" endWordPosition="4140">earning In order to learn the classifiers for the task decomposition described, two challenges must be addressed. The first is the complexity of the structure to be predicted. The task involves many interdependent predictions made by a variety of classifiers, and thus cannot be tackled by approaches that assume a particular type of graph structure, or restrict structure feature extraction in order to perform efficient dynamic programming. The second challenge is the lack of alignment information during training. Imitation learning algorithms such as SEARN (Daum´e III et al., 2009) and DAGGER (Ross et al., 2011) have been applied successfully to a variety of structured prediction tasks including summarization, biomedical event extraction and dynamic feature selection (Daum´e III et al., 2009; Vlachos, 2012; He et al., 2013) thanks to their ability to handle complex output spaces without exhaustive search and their flexibility in incorporating features based on the structured output. In this work we focus on DAGGER and extend it to handle the missing alignments. 6.1 Structured prediction with DAGGER The dataset aggregation (DAGGER) algorithm (Ross et al., 2011) forms the prediction of an instance s as</context>
</contexts>
<marker>Ross, Gordon, Bagnell, 2011</marker>
<rawString>St´ephane Ross, Geoffrey J. Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In 14th International Conference on Artificial Intelligence and Statistics, pages 627–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykema</author>
<author>Marie Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational linguistics,</journal>
<pages>26--3</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational linguistics, 26(3):339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Dilek Hakkani-T¨ur</author>
<author>Larry Heck</author>
</authors>
<title>What’s left to be understood in ATIS?</title>
<date>2010</date>
<booktitle>In IEEE Workshop on Spoken Language Technologies.</booktitle>
<marker>Tur, Hakkani-T¨ur, Heck, 2010</marker>
<rawString>Gokhan Tur, Dilek Hakkani-T¨ur, and Larry Heck. 2010. What’s left to be understood in ATIS? In IEEE Workshop on Spoken Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
</authors>
<title>An investigation of imitation learning algorithms for structured prediction.</title>
<date>2012</date>
<booktitle>Journal of Machine Learning Research Workshop and Conference Proceedings, Proceedings of the 10th European Workshop on Reinforcement Learning,</booktitle>
<pages>24--143</pages>
<contexts>
<context position="26546" citStr="Vlachos, 2012" startWordPosition="4168" endWordPosition="4169">rdependent predictions made by a variety of classifiers, and thus cannot be tackled by approaches that assume a particular type of graph structure, or restrict structure feature extraction in order to perform efficient dynamic programming. The second challenge is the lack of alignment information during training. Imitation learning algorithms such as SEARN (Daum´e III et al., 2009) and DAGGER (Ross et al., 2011) have been applied successfully to a variety of structured prediction tasks including summarization, biomedical event extraction and dynamic feature selection (Daum´e III et al., 2009; Vlachos, 2012; He et al., 2013) thanks to their ability to handle complex output spaces without exhaustive search and their flexibility in incorporating features based on the structured output. In this work we focus on DAGGER and extend it to handle the missing alignments. 6.1 Structured prediction with DAGGER The dataset aggregation (DAGGER) algorithm (Ross et al., 2011) forms the prediction of an instance s as a sequence of T actions ˆy1:T predicted by a learned policy which consists of one or more classifiers. 553 Algorithm 1: Imitation learning with DAGGER Input: training instances S, expert policy π*,</context>
</contexts>
<marker>Vlachos, 2012</marker>
<rawString>Andreas Vlachos. 2012. An investigation of imitation learning algorithms for structured prediction. Journal of Machine Learning Research Workshop and Conference Proceedings, Proceedings of the 10th European Workshop on Reinforcement Learning, 24:143–154.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marilyn A Walker</author>
<author>Alexander I Rudnicky</author>
<author>Rashmi Prasad</author>
<author>John S Aberdeen</author>
<author>Elizabeth Owen Bratt</author>
<author>John S Garofolo</author>
<author>Helen Wright Hastie</author>
<author>Audrey N Le</author>
<author>Bryan L Pellom</author>
<author>Alexandros Potamianos</author>
<author>Rebecca J Passonneau</author>
<author>Salim Roukos</author>
<author>Gregory A Sanders</author>
<author>Stephanie Seneff</author>
<author>David Stallard</author>
</authors>
<title>DARPA communicator: cross-system results for the 2001 evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="42142" citStr="Walker et al., 2002" startWordPosition="6792" endWordPosition="6795"> Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). However, in that work utterances were selected to be shorter than 6 words and to include one noun phrase present in the lexicon used during learning while ignoring short but common phrases such as “yes” and “no”; thus it is unclear whether it would be applicable to our dataset. Finally, dialog context is only taken into account in predicting the dialog act for each utterance. Even though our corpus contains coreference information, we did not attempt this task as it is difficult to evaluate and our performance on node prediction on which it relies is relatively low. We leave coreference reso</context>
</contexts>
<marker>Walker, Rudnicky, Prasad, Aberdeen, Bratt, Garofolo, Hastie, Le, Pellom, Potamianos, Passonneau, Roukos, Sanders, Seneff, Stallard, 2002</marker>
<rawString>Marilyn A. Walker, Alexander I. Rudnicky, Rashmi Prasad, John S. Aberdeen, Elizabeth Owen Bratt, John S. Garofolo, Helen Wright Hastie, Audrey N. Le, Bryan L. Pellom, Alexandros Potamianos, Rebecca J. Passonneau, Salim Roukos, Gregory A. Sanders, Stephanie Seneff, and David Stallard. 2002. DARPA communicator: cross-system results for the 2001 evaluation. In Proceedings of the 7th International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Lynn Webber</author>
</authors>
<title>A Formal Approach to Discourse Anaphora.</title>
<date>1978</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="9047" citStr="Webber, 1978" startWordPosition="1331" endWordPosition="1332">ates describing user actions, such as walk and turn, with arguments such as direction and along_location. • Predicates describing geographic relations, such as distance, route and position, using ref to denote relative positioning. • Predicates denoting whether an entity is introduced using a definite article (def), an indefinite (indef) or an indexical (index). • Predicates expressing numerical relations such as argmin and argmax. Coreference In order to model coreference we adopt the notion of discourse referents (DRs) and discourse entities (DEs) from Discourse Representation Theory (DRT) (Webber, 1978; Kamp and Reyle, 1993). DRs are referential expressions appearing in utterances which denote DEs, which are mental entities in the speaker’s model of discourse. Multiple DEs can refer to the same real-world entity; for example, in Fig. 1 “vapiano’s” refers to a different DE from the restaurant in the previous sentence (“the nearest italian”), even though they are likely to be the same real-world entity. We considered DEs instead of actual entities in the MRL because they allow us to capture the semantics of interactions such as the last exchange between the wizard and user. The MRL represents</context>
</contexts>
<marker>Webber, 1978</marker>
<rawString>Bonnie Lynn Webber. 1978. A Formal Approach to Discourse Anaphora. Ph.D. thesis, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
</authors>
<title>Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Sciences, The University of Texas at Austin.</institution>
<contexts>
<context position="1419" citStr="Zelle, 1995" startWordPosition="199" endWordPosition="200">parser for this corpus by adapting the imitation learning algorithm DAGGER without requiring alignment information during training. DAGGER improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignored. In this paper we presen</context>
<context position="11640" citStr="Zelle (1995)" startWordPosition="1761" endWordPosition="1762">ranscribed from recorded audio files. 17 dialogs were collected in total, 7 from the first scenario and 10 from the second. More details are reported in Hill et al. (2013). Given the varied nature of the dialogs, some of the user requests were not within the scope of the system. Furthermore, the proposed MRL has its own limitations; for example it does not have predicates to express temporal relationships. Thus, it was necessary to filter the utterances collected and decide which ones to annotate with MRs.2 In particular, we 2A similar filtering process was used for GeoQuery (Section 7.5.1 in Zelle (1995)) and ATIS (principles of interpretation document (/atis3/doc/pofi.doc) in the NIST CDs). 549 vocabulary type number of terms dialog acts 15 predicates 19 arguments 41 constants 9 entity types 26 properties 4 Table 1: MRL vocabulary used in the annotation did not annotate utterances falling into one or more of the following categories: • Utterances that are not human-interpretable, e.g. utterances that were interrupted too early to be interpretable. In such cases, the system is likely to respond with a repetition request. • Utterances that are human-interpretable but outside the scope of the s</context>
<context position="17054" citStr="Zelle, 1995" startWordPosition="2628" endWordPosition="2629">rgument structure to the MRL proposed here, including a lack of cover for temporal relations and scoping. However, due to the different application domains (MT vs. tourism-related activities), there are some differences. Since MT systems operate at the sentence-level, each sentence is interpreted in isolation in AMR, whilst our proposed MRL takes context into account. Also, AMR tries to account for all the words in a sentence, whilst our MRL only tries to capture the semantics of those words that are relevant to the application at hand. Other popular semantic parsing corpora include GeoQuery (Zelle, 1995) and Free-917 (Cai and Yates, 2013). Both consist exclusively of questions to be answered with a database query, the former considering a small American geography database and the latter the much wider Freebase database (Bollacker et al., 2008). Unlike SPACEBOOK and ATIS, there is no notion of context in either of these corpora. Furthermore, the NL utterances in these corpora are compiled to be interpreted as database queries, which is equivalent to only one of the dialog acts (set_question)in the SPACEBOOK corpus. Thus the latter allows the exploration of the application of dialog act tagging</context>
</contexts>
<marker>Zelle, 1995</marker>
<rawString>John M. Zelle. 1995. Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. Ph.D. thesis, Department of Computer Sciences, The University of Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>678--687</pages>
<contexts>
<context position="39296" citStr="Zettlemoyer and Collins, 2007" startWordPosition="6322" endWordPosition="6325">tionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe it is non-trivial to apply existing approaches to the new task, since, assuming a decomposition similar to that of Sec. 5.1, exhaustive search would be too expensive, and applying vanilla beam search would be difficult since different predictions result in beams of (sometimes radically) different lengths that are not comparable. We have a</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 678–687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,</booktitle>
<pages>976--984</pages>
<contexts>
<context position="38535" citStr="Zettlemoyer and Collins (2009)" startWordPosition="6199" endWordPosition="6202">parser evaluated on the other (still respecting the train/test split from before). When testing on the dialogs from the first scenario and training on the dialogs from the second, the overall performance using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic pa</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pages 976–984, Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
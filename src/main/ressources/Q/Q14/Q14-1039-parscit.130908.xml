<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000123">
<title confidence="0.9967655">
Joint Modeling of Opinion Expression Extraction
and Attribute Classification
</title>
<author confidence="0.99723">
Bishan Yang
</author>
<affiliation confidence="0.9970235">
Department of Computer Science
Cornell University
</affiliation>
<email confidence="0.992393">
bishan@cs.cornell.edu
</email>
<author confidence="0.985992">
Claire Cardie
</author>
<affiliation confidence="0.996976">
Department of Computer Science
Cornell University
</affiliation>
<email confidence="0.996464">
cardie@cs.cornell.edu
</email>
<sectionHeader confidence="0.998514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.991021235294118">
In this paper, we study the problems of opin-
ion expression extraction and expression-level
polarity and intensity classification. Tradi-
tional fine-grained opinion analysis systems
address these problems in isolation and thus
cannot capture interactions among the tex-
tual spans of opinion expressions and their
opinion-related properties. We present two
types of joint approaches that can account for
such interactions during 1) both learning and
inference or 2) only during inference. Exten-
sive experiments on a standard dataset demon-
strate that our approaches provide substantial
improvements over previously published re-
sults. By analyzing the results, we gain some
insight into the advantages of different joint
models.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999900979166667">
Automatic extraction of opinions from text has at-
tracted considerable attention in recent years. In
particular, significant research has focused on ex-
tracting detailed information for opinions at the fine-
grained level, e.g. identifying opinion expressions
within a sentence and predicting phrase-level po-
larity and intensity. The ability to extract fine-
grained opinion information is crucial in supporting
many opinion-mining applications such as opinion
summarization, opinion-oriented question answer-
ing and opinion retrieval.
In this paper, we focus on the problem of identi-
fying opinion expressions and classifying their at-
tributes. We consider as an opinion expression
any subjective expression that explicitly or implic-
itly conveys emotions, sentiment, beliefs, opinions
(i.e. private states) (Wiebe et al., 2005), and con-
sider two key attributes — polarity and intensity —
for characterizing the opinions. Consider the sen-
tence in Figure 1, for example. The phrases “a bias
in favor of” and “being severely criticized” are opin-
ion expressions containing positive sentiment with
medium intensity and negative sentiment with high
intensity, respectively.
Most existing approaches tackle the tasks of opin-
ion expression extraction and attribute classification
in isolation. The first task is typically formulated as
a sequence labeling problem, where the goal is to la-
bel the boundaries of text spans that correspond to
opinion expressions (Breck et al., 2007; Yang and
Cardie, 2012). The second task is usually treated as
a binary or multi-class classification problem (Wil-
son et al., 2005; Choi and Cardie, 2008; Yessenalina
and Cardie, 2011), where the goal is to assign a
class label to a text fragment (e.g. a phrase or a sen-
tence). Solutions to the two tasks can be applied in a
pipeline architecture to extract opinion expressions
and their attributes. However, pipeline systems suf-
fer from error propagation: opinion expression er-
rors propagate and lead to unrecoverable errors in
attribute classification.
Limited work has been done on the joint modeling
of opinion expression extraction and attribute clas-
sification. Choi and Cardie (2010) first proposed
a joint sequence labeling approach to extract opin-
ion expressions and label them with polarity and in-
tensity. Their approach treats both expression ex-
traction and attribute classification as token-level se-
</bodyText>
<page confidence="0.98446">
505
</page>
<bodyText confidence="0.465206">
Transactions of the Association for Computational Linguistics, vol. 2, pp. 505–516, 2014. Action Editor: Janyce Wiebe.
Submitted 4/2014; Revised 8/2014; Published November 1, 2014. c�2014 Association for Computational Linguistics.
He demonstrated a bias in favor of medium the rebels despite being severely criticized high.
</bodyText>
<figureCaption confidence="0.979019666666667">
Figure 1: An example sentence annotated with opinion expressions and their polarity and intensity. We use colored
boxes to mark the textual spans of opinion expressions where green (red) denotes positive (negative) polarity, and use
subscripts to denote intensity.
</figureCaption>
<bodyText confidence="0.999989261904762">
quence labeling tasks, and thus cannot model the
label distribution over expressions even though the
annotations are given at the expression level. Jo-
hansson and Moschitti (2011) considered a pipeline
of opinion extraction followed by polarity classifica-
tion and propose re-ranking its k-best outputs using
global features. One key issue, however, is that the
approach enumerates the k-best output in a pipeline
manner and thus they do not necessarily correspond
to the k-best global decisions. Moreover, as the
number of opinion attributes grows, it is not clear
how to identify the best k for each attribute.
In contrast to existing approaches, we formu-
late opinion expression extraction as a segmenta-
tion problem and attribute classification as segment-
level attribute labeling. To capture their interac-
tions, we present two types of joint approaches: (1)
joint learning approaches, which combine opinion
segment detection and attribute labeling into a sin-
gle probabilistic model, and estimate parameters for
this joint model; and (2) joint inference approaches,
which build separate models for opinion segment
detection and attribute labeling at training time, and
jointly apply these (via a single objective function)
only at test time to identify the best “combined” de-
cision of the two models.
To investigate the effectiveness of our approaches,
we conducted extensive experiments on a standard
corpus for fine-grained opinion analysis (the MPQA
corpus (Wiebe et al., 2005)). We found that
all of our proposed approaches provide substan-
tial improvements over the previously published re-
sults. We also compared our approaches to a strong
pipeline baseline and observed that joint learning re-
sults in a significant boost in precision while joint
inference, with an appropriate objective, can signifi-
cantly boost both precision and recall and obtain the
best overall performance. Error analysis provides
additional understanding of the differences between
the joint learning and joint inference approaches,
and suggests that joint inference can be more effec-
tive and more efficient for the task in practice.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999928925">
Significant research effort has been invested in
the task of fine-grained opinion analysis in recent
years (Wiebe et al., 2005; Wilson et al., 2009). Wil-
son et al. (2005) first motivated and studied phrase-
level polarity classification on an open-domain cor-
pus. Choi and Cardie (2008) developed inference
rules to capture compositional effects at the lexical
level on phrase-level polarity classification. Yesse-
nalina and Cardie (2011) and Socher et al. (2013)
learn continuous-valued phrase representations by
combining the representations of words within an
opinion expression and using them as features for
classifying polarity and intensity. All of these ap-
proaches assume the opinion expressions are avail-
able before training the classifiers. However, in
real-world settings, the spans of opinion expres-
sions within the sentence are not available. In fact,
Choi and Cardie (2008) demonstrated that the per-
formance of expression-level polarity classification
degrades as more surrounding (but irrelevant) con-
text is considered. This motivates the additional task
of identifying the spans of opinion expressions.
Opinion expression extraction has been success-
fully tackled via sequence tagging methods. Breck
et al. (2007) applied conditional random fields to as-
sign each token a label indicating whether it belongs
to an opinion expression or not. Yang and Cardie
(2012) employed a segment-level sequence labeler
based on semi-CRFs with rich phrase-level syntac-
tic features. In this work, we also utilize semi-CRFs
to model opinion expression extraction.
There has been limited work on the joint modeling
of opinion expression extraction and attribute classi-
fication. Choi and Cardie (2010) first developed a
joint sequence labeler that jointly tags opinions, po-
larity and intensity by training CRFs with hierarchi-
cal features (Zhao et al., 2008). One major drawback
of their approach is that it models both opinion ex-
traction and attribute labeling as tasks in token-level
sequence labeling, and thus cannot model their inter-
</bodyText>
<page confidence="0.996781">
506
</page>
<bodyText confidence="0.9999478">
actions at the expression-level. Johansson and Mos-
chitti (2011) and Johansson and Moschitti (2013)
propose a joint approach to opinion expression ex-
traction and polarity classification by re-ranking its
k-best output using global features. One major is-
sue with their approach is that the k-best candidates
were obtained without global reasoning about the
relative uncertainty in the individual stages. As the
number of considered attributes grows, it also be-
comes harder to decide how many predictions to se-
lect from each attribute classifier.
Compared to the existing approaches, our joint
models have the advantage of modeling opinion ex-
pression extraction and attribute classification at the
segment-level, and more importantly, they provide a
principled way of combining the segmentation and
classification components.
Our work follows a long line of joint modeling re-
search that has demonstrated great success for vari-
ous NLP tasks (Roth and Yih, 2004; Punyakanok et
al., 2004; Finkel and Manning, 2010; Rush et al.,
2010; Choi et al., 2006; Yang and Cardie, 2013).
Methods tend to fall into one of two joint mod-
eling frameworks: the first learns a joint model
that captures global dependencies; the other uses
independently-learned models and considers global
dependencies only during inference. In this work,
we study both types of joint approaches for opinion
expression extraction and opinion attribute classifi-
cation.
</bodyText>
<sectionHeader confidence="0.996361" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999902529411765">
In this section, we present our approaches for the
joint modeling of opinion expression extraction and
attribute classification. Specifically, given a sen-
tence, our goal is to identify the spans of opinion
expressions, and simultaneously assign their polar-
ity and intensity. Training data consists of a col-
lection of sentences with manually annotated opin-
ion expression spans, each associated with a polar-
ity label that takes values from {positive, negative,
neutral}, and an intensity label, taking values from
{high, medium, low}.
In the following, we first describe how we model
opinion expression extraction as a segment-level se-
quence labeling problem and model attribute predic-
tion as a classification problem. Then we propose
our joint models for combining opinion segmenta-
tion and attribute classification.
</bodyText>
<subsectionHeader confidence="0.998329">
3.1 Opinion Expression Extraction
</subsectionHeader>
<bodyText confidence="0.999781666666667">
The problem of opinion expression extraction as-
sumes tokenized sentences as input and outputs
the spans of the opinion expressions in each sen-
tence. Previous work has tackled this problem us-
ing token-based sequence labeling methods such as
CRFs (e.g. Breck et al. (2007), Yang and Cardie
(2012)). However, semi-Markov CRFs (Sarawagi
and Cohen, 2004) (henceforth semi-CRF) have been
shown more appropriate for the task than CRFs since
they allow contiguous spans in the input sequence
(e.g. a noun phrase) to be treated as a group rather
than as distinct tokens. Thus, they can easily capture
segment-level information like syntactic constituent
structure (Yang and Cardie, 2012). Therefore we
adopt the semi-CRF model for opinion expression
extraction here.
Given a sentence x, denote an opinion seg-
mentation as ys = h(s0, b0), ..., (sk, bk)i, where
the s0.k are consecutive segments that form a
segmentation of x; each segment si = (ti, ui)
consists of the positions of the start token ti and
an end token ui; and each si is associated with
a binary variable bi ∈ {I, O}, which indicates
whether it is an opinion expression (I) or not
(O). Take the sentence in Figure 1, for exam-
ple. The corresponding opinion segmentation is
ys = h((0, 0), O), ((1, 1), O), ((2, 6), I), ((7, g), O)
, ((9, 9), O), ((10,12), I), ((13,13), O)i, where
each segment corresponds to an opinion expression
or to a phrase unit that does not express any opinion.
Using a semi-Markov CRF, we model the condi-
tional distribution over all possible opinion segmen-
tations given the input x:
</bodyText>
<equation confidence="0.9995285">
P(ys|x) = exp{E|ys |i��θ · f(ysa, ysa−1, x)}
Ey�sEY exp{�|y�s|
i�� θ · f(y�sa, y�sa−1, x)}
(1)
</equation>
<bodyText confidence="0.9998728">
where θ denotes the model parameters, ysa = (si, bi)
and f denotes a feature function that encodes the po-
tentials of the boundaries for opinion segments and
the potentials of transitions between two consecutive
labeled segments.
</bodyText>
<page confidence="0.979913">
507
</page>
<bodyText confidence="0.999985916666667">
Note that the probability is normalized over all
possible opinion segmentations. To reduce the train-
ing complexity, we adopted the method described
in Yang and Cardie (2012), which only normalizes
over segment candidates that are plausible accord-
ing to the parsing structure of the sentence. Figure 2
shows some candidate segmentations generated for
an example sentence. Such a technique results in a
large reduction in training time and was shown to be
effective for identifying opinion expressions.
The standard training objective of a semi-CRF, is
to minimize the log loss
</bodyText>
<equation confidence="0.9482085">
log P(y(i)
s |x(i)) (2)
</equation>
<bodyText confidence="0.999777733333333">
It penalizes any predicted opinion expression whose
boundaries do not exactly align with the boundaries
of the correct opinion expressions using 0-1 loss.
Unfortunately, exact boundary matching is often not
used as an evaluation metric for opinion expres-
sion extraction since it is hard for human annota-
tors to agree on the exact boundaries of opinion ex-
pressions.1 Most previous work used proportional
matching (Johansson and Moschitti, 2013) as it takes
into account the overlapping proportion of the pre-
dicted and the correct opinion expressions to com-
pute precision and recall. To incorporate this eval-
uation metric into training, we use softmax-margin
(Gimpel and Smith, 2010) that replace P(y(i) s|x(i))
in (2) with Pcost(y(i)
</bodyText>
<equation confidence="0.910477">
s |x(i)), which equals
exp{P|ys|
i=1 θ · f(ysi, ysi−1, x)}
Py0s∈Y exp{P|y0s|
i=1 θ · f(y0si, y0si−1, x) + l(y0s, ys)}
</equation>
<bodyText confidence="0.989329">
and we define the loss function l(y0s, ys) as
</bodyText>
<equation confidence="0.7270555">
(✶{b0i =6 bj ∧ bi =6 O}  |sj  |s |sa|
+ ✶{b0i =6 bj ∧ bj =6 O} |sj |j|si |)
</equation>
<bodyText confidence="0.990728666666667">
which is the sum of the precision and recall errors of
segment labeling using proportional matching. The
loss-augmented probability is only computed during
</bodyText>
<footnote confidence="0.8795825">
1The inter-annotator agreement on boundaries of opinion
expressions is not stressed in MPQA (Wiebe et al., 2005).
</footnote>
<bodyText confidence="0.975897">
We hope to eradicate the eternal scourge of corruption .
</bodyText>
<equation confidence="0.99872075">
[ ][ ][ ] [ ][ ][ ][ ][ ]
[ ][ ][ ] [ ][ ][ ][ ]
[ ][ ][ ] [ ][ ][ ]
[ ][ ][ ] [ ][ ]
</equation>
<figureCaption confidence="0.99655">
Figure 2: Examples of Segmentation Candidates
</figureCaption>
<bodyText confidence="0.9933548">
training. The more the proposed labeled segmenta-
tion overlaps with the true labeled segmentation for
x, the less it will be penalized.
During inference, we can obtain the best labeled
segmentation by solving
</bodyText>
<equation confidence="0.961219166666667">
argmaxP(ys|x) = argmax X |ys |θ · f(ysi,ysi−1, x)
ys ys i=1
This can be done efficiently via dynamic program-
ming:
V (t) = argmax G(y, y0)+V (u−1) (3)
s=(u,t)∈s:t,y=(s,b),y0
</equation>
<bodyText confidence="0.99998425">
where s:t denotes all candidate segments ending at
position t and G(y, y0) = θ·f(y, y0, x). The optimal
ys∗ can be obtained by computing V (n), where n is
the length of the sentence.
</bodyText>
<subsectionHeader confidence="0.999548">
3.2 Opinion Attribute Classification
</subsectionHeader>
<bodyText confidence="0.999931">
We consider two types of opinion attributes: polar-
ity and intensity. For each attribute, we model the
multinomial distribution of an attribute class given
a text segment Denoting the class variable for each
attribute as aj, we have
</bodyText>
<equation confidence="0.9994315">
P(aj|xs) =exp{φj · gj(aj, xs)}
P a0∈Aj exp{φj · gj(a0, xs)} (4)
</equation>
<bodyText confidence="0.999993857142857">
where xs denotes a text segment, φj is a pa-
rameter vector and gj denotes feature functions
for attribute aj. The label space for polarity
classification is {positive, negative, neutral, 0}
and the label space for intensity classification is
{high, medium, low, 0}. We include an empty
value 0 to denote assigning no attribute value to
those text segments that are not opinion expressions.
In the following description of our joint mod-
els, we omit the superscript on the attribute variable
and derive our models with one single opinion at-
tribute for simplicity. The derivations can be carried
through with more than one opinion attribute by as-
suming the independence of different attributes.
</bodyText>
<equation confidence="0.9963544">
L(θ) = arg min
θ
−
XN
i=1
X |y0s|
i=1
ys|
X
j=1
</equation>
<page confidence="0.989488">
508
</page>
<subsectionHeader confidence="0.997721">
3.3 The Joint Models
</subsectionHeader>
<bodyText confidence="0.999995428571429">
We propose two types of joint models for opinion
segmentation and attribute classification: (1) joint
learning models, which train a single sequence la-
beling model that maximizes a joint probability dis-
tribution over segmentation and attribute labeling,
and infers the most probable labeled segmentations
according to the joint probability; and (2) joint infer-
ence models, which train a sequence labeling model
for opinion segmentation and separately train classi-
fication models for attribute labeling, and combine
the segmentation and classification models during
inference to make global decisions. In the follow-
ing, we first present the joint learning models and
then introduce the joint inference models.
</bodyText>
<subsectionHeader confidence="0.806401">
3.3.1 Joint Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.9999905625">
We can formulate joint opinion segmentation and
classification as a sequence labeling problem on the
label space Y = {y|y = h(s0, ˜b0), ..., (sk, ˜bk)i}
where ˜bi = (bi, ai) ∈ {I, O} × A, where bi is
a binary variable as described before and ai is an
attribute class variable associated with segment si.
Since only opinion expressions should be assigned
opinion attributes, we consider the following label-
ing constraints: ai = ∅ if and only if bi = O.
We can apply the same training and inference pro-
cedure described in Section 3.1 by replacing the la-
bel space y$ with the joint label space y. Note that
the feature functions are shared over the joint label
space. For the loss function in the loss-augmented
objective, the opinion segment label b is also re-
placed with the augmented label ˜b.
</bodyText>
<subsectionHeader confidence="0.876017">
3.3.2 Hierarchical Joint Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.999921105263158">
The above joint sequence labeling model does not
explicitly model the dependencies between opinion
segmentation and attribute labeling. The two sub-
tasks share the same set of features and parameters.
In the following, we introduce an alternative ap-
proach that explicitly models the conditional depen-
dency between opinion segmentation and attribute
labeling, and allows segmentation- and attribute-
specific parameters to be jointly learned in one sin-
gle model.
Note that the joint label space naturally forms
a hierarchical structure: the probability of choos-
ing a sequence label y can be interpreted as the
probability of first choosing an opinion segmenta-
tion y$ = h(s0, b0), ..., (sk, bk)i given the input x,
and then choose a sequence of attribute labels ya =
ha0, ..., aki given the chosen segment sequence. Fol-
lowing this intuition, the joint probability can be de-
composed as
</bodyText>
<equation confidence="0.9594676">
P(y|x) = P(y$|x)P(ya|y$,x)
where P(y$|x) is modeled as Equation (1) and
P(ya|y$, x) = � |ys |P(ai|ysi, x)
i=1
φ · g(ai, ysa, x)}
</equation>
<bodyText confidence="0.9999211">
where g denotes a feature function that encodes
attribute-specific information for discriminating dif-
ferent attribute classes for each segment.
For training, we can also apply a softmax-margin
by adding a loss function l(y0, y) to the denominator
of P(y|x) (as in the basic joint sequence labeling
model described in Section 3.3.1).
With the estimated parameters, we can infer the
optimal opinion segmentation and attribute labeling
by solving
</bodyText>
<equation confidence="0.603347">
argmaxP(y$|x)P(ya|y$, x)
ys,ya
</equation>
<bodyText confidence="0.9967848">
We can apply a similar dynamic programming pro-
cedure by replaceing y in Equation (3) with y =
(s, b, a) and G(y, y0) with θ·f(y, y0, x)+φ·g(y, x).
Our decomposition of labels and features is sim-
ilar to the hierarchical construction of CRF features
in Choi and Cardie (2010). The difference is that
our model is based on semi-CRFs and the decompo-
sition is based on a joint probability. We will show
that this results in better performance than the meth-
ods in Choi and Cardie (2010) in our experiments.
</bodyText>
<subsectionHeader confidence="0.596128">
3.3.3 Joint Inference
</subsectionHeader>
<bodyText confidence="0.9999218">
Modeling the joint probability of opinion seg-
mentation and attribute labeling is arguably elegant.
However, training can be expensive as the compu-
tation involves normalizing over all possible seg-
mentations and all possible attribute labelings for
</bodyText>
<equation confidence="0.988673666666667">
� |ys|
i=1
∝ exp{
</equation>
<page confidence="0.973533">
509
</page>
<bodyText confidence="0.99984175">
where U(ai|xsi) is a uncertainty function that mea-
sures the classification model’s uncertainty in its as-
signment of attribute class ai to segment xsi. In-
tuitively, we want to penalize attribute assignments
that are uncertain or favor attribute assignments with
low uncertainty. The prediction uncertainty is mea-
sured using the expected loss. The expected loss for
a predicted label a0 can be written as
</bodyText>
<equation confidence="0.952382">
�Ea|xsi[l(a,a0)] = P(a|xsi)l(a,a0)
a
</equation>
<bodyText confidence="0.974765">
where l(a, a0) is a loss function over a0 and the
true label a. We used the standard 0-1 loss func-
tion in our experiments2 and set U(ai|xsi) =
log(Ea|xsi [l(a, ai)]).
Both joint inference objectives can be solved effi-
ciently via dynamic programming.
</bodyText>
<sectionHeader confidence="0.99988" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999836">
We consider a set of basic features as well as task-
specific features for opinion segmentation and at-
tribute labeling, respectively.
</bodyText>
<subsectionHeader confidence="0.99532">
4.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.999700111111111">
Unigrams: word unigrams and POS tag unigrams
for all tokens in the segment candidate.
Bigrams: word bigrams and POS bigrams within
the segment candidate.
Phrase embeddings: for each segment candidate,
we associate with it a 300-dimensional phrase em-
bedding as a dense feature representation for the seg-
ment. We make use of the recently published word
embeddings trained on Google News (Mikolov et
al., 2013). For each segment, we compute the av-
erage of the word embedding vectors that comprise
the phrase. We omit words that are not found in the
vocabulary. If no words are found in the text seg-
ment, we assign a feature vector of zeros.
Opinion lexicon: For each word in the segment can-
didate, we include its polarity and intensity as indi-
cated in an existing Subjectivity Lexicon (Wilson et
al., 2005).
</bodyText>
<footnote confidence="0.917312">
2The loss function can be tuned to better tradeoff precision
and recall according to the applications at hand. We did not
explore this option in this paper.
</footnote>
<bodyText confidence="0.99946">
each segment. Thus, we also investigate joint in-
ference approaches which combine the separately-
trained models during inference without computing
the normalization term.
For opinion segmentation, we train a semi-CRF-
based model using the approach described in Sec-
tion 1. For attribute classification, we train a Max-
Ent model by maximizing P(aj|xs) in Equation (4).
As we only need to estimate the probability of an
attribute label given individual text segments, the
training data can be constructed by collecting a list
of text segments labeled with correct attribute labels.
The text segments do not need to form all possible
sentence segmentations. To construct such training
examples, we collected from each sentence all opin-
ion expressions labeled with their corresponding at-
tributes and use the remaining text segments as ex-
amples for the empty attribute value. The training of
the MaxEnt model is much more efficient than the
training of the segmentation model.
Joint Inference with Probability-based Esti-
mates To combine the separately-trained models at
inference time, a natural inference objective is to
jointly maximize the probability of opinion segmen-
tation and the probability of attribute labeling given
the chosen segmentation
</bodyText>
<equation confidence="0.931365">
argmaxP(ys|x)P0(ya|ys,x) (5)
</equation>
<bodyText confidence="0.655865">
ys,ya
We approximate the conditional probability as
</bodyText>
<equation confidence="0.998953">
P0(ya|ys, x) = � |ys |P(ai|xsi)α (6)
i=1
</equation>
<bodyText confidence="0.9987999">
where α E (0, 1]. We found that α &lt; 1 provides
better performance than α = 1 empirically. This is
an approximation since the distribution of attribute
labeling is estimated independently from the opinion
segmentation during training.
Joint Inference with Loss-based Estimates In-
stead of directly using the output probabilities of the
attribute classifiers, we explore an alternative that es-
timates P0(ya|ys, x) based on the prediction uncer-
tainty:
</bodyText>
<equation confidence="0.6771145">
P0(ya|ys, x) a exp(−α � |ys |U(ai|xsi)) (7)
i=1
</equation>
<page confidence="0.992248">
510
</page>
<subsectionHeader confidence="0.901298">
4.2 Segmentation-specific Features
</subsectionHeader>
<bodyText confidence="0.9939507">
Boundary words and POS tags: word-level fea-
tures (words, POS, lexicon) before and after the seg-
ment candidate.
Phrase structure: the syntactic categories of the
deepest constituents that cover the segment in the
parse tree, e.g. NP, VP, TO VB.
VP patterns: VP-related syntactic patterns de-
scribed in Yang and Cardie (2012), e.g. VPsubj,
VParg, which have been shown useful for opinion
expression extraction.
</bodyText>
<subsectionHeader confidence="0.994163">
4.3 Polarity-specific Features
</subsectionHeader>
<bodyText confidence="0.998842">
Polarity count: counts of positive, negative and
neutral words within the segment candidate accord-
ing to the opinion lexicon.
Negation: indicator for negators within the segment
candidate.
</bodyText>
<subsectionHeader confidence="0.809702">
4.4 Intensity-specific Features
</subsectionHeader>
<bodyText confidence="0.999144666666667">
Intensity count: counts of words with strong and
weak intensity within the segment candidate accord-
ing to the opinion lexicon.
Intensity dictionary: As suggested in Choi
and Cardie (2010), we include features indicat-
ing whether the segment contains an intensifier
(e.g. highly, really), a diminisher (e.g. little, less),
a strong modal verb (e.g. must, will), and a weak
modal verb (e.g. may, could).
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99898916">
All our experiments were conducted on the MPQA
corpus (Wiebe et al., 2005), a widely used corpus
for fine-grained opinion analysis. We used the same
evaluation setting as in Choi and Cardie (2010),
where 135 documents were used for development
and 10-fold cross-validation was performed on a dif-
ferent set of 400 documents. Each training fold con-
sists of sentences labeled with opinion expression
boundaries and each expression is labeled with po-
larity and intensity. Table 1 shows some statistics of
the evaluation data.
We used precision, recall and F1 as evaluation
metrics for opinion extraction and computed them
using both proportional matching and binary match-
ing criteria. Proportional matching considers the
overlapping proportion of a predicted expression s
and a gold standard expression s∗, and computes
precision as Es∈SEs*∈S* |sjs|*|/JSJ and recall as
Es∈SEs*∈S* |s|s*|* /JS∗J, where S and S∗ denote
the set of predicted opinion expressions and the set
of correct opinion expressions, respectively. Binary
matching is a more relaxed metric that considers a
predicted opinion expression to be correct if it over-
laps with a correct opinion expression.
We experimented with the following models:
</bodyText>
<listItem confidence="0.9488065625">
(1) PIPELINE: first extracts the spans of opinion
expressions using the semi-CRF model in Section
3.1, and then assigns polarity and intensity to the ex-
tracted opinion expressions using MaxEnt models in
Section 3.2. Note that the label space of the MaxEnt
models does not include ∅ since they assume that
all the opinion expressions extracted by the previous
stage are correct.
(2) JSL: the joint sequence labeling method de-
scribed in Section 3.3.1.
(3) HJSL: the hierarchical joint sequence labeling
method described in Section 3.3.2.
(4) JI-PROB: the joint inference method using
probability-based estimates (Equation 6).
(5) JI-LOSS: the joint inference method using
loss-based estimates (Equation 7).
</listItem>
<bodyText confidence="0.999780529411765">
We also compare our results with previously pub-
lished results from Choi and Cardie (2010) on the
same task.
All our models are log linear models. We use L-
BFGS with L2 regularization for training and set the
regularization parameter to 1.0. We set the scaling
parameter α in JI-PROB and JI-LOSS via grid search
over values between 0.1 and 1 with increments of
0.1 using the development set.
We consider the same set of features described in
Section 4 in all the models. For the pipeline and
joint inference models where the opinion segmen-
tator and attribute classifiers are separately trained,
we employ basic features plus segmentation-specific
features in the opinion segmentator; and employ ba-
sic features plus attribute-specific features in the at-
tribute classifiers.
</bodyText>
<sectionHeader confidence="0.704587" genericHeader="evaluation">
5.1 Results
</sectionHeader>
<bodyText confidence="0.999879666666667">
We would like to first investigate how much we can
gain from using the loss-augmented training com-
pared to using the standard training objective. Loss-
</bodyText>
<page confidence="0.99481">
511
</page>
<table confidence="0.9994765">
Number of Opinion Expressions
Positive Negative Neutral
2170 4863 6368
High Medium Low
2805 5721 4875
Number of Documents 400
Number of Sentences 8241
Average Length of Opinion Expressions 2.86 words
</table>
<tableCaption confidence="0.999894">
Table 1: Statistics of the evaluation corpus
</tableCaption>
<bodyText confidence="0.999949211764706">
augmented training can be applied to the training of
the opinion segmentation model used in the pipeline
method and the joint inference methods, or be ap-
plied to the training of the joint sequence labeling
approaches, JSL and HJSL (the loss function takes
into account both the span overlap and the match-
ing of attribute values). We evaluate two versions of
each method: one uses loss-augmented training and
one uses standard log-loss training. Table 2 shows
the results of opinion expression detection without
evaluating their attributes. Similar trends can be ob-
served in the results of opinion expression detection
with respect to each attribute. We can see that in-
corporating the evaluation-metric-based loss func-
tion during training consistently improves the per-
formance for all models in terms of F1 measure.
This confirms the effectiveness of loss-augmented
training of our sequence models for opinion extrac-
tion. As a result, all following results are based on
the loss-augmented version of our models.
Comparing the results of different models in Ta-
ble 2, we can see that PIPELINE provides a strong
baseline. In comparison, JSL and HJSL signifi-
cantly improve precision but fail in recall, which
indicates that joint sequence labeling is more con-
servative and precision-biased for extracting opinion
expressions. HJSL significantly outperforms JSL,
and this confirms the benefit of modeling the con-
ditional dependency between opinion segmentation
and attribute classification. In addition, we see that
combining opinion segmentation and attribute clas-
sification without joint training (JI-PROB and JI-
LOSS) hurt precision but improves recall (vs. JSL
and HJSL). JI-LOSS presents the best F1 perfor-
mance and significantly outperforms the PIPELINE
baseline in all evaluation metrics. This suggests that
JI-LOSS provides an effective joint inference objec-
tive and is able to provide more balanced precision
and recall than other joint approaches.
Table 3 shows the performance on opinion extrac-
tion with respect to polarity and intensity attributes.
Similarly, we can see that JI-LOSS outperforms all
other baselines in F1; HJSL outperforms JSL but
is slightly worse than PIPELINE in F1; JI-PROB is
recall-oriented and less effective than JI-LOSS.
We hypothesize that the worse performance of
joint sequence labeling is due to its strong assump-
tion on the dependencies between opinion segmen-
tation and attribute labeling in the training data.
For example, the expression “fundamentally unfair
and unjust” as a whole is labeled as an opinion ex-
pression with negative polarity. However, the sub-
expression “unjust” can be also viewed as a nega-
tive expression but it is not annotated as an opinion
expression in this example (as MPQA does not con-
sider nested opinion expressions). As a result, the
model would wrongly prefer an empty attribute to
the expression “unjust”. However, in our joint in-
ference approaches, the attribute classification mod-
els are trained independently from the segmentation
model, and the training examples for the classifiers
only consist of correctly labeled expressions (“un-
just” as a nested opinion expression in this example
would not be considered in the training data for the
attribute classifier). Therefore, the joint inference
approaches do not suffer from this issue. Although
joint inference does not account for task dependen-
cies during training, the promising performance of
JI-LOSS demonstrates that modeling label depen-
dencies during inference can be more effective than
the PIPELINE baseline.
In Table 3, we can see that the improvement of JI-
LOSS is less significant in the positive class and the
high class. This is due to the lack of training data in
these classes. The improvement in the medium class
is also less significant. This may be because it is in-
herently harder to disambiguate medium from low.
In general, we observe that extracting opinion ex-
pressions with correct intensity is a harder task than
extracting opinion expressions with correct polarity.
Table 4 presents the F1 scores (due to space limit
only F1 scores are reported) for all subtasks using
the binary matching metric. We include the previ-
ously published results of Choi and Cardie (2010)
for the same task using the same fold split and eval-
</bodyText>
<page confidence="0.990811">
512
</page>
<table confidence="0.999342428571429">
Loss-augmented Training Standard Training
P R F1 P R F1
PIPELINE 60.96 63.29 62.10 60.05 60.59 60.32
JSL 64.981 54.60 59.29 67.091 50.56 57.62
HJSL 66.16* 56.77 61.05 67.981 50.81 58.11
JI-PROB 50.95 77.44* 61.32 50.06 76.98* 60.54
JI-LOSS 63.771 64.511 64.04* 64.971 61.551 63.12*
</table>
<tableCaption confidence="0.973015">
Table 2: Opinion Expression Extraction (Proportional Matching). In all tables, we use bold to indicate the
highest score among all the methods; use ∗ to indicate statistically significant improvements (p &lt; 0.05) over
all the other methods under the paired-t test; use † to denote statistically significance (p &lt; 0.05) over the
pipeline baseline.
</tableCaption>
<table confidence="0.999974142857143">
Positive Negative Neutral
P R F1 P R F1 P R F1
PIPELINE 45.26 43.07 44.04 50.59 47.91 49.11 40.98 49.30 44.57
JSL 50.581 32.34 39.37 50.22 44.01 46.81 46.831 39.81 42.85
HJSL 50.341 37.06 42.59 53.291 43.98 48.07 47.291 43.27 45.03
JI-PROB 36.47 47.81* 41.24 40.83 54.40* 46.51 33.59 59.22* 42.66
JI-LOSS 46.441 44.581 45.40* 54.88* 48.50 51.40* 43.421 52.021 47.09*
High Medium Low
P R F1 P R F1 P R F1
PIPELINE 40.98 28.10 33.25 35.44 44.72 39.36 31.19 34.46 32.63
JSL 37.91 30.831 33.88 39.071 37.31 38.05 40.951 26.71 32.24
HJSL 41.05 28.80 33.63 39.061 39.71 39.17 40.011 29.88 34.12
JI-PROB 34.82 30.941 32.54 29.16 50.89* 36.89 25.06 42.99* 31.53
JI-LOSS 46.11* 26.36 33.39 37.581 43.58 40.15* 33.851 40.921 36.93*
</table>
<tableCaption confidence="0.999764">
Table 3: Opinion Extraction with Correct Attributes (Proportional Matching)
</tableCaption>
<bodyText confidence="0.999790545454546">
uation metric. CRF-JSL and CRF-HJSL are both
joint sequence labeling methods based on CRFs.
Different from JSL and HJSL, they perform se-
quence labeling at the token level instead of the seg-
ment level, and in HJSL, the decomposition of la-
bels are not based on the decomposition of the joint
probability of opinion segmentation and attribute la-
beling. We can see that both the pipeline and joint
methods clearly outperform previous results in all
evaluation criteria.3 We can also see that JI-LOSS
provides the best performance among all baselines.
</bodyText>
<subsubsectionHeader confidence="0.797207">
5.1.1 Error Analysis
</subsubsectionHeader>
<bodyText confidence="0.9971046">
Joint vs. Pipeline We found that many errors
made by the pipeline system are due to error prop-
agation. Table 5 lists three examples, representing
three types of the propagated errors:(1) the attribute
classifiers miss the prediction since the opinion ex-
</bodyText>
<footnote confidence="0.8347285">
3Significance test was not conducted over the results in Choi
and Cardie (2010) as we do not have their 10 fold results.
</footnote>
<bodyText confidence="0.97863705">
pression extractor fails to identify the opinion ex-
pression; (2) the attribute classifiers assign attributes
to a non-opinionated expression since it was mistak-
enly extracted; (3) the attribute classifiers misclas-
sify the attributes since the boundaries of opinion ex-
pressions are not correctly determined by the opin-
ion expression extractor. Our joint models are able
to correct many of these errors, such as the examples
in Table 5, due to the modeling of the dependency
between opinion expression extraction and attribute
classification.
Joint Learning vs. Joint Inference Note that
JSL and HJSL both employ joint learning while JI-
PROB and JI-LOSS employ joint inference. To in-
vestigate the difference between these two types of
joint models, we look into the errors made by HJSL
and JI-LOSS. In general, we observed that HJSL ex-
tracts many fewer opinion expressions compared to
JI-LOSS, and as a result, it presents high precision
but low recall. The first two examples in Table 6
</bodyText>
<page confidence="0.995379">
513
</page>
<table confidence="0.999967555555556">
Extraction Positive Negative Neutral High Medium Low
PIPELINE 73.30 51.50 58.45 52.45 39.34 47.08 39.05
JSL 69.76 45.24 57.11 50.25 41.481 45.88 36.49
HJSL 71.43 49.08 58.38 52.25 41.061 46.82 38.45
JI-PROB 74.371 50.93 58.20 54.031 39.80 46.65 40.731
JI-LOSS 75.11* 53.02* 62.01* 54.331 41.791 47.38 42.53*
Previous work (Choi and Cardie (2010))
CRF-JSL 60.5 41.9 50.3 41.2 38.4 37.6 28.0
CRF-HJSL 62.0 43.1 52.8 43.1 36.3 40.9 30.7
</table>
<tableCaption confidence="0.988184">
Table 4: Opinion Extraction Results (Binary Matching)
</tableCaption>
<table confidence="0.877522">
Example Sentences Pipeline Joint Models
It is the victim of an explosive situation high at the eco- No opinions × ✓
nomic, ...
A white farmer who was shot dead Monday was the the 10th to be killed medium × ✓
10th to be killed.
They would “ fall below minimum standards medium for minimum standards for humane ✓
humane medium treatment”. treatment medium ×
</table>
<tableCaption confidence="0.993312">
Table 5: Examples of mistakes made by the pipeline baseline that are corrected by the joint models
</tableCaption>
<bodyText confidence="0.999808058823529">
are cases where HJSL gains in precision and loses
in recall, respectively. The last example in Table 6
shows an error made by HJSL but corrected by JI-
LOSS. Theoretically, joint learning is more powerful
than joint inference as it models the task dependen-
cies during training. However, we only observe im-
provements on precision and see drops in recall. As
discussed before, we hypothesize that this is due to
the mismatch of dependency assumptions between
the model and the jointly annotated data. We found
that joint inference can be superior to both pipeline
and joint learning, and it is also much more efficient
in training. In our experiments on an Amazon EC2
instance with 64-bit processor, 4 CPUs and 15GB
memory, training for the joint learning approaches
took one hour for each training fold, but only 5 min-
utes for the joint inference approaches.
</bodyText>
<subsectionHeader confidence="0.997193">
5.2 Additional Experiments
5.2.1 Evaluation with Reranking
</subsectionHeader>
<bodyText confidence="0.999930103448276">
Previous work (Johansson and Moschitti, 2011)
showed that reranking is effective in improving the
pipeline of opinion expression extraction and polar-
ity classification. We extended their approach to
handle both polarity and intensity and investigated
the effect of reranking on both the pipeline and joint
models. For the pipeline model, we generated 64-
best (distinct) output with 4-best labeling at each
pipeline stage; for the joint models, we generated
50-best (distinct) output using Viterbi-like dynamic
programming. We trained the reranker using the on-
line PassiveAggressive algorithm (Crammer et al.,
2006) as in Johansson and Moschitti (2013) with
100 iterations and a regularization constant C =
0.01. For features, we included the probability out-
put by the base models, the polarity and intensity of
each pair of extracted opinion expressions, and the
word sequence and the POS sequence between the
adjacent pairs of extracted opinion expressions.
Table 7 shows the reranking performance (F1) for
all subtasks. We can see that after reranking, JI-
LOSS still provides the best performance and HJSL
achieves comparable performance to PIPELINE. We
also found that reranking leads to less performance
gain for the joint inference approaches than for the
joint learning approaches. This is because the k-best
output of JI-PROB and JI-LOSS present less diver-
sity than JSL and HJSL. A similar issue for rerank-
ing has also been discussed in Finkel et al. (2006).
</bodyText>
<subsectionHeader confidence="0.683014">
5.2.2 Evaluation on Sentence-level Tasks
</subsectionHeader>
<bodyText confidence="0.999977">
As an additional experiment, we consider a su-
pervised sentence-level sentiment classification task
using features derived from the prediction output
of different opinion extraction models. As a stan-
</bodyText>
<page confidence="0.992161">
514
</page>
<table confidence="0.983553142857143">
Example Sentences JointLearn JointInfer
The expression is undoubtedly strong and well ✓ well thought out medium ×
thought out high.
But the Sadc Ministerial Task Force said the election No opinions × ✓
was free and fair medium.
The president branded high as the “axis of evil” high in of evil high × ✓
his statement ...
</table>
<tableCaption confidence="0.963308666666667">
Table 6: Examples of mistakes that are made by the joint learning model but are corrected by the joint
inference model and vice versa. We use the same colored box notation as before, and use yellow color to
denote neutral sentiment.
</tableCaption>
<table confidence="0.999922666666667">
Extraction Positive Negative Neutral High Medium Low
PIPELINE + reranking 73.72 51.45 60.51 53.24 40.07 47.65 40.47
JSL + reranking 72.02 47.52 59.81 52.84 41.04† 46.58 39.40
HJSL + reranking 72.60 50.78 60.85 53.45 41.04† 47.75 40.08
JI-PROB + reranking 74.81† 51.45 59.59 53.98 40.66 46.87 40.80
JI-LOSS + reranking 75.59† 53.29∗ 62.50∗ 54.94∗ 41.79∗ 47.67 42.66∗
</table>
<tableCaption confidence="0.977523">
Table 7: Opinion Extraction with Reranking (Binary Matching)
</tableCaption>
<table confidence="0.999843428571429">
Features Acc Positive Negative Neutral
BOW 65.26 51.90 77.47 36.41
PIPELINE-OP 67.41 55.49 79.42 39.48
JSL-OP 65.86 55.97 77.68 36.46
HJSL-OP 66.79 55.12 79.29 37.56
JI-PROB-OP 67.13 56.49 79.30 38.49
JI-LOSS-OP 68.23∗ 57.32∗ 80.12∗ 40.45∗
</table>
<tableCaption confidence="0.998316">
Table 8: Sentence-level Sentiment Classification
</tableCaption>
<bodyText confidence="0.999859904761905">
dard baseline, we train a MaxEnt classifier using
unigrams, bigrams and opinion lexicon features ex-
tracted from the sentence. Using the prediction out-
put of an opinion extraction model, we construct fea-
tures by using only words from the extracted opinion
expressions, and include the predicted opinion at-
tributes as additional features. We hypothesize that
the more informative the extracted opinion expres-
sions are, the more they can contribute to sentence-
level sentiment classification as features. Table 8
shows the results in terms of classification accuracy
and F1 score in each sentiment category. BOW is
the standard MaxEnt baseline. We can see that us-
ing features constructed from the opinion expres-
sions always improved the performance. This con-
firms the informativeness of the extracted opinion
expressions. In particular, using the opinion expres-
sions extracted by JI-LOSS gives the best perfor-
mance among all the baselines in all evaluation crite-
ria. This is consistent with its superior performance
in our previous experiments.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991411764706">
We address the problem of opinion expression ex-
traction and opinion attribute classification by pre-
senting two types of joint models: joint learning,
which optimizes the parameters of different sub-
tasks in a joint probabilistic framework; joint infer-
ence, which optimizes the separately-trained mod-
els jointly during inference time. We show that
our models achieve substantially better performance
than the previously published results, and demon-
strate that joint inference with an appropriate objec-
tive can be more effective and efficient than joint
learning for the task. We also demonstrate the use-
fulness of output of our systems for sentence-level
sentiment analysis tasks. For future work, we plan
to improve joint modeling for the task by capturing
semantic relations among different opinion expres-
sions.
</bodyText>
<sectionHeader confidence="0.986107" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.74940375">
This work was supported in part by DARPA-BAA-12-
47 DEFT grant #12475008 and NSF grant BCS-0904822.
We thank the anonymous reviewers, Igor Labutov and the
Cornell NLP Group for helpful suggestions.
</bodyText>
<page confidence="0.997466">
515
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99973403030303">
E. Breck, Y. Choi, and C. Cardie. 2007. Identifying ex-
pressions of opinion in context. In Proceedings of the
international joint conference on Artifical intelligence.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for subsen-
tential sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Yejin Choi and Claire Cardie. 2010. Hierarchical se-
quential learning for extracting opinions and their at-
tributes. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics - Short Pa-
pers.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. The Journal of Machine Learn-
ing Research, 7:551–585.
Jenny Rose Finkel and Christopher D Manning. 2010.
Hierarchical joint learning: Improving joint parsing
and named entity recognition with non-jointly labeled
data. In Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Jenny Rose Finkel, Christopher D Manning, and An-
drew Y Ng. 2006. Solving the problem of cascading
errors: Approximate bayesian inference for linguistic
annotation pipelines. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Kevin Gimpel and Noah A Smith. 2010. Softmax-
margin crfs: Training log-linear models with cost
functions. In Human Language Technologies: Confer-
ence of the North American Chapter of the Association
for Computational Linguistics.
Richard Johansson and Alessandro Moschitti. 2011. Ex-
tracting opinion expressions and their polarities: ex-
ploration of pipelines and joint models. In Proceed-
ings of the Association for Computational Linguistics:
Human Language Technologies: short papers.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3):473–509.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear programming
inference. In Proceedings of the international confer-
ence on Computational Linguistics.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information ex-
traction. In Advances in Neural Information Process-
ing Systems.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2):165–210.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on human language technology and empirical methods
in natural language processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational linguistics, 35(3):399–433.
Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional random
fields. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning.
Bishan Yang and Claire Cardie. 2013. Joint inference for
fine-grained opinion extraction. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding
redundant features for crfs-based sentence sentiment
classification. In Proceedings of the conference on em-
pirical methods in natural language processing.
</reference>
<page confidence="0.998337">
516
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696014">
<title confidence="0.9991325">Joint Modeling of Opinion Expression and Attribute Classification</title>
<author confidence="0.98362">Bishan</author>
<affiliation confidence="0.928448">Department of Computer Cornell</affiliation>
<email confidence="0.998552">bishan@cs.cornell.edu</email>
<author confidence="0.976919">Claire</author>
<affiliation confidence="0.93527">Department of Computer Cornell</affiliation>
<email confidence="0.998819">cardie@cs.cornell.edu</email>
<abstract confidence="0.997502777777778">In this paper, we study the problems of opinion expression extraction and expression-level polarity and intensity classification. Traditional fine-grained opinion analysis systems address these problems in isolation and thus cannot capture interactions among the textual spans of opinion expressions and their opinion-related properties. We present two types of joint approaches that can account for such interactions during 1) both learning and inference or 2) only during inference. Extensive experiments on a standard dataset demonstrate that our approaches provide substantial improvements over previously published results. By analyzing the results, we gain some insight into the advantages of different joint models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Breck</author>
<author>Y Choi</author>
<author>C Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the international joint conference on Artifical intelligence.</booktitle>
<contexts>
<context position="2457" citStr="Breck et al., 2007" startWordPosition="347" endWordPosition="350">ributes — polarity and intensity — for characterizing the opinions. Consider the sentence in Figure 1, for example. The phrases “a bias in favor of” and “being severely criticized” are opinion expressions containing positive sentiment with medium intensity and negative sentiment with high intensity, respectively. Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint mode</context>
<context position="7311" citStr="Breck et al. (2007)" startWordPosition="1080" endWordPosition="1083">for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. However, in real-world settings, the spans of opinion expressions within the sentence are not available. In fact, Choi and Cardie (2008) demonstrated that the performance of expression-level polarity classification degrades as more surrounding (but irrelevant) context is considered. This motivates the additional task of identifying the spans of opinion expressions. Opinion expression extraction has been successfully tackled via sequence tagging methods. Breck et al. (2007) applied conditional random fields to assign each token a label indicating whether it belongs to an opinion expression or not. Yang and Cardie (2012) employed a segment-level sequence labeler based on semi-CRFs with rich phrase-level syntactic features. In this work, we also utilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical fea</context>
<context position="10678" citStr="Breck et al. (2007)" startWordPosition="1599" endWordPosition="1602">values from {high, medium, low}. In the following, we first describe how we model opinion expression extraction as a segment-level sequence labeling problem and model attribute prediction as a classification problem. Then we propose our joint models for combining opinion segmentation and attribute classification. 3.1 Opinion Expression Extraction The problem of opinion expression extraction assumes tokenized sentences as input and outputs the spans of the opinion expressions in each sentence. Previous work has tackled this problem using token-based sequence labeling methods such as CRFs (e.g. Breck et al. (2007), Yang and Cardie (2012)). However, semi-Markov CRFs (Sarawagi and Cohen, 2004) (henceforth semi-CRF) have been shown more appropriate for the task than CRFs since they allow contiguous spans in the input sequence (e.g. a noun phrase) to be treated as a group rather than as distinct tokens. Thus, they can easily capture segment-level information like syntactic constituent structure (Yang and Cardie, 2012). Therefore we adopt the semi-CRF model for opinion expression extraction here. Given a sentence x, denote an opinion segmentation as ys = h(s0, b0), ..., (sk, bk)i, where the s0.k are consecu</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>E. Breck, Y. Choi, and C. Cardie. 2007. Identifying expressions of opinion in context. In Proceedings of the international joint conference on Artifical intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with compositional semantics as structural inference for subsentential sentiment analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2611" citStr="Choi and Cardie, 2008" startWordPosition="373" endWordPosition="376">d “being severely criticized” are opinion expressions containing positive sentiment with medium intensity and negative sentiment with high intensity, respectively. Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling approach to extract op</context>
<context position="6370" citStr="Choi and Cardie (2008)" startWordPosition="946" endWordPosition="949">ive, can significantly boost both precision and recall and obtain the best overall performance. Error analysis provides additional understanding of the differences between the joint learning and joint inference approaches, and suggests that joint inference can be more effective and more efficient for the task in practice. 2 Related Work Significant research effort has been invested in the task of fine-grained opinion analysis in recent years (Wiebe et al., 2005; Wilson et al., 2009). Wilson et al. (2005) first motivated and studied phraselevel polarity classification on an open-domain corpus. Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification. Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. However, in real-world settings, the spans of opinion expressions within the sentence are not available. In fact, Choi and Cardie (2008)</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with compositional semantics as structural inference for subsentential sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Hierarchical sequential learning for extracting opinions and their attributes.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics - Short Papers.</booktitle>
<contexts>
<context position="3147" citStr="Choi and Cardie (2010)" startWordPosition="458" endWordPosition="461">nary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling approach to extract opinion expressions and label them with polarity and intensity. Their approach treats both expression extraction and attribute classification as token-level se505 Transactions of the Association for Computational Linguistics, vol. 2, pp. 505–516, 2014. Action Editor: Janyce Wiebe. Submitted 4/2014; Revised 8/2014; Published November 1, 2014. c�2014 Association for Computational Linguistics. He demonstrated a bias in favor of medium the rebels despite being severely criticized high. Figure 1: An example sentence annotated with opinio</context>
<context position="7780" citStr="Choi and Cardie (2010)" startWordPosition="1152" endWordPosition="1155">entifying the spans of opinion expressions. Opinion expression extraction has been successfully tackled via sequence tagging methods. Breck et al. (2007) applied conditional random fields to assign each token a label indicating whether it belongs to an opinion expression or not. Yang and Cardie (2012) employed a segment-level sequence labeler based on semi-CRFs with rich phrase-level syntactic features. In this work, we also utilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008). One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-level sequence labeling, and thus cannot model their inter506 actions at the expression-level. Johansson and Moschitti (2011) and Johansson and Moschitti (2013) propose a joint approach to opinion expression extraction and polarity classification by re-ranking its k-best output using global features. One major issue with</context>
<context position="19321" citStr="Choi and Cardie (2010)" startWordPosition="3053" endWordPosition="3056">for each segment. For training, we can also apply a softmax-margin by adding a loss function l(y0, y) to the denominator of P(y|x) (as in the basic joint sequence labeling model described in Section 3.3.1). With the estimated parameters, we can infer the optimal opinion segmentation and attribute labeling by solving argmaxP(y$|x)P(ya|y$, x) ys,ya We can apply a similar dynamic programming procedure by replaceing y in Equation (3) with y = (s, b, a) and G(y, y0) with θ·f(y, y0, x)+φ·g(y, x). Our decomposition of labels and features is similar to the hierarchical construction of CRF features in Choi and Cardie (2010). The difference is that our model is based on semi-CRFs and the decomposition is based on a joint probability. We will show that this results in better performance than the methods in Choi and Cardie (2010) in our experiments. 3.3.3 Joint Inference Modeling the joint probability of opinion segmentation and attribute labeling is arguably elegant. However, training can be expensive as the computation involves normalizing over all possible segmentations and all possible attribute labelings for � |ys| i=1 ∝ exp{ 509 where U(ai|xsi) is a uncertainty function that measures the classification model’</context>
<context position="24399" citStr="Choi and Cardie (2010)" startWordPosition="3857" endWordPosition="3860">.g. NP, VP, TO VB. VP patterns: VP-related syntactic patterns described in Yang and Cardie (2012), e.g. VPsubj, VParg, which have been shown useful for opinion expression extraction. 4.3 Polarity-specific Features Polarity count: counts of positive, negative and neutral words within the segment candidate according to the opinion lexicon. Negation: indicator for negators within the segment candidate. 4.4 Intensity-specific Features Intensity count: counts of words with strong and weak intensity within the segment candidate according to the opinion lexicon. Intensity dictionary: As suggested in Choi and Cardie (2010), we include features indicating whether the segment contains an intensifier (e.g. highly, really), a diminisher (e.g. little, less), a strong modal verb (e.g. must, will), and a weak modal verb (e.g. may, could). 5 Experiments All our experiments were conducted on the MPQA corpus (Wiebe et al., 2005), a widely used corpus for fine-grained opinion analysis. We used the same evaluation setting as in Choi and Cardie (2010), where 135 documents were used for development and 10-fold cross-validation was performed on a different set of 400 documents. Each training fold consists of sentences labeled</context>
<context position="26628" citStr="Choi and Cardie (2010)" startWordPosition="4203" endWordPosition="4206">essions using MaxEnt models in Section 3.2. Note that the label space of the MaxEnt models does not include ∅ since they assume that all the opinion expressions extracted by the previous stage are correct. (2) JSL: the joint sequence labeling method described in Section 3.3.1. (3) HJSL: the hierarchical joint sequence labeling method described in Section 3.3.2. (4) JI-PROB: the joint inference method using probability-based estimates (Equation 6). (5) JI-LOSS: the joint inference method using loss-based estimates (Equation 7). We also compare our results with previously published results from Choi and Cardie (2010) on the same task. All our models are log linear models. We use LBFGS with L2 regularization for training and set the regularization parameter to 1.0. We set the scaling parameter α in JI-PROB and JI-LOSS via grid search over values between 0.1 and 1 with increments of 0.1 using the development set. We consider the same set of features described in Section 4 in all the models. For the pipeline and joint inference models where the opinion segmentator and attribute classifiers are separately trained, we employ basic features plus segmentation-specific features in the opinion segmentator; and emp</context>
<context position="31929" citStr="Choi and Cardie (2010)" startWordPosition="5043" endWordPosition="5046">OSS is less significant in the positive class and the high class. This is due to the lack of training data in these classes. The improvement in the medium class is also less significant. This may be because it is inherently harder to disambiguate medium from low. In general, we observe that extracting opinion expressions with correct intensity is a harder task than extracting opinion expressions with correct polarity. Table 4 presents the F1 scores (due to space limit only F1 scores are reported) for all subtasks using the binary matching metric. We include the previously published results of Choi and Cardie (2010) for the same task using the same fold split and eval512 Loss-augmented Training Standard Training P R F1 P R F1 PIPELINE 60.96 63.29 62.10 60.05 60.59 60.32 JSL 64.981 54.60 59.29 67.091 50.56 57.62 HJSL 66.16* 56.77 61.05 67.981 50.81 58.11 JI-PROB 50.95 77.44* 61.32 50.06 76.98* 60.54 JI-LOSS 63.771 64.511 64.04* 64.971 61.551 63.12* Table 2: Opinion Expression Extraction (Proportional Matching). In all tables, we use bold to indicate the highest score among all the methods; use ∗ to indicate statistically significant improvements (p &lt; 0.05) over all the other methods under the paired-t tes</context>
<context position="34312" citStr="Choi and Cardie (2010)" startWordPosition="5434" endWordPosition="5437">f the joint probability of opinion segmentation and attribute labeling. We can see that both the pipeline and joint methods clearly outperform previous results in all evaluation criteria.3 We can also see that JI-LOSS provides the best performance among all baselines. 5.1.1 Error Analysis Joint vs. Pipeline We found that many errors made by the pipeline system are due to error propagation. Table 5 lists three examples, representing three types of the propagated errors:(1) the attribute classifiers miss the prediction since the opinion ex3Significance test was not conducted over the results in Choi and Cardie (2010) as we do not have their 10 fold results. pression extractor fails to identify the opinion expression; (2) the attribute classifiers assign attributes to a non-opinionated expression since it was mistakenly extracted; (3) the attribute classifiers misclassify the attributes since the boundaries of opinion expressions are not correctly determined by the opinion expression extractor. Our joint models are able to correct many of these errors, such as the examples in Table 5, due to the modeling of the dependency between opinion expression extraction and attribute classification. Joint Learning vs</context>
<context position="35688" citStr="Choi and Cardie (2010)" startWordPosition="5656" endWordPosition="5659"> these two types of joint models, we look into the errors made by HJSL and JI-LOSS. In general, we observed that HJSL extracts many fewer opinion expressions compared to JI-LOSS, and as a result, it presents high precision but low recall. The first two examples in Table 6 513 Extraction Positive Negative Neutral High Medium Low PIPELINE 73.30 51.50 58.45 52.45 39.34 47.08 39.05 JSL 69.76 45.24 57.11 50.25 41.481 45.88 36.49 HJSL 71.43 49.08 58.38 52.25 41.061 46.82 38.45 JI-PROB 74.371 50.93 58.20 54.031 39.80 46.65 40.731 JI-LOSS 75.11* 53.02* 62.01* 54.331 41.791 47.38 42.53* Previous work (Choi and Cardie (2010)) CRF-JSL 60.5 41.9 50.3 41.2 38.4 37.6 28.0 CRF-HJSL 62.0 43.1 52.8 43.1 36.3 40.9 30.7 Table 4: Opinion Extraction Results (Binary Matching) Example Sentences Pipeline Joint Models It is the victim of an explosive situation high at the eco- No opinions × ✓ nomic, ... A white farmer who was shot dead Monday was the the 10th to be killed medium × ✓ 10th to be killed. They would “ fall below minimum standards medium for minimum standards for humane ✓ humane medium treatment”. treatment medium × Table 5: Examples of mistakes made by the pipeline baseline that are corrected by the joint models ar</context>
</contexts>
<marker>Choi, Cardie, 2010</marker>
<rawString>Yejin Choi and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In Proceedings of the Annual Meeting of the Association for Computational Linguistics - Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Eric Breck</author>
<author>Claire Cardie</author>
</authors>
<title>Joint extraction of entities and relations for opinion recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9165" citStr="Choi et al., 2006" startWordPosition="1369" endWordPosition="1372">d attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goal is to identify the spans of opinion </context>
</contexts>
<marker>Choi, Breck, Cardie, 2006</marker>
<rawString>Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint extraction of entities and relations for opinion recognition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="37816" citStr="Crammer et al., 2006" startWordPosition="6003" endWordPosition="6006">nking Previous work (Johansson and Moschitti, 2011) showed that reranking is effective in improving the pipeline of opinion expression extraction and polarity classification. We extended their approach to handle both polarity and intensity and investigated the effect of reranking on both the pipeline and joint models. For the pipeline model, we generated 64- best (distinct) output with 4-best labeling at each pipeline stage; for the joint models, we generated 50-best (distinct) output using Viterbi-like dynamic programming. We trained the reranker using the online PassiveAggressive algorithm (Crammer et al., 2006) as in Johansson and Moschitti (2013) with 100 iterations and a regularization constant C = 0.01. For features, we included the probability output by the base models, the polarity and intensity of each pair of extracted opinion expressions, and the word sequence and the POS sequence between the adjacent pairs of extracted opinion expressions. Table 7 shows the reranking performance (F1) for all subtasks. We can see that after reranking, JILOSS still provides the best performance and HJSL achieves comparable performance to PIPELINE. We also found that reranking leads to less performance gain fo</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. The Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9127" citStr="Finkel and Manning, 2010" startWordPosition="1361" endWordPosition="1364">individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goa</context>
</contexts>
<marker>Finkel, Manning, 2010</marker>
<rawString>Jenny Rose Finkel and Christopher D Manning. 2010. Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="38665" citStr="Finkel et al. (2006)" startWordPosition="6142" endWordPosition="6145">xpressions, and the word sequence and the POS sequence between the adjacent pairs of extracted opinion expressions. Table 7 shows the reranking performance (F1) for all subtasks. We can see that after reranking, JILOSS still provides the best performance and HJSL achieves comparable performance to PIPELINE. We also found that reranking leads to less performance gain for the joint inference approaches than for the joint learning approaches. This is because the k-best output of JI-PROB and JI-LOSS present less diversity than JSL and HJSL. A similar issue for reranking has also been discussed in Finkel et al. (2006). 5.2.2 Evaluation on Sentence-level Tasks As an additional experiment, we consider a supervised sentence-level sentiment classification task using features derived from the prediction output of different opinion extraction models. As a stan514 Example Sentences JointLearn JointInfer The expression is undoubtedly strong and well ✓ well thought out medium × thought out high. But the Sadc Ministerial Task Force said the election No opinions × ✓ was free and fair medium. The president branded high as the “axis of evil” high in of evil high × ✓ his statement ... Table 6: Examples of mistakes that </context>
</contexts>
<marker>Finkel, Manning, Ng, 2006</marker>
<rawString>Jenny Rose Finkel, Christopher D Manning, and Andrew Y Ng. 2006. Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Softmaxmargin crfs: Training log-linear models with cost functions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13578" citStr="Gimpel and Smith, 2010" startWordPosition="2077" endWordPosition="2080">ries do not exactly align with the boundaries of the correct opinion expressions using 0-1 loss. Unfortunately, exact boundary matching is often not used as an evaluation metric for opinion expression extraction since it is hard for human annotators to agree on the exact boundaries of opinion expressions.1 Most previous work used proportional matching (Johansson and Moschitti, 2013) as it takes into account the overlapping proportion of the predicted and the correct opinion expressions to compute precision and recall. To incorporate this evaluation metric into training, we use softmax-margin (Gimpel and Smith, 2010) that replace P(y(i) s|x(i)) in (2) with Pcost(y(i) s |x(i)), which equals exp{P|ys| i=1 θ · f(ysi, ysi−1, x)} Py0s∈Y exp{P|y0s| i=1 θ · f(y0si, y0si−1, x) + l(y0s, ys)} and we define the loss function l(y0s, ys) as (✶{b0i =6 bj ∧ bi =6 O} |sj |s |sa| + ✶{b0i =6 bj ∧ bj =6 O} |sj |j|si |) which is the sum of the precision and recall errors of segment labeling using proportional matching. The loss-augmented probability is only computed during 1The inter-annotator agreement on boundaries of opinion expressions is not stressed in MPQA (Wiebe et al., 2005). We hope to eradicate the eternal scourge</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>Kevin Gimpel and Noah A Smith. 2010. Softmaxmargin crfs: Training log-linear models with cost functions. In Human Language Technologies: Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Extracting opinion expressions and their polarities: exploration of pipelines and joint models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association</booktitle>
<contexts>
<context position="4139" citStr="Johansson and Moschitti (2011)" startWordPosition="601" endWordPosition="605">Revised 8/2014; Published November 1, 2014. c�2014 Association for Computational Linguistics. He demonstrated a bias in favor of medium the rebels despite being severely criticized high. Figure 1: An example sentence annotated with opinion expressions and their polarity and intensity. We use colored boxes to mark the textual spans of opinion expressions where green (red) denotes positive (negative) polarity, and use subscripts to denote intensity. quence labeling tasks, and thus cannot model the label distribution over expressions even though the annotations are given at the expression level. Johansson and Moschitti (2011) considered a pipeline of opinion extraction followed by polarity classification and propose re-ranking its k-best outputs using global features. One key issue, however, is that the approach enumerates the k-best output in a pipeline manner and thus they do not necessarily correspond to the k-best global decisions. Moreover, as the number of opinion attributes grows, it is not clear how to identify the best k for each attribute. In contrast to existing approaches, we formulate opinion expression extraction as a segmentation problem and attribute classification as segmentlevel attribute labelin</context>
<context position="8183" citStr="Johansson and Moschitti (2011)" startWordPosition="1215" endWordPosition="1219">c features. In this work, we also utilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008). One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-level sequence labeling, and thus cannot model their inter506 actions at the expression-level. Johansson and Moschitti (2011) and Johansson and Moschitti (2013) propose a joint approach to opinion expression extraction and polarity classification by re-ranking its k-best output using global features. One major issue with their approach is that the k-best candidates were obtained without global reasoning about the relative uncertainty in the individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and </context>
<context position="37246" citStr="Johansson and Moschitti, 2011" startWordPosition="5919" endWordPosition="5922"> see drops in recall. As discussed before, we hypothesize that this is due to the mismatch of dependency assumptions between the model and the jointly annotated data. We found that joint inference can be superior to both pipeline and joint learning, and it is also much more efficient in training. In our experiments on an Amazon EC2 instance with 64-bit processor, 4 CPUs and 15GB memory, training for the joint learning approaches took one hour for each training fold, but only 5 minutes for the joint inference approaches. 5.2 Additional Experiments 5.2.1 Evaluation with Reranking Previous work (Johansson and Moschitti, 2011) showed that reranking is effective in improving the pipeline of opinion expression extraction and polarity classification. We extended their approach to handle both polarity and intensity and investigated the effect of reranking on both the pipeline and joint models. For the pipeline model, we generated 64- best (distinct) output with 4-best labeling at each pipeline stage; for the joint models, we generated 50-best (distinct) output using Viterbi-like dynamic programming. We trained the reranker using the online PassiveAggressive algorithm (Crammer et al., 2006) as in Johansson and Moschitti</context>
</contexts>
<marker>Johansson, Moschitti, 2011</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2011. Extracting opinion expressions and their polarities: exploration of pipelines and joint models. In Proceedings of the Association for Computational Linguistics: Human Language Technologies: short papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Relational features in fine-grained opinion analysis.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="8218" citStr="Johansson and Moschitti (2013)" startWordPosition="1221" endWordPosition="1224">tilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008). One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-level sequence labeling, and thus cannot model their inter506 actions at the expression-level. Johansson and Moschitti (2011) and Johansson and Moschitti (2013) propose a joint approach to opinion expression extraction and polarity classification by re-ranking its k-best output using global features. One major issue with their approach is that the k-best candidates were obtained without global reasoning about the relative uncertainty in the individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the seg</context>
<context position="13340" citStr="Johansson and Moschitti, 2013" startWordPosition="2039" endWordPosition="2042">ion in training time and was shown to be effective for identifying opinion expressions. The standard training objective of a semi-CRF, is to minimize the log loss log P(y(i) s |x(i)) (2) It penalizes any predicted opinion expression whose boundaries do not exactly align with the boundaries of the correct opinion expressions using 0-1 loss. Unfortunately, exact boundary matching is often not used as an evaluation metric for opinion expression extraction since it is hard for human annotators to agree on the exact boundaries of opinion expressions.1 Most previous work used proportional matching (Johansson and Moschitti, 2013) as it takes into account the overlapping proportion of the predicted and the correct opinion expressions to compute precision and recall. To incorporate this evaluation metric into training, we use softmax-margin (Gimpel and Smith, 2010) that replace P(y(i) s|x(i)) in (2) with Pcost(y(i) s |x(i)), which equals exp{P|ys| i=1 θ · f(ysi, ysi−1, x)} Py0s∈Y exp{P|y0s| i=1 θ · f(y0si, y0si−1, x) + l(y0s, ys)} and we define the loss function l(y0s, ys) as (✶{b0i =6 bj ∧ bi =6 O} |sj |s |sa| + ✶{b0i =6 bj ∧ bj =6 O} |sj |j|si |) which is the sum of the precision and recall errors of segment labeling </context>
<context position="37853" citStr="Johansson and Moschitti (2013)" startWordPosition="6009" endWordPosition="6012">on and Moschitti, 2011) showed that reranking is effective in improving the pipeline of opinion expression extraction and polarity classification. We extended their approach to handle both polarity and intensity and investigated the effect of reranking on both the pipeline and joint models. For the pipeline model, we generated 64- best (distinct) output with 4-best labeling at each pipeline stage; for the joint models, we generated 50-best (distinct) output using Viterbi-like dynamic programming. We trained the reranker using the online PassiveAggressive algorithm (Crammer et al., 2006) as in Johansson and Moschitti (2013) with 100 iterations and a regularization constant C = 0.01. For features, we included the probability output by the base models, the polarity and intensity of each pair of extracted opinion expressions, and the word sequence and the POS sequence between the adjacent pairs of extracted opinion expressions. Table 7 shows the reranking performance (F1) for all subtasks. We can see that after reranking, JILOSS still provides the best performance and HJSL achieves comparable performance to PIPELINE. We also found that reranking leads to less performance gain for the joint inference approaches than</context>
</contexts>
<marker>Johansson, Moschitti, 2013</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2013. Relational features in fine-grained opinion analysis. Computational Linguistics, 39(3):473–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="21099" citStr="Mikolov et al., 2013" startWordPosition="3342" endWordPosition="3345"> can be solved efficiently via dynamic programming. 4 Features We consider a set of basic features as well as taskspecific features for opinion segmentation and attribute labeling, respectively. 4.1 Basic Features Unigrams: word unigrams and POS tag unigrams for all tokens in the segment candidate. Bigrams: word bigrams and POS bigrams within the segment candidate. Phrase embeddings: for each segment candidate, we associate with it a 300-dimensional phrase embedding as a dense feature representation for the segment. We make use of the recently published word embeddings trained on Google News (Mikolov et al., 2013). For each segment, we compute the average of the word embedding vectors that comprise the phrase. We omit words that are not found in the vocabulary. If no words are found in the text segment, we assign a feature vector of zeros. Opinion lexicon: For each word in the segment candidate, we include its polarity and intensity as indicated in an existing Subjectivity Lexicon (Wilson et al., 2005). 2The loss function can be tuned to better tradeoff precision and recall according to the applications at hand. We did not explore this option in this paper. each segment. Thus, we also investigate joint</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of the international conference on Computational Linguistics.</booktitle>
<contexts>
<context position="9101" citStr="Punyakanok et al., 2004" startWordPosition="1357" endWordPosition="1360">ative uncertainty in the individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically,</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proceedings of the international conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<contexts>
<context position="9076" citStr="Roth and Yih, 2004" startWordPosition="1353" endWordPosition="1356">soning about the relative uncertainty in the individual stages. As the number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute clas</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>D. Roth and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="9146" citStr="Rush et al., 2010" startWordPosition="1365" endWordPosition="1368">number of considered attributes grows, it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goal is to identify th</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="10757" citStr="Sarawagi and Cohen, 2004" startWordPosition="1610" endWordPosition="1613">we model opinion expression extraction as a segment-level sequence labeling problem and model attribute prediction as a classification problem. Then we propose our joint models for combining opinion segmentation and attribute classification. 3.1 Opinion Expression Extraction The problem of opinion expression extraction assumes tokenized sentences as input and outputs the spans of the opinion expressions in each sentence. Previous work has tackled this problem using token-based sequence labeling methods such as CRFs (e.g. Breck et al. (2007), Yang and Cardie (2012)). However, semi-Markov CRFs (Sarawagi and Cohen, 2004) (henceforth semi-CRF) have been shown more appropriate for the task than CRFs since they allow contiguous spans in the input sequence (e.g. a noun phrase) to be treated as a group rather than as distinct tokens. Thus, they can easily capture segment-level information like syntactic constituent structure (Yang and Cardie, 2012). Therefore we adopt the semi-CRF model for opinion expression extraction here. Given a sentence x, denote an opinion segmentation as ys = h(s0, b0), ..., (sk, bk)i, where the s0.k are consecutive segments that form a segmentation of x; each segment si = (ti, ui) consist</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W Cohen. 2004. Semimarkov conditional random fields for information extraction. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6546" citStr="Socher et al. (2013)" startWordPosition="971" endWordPosition="974">int learning and joint inference approaches, and suggests that joint inference can be more effective and more efficient for the task in practice. 2 Related Work Significant research effort has been invested in the task of fine-grained opinion analysis in recent years (Wiebe et al., 2005; Wilson et al., 2009). Wilson et al. (2005) first motivated and studied phraselevel polarity classification on an open-domain corpus. Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification. Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. However, in real-world settings, the spans of opinion expressions within the sentence are not available. In fact, Choi and Cardie (2008) demonstrated that the performance of expression-level polarity classification degrades as more surrounding (but irrelevant) context is considered. This motivates the additiona</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1813" citStr="Wiebe et al., 2005" startWordPosition="247" endWordPosition="250">finegrained level, e.g. identifying opinion expressions within a sentence and predicting phrase-level polarity and intensity. The ability to extract finegrained opinion information is crucial in supporting many opinion-mining applications such as opinion summarization, opinion-oriented question answering and opinion retrieval. In this paper, we focus on the problem of identifying opinion expressions and classifying their attributes. We consider as an opinion expression any subjective expression that explicitly or implicitly conveys emotions, sentiment, beliefs, opinions (i.e. private states) (Wiebe et al., 2005), and consider two key attributes — polarity and intensity — for characterizing the opinions. Consider the sentence in Figure 1, for example. The phrases “a bias in favor of” and “being severely criticized” are opinion expressions containing positive sentiment with medium intensity and negative sentiment with high intensity, respectively. Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspon</context>
<context position="5440" citStr="Wiebe et al., 2005" startWordPosition="801" endWordPosition="804">int learning approaches, which combine opinion segment detection and attribute labeling into a single probabilistic model, and estimate parameters for this joint model; and (2) joint inference approaches, which build separate models for opinion segment detection and attribute labeling at training time, and jointly apply these (via a single objective function) only at test time to identify the best “combined” decision of the two models. To investigate the effectiveness of our approaches, we conducted extensive experiments on a standard corpus for fine-grained opinion analysis (the MPQA corpus (Wiebe et al., 2005)). We found that all of our proposed approaches provide substantial improvements over the previously published results. We also compared our approaches to a strong pipeline baseline and observed that joint learning results in a significant boost in precision while joint inference, with an appropriate objective, can significantly boost both precision and recall and obtain the best overall performance. Error analysis provides additional understanding of the differences between the joint learning and joint inference approaches, and suggests that joint inference can be more effective and more effi</context>
<context position="14136" citStr="Wiebe et al., 2005" startWordPosition="2177" endWordPosition="2180">into training, we use softmax-margin (Gimpel and Smith, 2010) that replace P(y(i) s|x(i)) in (2) with Pcost(y(i) s |x(i)), which equals exp{P|ys| i=1 θ · f(ysi, ysi−1, x)} Py0s∈Y exp{P|y0s| i=1 θ · f(y0si, y0si−1, x) + l(y0s, ys)} and we define the loss function l(y0s, ys) as (✶{b0i =6 bj ∧ bi =6 O} |sj |s |sa| + ✶{b0i =6 bj ∧ bj =6 O} |sj |j|si |) which is the sum of the precision and recall errors of segment labeling using proportional matching. The loss-augmented probability is only computed during 1The inter-annotator agreement on boundaries of opinion expressions is not stressed in MPQA (Wiebe et al., 2005). We hope to eradicate the eternal scourge of corruption . [ ][ ][ ] [ ][ ][ ][ ][ ] [ ][ ][ ] [ ][ ][ ][ ] [ ][ ][ ] [ ][ ][ ] [ ][ ][ ] [ ][ ] Figure 2: Examples of Segmentation Candidates training. The more the proposed labeled segmentation overlaps with the true labeled segmentation for x, the less it will be penalized. During inference, we can obtain the best labeled segmentation by solving argmaxP(ys|x) = argmax X |ys |θ · f(ysi,ysi−1, x) ys ys i=1 This can be done efficiently via dynamic programming: V (t) = argmax G(y, y0)+V (u−1) (3) s=(u,t)∈s:t,y=(s,b),y0 where s:t denotes all candid</context>
<context position="24701" citStr="Wiebe et al., 2005" startWordPosition="3906" endWordPosition="3909"> according to the opinion lexicon. Negation: indicator for negators within the segment candidate. 4.4 Intensity-specific Features Intensity count: counts of words with strong and weak intensity within the segment candidate according to the opinion lexicon. Intensity dictionary: As suggested in Choi and Cardie (2010), we include features indicating whether the segment contains an intensifier (e.g. highly, really), a diminisher (e.g. little, less), a strong modal verb (e.g. must, will), and a weak modal verb (e.g. may, could). 5 Experiments All our experiments were conducted on the MPQA corpus (Wiebe et al., 2005), a widely used corpus for fine-grained opinion analysis. We used the same evaluation setting as in Choi and Cardie (2010), where 135 documents were used for development and 10-fold cross-validation was performed on a different set of 400 documents. Each training fold consists of sentences labeled with opinion expression boundaries and each expression is labeled with polarity and intensity. Table 1 shows some statistics of the evaluation data. We used precision, recall and F1 as evaluation metrics for opinion extraction and computed them using both proportional matching and binary matching cri</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on human language technology and empirical methods in natural language processing.</booktitle>
<contexts>
<context position="2588" citStr="Wilson et al., 2005" startWordPosition="368" endWordPosition="372"> bias in favor of” and “being severely criticized” are opinion expressions containing positive sentiment with medium intensity and negative sentiment with high intensity, respectively. Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling</context>
<context position="6257" citStr="Wilson et al. (2005)" startWordPosition="928" endWordPosition="932">at joint learning results in a significant boost in precision while joint inference, with an appropriate objective, can significantly boost both precision and recall and obtain the best overall performance. Error analysis provides additional understanding of the differences between the joint learning and joint inference approaches, and suggests that joint inference can be more effective and more efficient for the task in practice. 2 Related Work Significant research effort has been invested in the task of fine-grained opinion analysis in recent years (Wiebe et al., 2005; Wilson et al., 2009). Wilson et al. (2005) first motivated and studied phraselevel polarity classification on an open-domain corpus. Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification. Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. However, in real-world </context>
<context position="21495" citStr="Wilson et al., 2005" startWordPosition="3414" endWordPosition="3417">egment candidate, we associate with it a 300-dimensional phrase embedding as a dense feature representation for the segment. We make use of the recently published word embeddings trained on Google News (Mikolov et al., 2013). For each segment, we compute the average of the word embedding vectors that comprise the phrase. We omit words that are not found in the vocabulary. If no words are found in the text segment, we assign a feature vector of zeros. Opinion lexicon: For each word in the segment candidate, we include its polarity and intensity as indicated in an existing Subjectivity Lexicon (Wilson et al., 2005). 2The loss function can be tuned to better tradeoff precision and recall according to the applications at hand. We did not explore this option in this paper. each segment. Thus, we also investigate joint inference approaches which combine the separatelytrained models during inference without computing the normalization term. For opinion segmentation, we train a semi-CRFbased model using the approach described in Section 1. For attribute classification, we train a MaxEnt model by maximizing P(aj|xs) in Equation (4). As we only need to estimate the probability of an attribute label given indivi</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on human language technology and empirical methods in natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<booktitle>Computational linguistics,</booktitle>
<pages>35--3</pages>
<contexts>
<context position="6235" citStr="Wilson et al., 2009" startWordPosition="924" endWordPosition="927">seline and observed that joint learning results in a significant boost in precision while joint inference, with an appropriate objective, can significantly boost both precision and recall and obtain the best overall performance. Error analysis provides additional understanding of the differences between the joint learning and joint inference approaches, and suggests that joint inference can be more effective and more efficient for the task in practice. 2 Related Work Significant research effort has been invested in the task of fine-grained opinion analysis in recent years (Wiebe et al., 2005; Wilson et al., 2009). Wilson et al. (2005) first motivated and studied phraselevel polarity classification on an open-domain corpus. Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification. Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. H</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational linguistics, 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Extracting opinion expressions with semi-markov conditional random fields.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="2481" citStr="Yang and Cardie, 2012" startWordPosition="351" endWordPosition="354">nd intensity — for characterizing the opinions. Consider the sentence in Figure 1, for example. The phrases “a bias in favor of” and “being severely criticized” are opinion expressions containing positive sentiment with medium intensity and negative sentiment with high intensity, respectively. Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expressi</context>
<context position="7460" citStr="Yang and Cardie (2012)" startWordPosition="1105" endWordPosition="1108">ver, in real-world settings, the spans of opinion expressions within the sentence are not available. In fact, Choi and Cardie (2008) demonstrated that the performance of expression-level polarity classification degrades as more surrounding (but irrelevant) context is considered. This motivates the additional task of identifying the spans of opinion expressions. Opinion expression extraction has been successfully tackled via sequence tagging methods. Breck et al. (2007) applied conditional random fields to assign each token a label indicating whether it belongs to an opinion expression or not. Yang and Cardie (2012) employed a segment-level sequence labeler based on semi-CRFs with rich phrase-level syntactic features. In this work, we also utilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008). One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-le</context>
<context position="10702" citStr="Yang and Cardie (2012)" startWordPosition="1603" endWordPosition="1606">dium, low}. In the following, we first describe how we model opinion expression extraction as a segment-level sequence labeling problem and model attribute prediction as a classification problem. Then we propose our joint models for combining opinion segmentation and attribute classification. 3.1 Opinion Expression Extraction The problem of opinion expression extraction assumes tokenized sentences as input and outputs the spans of the opinion expressions in each sentence. Previous work has tackled this problem using token-based sequence labeling methods such as CRFs (e.g. Breck et al. (2007), Yang and Cardie (2012)). However, semi-Markov CRFs (Sarawagi and Cohen, 2004) (henceforth semi-CRF) have been shown more appropriate for the task than CRFs since they allow contiguous spans in the input sequence (e.g. a noun phrase) to be treated as a group rather than as distinct tokens. Thus, they can easily capture segment-level information like syntactic constituent structure (Yang and Cardie, 2012). Therefore we adopt the semi-CRF model for opinion expression extraction here. Given a sentence x, denote an opinion segmentation as ys = h(s0, b0), ..., (sk, bk)i, where the s0.k are consecutive segments that form </context>
<context position="12470" citStr="Yang and Cardie (2012)" startWordPosition="1903" endWordPosition="1906">sing a semi-Markov CRF, we model the conditional distribution over all possible opinion segmentations given the input x: P(ys|x) = exp{E|ys |i��θ · f(ysa, ysa−1, x)} Ey�sEY exp{�|y�s| i�� θ · f(y�sa, y�sa−1, x)} (1) where θ denotes the model parameters, ysa = (si, bi) and f denotes a feature function that encodes the potentials of the boundaries for opinion segments and the potentials of transitions between two consecutive labeled segments. 507 Note that the probability is normalized over all possible opinion segmentations. To reduce the training complexity, we adopted the method described in Yang and Cardie (2012), which only normalizes over segment candidates that are plausible according to the parsing structure of the sentence. Figure 2 shows some candidate segmentations generated for an example sentence. Such a technique results in a large reduction in training time and was shown to be effective for identifying opinion expressions. The standard training objective of a semi-CRF, is to minimize the log loss log P(y(i) s |x(i)) (2) It penalizes any predicted opinion expression whose boundaries do not exactly align with the boundaries of the correct opinion expressions using 0-1 loss. Unfortunately, exa</context>
<context position="23874" citStr="Yang and Cardie (2012)" startWordPosition="3783" endWordPosition="3786">oint Inference with Loss-based Estimates Instead of directly using the output probabilities of the attribute classifiers, we explore an alternative that estimates P0(ya|ys, x) based on the prediction uncertainty: P0(ya|ys, x) a exp(−α � |ys |U(ai|xsi)) (7) i=1 510 4.2 Segmentation-specific Features Boundary words and POS tags: word-level features (words, POS, lexicon) before and after the segment candidate. Phrase structure: the syntactic categories of the deepest constituents that cover the segment in the parse tree, e.g. NP, VP, TO VB. VP patterns: VP-related syntactic patterns described in Yang and Cardie (2012), e.g. VPsubj, VParg, which have been shown useful for opinion expression extraction. 4.3 Polarity-specific Features Polarity count: counts of positive, negative and neutral words within the segment candidate according to the opinion lexicon. Negation: indicator for negators within the segment candidate. 4.4 Intensity-specific Features Intensity count: counts of words with strong and weak intensity within the segment candidate according to the opinion lexicon. Intensity dictionary: As suggested in Choi and Cardie (2010), we include features indicating whether the segment contains an intensifie</context>
</contexts>
<marker>Yang, Cardie, 2012</marker>
<rawString>Bishan Yang and Claire Cardie. 2012. Extracting opinion expressions with semi-markov conditional random fields. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9189" citStr="Yang and Cardie, 2013" startWordPosition="1373" endWordPosition="1376"> it also becomes harder to decide how many predictions to select from each attribute classifier. Compared to the existing approaches, our joint models have the advantage of modeling opinion expression extraction and attribute classification at the segment-level, and more importantly, they provide a principled way of combining the segmentation and classification components. Our work follows a long line of joint modeling research that has demonstrated great success for various NLP tasks (Roth and Yih, 2004; Punyakanok et al., 2004; Finkel and Manning, 2010; Rush et al., 2010; Choi et al., 2006; Yang and Cardie, 2013). Methods tend to fall into one of two joint modeling frameworks: the first learns a joint model that captures global dependencies; the other uses independently-learned models and considers global dependencies only during inference. In this work, we study both types of joint approaches for opinion expression extraction and opinion attribute classification. 3 Approach In this section, we present our approaches for the joint modeling of opinion expression extraction and attribute classification. Specifically, given a sentence, our goal is to identify the spans of opinion expressions, and simulta</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2642" citStr="Yessenalina and Cardie, 2011" startWordPosition="377" endWordPosition="380">cized” are opinion expressions containing positive sentiment with medium intensity and negative sentiment with high intensity, respectively. Most existing approaches tackle the tasks of opinion expression extraction and attribute classification in isolation. The first task is typically formulated as a sequence labeling problem, where the goal is to label the boundaries of text spans that correspond to opinion expressions (Breck et al., 2007; Yang and Cardie, 2012). The second task is usually treated as a binary or multi-class classification problem (Wilson et al., 2005; Choi and Cardie, 2008; Yessenalina and Cardie, 2011), where the goal is to assign a class label to a text fragment (e.g. a phrase or a sentence). Solutions to the two tasks can be applied in a pipeline architecture to extract opinion expressions and their attributes. However, pipeline systems suffer from error propagation: opinion expression errors propagate and lead to unrecoverable errors in attribute classification. Limited work has been done on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first proposed a joint sequence labeling approach to extract opinion expressions and label the</context>
<context position="6521" citStr="Yessenalina and Cardie (2011)" startWordPosition="965" endWordPosition="969"> of the differences between the joint learning and joint inference approaches, and suggests that joint inference can be more effective and more efficient for the task in practice. 2 Related Work Significant research effort has been invested in the task of fine-grained opinion analysis in recent years (Wiebe et al., 2005; Wilson et al., 2009). Wilson et al. (2005) first motivated and studied phraselevel polarity classification on an open-domain corpus. Choi and Cardie (2008) developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification. Yessenalina and Cardie (2011) and Socher et al. (2013) learn continuous-valued phrase representations by combining the representations of words within an opinion expression and using them as features for classifying polarity and intensity. All of these approaches assume the opinion expressions are available before training the classifiers. However, in real-world settings, the spans of opinion expressions within the sentence are not available. In fact, Choi and Cardie (2008) demonstrated that the performance of expression-level polarity classification degrades as more surrounding (but irrelevant) context is considered. Thi</context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Gen Wang</author>
</authors>
<title>Adding redundant features for crfs-based sentence sentiment classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing.</booktitle>
<contexts>
<context position="7936" citStr="Zhao et al., 2008" startWordPosition="1177" endWordPosition="1180">d conditional random fields to assign each token a label indicating whether it belongs to an opinion expression or not. Yang and Cardie (2012) employed a segment-level sequence labeler based on semi-CRFs with rich phrase-level syntactic features. In this work, we also utilize semi-CRFs to model opinion expression extraction. There has been limited work on the joint modeling of opinion expression extraction and attribute classification. Choi and Cardie (2010) first developed a joint sequence labeler that jointly tags opinions, polarity and intensity by training CRFs with hierarchical features (Zhao et al., 2008). One major drawback of their approach is that it models both opinion extraction and attribute labeling as tasks in token-level sequence labeling, and thus cannot model their inter506 actions at the expression-level. Johansson and Moschitti (2011) and Johansson and Moschitti (2013) propose a joint approach to opinion expression extraction and polarity classification by re-ranking its k-best output using global features. One major issue with their approach is that the k-best candidates were obtained without global reasoning about the relative uncertainty in the individual stages. As the number </context>
</contexts>
<marker>Zhao, Liu, Wang, 2008</marker>
<rawString>Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding redundant features for crfs-based sentence sentiment classification. In Proceedings of the conference on empirical methods in natural language processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
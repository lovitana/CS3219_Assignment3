<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.914849">
The Language Demographics of Amazon Mechanical Turk
</title>
<author confidence="0.997621">
Ellie Pavlick1 Matt Post2 Ann Irvine2 Dmitry Kachaev2 Chris Callison-Burch1,2
</author>
<affiliation confidence="0.9948545">
1Computer and Information Science Department, University of Pennsylvania
2Human Language Technology Center of Excellence, Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.980148" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999783896551724">
We present a large scale study of the languages
spoken by bilingual workers on Mechanical
Turk (MTurk). We establish a methodology
for determining the language skills of anony-
mous crowd workers that is more robust than
simple surveying. We validate workers’ self-
reported language skill claims by measuring
their ability to correctly translate words, and
by geolocating workers to see if they reside in
countries where the languages are likely to be
spoken. Rather than posting a one-off survey,
we posted paid tasks consisting of 1,000 as-
signments to translate a total of 10,000 words
in each of 100 languages. Our study ran
for several months, and was highly visible on
the MTurk crowdsourcing platform, increas-
ing the chances that bilingual workers would
complete it. Our study was useful both to cre-
ate bilingual dictionaries and to act as cen-
sus of the bilingual speakers on MTurk. We
use this data to recommend languages with the
largest speaker populations as good candidates
for other researchers who want to develop
crowdsourced, multilingual technologies. To
further demonstrate the value of creating data
via crowdsourcing, we hire workers to create
bilingual parallel corpora in six Indian lan-
guages, and use them to train statistical ma-
chine translation systems.
</bodyText>
<sectionHeader confidence="0.995739" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.99991811627907">
Crowdsourcing is a promising new mechanism for
collecting data for natural language processing re-
search. Access to a fast, cheap, and flexible work-
force allows us to collect new types of data, poten-
tially enabling new language technologies. Because
crowdsourcing platforms like Amazon Mechanical
Turk (MTurk) give researchers access to a world-
wide workforce, one obvious application of crowd-
sourcing is the creation of multilingual technologies.
With an increasing number of active crowd workers
located outside of the United States, there is even the
potential to reach fluent speakers of lower resource
languages. In this paper, we investigate the feasi-
bility of hiring language informants on MTurk by
conducting the first large-scale demographic study
of the languages spoken by workers on the platform.
There are several complicating factors when try-
ing to take a census of workers on MTurk. The
workers’ identities are anonymized, and Amazon
provides no information about their countries of ori-
gin or their language abilities. Posting a simple sur-
vey to have workers report this information may be
inadequate, since (a) many workers may never see
the survey, (b) many opt not to do one-off surveys
since potential payment is low, and (c) validating the
answers of respondents is not straightforward.
Our study establishes a methodology for deter-
mining the language demographics of anonymous
crowd workers that is more robust than simple sur-
veying. We ask workers what languages they speak
and what country they live in, and validate their
claims by measuring their ability to correctly trans-
late words and by recording their geolocation. To
increase the visibility and the desirability of our
tasks, we post 1,000 assignments in each of 100 lan-
guages. These tasks each consist of translating 10
foreign words into English. Two of the 10 words
have known translations, allowing us to validate that
the workers’ translations are accurate. We construct
bilingual dictionaries with up to 10,000 entries, with
the majority of entries being new.
Surveying thousands of workers allows us to ana-
lyze current speaker populations for 100 languages.
</bodyText>
<page confidence="0.993388">
79
</page>
<bodyText confidence="0.960189910714286">
Transactions of the Association for Computational Linguistics, 2 (2014) 79–92. Action Editor: Mirella Lapata.
Submitted 12/2013; Published 2/2014. c�2014 Association for Computational Linguistics.
111,99 81 ,99 8Figur e1 :Thenum ber ofworker sper cou ntr y.Thismap was ge ne rated basedo nge ol ocating
th e IP ad dressof 4, 983 worker sinours tud y. Omitted are 60wo rkerswh ow erel ocat edi n moreth anonec
oun trydur ing the study, a nd2 38wor ker s w hocouldnot b ege oloc at ed. Thesize ofthe circl esr eprese
nt sthenum bero fwor kersfrom eac hco untry.T het wolar gest ar eIndia(1 ,99 8wo rkers) andthe United St
ates(866) .To calibrate thesizes:thePhilippineshas142workers,Egypt has 25, Russia has10,andS
riLanka has 4 .Theda ta al so allo wsusto ans werqu
est ionslik e: Howq uicklyisw or k compl etedinagi
ven language?Are crowdsourced translat ionsr eli
ablyg oo d?Howof tendoworkers misre present t heirla
ngua ge abili- tiesto obtainfinan
c ialrewards ?2 B ackgrou ndan
dRelated WorkAmazon ’sMe chanica l T ur k(M
Turk )isanon-lin ema rket plac efor w orkthatgi
ves employersan dresea rc h ersacc ess to a la rge,l
ow-cos twork -force .MTurkall ow semploy erstop
rovidemi cr o-paym ent s inretu rnfor worke rscomp
leting mic ro-ta sks. T he basi cu nitso fwo rkonMT
urkare called‘Human Intell igence T asks’ (HI Ts)
.MTurk wa sde-signedt oacco mmod ate tasks tha tar
edifficult f orc ompute rs, butsimp lefo rpeople.Thi
sfacilit ates resea rchintohuman compu tation ,wh
er epeople ca n betreate d asa func tion call ( vonAhn ,2
005 ; Littl eetal .,2 009;Quinn and Bed er son ,2011)
.Ithas ap pli-cati ontor esea rchareaslikehu man-co
mputer inter-a ct ion ( Bigha m etal.,20 10 ;Ber nstein
etal.,20 10),co mputervi sio n(Soroki nandF orsy
th ,200 8; Den get al.,201 0; Rash tchian et al., 2010
),speec hpro-c es sin, (Marg eeta l. ,2.1 0; Lan eetal.
,2010;Parent and Es kenazi,2 01 1; Es kenazi eta l.,20
13) ,andnatu -ral langua ge pro ce ssin g (Sno w etal.,20
et al.,2011). Burch and Dredze,2010; Laws
ork completed OnMTurk,researchers whoneedw
sareoftenre- are called ‘Requesters’,and wor ker
market, mean- ferred to as‘Turkers’. MTurkis atrue
ocomplete the ingthatTurkersare freetochooset
terscanprice HITs whichinterest them,andR eques
ctworkers and theirtaskscompetitivelyto trytoat tra
ietal.,2011; havetheir tasksdone quickly(Fa ri d a n
remain anony- Singer and Mittal,2011).Turkers
ccursthrough mous toRequesters,and allpaymento
eptsubmitted Amazon.Requesters are abletoacc
ettheirstan- work orreject workthatdoes notme
ester tsaccep dards.Turkersareonly paid if aRequ
their work.
icalTurk asan SeveralreportsexamineMe ch an
hdonvirta and economicmarket(Ipeirotis,2010a;Le
od uced MTurk, Ernkvist,2011).WhenAmazonintr
ncredits,and it firstofferedpayment onlyinAm azo
lars.More re- lateroffereddirectpayment inUS dol
eforeigncur- cently,ithasexpanded toincludeon
spaymentsbe- rency, the Indianrupee.Despiteit
azoncredits, ing limitedto two currenciesorAm
rkersfrom 190 MTurkclaimsover halfamillionwo
geststhatits countries (Amazon,2013). This sug
</bodyText>
<equation confidence="0.48997975">
it/tkphtl
diverse set workerpopulation shouldrepresenta
08;Callison-
languages.
</equation>
<bodyText confidence="0.998622125">
A demographic study by Ipeirotis (2010b) fo-
cused on age, gender, martial status, income lev-
els, motivation for working on MTurk, and whether
workers used it as a primary or supplemental form
of income. The study contrasted Indian and US
workers. Ross et al. (2010) completed a longitudi-
nal follow-on study. A number of other studies have
informally investigated Turkers’ language abilities.
Munro and Tily (2011) compiled survey responses
of 2,000 Turkers, revealing that four of the six most
represented languages come from India (the top six
being Hindi, Malayalam, Tamil, Spanish, French,
and Telugu). Irvine and Klementiev (2010) had
Turkers evaluate the accuracy of translations that
had been automatically inducted from monolingual
texts. They examined translations of 100 words in
42 low-resource languages, and reported geolocated
countries for their workers (India, the US, Romania,
Pakistan, Macedonia, Latvia, Bangladesh and the
Philippines). Irvine and Klementiev discussed the
difficulty of quality control and assessing the plausi-
bility of workers’ language skills for rare languages,
which we address in this paper.
Several researchers have investigated using
MTurk to build bilingual parallel corpora for ma-
chine translation, a task which stands to benefit
low cost, high volume translation on demand (Ger-
mann, 2001). Ambati et al. (2010) conducted a pilot
study by posting 25 sentences to MTurk for Span-
ish, Chinese, Hindi, Telugu, Urdu, and Haitian Cre-
ole. In a study of 2000 Urdu sentences, Zaidan
and Callison-Burch (2011) presented methods for
achieving professional-level translation quality from
Turkers by soliciting multiple English translations
of each foreign sentence. Zbib et al. (2012) used
crowdsourcing to construct a 1.5 million word par-
allel corpus of dialect Arabic and English, train-
ing a statistical machine translation system that pro-
duced higher quality translations of dialect Arabic
than a system a trained on 100 times more Mod-
ern Standard Arabic-English parallel data. Zbib et
al. (2013) conducted a systematic study that showed
that training an MT system on crowdsourced trans-
lations resulted in the same performance as training
on professional translations, at s the cost. Hu et
al. (2010; Hu et al. (2011) performed crowdsourced
translation by having monolingual speakers collab-
orate and iteratively improve MT output.
</bodyText>
<table confidence="0.999634416666667">
English 689 Tamil 253 Malayalam 219
Hindi 149 Spanish 131 Telugu 87
Chinese 86 Romanian 85 Portuguese 82
Arabic 74 Kannada 72 German 66
French 63 Polish 61 Urdu 56
Tagalog 54 Marathi 48 Russian 44
Italian 43 Bengali 41 Gujarati 39
Hebrew 38 Dutch 37 Turkish 35
Vietnamese 34 Macedonian 31 Cebuano 29
Swedish 26 Bulgarian 25 Swahili 23
Hungarian 23 Catalan 22 Thai 22
Lithuanian 21 Punjabi 21 Others ≤ 20
</table>
<tableCaption confidence="0.999315">
Table 1: Self-reported native language of 3,216
</tableCaption>
<bodyText confidence="0.9721732">
bilingual Turkers. Not shown are 49 languages with
≤20 speakers. We omit 1,801 Turkers who did not
report their native language, 243 who reported 2 na-
tive languages, and 83 with ≥3 native languages.
Several researchers have examined cost optimiza-
tion using active learning techniques to select the
most useful sentences or fragments to translate (Am-
bati and Vogel, 2010; Bloodgood and Callison-
Burch, 2010; Ambati, 2012).
To contrast our research with previous work, the
main contributions of this paper are: (1) a robust
methodology for assessing the bilingual skills of
anonymous workers, (2) the largest-scale census to
date of language skills of workers on MTurk, and (3)
a detailed analysis of the data gathered in our study.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="introduction">
3 Experimental Design
</sectionHeader>
<bodyText confidence="0.999402285714286">
The central task in this study was to investigate Me-
chanical Turk’s bilingual population. We accom-
plished this through self-reported surveys combined
with a HIT to translate individual words for 100
languages. We evaluate the accuracy of the work-
ers’ translations against known translations. In cases
where these were not exact matches, we used a sec-
ond pass monolingual HIT, which asked English
speakers to evaluate if a worker-provided translation
was a synonym of the known translation.
Demographic questionnaire At the start of each
HIT, Turkers were asked to complete a brief survey
about their language abilities. The survey asked the
following questions:
</bodyText>
<listItem confidence="0.999949">
• Is [language] your native language?
• How many years have you spoken [language]?
</listItem>
<page confidence="0.923885">
81
</page>
<listItem confidence="0.999953333333333">
• Is English your native language?
• How many years have you spoken English?
• What country do you live in?
</listItem>
<bodyText confidence="0.999976621621622">
We automatically collected each worker’s current lo-
cation by geolocating their IP address. A total of
5,281 unique workers completed our HITs. Of these,
3,625 provided answers to our survey questions, and
we were able to geolocate 5,043. Figure 1 plots
the location of workers across 106 countries. Table
1 gives the most common self-reported native lan-
guages.
Selection of languages We drew our data from the
different language versions of Wikipedia. We se-
lected the 100 languages with the largest number of
articles 1 (Table 2). For each language, we chose
the 1,000 most viewed articles over a 1 year period,2
and extracted the 10,000 most frequent words from
them. The resulting vocabularies served as the input
to our translation HIT.
Translation HIT For the translation task, we
asked Turkers to translate individual words. We
showed each word in the context of three sentences
that were drawn from Wikipedia. Turkers were al-
lowed to mark that they were unable to translate a
word. Each task contained 10 words, 8 of which
were words with unknown translations, and 2 of
which were quality control words with known trans-
lations. We gave special instruction for translat-
ing names of people and places, giving examples
of how to handle ‘Barack Obama’ and ‘Australia’
using their interlanguage links. For languages with
non-Latin alphabets, names were transliterated.
The task paid $0.15 for the translation of 10
words. Each set of 10 words was independently
translated by three separate workers. 5,281 workers
completed 256,604 translation assignments, totaling
more than 3 million words, over a period of three
and a half months.
Gold standard translations A set of gold stan-
dard translations were automatically harvested from
</bodyText>
<footnote confidence="0.98629075">
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
2http://dumps.wikimedia.org/other/
pagecounts-raw/
</footnote>
<table confidence="0.9992896">
500K+ ARTICLES: German (de), English (en), Spanish (es), French
(fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese
(pt), Russian (ru)
100K-500K ARTICLES: Arabic (ar), Bulgarian (bg), Catalan (ca),
Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa),
Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu),
Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwe-
gian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Ser-
bian (sr), Swedish (sv), Turkish (tr), UKrainian (UK), Vietnamese
(vi), Waray-Waray (war), Chinese (zh)
10K-100K ARTICLES: Afrikaans (af) Amharic (am) Asturian (ast)
Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri
(bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki
(diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati
(gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Geor-
gian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian
(lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi
(mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / Nepal
Bhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicil-
ian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili
(sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba
(yo)
&lt;10K ARTICLES: Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo)
Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali
(so) Uzbek (uz) Wolof (wo)
</table>
<tableCaption confidence="0.998088">
Table 2: A list of the languages that were used in our
</tableCaption>
<bodyText confidence="0.982294576923077">
study, grouped by the number of Wikipedia articles
in the language. Each language’s code is given in
parentheses. These language codes are used in other
figures throughout this paper.
Wikipedia for every language to use as embedded
controls. We used Wikipedia’s inter-language links
to pair titles of English articles with their corre-
sponding foreign article’s title. To get a more trans-
latable set of pairs, we excluded any pairs where: (1)
the English word was not present in the WordNet
ontology (Miller, 1995), (2) either article title was
longer than a single word, (3) the English Wikipedia
page was a subcategory of person or place, or (4)
the English and the foreign titles were identical or a
substring of the other.
Manual evaluation of non-identical translations
We counted all translations that exactly matched
the gold standard translation as correct. For non-
exact matches we created a second-pass quality as-
surance HIT. Turkers were shown a pair of En-
glish words, one of which was a Turker’s transla-
tion of the foreign word used for quality control,
and the other of which was the gold-standard trans-
lation of the foreign word. Evaluators were asked
whether the two words had the same meaning, and
chose between three answers: ‘Yes’, ‘No’, or ‘Re-
</bodyText>
<page confidence="0.970337">
82
</page>
<figure confidence="0.947961166666667">
1.0
0.8
0.6
0.4
0.2
0.0
</figure>
<table confidence="0.863925453333333">
pt
bs
sh
tl
it
sr
ro
es
ms
de
af
te
hr
id
da
nl
tr
gu
sk
fi
he
ml
fr
ja
pa
bg
mk
no
gl
ht
ga
sv
cy
lv
hu
kn
az
be
lt
ko
ne
eo
ar
pl
mr
ca
cs
sw
ta
hi
bn
nn
ka
so
zh
jv
el
ceb
vi
bcl
is
su
uz
lb
bpy
scn
new
ur
sd
br
ps
ru
am
wo
bo
</table>
<figureCaption confidence="0.909373333333333">
Figure 4: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the pro-
portion of translations which exactly matched gold standard translations, and light blue indicate translations
which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language.
</figureCaption>
<bodyText confidence="0.99996275">
sion and non-professional translations as Zaidan and
Callison-Burch (2011) did. Instead we evaluate the
quality of the data by using it to train SMT systems.
We present results in section 5.
</bodyText>
<sectionHeader confidence="0.987792" genericHeader="method">
4 Measuring Translation Quality
</sectionHeader>
<bodyText confidence="0.999635142857143">
For single word translations, we calculate the qual-
ity of translations on the level of individual assign-
ments and aggregated over workers and languages.
We define an assignment’s quality as the proportion
of controls that are correct in a given assignment,
where correct means exactly correct or judged to be
synonymous.
</bodyText>
<equation confidence="0.9431135">
�ki S(trij E syns[gj]) (1)
1
Quality(ai) =
ki j=1
</equation>
<bodyText confidence="0.9997916875">
where ai is the ith assignment, ki is the number of
controls in ai, trij is the Turker’s provided transla-
tion of control word j in assignment i, gj is the gold
standard translation of control word j, syns[gj] is
the set of words judged to be synonymous with gj
and includes gj, and S(x) is Kronecker’s delta and
takes value 1 when x is true. Most assignments had
two known words embedded, so most assignments
had scores of either 0, 0.5, or 1.
Since computing overall quality for a language as
the average assignment quality score is biased to-
wards a small number of highly active Turkers, we
instead report language quality scores as the aver-
age per-Turker quality, where a Turker’s quality is
the average quality of all the assignments that she
completed:
</bodyText>
<equation confidence="0.97009525">
�
Quality(ti) = aj Quality(a)
Eassigns [i] 7 (2)
 |assigns[i] |
</equation>
<bodyText confidence="0.999964666666667">
where assigns[i] is the assignments completed
by Turker i, and Quality(a) is as above.
Quality for a language is then given by
</bodyText>
<equation confidence="0.996594">
E
tj∈turkers[i] Quality(tj)
Quality(li) =
</equation>
<bodyText confidence="0.9978752">
Cheating using machine translation One obvi-
ous way for workers to cheat is to use available
online translation tools. Although we followed
best practices to deter copying-and-pasting into on-
line MT systems by rendering words and sentences
</bodyText>
<equation confidence="0.9767485">
(3)
 |turkers[i] |
</equation>
<bodyText confidence="0.997348">
When a Turker completed assignments in more than
one language, their quality was computed separately
for each language. Figure 4 shows the transla-
tion quality for languages with contributions from
at least 50 workers.
</bodyText>
<page confidence="0.991795">
84
</page>
<bodyText confidence="0.9997065">
as images (Zaidan and Callison-Burch, 2011), this
strategy does not prevent workers from typing the
words into an MT system if they are able to type in
the language’s script.
To identify and remove workers who appeared to
be cheating by using Google Translate, we calcu-
lated each worker’s overlap with the Google transla-
tions. We used Google to translate all 10,000 words
for the 51 foreign languages that Google Trans-
late covered at the time of the study. We mea-
sured the percent of workers’ translations that ex-
actly matched the translation returned from Google.
Figure 5a shows overlap between Turkers’s trans-
lations and Google Translate. When overlap is high,
it seems likely that those Turkers are cheating. It is
also reasonable to assume that honest workers will
overlap with Google some amount of the time as
Google’s translations are usually accurate. We di-
vide the workers into three groups: those with very
high overlap with Google (likely cheating by using
Google to translate words), those with reasonable
overlap, and those with no overlap (likely cheating
by other means, for instance, by submitting random
text).
Our gold-standard controls are designed to iden-
tify workers that fall into the third group (those who
are spamming or providing useless translations), but
they will not effectively flag workers who are cheat-
ing with Google Translate. We therefore remove the
500 Turkers with the highest overlap with Google.
This equates to removing all workers with greater
than 70% overlap. Figure 5b shows that removing
workers at or above the 70% threshold retains 90%
of the collected translations and over 90% of the
workers.
Quality scores reported throughout the paper re-
flect only translations from Turkers whose overlap
with Google falls below this 70% threshold.
</bodyText>
<sectionHeader confidence="0.987976" genericHeader="method">
5 Data Analysis
</sectionHeader>
<bodyText confidence="0.9986075">
We performed an analysis of our data to address the
following questions:
</bodyText>
<listItem confidence="0.9340096">
• Do workers accurately represent their language
abilities? Should we constrain tasks by region?
• How quickly can we expect work to be com-
pleted in a particular language?
(a) Individual workers’ overlap with Google Translate.
</listItem>
<bodyText confidence="0.843004454545455">
We removed the 500 workers with the highest overlap
(shaded region on the left) from our analyses, as it is rea-
sonable to assume these workers are cheating by submit-
ting translations from Google. Workers with no overlap
(shaded region on the right) are also likely to be cheating,
e.g. by submitting random text.
(b) Cumulative distribution of overlap with Google trans-
late for workers and translations. We see that eliminating
all workers with &gt;70% overlap with google translate still
preserves 90% of translations and &gt;90% of workers.
Figure 5
</bodyText>
<listItem confidence="0.999870666666667">
• Can Turkers’ translations be used to train MT
systems?
• Do our dictionaries improve MT quality?
</listItem>
<bodyText confidence="0.598608">
Language skills and location We measured the
average quality of workers who were in countries
that plausibly speak a language, versus workers from
countries that did not have large speaker populations
of that language. We used the Ethnologue (Lewis
</bodyText>
<page confidence="0.998433">
85
</page>
<table confidence="0.999818863636364">
Avg. Turker quality (# Ts) Primary locations Primary locations
In region Out of region of Turkers in region of Turkers out of region
Hindi 0.63 (296) 0.69 (7) India (284) UAE (5) UK (3) Saudi Arabia (2) Russia (1) Oman (1)
Tamil 0.65 (273) ** 0.25 (2) India (266) US (3) Canada (2) Tunisia (1) Egypt (1)
Malayalam 0.76 (234) 0.83 (2) India (223) UAE (6) US (3) Saudi Arabia (1) Maldives (1)
Spanish 0.81 (191) 0.84 (18) US (122) Mexico (16) Spain (14) India (15) New Zealand (1) Brazil (1)
French 0.75 (170) 0.82 (11) India (62) US (45) France (23) Greece (2) Netherlands (1) Japan (1)
Chinese 0.60 (116) 0.55 (21) US (75) Singapore (13) China (9) Hong Kong (6) Australia (3) Germany (2)
German 0.82 (91) 0.77 (41) Germany (48) US (25) Austria (7) India (34) Netherlands (1) Greece (1)
Italian 0.86 (90) * 0.80 (42) Italy (42) US (29) Romania (7) India (33) Ireland (2) Spain (2)
Amharic 0.14 (16) ** 0.01 (99) US (14) Ethiopia (2) India (70) Georgia (9) Macedonia (5)
Kannada 0.70 (105) NA (0) India (105)
Arabic 0.74 (60) ** 0.60 (45) Egypt (19) Jordan (16) Morocco (9) US (19) India (11) Canada (3)
Sindhi 0.19 (96) 0.06 (9) India (58) Pakistan (37) US (1) Macedonia (4) Georgia (2) Indonesia (2)
Portuguese 0.87 (101) 0.96 (3) Brazil (44) Portugal (31) US (15) Romania (1) Japan (1) Israel (1)
Turkish 0.76 (76) 0.80 (27) Turkey (38) US (18) Macedonia (8) India (19) Pakistan (4) Taiwan (1)
Telugu 0.80 (102) 0.50 (1) India (98) US (3) UAE (1) Saudi Arabia (1)
Irish 0.74 (54) 0.71 (47) US (39) Ireland (13) UK (2) India (36) Romania (5) Macedonia (2)
Swedish 0.73 (54) 0.71 (45) US (25) Sweden (22) Finland (3) India (23) Macedonia (6) Croatia (2)
Czech 0.71 (45) * 0.61 (50) US (17) Czech Republic (14) Serbia (5) Macedonia (22) India (10) UK (5)
Russian 0.15 (67) * 0.12 (27) US (36) Moldova (7) Russia (6) India (14) Macedonia (4) UK (3)
Breton 0.17 (3) 0.18 (89) US (3) India (83) Macedonia (2) China (1)
</table>
<tableCaption confidence="0.999099">
Table 3: Translation quality when partitioning the translations into two groups, one containing translations
</tableCaption>
<bodyText confidence="0.971409096153847">
submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the
other containing translations from Turkers outside those regions. In general, in-region Turkers provide
higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10.
et al., 2013) to compile the list of countries where
each language is spoken. Table 3 compares the av-
erage translation quality of assignments completed
within the region of each language, and compares it
to the quality of assignments completed outside that
region.
Our workers reported speaking 95 languages na-
tively. US workers alone reported 61 native lan-
guages. Overall, 4,297 workers were located in a
region likely to speak the language from which they
were translating, and 2,778 workers were located
in countries considered out of region (meaning that
about a third of our 5,281 Turkers completed HITs
in multiple languages).
Table 3 shows the differences in translation qual-
ity when computed using in-region versus out-of-
region Turkers, for the languages with the greatest
number of workers. Within region workers typi-
cally produced higher quality translations. Given the
number of Indian workers on Mechanical Turk, it
is unsurprising that they represent majority of out-
of-region workers. For the languages that had more
than 75 out of region workers (Malay, Amharic, Ice-
landic, Sicilian, Wolof, and Breton), Indian workers
represented at least 70% of the out of region workers
in each language.
A few languages stand out for having suspiciously
strong performance by out of region workers, no-
tably Irish and Swedish, for which out of region
workers account for a near equivalent volume and
quality of translations to the in region workers. This
is admittedly implausible, considering the relatively
small number of Irish speakers worldwide, and the
very low number living in the countries in which our
Turkers were based (primarily India). Such results
highlight the fact that cheating using online transla-
tion resources is a real problem, and despite our best
efforts to remove workers using Google Translate,
some cheating is still evident. Restricting to within
region workers is an effective way to reduce the
prevalence of cheating. We discuss the languages
which are best supported by true native speakers in
section 6.
Speed of translation Figure 2 gives the comple-
tion times for 40 languages. The 10 languages to
finish in the shortest amount of time were: Tamil,
Malayalam, Telugu, Hindi, Macedonian, Spanish,
Serbian, Romanian, Gujarati, and Marathi. Seven of
the ten fastest languages are from India, which is un-
</bodyText>
<page confidence="0.997104">
86
</page>
<figureCaption confidence="0.993554">
Figure 6: The total volume of translations (measured
in English words) as a function of elapsed days.
</figureCaption>
<table confidence="0.99946825">
language sentence English + dictionary
pairs foreign words entries
Bengali 22k 732k 22k
Hindi 40k 1,488k 22k
Malayalam 32k 863k 23k
Tamil 38k 916k 25k
Telugu 46k 1,097k 21k
Urdu 35k 1,356k 20k
</table>
<tableCaption confidence="0.883543">
Table 4: Size of parallel corpora and bilingual dic-
tionaries collected for each language.
</tableCaption>
<bodyText confidence="0.999478631578948">
surprising given the geographic distribution of work-
ers. Some languages follow the pattern of having a
smattering of assignments completed early, with the
rate picking up later.
Figure 6 gives the throughput of the full-sentence
translation task for the six Indian languages. The
fastest language was Malayalam, for which we col-
lected half a million words of translations in just un-
der a week. Table 4 gives the size of the data set that
we created for each of these languages.
Training SMT systems We trained statistical
translation models from the parallel corpora that we
created for the six Indian languages using the Joshua
machine translation system (Post et al., 2012). Table
5 shows the translation performance when trained
on the bitexts alone, and when incorporating the
bilingual dictionaries created in our earlier HIT. The
scores reflect the performance when tested on held
out sentences from the training data. Adding the dic-
</bodyText>
<table confidence="0.990913">
language trained on bitext + BLEU
bitexts alone dictionaries Δ
Bengali 12.03 17.29 5.26
Hindi 16.19 18.10 1.91
Malayalam 6.65 9.72 3.07
Tamil 8.08 9.66 1.58
Telugu 11.94 13.70 1.76
Urdu 19.22 21.98 2.76
</table>
<tableCaption confidence="0.699791">
Table 5: BLEU scores for translating into English
using bilingual parallel corpora by themselves, and
with the addition of single-word dictionaries. Scores
are calculated using four reference translations and
represent the mean of three MERT runs.
</tableCaption>
<bodyText confidence="0.9996421">
tionaries to the training set produces consistent per-
formance gains, ranging from 1 to 5 BLEU points.
This represents a substantial improvement. It is
worth noting, however, that while the source doc-
uments for the full sentences used for testing were
kept disjoint from those used for training, there is
overlap between the source materials for the dictio-
naries and those from the test set, since both the dic-
tionaries and the bitext source sentences were drawn
from Wikipedia.
</bodyText>
<sectionHeader confidence="0.998016" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999879095238095">
Crowdsourcing platforms like Mechanical Turk give
researchers instant access to a diverse set of bilin-
gual workers. This opens up exciting new avenues
for researchers to develop new multilingual systems.
The demographics reported in this study are likely to
shift over time. Amazon may expand its payments to
new currencies. Posting long-running HITs in other
languages may recruit more speakers of those lan-
guages. New crowdsourcing platforms may emerge.
The data presented here provides a valuable snap-
shot of the current state of MTurk, and the methods
used can be applied generally in future research.
Based on our study, we can confidently recom-
mend 13 languages as good candidates for research
now: Dutch, French, German, Gujarati, Italian, Kan-
nada, Malayalam, Portuguese, Romanian, Serbian,
Spanish, Tagalog, and Telugu. These languages
have large Turker populations who complete tasks
quickly and accurately. Table 6 summarizes the
strengths and weaknesses of all 100 languages cov-
ered in our study. Several other languages are viable
</bodyText>
<figure confidence="0.970999416666666">
400,000
200,000
700,000
600,000
500,000
300,000
800,000
100,000
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
Hindi
32
</figure>
<page confidence="0.994356">
87
</page>
<table confidence="0.999479860465116">
workers quality speed
many high fast Dutch, French, German, Gu-
jarati, Italian, Kannada, Malay-
alam, Portuguese, Romanian,
Serbian, Spanish, Tagalog, Tel-
ugu
slow Arabic, Hebrew, Irish, Punjabi,
Swedish, Turkish
low fast Hindi, Marathi, Tamil, Urdu
or
medium
slow Bengali, Bishnupriya Ma-
nipuri, Cebuano, Chinese,
Nepali, Newar, Polish, Russian,
Sindhi, Tibetan
few high fast Bosnia, Croatian, Macedonian,
Malay, Serbo-Croatian
slow Afrikaans, Albanian,
Aragonese, Asturian, Basque,
Belarusian, Bulgarian, Central
Bicolano, Czech, Danish,
Finnish, Galacian, Greek,
Haitian, Hungarian, Icelandic,
Ilokano, Indonesian, Japanese,
Javanese, Kapampangan,
Kazakh, Korean, Lithuanian,
Low Saxon, Malagasy, Nor-
wegian (Bokmal), Sicilian,
Slovak, Slovenian, Thai, UKra-
nian, Uzbek, Waray-Waray,
West Frisian, Yoruba
low fast –
or Amharic, Armenian, Azer-
medium baijani, Breton, Catalan,
Georgian, Latvian, Luxembour-
gish, Neapolitian, Norwegian
(Nynorsk), Pashto, Pied-
montese, Somali, Sudanese,
Swahili, Tatar, Vietnamese,
Walloon, Welsh
slow
none low or slow Esperanto, Ido, Kurdish, Per-
medium sian, Quechua, Wolof, Zazaki
</table>
<tableCaption confidence="0.95447">
Table 6: The green box shows the best languages to
</tableCaption>
<bodyText confidence="0.999356568181819">
target on MTurk. These languages have many work-
ers who generate high quality results quickly. We
defined many workers as 50 or more active in-region
workers, high quality as ≥70% accuracy on the gold
standard controls, and fast if all of the 10,000 words
were completed within two weeks.
candidates provided adequate quality control mech-
anisms are used to select good workers.
Since Mechanical Turk provides financial incen-
tives for participation, many workers attempt to
complete tasks even if they do not have the lan-
guage skills necessary to do so. Since MTurk does
not provide any information about workers demo-
graphics, including their language competencies, it
can be hard to exclude such workers. As a result
naive data collection on MTurk may result in noisy
data. A variety of techniques should be incorporated
into crowdsourcing pipelines to ensure high quality
data. As a best practice, we suggest: (1) restricting
workers to countries that plausibly speak the foreign
language of interest, (2) embedding gold standard
controls or administering language pretests, rather
than relying solely on self-reported language skills,
and (3) excluding workers whose translations have
high overlap with online machine translation sys-
tems like Google translate. If cheating using exter-
nal resources is likely, then also consider (4) record-
ing information like time spent on a HIT (cumulative
and on individual items), patterns in keystroke logs,
tab/window focus, etc.
Although our study targeted bilingual workers on
Mechanical Turk, and neglected monolingual work-
ers, we believe our results reliably represent the cur-
rent speaker populations, since the vast majority of
the work available on the crowdsourced platform
is currently English-only. We therefore assume the
number of non-English speakers is small. In the fu-
ture, it may be desirable to recruit monolingual for-
eign workers. In such cases, we recommend other
tests to validate their language abilities in place of
our translation test. These could include perform-
ing narrative cloze, or listening to audio files con-
taining speech in different language and identifying
their language.
</bodyText>
<sectionHeader confidence="0.984858" genericHeader="method">
7 Data release
</sectionHeader>
<bodyText confidence="0.999959">
With the publication of this paper, we are releasing
all data and code used in this study. Our data release
includes the raw data, along with bilingual dictionar-
ies that are filtered to be high quality. It will include
256,604 translation assignments from 5,281 Turkers
and 20,952 synonym assignments from 1,005 Turk-
ers, along with meta information like geolocation
</bodyText>
<page confidence="0.996112">
88
</page>
<bodyText confidence="0.999713428571429">
and time submitted, plus external dictionaries used
for validation. The dictionaries will contain 1.5M
total translated words in 100 languages, along with
code to filter the dictionaries based on different cri-
teria. The data also includes parallel corpora for six
Indian languages, ranging in size between 700,000
to 1.5 million words.
</bodyText>
<sectionHeader confidence="0.996891" genericHeader="conclusions">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999618071428571">
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled “Crowdsourcing Translation” (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing offi-
cial policies or endorsements by DARPA or the U.S.
Government. This research was supported by the
Johns Hopkins University Human Language Tech-
nology Center of Excellence and through gifts from
Microsoft and Google.
The authors would like to thank the anonymous
reviewers for their thoughtful comments, which sub-
stantially improved this paper.
</bodyText>
<sectionHeader confidence="0.995827" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999190394736842">
Amazon. 2013. Service summary tour for re-
questers on Amazon Mechanical Turk. https://
requester.mturk.com/tour.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon’s
Mechanical Turk. Association for Computational Lin-
guistics.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mellon
University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceed-
ings of the ACM Symposium on User Interface Soft-
ware and Technology (UIST).
Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-
tle, Andrew Miller, Robert C. Miller, Robin Miller,
Aubrey Tatarowicz, Brandyn White, Samual White,
and Tom Yeh. 2010. VizWiz: nearly real-time an-
swers to visual questions. In Proceedings of the ACM
Symposium on User Interface Software and Technol-
ogy (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon’s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 1–12, Los Angeles,
June. Association for Computational Linguistics.
Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010.
What does classifying more than 10,000 image cate-
gories tell us? In Proceedings of the 12th European
Conference of Computer Vision (ECCV, pages 71–84.
Maxine Eskenazi, Gina-Anne Levow, Helen Meng,
Gabriel Parent, and David Suendermann. 2013.
Crowdsourcing for Speech Processing, Applications to
Data Collection, Transcription and Assessment. Wi-
ley.
Siamak Faridani, Bj¨orn Hartmann, and Panagiotis G.
Ipeirotis. 2011. What’s the right price? pricing tasks
for finishing on time. In Third AAAI Human Compu-
tation Workshop (HCOMP’11).
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of ACM SIGKDD
Workshop on Human Computation (HCOMP).
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency sms messages. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 399–404, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Panagiotis G. Ipeirotis. 2010a. Analyzing the mechani-
cal turk marketplace. In ACM XRDS, December.
Panagiotis G. Ipeirotis. 2010b. Demographics of
Mechanical Turk. Technical Report Working paper
</reference>
<page confidence="0.994191">
89
</page>
<reference confidence="0.999803897196262">
CeDER-10-01, New York University, Stern School of
Business.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Workshop on Creating Speech and
Language Data with MTurk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora
via mechanical-turk. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon’s Mechanical Turk, Los An-
geles.
Florian Laws, Christian Scheible, and Hinrich Sch¨utze.
2011. Active learning with amazon mechanical turk.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland.
Matthew Lease, Jessica Hullman, Jeffrey P. Bigham,
Juho Kim Michael S. Bernstein and, Walter Lasecki,
Saeideh Bakhshi, Tanushree Mitra, and Robert C.
Miller. 2013. Mechanical Turk is not anony-
mous. http://dx.doi.org/10.2139/ssrn.
2228728.
Vili Lehdonvirta and Mirko Ernkvist. 2011. Knowl-
edge map of the virtual economy: Converting
the virtual economy into development potential.
http://www.infodev.org/en/Document.
1056.pdf, April. An InfoDev Publication.
M. Paul Lewis, Gary F. Simons, and Charles D. Fennig
(eds.). 2013. Ethnologue: Languages of the world,
seventeenth edition. http://www.ethnologue.
com.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ’09), Paris.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010. Using the Amazon Mechanical Turk
to transcribe and annotate meeting speech for extrac-
tive summarization. In Workshop on Creating Speech
and Language Data with MTurk.
George A. Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM, 38(11):39–41.
Robert Munro and Hal Tily. 2011. The start of the
art: Introduction to the workshop on crowdsourcing
technologies for language and cognition studies. In
Crowdsourcing Technologies for Language and Cog-
nition Studies, Boulder.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 207–215. Association for
Computational Linguistics.
Gabriel Parent and Maxine Eskenazi. 2011. Speaking
to the crowd: looking at past achievements in using
crowdsourcing for speech and predicting future chal-
lenges. In Proceedings Interspeech 2011, Special Ses-
sion on Crowdsourcing.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 401–409, Montr´eal, Canada, June. Association
for Computational Linguistics.
Alexander J. Quinn and Benjamin B. Bederson. 2011.
Human computation: A survey and taxonomy of a
growing field. In Computer Human Interaction (CHI).
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using Amazon’s Mechanical Turk. In Workshop on
Creating Speech and Language Data with MTurk.
Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zal-
divar, and Bill Tomlinson. 2010. Who are the crowd-
workers?: Shifting demographics in Amazon Mechan-
ical Turk. In alt.CHI session of CHI 2010 extended
abstracts on human factors in computing systems, At-
lanta, Georgia.
Yaron Singer and Manas Mittal. 2011. Pricing mecha-
nisms for online labor markets. In Third AAAI Human
Computation Workshop (HCOMP’11).
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with amazon mechanical turk. In First
IEEE Workshop on Internet Vision at CVPR.
Luis von Ahn. 2005. Human Computation. Ph.D. thesis,
School of Computer Science, Carnegie Mellon Uni-
versity, Pittsburgh, PA.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220–
1229. Association for Computational Linguistics.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of Arabic dialects. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.977263">
90
</page>
<reference confidence="0.998533625">
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013. Sys-
tematic comparison of professional and crowdsourced
reference translations for machine translation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta,
Georgia.
</reference>
<page confidence="0.997691">
91
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999787">The Language Demographics of Amazon Mechanical Turk</title>
<author confidence="0.997629">Matt Ann Dmitry Chris</author>
<affiliation confidence="0.9979455">and Information Science Department, University of Language Technology Center of Excellence, Johns Hopkins University</affiliation>
<abstract confidence="0.957640206896552">We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers’ selfreported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. 1 Overview Crowdsourcing is a promising new mechanism for collecting data for natural language processing research. Access to a fast, cheap, and flexible workforce allows us to collect new types of data, potentially enabling new language technologies. Because crowdsourcing platforms like Amazon Mechanical Turk (MTurk) give researchers access to a worldwide workforce, one obvious application of crowdsourcing is the creation of multilingual technologies. With an increasing number of active crowd workers located outside of the United States, there is even the potential to reach fluent speakers of lower resource languages. In this paper, we investigate the feasibility of hiring language informants on MTurk by conducting the first large-scale demographic study of the languages spoken by workers on the platform. There are several complicating factors when trying to take a census of workers on MTurk. The workers’ identities are anonymized, and Amazon provides no information about their countries of origin or their language abilities. Posting a simple survey to have workers report this information may be inadequate, since (a) many workers may never see the survey, (b) many opt not to do one-off surveys since potential payment is low, and (c) validating the answers of respondents is not straightforward. Our study establishes a methodology for determining the language demographics of anonymous crowd workers that is more robust than simple surveying. We ask workers what languages they speak and what country they live in, and validate their claims by measuring their ability to correctly translate words and by recording their geolocation. To increase the visibility and the desirability of our tasks, we post 1,000 assignments in each of 100 languages. These tasks each consist of translating 10 foreign words into English. Two of the 10 words have known translations, allowing us to validate that the workers’ translations are accurate. We construct bilingual dictionaries with up to 10,000 entries, with the majority of entries being new. Surveying thousands of workers allows us to analyze current speaker populations for 100 languages. 79 Transactions of the Association for Computational Linguistics, 2 (2014) 79–92. Action Editor: Mirella Lapata. 12/2013; Published 2/2014. Association for Computational Linguistics. 81,99 e1 :Thenum ber ofworker sper cou ntr y.Thismap was ge ne rated basedo nge ol ocating e IP addressof 4, 983 worker sinours tud y. Omitted are 60wo rkerswh ow erel ocat edi n moreth anonec tryduring the study, a nd2 38wor ker s w hocouldnot b ege oloc at ed. Thesize ofthe circl esr eprese sthenumbero fwor kersfrom eac hco untry.T het wolar ar eIndia(1 ,99 8wo rkers) andthe United St ates(866).To calibrate has 25, Russia has10,andS has4 .Theda ta al so allo wsusto ans werqu ionslike: Howq uicklyisw or k compl etedinagi venlanguage?Are crowdsourced translat ionsr eli ood?Howof tendoworkers misre present t heirla geabilitiesto obtainfinan ialrewardsB ackgrou ndan ’sMe chanica l T ur k(M Turk)isanon-lin ema rket plac efor w orkthatgi vesemployersan dresea rc h ess to a la rge,l ow-costwork -force .MTurkall ow semploy erstop rovidemicr o-paym ent s inretu rnfor worke rscomp letingmic ro-ta sks. T he basi cu nitso fwo rkonMT urkarecalled‘Human Intell igence T asks’ (HI Ts) .MTurkwa sde-signedt oacco mmod ate tasks tha tar forc ompute rs, butsimp lefo rpeople.Thi sfacilitates resea compu tation ,wh epeople can betreate d asa func call ( vonAhn ,2 005; Littl eetal .,2 and Bed er son ,2011) appli-cati ontor esea rchareaslikehu man-co mputerinter-a ct ion ( Bigha m etal.,20 10 etal.,2010),co mputervi sio nandF orsy ,200 8;Den get al.,201 0; et al., 2010 ),speechpro-c es sin, (Marg eeta l. 0; Lan eetal. and Es kenazi,2 01 1; eta l.,20 13),andnatu -ral langua ge pro ce g (Sno w etal.,20 al.,2011).and Laws completedOnMTurk,researchers whoneedw sareoftenre-are called wor ker mean-ferred to as‘Turkers’. atrue theingthatTurkersare freetochooset terscanpriceHITs them,andR eques andtheirtaskscompetitivelyto trytoat tra ietal.,2011;havetheir tasksdone ri d a n anony-and Mittal,2011).Turkers ccursthroughmous allpaymento eptsubmittedAmazon.Requesters are abletoacc ettheirstan-work orreject workthatdoes notme tsaccepdards.Turkersareonly paid if aRequ work. asanSeveralreportsexamineMe ch an and uced MTurk, ncredits,andit firstofferedpayment azo re-lateroffereddirectpayment inUS dol eforeigncur-cently,ithasexpanded toincludeon spaymentsbe-rency, the Indianrupee.Despiteit azoncredits,ing limitedto two 190MTurkclaimsover halfamillionwo geststhatits(Amazon,2013). This sug it/tkphtl setworkerpopulation shouldrepresenta 08;Callisonlanguages. A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training professional translations, at cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. English 689 Tamil 253 Malayalam 219 Hindi 149 Spanish 131 Telugu 87 Chinese 86 Romanian 85 Portuguese 82 Arabic 74 Kannada 72 German 66 French 63 Polish 61 Urdu 56 Tagalog 54 Marathi 48 Russian 44 Italian 43 Bengali 41 Gujarati 39 Hebrew 38 Dutch 37 Turkish 35 Vietnamese 34 Macedonian 31 Cebuano 29 Swedish 26 Bulgarian 25 Swahili 23 Hungarian 23 Catalan 22 Thai 22 Lithuanian 21 Punjabi 21 Others Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 nalanguages, and 83 with native languages. Several researchers have examined cost optimization using active learning techniques to select the most useful sentences or fragments to translate (Am-</abstract>
<note confidence="0.649165666666667">bati and Vogel, 2010; Bloodgood and Callison- Burch, 2010; Ambati, 2012). To contrast our research with previous work, the</note>
<abstract confidence="0.994537958904109">main contributions of this paper are: (1) a robust methodology for assessing the bilingual skills of anonymous workers, (2) the largest-scale census to date of language skills of workers on MTurk, and (3) a detailed analysis of the data gathered in our study. 3 Experimental Design The central task in this study was to investigate Mechanical Turk’s bilingual population. We accomplished this through self-reported surveys combined with a HIT to translate individual words for 100 languages. We evaluate the accuracy of the workers’ translations against known translations. In cases where these were not exact matches, we used a second pass monolingual HIT, which asked English speakers to evaluate if a worker-provided translation was a synonym of the known translation. questionnaire the start of each HIT, Turkers were asked to complete a brief survey about their language abilities. The survey asked the following questions: • Is [language] your native language? • How many years have you spoken [language]? 81 • Is English your native language? • How many years have you spoken English? • What country do you live in? We automatically collected each worker’s current location by geolocating their IP address. A total of 5,281 unique workers completed our HITs. Of these, 3,625 provided answers to our survey questions, and we were able to geolocate 5,043. Figure 1 plots the location of workers across 106 countries. Table 1 gives the most common self-reported native languages. of languages drew our data from the different language versions of Wikipedia. We selected the 100 languages with the largest number of 1(Table 2). For each language, we chose 1,000 most viewed articles over a 1 year and extracted the 10,000 most frequent words from them. The resulting vocabularies served as the input to our translation HIT. HIT the translation task, we asked Turkers to translate individual words. We showed each word in the context of three sentences that were drawn from Wikipedia. Turkers were allowed to mark that they were unable to translate a word. Each task contained 10 words, 8 of which were words with unknown translations, and 2 of which were quality control words with known translations. We gave special instruction for translating names of people and places, giving examples of how to handle ‘Barack Obama’ and ‘Australia’ using their interlanguage links. For languages with non-Latin alphabets, names were transliterated. The task paid $0.15 for the translation of 10 words. Each set of 10 words was independently translated by three separate workers. 5,281 workers completed 256,604 translation assignments, totaling more than 3 million words, over a period of three and a half months. standard translations set of gold standard translations were automatically harvested from Wikipedias pagecounts-raw/ German (de), English (en), Spanish (es), French (fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese (pt), Russian (ru) Arabic (ar), Bulgarian (bg), Catalan (ca), Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa), Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu), Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwe-gian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Ser-bian (sr), Swedish (sv), Turkish (tr), UKrainian (UK), Vietnamese (vi), Waray-Waray (war), Chinese (zh) Afrikaans (af) Amharic (am) Asturian (ast) Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri (bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki (diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati (gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Geor-gian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian (lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi (mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / Nepal Bhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicil-ian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili (sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba (yo) Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo) Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali (so) Uzbek (uz) Wolof (wo) Table 2: A list of the languages that were used in our study, grouped by the number of Wikipedia articles in the language. Each language’s code is given in parentheses. These language codes are used in other figures throughout this paper. Wikipedia for every language to use as embedded controls. We used Wikipedia’s inter-language links to pair titles of English articles with their corresponding foreign article’s title. To get a more translatable set of pairs, we excluded any pairs where: (1) the English word was not present in the WordNet ontology (Miller, 1995), (2) either article title was longer than a single word, (3) the English Wikipedia page was a subcategory of person or place, or (4) the English and the foreign titles were identical or a substring of the other. Manual evaluation of non-identical translations We counted all translations that exactly matched the gold standard translation as correct. For nonexact matches we created a second-pass quality assurance HIT. Turkers were shown a pair of English words, one of which was a Turker’s translation of the foreign word used for quality control, and the other of which was the gold-standard translation of the foreign word. Evaluators were asked whether the two words had the same meaning, and between three answers: ‘Yes’, ‘No’, or ‘Re- 82 1.0 0.8 0.6 0.4 0.2 0.0 pt bs sh tl it sr ro es ms de af te hr id da nl tr gu sk fi he ml fr ja pa bg mk no gl ht ga sv cy lv hu kn az be lt ko ne eo ar pl mr ca cs sw ta hi bn nn ka so zh jv el ceb vi bcl is su uz lb bpy scn new ur sd br ps ru am wo bo Figure 4: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the proportion of translations which exactly matched gold standard translations, and light blue indicate translations which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language. sion and non-professional translations as Zaidan and Callison-Burch (2011) did. Instead we evaluate the quality of the data by using it to train SMT systems. We present results in section 5. 4 Measuring Translation Quality For single word translations, we calculate the quality of translations on the level of individual assignments and aggregated over workers and languages. We define an assignment’s quality as the proportion of controls that are correct in a given assignment, where correct means exactly correct or judged to be synonymous. 1 = the assignment, the number of in the Turker’s provided translaof control word assignment the gold translation of control word set of words judged to be synonymous with includes and Kronecker’s delta and value 1 when true. Most assignments had two known words embedded, so most assignments had scores of either 0, 0.5, or 1. Since computing overall quality for a language as the average assignment quality score is biased towards a small number of highly active Turkers, we instead report language quality scores as the average per-Turker quality, where a Turker’s quality is the average quality of all the assignments that she completed: � = the assignments completed Turker and is as above. Quality for a language is then given by E = using machine translation obvious way for workers to cheat is to use available online translation tools. Although we followed best practices to deter copying-and-pasting into online MT systems by rendering words and sentences (3) When a Turker completed assignments in more than one language, their quality was computed separately for each language. Figure 4 shows the translation quality for languages with contributions from at least 50 workers. 84 as images (Zaidan and Callison-Burch, 2011), this strategy does not prevent workers from typing the words into an MT system if they are able to type in the language’s script. To identify and remove workers who appeared to be cheating by using Google Translate, we calculated each worker’s overlap with the Google translations. We used Google to translate all 10,000 words for the 51 foreign languages that Google Translate covered at the time of the study. We measured the percent of workers’ translations that exactly matched the translation returned from Google. Figure 5a shows overlap between Turkers’s translations and Google Translate. When overlap is high, it seems likely that those Turkers are cheating. It is also reasonable to assume that honest workers will overlap with Google some amount of the time as Google’s translations are usually accurate. We divide the workers into three groups: those with very high overlap with Google (likely cheating by using Google to translate words), those with reasonable overlap, and those with no overlap (likely cheating by other means, for instance, by submitting random text). Our gold-standard controls are designed to identify workers that fall into the third group (those who are spamming or providing useless translations), but they will not effectively flag workers who are cheating with Google Translate. We therefore remove the 500 Turkers with the highest overlap with Google. This equates to removing all workers with greater than 70% overlap. Figure 5b shows that removing workers at or above the 70% threshold retains 90% of the collected translations and over 90% of the workers. Quality scores reported throughout the paper reflect only translations from Turkers whose overlap with Google falls below this 70% threshold. 5 Data Analysis We performed an analysis of our data to address the following questions: • Do workers accurately represent their language abilities? Should we constrain tasks by region? • How quickly can we expect work to be completed in a particular language? (a) Individual workers’ overlap with Google Translate. We removed the 500 workers with the highest overlap (shaded region on the left) from our analyses, as it is reasonable to assume these workers are cheating by submitting translations from Google. Workers with no overlap (shaded region on the right) are also likely to be cheating, e.g. by submitting random text. (b) Cumulative distribution of overlap with Google translate for workers and translations. We see that eliminating workers with overlap with google translate still 90% of translations and of workers. Figure 5 • Can Turkers’ translations be used to train MT systems? • Do our dictionaries improve MT quality? skills and location measured the average quality of workers who were in countries that plausibly speak a language, versus workers from countries that did not have large speaker populations of that language. We used the Ethnologue (Lewis 85 Avg. Turker quality (# Ts) Primary locations of Turkers in region Primary locations of Turkers out of region In region Out of region</abstract>
<note confidence="0.98986915">Hindi 0.63 (296) India (284) UAE (5) UK (3) Saudi Arabia (2) Russia (1) Oman (1) Tamil ** 0.25 (2) India (266) US (3) Canada (2) Tunisia (1) Egypt (1) Malayalam 0.76 (234) India (223) UAE (6) US (3) Saudi Arabia (1) Maldives (1) Spanish 0.81 (191) US (122) Mexico (16) Spain (14) India (15) New Zealand (1) Brazil (1) French 0.75 (170) India (62) US (45) France (23) Greece (2) Netherlands (1) Japan (1) Chinese 0.55 (21) US (75) Singapore (13) China (9) Hong Kong (6) Australia (3) Germany (2) German 0.77 (41) Germany (48) US (25) Austria (7) India (34) Netherlands (1) Greece (1) Italian * 0.80 (42) Italy (42) US (29) Romania (7) India (33) Ireland (2) Spain (2) Amharic ** 0.01 (99) US (14) Ethiopia (2) India (70) Georgia (9) Macedonia (5) Kannada 0.70 (105) NA (0) India (105) Arabic ** 0.60 (45) Egypt (19) Jordan (16) Morocco (9) US (19) India (11) Canada (3) Sindhi 0.06 (9) India (58) Pakistan (37) US (1) Macedonia (4) Georgia (2) Indonesia (2) Portuguese 0.87 (101) Brazil (44) Portugal (31) US (15) Romania (1) Japan (1) Israel (1) Turkish 0.76 (76) Turkey (38) US (18) Macedonia (8) India (19) Pakistan (4) Taiwan (1) Telugu 0.50 (1) India (98) US (3) UAE (1) Saudi Arabia (1) Irish 0.71 (47) US (39) Ireland (13) UK (2) India (36) Romania (5) Macedonia (2) Swedish 0.71 (45) US (25) Sweden (22) Finland (3) India (23) Macedonia (6) Croatia (2) Czech * 0.61 (50) US (17) Czech Republic (14) Serbia (5) Macedonia (22) India (10) UK (5) Russian * 0.12 (27) US (36) Moldova (7) Russia (6) India (14) Macedonia (4) UK (3) Breton 0.17 (3) US (3) India (83) Macedonia (2) China (1)</note>
<abstract confidence="0.98061222">Table 3: Translation quality when partitioning the translations into two groups, one containing translations submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the other containing translations from Turkers outside those regions. In general, in-region Turkers provide higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10. et al., 2013) to compile the list of countries where each language is spoken. Table 3 compares the average translation quality of assignments completed within the region of each language, and compares it to the quality of assignments completed outside that region. Our workers reported speaking 95 languages natively. US workers alone reported 61 native languages. Overall, 4,297 workers were located in a region likely to speak the language from which they were translating, and 2,778 workers were located in countries considered out of region (meaning that about a third of our 5,281 Turkers completed HITs in multiple languages). Table 3 shows the differences in translation quality when computed using in-region versus out-ofregion Turkers, for the languages with the greatest number of workers. Within region workers typically produced higher quality translations. Given the number of Indian workers on Mechanical Turk, it is unsurprising that they represent majority of outof-region workers. For the languages that had more than 75 out of region workers (Malay, Amharic, Icelandic, Sicilian, Wolof, and Breton), Indian workers represented at least 70% of the out of region workers in each language. A few languages stand out for having suspiciously strong performance by out of region workers, notably Irish and Swedish, for which out of region workers account for a near equivalent volume and quality of translations to the in region workers. This is admittedly implausible, considering the relatively small number of Irish speakers worldwide, and the very low number living in the countries in which our Turkers were based (primarily India). Such results highlight the fact that cheating using online translation resources is a real problem, and despite our best efforts to remove workers using Google Translate, some cheating is still evident. Restricting to within region workers is an effective way to reduce the prevalence of cheating. We discuss the languages which are best supported by true native speakers in section 6. of translation 2 gives the completion times for 40 languages. The 10 languages to finish in the shortest amount of time were: Tamil,</abstract>
<keyword confidence="0.491077">Malayalam, Telugu, Hindi, Macedonian, Spanish, Serbian, Romanian, Gujarati, and Marathi. Seven of</keyword>
<abstract confidence="0.988847243589743">ten fastest languages are from India, which is un- 86 Figure 6: The total volume of translations (measured in English words) as a function of elapsed days. language sentence English + foreign words dictionary entries pairs Bengali 22k 732k 22k Hindi 40k 1,488k 22k Malayalam 32k 863k 23k Tamil 38k 916k 25k Telugu 46k 1,097k 21k Urdu 35k 1,356k 20k Table 4: Size of parallel corpora and bilingual dictionaries collected for each language. surprising given the geographic distribution of workers. Some languages follow the pattern of having a smattering of assignments completed early, with the rate picking up later. Figure 6 gives the throughput of the full-sentence translation task for the six Indian languages. The fastest language was Malayalam, for which we collected half a million words of translations in just under a week. Table 4 gives the size of the data set that we created for each of these languages. SMT systems trained statistical translation models from the parallel corpora that we created for the six Indian languages using the Joshua machine translation system (Post et al., 2012). Table 5 shows the translation performance when trained on the bitexts alone, and when incorporating the bilingual dictionaries created in our earlier HIT. The scores reflect the performance when tested on held sentences from the training data. Adding the diclanguage trained on bitext + BLEU Δ bitexts alone dictionaries Bengali 12.03 17.29 5.26 Hindi 16.19 18.10 1.91 Malayalam 6.65 9.72 3.07 Tamil 8.08 9.66 1.58 Telugu 11.94 13.70 1.76 Urdu 19.22 21.98 2.76 Table 5: BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-word dictionaries. Scores are calculated using four reference translations and represent the mean of three MERT runs. tionaries to the training set produces consistent performance gains, ranging from 1 to 5 BLEU points. This represents a substantial improvement. It is worth noting, however, that while the source documents for the full sentences used for testing were kept disjoint from those used for training, there is overlap between the source materials for the dictionaries and those from the test set, since both the dictionaries and the bitext source sentences were drawn from Wikipedia. 6 Discussion Crowdsourcing platforms like Mechanical Turk give researchers instant access to a diverse set of bilingual workers. This opens up exciting new avenues for researchers to develop new multilingual systems. The demographics reported in this study are likely to shift over time. Amazon may expand its payments to new currencies. Posting long-running HITs in other languages may recruit more speakers of those languages. New crowdsourcing platforms may emerge. The data presented here provides a valuable snapshot of the current state of MTurk, and the methods used can be applied generally in future research. Based on our study, we can confidently recommend 13 languages as good candidates for research now: Dutch, French, German, Gujarati, Italian, Kannada, Malayalam, Portuguese, Romanian, Serbian, Spanish, Tagalog, and Telugu. These languages have large Turker populations who complete tasks quickly and accurately. Table 6 summarizes the strengths and weaknesses of all 100 languages covered in our study. Several other languages are viable</abstract>
<address confidence="0.874040333333333">400,000 200,000 700,000 600,000 500,000 300,000 800,000 100,000 0</address>
<phone confidence="0.726524">0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30</phone>
<abstract confidence="0.814811777777778">Hindi 32 87 workers quality speed many high fast Dutch, French, German, Gu-jarati, Italian, Kannada, Malay-alam, Portuguese, Romanian, Serbian, Spanish, Tagalog, Tel-ugu slow Arabic, Hebrew, Irish, Punjabi, Swedish, Turkish low or fast Hindi, Marathi, Tamil, Urdu medium slow Bishnupriya Ma-</abstract>
<address confidence="0.6331775">nipuri, Cebuano, Nepali, Newar, Polish, Russian,</address>
<keyword confidence="0.802438636363636">Sindhi, Tibetan few high fast Bosnia, Croatian, Macedonian, Malay, Serbo-Croatian slow Afrikaans, Albanian, Aragonese, Asturian, Basque, Belarusian, Bulgarian, Central Bicolano, Czech, Danish, Finnish, Galacian, Haitian, Hungarian, Icelandic, Ilokano, Indonesian, Japanese, Javanese, Kapampangan, Kazakh, Korean, Saxon, Malagasy, Norwegian (Bokmal), Slovenian, Thai, UKranian, Uzbek, West Frisian, Yoruba low or fast – medium Armenian, Azerbaijani, Breton, Catalan, Georgian, Latvian, Luxembour-gish, Neapolitian, Norwegian Pashto, Piedmontese, Somali, Sudanese, Swahili, Tatar,</keyword>
<address confidence="0.798661">Walloon, Welsh</address>
<abstract confidence="0.985616119047619">slow none low or medium slow Esperanto, Ido, Kurdish, Per-sian, Quechua, Wolof, Zazaki Table 6: The green box shows the best languages to target on MTurk. These languages have many workers who generate high quality results quickly. We as 50 or more active in-region as accuracy on the gold controls, and all of the 10,000 words were completed within two weeks. candidates provided adequate quality control mechanisms are used to select good workers. Since Mechanical Turk provides financial incentives for participation, many workers attempt to complete tasks even if they do not have the language skills necessary to do so. Since MTurk does not provide any information about workers demographics, including their language competencies, it can be hard to exclude such workers. As a result naive data collection on MTurk may result in noisy data. A variety of techniques should be incorporated into crowdsourcing pipelines to ensure high quality data. As a best practice, we suggest: (1) restricting workers to countries that plausibly speak the foreign language of interest, (2) embedding gold standard controls or administering language pretests, rather than relying solely on self-reported language skills, and (3) excluding workers whose translations have high overlap with online machine translation systems like Google translate. If cheating using external resources is likely, then also consider (4) recording information like time spent on a HIT (cumulative and on individual items), patterns in keystroke logs, tab/window focus, etc. Although our study targeted bilingual workers on Mechanical Turk, and neglected monolingual workers, we believe our results reliably represent the current speaker populations, since the vast majority of the work available on the crowdsourced platform is currently English-only. We therefore assume the number of non-English speakers is small. In the future, it may be desirable to recruit monolingual foreign workers. In such cases, we recommend other tests to validate their language abilities in place of our translation test. These could include performing narrative cloze, or listening to audio files containing speech in different language and identifying their language. 7 Data release With the publication of this paper, we are releasing all data and code used in this study. Our data release includes the raw data, along with bilingual dictionaries that are filtered to be high quality. It will include 256,604 translation assignments from 5,281 Turkers and 20,952 synonym assignments from 1,005 Turkers, along with meta information like geolocation 88 and time submitted, plus external dictionaries used for validation. The dictionaries will contain 1.5M total translated words in 100 languages, along with code to filter the dictionaries based on different criteria. The data also includes parallel corpora for six Indian languages, ranging in size between 700,000 to 1.5 million words. 8 Acknowledgements This material is based on research sponsored by a DARPA Computer Science Study Panel phase 3 award entitled “Crowdsourcing Translation” (contract D12PC00368). The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements by DARPA or the U.S. Government. This research was supported by the Johns Hopkins University Human Language Technology Center of Excellence and through gifts from Microsoft and Google. The authors would like to thank the anonymous reviewers for their thoughtful comments, which substantially improved this paper. References 2013. Service summary tour for reon Amazon Mechanical Turk. Vamshi Ambati and Stephan Vogel. 2010. Can crowds build parallel corpora for machine translation systems? of the NAACL HLT 2010 Workshop on</abstract>
<note confidence="0.817605875">Creating Speech and Language Data with Amazon’s Association for Computational Linguistics. Vamshi Ambati, Stephan Vogel, and Jaime Carbonell. 2010. Active learning and crowd-sourcing for matranslation. In of the 7th International Conference on Language Resources and Evalu- Ambati. 2012. Learning and Crowd-</note>
<title confidence="0.944325">sourcing for Machine Translation in Low Resource</title>
<author confidence="0.926562">Ph D thesis</author>
<author confidence="0.926562">Language Technologies In-</author>
<affiliation confidence="0.976118">stitute, School of Computer Science, Carnegie Mellon</affiliation>
<address confidence="0.821762">University, Pittsburgh, PA.</address>
<author confidence="0.941631666666667">Soylent</author>
<affiliation confidence="0.405112">word processor with a crowd inside. In Proceedings of the ACM Symposium on User Interface Softand Technology</affiliation>
<author confidence="0.986009333333333">Jeffrey P Bigham</author>
<author confidence="0.986009333333333">Chandrika Jayant</author>
<author confidence="0.986009333333333">Hanjie Ji</author>
<author confidence="0.986009333333333">Greg Little</author>
<author confidence="0.986009333333333">Andrew Miller</author>
<author confidence="0.986009333333333">Robert C Miller</author>
<author confidence="0.986009333333333">Robin Miller</author>
<author confidence="0.986009333333333">Aubrey Tatarowicz</author>
<author confidence="0.986009333333333">Brandyn White</author>
<author confidence="0.986009333333333">Samual White</author>
<note confidence="0.681101875">and Tom Yeh. 2010. VizWiz: nearly real-time anto visual questions. In of the ACM Symposium on User Interface Software and Technol- Michael Bloodgood and Chris Callison-Burch. 2010. Large-scale cost-focused active learning for statistimachine translation. In of the 48th Annual Meeting of the Association for Computational Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical In of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Mechanical pages 1–12, Los Angeles, June. Association for Computational Linguistics. Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010. What does classifying more than 10,000 image catetell us? In of the 12th European of Computer Vision pages 71–84. Maxine Eskenazi, Gina-Anne Levow, Helen Meng, Gabriel Parent, and David Suendermann. 2013. Crowdsourcing for Speech Processing, Applications to Collection, Transcription and Wiley. Siamak Faridani, Bj¨orn Hartmann, and Panagiotis G. Ipeirotis. 2011. What’s the right price? pricing tasks finishing on time. In AAAI Human Compu- Workshop Ulrich Germann. 2001. Building a statistical machine translation system from scratch: How much bang for buck can we expect? In 2001 Workshop on Machine Toulouse, France. Chang Hu, Benjamin B. Bederson, and Philip Resnik. 2010. Translation by iterative collaboration between</note>
<title confidence="0.688112">users. In of ACM SIGKDD on Human Computation</title>
<author confidence="0.834588">Chang Hu</author>
<author confidence="0.834588">Philip Resnik</author>
<author confidence="0.834588">Yakov Kronrod</author>
<author confidence="0.834588">Vladimir Ei-</author>
<abstract confidence="0.8606232">delman, Olivia Buzek, and Benjamin B. Bederson. 2011. The value of monolingual crowdsourcing in a real-world translation scenario: Simulation using creole emergency sms messages. In Proceedings of the Sixth Workshop on Statistical Ma-</abstract>
<note confidence="0.8458565">pages 399–404, Edinburgh, Scotland, July. Association for Computational Linguistics. Panagiotis G. Ipeirotis. 2010a. Analyzing the mechaniturk marketplace. In December. Panagiotis G. Ipeirotis. 2010b. Demographics of Mechanical Turk. Technical Report Working paper 89 CeDER-10-01, New York University, Stern School of Business. Ann Irvine and Alexandre Klementiev. 2010. Using Me-</note>
<abstract confidence="0.819950583333333">chanical Turk to annotate lexicons for less commonly languages. In on Creating Speech and Data with Ian Lane, Matthias Eck, Kay Rottmann, and Alex Waibel. 2010. Tools for collecting speech corpora mechanical-turk. In of the NAACL HLT 2010 Workshop on Creating Speech and Lan- Data with Amazon’s Mechanical Los Angeles. Florian Laws, Christian Scheible, and Hinrich Sch¨utze. 2011. Active learning with amazon mechanical turk. of the 2011 Conference on Empirical</abstract>
<affiliation confidence="0.357978">in Natural Language Edinburgh,</affiliation>
<address confidence="0.682943">Scotland.</address>
<author confidence="0.834982">Matthew Lease</author>
<author confidence="0.834982">Jessica Hullman</author>
<author confidence="0.834982">Jeffrey P Bigham</author>
<author confidence="0.834982">Juho Kim Michael S Bernstein and</author>
<author confidence="0.834982">Walter Lasecki</author>
<note confidence="0.5727922">Saeideh Bakhshi, Tanushree Mitra, and Robert C. Miller. 2013. Mechanical Turk is not anony- Vili Lehdonvirta and Mirko Ernkvist. 2011. Knowledge map of the virtual economy: Converting the virtual economy into development potential.</note>
<web confidence="0.858537">http://www.infodev.org/en/Document.</web>
<title confidence="0.832823">April. An InfoDev Publication.</title>
<author confidence="0.863137">M Paul Lewis</author>
<author confidence="0.863137">Gary F Simons</author>
<author confidence="0.863137">Charles D Fennig</author>
<abstract confidence="0.836467">(eds.). 2013. Ethnologue: Languages of the world, edition.</abstract>
<author confidence="0.719631">Turkit Tools for iterative tasks on me-</author>
<title confidence="0.5927336">turk. In of the Workshop on Human Computation at the International Conference on Knowledge Discovery and Data Mining (KDD- Paris. Matthew Marge, Satanjeev Banerjee, and Alexander</title>
<author confidence="0.895012">Using the Amazon Mechanical Turk</author>
<title confidence="0.655789333333333">to transcribe and annotate meeting speech for extracsummarization. In on Creating Speech Language Data with</title>
<author confidence="0.69648">WordNet a lexical database for</author>
<note confidence="0.31305">of the 38(11):39–41. Robert Munro and Hal Tily. 2011. The start of the</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amazon</author>
</authors>
<title>Service summary tour for requesters on Amazon Mechanical Turk.</title>
<date>2013</date>
<note>https:// requester.mturk.com/tour.</note>
<marker>Amazon, 2013</marker>
<rawString>Amazon. 2013. Service summary tour for requesters on Amazon Mechanical Turk. https:// requester.mturk.com/tour.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
</authors>
<title>Can crowds build parallel corpora for machine translation systems?</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10053" citStr="Ambati and Vogel, 2010" startWordPosition="1537" endWordPosition="1541">alian 43 Bengali 41 Gujarati 39 Hebrew 38 Dutch 37 Turkish 35 Vietnamese 34 Macedonian 31 Cebuano 29 Swedish 26 Bulgarian 25 Swahili 23 Hungarian 23 Catalan 22 Thai 22 Lithuanian 21 Punjabi 21 Others ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with ≤20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with ≥3 native languages. Several researchers have examined cost optimization using active learning techniques to select the most useful sentences or fragments to translate (Ambati and Vogel, 2010; Bloodgood and CallisonBurch, 2010; Ambati, 2012). To contrast our research with previous work, the main contributions of this paper are: (1) a robust methodology for assessing the bilingual skills of anonymous workers, (2) the largest-scale census to date of language skills of workers on MTurk, and (3) a detailed analysis of the data gathered in our study. 3 Experimental Design The central task in this study was to investigate Mechanical Turk’s bilingual population. We accomplished this through self-reported surveys combined with a HIT to translate individual words for 100 languages. We eval</context>
</contexts>
<marker>Ambati, Vogel, 2010</marker>
<rawString>Vamshi Ambati and Stephan Vogel. 2010. Can crowds build parallel corpora for machine translation systems? In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
<author>Jaime Carbonell</author>
</authors>
<title>Active learning and crowd-sourcing for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="8221" citStr="Ambati et al. (2010)" startWordPosition="1242" endWordPosition="1245">ual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100</context>
</contexts>
<marker>Ambati, Vogel, Carbonell, 2010</marker>
<rawString>Vamshi Ambati, Stephan Vogel, and Jaime Carbonell. 2010. Active learning and crowd-sourcing for machine translation. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
</authors>
<title>Active Learning and Crowdsourcing for Machine Translation in Low Resource Scenarios.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Language Technologies Institute, School of Computer Science, Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="10103" citStr="Ambati, 2012" startWordPosition="1547" endWordPosition="1548">35 Vietnamese 34 Macedonian 31 Cebuano 29 Swedish 26 Bulgarian 25 Swahili 23 Hungarian 23 Catalan 22 Thai 22 Lithuanian 21 Punjabi 21 Others ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown are 49 languages with ≤20 speakers. We omit 1,801 Turkers who did not report their native language, 243 who reported 2 native languages, and 83 with ≥3 native languages. Several researchers have examined cost optimization using active learning techniques to select the most useful sentences or fragments to translate (Ambati and Vogel, 2010; Bloodgood and CallisonBurch, 2010; Ambati, 2012). To contrast our research with previous work, the main contributions of this paper are: (1) a robust methodology for assessing the bilingual skills of anonymous workers, (2) the largest-scale census to date of language skills of workers on MTurk, and (3) a detailed analysis of the data gathered in our study. 3 Experimental Design The central task in this study was to investigate Mechanical Turk’s bilingual population. We accomplished this through self-reported surveys combined with a HIT to translate individual words for 100 languages. We evaluate the accuracy of the workers’ translations aga</context>
</contexts>
<marker>Ambati, 2012</marker>
<rawString>Vamshi Ambati. 2012. Active Learning and Crowdsourcing for Machine Translation in Low Resource Scenarios. Ph.D. thesis, Language Technologies Institute, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael S Bernstein</author>
<author>Greg Little</author>
<author>Robert C Miller</author>
<author>Bjrn Hartmann</author>
<author>Mark S Ackerman</author>
<author>David R Karger</author>
<author>David Crowell</author>
<author>Katrina Panovich</author>
</authors>
<title>Soylent: a word processor with a crowd inside.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).</booktitle>
<marker>Bernstein, Little, Miller, Hartmann, Ackerman, Karger, Crowell, Panovich, 2010</marker>
<rawString>Michael S. Bernstein, Greg Little, Robert C. Miller, Bjrn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich. 2010. Soylent: a word processor with a crowd inside. In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey P Bigham</author>
<author>Chandrika Jayant</author>
<author>Hanjie Ji</author>
<author>Greg Little</author>
<author>Andrew Miller</author>
<author>Robert C Miller</author>
<author>Robin Miller</author>
<author>Aubrey Tatarowicz</author>
<author>Brandyn White</author>
<author>Samual White</author>
<author>Tom Yeh</author>
</authors>
<title>VizWiz: nearly real-time answers to visual questions.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).</booktitle>
<marker>Bigham, Jayant, Ji, Little, Miller, Miller, Miller, Tatarowicz, White, White, Yeh, 2010</marker>
<rawString>Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C. Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, and Tom Yeh. 2010. VizWiz: nearly real-time answers to visual questions. In Proceedings of the ACM Symposium on User Interface Software and Technology (UIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Large-scale cost-focused active learning for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010. Large-scale cost-focused active learning for statistical machine translation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles,</location>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12, Los Angeles, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Alexander Berg</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>What does classifying more than 10,000 image categories tell us?</title>
<date>2010</date>
<booktitle>In Proceedings of the 12th European Conference of Computer Vision (ECCV,</booktitle>
<pages>71--84</pages>
<marker>Deng, Berg, Li, Fei-Fei, 2010</marker>
<rawString>Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010. What does classifying more than 10,000 image categories tell us? In Proceedings of the 12th European Conference of Computer Vision (ECCV, pages 71–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxine Eskenazi</author>
<author>Gina-Anne Levow</author>
<author>Helen Meng</author>
<author>Gabriel Parent</author>
<author>David Suendermann</author>
</authors>
<title>Crowdsourcing for Speech Processing, Applications to Data Collection, Transcription and Assessment.</title>
<date>2013</date>
<publisher>Wiley.</publisher>
<marker>Eskenazi, Levow, Meng, Parent, Suendermann, 2013</marker>
<rawString>Maxine Eskenazi, Gina-Anne Levow, Helen Meng, Gabriel Parent, and David Suendermann. 2013. Crowdsourcing for Speech Processing, Applications to Data Collection, Transcription and Assessment. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siamak Faridani</author>
<author>Bj¨orn Hartmann</author>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>What’s the right price? pricing tasks for finishing on time.</title>
<date>2011</date>
<booktitle>In Third AAAI Human Computation Workshop (HCOMP’11).</booktitle>
<marker>Faridani, Hartmann, Ipeirotis, 2011</marker>
<rawString>Siamak Faridani, Bj¨orn Hartmann, and Panagiotis G. Ipeirotis. 2011. What’s the right price? pricing tasks for finishing on time. In Third AAAI Human Computation Workshop (HCOMP’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
</authors>
<title>Building a statistical machine translation system from scratch: How much bang for the buck can we expect?</title>
<date>2001</date>
<booktitle>In ACL 2001 Workshop on Data-Driven Machine Translation,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="8199" citStr="Germann, 2001" startWordPosition="1239" endWordPosition="1241">ed from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a s</context>
</contexts>
<marker>Germann, 2001</marker>
<rawString>Ulrich Germann. 2001. Building a statistical machine translation system from scratch: How much bang for the buck can we expect? In ACL 2001 Workshop on Data-Driven Machine Translation, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Hu</author>
<author>Benjamin B Bederson</author>
<author>Philip Resnik</author>
</authors>
<title>Translation by iterative collaboration between monolingual users.</title>
<date>2010</date>
<booktitle>In Proceedings of ACM SIGKDD Workshop on Human Computation (HCOMP).</booktitle>
<contexts>
<context position="9100" citStr="Hu et al. (2010" startWordPosition="1380" endWordPosition="1383"> Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at s the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. English 689 Tamil 253 Malayalam 219 Hindi 149 Spanish 131 Telugu 87 Chinese 86 Romanian 85 Portuguese 82 Arabic 74 Kannada 72 German 66 French 63 Polish 61 Urdu 56 Tagalog 54 Marathi 48 Russian 44 Italian 43 Bengali 41 Gujarati 39 Hebrew 38 Dutch 37 Turkish 35 Vietnamese 34 Macedonian 31 Cebuano 29 Swedish 26 Bulgarian 25 Swahili 23 Hungarian 23 Catalan 22 Thai 22 Lithuanian 21 Punjabi 21 Others ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turker</context>
</contexts>
<marker>Hu, Bederson, Resnik, 2010</marker>
<rawString>Chang Hu, Benjamin B. Bederson, and Philip Resnik. 2010. Translation by iterative collaboration between monolingual users. In Proceedings of ACM SIGKDD Workshop on Human Computation (HCOMP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Hu</author>
<author>Philip Resnik</author>
<author>Yakov Kronrod</author>
<author>Vladimir Eidelman</author>
<author>Olivia Buzek</author>
<author>Benjamin B Bederson</author>
</authors>
<title>The value of monolingual crowdsourcing in a real-world translation scenario: Simulation using haitian creole emergency sms messages.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>399--404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="9118" citStr="Hu et al. (2011)" startWordPosition="1384" endWordPosition="1387">iting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at s the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. English 689 Tamil 253 Malayalam 219 Hindi 149 Spanish 131 Telugu 87 Chinese 86 Romanian 85 Portuguese 82 Arabic 74 Kannada 72 German 66 French 63 Polish 61 Urdu 56 Tagalog 54 Marathi 48 Russian 44 Italian 43 Bengali 41 Gujarati 39 Hebrew 38 Dutch 37 Turkish 35 Vietnamese 34 Macedonian 31 Cebuano 29 Swedish 26 Bulgarian 25 Swahili 23 Hungarian 23 Catalan 22 Thai 22 Lithuanian 21 Punjabi 21 Others ≤ 20 Table 1: Self-reported native language of 3,216 bilingual Turkers. Not shown are 4</context>
</contexts>
<marker>Hu, Resnik, Kronrod, Eidelman, Buzek, Bederson, 2011</marker>
<rawString>Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Eidelman, Olivia Buzek, and Benjamin B. Bederson. 2011. The value of monolingual crowdsourcing in a real-world translation scenario: Simulation using haitian creole emergency sms messages. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 399–404, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Analyzing the mechanical turk marketplace.</title>
<date>2010</date>
<booktitle>In ACM XRDS,</booktitle>
<contexts>
<context position="6905" citStr="Ipeirotis (2010" startWordPosition="1046" endWordPosition="1047">areonly paid if aRequ their work. icalTurk asan SeveralreportsexamineMe ch an hdonvirta and economicmarket(Ipeirotis,2010a;Le od uced MTurk, Ernkvist,2011).WhenAmazonintr ncredits,and it firstofferedpayment onlyinAm azo lars.More re- lateroffereddirectpayment inUS dol eforeigncur- cently,ithasexpanded toincludeon spaymentsbe- rency, the Indianrupee.Despiteit azoncredits, ing limitedto two currenciesorAm rkersfrom 190 MTurkclaimsover halfamillionwo geststhatits countries (Amazon,2013). This sug it/tkphtl diverse set workerpopulation shouldrepresenta 08;Callisonlanguages. A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had </context>
</contexts>
<marker>Ipeirotis, 2010</marker>
<rawString>Panagiotis G. Ipeirotis. 2010a. Analyzing the mechanical turk marketplace. In ACM XRDS, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panagiotis G Ipeirotis</author>
</authors>
<title>Demographics of Mechanical Turk.</title>
<date>2010</date>
<tech>Technical Report Working paper CeDER-10-01,</tech>
<institution>York University, Stern School of Business.</institution>
<location>New</location>
<contexts>
<context position="6905" citStr="Ipeirotis (2010" startWordPosition="1046" endWordPosition="1047">areonly paid if aRequ their work. icalTurk asan SeveralreportsexamineMe ch an hdonvirta and economicmarket(Ipeirotis,2010a;Le od uced MTurk, Ernkvist,2011).WhenAmazonintr ncredits,and it firstofferedpayment onlyinAm azo lars.More re- lateroffereddirectpayment inUS dol eforeigncur- cently,ithasexpanded toincludeon spaymentsbe- rency, the Indianrupee.Despiteit azoncredits, ing limitedto two currenciesorAm rkersfrom 190 MTurkclaimsover halfamillionwo geststhatits countries (Amazon,2013). This sug it/tkphtl diverse set workerpopulation shouldrepresenta 08;Callisonlanguages. A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had </context>
</contexts>
<marker>Ipeirotis, 2010</marker>
<rawString>Panagiotis G. Ipeirotis. 2010b. Demographics of Mechanical Turk. Technical Report Working paper CeDER-10-01, New York University, Stern School of Business.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using Mechanical Turk to annotate lexicons for less commonly used languages.</title>
<date>2010</date>
<booktitle>In Workshop on Creating Speech and Language Data with MTurk.</booktitle>
<contexts>
<context position="7500" citStr="Irvine and Klementiev (2010)" startWordPosition="1137" endWordPosition="1140">ographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine tran</context>
</contexts>
<marker>Irvine, Klementiev, 2010</marker>
<rawString>Ann Irvine and Alexandre Klementiev. 2010. Using Mechanical Turk to annotate lexicons for less commonly used languages. In Workshop on Creating Speech and Language Data with MTurk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Lane</author>
<author>Matthias Eck</author>
<author>Kay Rottmann</author>
<author>Alex Waibel</author>
</authors>
<title>Tools for collecting speech corpora via mechanical-turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical</booktitle>
<location>Turk, Los Angeles.</location>
<marker>Lane, Eck, Rottmann, Waibel, 2010</marker>
<rawString>Ian Lane, Matthias Eck, Kay Rottmann, and Alex Waibel. 2010. Tools for collecting speech corpora via mechanical-turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Laws</author>
<author>Christian Scheible</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Active learning with amazon mechanical turk.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Edinburgh, Scotland.</location>
<marker>Laws, Scheible, Sch¨utze, 2011</marker>
<rawString>Florian Laws, Christian Scheible, and Hinrich Sch¨utze. 2011. Active learning with amazon mechanical turk. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Jessica Hullman</author>
<author>Jeffrey P Bigham</author>
<author>Juho Kim Michael S Bernstein and</author>
<author>Walter Lasecki</author>
<author>Saeideh Bakhshi</author>
<author>Tanushree Mitra</author>
<author>Robert C Miller</author>
</authors>
<title>Mechanical Turk is not anonymous.</title>
<date>2013</date>
<pages>10--2139</pages>
<marker>Lease, Hullman, Bigham, and, Lasecki, Bakhshi, Mitra, Miller, 2013</marker>
<rawString>Matthew Lease, Jessica Hullman, Jeffrey P. Bigham, Juho Kim Michael S. Bernstein and, Walter Lasecki, Saeideh Bakhshi, Tanushree Mitra, and Robert C. Miller. 2013. Mechanical Turk is not anonymous. http://dx.doi.org/10.2139/ssrn. 2228728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vili Lehdonvirta</author>
<author>Mirko Ernkvist</author>
</authors>
<title>Knowledge map of the virtual economy: Converting the virtual economy into development potential.</title>
<date>2011</date>
<tech>http://www.infodev.org/en/Document. 1056.pdf,</tech>
<marker>Lehdonvirta, Ernkvist, 2011</marker>
<rawString>Vili Lehdonvirta and Mirko Ernkvist. 2011. Knowledge map of the virtual economy: Converting the virtual economy into development potential. http://www.infodev.org/en/Document. 1056.pdf, April. An InfoDev Publication.</rawString>
</citation>
<citation valid="true">
<title>Ethnologue: Languages of the world, seventeenth edition.</title>
<date>2013</date>
<editor>M. Paul Lewis, Gary F. Simons, and Charles D. Fennig (eds.).</editor>
<note>http://www.ethnologue. com.</note>
<contexts>
<context position="8897" citStr="(2013)" startWordPosition="1350" endWordPosition="1350"> Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at s the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. English 689 Tamil 253 Malayalam 219 Hindi 149 Spanish 131 Telugu 87 Chinese 86 Romanian 85 Portuguese 82 Arabic 74 Kannada 72 German 66 French 63 Polish 61 Urdu 56 Tagalog 54 Marathi 48 Russian 44 Italian 43 Bengali 41 Gujarati 39 Hebrew 38 Dutch 37 Turkish 35 Viet</context>
</contexts>
<marker>2013</marker>
<rawString>M. Paul Lewis, Gary F. Simons, and Charles D. Fennig (eds.). 2013. Ethnologue: Languages of the world, seventeenth edition. http://www.ethnologue. com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Little</author>
<author>Lydia B Chilton</author>
<author>Rob Miller</author>
<author>Max Goldman</author>
</authors>
<title>Turkit: Tools for iterative tasks on mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Human Computation at the International Conference on Knowledge Discovery and Data Mining (KDDHCOMP ’09),</booktitle>
<location>Paris.</location>
<marker>Little, Chilton, Miller, Goldman, 2009</marker>
<rawString>Greg Little, Lydia B. Chilton, Rob Miller, and Max Goldman. 2009. Turkit: Tools for iterative tasks on mechanical turk. In Proceedings of the Workshop on Human Computation at the International Conference on Knowledge Discovery and Data Mining (KDDHCOMP ’09), Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Marge</author>
<author>Satanjeev Banerjee</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Using the Amazon Mechanical Turk to transcribe and annotate meeting speech for extractive summarization.</title>
<date>2010</date>
<booktitle>In Workshop on Creating Speech and Language Data with MTurk.</booktitle>
<marker>Marge, Banerjee, Rudnicky, 2010</marker>
<rawString>Matthew Marge, Satanjeev Banerjee, and Alexander Rudnicky. 2010. Using the Amazon Mechanical Turk to transcribe and annotate meeting speech for extractive summarization. In Workshop on Creating Speech and Language Data with MTurk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="15150" citStr="Miller, 1995" startWordPosition="2330" endWordPosition="2331">indhi (sd) Somali (so) Uzbek (uz) Wolof (wo) Table 2: A list of the languages that were used in our study, grouped by the number of Wikipedia articles in the language. Each language’s code is given in parentheses. These language codes are used in other figures throughout this paper. Wikipedia for every language to use as embedded controls. We used Wikipedia’s inter-language links to pair titles of English articles with their corresponding foreign article’s title. To get a more translatable set of pairs, we excluded any pairs where: (1) the English word was not present in the WordNet ontology (Miller, 1995), (2) either article title was longer than a single word, (3) the English Wikipedia page was a subcategory of person or place, or (4) the English and the foreign titles were identical or a substring of the other. Manual evaluation of non-identical translations We counted all translations that exactly matched the gold standard translation as correct. For nonexact matches we created a second-pass quality assurance HIT. Turkers were shown a pair of English words, one of which was a Turker’s translation of the foreign word used for quality control, and the other of which was the gold-standard tran</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Munro</author>
<author>Hal Tily</author>
</authors>
<title>The start of the art: Introduction to the workshop on crowdsourcing technologies for language and cognition studies.</title>
<date>2011</date>
<booktitle>In Crowdsourcing Technologies for Language and Cognition Studies,</booktitle>
<location>Boulder.</location>
<contexts>
<context position="7279" citStr="Munro and Tily (2011)" startWordPosition="1104" endWordPosition="1107">dits, ing limitedto two currenciesorAm rkersfrom 190 MTurkclaimsover halfamillionwo geststhatits countries (Amazon,2013). This sug it/tkphtl diverse set workerpopulation shouldrepresenta 08;Callisonlanguages. A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their workers (India, the US, Romania, Pakistan, Macedonia, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality </context>
</contexts>
<marker>Munro, Tily, 2011</marker>
<rawString>Robert Munro and Hal Tily. 2011. The start of the art: Introduction to the workshop on crowdsourcing technologies for language and cognition studies. In Crowdsourcing Technologies for Language and Cognition Studies, Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Novotney</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Cheap, fast and good enough: Automatic speech recognition with non-expert transcription.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>207--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Novotney, Callison-Burch, 2010</marker>
<rawString>Scott Novotney and Chris Callison-Burch. 2010. Cheap, fast and good enough: Automatic speech recognition with non-expert transcription. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 207–215. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Parent</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Speaking to the crowd: looking at past achievements in using crowdsourcing for speech and predicting future challenges.</title>
<date>2011</date>
<booktitle>In Proceedings Interspeech 2011, Special Session on Crowdsourcing.</booktitle>
<marker>Parent, Eskenazi, 2011</marker>
<rawString>Gabriel Parent and Maxine Eskenazi. 2011. Speaking to the crowd: looking at past achievements in using crowdsourcing for speech and predicting future challenges. In Proceedings Interspeech 2011, Special Session on Crowdsourcing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Constructing parallel corpora for six indian languages via crowdsourcing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>401--409</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="27259" citStr="Post et al., 2012" startWordPosition="4374" endWordPosition="4377">Some languages follow the pattern of having a smattering of assignments completed early, with the rate picking up later. Figure 6 gives the throughput of the full-sentence translation task for the six Indian languages. The fastest language was Malayalam, for which we collected half a million words of translations in just under a week. Table 4 gives the size of the data set that we created for each of these languages. Training SMT systems We trained statistical translation models from the parallel corpora that we created for the six Indian languages using the Joshua machine translation system (Post et al., 2012). Table 5 shows the translation performance when trained on the bitexts alone, and when incorporating the bilingual dictionaries created in our earlier HIT. The scores reflect the performance when tested on held out sentences from the training data. Adding the diclanguage trained on bitext + BLEU bitexts alone dictionaries Δ Bengali 12.03 17.29 5.26 Hindi 16.19 18.10 1.91 Malayalam 6.65 9.72 3.07 Tamil 8.08 9.66 1.58 Telugu 11.94 13.70 1.76 Urdu 19.22 21.98 2.76 Table 5: BLEU scores for translating into English using bilingual parallel corpora by themselves, and with the addition of single-wor</context>
</contexts>
<marker>Post, Callison-Burch, Osborne, 2012</marker>
<rawString>Matt Post, Chris Callison-Burch, and Miles Osborne. 2012. Constructing parallel corpora for six indian languages via crowdsourcing. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 401–409, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander J Quinn</author>
<author>Benjamin B Bederson</author>
</authors>
<title>Human computation: A survey and taxonomy of a growing field.</title>
<date>2011</date>
<booktitle>In Computer Human Interaction (CHI).</booktitle>
<marker>Quinn, Bederson, 2011</marker>
<rawString>Alexander J. Quinn and Benjamin B. Bederson. 2011. Human computation: A survey and taxonomy of a growing field. In Computer Human Interaction (CHI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Workshop on Creating Speech and Language Data with MTurk.</booktitle>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using Amazon’s Mechanical Turk. In Workshop on Creating Speech and Language Data with MTurk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Ross</author>
<author>Lilly Irani</author>
<author>M Six Silberman</author>
<author>Andrew Zaldivar</author>
<author>Bill Tomlinson</author>
</authors>
<title>Who are the crowdworkers?: Shifting demographics in Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In alt.CHI session of CHI</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="7131" citStr="Ross et al. (2010)" startWordPosition="1083" endWordPosition="1086">s.More re- lateroffereddirectpayment inUS dol eforeigncur- cently,ithasexpanded toincludeon spaymentsbe- rency, the Indianrupee.Despiteit azoncredits, ing limitedto two currenciesorAm rkersfrom 190 MTurkclaimsover halfamillionwo geststhatits countries (Amazon,2013). This sug it/tkphtl diverse set workerpopulation shouldrepresenta 08;Callisonlanguages. A demographic study by Ipeirotis (2010b) focused on age, gender, martial status, income levels, motivation for working on MTurk, and whether workers used it as a primary or supplemental form of income. The study contrasted Indian and US workers. Ross et al. (2010) completed a longitudinal follow-on study. A number of other studies have informally investigated Turkers’ language abilities. Munro and Tily (2011) compiled survey responses of 2,000 Turkers, revealing that four of the six most represented languages come from India (the top six being Hindi, Malayalam, Tamil, Spanish, French, and Telugu). Irvine and Klementiev (2010) had Turkers evaluate the accuracy of translations that had been automatically inducted from monolingual texts. They examined translations of 100 words in 42 low-resource languages, and reported geolocated countries for their worke</context>
</contexts>
<marker>Ross, Irani, Silberman, Zaldivar, Tomlinson, 2010</marker>
<rawString>Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zaldivar, and Bill Tomlinson. 2010. Who are the crowdworkers?: Shifting demographics in Amazon Mechanical Turk. In alt.CHI session of CHI 2010 extended abstracts on human factors in computing systems, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaron Singer</author>
<author>Manas Mittal</author>
</authors>
<title>Pricing mechanisms for online labor markets.</title>
<date>2011</date>
<booktitle>In Third AAAI Human Computation Workshop (HCOMP’11).</booktitle>
<marker>Singer, Mittal, 2011</marker>
<rawString>Yaron Singer and Manas Mittal. 2011. Pricing mechanisms for online labor markets. In Third AAAI Human Computation Workshop (HCOMP’11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast - but is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Sorokin</author>
<author>David Forsyth</author>
</authors>
<title>Utility data annotation with amazon mechanical turk.</title>
<date>2008</date>
<booktitle>In First IEEE Workshop on Internet Vision at CVPR.</booktitle>
<marker>Sorokin, Forsyth, 2008</marker>
<rawString>Alexander Sorokin and David Forsyth. 2008. Utility data annotation with amazon mechanical turk. In First IEEE Workshop on Internet Vision at CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
</authors>
<title>Human Computation.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<marker>von Ahn, 2005</marker>
<rawString>Luis von Ahn. 2005. Human Computation. Ph.D. thesis, School of Computer Science, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from nonprofessionals.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1220--1229</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="8409" citStr="Zaidan and Callison-Burch (2011)" startWordPosition="1274" endWordPosition="1277">a, Latvia, Bangladesh and the Philippines). Irvine and Klementiev discussed the difficulty of quality control and assessing the plausibility of workers’ language skills for rare languages, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in th</context>
<context position="16550" citStr="Zaidan and Callison-Burch (2011)" startWordPosition="2595" endWordPosition="2598">2 0.0 pt bs sh tl it sr ro es ms de af te hr id da nl tr gu sk fi he ml fr ja pa bg mk no gl ht ga sv cy lv hu kn az be lt ko ne eo ar pl mr ca cs sw ta hi bn nn ka so zh jv el ceb vi bcl is su uz lb bpy scn new ur sd br ps ru am wo bo Figure 4: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the proportion of translations which exactly matched gold standard translations, and light blue indicate translations which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language. sion and non-professional translations as Zaidan and Callison-Burch (2011) did. Instead we evaluate the quality of the data by using it to train SMT systems. We present results in section 5. 4 Measuring Translation Quality For single word translations, we calculate the quality of translations on the level of individual assignments and aggregated over workers and languages. We define an assignment’s quality as the proportion of controls that are correct in a given assignment, where correct means exactly correct or judged to be synonymous. �ki S(trij E syns[gj]) (1) 1 Quality(ai) = ki j=1 where ai is the ith assignment, ki is the number of controls in ai, trij is the </context>
<context position="18581" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="2936" endWordPosition="2939"> and Quality(a) is as above. Quality for a language is then given by E tj∈turkers[i] Quality(tj) Quality(li) = Cheating using machine translation One obvious way for workers to cheat is to use available online translation tools. Although we followed best practices to deter copying-and-pasting into online MT systems by rendering words and sentences (3) |turkers[i] | When a Turker completed assignments in more than one language, their quality was computed separately for each language. Figure 4 shows the translation quality for languages with contributions from at least 50 workers. 84 as images (Zaidan and Callison-Burch, 2011), this strategy does not prevent workers from typing the words into an MT system if they are able to type in the language’s script. To identify and remove workers who appeared to be cheating by using Google Translate, we calculated each worker’s overlap with the Google translations. We used Google to translate all 10,000 words for the 51 foreign languages that Google Translate covered at the time of the study. We measured the percent of workers’ translations that exactly matched the translation returned from Google. Figure 5a shows overlap between Turkers’s translations and Google Translate. W</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from nonprofessionals. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1220– 1229. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Erika Malchiodi</author>
<author>Jacob Devlin</author>
<author>David Stallard</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Machine translation of Arabic dialects.</title>
<date>2012</date>
<booktitle>In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8582" citStr="Zbib et al. (2012)" startWordPosition="1296" endWordPosition="1299">, which we address in this paper. Several researchers have investigated using MTurk to build bilingual parallel corpora for machine translation, a task which stands to benefit low cost, high volume translation on demand (Germann, 2001). Ambati et al. (2010) conducted a pilot study by posting 25 sentences to MTurk for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at s the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speake</context>
</contexts>
<marker>Zbib, Malchiodi, Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, Callison-Burch, 2012</marker>
<rawString>Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan, and Chris Callison-Burch. 2012. Machine translation of Arabic dialects. In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Gretchen Markiewicz</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Systematic comparison of professional and crowdsourced reference translations for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Atlanta,</location>
<contexts>
<context position="8897" citStr="Zbib et al. (2013)" startWordPosition="1347" endWordPosition="1350">for Spanish, Chinese, Hindi, Telugu, Urdu, and Haitian Creole. In a study of 2000 Urdu sentences, Zaidan and Callison-Burch (2011) presented methods for achieving professional-level translation quality from Turkers by soliciting multiple English translations of each foreign sentence. Zbib et al. (2012) used crowdsourcing to construct a 1.5 million word parallel corpus of dialect Arabic and English, training a statistical machine translation system that produced higher quality translations of dialect Arabic than a system a trained on 100 times more Modern Standard Arabic-English parallel data. Zbib et al. (2013) conducted a systematic study that showed that training an MT system on crowdsourced translations resulted in the same performance as training on professional translations, at s the cost. Hu et al. (2010; Hu et al. (2011) performed crowdsourced translation by having monolingual speakers collaborate and iteratively improve MT output. English 689 Tamil 253 Malayalam 219 Hindi 149 Spanish 131 Telugu 87 Chinese 86 Romanian 85 Portuguese 82 Arabic 74 Kannada 72 German 66 French 63 Polish 61 Urdu 56 Tagalog 54 Marathi 48 Russian 44 Italian 43 Bengali 41 Gujarati 39 Hebrew 38 Dutch 37 Turkish 35 Viet</context>
</contexts>
<marker>Zbib, Markiewicz, Matsoukas, Schwartz, Makhoul, 2013</marker>
<rawString>Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas, Richard Schwartz, and John Makhoul. 2013. Systematic comparison of professional and crowdsourced reference translations for machine translation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000047">
<title confidence="0.995925">
Crosslingual and Multilingual Construction
of Syntax-Based Vector Space Models
</title>
<author confidence="0.961121">
Jason Utt and Sebastian Padó
</author>
<affiliation confidence="0.6803605">
Institut für Maschinelle Sprachverarbeitung
Universität Stuttgart
</affiliation>
<email confidence="0.990466">
[uttjn|pado]@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.989675" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.939596523809524">
Syntax-based distributional models of lexical
semantics provide a flexible and linguistically
adequate representation of co-occurrence infor-
mation. However, their construction requires
large, accurately parsed corpora, which are un-
available for most languages.
In this paper, we develop a number of meth-
ods to overcome this obstacle. We describe
(a) a crosslingual approach that constructs a
syntax-based model for a new language requir-
ing only an English resource and a translation
lexicon; and (b) multilingual approaches that
combine crosslingual with monolingual infor-
mation, subject to availability. We evaluate
on two lexical semantic benchmarks in Ger-
man and Croatian. We find that the models
exhibit complementary profiles: crosslingual
models yield higher accuracies while monolin-
gual models provide better coverage. In addi-
tion, we show that simple multilingual models
can successfully combine their strengths.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999914163265306">
Building on the Distributional Hypothesis (Harris,
1954; Miller and Charles, 1991), which states that
words occurring in similar contexts are similar in
meaning, distributional semantic models (DSMs) rep-
resent a word’s meaning via its occurrence in context
in large corpora. Vector spaces, the most widely used
type of DSMs, represent words as vectors in a high-
dimensional space whose dimensions correspond to
features of the words’ contexts. Word spaces repre-
sent the simplest case of DSMs in which the dimen-
sions are simply the context words (Schütze, 1992).
A notable subclass of DSMs are syntax-based mod-
els (Lin, 1998; Baroni and Lenci, 2010) which use
(lexicalized) syntactic relations as dimensions. They
are able to model more fine-grained distinctions than
word spaces and have been found to be useful for
tasks such as selectional preference learning (Erk et
al., 2010), verb class induction (Schulte im Walde,
2006), analogical reasoning (Turney, 2006), and alter-
nation discovery (Joanis et al., 2006). Despite their
flexibility and usefulness, syntax-based DSMs are
used less often than word-based spaces. An impor-
tant reason is that their construction requires accurate
parsers, which are unavailable for many languages.
In addition, syntax-based DSMs are inherently more
sparse than word spaces, which calls for a large cor-
pus of well parsable data. It is thus not surprising
that besides English (Baroni and Lenci, 2010), only
few other languages possess large-scale syntax-based
DSMs (Padó and Utt, 2012; Šnajder et al., 2013).
This paper develops methods that take advantage
of the resource gradient between English and other
languages, exploiting the higher-quality resources of
the former to induce resources for target languages
among the latter, by translating the word-link-word
co-occurrences that underlie syntax-based DSMs.
This directly provides a crosslingual method to con-
struct syntax-based DSMs for target languages with-
out any target language data, requiring only an En-
glish syntax-based DSM and a translation lexicon.
Such lexicons are available for many language pairs,
and we outline a method to reduce ambiguity inherent
in such dictionaries. We describe a set of multilin-
gual methods that can combine corpus evidence from
English and the target language to further improve
the performance of the obtained DSM.
We consider two target languages, German and
Croatian, as examples of one close and one more
remote target language. For evaluation, we use two
</bodyText>
<page confidence="0.99224">
245
</page>
<tableCaption confidence="0.3555295">
Transactions of the Association for Computational Linguistics, 2 (2014) 245–258. Action Editor: Patrick Pantel.
Submitted 11/2013; Revised 3/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
</tableCaption>
<figure confidence="0.98310025">
abilityN
comp: 5
pushV
subj-1: 3 in: 1
obj-1: 10
initiativeN directionN
intendV
comp: 2
</figure>
<table confidence="0.779648285714286">
W x LW push WLxW (push, (push, (push, (push,
comp−1) in) obj−1) subj−1)
�comp−1, intend~ 2
�comp−1, ability~ 5 ability 5 0 0 0
�subj−1, initiative~ 3 direction 0 1 0 0
�obj−1, initiative~ 10 initiative 0 0 10 3
(in, direction) 1 intend 2 0 0 0
</table>
<figure confidence="0.814435">
(a) DM as graph (b) DM as W x LW matrix (c) DM as WL x W matrix
</figure>
<figureCaption confidence="0.999793">
Figure 1: Distributional Memory sample around to push represented as a graph (a) and two matrices (b, c)
</figureCaption>
<bodyText confidence="0.999976190476191">
tasks, namely synonym choice and semantic similar-
ity prediction. For both languages and tasks, mono-
lingually constructed DSMs can provide strong base-
lines. We find similar patterns across tasks and target
languages: the crosslingually constructed DSM can
be parametrized so that it becomes superior to an
existing monolingual DSM in quality, even if inferior
in coverage. A simple multilingual backoff can com-
bine the crosslingual model’s high quality with the
monolingual model’s high coverage.
Structure of the paper. We begin by sketching the
structure of Distributional Memory, a general frame-
work for syntax-based semantic spaces, in Section 2.
Our main contributions follow in Sections 3 and 4,
namely, a family of models for the crosslingual and
multilingual construction of DSMs. The second part
of the paper is concerned with evaluation. Section 5
describes our experimental setup after which we dis-
cuss our results for German (Section 6) and Croatian
(Section 7). The paper concludes with related work
(Section 8) and a general discussion (Section 9).
</bodyText>
<sectionHeader confidence="0.8307155" genericHeader="introduction">
2 Distributional Memory: A General
Model of Syntax-based Vector Spaces
</sectionHeader>
<subsectionHeader confidence="0.996855">
2.1 Motivation and Definition
</subsectionHeader>
<bodyText confidence="0.999702102040816">
Simple syntax-based DSMs represent target words
in terms of dimensions labeled with word-relation
pairs (Lin, 1998; Grefenstette, 1994). Unfortunately,
this representation only supports tasks that compare
pairs of words with regard to their meaning (e.g., in
synonymy detection or selectional preferences), but
not for tasks such as analogical reasoning, where sets
of word pairs are compared (Turney, 2006).
To unify syntax-based DSMs, Baroni and Lenci
(2010) proposed the Distributional Memory (DM)
model which captures distributional information at
the more general level of word-link-word triples,
stored as a third order co-occurrence tensor. The
DM tensor can be seen as a set of ordered word-
link-word tuples such as (pencil obj use) associated
with a scoring function a: W x L x W -+ R+ that
scores, for example, (pencil obj use) more highly
than (elephant obj use).
The DM tensor can be visualized as a directed
graph whose nodes are labeled with lemmas and
whose edges are labeled with links and scores. As
an example, Figure (1a) shows five links for the verb
push in the English DM, including subject, object,
prepositional adjunct, and governing verbs.
DSMs for individual tasks can be obtained by “ma-
tricizing” the tensor into two-dimensional matrices
corresponding to standard vector spaces. The matrix
in Figure (1b) shows the word by link-word space
(W x LW). It represents words w in terms of pairs
(l, w) of a link and a context word. This space models
similarity among words, e.g. for thesaurus construc-
tion (Lin, 1998). The example matrix in Figure (1c)
represents a word-link by word space (WL x W). It
characterizes pairs (w, l) through context words w,
which can be understood as selectional preferences.
DM does not assume a specific source for building
the graph. However, all existing DM resources were
extracted from large dependency-parsed corpora such
as UKWAC (Baroni et al., 2008). In the simplest case,
the set of labels L is (a subset of) the dependency
relations in the corpus, and the scoring function a is a
measure of association between the governor and the
dependent (see Baroni and Lenci (2010) for details).
However, the most robust DMs (including Baroni
and Lenci’s LexDM and TypeDM) use both syntactic
and lexicalized links, i.e. links which contain words
themselves, as well as surface form-based links, e.g.,
observed subject-verb-object triples in the corpus
lead to a (subject verb object) edge in the DM graph.
</bodyText>
<page confidence="0.996777">
246
</page>
<subsectionHeader confidence="0.939023">
2.2 DMs for Other Languages
</subsectionHeader>
<bodyText confidence="0.99982425">
Given the appealing properties of Distributional
Memory, it may be surprising that not many com-
parable resources exist for other languages. To our
knowledge, comparable resources exist only for Ger-
man (Padó and Utt, 2012) and Croatian (Šnajder et al.,
2013). Both studies replicate the monolingual DM
construction process outlined by Baroni and Lenci for
the respective languages. For German, the process is
relatively unproblematic, since German is relatively
well-equipped in terms of corpora and parsers. In
contrast, Šnajder et al. (2013) faced serious resource
scarcity while building a Croatian DM and had to
go to considerable lengths to clean a large web cor-
pus and to optimize the linguistic processing tools.
The resulting DM outperforms a monolingual con-
text word model for nouns and verbs, but performs
worse than the word-based model for (generally rarer)
adjectives. As a direct consequence, high-quality
syntax-based DSMs can only be constructed for a
limited set of languages.
</bodyText>
<sectionHeader confidence="0.988936" genericHeader="method">
3 Crosslingual Construction of DMs
</sectionHeader>
<subsectionHeader confidence="0.996714">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999399043478261">
As outlined in the previous section, there is a bottle-
neck in many languages regarding both large, clean
corpora as well as processing pipelines that result in
high-accuracy dependency parses. To address this
problem, we propose to induce Distributional Memo-
ries for such languages crosslingually by translating
a source language DM into the target language.
By adopting English as the source language we
can take advantage of the resource gradient, that is,
the higher maturity of English NLP techniques, such
as parsers, compared to most other languages. For
many languages, treebanks have become available
only within the last ten years (Buchholz and Marsi,
2006), if at all, while English has been at the fore-
front of NLP development for several decades, and a
number of highly accurate dependency parsers exist
(McDonald et al., 2005; Nivre, 2006). At the same
time, English arguably possesses the widest range of
large and well-cleaned corpora of any language.
To make our approaches applicable to as many
target languages as possible, we assume in this sec-
tion that very few resources for the target language
are available. The crosslingual methods we develop
</bodyText>
<figureCaption confidence="0.9047095">
Figure 2: Sample of the English-German dict.cc
dictionary; translations shown as dashed lines.
</figureCaption>
<bodyText confidence="0.999885576923077">
here work without any target language corpora, either
monolingual or bilingual. The only knowledge we
use is a simple translation lexicon, that is, a list of
translation pairs without translation probabilities, as
shown in Figure 2. Translation lexicons of this type
are arguably the most common bilingual resource
and accurate ones exist for virtually any language
pair (Soderland et al., 2009), even for languages with
few available corpora. Furthermore, such translation
lexicons are often crowdsourced and are available
for download. For example, the website dict.cc
provides numerous such lexicons for German and
English.
This approach promises in particular to yield mod-
els with a quality-coverage profile complementary
to that of monolingual models (Mohammad et al.,
2007; Peirsman and Padó, 2011): Crosslingual DMs
are extracted from source language corpora which
we assume to be parsed more accurately than target
language corpora. In addition, the translation pro-
cess can be designed to act as a further filtering step
(cf. Section 3.4 below), thus optimizing crosslingual
models for higher quality at the expense of cover-
age. In contrast, monolingual models – in particular
for under-resourced languages – often hit a quality
ceiling, but can generally guarantee high coverage.
</bodyText>
<subsectionHeader confidence="0.998839">
3.2 Translating DMs with Translation Lexicons
</subsectionHeader>
<bodyText confidence="0.999374272727273">
We conceptualize DM as a directed graph (see Fig-
ure 1), which allows us to phrase translation in graph
terms (Mihalcea and Radev, 2011). A DM is a triple
(V, E, a) where V is a set of vertices (i.e., the vocab-
ulary), E a set of typed edges between words, repre-
sented as word-link-word triples (cf. Section 2), and
a, an edge-weighting function. We will use S and T
to refer to the source and target language vocabular-
ies, respectively, and (VS, ES, aS) and (VT, ET, aT)
to denote source and target language DMs.
We can now ask how the shape of the graph
</bodyText>
<equation confidence="0.996165333333333">
groveN
copseN
forestN
timberN
woodN
HainN
GehšlzN
WaldN
HolzN
</equation>
<page confidence="0.993786">
247
</page>
<bodyText confidence="0.993778">
changes under translation. In an ideal world, a
translation lexicon would be a bijective function be-
tween the source and target language vocabularies:
Tr : S -+ T. Then, the transformation would merely
constitute a relabeling. We would then construct the
German DM graph by exchanging all English node
labels with German node labels, i.e., VT = T, and
creating a German edge for each English edge.1
</bodyText>
<subsectionHeader confidence="0.99949">
3.3 Ambiguity in Unfiltered Translation
</subsectionHeader>
<bodyText confidence="0.984696714285714">
The dictionary fragment in Figure 2 shows that trans-
lation is not bijective but a many-to-many relation.
In fact, taking the English–German dict.cc lex-
icon as an example, there is an average of 2.3 Ger-
man translations for each English lemma, and an
average of 1.9 English translations for each German
lemma. We model this situation using two functions:
Tr : S -+ 2T translates source words into sets of
target words, and Tr−1 : T -+ 2S translates target
words back into the source language.
The naive way to translate nodes using Tr is to use
all translations for a given word. Thus, for each edge
in the source DM between lemmas s1 and s2, we ob-
tain  |Tr(s1) |·  |Tr(s2) |edges in the target language:
</bodyText>
<equation confidence="0.9999195">
ET = {(t1,l, t2)  |](s1,l, s2) E ES :
t1 E Tr(s1) n t2 E Tr(s2)1 (1)
</equation>
<bodyText confidence="0.995512">
The score aT of a target edge is defined as the mean
of the scores of all source edges that map to it.
</bodyText>
<equation confidence="0.99316">
�aT (t1, l, t2) =
s1∈Tr−1(t1)
s2∈Tr−1(t2)
(2)
</equation>
<bodyText confidence="0.9999498">
We take the mean as it is less sensitive to outliers than
maximum or minimum. In addition, unlike taking the
sum, it is also automatically normalized regarding the
number of translations, thus penalizing words with
many unrelated senses.
A look at Figures 1 and 3, however, indicates that
this procedure overgenerates. This is problematic
on two levels. First, the target language graph will
contain a very large number of edges (e.g., using
dict.cc, the edge (text sbj_tr use) has 42 German
</bodyText>
<footnote confidence="0.972560333333333">
1We build on the assumption that dependency relations are
language-independent which, while incorrect, represents a rea-
sonable simplification (McDonald et al., 2013).
</footnote>
<figureCaption confidence="0.999693">
Figure 3: Unfiltered edge translation (EN–DE)
</figureCaption>
<bodyText confidence="0.998273294117647">
translations). Second, the correctness of the target
DM suffers. For some cases, such as copse – Gehölz,
Hain, the various translations are synonymous, and
Eq. (1) is appropriate. In other cases, multiple trans-
lations indicate lexical ambiguity of the source term.
For example, the two translations of wood correspond
to its senses as forest (Wald) and timber (Holz), re-
spectively. In such cases, Eq. (1) confuses the senses,
as the example in Figure 3 illustrates. The left-hand
side shows DM edges between wood and two adjec-
tival modifiers, namely precut (which is more plau-
sible for the timber sense) and great (which is more
plausible for the forest sense). The right-hand side
shows (part of) the German translations according
to Eq. (1): both Holz (timber) and Wald (forest) are
linked to both adjectives, leading to spurious edges
in the German DM.
</bodyText>
<subsectionHeader confidence="0.998293">
3.4 Filtering by Backtranslation
</subsectionHeader>
<bodyText confidence="0.999840105263158">
Since the nature of the translation is not indicated
in the translation lexicon, we exploit typical redun-
dancies in the source DM, which often contains
“quasi-synonymous” edges that express the same
relation with different words, e.g., (book obj read)
and (novel obj read). This allows us to score target
edge candidates by how well we can “backtranslate”
(Somers, 2005) them into the source language.
This idea is illustrated in Figure 4. We still
assume, as above, that wood has two translations,
but that precut has only one. For the English
edge (precut mod wood), we obtain two German
candidate edges, namely (zugeschnitten mod Holz)
and (zugeschnitten mod Wald). When back-
translating these candidates, the first one,
(zugeschnitten mod Wald), maps only onto the origi-
nal edge. The second one, (zugeschnitten mod Holz),
is backtranslated into a different source edge,
(precut mod timber), which makes it more probable.
</bodyText>
<figure confidence="0.932947857142857">
greatA
groKA
WaldN
woodN
HolzN
mod
mod mod
mod
zugeschnittenA
mod
mod
precutA
aS(s1, l, s2)
|Tr−1(t1) |·  |Tr−1(t2)|
</figure>
<page confidence="0.726525">
248
</page>
<figureCaption confidence="0.996631">
Figure 4: Backtranslation filtering. Original and
winning edges shown in boldface.
</figureCaption>
<table confidence="0.998904166666667">
Covered items
Model Corr. Cov.
DM.DE (AllL) .43 .60
DM.DE (SPrfL) .43 .60
DM.XL EN-+DE filter (AllL) .42 .61
DM.XL EN-+DE filter (SPrfL) .49 .49
</table>
<tableCaption confidence="0.730634">
Table 1: Coverage and Correlation (Pearson’s r)
for predicting word similarity, contrasting link types
(all links vs. selectional preference links)
</tableCaption>
<figure confidence="0.978021090909091">
forestN
mod
WaldN
HolzN
zugeschnittenA
mod
mod
mod
precutA
woodN
timberN
</figure>
<bodyText confidence="0.9998565">
We operationalize this by adding another condition
to Eq. (1), namely that target edges must be among
the highest-scoring edges for some source edge.
Recall that our target scores QT are already defined
in terms of source edge scores, so no redefinition of
the scoring function is necessary.
</bodyText>
<equation confidence="0.996552">
(3)
QT(t1, l, t2) = max QT(t, l, t&apos;), }
t∈Tr(s1)
t&apos;∈Tr(s2)
</equation>
<bodyText confidence="0.999863">
where QT(t,l, t&apos;) is the score as defined in Eq. 2.
This filtering scheme is fairly liberal: we do not limit
the number of target edges that a source edge can
translate to. A stricter variant could, e.g., abstain
from translating a source edge if no unique best edge
exists. We leave such variants to future work.
</bodyText>
<subsectionHeader confidence="0.991394">
3.5 Defining Similarity
</subsectionHeader>
<bodyText confidence="0.99998137037037">
Recall from Figure 1 that DM contains information
about both “incoming” as well as “outgoing” links.
Monolingually constructed DMs by default use all
of these relations since the information is reliable.
The situation is not as clear in a crosslingual setting.
Our intuition is that selectional preferences are most
informative and most likely to survive translation.
For example, for verbs we expect knowledge about
their arguments to be more informative than about
their governors. Conversely, for nouns we want to
use knowledge about the verbs that they occur with
rather than their arguments or modifiers.
We implement this idea by computing semantic
similarity between word vectors either on complete
vectors (condition “AllL”) or on a filtered version
that uses only inverse links for verbs and only regular
links for nouns and adjectives (condition “SPrfL”).
Table 1 shows the results of preliminary experiments
on a semantic similarity dataset (details in Section 5).
They bear out our hypothesis: in the monolingual
setting, there is almost no difference. Thus, in line
with previous work, we adopt (AllL) for DM.DE. In
contrast, we see a clear quality-coverage trade-off
in the crosslingual scenario, with a higher quality
for (SPrfL). Since this corresponds to our focus on
higher precision for crosslingual models, we will
adopt (SPrfL) for all crosslingual DMs.
</bodyText>
<sectionHeader confidence="0.99291" genericHeader="method">
4 Multilingual Construction of DMs
</sectionHeader>
<bodyText confidence="0.99908756">
The crosslingual models described in the previous
section do not use any corpus information from the
target language: As previously discussed, our ratio-
nale is to make the methods as widely applicable as
possible. However, this assumption may be too cau-
tious as more corpora and parsers continually become
available. In order to take advantage of such devel-
opments, this section discusses two simple methods
for combining monolingually and crosslingually con-
structed DMs, thereby combining corpus evidence
from both the source and the target language.
We concentrate on methods that can be applied
to DMs directly, e.g. by researchers who do not
have access to the source corpora. Moreover, we
combine not the graphs, but the resulting semantic
similarities.2 We take our inspiration from work on
combining and smoothing n-gram language mod-
els, where the usual operations are interpolation and
back-off (Chen and Goodman, 1998). Note that in
our case, the two models to be combined are assumed
to have complementary properties, with the monolin-
2We conducted experiments with graph merging but found
that the different topologies of the monolingual and crosslingual
DMs make it difficult to merge the graphs in a manner that
combines the information from both graphs.
</bodyText>
<equation confidence="0.999247">
ET = {(t1,l, t2) I I (s1,l, s2) E ES :
t1 E Tr(s1) ∧ t2 E Tr(s2)∧
</equation>
<page confidence="0.997754">
249
</page>
<figure confidence="0.997009666666667">
Demagoge
1 Miesmacher x
2 guter Redner x
3 skrupelloser
Hetzer
4 Meinungsforscher x
</figure>
<bodyText confidence="0.99926975">
gual model having higher coverage and the crosslin-
gual model higher quality (cf. Section 3.1). For
this reason, we assume that a linear interpolation of
the models’ similarities for each word pair will not
perform well. Our first strategy is a simple backoff
combination (DM.MULTI Backoff) that starts with
the crosslingual model and falls back to the mono-
lingual model in the case of zero-similarities. Our
second strategy follows the intuition that both noise
and sparse data tend to result in underestimated sim-
ilarities. This leads us to the DM.MULTI MaxSim
model: It takes the predictions from the monolingual
and crosslingual model and takes the higher one.
Both DM.MULTI variants combine predictions
from two models and implicitly assume that the pre-
dictions are drawn from the same score distribution.
Since this is not guaranteed, we standardize all scores
before combination, that is, we linearly transform the
values so that the resulting distribution has a mean of
0 and a standard deviation of 1.
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999973222222222">
To show the benefits of our crosslingual methods, we
perform experiments for the language pairs English–
German and English–Croatian. These languages ex-
emplify variability on the resource gradient: The
resource situation is best for English, still relatively
good for German, and most difficult for Croatian.
This section outlines the experiments for German;
Section 7 focuses on Croatian. We evaluate our mod-
els on two standard tasks from lexical semantics:
synonym choice and the prediction of human relat-
edness judgments. Even though these two tasks are
in-vitro, they are widely used for model selection in
distributional space models and we can compare the
results of our models against previous work. The
two tasks test how well the models can account for
two different aspects of lexical semantics, namely
a specific lexical relation (synonymy) and general
semantic relatedness.
</bodyText>
<subsectionHeader confidence="0.986725">
5.1 Tasks and Datasets
</subsectionHeader>
<bodyText confidence="0.9881655">
Our first task is synonym detection, where models
have to identify the true synonym for a target word
from four candidates. We use the German Reader’s
Digest Word Power (RDWP) dataset (Wallace and
</bodyText>
<figure confidence="0.971781333333333">
demagogue
grinch
able speaker
unscrupulous
agitator
pollster
(a) Task 1: synonym target with four candidates
Word Pair Similarity
Absage - ablehnen 3.5
(rejection - refuse)
Absage - Stellenanzeige 1.875
(rejection -job advertisement)
Affe - Gepäckkontrolle 0.125
(monkey - luggage inspection)
(b) Task 2: semantic similarity (range: 0–4)
</figure>
<tableCaption confidence="0.986823">
Table 2: Example items from evaluation tasks
</tableCaption>
<bodyText confidence="0.999617416666667">
Wallace, 2005) which contains 984 items.3 RDWP
is similar to the English TOEFL data (Landauer and
Dumais, 1997), but can contain short phrases among
the candidates (cf. example in Table 2a).
Our second evaluation tests how well the models
predict similarities for German word pairs including
closely related, somewhat related, and unrelated word
pairs (cf. Table 2b). We use the Gur350 dataset4
which contains 350 word pairs scored for relatedness
by native German taggers on a five-point Likert scale
between 0 (unrelated) and 4 (synonymous). Both
datasets contain nouns, verbs and adjectives.
</bodyText>
<subsectionHeader confidence="0.991287">
5.2 Procedure
</subsectionHeader>
<bodyText confidence="0.999992461538462">
Starting from a DM model, we matricize it into a
word by link-word space (W x LW) and compute
similarities between words with Cosine similarity. In
Exp. 1, we compute the semantic similarities of the
target with each candidate and predict the candidate
with the highest similarity to the target. For phrasal
candidates, we compute the similarity between the
target and all constituent words and take the maxi-
mum. We follow Mohammad et al. (2007) in assign-
ing partial credit to a model when the candidates of
a target are tied for maximal similarity. We evaluate
the models on Exp. 2 by calculating the strength of
the correlation between the model predictions and the
</bodyText>
<footnote confidence="0.9999365">
3Available from: http://goo.gl/PN42E
4Available from: http://goo.gl/3Dflf1
</footnote>
<page confidence="0.99288">
250
</page>
<bodyText confidence="0.998761">
human relatedness judgments. We use Pearson’s cor-
relation coefficient since it is the de facto evaluation
measure in relevant earlier work.5
On both tasks, we compare the models in two
conditions. In the first condition (“All”), models
are forced to make predictions for all items in the
dataset even if they have no information about the
item. In the second condition (“Covered”), models
are allowed to abstain in the case of zero similarities.
For Exp. 1, we report the accuracy (the number of
correctly recognized synonyms divided by the num-
ber of attempted problems) and coverage (the ratio
of items attempted; always 1 for the “All” condi-
tion). Items are considered covered if at least one
candidate has a non-zero similarity to the target. In
Exp. 2, we measure the correlation between the se-
mantic similarities and human judgments for word
pairs. Coverage is calculated as the percentage of
items with similarity greater 0.
Differences between models are tested for signifi-
cance using bootstrap resampling (Efron and Tibshi-
rani, 1993), always in the “All” condition.
</bodyText>
<subsectionHeader confidence="0.990343">
5.3 Models
</subsectionHeader>
<bodyText confidence="0.99988825">
We consider three types of DM models (monolingual,
crosslingual and multilingual), bag-of-words models
and a set of models proposed in the literature.
Monolingual model. We use DM.DE (Padó and
Utt, 2012), constructed from a 900M-token web cor-
pus, SDEWAC, parsed with MATE (Bohnet, 2010).6
As discussed in Section 3.5, we consider all links
(AllL) for the monolingual model.
Crosslingual models. The starting point for the
crosslingual models is Baroni and Lenci (2010)’s En-
glish TypeDM model extracted from approximately
3B tokens of Wikipedia and web corpus text parsed
with MaltParser (Nivre, 2006).7 DM.XL naive imple-
ments Eq. (1), and DM.XL filter implements Eq. (3).
As our translation lexicon, we use the community-
built English–German dict.cc online dictionary.8
</bodyText>
<footnote confidence="0.991306875">
5We note that since the data are not normally distributed,
a non-parametric correlation coefficient would be more appro-
priate. While we omitted them due to space limitations in this
paper, we will provide Spearman ρ results for all models online
at http://goo.gl/uxuffp.
6Available from http://goo.gl/H6gViT.
7Available from http://goo.gl/63ajCI.
8Available from http://goo.gl/re44Hg.
</footnote>
<table confidence="0.99990275">
Adj Noun Verbs Total
English 37K 78K 8K 123K
German 35L 99K 9K 143K
Translation pairs 77K 172K 28K 277K
</table>
<tableCaption confidence="0.999983">
Table 3: Size of the dict.cc dictionary
Table 4: Sizes of various DM resources
</tableCaption>
<bodyText confidence="0.9583495">
The statistics of the dictionary in Table 3 show that it
is quite large and covers many adjectives and nouns,
but relatively few verbs. We had to exclude much ver-
bal data due to ill-structured entries or phrasal entries.
Following Section 3.5, we only consider selectional
preference links (SPrfL) for the crosslingual model.
Multilingual models. We consider the two models
described in Section 4, namely DM.MULTI Backoff
and DM.MULTI MaxSim, each combining DM.DE
(AllL) with DM.XL filter (SPrfL).
Bag-of-words models. We build a standard BOW
model from the same German corpus SDEWAC used
for DM.DE. We assume a window of 10 context
words to the left and right. We use the top 10K most
frequent content words (nouns, adjectives, verbs and
adverbs) as dimensions. Our second BOW model
(BOW PCA500) was reduced to 500 dimensions by
applying principle component analysis, a technique
generally used to increase robustness to parameter
choice and to combat sparsity.9
Models from the literature. We compare our
models against the state of the art, represented by
the respective best models from two previous studies
(Zesch et al., 2007; Mohammad et al., 2007). They
comprise monolingual ontology-based models that
use GermaNet, (German) Wikipedia, or both (LinGN,
9We also built models using smaller context windows and
Latent Semantic Analysis (LSA, Landauer, 1997), both with
500 dimensions and with an automatically optimized number
of dimensions (Wild et al., 2008). Since these spaces did not
consistently yield better results than the reported models using
PCA, we do not report the results in detail.
</bodyText>
<figure confidence="0.976581357142857">
Class
monolingual
crosslingual &amp;
multilingual (DE)
Model
DM.DE (DE)
TYPEDM (EN)
DM.XL naive
DM.XL filter
Nodes Edges
3.5M 78M
31K 131M
63K 5B
63K 1.7B
</figure>
<page confidence="0.990862">
251
</page>
<table confidence="0.999200882352941">
Model All Covered
Acc Acc Cov
Baselines and word-based DSMs
1 Random .25 .25 1
2 Frequency .31 .31 1
3 BOW .46 .46 .98
4 BOW PCA500 .55 .55 .98
Syntax-based DSMs
5 DM.DE (AllL) .48 .53 .84
6 DM.XL EN→DE naive (SPrfL) .47 .63 .58
7 DM.XL EN→DE filter (SPrfL) .46 .61 .58
8 DM.MULTI Backoff(7,5) .54 .58 .89
9 DM.MULTI MaxSim(7,5) .55 .59 .89
Models from the literature
10 Lindist [MGHZ07] NA .52 .45
11 HPG [MGHZ07] NA .77 .22
12 JC [MGHZ07] NA .44 .36
</table>
<tableCaption confidence="0.87819925">
Table 5: Exp. 1: Accuracy and Coverage for synonym
choice on the Reader’s Digest Word Choice dataset.
MGHZ07: Mohammad et al. (2007). Best results for
each model class in bold.
</tableCaption>
<bodyText confidence="0.999753736842105">
HPG, JC, PL); and crosslingual distributional mod-
els that represent the meaning of German lemmas in
terms English thesaurus categories (Lindist).
DM model statistics. Table 4 shows the sizes of
the various DMs. The German and English monolin-
gual DMs are markedly different: the English DM
is much more compact, covering only 30K lemmas
while the German DM covers 3.5M lemmas, and at
the same time much denser. This discrepancy is due
to the larger English corpus and the inclusion of very
low-frequency items in DM.DE. The crosslingual
models created from TYPEDM cover 63K lemmas in
German, about twice the English coverage but still al-
most two orders of magnitude below the monolingual
DM.DE. They become very large: naive translation
increases the number of edges by a factor of 30, and
filtered translation still by a factor of 13. This means
filtering does reduce the size of the resulting DM, but
there is still considerable overgeneration.
</bodyText>
<sectionHeader confidence="0.996421" genericHeader="method">
6 Experimental Evaluation on German
</sectionHeader>
<bodyText confidence="0.999954333333333">
The experimental results for the two experiments are
shown in Tables 5 and 6, structured by model type.
We observe similar patterns for the two experiments.
</bodyText>
<table confidence="0.99914225">
Model All Covered
Corr Corr Cov
Baselines and word-based DSMs
1 Frequency .13 .13 1
2 BOW .20 .21 .97
3 BOW PCA500 .34 .37 .97
Syntax-based DSMs
4 DM.DE (AllL) [PU12] .38 .43 .60
5 DM.XL EN→DE naive (SPrfL) .28 .45 .49
6 DM.XL EN→DE filter (SPrfL) .33 .49 .49
7 DM.MULTI Backoff(6,4) .40 .45 .69
8 DM.MULTI MaxSim(6,4) .42 .47 .69
Models from the literature
9 LinGN [MGHZ07] NA .50 .26
10 Lindist [MGHZ07] NA .51 .26
11 JCGN+PLWP [ZGM07] NA .59 .33
</table>
<tableCaption confidence="0.984608">
Table 6: Exp. 2: Coverage and correlation (Pear-
</tableCaption>
<bodyText confidence="0.945827565217391">
son’s r) for predicting word similarity on the Gur350
dataset. MGHZ07: Mohammad et al. (2007)8,
ZGM07: Zesch et al. (2007)9, PU12: Padó and Utt
(2012). Best results for each model class in bold.
Baselines and word-based DSMs. In both cases,
uninformed baselines (random and frequency) per-
form badly. (In Exp. 1, the frequency baseline pre-
dicts the most frequent item as synonym; in Exp. 2,
it predicts min(f(w1), f(w2)).) In contrast, word-
based DSMs perform quite well, particularly the
dimensionality-reduced model (BOW PCA).
Syntax-based DSM. We see a consistent quality
versus coverage tradeoff among the different classes
of syntax-based DSMs. The monolingual DM.DE
model is significantly outperformed by the BOW
model on Exp. 1 (p&lt;0.01), but numerically outper-
forms it on Exp. 2 (difference not significant).
In both tasks, the crosslingual DM.XL models out-
perform both DM.DE and BOW PCA in terms of
quality: They achieve the numerically highest accu-
racy (and correlation, respectively) among all syntax-
based models. This high quality comes at a low cov-
erage, matching our intuitions about the profile of the
</bodyText>
<footnote confidence="0.878352142857143">
8Mohammad et al. (2007) do not provide coverage numbers
in their paper. We appreciate the support of Torsten Zesch and
Saif Mohammad in recovering the necessary information.
9Zesch et al. (2007) report results for the subset of Gur350 in
the intersection of GermaNet and Wikipedia. Thus, their models
may have higher coverage on the complete Gur350, but to our
knowledge these numbers have not been published.
</footnote>
<page confidence="0.99588">
252
</page>
<bodyText confidence="0.999584340425532">
crosslingual model. Filtering leads to a significant
improvement in Exp. 2 (p&lt;0.05) but not in Exp. 1.
The multilingual models (DM.MULTI) perform
even better. They nearly retain the quality of the
crosslingual models (accuracy of .59 vs. .63 for
Exp. 1, correlation of .47 vs. .49 for Exp. 2) but
attain higher coverage (89% in Exp. 1 and 69% in
Exp. 2) Notably, the coverage is even higher than that
of the DM.DE models, attesting to the complemen-
tarity of mono- and crosslingual information.
The differences among the DM.MULTI models are
small, but MaxSim does a little better and performs
best overall. In Exp. 1, it does significantly better in
the all-items evaluation than all other syntax-based
models (p&lt;0.01). The differences in Exp. 2 are only
significant at p&lt;0.05 for the model pair 6–8; we
attribute this to the smaller size of the dataset.
In sum, we can construct crosslingual DMs with-
out any use of target language corpora that mirror or
even exceed the performance of monolingual DMs.
If monolingual data is available, the combination
of corpus evidence provides a substantial advantage
over both monolingual and crosslingual models, even
for German, a language with large, relatively reliably
parsed corpora. Users can choose among different
models with different coverage/quality tradeoffs.
Comparison to models from the literature. Mod-
els from the literature are shown at the bottom of the
two tables. They generally obtain the highest ac-
curacy (or correlation, respectively), but only cover
a relatively small part of the datasets. In particu-
lar, the models with a quality higher than the DM
variants (11 in Exp. 1 and 10 and 11 in Exp. 2) ex-
hibit a coverage of less than half than that of the
DM.MULTI models. This appears to show the usual
trade-off between hand-constructed knowledge and
automatically acquired knowledge (Gildea and Ju-
rafsky, 2002). However, we can similarly bias our
DMs towards accuracy with the aid of a simple fre-
quency filter that only permits predictions for items
where all involved lemmas occur more frequently
in the German corpus than some threshold. Setting
these thresholds to match the coverage figures of the
best ontology-based models, DM.MULTI MaxSim al-
most reaches the ontology-based results: On Exp. 1,
for a coverage of .22 we obtain an accuracy of .68
(ontology-based model: .77), and on Exp. 2, we ob-
</bodyText>
<construct confidence="0.691526666666667">
Couscous (couscous), Albino (albino)
kursorisch (cursory), süffisant (smug)
erodieren (to erode), moussieren (to fizz)
</construct>
<tableCaption confidence="0.612722">
Table 7: Words of foreign origin better represented
by the multilingual model
</tableCaption>
<bodyText confidence="0.998965657894737">
tain a correlation of .60 (ontology-based model: .59)
at a coverage of .33.10 Thus, our DM models approx-
imate the quality of ontology-based models without
using any handcrafted resources.
Differences between Exp. 1 and 2. The two main
differences between the experiments are (a) the per-
formance of DM relative to the BOW baseline, and
(b) the impact of backtranslation filtering. In Exp. 1,
the BOW performs as well as DM.MULTI, and the
unfiltered DM.XL has a slight edge (2% accuracy)
over the filtered one. In contrast, in Exp. 2 filtering
leads to a major improvement and DM.MULTI does
substantially better than BOW PCA. Our analyses
attribute this difference to the nature of the two tasks
(cf. Section 5). Exp. 1 requires the recognition of
synonyms. Here, the main determinant of success
is whether the actual synonym receives the highest
similarity or not, irrespective of the margin to the
competing candidates. This margin does increase
from 0.09 in the naive to 0.11 in the filtered DM.XL,
but the overall number of correct predictions remains
almost unchanged. In contrast, Exp. 2 covers the
whole range from highly similar to unrelated word
pairs, and the correlation evaluation is sensitive to
the relative size of similarities produced by the DM
across many word pairs. The improvement we see
indicates that filtering improves the overall scaling
of the similarities, but this effect is masked by the
decision criterion in Exp. 1.
Qualitative analysis. Comparing DM.DE with
DM.MULTI, the question arises: can we further char-
acterize the benefits that the inclusion of crosslingual
corpus evidence confers to monolingual models? We
first inspected Exp. 1 for synonyms that were cor-
rectly recognized by DM.MULTI MaxSim but not
DM.DE, and found a large number of words of for-
eign origin (see Table 7). These words tend to be rare
in the German corpus in the form of technical, slang,
</bodyText>
<footnote confidence="0.9973075">
10We cannot provide significance tests since we do not have
item-wise predictions for the models from the literature.
</footnote>
<note confidence="0.52815">
Nouns
Adjectives
Verbs
</note>
<page confidence="0.997141">
253
</page>
<bodyText confidence="0.999958470588235">
or elevated register terms. Due to their low level
of ambiguity as well as the fact that their English
translations are often more frequent, the crosslingual
model represents them more sensibly.
We then inspected Exp. 2 in a similar way but
found it more difficult to identify salient improved
classes since the improvement is mostly in terms
of coverage. The data set for Exp. 2 includes
proper nouns, such as Berlin/Berlin-Kreuzberg,
Benedetto/Benedikt, which are unlikely to be cov-
ered by a translation lexicon. It also contains items
that encode world knowledge such as Ratzinger/Papst
(pope) which has a better chance of being covered
by target language corpora. This pair is not covered
by the DM.XL models, but the monolingual mod-
els (DM.DE, BOW, and BOW PCA) assign it the
similarities .23, .66, and .89, respectively.
</bodyText>
<sectionHeader confidence="0.983382" genericHeader="method">
7 Experimental Evaluation on Croatian
</sectionHeader>
<bodyText confidence="0.999788333333333">
Our third experiment considers a language that is
more different from English than German, namely
Croatian, a Slavic language. Available resources for
Croatian are more limited than for English or Ger-
man. Since syntactic analysis used to be a bottleneck,
the first syntax-based DSM for Croatian, DM.HR,
became available only last year (Šnajder et al., 2013).
As for evaluation datasets, there are no human similar-
ity judgments, but there is a synonym choice dataset
(CroSyn – see Karan et al. (2012) for details).
Thus, our Croatian evaluation is a synonym choice
task parallel to Exp. 1 for German. We take DM.HR
as the monolingual model which was built from a
dependency-parsed Croatian web corpus of 1.2B to-
kens. We construct a crosslingual model by starting
from Baroni and Lenci’s English TypeDM and using
Taktika Nova’s freely available English–Croatian dic-
tionary11 with 105K translation pairs. After remov-
ing entries with more than one word per language,
we were left with 95K pairs, considerably fewer than
for English–German. We apply the methods from
Section 3 for edge translation and filtering. The re-
sulting filtered Croatian DM.XL has 47K nodes and
315M edges, about one order of magnitude smaller
than the German crosslingual resource. Finally, we
combined DM.HR with the crosslingual DM (as in
Section 4) to obtain multilingual Croatian DMs.
</bodyText>
<footnote confidence="0.962007">
11Available from http://goo.gl/xHUjJH
</footnote>
<table confidence="0.997658">
Model All Covered
Acc Acc Cov
Word-based DSMs
1 BOW-LSA [SPA13] .66 .66 1
Syntax-based DSMs
2 DM.HR (AllL) .65 .65 .99
3 DM.XL EN→HR naive (SPrfL) .43 .50 .71
4 DM.XL EN→HR filter (SPrfL) .58 .71 .71
5 DM.MULTI Backoff(4,2) .69 .69 .99
6 DM.MULTI MaxSim(4,2) .70 .70 .99
</table>
<tableCaption confidence="0.995891">
Table 8: Experiment 3: Accuracy and Coverage for
</tableCaption>
<bodyText confidence="0.97991824">
synonym choice on the CroSyn dataset. SPA13: Šna-
jder et al. (2013). In boldface: best results.
Table 8 shows the results which correspond closely
to those for Exp. 1. A dimensionality-reduced BOW
space performs competitively with the monolingual
DM.HR (Šnajder et al., 2013). The crosslingual DM
is again able to improve accuracy over DM.HR (by
6%) but drops in coverage. Again, the multilingual
models perform best: DM.MULTI MaxSim loses only
1% accuracy compared to the crosslingual model but
achieves almost perfect coverage. The differences to
DM.HR and DM.XL are both significant (p&lt;0.01).12
The two major differences to the German synonym
choice task (Exp. 1) are that (a) filtering plays an
essential role for Croatian (increase in accuracy by
15%) and (b) DM.MULTI clearly outperforms the
BOW model. We attribute the difference to the semi-
automatic construction of the Croatian dataset from
machine-readable dictionaries. Overall, the results
for Croatian are encouraging. They demonstrate that
languages where parsing technology is still develop-
ing can in particular profit from cross- and multilin-
gual methods. This is true even for relatively small
translation dictionaries, matching previous results
from the literature (Peirsman and Padó, 2011).
</bodyText>
<sectionHeader confidence="0.999721" genericHeader="related work">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999962">
Given the resource gradient between English and
other languages, the crosslingual induction of linguis-
tic information has been an active topic of research.
Many studies use parallel corpora. Annotation pro-
jection (Yarowsky and Ngai, 2001) transfers source
language annotation directly onto target language
</bodyText>
<footnote confidence="0.9821085">
12We cannot provide significances for the BOW results be-
cause we again do not have per-item predictions.
</footnote>
<page confidence="0.9975">
254
</page>
<bodyText confidence="0.999957983333333">
sentence. It has been applied to various linguistic
levels such as POS tagging and syntax (Hi and Hwa,
2005; Hwa et al., 2005, among others). Other studies
use parallel data as indirect supervision for monolin-
gual tasks. Diab and Resnik (2002) use translations
as word sense labels; van der Plas and Tiedemann
(2006) exploit multilingual distributional semantics
for robust synonymy extraction. Naseem et al. (2009)
learn unsupervised POS taggers on multilingual paral-
lel data, exploiting the differences between languages
as soft constraints. Titov and Klementiev (2012) and
Kozhevnikov and Titov (2013) induce shallow se-
mantic parsers from parallel data. Klementiev et al.
(2012) approach document classification with multi-
task learning, inducing a multilingual DSM.
Since parallel corpora are not available in large
quantities, other studies use comparable corpora
which can provide additional features from the other
language. For example, Merlo et al. (2002) improve
English verb classification with new features derived
from Chinese translations. De Smet and Moens
(2009) learn multilingual topic models for news ag-
gregation. Peirsman and Padó (2011) use comparable
corpora to transfer selectional preferences and senti-
ment labels. Wikipedia can be seen as a particularly
rich type of comparable corpus with additional link
structure. It has been used to compute semantic re-
latedness (Navigli and Ponzetto, 2012; Navigli and
Ponzetto, 2010) and to compute conceptual document
representations for crosslingual information retrieval
(Potthast et al., 2008; Cimiano et al., 2009).
Our work, does not require parallel or comparable
corpora. We note, however, that translation lexicons
such as the ones we use can be extracted from compa-
rable corpora (Rapp, 1999; Vuli´c and Moens, 2012,
and many others), though few papers are concerned
with the translation at the level of semantic relations,
as we are. Similar in this respect is Fung and Chen
(2004), who translate FrameNet (Baker et al., 1998)
into Chinese with a bilingual ontology. They use
a relation-based pruning scheme that is somewhat
comparable to our backtranslation filtering.
To our knowledge, the most similar work to ours
is (Mohammad et al., 2007), which also considers
DSMs, albeit a different variety, namely concept-
based DSMs where targets are characterized in terms
of their distribution over categories of Roget’s the-
saurus. Like our work, their study creates crosslin-
gual DSMs for German using a translation lexicon.
It follows a different strategy, however: it collects co-
occurrence counts from a German corpus and trans-
lates the context dimensions into the English Roget
categories. Therefore, it crucially requires a large tar-
get language corpus, which our crosslingual methods
(Section 3) avoid. Its use of a target language corpus
resembles our multilingual methods (Section 4), but
unlike them, does not combine corpus evidence from
both languages. In sum, we believe that our methods
are more adaptable to different scenarios, being able
to use whatever data is available in either language.
</bodyText>
<sectionHeader confidence="0.997856" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999969275862069">
The appeal of syntax-based distributional spaces lies
in their promise of flexible and linguistically more
appropriate models for many phenomena in lexical
semantics. A major obstacle to their adoption for
novel languages has been the significantly higher
requirements on resources compared to word spaces.
In this paper, we have demonstrated that this ob-
stacle can be overcome by transferring English dis-
tributional information along the resource gradient
into target languages such as German and Croatian.
The simplest models, which are based solely on the
English Distributional Memory (DM) resource and a
translation lexicon, already beat monolingual DMs in
quality. These crosslingual models suffer from lower
coverage but can be combined with the monolingual
DM yielding a multilingual DM that maintains com-
petitive accuracy while achieving significantly higher
coverage than either individual model. The outcomes
of our experiments are mostly stable across the lan-
guages and tasks presented, which leads us to assume
the methodology successfully generalizes.13
Directions for future research include (a), more
stringent filtering of spurious edges in DM.XL mod-
els to make the graph topology more similar to mono-
lingual models and enable graph merging to obtain
unified multilingual models; (b), the extension of our
approach to more than two languages; (c), dimen-
sionality reduction for tensor-based DSMs both for
efficiency reasons and to improve performance.
</bodyText>
<footnote confidence="0.9458525">
13The German DMs are publicly available from http://
goo.gl/uxuffp.
</footnote>
<page confidence="0.997142">
255
</page>
<sectionHeader confidence="0.998059" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999606333333333">
We gratefully acknowledge partial funding of our
research by the DFG (SFB 732, Project D6) and the
EC (Project EXCITEMENT, FP7 ICT-287923).
</bodyText>
<sectionHeader confidence="0.990257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99965291">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceedings
of the joint Annual Meeting of the Association for Com-
putational Linguistics and International Conference
on Computational Linguistics, pages 86–90, Montréal,
QC.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):1–49.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2008. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed Web-
Crawled Corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, pages 89–97, Beijing, China.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computational
Natural Language Learning, pages 149–164, New York,
NY.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Center for Research
in Computing Technology, Harvard University.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit vs. latent con-
cept models for cross-language information retrieval.
In Proceedings of the International Joint Conference
on Artificial Intelligence, pages 1513–1518, Pasadena,
CA.
Wim De Smet and Marie-Francine Moens. 2009. Cross-
language linking of news stories on the web using inter-
lingual topic modelling. In Proceedings of the CIKM
Workshop on Social Web Search and Mining, pages
57–64, Hong Kong.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 255–262,
Philadelphia, PA.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York, NY.
Katrin Erk, Sebastian Padó, and Ulrike Padó. 2010. A
Flexible, Corpus-Driven Model of Regular and Inverse
Selectional Preferences. Computational Linguistics,
36(4):723–763.
Pascale Fung and Benfeng Chen. 2004. BiFrameNet:
Bilingual Frame Semantics Resources Construction by
crosslingual Induction. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
pages 931–935, Geneva, Switzerland.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245–288.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Boston/Norwell, MA.
Zelig S. Harris. 1954. Distributional structure. Word,
10(23):146–162.
Chenhai Hi and Rebecca Hwa. 2005. A Backoff Model
for Bootstrapping Resources for Non-English Lan-
guages. In Proceedings of the joint Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 851–
858, Vancouver, BC.
Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
Parsers via Syntactic Projection across Parallel Texts.
Journal of Natural Language Engineering, 11(3):311–
325.
Eric Joanis, Suzanne Stevenson, and David James. 2006.
A general feature space for automatic verb classifica-
tion. Natural Language Engineering, 14(03):337–367.
Mladen Karan, Jan Šnajder, and Bojana Dalbelo Baši´c.
2012. Distributional semantics approach to detecting
synonyms in Croatian language. In Proceedings of the
Eighth Language Technologies Conference, Ljubljana,
Slovenia.
Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.
2012. Inducing crosslingual distributed representations
of words. In Proceedings of the International Confer-
ence on Computational Linguistics, pages 1459–1474,
Mumbai, India.
Mikhail Kozhevnikov and Ivan Titov. 2013. Crosslingual
transfer of semantic role models. In Proceedings of the
51th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1190–1200, Sofia, Bulgaria.
Thomas K Landauer and Susan T Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211–240.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768–774, Montreal, QC.
</reference>
<page confidence="0.976119">
256
</page>
<reference confidence="0.999879527777778">
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of the
Conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, pages
523–530, Vancouver, BC.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Täckström, Claudia Bedini, Núria Bertomeu Castelló,
and Jungmee Lee. 2013. Universal dependency annota-
tion for multilingual parsing. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics, pages 92–97, Sofia, Bulgaria.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and Gi-
anluca Allaria. 2002. A multilingual paradigm for
automatic verb classification. In Proceedings of the
40th Annual Meeting on Association for Computational
Linguistics, pages 207–214, Philadelphia, PA.
Rada Mihalcea and Dragomir Radev. 2011. Graph-
based Natural Language Processing and Information
Retrieval. Cambridge University Press, Cambridge,
UK.
George A. Miller and Walter G. Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1–28.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Crosslingual distributional pro-
files of concepts for measuring semantic distance. In
Proceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 571–580,
Prague, Czech Republic.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual Part-of-Speech
Tagging : Two Unsupervised Approaches. Journal of
Artificial Intelligence Research, 36:1–45.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belNet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
216–225, Uppsala, Sweden.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belrelate! a joint multilingual approach to comput-
ing semantic relatedness. In Proceedings of the 26th
Conference on Artificial Intelligence, pages 108–114,
Toronto, ON.
Joakim Nivre. 2006. Inductive Dependency Parsing.
Springer, Dordrecht, Netherlands.
Sebastian Padó and Jason Utt. 2012. A distributional
memory for German. In Proceedings of the KONVENS
2012 workshop on recent developments and applica-
tions of lexical-semantic resources, pages 462–470, Vi-
enna, Austria.
Yves Peirsman and Sebastian Padó. 2011. Semantic
relations in bilingual lexicons. ACM Transactions in
Speech and Language Processing, 8(2):3:1–3:21.
Lonneke van der Plas and Jörg Tiedemann. 2006. Finding
synonyms using automatic word alignment and mea-
sures of distributional similarity. In Proceedings of
joint Annual Meeting of the Association for Compu-
tational Linguistics and International Conference on
Computational Linguistics, pages 866–873, Sydney,
Australia.
Martin Potthast, Benno Stein, and Maik Anderka. 2008.
A wikipedia-based multilingual retrieval model. In Pro-
ceedings of the European Conference on Information
Retrieval, pages 522–530, Glasgow, Scotland.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics on Com-
putational Linguistics, pages 519–526, College Park,
MD.
Sabine Schulte im Walde. 2006. Experiments on the
Automatic Induction of German Semantic Verb Classes.
Computational Linguistics, 32(2):159–194.
Hinrich Schütze. 1992. Dimensions of meaning. In
Proceedings of Supercomputing ’92, pages 787–796,
Minneapolis, MN.
Stephen Soderland, Oren Etzioni, Daniel S Weld, Michael
Skinner, Jeff Bilmes, et al. 2009. Compiling a Massive,
Multilingual Dictionary via Probabilistic Inference. In
Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing, pages
262–270, Suntec, Singapore.
Harold Somers. 2005. Round-trip translation: What is
it good for? In Proceedings of the Australasian Lan-
guage Technology Workshop, pages 127–133, Sydney,
Australia.
Jan Šnajder, Sebastian Padó, and Željko Agi´c. 2013.
Building and evaluating a distributional memory for
Croatian. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, pages
784–789, Sofia, Bulgaria.
Ivan Titov and Alexandre Klementiev. 2012. Crosslingual
induction of semantic roles. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 647–656, Jeju Island, South Korea.
Peter Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Ivan Vuli´c and Marie-Francine Moens. 2012. Detecting
highly confident word translations from comparable
corpora without any prior knowledge. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics, pages
449–459, Avignon, France.
</reference>
<page confidence="0.959374">
257
</page>
<reference confidence="0.999282333333333">
DeWitt Wallace and Lila Acheson Wallace. 2005.
Reader’s Digest, das Beste für Deutschland. Verlag
Das Beste, Stuttgart, Germany.
Fridolin Wild, Christina Stahl, Gerald Stermsek, and
Gustaf Neumann. 2008. Parameters driving effective-
ness of automated essay scoring with LSA. In Proceed-
ings of the 9th Computer-Aided Assessment Conference,
pages 485–494, Loughborough, UK.
David Yarowsky and Grace Ngai. 2001. Inducing Mul-
tilingual POS Taggers and NP Bracketers via Robust
Projection across Aligned Corpora. In Proceedings of
the 2nd Annual Meeting of the North American Chapter
of the Association for Computational Linguistics, pages
200–207, Pittsburgh, PA.
Torsten Zesch, Iryna Gurevych, and Max Mühlhäuser.
2007. Comparing Wikipedia and German Wordnet by
evaluating semantic relatedness on multiple datasets. In
Human Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 205–208, Rochester,
NY.
</reference>
<page confidence="0.996108">
258
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780836">
<title confidence="0.9872635">Crosslingual and Multilingual of Syntax-Based Vector Space Models</title>
<author confidence="0.913191">Utt</author>
<affiliation confidence="0.9351045">Institut für Maschinelle Universität</affiliation>
<email confidence="0.958503">[uttjn|pado]@ims.uni-stuttgart.de</email>
<abstract confidence="0.999742636363636">Syntax-based distributional models of lexical semantics provide a flexible and linguistically adequate representation of co-occurrence information. However, their construction requires large, accurately parsed corpora, which are unavailable for most languages. In this paper, we develop a number of methods to overcome this obstacle. We describe a that constructs a syntax-based model for a new language requiring only an English resource and a translation and (b) that combine crosslingual with monolingual information, subject to availability. We evaluate on two lexical semantic benchmarks in German and Croatian. We find that the models exhibit complementary profiles: crosslingual models yield higher accuracies while monolingual models provide better coverage. In addition, we show that simple multilingual models can successfully combine their strengths.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>86--90</pages>
<location>Montréal, QC.</location>
<contexts>
<context position="42960" citStr="Baker et al., 1998" startWordPosition="6861" endWordPosition="6864">latedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work to ours is (Mohammad et al., 2007), which also considers DSMs, albeit a different variety, namely conceptbased DSMs where targets are characterized in terms of their distribution over categories of Roget’s thesaurus. Like our work, their study creates crosslingual DSMs for German using a translation lexicon. It follows a different strategy, however: it collects cooccurrence counts from a German corpus and transla</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 86–90, Montréal, QC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="1800" citStr="Baroni and Lenci, 2010" startWordPosition="254" endWordPosition="257">tional Hypothesis (Harris, 1954; Miller and Charles, 1991), which states that words occurring in similar contexts are similar in meaning, distributional semantic models (DSMs) represent a word’s meaning via its occurrence in context in large corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, </context>
<context position="5987" citStr="Baroni and Lenci (2010)" startWordPosition="904" endWordPosition="907">elated work (Section 8) and a general discussion (Section 9). 2 Distributional Memory: A General Model of Syntax-based Vector Spaces 2.1 Motivation and Definition Simple syntax-based DSMs represent target words in terms of dimensions labeled with word-relation pairs (Lin, 1998; Grefenstette, 1994). Unfortunately, this representation only supports tasks that compare pairs of words with regard to their meaning (e.g., in synonymy detection or selectional preferences), but not for tasks such as analogical reasoning, where sets of word pairs are compared (Turney, 2006). To unify syntax-based DSMs, Baroni and Lenci (2010) proposed the Distributional Memory (DM) model which captures distributional information at the more general level of word-link-word triples, stored as a third order co-occurrence tensor. The DM tensor can be seen as a set of ordered wordlink-word tuples such as (pencil obj use) associated with a scoring function a: W x L x W -+ R+ that scores, for example, (pencil obj use) more highly than (elephant obj use). The DM tensor can be visualized as a directed graph whose nodes are labeled with lemmas and whose edges are labeled with links and scores. As an example, Figure (1a) shows five links for</context>
<context position="7661" citStr="Baroni and Lenci (2010)" startWordPosition="1187" endWordPosition="1190">us construction (Lin, 1998). The example matrix in Figure (1c) represents a word-link by word space (WL x W). It characterizes pairs (w, l) through context words w, which can be understood as selectional preferences. DM does not assume a specific source for building the graph. However, all existing DM resources were extracted from large dependency-parsed corpora such as UKWAC (Baroni et al., 2008). In the simplest case, the set of labels L is (a subset of) the dependency relations in the corpus, and the scoring function a is a measure of association between the governor and the dependent (see Baroni and Lenci (2010) for details). However, the most robust DMs (including Baroni and Lenci’s LexDM and TypeDM) use both syntactic and lexicalized links, i.e. links which contain words themselves, as well as surface form-based links, e.g., observed subject-verb-object triples in the corpus lead to a (subject verb object) edge in the DM graph. 246 2.2 DMs for Other Languages Given the appealing properties of Distributional Memory, it may be surprising that not many comparable resources exist for other languages. To our knowledge, comparable resources exist only for German (Padó and Utt, 2012) and Croatian (Šnajder</context>
<context position="25671" citStr="Baroni and Lenci (2010)" startWordPosition="4085" endWordPosition="4088"> Differences between models are tested for significance using bootstrap resampling (Efron and Tibshirani, 1993), always in the “All” condition. 5.3 Models We consider three types of DM models (monolingual, crosslingual and multilingual), bag-of-words models and a set of models proposed in the literature. Monolingual model. We use DM.DE (Padó and Utt, 2012), constructed from a 900M-token web corpus, SDEWAC, parsed with MATE (Bohnet, 2010).6 As discussed in Section 3.5, we consider all links (AllL) for the monolingual model. Crosslingual models. The starting point for the crosslingual models is Baroni and Lenci (2010)’s English TypeDM model extracted from approximately 3B tokens of Wikipedia and web corpus text parsed with MaltParser (Nivre, 2006).7 DM.XL naive implements Eq. (1), and DM.XL filter implements Eq. (3). As our translation lexicon, we use the communitybuilt English–German dict.cc online dictionary.8 5We note that since the data are not normally distributed, a non-parametric correlation coefficient would be more appropriate. While we omitted them due to space limitations in this paper, we will provide Spearman ρ results for all models online at http://goo.gl/uxuffp. 6Available from http://goo.g</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):1–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A Collection of Very Large Linguistically Processed WebCrawled Corpora. Language Resources and Evaluation,</title>
<date>2008</date>
<pages>43--3</pages>
<contexts>
<context position="7438" citStr="Baroni et al., 2008" startWordPosition="1147" endWordPosition="1150">vector spaces. The matrix in Figure (1b) shows the word by link-word space (W x LW). It represents words w in terms of pairs (l, w) of a link and a context word. This space models similarity among words, e.g. for thesaurus construction (Lin, 1998). The example matrix in Figure (1c) represents a word-link by word space (WL x W). It characterizes pairs (w, l) through context words w, which can be understood as selectional preferences. DM does not assume a specific source for building the graph. However, all existing DM resources were extracted from large dependency-parsed corpora such as UKWAC (Baroni et al., 2008). In the simplest case, the set of labels L is (a subset of) the dependency relations in the corpus, and the scoring function a is a measure of association between the governor and the dependent (see Baroni and Lenci (2010) for details). However, the most robust DMs (including Baroni and Lenci’s LexDM and TypeDM) use both syntactic and lexicalized links, i.e. links which contain words themselves, as well as surface form-based links, e.g., observed subject-verb-object triples in the corpus lead to a (subject verb object) edge in the DM graph. 246 2.2 DMs for Other Languages Given the appealing </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2008</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2008. The WaCky Wide Web: A Collection of Very Large Linguistically Processed WebCrawled Corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>89--97</pages>
<location>Beijing, China.</location>
<contexts>
<context position="25489" citStr="Bohnet, 2010" startWordPosition="4059" endWordPosition="4060">e measure the correlation between the semantic similarities and human judgments for word pairs. Coverage is calculated as the percentage of items with similarity greater 0. Differences between models are tested for significance using bootstrap resampling (Efron and Tibshirani, 1993), always in the “All” condition. 5.3 Models We consider three types of DM models (monolingual, crosslingual and multilingual), bag-of-words models and a set of models proposed in the literature. Monolingual model. We use DM.DE (Padó and Utt, 2012), constructed from a 900M-token web corpus, SDEWAC, parsed with MATE (Bohnet, 2010).6 As discussed in Section 3.5, we consider all links (AllL) for the monolingual model. Crosslingual models. The starting point for the crosslingual models is Baroni and Lenci (2010)’s English TypeDM model extracted from approximately 3B tokens of Wikipedia and web corpus text parsed with MaltParser (Nivre, 2006).7 DM.XL naive implements Eq. (1), and DM.XL filter implements Eq. (3). As our translation lexicon, we use the communitybuilt English–German dict.cc online dictionary.8 5We note that since the data are not normally distributed, a non-parametric correlation coefficient would be more app</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 89–97, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning,</booktitle>
<pages>149--164</pages>
<location>New York, NY.</location>
<contexts>
<context position="9727" citStr="Buchholz and Marsi, 2006" startWordPosition="1508" endWordPosition="1511">e is a bottleneck in many languages regarding both large, clean corpora as well as processing pipelines that result in high-accuracy dependency parses. To address this problem, we propose to induce Distributional Memories for such languages crosslingually by translating a source language DM into the target language. By adopting English as the source language we can take advantage of the resource gradient, that is, the higher maturity of English NLP techniques, such as parsers, compared to most other languages. For many languages, treebanks have become available only within the last ten years (Buchholz and Marsi, 2006), if at all, while English has been at the forefront of NLP development for several decades, and a number of highly accurate dependency parsers exist (McDonald et al., 2005; Nivre, 2006). At the same time, English arguably possesses the widest range of large and well-cleaned corpora of any language. To make our approaches applicable to as many target languages as possible, we assume in this section that very few resources for the target language are available. The crosslingual methods we develop Figure 2: Sample of the English-German dict.cc dictionary; translations shown as dashed lines. here</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Center for Research in Computing Technology, Harvard University.</institution>
<contexts>
<context position="19748" citStr="Chen and Goodman, 1998" startWordPosition="3140" endWordPosition="3143">e available. In order to take advantage of such developments, this section discusses two simple methods for combining monolingually and crosslingually constructed DMs, thereby combining corpus evidence from both the source and the target language. We concentrate on methods that can be applied to DMs directly, e.g. by researchers who do not have access to the source corpora. Moreover, we combine not the graphs, but the resulting semantic similarities.2 We take our inspiration from work on combining and smoothing n-gram language models, where the usual operations are interpolation and back-off (Chen and Goodman, 1998). Note that in our case, the two models to be combined are assumed to have complementary properties, with the monolin2We conducted experiments with graph merging but found that the different topologies of the monolingual and crosslingual DMs make it difficult to merge the graphs in a manner that combines the information from both graphs. ET = {(t1,l, t2) I I (s1,l, s2) E ES : t1 E Tr(s1) ∧ t2 E Tr(s2)∧ 249 Demagoge 1 Miesmacher x 2 guter Redner x 3 skrupelloser Hetzer 4 Meinungsforscher x gual model having higher coverage and the crosslingual model higher quality (cf. Section 3.1). For this re</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Antje Schultz</author>
<author>Sergej Sizov</author>
<author>Philipp Sorg</author>
<author>Steffen Staab</author>
</authors>
<title>Explicit vs. latent concept models for cross-language information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1513--1518</pages>
<location>Pasadena, CA.</location>
<contexts>
<context position="42543" citStr="Cimiano et al., 2009" startWordPosition="6791" endWordPosition="6794">2) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work</context>
</contexts>
<marker>Cimiano, Schultz, Sizov, Sorg, Staab, 2009</marker>
<rawString>Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp Sorg, and Steffen Staab. 2009. Explicit vs. latent concept models for cross-language information retrieval. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 1513–1518, Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Crosslanguage linking of news stories on the web using interlingual topic modelling.</title>
<date>2009</date>
<booktitle>In Proceedings of the CIKM Workshop on Social Web Search and Mining,</booktitle>
<pages>57--64</pages>
<location>Hong Kong.</location>
<marker>De Smet, Moens, 2009</marker>
<rawString>Wim De Smet and Marie-Francine Moens. 2009. Crosslanguage linking of news stories on the web using interlingual topic modelling. In Proceedings of the CIKM Workshop on Social Web Search and Mining, pages 57–64, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>255--262</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="41204" citStr="Diab and Resnik (2002)" startWordPosition="6599" endWordPosition="6602">English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12We cannot provide significances for the BOW results because we again do not have per-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studie</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 255–262, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall,</title>
<date>1993</date>
<location>New York, NY.</location>
<contexts>
<context position="25159" citStr="Efron and Tibshirani, 1993" startWordPosition="4005" endWordPosition="4009"> the case of zero similarities. For Exp. 1, we report the accuracy (the number of correctly recognized synonyms divided by the number of attempted problems) and coverage (the ratio of items attempted; always 1 for the “All” condition). Items are considered covered if at least one candidate has a non-zero similarity to the target. In Exp. 2, we measure the correlation between the semantic similarities and human judgments for word pairs. Coverage is calculated as the percentage of items with similarity greater 0. Differences between models are tested for significance using bootstrap resampling (Efron and Tibshirani, 1993), always in the “All” condition. 5.3 Models We consider three types of DM models (monolingual, crosslingual and multilingual), bag-of-words models and a set of models proposed in the literature. Monolingual model. We use DM.DE (Padó and Utt, 2012), constructed from a 900M-token web corpus, SDEWAC, parsed with MATE (Bohnet, 2010).6 As discussed in Section 3.5, we consider all links (AllL) for the monolingual model. Crosslingual models. The starting point for the crosslingual models is Baroni and Lenci (2010)’s English TypeDM model extracted from approximately 3B tokens of Wikipedia and web corp</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
<author>Ulrike Padó</author>
</authors>
<title>A Flexible,</title>
<date>2010</date>
<booktitle>Corpus-Driven Model of Regular and Inverse Selectional Preferences. Computational Linguistics,</booktitle>
<pages>36--4</pages>
<contexts>
<context position="2032" citStr="Erk et al., 2010" startWordPosition="290" endWordPosition="293">rge corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, syntax-based DSMs are inherently more sparse than word spaces, which calls for a large corpus of well parsable data. It is thus not surprising that besides English (Baroni and Lenci, 2010), only few other languages possess large-sca</context>
</contexts>
<marker>Erk, Padó, Padó, 2010</marker>
<rawString>Katrin Erk, Sebastian Padó, and Ulrike Padó. 2010. A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences. Computational Linguistics, 36(4):723–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Benfeng Chen</author>
</authors>
<title>BiFrameNet: Bilingual Frame Semantics Resources Construction by crosslingual Induction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>931--935</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="42915" citStr="Fung and Chen (2004)" startWordPosition="6854" endWordPosition="6857">cture. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work to ours is (Mohammad et al., 2007), which also considers DSMs, albeit a different variety, namely conceptbased DSMs where targets are characterized in terms of their distribution over categories of Roget’s thesaurus. Like our work, their study creates crosslingual DSMs for German using a translation lexicon. It follows a different strategy, however: it collects cooccur</context>
</contexts>
<marker>Fung, Chen, 2004</marker>
<rawString>Pascale Fung and Benfeng Chen. 2004. BiFrameNet: Bilingual Frame Semantics Resources Construction by crosslingual Induction. In Proceedings of the 20th International Conference on Computational Linguistics, pages 931–935, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="33963" citStr="Gildea and Jurafsky, 2002" startWordPosition="5442" endWordPosition="5446">ong different models with different coverage/quality tradeoffs. Comparison to models from the literature. Models from the literature are shown at the bottom of the two tables. They generally obtain the highest accuracy (or correlation, respectively), but only cover a relatively small part of the datasets. In particular, the models with a quality higher than the DM variants (11 in Exp. 1 and 10 and 11 in Exp. 2) exhibit a coverage of less than half than that of the DM.MULTI models. This appears to show the usual trade-off between hand-constructed knowledge and automatically acquired knowledge (Gildea and Jurafsky, 2002). However, we can similarly bias our DMs towards accuracy with the aid of a simple frequency filter that only permits predictions for items where all involved lemmas occur more frequently in the German corpus than some threshold. Setting these thresholds to match the coverage figures of the best ontology-based models, DM.MULTI MaxSim almost reaches the ontology-based results: On Exp. 1, for a coverage of .22 we obtain an accuracy of .68 (ontology-based model: .77), and on Exp. 2, we obCouscous (couscous), Albino (albino) kursorisch (cursory), süffisant (smug) erodieren (to erode), moussieren (</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston/Norwell, MA.</location>
<contexts>
<context position="5662" citStr="Grefenstette, 1994" startWordPosition="858" endWordPosition="859"> in Sections 3 and 4, namely, a family of models for the crosslingual and multilingual construction of DSMs. The second part of the paper is concerned with evaluation. Section 5 describes our experimental setup after which we discuss our results for German (Section 6) and Croatian (Section 7). The paper concludes with related work (Section 8) and a general discussion (Section 9). 2 Distributional Memory: A General Model of Syntax-based Vector Spaces 2.1 Motivation and Definition Simple syntax-based DSMs represent target words in terms of dimensions labeled with word-relation pairs (Lin, 1998; Grefenstette, 1994). Unfortunately, this representation only supports tasks that compare pairs of words with regard to their meaning (e.g., in synonymy detection or selectional preferences), but not for tasks such as analogical reasoning, where sets of word pairs are compared (Turney, 2006). To unify syntax-based DSMs, Baroni and Lenci (2010) proposed the Distributional Memory (DM) model which captures distributional information at the more general level of word-link-word triples, stored as a third order co-occurrence tensor. The DM tensor can be seen as a set of ordered wordlink-word tuples such as (pencil obj </context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Boston/Norwell, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="1208" citStr="Harris, 1954" startWordPosition="160" endWordPosition="161">constructs a syntax-based model for a new language requiring only an English resource and a translation lexicon; and (b) multilingual approaches that combine crosslingual with monolingual information, subject to availability. We evaluate on two lexical semantic benchmarks in German and Croatian. We find that the models exhibit complementary profiles: crosslingual models yield higher accuracies while monolingual models provide better coverage. In addition, we show that simple multilingual models can successfully combine their strengths. 1 Introduction Building on the Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991), which states that words occurring in similar contexts are similar in meaning, distributional semantic models (DSMs) represent a word’s meaning via its occurrence in context in large corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which u</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zelig S. Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenhai Hi</author>
<author>Rebecca Hwa</author>
</authors>
<title>A Backoff Model for Bootstrapping Resources for Non-English Languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>851--858</pages>
<location>Vancouver, BC.</location>
<contexts>
<context position="41068" citStr="Hi and Hwa, 2005" startWordPosition="6577" endWordPosition="6580">ries, matching previous results from the literature (Peirsman and Padó, 2011). 8 Related Work Given the resource gradient between English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12We cannot provide significances for the BOW results because we again do not have per-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classif</context>
</contexts>
<marker>Hi, Hwa, 2005</marker>
<rawString>Chenhai Hi and Rebecca Hwa. 2005. A Backoff Model for Bootstrapping Resources for Non-English Languages. In Proceedings of the joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 851– 858, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philipp Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping Parsers via Syntactic Projection across Parallel Texts.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<pages>325</pages>
<contexts>
<context position="41086" citStr="Hwa et al., 2005" startWordPosition="6581" endWordPosition="6584">vious results from the literature (Peirsman and Padó, 2011). 8 Related Work Given the resource gradient between English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12We cannot provide significances for the BOW results because we again do not have per-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multi</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping Parsers via Syntactic Projection across Parallel Texts. Journal of Natural Language Engineering, 11(3):311– 325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Joanis</author>
<author>Suzanne Stevenson</author>
<author>David James</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>14</volume>
<issue>03</issue>
<contexts>
<context position="2165" citStr="Joanis et al., 2006" startWordPosition="309" endWordPosition="312">ons correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, syntax-based DSMs are inherently more sparse than word spaces, which calls for a large corpus of well parsable data. It is thus not surprising that besides English (Baroni and Lenci, 2010), only few other languages possess large-scale syntax-based DSMs (Padó and Utt, 2012; Šnajder et al., 2013). This paper develops methods that take advantage of the resource grad</context>
</contexts>
<marker>Joanis, Stevenson, James, 2006</marker>
<rawString>Eric Joanis, Suzanne Stevenson, and David James. 2006. A general feature space for automatic verb classification. Natural Language Engineering, 14(03):337–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mladen Karan</author>
<author>Jan Šnajder</author>
<author>Bojana Dalbelo Baši´c</author>
</authors>
<title>Distributional semantics approach to detecting synonyms in Croatian language.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth Language Technologies Conference,</booktitle>
<location>Ljubljana, Slovenia.</location>
<marker>Karan, Šnajder, Baši´c, 2012</marker>
<rawString>Mladen Karan, Jan Šnajder, and Bojana Dalbelo Baši´c. 2012. Distributional semantics approach to detecting synonyms in Croatian language. In Proceedings of the Eighth Language Technologies Conference, Ljubljana, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics,</booktitle>
<pages>1459--1474</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="41642" citStr="Klementiev et al. (2012)" startWordPosition="6661" endWordPosition="6664">s such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly ric</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of the International Conference on Computational Linguistics, pages 1459–1474, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikhail Kozhevnikov</author>
<author>Ivan Titov</author>
</authors>
<title>Crosslingual transfer of semantic role models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1190--1200</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="41565" citStr="Kozhevnikov and Titov (2013)" startWordPosition="6649" endWordPosition="6652">r-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional </context>
</contexts>
<marker>Kozhevnikov, Titov, 2013</marker>
<rawString>Mikhail Kozhevnikov and Ivan Titov. 2013. Crosslingual transfer of semantic role models. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, pages 1190–1200, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="22873" citStr="Landauer and Dumais, 1997" startWordPosition="3639" endWordPosition="3642">y the true synonym for a target word from four candidates. We use the German Reader’s Digest Word Power (RDWP) dataset (Wallace and demagogue grinch able speaker unscrupulous agitator pollster (a) Task 1: synonym target with four candidates Word Pair Similarity Absage - ablehnen 3.5 (rejection - refuse) Absage - Stellenanzeige 1.875 (rejection -job advertisement) Affe - Gepäckkontrolle 0.125 (monkey - luggage inspection) (b) Task 2: semantic similarity (range: 0–4) Table 2: Example items from evaluation tasks Wallace, 2005) which contains 984 items.3 RDWP is similar to the English TOEFL data (Landauer and Dumais, 1997), but can contain short phrases among the candidates (cf. example in Table 2a). Our second evaluation tests how well the models predict similarities for German word pairs including closely related, somewhat related, and unrelated word pairs (cf. Table 2b). We use the Gur350 dataset4 which contains 350 word pairs scored for relatedness by native German taggers on a five-point Likert scale between 0 (unrelated) and 4 (synonymous). Both datasets contain nouns, verbs and adjectives. 5.2 Procedure Starting from a DM model, we matricize it into a word by link-word space (W x LW) and compute similari</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K Landauer and Susan T Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montreal, QC.</location>
<contexts>
<context position="1775" citStr="Lin, 1998" startWordPosition="252" endWordPosition="253">he Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991), which states that words occurring in similar contexts are similar in meaning, distributional semantic models (DSMs) represent a word’s meaning via its occurrence in context in large corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many</context>
<context position="5641" citStr="Lin, 1998" startWordPosition="856" endWordPosition="857">ions follow in Sections 3 and 4, namely, a family of models for the crosslingual and multilingual construction of DSMs. The second part of the paper is concerned with evaluation. Section 5 describes our experimental setup after which we discuss our results for German (Section 6) and Croatian (Section 7). The paper concludes with related work (Section 8) and a general discussion (Section 9). 2 Distributional Memory: A General Model of Syntax-based Vector Spaces 2.1 Motivation and Definition Simple syntax-based DSMs represent target words in terms of dimensions labeled with word-relation pairs (Lin, 1998; Grefenstette, 1994). Unfortunately, this representation only supports tasks that compare pairs of words with regard to their meaning (e.g., in synonymy detection or selectional preferences), but not for tasks such as analogical reasoning, where sets of word pairs are compared (Turney, 2006). To unify syntax-based DSMs, Baroni and Lenci (2010) proposed the Distributional Memory (DM) model which captures distributional information at the more general level of word-link-word triples, stored as a third order co-occurrence tensor. The DM tensor can be seen as a set of ordered wordlink-word tuples</context>
<context position="7065" citStr="Lin, 1998" startWordPosition="1089" endWordPosition="1090">ose nodes are labeled with lemmas and whose edges are labeled with links and scores. As an example, Figure (1a) shows five links for the verb push in the English DM, including subject, object, prepositional adjunct, and governing verbs. DSMs for individual tasks can be obtained by “matricizing” the tensor into two-dimensional matrices corresponding to standard vector spaces. The matrix in Figure (1b) shows the word by link-word space (W x LW). It represents words w in terms of pairs (l, w) of a link and a context word. This space models similarity among words, e.g. for thesaurus construction (Lin, 1998). The example matrix in Figure (1c) represents a word-link by word space (WL x W). It characterizes pairs (w, l) through context words w, which can be understood as selectional preferences. DM does not assume a specific source for building the graph. However, all existing DM resources were extracted from large dependency-parsed corpora such as UKWAC (Baroni et al., 2008). In the simplest case, the set of labels L is (a subset of) the dependency relations in the corpus, and the scoring function a is a measure of association between the governor and the dependent (see Baroni and Lenci (2010) for</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 768–774, Montreal, QC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<location>Vancouver, BC.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523–530, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Täckström, Claudia Bedini, Núria Bertomeu Castelló, and</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>92--97</pages>
<location>Sofia, Bulgaria.</location>
<marker>McDonald, Nivre, 2013</marker>
<rawString>Ryan McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Täckström, Claudia Bedini, Núria Bertomeu Castelló, and Jungmee Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Suzanne Stevenson</author>
<author>Vivian Tsang</author>
<author>Gianluca Allaria</author>
</authors>
<title>A multilingual paradigm for automatic verb classification.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>207--214</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="41924" citStr="Merlo et al. (2002)" startWordPosition="6701" endWordPosition="6704">butional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Ci</context>
</contexts>
<marker>Merlo, Stevenson, Tsang, Allaria, 2002</marker>
<rawString>Paola Merlo, Suzanne Stevenson, Vivian Tsang, and Gianluca Allaria. 2002. A multilingual paradigm for automatic verb classification. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 207–214, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dragomir Radev</author>
</authors>
<title>Graphbased Natural Language Processing and Information Retrieval.</title>
<date>2011</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="11793" citStr="Mihalcea and Radev, 2011" startWordPosition="1829" endWordPosition="1832">rce language corpora which we assume to be parsed more accurately than target language corpora. In addition, the translation process can be designed to act as a further filtering step (cf. Section 3.4 below), thus optimizing crosslingual models for higher quality at the expense of coverage. In contrast, monolingual models – in particular for under-resourced languages – often hit a quality ceiling, but can generally guarantee high coverage. 3.2 Translating DMs with Translation Lexicons We conceptualize DM as a directed graph (see Figure 1), which allows us to phrase translation in graph terms (Mihalcea and Radev, 2011). A DM is a triple (V, E, a) where V is a set of vertices (i.e., the vocabulary), E a set of typed edges between words, represented as word-link-word triples (cf. Section 2), and a, an edge-weighting function. We will use S and T to refer to the source and target language vocabularies, respectively, and (VS, ES, aS) and (VT, ET, aT) to denote source and target language DMs. We can now ask how the shape of the graph groveN copseN forestN timberN woodN HainN GehšlzN WaldN HolzN 247 changes under translation. In an ideal world, a translation lexicon would be a bijective function between the sourc</context>
</contexts>
<marker>Mihalcea, Radev, 2011</marker>
<rawString>Rada Mihalcea and Dragomir Radev. 2011. Graphbased Natural Language Processing and Information Retrieval. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="1235" citStr="Miller and Charles, 1991" startWordPosition="162" endWordPosition="165">yntax-based model for a new language requiring only an English resource and a translation lexicon; and (b) multilingual approaches that combine crosslingual with monolingual information, subject to availability. We evaluate on two lexical semantic benchmarks in German and Croatian. We find that the models exhibit complementary profiles: crosslingual models yield higher accuracies while monolingual models provide better coverage. In addition, we show that simple multilingual models can successfully combine their strengths. 1 Introduction Building on the Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991), which states that words occurring in similar contexts are similar in meaning, distributional semantic models (DSMs) represent a word’s meaning via its occurrence in context in large corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic </context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Iryna Gurevych</author>
<author>Graeme Hirst</author>
<author>Torsten Zesch</author>
</authors>
<title>Crosslingual distributional profiles of concepts for measuring semantic distance.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>571--580</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11101" citStr="Mohammad et al., 2007" startWordPosition="1720" endWordPosition="1723"> translation pairs without translation probabilities, as shown in Figure 2. Translation lexicons of this type are arguably the most common bilingual resource and accurate ones exist for virtually any language pair (Soderland et al., 2009), even for languages with few available corpora. Furthermore, such translation lexicons are often crowdsourced and are available for download. For example, the website dict.cc provides numerous such lexicons for German and English. This approach promises in particular to yield models with a quality-coverage profile complementary to that of monolingual models (Mohammad et al., 2007; Peirsman and Padó, 2011): Crosslingual DMs are extracted from source language corpora which we assume to be parsed more accurately than target language corpora. In addition, the translation process can be designed to act as a further filtering step (cf. Section 3.4 below), thus optimizing crosslingual models for higher quality at the expense of coverage. In contrast, monolingual models – in particular for under-resourced languages – often hit a quality ceiling, but can generally guarantee high coverage. 3.2 Translating DMs with Translation Lexicons We conceptualize DM as a directed graph (se</context>
<context position="23816" citStr="Mohammad et al. (2007)" startWordPosition="3790" endWordPosition="3793">for relatedness by native German taggers on a five-point Likert scale between 0 (unrelated) and 4 (synonymous). Both datasets contain nouns, verbs and adjectives. 5.2 Procedure Starting from a DM model, we matricize it into a word by link-word space (W x LW) and compute similarities between words with Cosine similarity. In Exp. 1, we compute the semantic similarities of the target with each candidate and predict the candidate with the highest similarity to the target. For phrasal candidates, we compute the similarity between the target and all constituent words and take the maximum. We follow Mohammad et al. (2007) in assigning partial credit to a model when the candidates of a target are tied for maximal similarity. We evaluate the models on Exp. 2 by calculating the strength of the correlation between the model predictions and the 3Available from: http://goo.gl/PN42E 4Available from: http://goo.gl/3Dflf1 250 human relatedness judgments. We use Pearson’s correlation coefficient since it is the de facto evaluation measure in relevant earlier work.5 On both tasks, we compare the models in two conditions. In the first condition (“All”), models are forced to make predictions for all items in the dataset ev</context>
<context position="27695" citStr="Mohammad et al., 2007" startWordPosition="4401" endWordPosition="4404">ndard BOW model from the same German corpus SDEWAC used for DM.DE. We assume a window of 10 context words to the left and right. We use the top 10K most frequent content words (nouns, adjectives, verbs and adverbs) as dimensions. Our second BOW model (BOW PCA500) was reduced to 500 dimensions by applying principle component analysis, a technique generally used to increase robustness to parameter choice and to combat sparsity.9 Models from the literature. We compare our models against the state of the art, represented by the respective best models from two previous studies (Zesch et al., 2007; Mohammad et al., 2007). They comprise monolingual ontology-based models that use GermaNet, (German) Wikipedia, or both (LinGN, 9We also built models using smaller context windows and Latent Semantic Analysis (LSA, Landauer, 1997), both with 500 dimensions and with an automatically optimized number of dimensions (Wild et al., 2008). Since these spaces did not consistently yield better results than the reported models using PCA, we do not report the results in detail. Class monolingual crosslingual &amp; multilingual (DE) Model DM.DE (DE) TYPEDM (EN) DM.XL naive DM.XL filter Nodes Edges 3.5M 78M 31K 131M 63K 5B 63K 1.7B </context>
<context position="30656" citStr="Mohammad et al. (2007)" startWordPosition="4908" endWordPosition="4911">atterns for the two experiments. Model All Covered Corr Corr Cov Baselines and word-based DSMs 1 Frequency .13 .13 1 2 BOW .20 .21 .97 3 BOW PCA500 .34 .37 .97 Syntax-based DSMs 4 DM.DE (AllL) [PU12] .38 .43 .60 5 DM.XL EN→DE naive (SPrfL) .28 .45 .49 6 DM.XL EN→DE filter (SPrfL) .33 .49 .49 7 DM.MULTI Backoff(6,4) .40 .45 .69 8 DM.MULTI MaxSim(6,4) .42 .47 .69 Models from the literature 9 LinGN [MGHZ07] NA .50 .26 10 Lindist [MGHZ07] NA .51 .26 11 JCGN+PLWP [ZGM07] NA .59 .33 Table 6: Exp. 2: Coverage and correlation (Pearson’s r) for predicting word similarity on the Gur350 dataset. MGHZ07: Mohammad et al. (2007)8, ZGM07: Zesch et al. (2007)9, PU12: Padó and Utt (2012). Best results for each model class in bold. Baselines and word-based DSMs. In both cases, uninformed baselines (random and frequency) perform badly. (In Exp. 1, the frequency baseline predicts the most frequent item as synonym; in Exp. 2, it predicts min(f(w1), f(w2)).) In contrast, wordbased DSMs perform quite well, particularly the dimensionality-reduced model (BOW PCA). Syntax-based DSM. We see a consistent quality versus coverage tradeoff among the different classes of syntax-based DSMs. The monolingual DM.DE model is significantly </context>
<context position="43178" citStr="Mohammad et al., 2007" startWordPosition="6895" endWordPosition="6898">oes not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work to ours is (Mohammad et al., 2007), which also considers DSMs, albeit a different variety, namely conceptbased DSMs where targets are characterized in terms of their distribution over categories of Roget’s thesaurus. Like our work, their study creates crosslingual DSMs for German using a translation lexicon. It follows a different strategy, however: it collects cooccurrence counts from a German corpus and translates the context dimensions into the English Roget categories. Therefore, it crucially requires a large target language corpus, which our crosslingual methods (Section 3) avoid. Its use of a target language corpus resem</context>
</contexts>
<marker>Mohammad, Gurevych, Hirst, Zesch, 2007</marker>
<rawString>Saif Mohammad, Iryna Gurevych, Graeme Hirst, and Torsten Zesch. 2007. Crosslingual distributional profiles of concepts for measuring semantic distance. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 571–580, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Benjamin Snyder</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Multilingual Part-of-Speech Tagging : Two Unsupervised Approaches.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>36--1</pages>
<contexts>
<context position="41376" citStr="Naseem et al. (2009)" startWordPosition="6623" endWordPosition="6626">on (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12We cannot provide significances for the BOW results because we again do not have per-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new featur</context>
</contexts>
<marker>Naseem, Snyder, Eisenstein, Barzilay, 2009</marker>
<rawString>Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and Regina Barzilay. 2009. Multilingual Part-of-Speech Tagging : Two Unsupervised Approaches. Journal of Artificial Intelligence Research, 36:1–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: Building a very large multilingual semantic network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>216--225</pages>
<location>Uppsala,</location>
<contexts>
<context position="42407" citStr="Navigli and Ponzetto, 2010" startWordPosition="6773" endWordPosition="6776">quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They u</context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a very large multilingual semantic network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216–225, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Babelrelate! a joint multilingual approach to computing semantic relatedness.</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Conference on Artificial Intelligence,</booktitle>
<pages>108--114</pages>
<location>Toronto, ON.</location>
<contexts>
<context position="42378" citStr="Navigli and Ponzetto, 2012" startWordPosition="6769" endWordPosition="6772"> are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. Babelrelate! a joint multilingual approach to computing semantic relatedness. In Proceedings of the 26th Conference on Artificial Intelligence, pages 108–114, Toronto, ON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>Dordrecht, Netherlands.</location>
<contexts>
<context position="9913" citStr="Nivre, 2006" startWordPosition="1542" endWordPosition="1543">istributional Memories for such languages crosslingually by translating a source language DM into the target language. By adopting English as the source language we can take advantage of the resource gradient, that is, the higher maturity of English NLP techniques, such as parsers, compared to most other languages. For many languages, treebanks have become available only within the last ten years (Buchholz and Marsi, 2006), if at all, while English has been at the forefront of NLP development for several decades, and a number of highly accurate dependency parsers exist (McDonald et al., 2005; Nivre, 2006). At the same time, English arguably possesses the widest range of large and well-cleaned corpora of any language. To make our approaches applicable to as many target languages as possible, we assume in this section that very few resources for the target language are available. The crosslingual methods we develop Figure 2: Sample of the English-German dict.cc dictionary; translations shown as dashed lines. here work without any target language corpora, either monolingual or bilingual. The only knowledge we use is a simple translation lexicon, that is, a list of translation pairs without transl</context>
<context position="25803" citStr="Nivre, 2006" startWordPosition="4107" endWordPosition="4108"> 5.3 Models We consider three types of DM models (monolingual, crosslingual and multilingual), bag-of-words models and a set of models proposed in the literature. Monolingual model. We use DM.DE (Padó and Utt, 2012), constructed from a 900M-token web corpus, SDEWAC, parsed with MATE (Bohnet, 2010).6 As discussed in Section 3.5, we consider all links (AllL) for the monolingual model. Crosslingual models. The starting point for the crosslingual models is Baroni and Lenci (2010)’s English TypeDM model extracted from approximately 3B tokens of Wikipedia and web corpus text parsed with MaltParser (Nivre, 2006).7 DM.XL naive implements Eq. (1), and DM.XL filter implements Eq. (3). As our translation lexicon, we use the communitybuilt English–German dict.cc online dictionary.8 5We note that since the data are not normally distributed, a non-parametric correlation coefficient would be more appropriate. While we omitted them due to space limitations in this paper, we will provide Spearman ρ results for all models online at http://goo.gl/uxuffp. 6Available from http://goo.gl/H6gViT. 7Available from http://goo.gl/63ajCI. 8Available from http://goo.gl/re44Hg. Adj Noun Verbs Total English 37K 78K 8K 123K G</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing. Springer, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Padó</author>
<author>Jason Utt</author>
</authors>
<title>A distributional memory for German.</title>
<date>2012</date>
<booktitle>In Proceedings of the KONVENS</booktitle>
<pages>462--470</pages>
<location>Vienna, Austria.</location>
<contexts>
<context position="2672" citStr="Padó and Utt, 2012" startWordPosition="386" endWordPosition="389"> (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, syntax-based DSMs are inherently more sparse than word spaces, which calls for a large corpus of well parsable data. It is thus not surprising that besides English (Baroni and Lenci, 2010), only few other languages possess large-scale syntax-based DSMs (Padó and Utt, 2012; Šnajder et al., 2013). This paper develops methods that take advantage of the resource gradient between English and other languages, exploiting the higher-quality resources of the former to induce resources for target languages among the latter, by translating the word-link-word co-occurrences that underlie syntax-based DSMs. This directly provides a crosslingual method to construct syntax-based DSMs for target languages without any target language data, requiring only an English syntax-based DSM and a translation lexicon. Such lexicons are available for many language pairs, and we outline a</context>
<context position="8239" citStr="Padó and Utt, 2012" startWordPosition="1278" endWordPosition="1281">he dependent (see Baroni and Lenci (2010) for details). However, the most robust DMs (including Baroni and Lenci’s LexDM and TypeDM) use both syntactic and lexicalized links, i.e. links which contain words themselves, as well as surface form-based links, e.g., observed subject-verb-object triples in the corpus lead to a (subject verb object) edge in the DM graph. 246 2.2 DMs for Other Languages Given the appealing properties of Distributional Memory, it may be surprising that not many comparable resources exist for other languages. To our knowledge, comparable resources exist only for German (Padó and Utt, 2012) and Croatian (Šnajder et al., 2013). Both studies replicate the monolingual DM construction process outlined by Baroni and Lenci for the respective languages. For German, the process is relatively unproblematic, since German is relatively well-equipped in terms of corpora and parsers. In contrast, Šnajder et al. (2013) faced serious resource scarcity while building a Croatian DM and had to go to considerable lengths to clean a large web corpus and to optimize the linguistic processing tools. The resulting DM outperforms a monolingual context word model for nouns and verbs, but performs worse </context>
<context position="25406" citStr="Padó and Utt, 2012" startWordPosition="4044" endWordPosition="4047">d covered if at least one candidate has a non-zero similarity to the target. In Exp. 2, we measure the correlation between the semantic similarities and human judgments for word pairs. Coverage is calculated as the percentage of items with similarity greater 0. Differences between models are tested for significance using bootstrap resampling (Efron and Tibshirani, 1993), always in the “All” condition. 5.3 Models We consider three types of DM models (monolingual, crosslingual and multilingual), bag-of-words models and a set of models proposed in the literature. Monolingual model. We use DM.DE (Padó and Utt, 2012), constructed from a 900M-token web corpus, SDEWAC, parsed with MATE (Bohnet, 2010).6 As discussed in Section 3.5, we consider all links (AllL) for the monolingual model. Crosslingual models. The starting point for the crosslingual models is Baroni and Lenci (2010)’s English TypeDM model extracted from approximately 3B tokens of Wikipedia and web corpus text parsed with MaltParser (Nivre, 2006).7 DM.XL naive implements Eq. (1), and DM.XL filter implements Eq. (3). As our translation lexicon, we use the communitybuilt English–German dict.cc online dictionary.8 5We note that since the data are n</context>
<context position="30713" citStr="Padó and Utt (2012)" startWordPosition="4918" endWordPosition="4921"> Cov Baselines and word-based DSMs 1 Frequency .13 .13 1 2 BOW .20 .21 .97 3 BOW PCA500 .34 .37 .97 Syntax-based DSMs 4 DM.DE (AllL) [PU12] .38 .43 .60 5 DM.XL EN→DE naive (SPrfL) .28 .45 .49 6 DM.XL EN→DE filter (SPrfL) .33 .49 .49 7 DM.MULTI Backoff(6,4) .40 .45 .69 8 DM.MULTI MaxSim(6,4) .42 .47 .69 Models from the literature 9 LinGN [MGHZ07] NA .50 .26 10 Lindist [MGHZ07] NA .51 .26 11 JCGN+PLWP [ZGM07] NA .59 .33 Table 6: Exp. 2: Coverage and correlation (Pearson’s r) for predicting word similarity on the Gur350 dataset. MGHZ07: Mohammad et al. (2007)8, ZGM07: Zesch et al. (2007)9, PU12: Padó and Utt (2012). Best results for each model class in bold. Baselines and word-based DSMs. In both cases, uninformed baselines (random and frequency) perform badly. (In Exp. 1, the frequency baseline predicts the most frequent item as synonym; in Exp. 2, it predicts min(f(w1), f(w2)).) In contrast, wordbased DSMs perform quite well, particularly the dimensionality-reduced model (BOW PCA). Syntax-based DSM. We see a consistent quality versus coverage tradeoff among the different classes of syntax-based DSMs. The monolingual DM.DE model is significantly outperformed by the BOW model on Exp. 1 (p&lt;0.01), but num</context>
</contexts>
<marker>Padó, Utt, 2012</marker>
<rawString>Sebastian Padó and Jason Utt. 2012. A distributional memory for German. In Proceedings of the KONVENS 2012 workshop on recent developments and applications of lexical-semantic resources, pages 462–470, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Sebastian Padó</author>
</authors>
<title>Semantic relations in bilingual lexicons.</title>
<date>2011</date>
<journal>ACM Transactions in Speech and Language Processing,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="11127" citStr="Peirsman and Padó, 2011" startWordPosition="1724" endWordPosition="1727">out translation probabilities, as shown in Figure 2. Translation lexicons of this type are arguably the most common bilingual resource and accurate ones exist for virtually any language pair (Soderland et al., 2009), even for languages with few available corpora. Furthermore, such translation lexicons are often crowdsourced and are available for download. For example, the website dict.cc provides numerous such lexicons for German and English. This approach promises in particular to yield models with a quality-coverage profile complementary to that of monolingual models (Mohammad et al., 2007; Peirsman and Padó, 2011): Crosslingual DMs are extracted from source language corpora which we assume to be parsed more accurately than target language corpora. In addition, the translation process can be designed to act as a further filtering step (cf. Section 3.4 below), thus optimizing crosslingual models for higher quality at the expense of coverage. In contrast, monolingual models – in particular for under-resourced languages – often hit a quality ceiling, but can generally guarantee high coverage. 3.2 Translating DMs with Translation Lexicons We conceptualize DM as a directed graph (see Figure 1), which allows </context>
<context position="40529" citStr="Peirsman and Padó, 2011" startWordPosition="6494" endWordPosition="6497">man synonym choice task (Exp. 1) are that (a) filtering plays an essential role for Croatian (increase in accuracy by 15%) and (b) DM.MULTI clearly outperforms the BOW model. We attribute the difference to the semiautomatic construction of the Croatian dataset from machine-readable dictionaries. Overall, the results for Croatian are encouraging. They demonstrate that languages where parsing technology is still developing can in particular profit from cross- and multilingual methods. This is true even for relatively small translation dictionaries, matching previous results from the literature (Peirsman and Padó, 2011). 8 Related Work Given the resource gradient between English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12We cannot provide significances for the BOW results because we again do not have per-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel</context>
<context position="42117" citStr="Peirsman and Padó (2011)" startWordPosition="6729" endWordPosition="6732">onstraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rap</context>
</contexts>
<marker>Peirsman, Padó, 2011</marker>
<rawString>Yves Peirsman and Sebastian Padó. 2011. Semantic relations in bilingual lexicons. ACM Transactions in Speech and Language Processing, 8(2):3:1–3:21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>Jörg Tiedemann</author>
</authors>
<title>Finding synonyms using automatic word alignment and measures of distributional similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>866--873</pages>
<location>Sydney, Australia.</location>
<marker>van der Plas, Tiedemann, 2006</marker>
<rawString>Lonneke van der Plas and Jörg Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. In Proceedings of joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 866–873, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Maik Anderka</author>
</authors>
<title>A wikipedia-based multilingual retrieval model.</title>
<date>2008</date>
<booktitle>In Proceedings of the European Conference on Information Retrieval,</booktitle>
<pages>522--530</pages>
<location>Glasgow, Scotland.</location>
<contexts>
<context position="42520" citStr="Potthast et al., 2008" startWordPosition="6787" endWordPosition="6790">mple, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge</context>
</contexts>
<marker>Potthast, Stein, Anderka, 2008</marker>
<rawString>Martin Potthast, Benno Stein, and Maik Anderka. 2008. A wikipedia-based multilingual retrieval model. In Proceedings of the European Conference on Information Retrieval, pages 522–530, Glasgow, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>519--526</pages>
<location>College Park, MD.</location>
<contexts>
<context position="42724" citStr="Rapp, 1999" startWordPosition="6823" endWordPosition="6824">11) use comparable corpora to transfer selectional preferences and sentiment labels. Wikipedia can be seen as a particularly rich type of comparable corpus with additional link structure. It has been used to compute semantic relatedness (Navigli and Ponzetto, 2012; Navigli and Ponzetto, 2010) and to compute conceptual document representations for crosslingual information retrieval (Potthast et al., 2008; Cimiano et al., 2009). Our work, does not require parallel or comparable corpora. We note, however, that translation lexicons such as the ones we use can be extracted from comparable corpora (Rapp, 1999; Vuli´c and Moens, 2012, and many others), though few papers are concerned with the translation at the level of semantic relations, as we are. Similar in this respect is Fung and Chen (2004), who translate FrameNet (Baker et al., 1998) into Chinese with a bilingual ontology. They use a relation-based pruning scheme that is somewhat comparable to our backtranslation filtering. To our knowledge, the most similar work to ours is (Mohammad et al., 2007), which also considers DSMs, albeit a different variety, namely conceptbased DSMs where targets are characterized in terms of their distribution o</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, pages 519–526, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Experiments on the Automatic Induction of German Semantic Verb Classes.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="2079" citStr="Walde, 2006" startWordPosition="299" endWordPosition="300">e of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, syntax-based DSMs are inherently more sparse than word spaces, which calls for a large corpus of well parsable data. It is thus not surprising that besides English (Baroni and Lenci, 2010), only few other languages possess large-scale syntax-based DSMs (Padó and Utt, 2012; Šnajd</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>Sabine Schulte im Walde. 2006. Experiments on the Automatic Induction of German Semantic Verb Classes. Computational Linguistics, 32(2):159–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Dimensions of meaning.</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing ’92,</booktitle>
<pages>787--796</pages>
<location>Minneapolis, MN.</location>
<contexts>
<context position="1712" citStr="Schütze, 1992" startWordPosition="241" endWordPosition="242">n successfully combine their strengths. 1 Introduction Building on the Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991), which states that words occurring in similar contexts are similar in meaning, distributional semantic models (DSMs) represent a word’s meaning via its occurrence in context in large corpora. Vector spaces, the most widely used type of DSMs, represent words as vectors in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their constru</context>
</contexts>
<marker>Schütze, 1992</marker>
<rawString>Hinrich Schütze. 1992. Dimensions of meaning. In Proceedings of Supercomputing ’92, pages 787–796, Minneapolis, MN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Michael Skinner</author>
<author>Jeff Bilmes</author>
</authors>
<title>Compiling a Massive, Multilingual Dictionary via Probabilistic Inference.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing,</booktitle>
<pages>262--270</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="10718" citStr="Soderland et al., 2009" startWordPosition="1665" endWordPosition="1668">ssible, we assume in this section that very few resources for the target language are available. The crosslingual methods we develop Figure 2: Sample of the English-German dict.cc dictionary; translations shown as dashed lines. here work without any target language corpora, either monolingual or bilingual. The only knowledge we use is a simple translation lexicon, that is, a list of translation pairs without translation probabilities, as shown in Figure 2. Translation lexicons of this type are arguably the most common bilingual resource and accurate ones exist for virtually any language pair (Soderland et al., 2009), even for languages with few available corpora. Furthermore, such translation lexicons are often crowdsourced and are available for download. For example, the website dict.cc provides numerous such lexicons for German and English. This approach promises in particular to yield models with a quality-coverage profile complementary to that of monolingual models (Mohammad et al., 2007; Peirsman and Padó, 2011): Crosslingual DMs are extracted from source language corpora which we assume to be parsed more accurately than target language corpora. In addition, the translation process can be designed t</context>
</contexts>
<marker>Soderland, Etzioni, Weld, Skinner, Bilmes, 2009</marker>
<rawString>Stephen Soderland, Oren Etzioni, Daniel S Weld, Michael Skinner, Jeff Bilmes, et al. 2009. Compiling a Massive, Multilingual Dictionary via Probabilistic Inference. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing, pages 262–270, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
</authors>
<title>Round-trip translation: What is it good for?</title>
<date>2005</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop,</booktitle>
<pages>127--133</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="15599" citStr="Somers, 2005" startWordPosition="2483" endWordPosition="2484"> forest sense). The right-hand side shows (part of) the German translations according to Eq. (1): both Holz (timber) and Wald (forest) are linked to both adjectives, leading to spurious edges in the German DM. 3.4 Filtering by Backtranslation Since the nature of the translation is not indicated in the translation lexicon, we exploit typical redundancies in the source DM, which often contains “quasi-synonymous” edges that express the same relation with different words, e.g., (book obj read) and (novel obj read). This allows us to score target edge candidates by how well we can “backtranslate” (Somers, 2005) them into the source language. This idea is illustrated in Figure 4. We still assume, as above, that wood has two translations, but that precut has only one. For the English edge (precut mod wood), we obtain two German candidate edges, namely (zugeschnitten mod Holz) and (zugeschnitten mod Wald). When backtranslating these candidates, the first one, (zugeschnitten mod Wald), maps only onto the original edge. The second one, (zugeschnitten mod Holz), is backtranslated into a different source edge, (precut mod timber), which makes it more probable. greatA groKA WaldN woodN HolzN mod mod mod mod</context>
</contexts>
<marker>Somers, 2005</marker>
<rawString>Harold Somers. 2005. Round-trip translation: What is it good for? In Proceedings of the Australasian Language Technology Workshop, pages 127–133, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Šnajder</author>
<author>Sebastian Padó</author>
<author>Željko Agi´c</author>
</authors>
<title>Building and evaluating a distributional memory for Croatian.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>784--789</pages>
<location>Sofia, Bulgaria.</location>
<marker>Šnajder, Padó, Agi´c, 2013</marker>
<rawString>Jan Šnajder, Sebastian Padó, and Željko Agi´c. 2013. Building and evaluating a distributional memory for Croatian. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 784–789, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Crosslingual induction of semantic roles.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>647--656</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="41532" citStr="Titov and Klementiev (2012)" startWordPosition="6644" endWordPosition="6647"> because we again do not have per-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn unsupervised POS taggers on multilingual parallel data, exploiting the differences between languages as soft constraints. Titov and Klementiev (2012) and Kozhevnikov and Titov (2013) induce shallow semantic parsers from parallel data. Klementiev et al. (2012) approach document classification with multitask learning, inducing a multilingual DSM. Since parallel corpora are not available in large quantities, other studies use comparable corpora which can provide additional features from the other language. For example, Merlo et al. (2002) improve English verb classification with new features derived from Chinese translations. De Smet and Moens (2009) learn multilingual topic models for news aggregation. Peirsman and Padó (2011) use comparable</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. Crosslingual induction of semantic roles. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 647–656, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="2116" citStr="Turney, 2006" startWordPosition="303" endWordPosition="304">s in a highdimensional space whose dimensions correspond to features of the words’ contexts. Word spaces represent the simplest case of DSMs in which the dimensions are simply the context words (Schütze, 1992). A notable subclass of DSMs are syntax-based models (Lin, 1998; Baroni and Lenci, 2010) which use (lexicalized) syntactic relations as dimensions. They are able to model more fine-grained distinctions than word spaces and have been found to be useful for tasks such as selectional preference learning (Erk et al., 2010), verb class induction (Schulte im Walde, 2006), analogical reasoning (Turney, 2006), and alternation discovery (Joanis et al., 2006). Despite their flexibility and usefulness, syntax-based DSMs are used less often than word-based spaces. An important reason is that their construction requires accurate parsers, which are unavailable for many languages. In addition, syntax-based DSMs are inherently more sparse than word spaces, which calls for a large corpus of well parsable data. It is thus not surprising that besides English (Baroni and Lenci, 2010), only few other languages possess large-scale syntax-based DSMs (Padó and Utt, 2012; Šnajder et al., 2013). This paper develops</context>
<context position="5934" citStr="Turney, 2006" startWordPosition="898" endWordPosition="899">ian (Section 7). The paper concludes with related work (Section 8) and a general discussion (Section 9). 2 Distributional Memory: A General Model of Syntax-based Vector Spaces 2.1 Motivation and Definition Simple syntax-based DSMs represent target words in terms of dimensions labeled with word-relation pairs (Lin, 1998; Grefenstette, 1994). Unfortunately, this representation only supports tasks that compare pairs of words with regard to their meaning (e.g., in synonymy detection or selectional preferences), but not for tasks such as analogical reasoning, where sets of word pairs are compared (Turney, 2006). To unify syntax-based DSMs, Baroni and Lenci (2010) proposed the Distributional Memory (DM) model which captures distributional information at the more general level of word-link-word triples, stored as a third order co-occurrence tensor. The DM tensor can be seen as a set of ordered wordlink-word tuples such as (pencil obj use) associated with a scoring function a: W x L x W -+ R+ that scores, for example, (pencil obj use) more highly than (elephant obj use). The DM tensor can be visualized as a directed graph whose nodes are labeled with lemmas and whose edges are labeled with links and sc</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Detecting highly confident word translations from comparable corpora without any prior knowledge.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>449--459</pages>
<location>Avignon, France.</location>
<marker>Vuli´c, Moens, 2012</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2012. Detecting highly confident word translations from comparable corpora without any prior knowledge. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449–459, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DeWitt Wallace</author>
<author>Lila Acheson Wallace</author>
</authors>
<title>Reader’s Digest, das Beste für Deutschland. Verlag Das Beste,</title>
<date>2005</date>
<location>Stuttgart, Germany.</location>
<marker>Wallace, Wallace, 2005</marker>
<rawString>DeWitt Wallace and Lila Acheson Wallace. 2005. Reader’s Digest, das Beste für Deutschland. Verlag Das Beste, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fridolin Wild</author>
<author>Christina Stahl</author>
<author>Gerald Stermsek</author>
<author>Gustaf Neumann</author>
</authors>
<title>Parameters driving effectiveness of automated essay scoring with LSA.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Computer-Aided Assessment Conference,</booktitle>
<pages>485--494</pages>
<location>Loughborough, UK.</location>
<contexts>
<context position="28005" citStr="Wild et al., 2008" startWordPosition="4445" endWordPosition="4448">e component analysis, a technique generally used to increase robustness to parameter choice and to combat sparsity.9 Models from the literature. We compare our models against the state of the art, represented by the respective best models from two previous studies (Zesch et al., 2007; Mohammad et al., 2007). They comprise monolingual ontology-based models that use GermaNet, (German) Wikipedia, or both (LinGN, 9We also built models using smaller context windows and Latent Semantic Analysis (LSA, Landauer, 1997), both with 500 dimensions and with an automatically optimized number of dimensions (Wild et al., 2008). Since these spaces did not consistently yield better results than the reported models using PCA, we do not report the results in detail. Class monolingual crosslingual &amp; multilingual (DE) Model DM.DE (DE) TYPEDM (EN) DM.XL naive DM.XL filter Nodes Edges 3.5M 78M 31K 131M 63K 5B 63K 1.7B 251 Model All Covered Acc Acc Cov Baselines and word-based DSMs 1 Random .25 .25 1 2 Frequency .31 .31 1 3 BOW .46 .46 .98 4 BOW PCA500 .55 .55 .98 Syntax-based DSMs 5 DM.DE (AllL) .48 .53 .84 6 DM.XL EN→DE naive (SPrfL) .47 .63 .58 7 DM.XL EN→DE filter (SPrfL) .46 .61 .58 8 DM.MULTI Backoff(7,5) .54 .58 .89 </context>
</contexts>
<marker>Wild, Stahl, Stermsek, Neumann, 2008</marker>
<rawString>Fridolin Wild, Christina Stahl, Gerald Stermsek, and Gustaf Neumann. 2008. Parameters driving effectiveness of automated essay scoring with LSA. In Proceedings of the 9th Computer-Aided Assessment Conference, pages 485–494, Loughborough, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection across Aligned Corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>200--207</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="40784" citStr="Yarowsky and Ngai, 2001" startWordPosition="6532" endWordPosition="6535">taset from machine-readable dictionaries. Overall, the results for Croatian are encouraging. They demonstrate that languages where parsing technology is still developing can in particular profit from cross- and multilingual methods. This is true even for relatively small translation dictionaries, matching previous results from the literature (Peirsman and Padó, 2011). 8 Related Work Given the resource gradient between English and other languages, the crosslingual induction of linguistic information has been an active topic of research. Many studies use parallel corpora. Annotation projection (Yarowsky and Ngai, 2001) transfers source language annotation directly onto target language 12We cannot provide significances for the BOW results because we again do not have per-item predictions. 254 sentence. It has been applied to various linguistic levels such as POS tagging and syntax (Hi and Hwa, 2005; Hwa et al., 2005, among others). Other studies use parallel data as indirect supervision for monolingual tasks. Diab and Resnik (2002) use translations as word sense labels; van der Plas and Tiedemann (2006) exploit multilingual distributional semantics for robust synonymy extraction. Naseem et al. (2009) learn u</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection across Aligned Corpora. In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 200–207, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
<author>Max Mühlhäuser</author>
</authors>
<title>Comparing Wikipedia and German Wordnet by evaluating semantic relatedness on multiple datasets.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>205--208</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="27671" citStr="Zesch et al., 2007" startWordPosition="4397" endWordPosition="4400">dels. We build a standard BOW model from the same German corpus SDEWAC used for DM.DE. We assume a window of 10 context words to the left and right. We use the top 10K most frequent content words (nouns, adjectives, verbs and adverbs) as dimensions. Our second BOW model (BOW PCA500) was reduced to 500 dimensions by applying principle component analysis, a technique generally used to increase robustness to parameter choice and to combat sparsity.9 Models from the literature. We compare our models against the state of the art, represented by the respective best models from two previous studies (Zesch et al., 2007; Mohammad et al., 2007). They comprise monolingual ontology-based models that use GermaNet, (German) Wikipedia, or both (LinGN, 9We also built models using smaller context windows and Latent Semantic Analysis (LSA, Landauer, 1997), both with 500 dimensions and with an automatically optimized number of dimensions (Wild et al., 2008). Since these spaces did not consistently yield better results than the reported models using PCA, we do not report the results in detail. Class monolingual crosslingual &amp; multilingual (DE) Model DM.DE (DE) TYPEDM (EN) DM.XL naive DM.XL filter Nodes Edges 3.5M 78M 3</context>
<context position="30685" citStr="Zesch et al. (2007)" startWordPosition="4913" endWordPosition="4916"> Model All Covered Corr Corr Cov Baselines and word-based DSMs 1 Frequency .13 .13 1 2 BOW .20 .21 .97 3 BOW PCA500 .34 .37 .97 Syntax-based DSMs 4 DM.DE (AllL) [PU12] .38 .43 .60 5 DM.XL EN→DE naive (SPrfL) .28 .45 .49 6 DM.XL EN→DE filter (SPrfL) .33 .49 .49 7 DM.MULTI Backoff(6,4) .40 .45 .69 8 DM.MULTI MaxSim(6,4) .42 .47 .69 Models from the literature 9 LinGN [MGHZ07] NA .50 .26 10 Lindist [MGHZ07] NA .51 .26 11 JCGN+PLWP [ZGM07] NA .59 .33 Table 6: Exp. 2: Coverage and correlation (Pearson’s r) for predicting word similarity on the Gur350 dataset. MGHZ07: Mohammad et al. (2007)8, ZGM07: Zesch et al. (2007)9, PU12: Padó and Utt (2012). Best results for each model class in bold. Baselines and word-based DSMs. In both cases, uninformed baselines (random and frequency) perform badly. (In Exp. 1, the frequency baseline predicts the most frequent item as synonym; in Exp. 2, it predicts min(f(w1), f(w2)).) In contrast, wordbased DSMs perform quite well, particularly the dimensionality-reduced model (BOW PCA). Syntax-based DSM. We see a consistent quality versus coverage tradeoff among the different classes of syntax-based DSMs. The monolingual DM.DE model is significantly outperformed by the BOW model</context>
</contexts>
<marker>Zesch, Gurevych, Mühlhäuser, 2007</marker>
<rawString>Torsten Zesch, Iryna Gurevych, and Max Mühlhäuser. 2007. Comparing Wikipedia and German Wordnet by evaluating semantic relatedness on multiple datasets. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 205–208, Rochester, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
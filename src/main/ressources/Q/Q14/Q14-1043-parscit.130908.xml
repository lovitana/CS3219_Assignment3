<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000562">
<title confidence="0.999009">
Exploring Compositional Architectures and Word Vector Representations
for Prepositional Phrase Attachment
</title>
<author confidence="0.999472">
Yonatan Belinkov, Tao Lei, Regina Barzilay Amir Globerson
</author>
<affiliation confidence="0.999534">
Massachusetts Institute of Technology The Hebrew University
</affiliation>
<email confidence="0.988577">
{belinkov, taolei, regina}@csail.mit.edu gamir@cs.huji.ac.il
</email>
<sectionHeader confidence="0.998575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999613678571428">
Prepositional phrase (PP) attachment disam-
biguation is a known challenge in syntactic
parsing. The lexical sparsity associated with
PP attachments motivates research in word
representations that can capture pertinent syn-
tactic and semantic features of the word. One
promising solution is to use word vectors in-
duced from large amounts of raw text. How-
ever, state-of-the-art systems that employ such
representations yield modest gains in PP at-
tachment accuracy.
In this paper, we show that word vector repre-
sentations can yield significant PP attachment
performance gains. This is achieved via a
non-linear architecture that is discriminatively
trained to maximize PP attachment accuracy.
The architecture is initialized with word vec-
tors trained from unlabeled data, and relearns
those to maximize attachment accuracy. We
obtain additional performance gains with al-
ternative representations such as dependency-
based word vectors. When tested on both En-
glish and Arabic datasets, our method outper-
forms both a strong SVM classifier and state-
of-the-art parsers. For instance, we achieve
82.6% PP attachment accuracy on Arabic,
while the Turbo and Charniak self-trained
parsers obtain 76.7% and 80.8% respectively.1
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998477">
The problem of prepositional phrase (PP) attach-
ment disambiguation has been under investigation
</bodyText>
<footnote confidence="0.9209385">
1The code and data for this work are available at http:
//groups.csail.mit.edu/rbg/code/pp.
</footnote>
<bodyText confidence="0.993132">
She ate spaghetti with butter
She ate spaghetti with chopsticks
</bodyText>
<figureCaption confidence="0.743126">
Figure 1: Two sentences illustrating the importance
</figureCaption>
<bodyText confidence="0.978672047619048">
of lexicalization in PP attachment decisions. In the
top sentence, the PP with butter attaches to the noun
spaghetti. In the bottom sentence, the PP with chop-
sticks attaches to the verb ate.
for a long time. However, despite at least two
decades of research (Brill and Resnik, 1994; Rat-
naparkhi et al., 1994; Collins and Brooks, 1995), it
remains a major source of errors for state-of-the-art
parsers. For instance, in a comparative evaluation of
parser performance on the Wall Street Journal cor-
pus, Kummerfeld et al. (2012) report that PP attach-
ment is the largest source of errors across all parsers.
Moreover, the extent of improvement over time has
been rather limited, amounting to about 32% error
reduction since the work of (Collins, 1997).
PP attachments are inherently lexicalized and
part-of-speech (POS) tags are not sufficient for their
correct disambiguation. For example, the two sen-
tences in Figure 1 vary by a single noun — butter
vs chopsticks. However, this word determines the
structure of the whole PP attachment. If the corre-
</bodyText>
<page confidence="0.987809">
561
</page>
<bodyText confidence="0.988986035087719">
Transactions of the Association for Computational Linguistics, vol. 2, pp. 561–572, 2014. Action Editor: Ryan McDonald.
Submission batch: 10/2014; Revision batch 11/2014; Published 12/2014. c�2014 Association for Computational Linguistics.
sponding word is not observed in the training data, a
standard lexicalized parser does not have sufficient
information to distinguish between these two cases.
In fact, 72% of head-child pairs (e.g. spaghetti-
butter) from the Wall Street Journal test set are un-
seen in training. Not surprisingly, resolving these
ambiguities is challenging for parsers that have re-
stricted access to word semantics.
These considerations have motivated recent ex-
plorations in using distributed word representa-
tions for syntactic parsing (Cirik and S¸ensoy, 2013;
Socher et al., 2013; Lei et al., 2014). Low-
dimensional word embeddings help unveil seman-
tic similarity between words, thereby alleviating
the data sparsity problem associated with PP at-
tachment. In this context, large amounts of raw
data used to construct embeddings effectively en-
rich limited syntactic annotations. While these ap-
proaches show initial promise, they still lag behind
self-trained parsers (McClosky et al., 2006). These
parsers also utilize raw data but in a different way:
self-trained parsers use it to get additional (noisy)
annotations, without computing new word represen-
tations. These results suggest that embedding-based
representations have not yet been utilized to their
full potential.
We show that embedding-based representations
can indeed significantly improve PP attachment ac-
curacy. We achieve this by using such represen-
tations within a compositional neural network ar-
chitecture. The representations are initially learned
from an unlabeled corpus, but are then further dis-
criminatively trained to maximize PP attachment
accuracy. We also explore alternative representa-
tions such as dependency-based word vectors that
are trained from parsed texts using the syntactic con-
text in a dependency tree.
We test our approach for PP attachment disam-
biguation on English and Arabic datasets, com-
paring it to full-scale parsers and a support vec-
tor machine (SVM) ranker. Our model outper-
forms all baselines, including a self-trained parser.
The difference is particularly apparent on Arabic.
For instance, our model achieves PP attachment
accuracy of 82.6% while the Turbo (Martins et
al., 2013), RBG (Lei et al., 2014), and Charniak
self-trained (McClosky et al., 2006) parsers obtain
76.7%, 80.3%, and 80.8% respectively. Our results
demonstrate that relearning the embeddings con-
tributes to the model performance, across a range of
configurations. We also notice that representations
based on syntactic context are more powerful than
those based on linear context. This may explain the
improved performance of self-trained parsers over
parsers that rely on linear context embeddings.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998771324324324">
Problem formulation Typically, PP attachment
disambiguation is modeled as a binary classification
decision between a preceding noun or verb (Brill
and Resnik, 1994; Ratnaparkhi et al., 1994; Collins
and Brooks, 1995; Olteanu and Moldovan, 2005;
ˇSuster, 2012). In addition, the problem of PP at-
tachment has also been addressed in the context of
full parsing (Atterer and Sch¨utze, 2007; Agirre et al.,
2008). For instance, Green (2009) engineered state-
split features for the Stanford parser to improve Ara-
bic PP attachment.
In this work, we do isolate PP attachments from
other parsing decisions. At the same time, we con-
sider a more realistic scenario where multiple can-
didate heads are allowed. We also compare against
full-scale parsers and show that our model predic-
tions improve a state-of-the-art dependency parser.
Information sources Lexical sparsity associated
with disambiguating PP attachments (Figure 1) has
spurred researchers to exploit a wide range of infor-
mation sources. On the one hand, researchers have
explored using manually crafted resources (Stetina
and Nagao, 1997; Gamallo et al., 2003; Olteanu
and Moldovan, 2005; Medimi and Bhattacharyya,
2007). For instance, Agirre et al. (2008) demon-
strate that using WordNet semantic classes bene-
fits PP attachment performance. On the other hand,
researchers have looked into using co-occurrence
statistics from raw text (Volk, 2002; Olteanu and
Moldovan, 2005; Gala and Lafourcade, 2007). Such
statistics can be translated into word vectors from
which a cosine similarity score is calculated (ˇSuster,
2012). We also rely on word vectors, but our model
captures more complex relations among them.
Algorithmic approach Our work is most similar
to recursive neural network parsers (Costa et al.,
2003; Menchetti et al., 2005; Socher et al., 2010). In
</bodyText>
<page confidence="0.995224">
562
</page>
<bodyText confidence="0.999323294117647">
particular, Socher et al. (2013) obtain good parsing
performance by building compositional representa-
tions from word vectors. However, to combat the
computational complexity of the full parsing sce-
nario, they rely on a probabilistic context-free gram-
mar to prune search space. In contrast, focusing on
PP attachment allows us to consider various neu-
ral network architectures that are more appropriate
for this task, including ternary, binary, and distance-
dependent compositions. Furthermore, we investi-
gate modifications to the original word vectors in
several important directions: enriching word vectors
with semantic and syntactic knowledge resources,
relearning them by backpropagating errors from su-
pervised data, and using dependency-based vectors.
We show that such modifications lead to better word
vectors and significant performance gains.
</bodyText>
<sectionHeader confidence="0.996885" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9994541875">
We begin by introducing some notation. All vectors
v E Rn are assumed to be column vectors. We de-
note a given sentence by x and the set of prepositions
in x by PR(x). In other words, PR(x) is the set of
words whose POS tags are prep. The PP attachment
label of the preposition z E PR(x) is denoted by
y(z) E x. Namely, y(z) = h indicates that the head
of the preposition z is h.
Our classification approach is to construct a scor-
ing function s(x, z, h; 0) for a preposition z E
PR(x) and its candidate head h in the sentence x.
We then choose the head by maximizing s(x, z, h; 0)
over h. The set of possible candidates {h} can be of
arbitrary size, thus departing from the binary classi-
fication scenario considered in much of the previous
work (Section 2). The set of parameters is 0.
</bodyText>
<subsectionHeader confidence="0.99656">
3.1 Compositional framework
</subsectionHeader>
<bodyText confidence="0.978874727272727">
Our approach to constructing the score function is
as follows. First, we assume that all words in the
sentence are represented as vectors in Rn. Next,
we compose vectors corresponding to the relevant
preposition, its candidate head, and other words in
the sentence to obtain a new vector p E Rn. The
final score is a linear function of this vector.
The basic composition operation is defined as a
single layer in a neural network (Socher et al., 2010).
Given vectors u, v E Rn, representing two words,
we form a new vector via a function:
</bodyText>
<equation confidence="0.975085">
g(W[u; v] + b) E Rn (1)
</equation>
<bodyText confidence="0.999681444444444">
where b E Rn is a vector of bias terms, [u; v] E R2n
is a concatenation of u and v into a column vector,
W E Rn×2n is a composition matrix, and g is a
non-linear activation function.2
Given a candidate head h for preposition z, we
apply such compositions to a set of words, resulting
in a vector p. The final score s(x, z, h; 0) is given
by w · p, where w E Rn is a weight vector. The
parameters to be learned are 0 = (W, b, w).
</bodyText>
<subsectionHeader confidence="0.99968">
3.2 Composition architectures
</subsectionHeader>
<bodyText confidence="0.999908392857143">
There are various possible ways to compose and ob-
tain the vector p. Table 1 shows three basic compo-
sition architectures that are used in our model. In all
cases, elements like the head of the PP, the preposi-
tion, and the first child of the preposition are com-
posed using Eq. 1 to derive a parent vector that is
then scored by the score vector w. The architec-
tures differ in the number of compositions and their
type. For instance, the Head-Child model uses only
the head and child in a single composition, ignoring
the preposition. The Head-Prep-Child-Ternary com-
poses all three elements simultenuously, reflecting
ternary interactions. The Head-Prep-Child model,
on the other hand, first composes the preposition and
child to form a parent p1 representing the PP, then
composes p1 with the head into another parent p2
(= p) that is scored by w. This two-step process fa-
cilitates capturing different syntactic relations with
different composition matrices. We turn to this next.
Granularity The basic composition architectures
(Table 1) assume a global matrix W for all composi-
tion operations. In the case of the Head-Prep-Child
model, we also consider a local variant with differ-
ent matrices for the two compositions: W bottom for
composing the preposition z with its child c into a
parent p1 representing the PP, and Wtop for com-
posing the head h with p1 into a parent p2. The
composition equations are then:
</bodyText>
<equation confidence="0.999637">
p1 = g(Wbottom[z; c] + bbottom)
p2 = g(Wtop[h; p1] + btop)
</equation>
<footnote confidence="0.989585">
2We use tanh which performed slightly better than sigmoid
in preliminary experiments.
</footnote>
<page confidence="0.995932">
563
</page>
<table confidence="0.947974125">
Model Equations Structure
Head-Child (HC) p = g(W[h; c] + b) p
h c
Head-Prep-Child (HPC) p1 = g(W[z; c] + b) p2
p2 = g(W[h; p1] + b) h p1
z c
Head-Prep-Child-Ternary p = g(WTern[h; z; c] + b) p
(HPCT) h z c
</table>
<tableCaption confidence="0.991629">
Table 1: Basic composition architectures. h, z, c ∈ Rn are vectors for the head, the preposition, and
</tableCaption>
<bodyText confidence="0.9929082">
its child respectively; p, p1, p2 ∈ Rn are parent vectors created during composition operations; W ∈
Rnx2n, WTern ∈ Rnx3n are binary and ternary composition matrices respectively; b ∈ Rn is a bias term;
and g is a non-linear function.
In this case, the set of parameters is θ =
(Wtop; btop; Wbottom; bbottom; w). We call this
variant the Head-Prep-Child-Local (HPCL) model.
The composition architectures described thus far
only considered the composed words but not their
relative position in the sentence. Such position in-
formation may be useful, since candidates closer to
the preposition are typically more likely to attach.
To model this difference, we introduce distance-
dependent parameters and modify the Head-Prep-
Child model (Table 1, middle row) as follows: for
a head h at distance d from the preposition, we let:
</bodyText>
<equation confidence="0.98692">
p2 = g(Wd[h; p1] + bd)
</equation>
<bodyText confidence="0.999807523809524">
where Wd ∈ Rnx2n and bd ∈ Rn are the ma-
trix and bias for composing with heads at dis-
tance d from the preposition. p1 is defined as
in Table 1. The set of parameters is then θ =
({Wd; bd}d; W; b; w). To reduce the number of
parameters we use only d = 1, ... , 5, and clip dis-
tances greater than 5. We name this model Head-
Prep-Child-Dist (HPCD).
Context It may also be useful to exploit words sur-
rounding the candidate head such as the following
word. This can be integrated in the composition ar-
chitectures in the following way: for each candidate
head, represented by a vector h ∈ Rn, concatenate
a vector representing the word following the can-
didate. If such a vector is not available, append a
zero vector. This results in a new vector h&apos; ∈ R2n
representing the head. To compose it with a vector
p1 ∈ Rn representing the PP, we use a composition
matrix of size n × 3n, similar to the ternary com-
position described above. We refer to this model as
Head-Prep-Child-Next (HPCN).
</bodyText>
<subsectionHeader confidence="0.997393">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.9997705">
For training, we adopt a max-margin framework.
Given a training corpus of pairs of sentences and
attachments, {x(i), y(i)}, we seek to minimize the
following objective function:
</bodyText>
<equation confidence="0.9989165">
J(θ) = T E max Is(x(i),z, h; θ)
i=1 z∈PR(x(i)) h
]
−s(x(i), z, y(i)(z); θ) + A(h, y(i)(z)) (2)
</equation>
<bodyText confidence="0.9747725">
where A is the zero-one loss.
For optimization we use minibatch AdaGrad
(Duchi et al., 2011). Note that the objective is non-
differentiable so AdaGrad is used with the subgradi-
ent of J(θ), calculated with backpropagation.
For regularization we use Dropout (Hinton et al.,
2012), a recent method for preventing co-adaptation
of features, where input units to the neural network
</bodyText>
<page confidence="0.988743">
564
</page>
<bodyText confidence="0.99975535">
are randomly dropped. Random dropping occurs in-
dependently for each training example and has the
effect of creating multiple thinned networks that are
trained with shared parameters. In our implementa-
tion we dropout input units before each non-linear
layer, including the initial word vectors. We do not
dropout units after the final non-linear layer. Note
that Dropout is known to be especially useful when
combined with AdaGrad (Wager et al., 2013).
Hyperparameters and initialization We use the
following default hyperparameters without further
tuning unless noted otherwise: Dropout parameter
p = 0.5 (Hinton et al., 2012), AdaGrad initial learn-
ing rate q = 1.0 (Dyer, n.d.), and minibatch size
of 500. Learned parameters are initialized simi-
larly to previous work (Bengio and Glorot, 2010;
Socher et al., 2013): composition matrices are set
to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias
terms b are set to zero; and the weight vector is set
to w ∼ U(−1√n, 1√n).
</bodyText>
<sectionHeader confidence="0.956162" genericHeader="method">
4 Word vector representations
</sectionHeader>
<bodyText confidence="0.999980761904762">
Our approach assumes a vector representation for
each word. Such representations have gained popu-
larity in recent years, due to the ability to train them
from large unlabeled datasets, and their ease of use
in a wide variety of tasks (Turian et al., 2010).
There are various approaches to training vector
representations (Collobert and Weston, 2008; Ben-
gio et al., 2009). Here we chose to focus on the
Skip-gram method recently proposed by Mikolov et
al. (2013a). The Skip-gram model maximizes the
average log-probability of every word generating its
context, which is modeled via a neural net architec-
ture, but without the non-linearity. To improve effi-
ciency, this probability is approximated by a hierar-
chical softmax (Mikolov et al., 2013b) with vocabu-
lary words represented in a binary Huffman tree.3
In the simplest variant of our method, we train
the Skip-gram representation on unlabeled text, and
use it as a fixed representation when training the PP
attachment model (see Section 3.3). Below we con-
sider several variations on this approach.
</bodyText>
<footnote confidence="0.9856995">
3Preliminary experiments with other model variations (e.g.
negative sampling) have not led to notable performance gains.
</footnote>
<figureCaption confidence="0.76090825">
Figure 2: Illustration of an enriched word vector.
Initial dimensions learned from raw texts are en-
riched with binary vectors indicating part-of-speech
tags, VerbNet frames, and WordNet hypernyms.
</figureCaption>
<subsectionHeader confidence="0.99494">
4.1 Relearning word vectors
</subsectionHeader>
<bodyText confidence="0.999980954545455">
The Skip-gram word vectors are originally learned
from raw text, with the objective of maximizing the
likelihood of co-occurring words. Here our goal is
to maximize PP attachment accuracy, and it is possi-
ble that a different representation is optimal for this
task. We may thus take a discriminative approach
and update the vectors to maximize PP attachment
accuracy. Technically this just requires taking the
subgradient of our objective (Eq. 2) with respect to
the word vectors, and updating them accordingly.
Adding the word vectors as parameters signifi-
cantly increases the number of free parameters in the
model, and may lead to overfitting. To reduce this
effect, we use Dropout regularization (Section 3.3).
We also employ a smaller initial learning rate for the
word vectors compared to other model parameters.4
Finally, note that since the objective is non-
convex, the vectors obtained after this procedure
will typically depend on the initial value used. The
relearning procedure may thus be viewed as fine-
tuning the word vectors to improve PP attachment
accuracy.
</bodyText>
<subsectionHeader confidence="0.971754">
4.2 Enriching word vectors
</subsectionHeader>
<bodyText confidence="0.999933363636364">
The word vectors we use are trained from raw text.
However, it is easy to enrich them using structured
knowledge resources such as VerbNet or WordNet,
as well as morpho-syntactic information available in
treebanks.
Our approach to enriching word vectors is to ex-
tend them with binary vectors. For example, given
a vector h for the candidate head, we add binary-
valued dimensions for its part-of-speech and that of
the following word. Next we add a binary dimension
for VerbNet indicating whether the candidate head
</bodyText>
<footnote confidence="0.7799405">
4We tuned η = [0.001, 0.01, 0.1, 1] on the English dev set
and chose the best value (η = 0.001) for all other experiments.
</footnote>
<page confidence="0.995123">
565
</page>
<bodyText confidence="0.999694">
appears with the preposition in a verb frame. Finally,
for each top hypernym in WordNet, we add a binary
dimension indicating whether it is a hypernym of
the candidate head, aiming for semantic clustering
information. Note that we do not perform sense dis-
ambiguation so this information may be noisy.
Figure 2 illustrates the resulting enriched vector.
Similar dimensions are appended to vectors repre-
senting other words participating in the composi-
tions. Our experiments show that such an extension
significantly improves performance.
</bodyText>
<subsectionHeader confidence="0.991557">
4.3 Syntactic word vectors
</subsectionHeader>
<bodyText confidence="0.999982">
In the standard Skip-gram model word vectors are
trained from raw text using the linear context of
neighboring words. We also consider an alterna-
tive method for creating word vectors by using the
syntactic context of words. Such syntactic context
is expected to be relevant for resolving PP attach-
ments. Given a dependency-parsed text, we follow
Bansal et al. (2014) and create a new corpus of tu-
ples (l, g, p, c, l), for every word c, its parent p with
dependency label l, and its grandparent g. Then we
train an ordinary Skip-gram model on this corpus,
but with a small window size of 2. Note that the label
l appears on both ends so it contributes to the con-
text of the word as well as its grandparent. We find
that syntactic vectors yield significant performance
gains compared to standard vectors.5
</bodyText>
<sectionHeader confidence="0.999816" genericHeader="method">
5 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.998939">
5.1 Extracting PP attachments
</subsectionHeader>
<bodyText confidence="0.998574769230769">
Instances of PP attachment decisions are extracted
from standard treebanks. We use the CATiB de-
pendency treebank (Habash and Roth, 2009) for
Arabic and a conversion of the Penn treebank
(PTB) to dependency format for English.6 Standard
train/dev/test splits are used: sections 2-21/22/23 of
the PTB for English, and the split from the SPRML
shared-task for Arabic (Seddah et al., 2013). As Ta-
ble 2 shows, the datasets of the two languages are
fairly similar in size, except for the much larger set
of prepositions in the English data.
Extracting instances of PP attachments from the
treebanks is done in the following way. For each
</bodyText>
<footnote confidence="0.838897">
5We also experimented with another method for creating
syntactic vectors by Levy and Goldberg (2014) and observed
</footnote>
<table confidence="0.999844444444444">
Arabic English
Train Test Train Test
Total 42,387 3,917 35,359 1,951
Candidates 4.5 4.3 3.7 3.6
Vocab sizes
All 8,230 2,944 11,429 2,440
Heads 8,225 2,936 10,395 2,133
Preps 13 10 72 46
Children 4,222 1,424 5,504 983
</table>
<tableCaption confidence="0.978054">
Table 2: Statistics of extracted PP attachments,
showing total sizes, average number of candidate
heads, and vocabulary sizes.
</tableCaption>
<table confidence="0.99971675">
Arabic English
Corpus arTenTen Wikipedia BLLIP
Tokens 130M 120M 43M
Types 43K 218K 317K
</table>
<tableCaption confidence="0.9953875">
Table 3: Statistics of Arabic and English corpora
used for creating word vectors.
</tableCaption>
<bodyText confidence="0.999612">
preposition, we look for all possible candidate heads
in a fixed preceding window. Typically, these will
be nouns or verbs. Only prepositions with a noun
child are considered, leaving out some rare excep-
tions. Empirically, limiting candidate heads to ap-
pear close enough before the preposition is not an
unrealistic assumption: we choose a 10-word win-
dow and find that it covers about 94/99% of Ara-
bic/English PP attachments. Unambiguous attach-
ments with a single possible candidate are discarded.
</bodyText>
<subsectionHeader confidence="0.998171">
5.2 Creating word vectors
</subsectionHeader>
<bodyText confidence="0.926032857142857">
The initial word vectors are created from raw texts
using the Skip-gram model with hierarchical soft-
max, as described in Section 4.7 We use a portion of
Wikipedia for English8 and the arTenTen corpus for
Arabic, containing web texts crawled in 2012 (Be-
linkov et al., 2013; Arts et al., 2014). Table 3
similar performance gains.
</bodyText>
<footnote confidence="0.998941142857143">
6We used the Pennconverter tool: http://nlp.cs.
lth.se/software/treebank-converter.
7We used the word2vec tool: https://code.google.
com/p/word2vec, with default settings. We experimented
with word vectors of 25, 50, 100, and 200 dimensions, and
found 100 to work best in most cases.
8http://mattmahoney.net/dc/textdata.
</footnote>
<page confidence="0.995634">
566
</page>
<bodyText confidence="0.999973192307692">
shows the comparable sizes of the datasets. The
Arabic corpus has been tokenized and lemmatized
with MADA (Habash and Rambow, 2005; Habash et
al., 2005), a necessary procedure in order to separate
some prepositions from their child words. In addi-
tion, lemmatization reduces vocabulary size and fa-
cilitates sharing information between different mor-
phological variants that have the same meaning.
For syntactic word vectors, we use the English
vectors in (Bansal et al., 2014), which were trained
from a parsed BLLIP corpus (minus PTB). For Ara-
bic, we first convert the morphologically-processed
arTenTen corpus to CoNLL format with the SPMRL
shared-task scripts (Seddah et al., 2013). Then we
parse the corpus with a baseline MST parser (Sec-
tion 5.3) and create syntactic word vectors as de-
scribed in Section 4.3. The Arabic syntactic vectors
will be made available to the research community.
For enriching word vectors, we use part-of-speech
information9 from the treebanks as well as the Ara-
bic and English VerbNets (Kipper et al., 2008;
Mousser, 2010) and WordNets (Rodr´ıquez et al.,
2008; Princeton University, 2010). In total, these
resources add to each word vector 46/67 extended
dimensions in Arabic/English, representing syntac-
tic and semantic information about the word.
</bodyText>
<subsectionHeader confidence="0.992803">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.9996473125">
We compare against full-scale parsers, an SVM
ranker, and a simple but strong baseline of always
choosing the closest candidate head.
Parsers We mostly compare with dependency
parsers, including the state-of-the-art Turbo (Mar-
tins et al., 2010; Martins et al., 2013) and RBG
parsers (Lei et al., 2014), in addition to a second-
order MST parser (McDonald et al., 2005) and
the Malt parser (Nivre et al., 2006). We also
compare with two constituency parsers: an RNN
parser (Socher et al., 2013), which also uses word
vectors and a neural network approach, and the
Charniak self-trained reranking parser (McClosky et
al., 2006). We train all parsers on the train/dev sets
and report their PP attachment accuracy on the test
sets.10 For the self-trained parser we followed the
</bodyText>
<footnote confidence="0.941077">
9We use gold POS tags in all systems and experiments.
10The only exception is the RNN parser, for which we use
the built-in English model in Stanford parser’s (version 3.4);
</footnote>
<table confidence="0.999385111111111">
Source Feature Template
Treebank hw, pw, cw, hw-cw,
ht, nt, hpd
WordNet hh, ch
VerbNet hpvf
Word Vectors sim(hv,cv)
Brown Clusters hc*, pc*, cc*,
hc4, pc4, cc4,
hc*-cc*, hc4-cc4
</table>
<tableCaption confidence="0.995188">
Table 4: Feature templates for the SVM baseline.
</tableCaption>
<bodyText confidence="0.983025272727272">
Bi-lexical templates appear with a “-”. Abbrevi-
ations: hw/pw/cw = head/prep/child word, ht =
head tag, nt = next word tag, hpd = head-prep dis-
tance; hh/ch = head/child hypernym; hpvf = head-
prep found in verb frame; hv/cv = head/child vec-
tor; hc*/pc*/cc* = head/prep/child full bit string,
hc4/pc4/cc4 = head/prep/child 4-bit prefix.
procedure in (McClosky et al., 2006) with the same
unsupervised datasets that are used in our PP model.
SVM We consider a learning-to-rank formulation
for our problem, where each example provides a cor-
rect candidate head and several incorrect candidates.
We order these in a simple list where the correct can-
didate has the highest rank and all other candidates
have a single lower rank. We then rank these with
an SVM ranker11 and select the top candidate. This
formulation is necessary because we depart from the
binary classification scenario that was used in previ-
ous work (Section 2).
The SVM ranker uses the following features: the
candidate head, preposition, and child; bi-lexical
conjunctions of head-child; part-of-speech tags of
the head and the following word; and the candi-
date head’s distance from the preposition. We also
add top WordNet hypernyms for head and child,
and an indicator of whether the preposition appears
in the head’s sub-categorization frame in VerbNet.
This configuration parallels the information used in
our model but fails to exploit raw data. Therefore,
we consider two more types of features. First, we
use word vectors by computing cosine similarity be-
tween vectors of the candidate head and the child
for Arabic we do train a new RNN model.
</bodyText>
<footnote confidence="0.7079805">
11We use SVMRank: http://www.cs.cornell.
edu/people/tj/svm_light/svm_rank.html.
</footnote>
<page confidence="0.995765">
567
</page>
<bodyText confidence="0.9999884">
of the preposition. This feature was found useful
in previous work on PP attachment (ˇSuster, 2012).
While this limits the contribution of the word vec-
tors to the learned model to one dimension, attempts
to use more dimensions in the SVM were unsuccess-
ful.12 In contrast, the compositional models better
capture the full dimensionality of the word vectors.
A second type of features induced from raw data
that we consider are Brown clusters, which were
found to be useful in dependency parsing (Koo et
al., 2008). Compared to distributed vectors, Brown
clusters provide a more discrete representation that
is easier to incorporate in the SVM. We create clus-
ters from our unsupervised corpora using the Liang
(2005) implementation of Brown’s algorithm, and
add features in the spirit of (Koo et al., 2008).
Specifically, we add full and prefixed bit strings for
the head, preposition, and child, as well as bi-lexical
versions for head-child pairs.13 Table 4 shows a
summary of the SVM features.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999975380952381">
Table 5 summarizes the results of our model and
other systems. Our best results are obtained with the
Head-Prep-Child-Dist (HPCD) model using syn-
tactic vectors, enriching, and relearning. The full
model outperforms both full-scale parsers and a ded-
icated SVM model. More advanced parsers do
demonstrate higher accuracy on the PP attachment
task, but our method outperforms them as well. Note
that the self-trained reranking parser (Charniak-RS)
performs especially well and quite better than the
RNN parser. This trend is consistent with the results
in (Kummerfeld et al., 2012; Socher et al., 2013).
Our compositional architecture is effective in ex-
ploiting raw data: using only standard word vec-
tors with no enriching, our HPCD (basic) model per-
forms comparably to an SVM with access to all en-
riching features. Once we improve the representa-
tion, we outperform both the SVM and full parsers.
In comparison, the contribution of raw data to the
SVM, as either word vectors or Brown clusters, is
rather limited.
</bodyText>
<footnote confidence="0.760771">
12For example, we tried adding all word vector dimensions
as features, as well as element-wise products of the vectors rep-
resenting the head and the child.
13As in (Koo et al., 2008), we limit the number of unique bit
strings to 1,000 so full strings are not equivalent to word forms.
</footnote>
<table confidence="0.999701277777778">
System Arabic English
Closest 62.7 81.7
SVM 77.0 86.0
w/ word vectors 77.5 85.9
w/ Brown clusters 78.0 85.7
w/ Brown clusters+prefixes 77.0 85.7
Malt 75.4 79.7
MST 76.7 86.8
Turbo 76.7 88.3
RBG 80.3 88.4
RNN 68.9 85.1
Charniak-RS 80.8 88.6
HPCD (basic) 77.1 85.4
w/ enriching 80.4 87.7
w/ syntactic 79.1 87.1
w/ relearning 80.0 86.6
HPCD (full) 82.6 88.7
RBG + HPCD (full) 82.7 90.1
</table>
<tableCaption confidence="0.986818">
Table 5: PP attachment accuracy of our HPCD
</tableCaption>
<bodyText confidence="0.99831104">
model compared to other systems. HPCD (full) uses
syntactic vectors with enriching and relearning. The
last row is a modified RBG parser with a feature for
the PP predictions of our model.
The relative performance is consistent across both
English and Arabic. The table also demonstrates that
the Arabic dataset is more challenging for all mod-
els. This can be explained by a larger average candi-
date set (Table 2), a freer word order that manifests
in longer attachments (average head and PP distance
is 3.3 in Arabic vs 1.5 in English), and the lexical
sparsity induced by the richer morphology.
Effect on parsing To investigate how our PP at-
tachment model contributes to the general parsing
task, we incorporated the predictions of our model
in an existing dependency parser. We modified the
RBG parser (Lei et al., 2014) such that a binary arc
feature fires for every PP attachment predicted by
our model. For both test sets, we find that the pars-
ing performance, measured as the unlabeled attach-
ment score (UAS), increases by adding the predic-
tions in this way (Table 6). The modified parser also
achieves the best PP attachment numbers (Table 5).
Interestingly, incorporating the PP predictions in
a parser leads to a gain in parsing performance that
</bodyText>
<page confidence="0.994359">
568
</page>
<table confidence="0.65008325">
System Arabic English
RBG 87.70 93.96
RBG + predicted PP 87.95 94.05
RBG + oracle PP 89.09 94.42
</table>
<tableCaption confidence="0.7996745">
Table 6: Parsing performance (UAS) of the RBG
parser, with predicted and oracle PPs.
</tableCaption>
<bodyText confidence="0.999808791666667">
is relatively larger than the gain in PP accuracy. For
example, relative to an oracle upper bound of forc-
ing gold PP arcs in the parser output (Table 6), the
reduction in English parsing errors is 20%, whereas
the reduction in PP errors is only 15%. This affirms
the importance of PP attachment disambiguation for
predicting other attachments in the sentence.
RRR dataset Much of the previous work on PP
attachment focused on a binary classification sce-
nario (Section 2) and has been evaluated on the RRR
dataset (Ratnaparkhi et al., 1994). Such systems
cannot be easily evaluated in our setting which al-
lows multiple candidate heads. On the other hand,
our full model exploits contextual information that
is not available in the RRR dataset. Nevertheless,
using a simpler version of our model we obtain an
accuracy of 85.6% on the RRR test set.14 This is
comparable to much of the previous work (Olteanu
and Moldovan, 2005), but still lags behind the 88.1%
of Stetina and Nagao (1997), who also used Word-
Net information. However, our use of WordNet is
rather limited compared to theirs, indicating that our
enriching method can be improved with other types
of information.
</bodyText>
<subsectionHeader confidence="0.999377">
6.1 Alternative composition architectures
</subsectionHeader>
<bodyText confidence="0.9999772">
In this section we analyze how different composition
architectures (Section 3.2) contribute to the overall
performance. To isolate the contribution of the ar-
chitecture, we focus on standard (linear) word vec-
tors, with no relearning or enriching. As Figure 3
shows, simpler models tend to perform worse than
more complex ones. The best variants use differ-
ent composition matrices based on the distance of
the candidate head from the PP (HPCD, HPCDN).
While the results shown are for 100-dimensional
</bodyText>
<footnote confidence="0.982280333333333">
14Here we applied basic preprocessing similarly to (Collins
and Brooks, 1995), converting 4-digit numbers to YEAR and
other numbers to NUMBER; other tokens were lower-cased.
</footnote>
<figureCaption confidence="0.915753">
Figure 3: PP attachment accuracy of different archi-
tectures. (HC) uses only the candidate head and the
</figureCaption>
<bodyText confidence="0.992083">
child of the preposition; (HPC*) models use head,
preposition, and child, with the following variants:
(HPCT) ternary composition; (HPCL) local matri-
ces for top and bottom compositions; (HPCN) con-
text words; (HPCD) distance-dependent matrices;
(HPCDN) combines HPCD+HPCN.
vectors, similar trends are observed with lower di-
mensions, although the gaps between simple and
complex models are then more substantial.
We have also experimented with compositions
through the entire PP subtree. However, this resulted
in a performance drop (to about 50%), implying that
adding more words to the composite representation
of the PP does not lead to a distinguishing represen-
tation with regards to the possible candidate heads.
</bodyText>
<subsectionHeader confidence="0.999759">
6.2 Alternative representations
</subsectionHeader>
<bodyText confidence="0.999825923076923">
In this section, we analyze how different word vector
representations (Section 4) contribute to our model.
We focus on the HPCD model, which builds a two-
step composite structure with distance-dependent
composition matrices. We take the basic represen-
tation to be standard (linear) word vectors, without
enriching or relearning. In each paragraph below,
we investigate how a different aspect of the repre-
sentation affects PP attachment performance.
Relearning word vectors In traditional architec-
tures, the process of word vector induction is inde-
pendent of the way the vector is used in the pars-
ing algorithm. We hypothesize that by connecting
</bodyText>
<page confidence="0.997947">
569
</page>
<figureCaption confidence="0.9943815">
Figure 4: Effects of relearning standard word vec-
tors in English and Arabic.
</figureCaption>
<bodyText confidence="0.999356633333333">
these two processes and tailoring the word vectors
to the task at hand, we can further improve the ac-
curacy of the PP attachments. We thus relearn the
word vectors during training as described in Sec-
tion 4.1. Indeed, as Figure 4 shows, doing so con-
sistently improves performance, especially with low
dimensional vectors. Interestingly, syntactic word
vectors also benefit from the update (Table 8). This
indicates that the supervised PP attachments provide
complementary signal to noisy dependencies used to
construct syntactic vectors.
Enriching word vectors A substantial body of
work has demonstrated that multiple features can
help in disambiguating PP attachments (Section 2).
To this end, we enrich word vectors with addi-
tional knowledge resources (Section 4.2). As Ta-
ble 7 shows, this enrichment yields sizable perfor-
mance gains. Most of the gain comes from part-
of-speech information, while WordNet and VerbNet
have a smaller contribution. Updating the word vec-
tors during training has an additional positive effect.
Note that even with no enrichment, our model
performs comparably to an SVM with access to all
enriching features (Table 5). When enriched, our
model outperforms the SVM by a margin of 2-3%.
With relearning, the gaps are even larger.
Syntactic word vectors While most of the work
in parsing relies on linear word vectors (Socher et
al., 2013; Lei et al., 2014), we consider an alter-
native vector representation that captures syntactic
</bodyText>
<table confidence="0.9989495">
Representation Arabic English
w/o enriching 77.1 85.4
w/ enriching
+POS 78.5 86.4
+NextPOS 79.7 87.5
+WordNet+VerbNet 80.4 87.7
w/ enriching+relearning 81.7 88.1
w/ enriching+relearn.+syn. 82.6 88.7
</table>
<tableCaption confidence="0.984624">
Table 7: PP attachment accuracy when enriching
</tableCaption>
<bodyText confidence="0.526401333333333">
word vectors with part-of-speech tags of the candi-
date head (POS) and the following word (NextPOS),
and with WordNet and VerbNet features.
</bodyText>
<table confidence="0.9952195">
Representation Arabic English
Linear 77.1 85.4
Syntactic 79.1 87.1
Syntactic w/ relearning 80.7 87.7
</table>
<tableCaption confidence="0.929761">
Table 8: PP attachment accuracy of linear (standard)
and syntactic (dependency-based) word vectors.
</tableCaption>
<bodyText confidence="0.998137">
context. As described in Section 4.3, such vectors
are induced from a large corpus processed by an au-
tomatic dependency parser. While the corpus is most
likely fraught with parsing mistakes, it still contains
sufficient dependency information for learning high-
quality word vectors. Table 8 confirms our assump-
tions: using syntactically-informed vectors yields
significant performance gains.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999939461538462">
This work explores word representations for PP at-
tachment disambiguation, a key problem in syntac-
tic parsing. We show that word vectors, induced
from large volumes of raw data, yield significant
PP attachment performance gains. This is achieved
via a non-linear architecture that is discriminatively
trained to maximize PP attachment accuracy. We
demonstrate performance gains by using alternative
representations such as syntactic word vectors and
by enriching vectors with semantic and syntactic in-
formation. We also find that the predictions of our
model improve the parsing performance of a state-
of-the-art dependency parser.
</bodyText>
<page confidence="0.993221">
570
</page>
<sectionHeader confidence="0.999205" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999970214285714">
This research is developed in collaboration with
the Arabic Language Technologies (ALT) group at
Qatar Computing Research Institute (QCRI) within
the IYAS project. The authors acknowledge the sup-
port of the U.S. Army Research Office under grant
number W911NF-10-1-0533, the DARPA BOLT
program and the US-Israel Binational Science Foun-
dation (BSF, Grant No 2012330). We thank the
MIT NLP group and the TACL reviewers for their
comments, and Djam´e Seddah and Mohit Bansal for
helping with scripts and data. Any opinions, find-
ings, conclusions, or recommendations expressed in
this paper are those of the authors, and do not neces-
sarily reflect the views of the funding organizations.
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999632423529411">
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving Parsing and PP Attachment Per-
formance with Sense Information. In Proceedings of
ACL-HLT.
Tressy Arts, Yonatan Belinkov, Nizar Habash, Adam Kil-
garriff, and Vit Suchomel. 2014. arTenTen: Arabic
Corpus and Word Sketches. Journal of King Saud Uni-
versity - Computer and Information Sciences.
Michaela Atterer and Hinrich Sch¨utze. 2007. Preposi-
tional Phrase Attachment Without Oracles. Computa-
tional Linguistics, 33(4).
Mohit Bansal, Keving Gimpel, and Karen Livescu. 2014.
Tailoring Continuous Word Representations for De-
pendency Parsing. In Proceedings of ACL.
Yonatan Belinkov, Nizar Habash, Adam Kilgarriff, Noam
Ordan, Ryan Roth, and V´ıt Suchomel. 2013. arTen-
Ten: a new, vast corpus for Arabic. In Proceedings of
WACL.
Yoshua Bengio and Xavier Glorot. 2010. Understanding
the difficulty of training deep feedforward neural net-
works. In Proceedings of AISTATS, volume 9, May.
Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and
Jason Weston. 2009. Curriculum learning. In Pro-
ceedings of ICML.
Eric Brill and Philip Resnik. 1994. A Rule-Based
Approach to Prepositional Phrase Attachment Disam-
biguation. In Proceedings of COLING, volume 2.
Volkan Cirik and H¨usn¨u S¸ensoy. 2013. The AI-KU Sys-
tem at the SPMRL 2013 Shared Task : Unsupervised
Features for Dependency Parsing. In Proceedings of
SPMRL.
Michael Collins and James Brooks. 1995. Prepo-
sitional Phrase Attachment through a Backed-Off
Model. CoRR.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings ofACL.
Ronan Collobert and Jason Weston. 2008. A Unified
Architecture for Natural Language Processing: Deep
Neural Networks with Multitask Learning. In Pro-
ceedings of ICML.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo, and
Giovanni Soda. 2003. Towards Incremental Parsing of
Natural Language Using Recursive Neural Networks.
Applied Intelligence, 19(1-2).
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. JMLR, 12.
Chris Dyer. n.d. Notes on AdaGrad. Unpublished
manuscript, available at http://www.ark.cs.
cmu.edu/cdyer/adagrad.pdf.
Nuria Gala and Mathieu Lafourcade. 2007. PP attach-
ment ambiguity resolution with corpus-based pattern
distributions and lexical signatures. ECTI-CIT Trans-
actions on Computer and Information Technology, 2.
Pablo Gamallo, Alexandre Agustini, and Gabriel P.
Lopes. 2003. Acquiring Semantic Classes to Elabo-
rate Attachment Heuristics. In Progress in Artificial
Intelligence, volume 2902 of LNCS. Springer Berlin
Heidelberg.
Spence Green. 2009. Improving Parsing Per-
formance for Arabic PP Attachment Ambi-
guity. Unpublished manuscript, available at
http://www-nlp.stanford.edu/courses/
cs224n/2009/fp/30-tempremove.pdf.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
ACL.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of the
ACL-IJCNLP.
Nizar Habash, Owen Rambow, and Ryan Roth. 2005.
MADA+TOKAN: A Toolkit for Arabic Tokenization,
Diacritization, Morphological Disambiguation, POS
Tagging, Stemming and Lemmatization. In Proceed-
ings of the Second International Conference on Arabic
Language Resources and Tools.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Im-
proving neural networks by preventing co-adaptation
of feature detectors. CoRR.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification of
English verbs. Language Resources and Evaluation,
42(1).
</reference>
<page confidence="0.972985">
571
</page>
<reference confidence="0.99979223655914">
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-HLT.
Jonathan K. Kummerfeld, David Hall, James R. Curran,
and Dan Klein. 2012. Parser Showdown at the Wall
Street Corral: An Empirical Investigation of Error
Types in Parser Output. In Proceedings of EMNLP-
CoNLL.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-Rank Tensors for Scor-
ing Dependency Structures. In Proceedings of ACL.
Omer Levy and Yoav Goldberg. 2014. Dependency-
Based Word Embeddings. In Proceedings of ACL.
Percy Liang. 2005. Semi-Supervised Learning for Natu-
ral Language. Master’s thesis, Massachusetts Institute
of Technology.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of EMNLP.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the Turbo: Fast Third-Order Non-
Projective Turbo Parsers. In Proceedings of ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective Self-Training for Parsing. In Proceed-
ings of HLT-NAACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Proceedings of ACL.
Srinivas Medimi and Pushpak Bhattacharyya. 2007. A
Flexible Unsupervised PP-attachment Method Using
Semantic Information. In Proceedings of IJCAI.
Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and
Massimiliano Pontil. 2005. Wide coverage natural
language processing using kernel methods and neural
networks for structured data. Pattern Recognition Let-
ters, 26(12).
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Represen-
tations in Vector Space. In Proceedings of Workshop
atICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,
and Jeffrey Dean. 2013b. Distributed Representations
of Words and Phrases and their Compositionality. In
Proceedings of NIPS.
Jaouad Mousser. 2010. A Large Coverage Verb Taxon-
omy for Arabic. In Proceedings of LREC.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A
Data-Driven Parser-Generator for Dependency Pars-
ing. In Proceedings of LREC.
Marian Olteanu and Dan Moldovan. 2005. PP-
attachment Disambiguation using Large Context. In
Proceedings of HLT-EMNLP.
Princeton University. 2010. WordNet. http://
wordnet.princeton.edu.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A Maximum Entropy Model for Prepositional
Phrase Attachment. In Proceedings of HLT.
Horacio Rodr´ıquez, David Farwell, Javi Ferreres, Manuel
Bertran, Musa Alkhalifa, and M. Antonia Mart´ı. 2008.
Arabic WordNet: Semi-automatic Extensions using
Bayesian Inference. In Proceedings of LREC.
Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Can-
dito, Jinho D. Choi, Rich´ard Farkas, Jennifer Fos-
ter, et al. 2013. Overview of the SPMRL 2013
Shared Task: A Cross-Framework Evaluation of Pars-
ing Morphologically Rich Languages. In Proceedings
of SPMRL.
Richard Socher, Christopher D. Manning, and Andrew Y.
Ng. 2010. Learning Continuous Phrase Representa-
tions and Syntactic Parsing with Recursive Neural Net-
works. In Proceedings of NIPS Deep Learning and
Unsupervised Feature Learning Workshop.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with Compositional
Vector Grammars. In Proceedings of ACL.
Jiri Stetina and Makoto Nagao. 1997. Corpus Based
PP Attachment Ambiguity Resolution with a Semantic
Dictionary. In Fifth Workshop on Very Large Corpora.
Simon ˇSuster. 2012. Resolving PP-attachment ambi-
guity in French with distributional methods. Mas-
ter’s thesis, Universit´e de Lorraine &amp; Rijksuniversiteit
Groningen.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of ACL.
Martin Volk. 2002. Combining Unsupervised and Super-
vised Methods for PP Attachment Disambiguation. In
Proceedings of COLING.
Stefan Wager, Sida Wang, and Percy Liang. 2013.
Dropout Training as Adaptive Regularization. In Pro-
ceedings of NIPS.
</reference>
<page confidence="0.997411">
572
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.467908">
<title confidence="0.9996985">Exploring Compositional Architectures and Word Vector for Prepositional Phrase Attachment</title>
<author confidence="0.985">Yonatan Belinkov</author>
<author confidence="0.985">Tao Lei</author>
<author confidence="0.985">Regina Barzilay Amir Globerson</author>
<affiliation confidence="0.999942">Massachusetts Institute of Technology The Hebrew University</affiliation>
<email confidence="0.970724">taolei,gamir@cs.huji.ac.il</email>
<abstract confidence="0.999343571428572">Prepositional phrase (PP) attachment disambiguation is a known challenge in syntactic parsing. The lexical sparsity associated with PP attachments motivates research in word representations that can capture pertinent syntactic and semantic features of the word. One promising solution is to use word vectors induced from large amounts of raw text. However, state-of-the-art systems that employ such representations yield modest gains in PP attachment accuracy. In this paper, we show that word vector representations can yield significant PP attachment performance gains. This is achieved via a non-linear architecture that is discriminatively trained to maximize PP attachment accuracy. The architecture is initialized with word vectors trained from unlabeled data, and relearns those to maximize attachment accuracy. We obtain additional performance gains with alternative representations such as dependencybased word vectors. When tested on both English and Arabic datasets, our method outperforms both a strong SVM classifier and stateof-the-art parsers. For instance, we achieve 82.6% PP attachment accuracy on Arabic, while the Turbo and Charniak self-trained</abstract>
<intro confidence="0.49679">obtain 76.7% and 80.8%</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Timothy Baldwin</author>
<author>David Martinez</author>
</authors>
<title>Improving Parsing and PP Attachment Performance with Sense Information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="6190" citStr="Agirre et al., 2008" startWordPosition="917" endWordPosition="920">sed on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇSuster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, re</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving Parsing and PP Attachment Performance with Sense Information. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tressy Arts</author>
<author>Yonatan Belinkov</author>
<author>Nizar Habash</author>
<author>Adam Kilgarriff</author>
<author>Vit Suchomel</author>
</authors>
<title>arTenTen: Arabic Corpus and Word Sketches.</title>
<date>2014</date>
<journal>Journal of King Saud University - Computer and Information Sciences.</journal>
<contexts>
<context position="22284" citStr="Arts et al., 2014" startWordPosition="3622" endWordPosition="3625"> exceptions. Empirically, limiting candidate heads to appear close enough before the preposition is not an unrealistic assumption: we choose a 10-word window and find that it covers about 94/99% of Arabic/English PP attachments. Unambiguous attachments with a single possible candidate are discarded. 5.2 Creating word vectors The initial word vectors are created from raw texts using the Skip-gram model with hierarchical softmax, as described in Section 4.7 We use a portion of Wikipedia for English8 and the arTenTen corpus for Arabic, containing web texts crawled in 2012 (Belinkov et al., 2013; Arts et al., 2014). Table 3 similar performance gains. 6We used the Pennconverter tool: http://nlp.cs. lth.se/software/treebank-converter. 7We used the word2vec tool: https://code.google. com/p/word2vec, with default settings. We experimented with word vectors of 25, 50, 100, and 200 dimensions, and found 100 to work best in most cases. 8http://mattmahoney.net/dc/textdata. 566 shows the comparable sizes of the datasets. The Arabic corpus has been tokenized and lemmatized with MADA (Habash and Rambow, 2005; Habash et al., 2005), a necessary procedure in order to separate some prepositions from their child words.</context>
</contexts>
<marker>Arts, Belinkov, Habash, Kilgarriff, Suchomel, 2014</marker>
<rawString>Tressy Arts, Yonatan Belinkov, Nizar Habash, Adam Kilgarriff, and Vit Suchomel. 2014. arTenTen: Arabic Corpus and Word Sketches. Journal of King Saud University - Computer and Information Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Atterer</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Prepositional Phrase Attachment Without Oracles.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<marker>Atterer, Sch¨utze, 2007</marker>
<rawString>Michaela Atterer and Hinrich Sch¨utze. 2007. Prepositional Phrase Attachment Without Oracles. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Keving Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring Continuous Word Representations for Dependency Parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="19720" citStr="Bansal et al. (2014)" startWordPosition="3193" endWordPosition="3196"> illustrates the resulting enriched vector. Similar dimensions are appended to vectors representing other words participating in the compositions. Our experiments show that such an extension significantly improves performance. 4.3 Syntactic word vectors In the standard Skip-gram model word vectors are trained from raw text using the linear context of neighboring words. We also consider an alternative method for creating word vectors by using the syntactic context of words. Such syntactic context is expected to be relevant for resolving PP attachments. Given a dependency-parsed text, we follow Bansal et al. (2014) and create a new corpus of tuples (l, g, p, c, l), for every word c, its parent p with dependency label l, and its grandparent g. Then we train an ordinary Skip-gram model on this corpus, but with a small window size of 2. Note that the label l appears on both ends so it contributes to the context of the word as well as its grandparent. We find that syntactic vectors yield significant performance gains compared to standard vectors.5 5 Experimental setup 5.1 Extracting PP attachments Instances of PP attachment decisions are extracted from standard treebanks. We use the CATiB dependency treeban</context>
<context position="23120" citStr="Bansal et al., 2014" startWordPosition="3741" endWordPosition="3744">rimented with word vectors of 25, 50, 100, and 200 dimensions, and found 100 to work best in most cases. 8http://mattmahoney.net/dc/textdata. 566 shows the comparable sizes of the datasets. The Arabic corpus has been tokenized and lemmatized with MADA (Habash and Rambow, 2005; Habash et al., 2005), a necessary procedure in order to separate some prepositions from their child words. In addition, lemmatization reduces vocabulary size and facilitates sharing information between different morphological variants that have the same meaning. For syntactic word vectors, we use the English vectors in (Bansal et al., 2014), which were trained from a parsed BLLIP corpus (minus PTB). For Arabic, we first convert the morphologically-processed arTenTen corpus to CoNLL format with the SPMRL shared-task scripts (Seddah et al., 2013). Then we parse the corpus with a baseline MST parser (Section 5.3) and create syntactic word vectors as described in Section 4.3. The Arabic syntactic vectors will be made available to the research community. For enriching word vectors, we use part-of-speech information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ı</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Keving Gimpel, and Karen Livescu. 2014. Tailoring Continuous Word Representations for Dependency Parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Belinkov</author>
<author>Nizar Habash</author>
<author>Adam Kilgarriff</author>
<author>Noam Ordan</author>
<author>Ryan Roth</author>
<author>V´ıt Suchomel</author>
</authors>
<title>arTenTen: a new, vast corpus for Arabic.</title>
<date>2013</date>
<booktitle>In Proceedings of WACL.</booktitle>
<contexts>
<context position="22264" citStr="Belinkov et al., 2013" startWordPosition="3617" endWordPosition="3621">, leaving out some rare exceptions. Empirically, limiting candidate heads to appear close enough before the preposition is not an unrealistic assumption: we choose a 10-word window and find that it covers about 94/99% of Arabic/English PP attachments. Unambiguous attachments with a single possible candidate are discarded. 5.2 Creating word vectors The initial word vectors are created from raw texts using the Skip-gram model with hierarchical softmax, as described in Section 4.7 We use a portion of Wikipedia for English8 and the arTenTen corpus for Arabic, containing web texts crawled in 2012 (Belinkov et al., 2013; Arts et al., 2014). Table 3 similar performance gains. 6We used the Pennconverter tool: http://nlp.cs. lth.se/software/treebank-converter. 7We used the word2vec tool: https://code.google. com/p/word2vec, with default settings. We experimented with word vectors of 25, 50, 100, and 200 dimensions, and found 100 to work best in most cases. 8http://mattmahoney.net/dc/textdata. 566 shows the comparable sizes of the datasets. The Arabic corpus has been tokenized and lemmatized with MADA (Habash and Rambow, 2005; Habash et al., 2005), a necessary procedure in order to separate some prepositions fro</context>
</contexts>
<marker>Belinkov, Habash, Kilgarriff, Ordan, Roth, Suchomel, 2013</marker>
<rawString>Yonatan Belinkov, Nizar Habash, Adam Kilgarriff, Noam Ordan, Ryan Roth, and V´ıt Suchomel. 2013. arTenTen: a new, vast corpus for Arabic. In Proceedings of WACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Xavier Glorot</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of AISTATS,</booktitle>
<volume>9</volume>
<contexts>
<context position="15447" citStr="Bengio and Glorot, 2010" startWordPosition="2502" endWordPosition="2505"> parameters. In our implementation we dropout input units before each non-linear layer, including the initial word vectors. We do not dropout units after the final non-linear layer. Note that Dropout is known to be especially useful when combined with AdaGrad (Wager et al., 2013). Hyperparameters and initialization We use the following default hyperparameters without further tuning unless noted otherwise: Dropout parameter p = 0.5 (Hinton et al., 2012), AdaGrad initial learning rate q = 1.0 (Dyer, n.d.), and minibatch size of 500. Learned parameters are initialized similarly to previous work (Bengio and Glorot, 2010; Socher et al., 2013): composition matrices are set to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias terms b are set to zero; and the weight vector is set to w ∼ U(−1√n, 1√n). 4 Word vector representations Our approach assumes a vector representation for each word. Such representations have gained popularity in recent years, due to the ability to train them from large unlabeled datasets, and their ease of use in a wide variety of tasks (Turian et al., 2010). There are various approaches to training vector representations (Collobert and Weston, 2008; Bengio et al., 2009). Here we chose to focus </context>
</contexts>
<marker>Bengio, Glorot, 2010</marker>
<rawString>Yoshua Bengio and Xavier Glorot. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of AISTATS, volume 9, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>J´erˆome Louradour</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="16022" citStr="Bengio et al., 2009" startWordPosition="2603" endWordPosition="2607">rly to previous work (Bengio and Glorot, 2010; Socher et al., 2013): composition matrices are set to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias terms b are set to zero; and the weight vector is set to w ∼ U(−1√n, 1√n). 4 Word vector representations Our approach assumes a vector representation for each word. Such representations have gained popularity in recent years, due to the ability to train them from large unlabeled datasets, and their ease of use in a wide variety of tasks (Turian et al., 2010). There are various approaches to training vector representations (Collobert and Weston, 2008; Bengio et al., 2009). Here we chose to focus on the Skip-gram method recently proposed by Mikolov et al. (2013a). The Skip-gram model maximizes the average log-probability of every word generating its context, which is modeled via a neural net architecture, but without the non-linearity. To improve efficiency, this probability is approximated by a hierarchical softmax (Mikolov et al., 2013b) with vocabulary words represented in a binary Huffman tree.3 In the simplest variant of our method, we train the Skip-gram representation on unlabeled text, and use it as a fixed representation when training the PP attachment</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Philip Resnik</author>
</authors>
<title>A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING,</booktitle>
<volume>2</volume>
<contexts>
<context position="2106" citStr="Brill and Resnik, 1994" startWordPosition="300" endWordPosition="303"> 80.8% respectively.1 1 Introduction The problem of prepositional phrase (PP) attachment disambiguation has been under investigation 1The code and data for this work are available at http: //groups.csail.mit.edu/rbg/code/pp. She ate spaghetti with butter She ate spaghetti with chopsticks Figure 1: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP with butter attaches to the noun spaghetti. In the bottom sentence, the PP with chopsticks attaches to the verb ate. for a long time. However, despite at least two decades of research (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995), it remains a major source of errors for state-of-the-art parsers. For instance, in a comparative evaluation of parser performance on the Wall Street Journal corpus, Kummerfeld et al. (2012) report that PP attachment is the largest source of errors across all parsers. Moreover, the extent of improvement over time has been rather limited, amounting to about 32% error reduction since the work of (Collins, 1997). PP attachments are inherently lexicalized and part-of-speech (POS) tags are not sufficient for their correct disambiguation. For exa</context>
<context position="5946" citStr="Brill and Resnik, 1994" startWordPosition="877" endWordPosition="880">ned (McClosky et al., 2006) parsers obtain 76.7%, 80.3%, and 80.8% respectively. Our results demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇSuster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Eric Brill and Philip Resnik. 1994. A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation. In Proceedings of COLING, volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volkan Cirik</author>
<author>H¨usn¨u S¸ensoy</author>
</authors>
<title>Shared Task : Unsupervised Features for Dependency Parsing.</title>
<date>2013</date>
<booktitle>The AI-KU System at the SPMRL</booktitle>
<marker>Cirik, S¸ensoy, 2013</marker>
<rawString>Volkan Cirik and H¨usn¨u S¸ensoy. 2013. The AI-KU System at the SPMRL 2013 Shared Task : Unsupervised Features for Dependency Parsing. In Proceedings of SPMRL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional Phrase Attachment through a Backed-Off Model.</title>
<date>1995</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="2159" citStr="Collins and Brooks, 1995" startWordPosition="309" endWordPosition="312">f prepositional phrase (PP) attachment disambiguation has been under investigation 1The code and data for this work are available at http: //groups.csail.mit.edu/rbg/code/pp. She ate spaghetti with butter She ate spaghetti with chopsticks Figure 1: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP with butter attaches to the noun spaghetti. In the bottom sentence, the PP with chopsticks attaches to the verb ate. for a long time. However, despite at least two decades of research (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995), it remains a major source of errors for state-of-the-art parsers. For instance, in a comparative evaluation of parser performance on the Wall Street Journal corpus, Kummerfeld et al. (2012) report that PP attachment is the largest source of errors across all parsers. Moreover, the extent of improvement over time has been rather limited, amounting to about 32% error reduction since the work of (Collins, 1997). PP attachments are inherently lexicalized and part-of-speech (POS) tags are not sufficient for their correct disambiguation. For example, the two sentences in Figure 1 vary by a single </context>
<context position="5998" citStr="Collins and Brooks, 1995" startWordPosition="885" endWordPosition="888">80.3%, and 80.8% respectively. Our results demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇSuster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency p</context>
<context position="32772" citStr="Collins and Brooks, 1995" startWordPosition="5325" endWordPosition="5328">mation. 6.1 Alternative composition architectures In this section we analyze how different composition architectures (Section 3.2) contribute to the overall performance. To isolate the contribution of the architecture, we focus on standard (linear) word vectors, with no relearning or enriching. As Figure 3 shows, simpler models tend to perform worse than more complex ones. The best variants use different composition matrices based on the distance of the candidate head from the PP (HPCD, HPCDN). While the results shown are for 100-dimensional 14Here we applied basic preprocessing similarly to (Collins and Brooks, 1995), converting 4-digit numbers to YEAR and other numbers to NUMBER; other tokens were lower-cased. Figure 3: PP attachment accuracy of different architectures. (HC) uses only the candidate head and the child of the preposition; (HPC*) models use head, preposition, and child, with the following variants: (HPCT) ternary composition; (HPCL) local matrices for top and bottom compositions; (HPCN) context words; (HPCD) distance-dependent matrices; (HPCDN) combines HPCD+HPCN. vectors, similar trends are observed with lower dimensions, although the gaps between simple and complex models are then more su</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional Phrase Attachment through a Backed-Off Model. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2572" citStr="Collins, 1997" startWordPosition="378" endWordPosition="379">entence, the PP with chopsticks attaches to the verb ate. for a long time. However, despite at least two decades of research (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995), it remains a major source of errors for state-of-the-art parsers. For instance, in a comparative evaluation of parser performance on the Wall Street Journal corpus, Kummerfeld et al. (2012) report that PP attachment is the largest source of errors across all parsers. Moreover, the extent of improvement over time has been rather limited, amounting to about 32% error reduction since the work of (Collins, 1997). PP attachments are inherently lexicalized and part-of-speech (POS) tags are not sufficient for their correct disambiguation. For example, the two sentences in Figure 1 vary by a single noun — butter vs chopsticks. However, this word determines the structure of the whole PP attachment. If the corre561 Transactions of the Association for Computational Linguistics, vol. 2, pp. 561–572, 2014. Action Editor: Ryan McDonald. Submission batch: 10/2014; Revision batch 11/2014; Published 12/2014. c�2014 Association for Computational Linguistics. sponding word is not observed in the training data, a st</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="16000" citStr="Collobert and Weston, 2008" startWordPosition="2599" endWordPosition="2602">eters are initialized similarly to previous work (Bengio and Glorot, 2010; Socher et al., 2013): composition matrices are set to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias terms b are set to zero; and the weight vector is set to w ∼ U(−1√n, 1√n). 4 Word vector representations Our approach assumes a vector representation for each word. Such representations have gained popularity in recent years, due to the ability to train them from large unlabeled datasets, and their ease of use in a wide variety of tasks (Turian et al., 2010). There are various approaches to training vector representations (Collobert and Weston, 2008; Bengio et al., 2009). Here we chose to focus on the Skip-gram method recently proposed by Mikolov et al. (2013a). The Skip-gram model maximizes the average log-probability of every word generating its context, which is modeled via a neural net architecture, but without the non-linearity. To improve efficiency, this probability is approximated by a hierarchical softmax (Mikolov et al., 2013b) with vocabulary words represented in a binary Huffman tree.3 In the simplest variant of our method, we train the Skip-gram representation on unlabeled text, and use it as a fixed representation when trai</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
<author>Vincenzo Lombardo</author>
<author>Giovanni Soda</author>
</authors>
<title>Towards Incremental Parsing of Natural Language Using Recursive Neural Networks. Applied Intelligence,</title>
<date>2003</date>
<contexts>
<context position="7546" citStr="Costa et al., 2003" startWordPosition="1124" endWordPosition="1127">i and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In 562 particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of the full parsing scenario, they rely on a probabilistic context-free grammar to prune search space. In contrast, focusing on PP attachment allows us to consider various neural network architectures that are more appropriate for this task, including ternary, binary, and distancedependent compositions. Furthermore, we investigate modifications to the original word vec</context>
</contexts>
<marker>Costa, Frasconi, Lombardo, Soda, 2003</marker>
<rawString>Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo, and Giovanni Soda. 2003. Towards Incremental Parsing of Natural Language Using Recursive Neural Networks. Applied Intelligence, 19(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<date>2011</date>
<booktitle>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. JMLR,</booktitle>
<pages>12</pages>
<contexts>
<context position="14366" citStr="Duchi et al., 2011" startWordPosition="2334" endWordPosition="2337"> h&apos; ∈ R2n representing the head. To compose it with a vector p1 ∈ Rn representing the PP, we use a composition matrix of size n × 3n, similar to the ternary composition described above. We refer to this model as Head-Prep-Child-Next (HPCN). 3.3 Training For training, we adopt a max-margin framework. Given a training corpus of pairs of sentences and attachments, {x(i), y(i)}, we seek to minimize the following objective function: J(θ) = T E max Is(x(i),z, h; θ) i=1 z∈PR(x(i)) h ] −s(x(i), z, y(i)(z); θ) + A(h, y(i)(z)) (2) where A is the zero-one loss. For optimization we use minibatch AdaGrad (Duchi et al., 2011). Note that the objective is nondifferentiable so AdaGrad is used with the subgradient of J(θ), calculated with backpropagation. For regularization we use Dropout (Hinton et al., 2012), a recent method for preventing co-adaptation of features, where input units to the neural network 564 are randomly dropped. Random dropping occurs independently for each training example and has the effect of creating multiple thinned networks that are trained with shared parameters. In our implementation we dropout input units before each non-linear layer, including the initial word vectors. We do not dropout </context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. JMLR, 12.</rawString>
</citation>
<citation valid="false">
<authors>
<author>n d</author>
</authors>
<note>Notes on AdaGrad. Unpublished manuscript, available at http://www.ark.cs. cmu.edu/cdyer/adagrad.pdf.</note>
<marker>d, </marker>
<rawString>Chris Dyer. n.d. Notes on AdaGrad. Unpublished manuscript, available at http://www.ark.cs. cmu.edu/cdyer/adagrad.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuria Gala</author>
<author>Mathieu Lafourcade</author>
</authors>
<title>PP attachment ambiguity resolution with corpus-based pattern distributions and lexical signatures.</title>
<date>2007</date>
<journal>ECTI-CIT Transactions on Computer and Information Technology,</journal>
<volume>2</volume>
<contexts>
<context position="7235" citStr="Gala and Lafourcade, 2007" startWordPosition="1075" endWordPosition="1078">rmation sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored using manually crafted resources (Stetina and Nagao, 1997; Gamallo et al., 2003; Olteanu and Moldovan, 2005; Medimi and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In 562 particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of the full parsing scenario, they rely on a probabilistic c</context>
</contexts>
<marker>Gala, Lafourcade, 2007</marker>
<rawString>Nuria Gala and Mathieu Lafourcade. 2007. PP attachment ambiguity resolution with corpus-based pattern distributions and lexical signatures. ECTI-CIT Transactions on Computer and Information Technology, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gamallo</author>
<author>Alexandre Agustini</author>
<author>Gabriel P Lopes</author>
</authors>
<title>Acquiring Semantic Classes to Elaborate Attachment Heuristics.</title>
<date>2003</date>
<booktitle>In Progress in Artificial Intelligence,</booktitle>
<volume>2902</volume>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="6893" citStr="Gamallo et al., 2003" startWordPosition="1025" endWordPosition="1028"> to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored using manually crafted resources (Stetina and Nagao, 1997; Gamallo et al., 2003; Olteanu and Moldovan, 2005; Medimi and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to</context>
</contexts>
<marker>Gamallo, Agustini, Lopes, 2003</marker>
<rawString>Pablo Gamallo, Alexandre Agustini, and Gabriel P. Lopes. 2003. Acquiring Semantic Classes to Elaborate Attachment Heuristics. In Progress in Artificial Intelligence, volume 2902 of LNCS. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
</authors>
<title>Improving Parsing Performance for Arabic PP Attachment Ambiguity.</title>
<date>2009</date>
<note>Unpublished manuscript, available at http://www-nlp.stanford.edu/courses/ cs224n/2009/fp/30-tempremove.pdf.</note>
<contexts>
<context position="6218" citStr="Green (2009)" startWordPosition="923" endWordPosition="924">werful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇSuster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored usin</context>
</contexts>
<marker>Green, 2009</marker>
<rawString>Spence Green. 2009. Improving Parsing Performance for Arabic PP Attachment Ambiguity. Unpublished manuscript, available at http://www-nlp.stanford.edu/courses/ cs224n/2009/fp/30-tempremove.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="22776" citStr="Habash and Rambow, 2005" startWordPosition="3688" endWordPosition="3691">ia for English8 and the arTenTen corpus for Arabic, containing web texts crawled in 2012 (Belinkov et al., 2013; Arts et al., 2014). Table 3 similar performance gains. 6We used the Pennconverter tool: http://nlp.cs. lth.se/software/treebank-converter. 7We used the word2vec tool: https://code.google. com/p/word2vec, with default settings. We experimented with word vectors of 25, 50, 100, and 200 dimensions, and found 100 to work best in most cases. 8http://mattmahoney.net/dc/textdata. 566 shows the comparable sizes of the datasets. The Arabic corpus has been tokenized and lemmatized with MADA (Habash and Rambow, 2005; Habash et al., 2005), a necessary procedure in order to separate some prepositions from their child words. In addition, lemmatization reduces vocabulary size and facilitates sharing information between different morphological variants that have the same meaning. For syntactic word vectors, we use the English vectors in (Bansal et al., 2014), which were trained from a parsed BLLIP corpus (minus PTB). For Arabic, we first convert the morphologically-processed arTenTen corpus to CoNLL format with the SPMRL shared-task scripts (Seddah et al., 2013). Then we parse the corpus with a baseline MST p</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow. 2005. Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
</authors>
<title>CATiB: The Columbia Arabic Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP.</booktitle>
<contexts>
<context position="20345" citStr="Habash and Roth, 2009" startWordPosition="3304" endWordPosition="3307">d create a new corpus of tuples (l, g, p, c, l), for every word c, its parent p with dependency label l, and its grandparent g. Then we train an ordinary Skip-gram model on this corpus, but with a small window size of 2. Note that the label l appears on both ends so it contributes to the context of the word as well as its grandparent. We find that syntactic vectors yield significant performance gains compared to standard vectors.5 5 Experimental setup 5.1 Extracting PP attachments Instances of PP attachment decisions are extracted from standard treebanks. We use the CATiB dependency treebank (Habash and Roth, 2009) for Arabic and a conversion of the Penn treebank (PTB) to dependency format for English.6 Standard train/dev/test splits are used: sections 2-21/22/23 of the PTB for English, and the split from the SPRML shared-task for Arabic (Seddah et al., 2013). As Table 2 shows, the datasets of the two languages are fairly similar in size, except for the much larger set of prepositions in the English data. Extracting instances of PP attachments from the treebanks is done in the following way. For each 5We also experimented with another method for creating syntactic vectors by Levy and Goldberg (2014) and</context>
</contexts>
<marker>Habash, Roth, 2009</marker>
<rawString>Nizar Habash and Ryan Roth. 2009. CATiB: The Columbia Arabic Treebank. In Proceedings of the ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</booktitle>
<contexts>
<context position="22798" citStr="Habash et al., 2005" startWordPosition="3692" endWordPosition="3695">rTenTen corpus for Arabic, containing web texts crawled in 2012 (Belinkov et al., 2013; Arts et al., 2014). Table 3 similar performance gains. 6We used the Pennconverter tool: http://nlp.cs. lth.se/software/treebank-converter. 7We used the word2vec tool: https://code.google. com/p/word2vec, with default settings. We experimented with word vectors of 25, 50, 100, and 200 dimensions, and found 100 to work best in most cases. 8http://mattmahoney.net/dc/textdata. 566 shows the comparable sizes of the datasets. The Arabic corpus has been tokenized and lemmatized with MADA (Habash and Rambow, 2005; Habash et al., 2005), a necessary procedure in order to separate some prepositions from their child words. In addition, lemmatization reduces vocabulary size and facilitates sharing information between different morphological variants that have the same meaning. For syntactic word vectors, we use the English vectors in (Bansal et al., 2014), which were trained from a parsed BLLIP corpus (minus PTB). For Arabic, we first convert the morphologically-processed arTenTen corpus to CoNLL format with the SPMRL shared-task scripts (Seddah et al., 2013). Then we parse the corpus with a baseline MST parser (Section 5.3) an</context>
</contexts>
<marker>Habash, Rambow, Roth, 2005</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2005. MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization. In Proceedings of the Second International Conference on Arabic Language Resources and Tools.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2012</date>
<publisher>CoRR.</publisher>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>A large-scale classification of English verbs.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="23683" citStr="Kipper et al., 2008" startWordPosition="3833" endWordPosition="3836">ors, we use the English vectors in (Bansal et al., 2014), which were trained from a parsed BLLIP corpus (minus PTB). For Arabic, we first convert the morphologically-processed arTenTen corpus to CoNLL format with the SPMRL shared-task scripts (Seddah et al., 2013). Then we parse the corpus with a baseline MST parser (Section 5.3) and create syntactic word vectors as described in Section 4.3. The Arabic syntactic vectors will be made available to the research community. For enriching word vectors, we use part-of-speech information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classification of English verbs. Language Resources and Evaluation, 42(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT.</booktitle>
<contexts>
<context position="27323" citStr="Koo et al., 2008" startWordPosition="4419" endWordPosition="4422">ain a new RNN model. 11We use SVMRank: http://www.cs.cornell. edu/people/tj/svm_light/svm_rank.html. 567 of the preposition. This feature was found useful in previous work on PP attachment (ˇSuster, 2012). While this limits the contribution of the word vectors to the learned model to one dimension, attempts to use more dimensions in the SVM were unsuccessful.12 In contrast, the compositional models better capture the full dimensionality of the word vectors. A second type of features induced from raw data that we consider are Brown clusters, which were found to be useful in dependency parsing (Koo et al., 2008). Compared to distributed vectors, Brown clusters provide a more discrete representation that is easier to incorporate in the SVM. We create clusters from our unsupervised corpora using the Liang (2005) implementation of Brown’s algorithm, and add features in the spirit of (Koo et al., 2008). Specifically, we add full and prefixed bit strings for the head, preposition, and child, as well as bi-lexical versions for head-child pairs.13 Table 4 shows a summary of the SVM features. 6 Results Table 5 summarizes the results of our model and other systems. Our best results are obtained with the Head-</context>
<context position="29009" citStr="Koo et al., 2008" startWordPosition="4693" endWordPosition="4696">2; Socher et al., 2013). Our compositional architecture is effective in exploiting raw data: using only standard word vectors with no enriching, our HPCD (basic) model performs comparably to an SVM with access to all enriching features. Once we improve the representation, we outperform both the SVM and full parsers. In comparison, the contribution of raw data to the SVM, as either word vectors or Brown clusters, is rather limited. 12For example, we tried adding all word vector dimensions as features, as well as element-wise products of the vectors representing the head and the child. 13As in (Koo et al., 2008), we limit the number of unique bit strings to 1,000 so full strings are not equivalent to word forms. System Arabic English Closest 62.7 81.7 SVM 77.0 86.0 w/ word vectors 77.5 85.9 w/ Brown clusters 78.0 85.7 w/ Brown clusters+prefixes 77.0 85.7 Malt 75.4 79.7 MST 76.7 86.8 Turbo 76.7 88.3 RBG 80.3 88.4 RNN 68.9 85.1 Charniak-RS 80.8 88.6 HPCD (basic) 77.1 85.4 w/ enriching 80.4 87.7 w/ syntactic 79.1 87.1 w/ relearning 80.0 86.6 HPCD (full) 82.6 88.7 RBG + HPCD (full) 82.7 90.1 Table 5: PP attachment accuracy of our HPCD model compared to other systems. HPCD (full) uses syntactic vectors wi</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>David Hall</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="2350" citStr="Kummerfeld et al. (2012)" startWordPosition="339" endWordPosition="342"> with butter She ate spaghetti with chopsticks Figure 1: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP with butter attaches to the noun spaghetti. In the bottom sentence, the PP with chopsticks attaches to the verb ate. for a long time. However, despite at least two decades of research (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995), it remains a major source of errors for state-of-the-art parsers. For instance, in a comparative evaluation of parser performance on the Wall Street Journal corpus, Kummerfeld et al. (2012) report that PP attachment is the largest source of errors across all parsers. Moreover, the extent of improvement over time has been rather limited, amounting to about 32% error reduction since the work of (Collins, 1997). PP attachments are inherently lexicalized and part-of-speech (POS) tags are not sufficient for their correct disambiguation. For example, the two sentences in Figure 1 vary by a single noun — butter vs chopsticks. However, this word determines the structure of the whole PP attachment. If the corre561 Transactions of the Association for Computational Linguistics, vol. 2, pp.</context>
<context position="28393" citStr="Kummerfeld et al., 2012" startWordPosition="4587" endWordPosition="4590">hows a summary of the SVM features. 6 Results Table 5 summarizes the results of our model and other systems. Our best results are obtained with the Head-Prep-Child-Dist (HPCD) model using syntactic vectors, enriching, and relearning. The full model outperforms both full-scale parsers and a dedicated SVM model. More advanced parsers do demonstrate higher accuracy on the PP attachment task, but our method outperforms them as well. Note that the self-trained reranking parser (Charniak-RS) performs especially well and quite better than the RNN parser. This trend is consistent with the results in (Kummerfeld et al., 2012; Socher et al., 2013). Our compositional architecture is effective in exploiting raw data: using only standard word vectors with no enriching, our HPCD (basic) model performs comparably to an SVM with access to all enriching features. Once we improve the representation, we outperform both the SVM and full parsers. In comparison, the contribution of raw data to the SVM, as either word vectors or Brown clusters, is rather limited. 12For example, we tried adding all word vector dimensions as features, as well as element-wise products of the vectors representing the head and the child. 13As in (K</context>
</contexts>
<marker>Kummerfeld, Hall, Curran, Klein, 2012</marker>
<rawString>Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. 2012. Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-Rank Tensors for Scoring Dependency Structures.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3697" citStr="Lei et al., 2014" startWordPosition="544" endWordPosition="547">tion for Computational Linguistics. sponding word is not observed in the training data, a standard lexicalized parser does not have sufficient information to distinguish between these two cases. In fact, 72% of head-child pairs (e.g. spaghettibutter) from the Wall Street Journal test set are unseen in training. Not surprisingly, resolving these ambiguities is challenging for parsers that have restricted access to word semantics. These considerations have motivated recent explorations in using distributed word representations for syntactic parsing (Cirik and S¸ensoy, 2013; Socher et al., 2013; Lei et al., 2014). Lowdimensional word embeddings help unveil semantic similarity between words, thereby alleviating the data sparsity problem associated with PP attachment. In this context, large amounts of raw data used to construct embeddings effectively enrich limited syntactic annotations. While these approaches show initial promise, they still lag behind self-trained parsers (McClosky et al., 2006). These parsers also utilize raw data but in a different way: self-trained parsers use it to get additional (noisy) annotations, without computing new word representations. These results suggest that embedding-</context>
<context position="5300" citStr="Lei et al., 2014" startWordPosition="784" endWordPosition="787">d to maximize PP attachment accuracy. We also explore alternative representations such as dependency-based word vectors that are trained from parsed texts using the syntactic context in a dependency tree. We test our approach for PP attachment disambiguation on English and Arabic datasets, comparing it to full-scale parsers and a support vector machine (SVM) ranker. Our model outperforms all baselines, including a self-trained parser. The difference is particularly apparent on Arabic. For instance, our model achieves PP attachment accuracy of 82.6% while the Turbo (Martins et al., 2013), RBG (Lei et al., 2014), and Charniak self-trained (McClosky et al., 2006) parsers obtain 76.7%, 80.3%, and 80.8% respectively. Our results demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a </context>
<context position="24242" citStr="Lei et al., 2014" startWordPosition="3917" endWordPosition="3920">ll as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9We use gold POS tags in all systems and experiments. 10The only exception is the RNN parser, for which we use the built-in Engli</context>
<context position="30362" citStr="Lei et al., 2014" startWordPosition="4927" endWordPosition="4930">ormance is consistent across both English and Arabic. The table also demonstrates that the Arabic dataset is more challenging for all models. This can be explained by a larger average candidate set (Table 2), a freer word order that manifests in longer attachments (average head and PP distance is 3.3 in Arabic vs 1.5 in English), and the lexical sparsity induced by the richer morphology. Effect on parsing To investigate how our PP attachment model contributes to the general parsing task, we incorporated the predictions of our model in an existing dependency parser. We modified the RBG parser (Lei et al., 2014) such that a binary arc feature fires for every PP attachment predicted by our model. For both test sets, we find that the parsing performance, measured as the unlabeled attachment score (UAS), increases by adding the predictions in this way (Table 6). The modified parser also achieves the best PP attachment numbers (Table 5). Interestingly, incorporating the PP predictions in a parser leads to a gain in parsing performance that 568 System Arabic English RBG 87.70 93.96 RBG + predicted PP 87.95 94.05 RBG + oracle PP 89.09 94.42 Table 6: Parsing performance (UAS) of the RBG parser, with predict</context>
<context position="35828" citStr="Lei et al., 2014" startWordPosition="5801" endWordPosition="5804">). As Table 7 shows, this enrichment yields sizable performance gains. Most of the gain comes from partof-speech information, while WordNet and VerbNet have a smaller contribution. Updating the word vectors during training has an additional positive effect. Note that even with no enrichment, our model performs comparably to an SVM with access to all enriching features (Table 5). When enriched, our model outperforms the SVM by a margin of 2-3%. With relearning, the gaps are even larger. Syntactic word vectors While most of the work in parsing relies on linear word vectors (Socher et al., 2013; Lei et al., 2014), we consider an alternative vector representation that captures syntactic Representation Arabic English w/o enriching 77.1 85.4 w/ enriching +POS 78.5 86.4 +NextPOS 79.7 87.5 +WordNet+VerbNet 80.4 87.7 w/ enriching+relearning 81.7 88.1 w/ enriching+relearn.+syn. 82.6 88.7 Table 7: PP attachment accuracy when enriching word vectors with part-of-speech tags of the candidate head (POS) and the following word (NextPOS), and with WordNet and VerbNet features. Representation Arabic English Linear 77.1 85.4 Syntactic 79.1 87.1 Syntactic w/ relearning 80.7 87.7 Table 8: PP attachment accuracy of line</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-Rank Tensors for Scoring Dependency Structures. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>DependencyBased Word Embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="20941" citStr="Levy and Goldberg (2014)" startWordPosition="3403" endWordPosition="3406">bank (Habash and Roth, 2009) for Arabic and a conversion of the Penn treebank (PTB) to dependency format for English.6 Standard train/dev/test splits are used: sections 2-21/22/23 of the PTB for English, and the split from the SPRML shared-task for Arabic (Seddah et al., 2013). As Table 2 shows, the datasets of the two languages are fairly similar in size, except for the much larger set of prepositions in the English data. Extracting instances of PP attachments from the treebanks is done in the following way. For each 5We also experimented with another method for creating syntactic vectors by Levy and Goldberg (2014) and observed Arabic English Train Test Train Test Total 42,387 3,917 35,359 1,951 Candidates 4.5 4.3 3.7 3.6 Vocab sizes All 8,230 2,944 11,429 2,440 Heads 8,225 2,936 10,395 2,133 Preps 13 10 72 46 Children 4,222 1,424 5,504 983 Table 2: Statistics of extracted PP attachments, showing total sizes, average number of candidate heads, and vocabulary sizes. Arabic English Corpus arTenTen Wikipedia BLLIP Tokens 130M 120M 43M Types 43K 218K 317K Table 3: Statistics of Arabic and English corpora used for creating word vectors. preposition, we look for all possible candidate heads in a fixed precedi</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. DependencyBased Word Embeddings. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-Supervised Learning for Natural Language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="27525" citStr="Liang (2005)" startWordPosition="4452" endWordPosition="4453">hile this limits the contribution of the word vectors to the learned model to one dimension, attempts to use more dimensions in the SVM were unsuccessful.12 In contrast, the compositional models better capture the full dimensionality of the word vectors. A second type of features induced from raw data that we consider are Brown clusters, which were found to be useful in dependency parsing (Koo et al., 2008). Compared to distributed vectors, Brown clusters provide a more discrete representation that is easier to incorporate in the SVM. We create clusters from our unsupervised corpora using the Liang (2005) implementation of Brown’s algorithm, and add features in the spirit of (Koo et al., 2008). Specifically, we add full and prefixed bit strings for the head, preposition, and child, as well as bi-lexical versions for head-child pairs.13 Table 4 shows a summary of the SVM features. 6 Results Table 5 summarizes the results of our model and other systems. Our best results are obtained with the Head-Prep-Child-Dist (HPCD) model using syntactic vectors, enriching, and relearning. The full model outperforms both full-scale parsers and a dedicated SVM model. More advanced parsers do demonstrate higher</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-Supervised Learning for Natural Language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
<author>Pedro Aguiar</author>
<author>Mario Figueiredo</author>
</authors>
<title>Turbo Parsers: Dependency Parsing by Approximate Variational Inference.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="24184" citStr="Martins et al., 2010" startWordPosition="3905" endWordPosition="3909">, we use part-of-speech information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9We use gold POS tags in all systems and experiments. 10The only except</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo Parsers: Dependency Parsing by Approximate Variational Inference. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Miguel Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the Turbo: Fast Third-Order NonProjective Turbo Parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5276" citStr="Martins et al., 2013" startWordPosition="779" endWordPosition="782">ther discriminatively trained to maximize PP attachment accuracy. We also explore alternative representations such as dependency-based word vectors that are trained from parsed texts using the syntactic context in a dependency tree. We test our approach for PP attachment disambiguation on English and Arabic datasets, comparing it to full-scale parsers and a support vector machine (SVM) ranker. Our model outperforms all baselines, including a self-trained parser. The difference is particularly apparent on Arabic. For instance, our model achieves PP attachment accuracy of 82.6% while the Turbo (Martins et al., 2013), RBG (Lei et al., 2014), and Charniak self-trained (McClosky et al., 2006) parsers obtain 76.7%, 80.3%, and 80.8% respectively. Our results demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classifica</context>
<context position="24207" citStr="Martins et al., 2013" startWordPosition="3910" endWordPosition="3913">h information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9We use gold POS tags in all systems and experiments. 10The only exception is the RNN parser, </context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the Turbo: Fast Third-Order NonProjective Turbo Parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective Self-Training for Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="4087" citStr="McClosky et al., 2006" startWordPosition="601" endWordPosition="604"> that have restricted access to word semantics. These considerations have motivated recent explorations in using distributed word representations for syntactic parsing (Cirik and S¸ensoy, 2013; Socher et al., 2013; Lei et al., 2014). Lowdimensional word embeddings help unveil semantic similarity between words, thereby alleviating the data sparsity problem associated with PP attachment. In this context, large amounts of raw data used to construct embeddings effectively enrich limited syntactic annotations. While these approaches show initial promise, they still lag behind self-trained parsers (McClosky et al., 2006). These parsers also utilize raw data but in a different way: self-trained parsers use it to get additional (noisy) annotations, without computing new word representations. These results suggest that embedding-based representations have not yet been utilized to their full potential. We show that embedding-based representations can indeed significantly improve PP attachment accuracy. We achieve this by using such representations within a compositional neural network architecture. The representations are initially learned from an unlabeled corpus, but are then further discriminatively trained to</context>
<context position="5351" citStr="McClosky et al., 2006" startWordPosition="791" endWordPosition="794">explore alternative representations such as dependency-based word vectors that are trained from parsed texts using the syntactic context in a dependency tree. We test our approach for PP attachment disambiguation on English and Arabic datasets, comparing it to full-scale parsers and a support vector machine (SVM) ranker. Our model outperforms all baselines, including a self-trained parser. The difference is particularly apparent on Arabic. For instance, our model achieves PP attachment accuracy of 82.6% while the Turbo (Martins et al., 2013), RBG (Lei et al., 2014), and Charniak self-trained (McClosky et al., 2006) parsers obtain 76.7%, 80.3%, and 80.8% respectively. Our results demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Rat</context>
<context position="24564" citStr="McClosky et al., 2006" startWordPosition="3971" endWordPosition="3974">e compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9We use gold POS tags in all systems and experiments. 10The only exception is the RNN parser, for which we use the built-in English model in Stanford parser’s (version 3.4); Source Feature Template Treebank hw, pw, cw, hw-cw, ht, nt, hpd WordNet hh, ch VerbNet hpvf Word Vectors sim(hv,cv) Brown Clusters hc*, pc*, cc*, hc4, pc4, cc4, hc*-cc*, hc4-cc4 Table 4: Feature templates for the SVM baseline. Bi-lexical templates appear with a “-”. Abbreviati</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective Self-Training for Parsing. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Large-Margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="24307" citStr="McDonald et al., 2005" startWordPosition="3929" endWordPosition="3932">Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9We use gold POS tags in all systems and experiments. 10The only exception is the RNN parser, for which we use the built-in English model in Stanford parser’s (version 3.4); Source Feature Templ</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online Large-Margin Training of Dependency Parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Medimi</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>A Flexible Unsupervised PP-attachment Method Using Semantic Information.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="6954" citStr="Medimi and Bhattacharyya, 2007" startWordPosition="1033" endWordPosition="1036"> do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored using manually crafted resources (Stetina and Nagao, 1997; Gamallo et al., 2003; Olteanu and Moldovan, 2005; Medimi and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menche</context>
</contexts>
<marker>Medimi, Bhattacharyya, 2007</marker>
<rawString>Srinivas Medimi and Pushpak Bhattacharyya. 2007. A Flexible Unsupervised PP-attachment Method Using Semantic Information. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sauro Menchetti</author>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Wide coverage natural language processing using kernel methods and neural networks for structured data.</title>
<date>2005</date>
<journal>Pattern Recognition Letters,</journal>
<volume>26</volume>
<issue>12</issue>
<contexts>
<context position="7570" citStr="Menchetti et al., 2005" startWordPosition="1128" endWordPosition="1131"> 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In 562 particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of the full parsing scenario, they rely on a probabilistic context-free grammar to prune search space. In contrast, focusing on PP attachment allows us to consider various neural network architectures that are more appropriate for this task, including ternary, binary, and distancedependent compositions. Furthermore, we investigate modifications to the original word vectors in several importan</context>
</contexts>
<marker>Menchetti, Costa, Frasconi, Pontil, 2005</marker>
<rawString>Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and Massimiliano Pontil. 2005. Wide coverage natural language processing using kernel methods and neural networks for structured data. Pattern Recognition Letters, 26(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop atICLR.</booktitle>
<contexts>
<context position="16112" citStr="Mikolov et al. (2013" startWordPosition="2620" endWordPosition="2623">are set to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias terms b are set to zero; and the weight vector is set to w ∼ U(−1√n, 1√n). 4 Word vector representations Our approach assumes a vector representation for each word. Such representations have gained popularity in recent years, due to the ability to train them from large unlabeled datasets, and their ease of use in a wide variety of tasks (Turian et al., 2010). There are various approaches to training vector representations (Collobert and Weston, 2008; Bengio et al., 2009). Here we chose to focus on the Skip-gram method recently proposed by Mikolov et al. (2013a). The Skip-gram model maximizes the average log-probability of every word generating its context, which is modeled via a neural net architecture, but without the non-linearity. To improve efficiency, this probability is approximated by a hierarchical softmax (Mikolov et al., 2013b) with vocabulary words represented in a binary Huffman tree.3 In the simplest variant of our method, we train the Skip-gram representation on unlabeled text, and use it as a fixed representation when training the PP attachment model (see Section 3.3). Below we consider several variations on this approach. 3Prelimin</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop atICLR.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<booktitle>2013b. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS.</booktitle>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaouad Mousser</author>
</authors>
<title>A Large Coverage Verb Taxonomy for Arabic.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="23699" citStr="Mousser, 2010" startWordPosition="3837" endWordPosition="3838">sh vectors in (Bansal et al., 2014), which were trained from a parsed BLLIP corpus (minus PTB). For Arabic, we first convert the morphologically-processed arTenTen corpus to CoNLL format with the SPMRL shared-task scripts (Seddah et al., 2013). Then we parse the corpus with a baseline MST parser (Section 5.3) and create syntactic word vectors as described in Section 4.3. The Arabic syntactic vectors will be made available to the research community. For enriching word vectors, we use part-of-speech information9 from the treebanks as well as the Arabic and English VerbNets (Kipper et al., 2008; Mousser, 2010) and WordNets (Rodr´ıquez et al., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al</context>
</contexts>
<marker>Mousser, 2010</marker>
<rawString>Jaouad Mousser. 2010. A Large Coverage Verb Taxonomy for Arabic. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>MaltParser: A Data-Driven Parser-Generator for Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="24348" citStr="Nivre et al., 2006" startWordPosition="3937" endWordPosition="3940">l., 2008; Princeton University, 2010). In total, these resources add to each word vector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9We use gold POS tags in all systems and experiments. 10The only exception is the RNN parser, for which we use the built-in English model in Stanford parser’s (version 3.4); Source Feature Template Treebank hw, pw, cw, hw-cw, ht, nt, h</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A Data-Driven Parser-Generator for Dependency Parsing. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marian Olteanu</author>
<author>Dan Moldovan</author>
</authors>
<title>PPattachment Disambiguation using Large Context.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="6026" citStr="Olteanu and Moldovan, 2005" startWordPosition="889" endWordPosition="892">ely. Our results demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇSuster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources L</context>
<context position="31905" citStr="Olteanu and Moldovan, 2005" startWordPosition="5190" endWordPosition="5193">sambiguation for predicting other attachments in the sentence. RRR dataset Much of the previous work on PP attachment focused on a binary classification scenario (Section 2) and has been evaluated on the RRR dataset (Ratnaparkhi et al., 1994). Such systems cannot be easily evaluated in our setting which allows multiple candidate heads. On the other hand, our full model exploits contextual information that is not available in the RRR dataset. Nevertheless, using a simpler version of our model we obtain an accuracy of 85.6% on the RRR test set.14 This is comparable to much of the previous work (Olteanu and Moldovan, 2005), but still lags behind the 88.1% of Stetina and Nagao (1997), who also used WordNet information. However, our use of WordNet is rather limited compared to theirs, indicating that our enriching method can be improved with other types of information. 6.1 Alternative composition architectures In this section we analyze how different composition architectures (Section 3.2) contribute to the overall performance. To isolate the contribution of the architecture, we focus on standard (linear) word vectors, with no relearning or enriching. As Figure 3 shows, simpler models tend to perform worse than m</context>
</contexts>
<marker>Olteanu, Moldovan, 2005</marker>
<rawString>Marian Olteanu and Dan Moldovan. 2005. PPattachment Disambiguation using Large Context. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="false">
<date>2010</date>
<institution>Princeton University.</institution>
<note>WordNet. http:// wordnet.princeton.edu.</note>
<marker>2010</marker>
<rawString>Princeton University. 2010. WordNet. http:// wordnet.princeton.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
<author>Jeff Reynar</author>
<author>Salim Roukos</author>
</authors>
<title>A Maximum Entropy Model for Prepositional Phrase Attachment.</title>
<date>1994</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="2132" citStr="Ratnaparkhi et al., 1994" startWordPosition="304" endWordPosition="308">Introduction The problem of prepositional phrase (PP) attachment disambiguation has been under investigation 1The code and data for this work are available at http: //groups.csail.mit.edu/rbg/code/pp. She ate spaghetti with butter She ate spaghetti with chopsticks Figure 1: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP with butter attaches to the noun spaghetti. In the bottom sentence, the PP with chopsticks attaches to the verb ate. for a long time. However, despite at least two decades of research (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995), it remains a major source of errors for state-of-the-art parsers. For instance, in a comparative evaluation of parser performance on the Wall Street Journal corpus, Kummerfeld et al. (2012) report that PP attachment is the largest source of errors across all parsers. Moreover, the extent of improvement over time has been rather limited, amounting to about 32% error reduction since the work of (Collins, 1997). PP attachments are inherently lexicalized and part-of-speech (POS) tags are not sufficient for their correct disambiguation. For example, the two sentences in</context>
<context position="5972" citStr="Ratnaparkhi et al., 1994" startWordPosition="881" endWordPosition="884">06) parsers obtain 76.7%, 80.3%, and 80.8% respectively. Our results demonstrate that relearning the embeddings contributes to the model performance, across a range of configurations. We also notice that representations based on syntactic context are more powerful than those based on linear context. This may explain the improved performance of self-trained parsers over parsers that rely on linear context embeddings. 2 Related Work Problem formulation Typically, PP attachment disambiguation is modeled as a binary classification decision between a preceding noun or verb (Brill and Resnik, 1994; Ratnaparkhi et al., 1994; Collins and Brooks, 1995; Olteanu and Moldovan, 2005; ˇSuster, 2012). In addition, the problem of PP attachment has also been addressed in the context of full parsing (Atterer and Sch¨utze, 2007; Agirre et al., 2008). For instance, Green (2009) engineered statesplit features for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a sta</context>
<context position="31520" citStr="Ratnaparkhi et al., 1994" startWordPosition="5125" endWordPosition="5128"> Table 6: Parsing performance (UAS) of the RBG parser, with predicted and oracle PPs. is relatively larger than the gain in PP accuracy. For example, relative to an oracle upper bound of forcing gold PP arcs in the parser output (Table 6), the reduction in English parsing errors is 20%, whereas the reduction in PP errors is only 15%. This affirms the importance of PP attachment disambiguation for predicting other attachments in the sentence. RRR dataset Much of the previous work on PP attachment focused on a binary classification scenario (Section 2) and has been evaluated on the RRR dataset (Ratnaparkhi et al., 1994). Such systems cannot be easily evaluated in our setting which allows multiple candidate heads. On the other hand, our full model exploits contextual information that is not available in the RRR dataset. Nevertheless, using a simpler version of our model we obtain an accuracy of 85.6% on the RRR test set.14 This is comparable to much of the previous work (Olteanu and Moldovan, 2005), but still lags behind the 88.1% of Stetina and Nagao (1997), who also used WordNet information. However, our use of WordNet is rather limited compared to theirs, indicating that our enriching method can be improve</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A Maximum Entropy Model for Prepositional Phrase Attachment. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Rodr´ıquez</author>
<author>David Farwell</author>
<author>Javi Ferreres</author>
<author>Manuel Bertran</author>
<author>Musa Alkhalifa</author>
<author>M Antonia Mart´ı</author>
</authors>
<title>Arabic WordNet: Semi-automatic Extensions using Bayesian Inference.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Rodr´ıquez, Farwell, Ferreres, Bertran, Alkhalifa, Mart´ı, 2008</marker>
<rawString>Horacio Rodr´ıquez, David Farwell, Javi Ferreres, Manuel Bertran, Musa Alkhalifa, and M. Antonia Mart´ı. 2008. Arabic WordNet: Semi-automatic Extensions using Bayesian Inference. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
<author>Sandra K¨ubler</author>
<author>Marie Candito</author>
<author>Jinho D Choi</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
</authors>
<title>Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages.</title>
<date>2013</date>
<journal>Overview of the SPMRL</journal>
<booktitle>In Proceedings of SPMRL.</booktitle>
<marker>Seddah, Tsarfaty, K¨ubler, Candito, Choi, Farkas, Foster, 2013</marker>
<rawString>Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer Foster, et al. 2013. Overview of the SPMRL 2013 Shared Task: A Cross-Framework Evaluation of Parsing Morphologically Rich Languages. In Proceedings of SPMRL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks.</title>
<date>2010</date>
<booktitle>In Proceedings of NIPS Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="7592" citStr="Socher et al., 2010" startWordPosition="1132" endWordPosition="1135">irre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In 562 particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of the full parsing scenario, they rely on a probabilistic context-free grammar to prune search space. In contrast, focusing on PP attachment allows us to consider various neural network architectures that are more appropriate for this task, including ternary, binary, and distancedependent compositions. Furthermore, we investigate modifications to the original word vectors in several important directions: enrichin</context>
<context position="9723" citStr="Socher et al., 2010" startWordPosition="1492" endWordPosition="1495">ize, thus departing from the binary classification scenario considered in much of the previous work (Section 2). The set of parameters is 0. 3.1 Compositional framework Our approach to constructing the score function is as follows. First, we assume that all words in the sentence are represented as vectors in Rn. Next, we compose vectors corresponding to the relevant preposition, its candidate head, and other words in the sentence to obtain a new vector p E Rn. The final score is a linear function of this vector. The basic composition operation is defined as a single layer in a neural network (Socher et al., 2010). Given vectors u, v E Rn, representing two words, we form a new vector via a function: g(W[u; v] + b) E Rn (1) where b E Rn is a vector of bias terms, [u; v] E R2n is a concatenation of u and v into a column vector, W E Rn×2n is a composition matrix, and g is a non-linear activation function.2 Given a candidate head h for preposition z, we apply such compositions to a set of words, resulting in a vector p. The final score s(x, z, h; 0) is given by w · p, where w E Rn is a weight vector. The parameters to be learned are 0 = (W, b, w). 3.2 Composition architectures There are various possible wa</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks. In Proceedings of NIPS Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3678" citStr="Socher et al., 2013" startWordPosition="540" endWordPosition="543">/2014. c�2014 Association for Computational Linguistics. sponding word is not observed in the training data, a standard lexicalized parser does not have sufficient information to distinguish between these two cases. In fact, 72% of head-child pairs (e.g. spaghettibutter) from the Wall Street Journal test set are unseen in training. Not surprisingly, resolving these ambiguities is challenging for parsers that have restricted access to word semantics. These considerations have motivated recent explorations in using distributed word representations for syntactic parsing (Cirik and S¸ensoy, 2013; Socher et al., 2013; Lei et al., 2014). Lowdimensional word embeddings help unveil semantic similarity between words, thereby alleviating the data sparsity problem associated with PP attachment. In this context, large amounts of raw data used to construct embeddings effectively enrich limited syntactic annotations. While these approaches show initial promise, they still lag behind self-trained parsers (McClosky et al., 2006). These parsers also utilize raw data but in a different way: self-trained parsers use it to get additional (noisy) annotations, without computing new word representations. These results sugg</context>
<context position="7633" citStr="Socher et al. (2013)" startWordPosition="1139" endWordPosition="1142"> WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In 562 particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of the full parsing scenario, they rely on a probabilistic context-free grammar to prune search space. In contrast, focusing on PP attachment allows us to consider various neural network architectures that are more appropriate for this task, including ternary, binary, and distancedependent compositions. Furthermore, we investigate modifications to the original word vectors in several important directions: enriching word vectors with semantic and syntacti</context>
<context position="15469" citStr="Socher et al., 2013" startWordPosition="2506" endWordPosition="2509">mentation we dropout input units before each non-linear layer, including the initial word vectors. We do not dropout units after the final non-linear layer. Note that Dropout is known to be especially useful when combined with AdaGrad (Wager et al., 2013). Hyperparameters and initialization We use the following default hyperparameters without further tuning unless noted otherwise: Dropout parameter p = 0.5 (Hinton et al., 2012), AdaGrad initial learning rate q = 1.0 (Dyer, n.d.), and minibatch size of 500. Learned parameters are initialized similarly to previous work (Bengio and Glorot, 2010; Socher et al., 2013): composition matrices are set to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias terms b are set to zero; and the weight vector is set to w ∼ U(−1√n, 1√n). 4 Word vector representations Our approach assumes a vector representation for each word. Such representations have gained popularity in recent years, due to the ability to train them from large unlabeled datasets, and their ease of use in a wide variety of tasks (Turian et al., 2010). There are various approaches to training vector representations (Collobert and Weston, 2008; Bengio et al., 2009). Here we chose to focus on the Skip-gram metho</context>
<context position="24432" citStr="Socher et al., 2013" startWordPosition="3951" endWordPosition="3954">ector 46/67 extended dimensions in Arabic/English, representing syntactic and semantic information about the word. 5.3 Baselines We compare against full-scale parsers, an SVM ranker, and a simple but strong baseline of always choosing the closest candidate head. Parsers We mostly compare with dependency parsers, including the state-of-the-art Turbo (Martins et al., 2010; Martins et al., 2013) and RBG parsers (Lei et al., 2014), in addition to a secondorder MST parser (McDonald et al., 2005) and the Malt parser (Nivre et al., 2006). We also compare with two constituency parsers: an RNN parser (Socher et al., 2013), which also uses word vectors and a neural network approach, and the Charniak self-trained reranking parser (McClosky et al., 2006). We train all parsers on the train/dev sets and report their PP attachment accuracy on the test sets.10 For the self-trained parser we followed the 9We use gold POS tags in all systems and experiments. 10The only exception is the RNN parser, for which we use the built-in English model in Stanford parser’s (version 3.4); Source Feature Template Treebank hw, pw, cw, hw-cw, ht, nt, hpd WordNet hh, ch VerbNet hpvf Word Vectors sim(hv,cv) Brown Clusters hc*, pc*, cc*,</context>
<context position="28415" citStr="Socher et al., 2013" startWordPosition="4591" endWordPosition="4594"> features. 6 Results Table 5 summarizes the results of our model and other systems. Our best results are obtained with the Head-Prep-Child-Dist (HPCD) model using syntactic vectors, enriching, and relearning. The full model outperforms both full-scale parsers and a dedicated SVM model. More advanced parsers do demonstrate higher accuracy on the PP attachment task, but our method outperforms them as well. Note that the self-trained reranking parser (Charniak-RS) performs especially well and quite better than the RNN parser. This trend is consistent with the results in (Kummerfeld et al., 2012; Socher et al., 2013). Our compositional architecture is effective in exploiting raw data: using only standard word vectors with no enriching, our HPCD (basic) model performs comparably to an SVM with access to all enriching features. Once we improve the representation, we outperform both the SVM and full parsers. In comparison, the contribution of raw data to the SVM, as either word vectors or Brown clusters, is rather limited. 12For example, we tried adding all word vector dimensions as features, as well as element-wise products of the vectors representing the head and the child. 13As in (Koo et al., 2008), we l</context>
<context position="35809" citStr="Socher et al., 2013" startWordPosition="5797" endWordPosition="5800">esources (Section 4.2). As Table 7 shows, this enrichment yields sizable performance gains. Most of the gain comes from partof-speech information, while WordNet and VerbNet have a smaller contribution. Updating the word vectors during training has an additional positive effect. Note that even with no enrichment, our model performs comparably to an SVM with access to all enriching features (Table 5). When enriched, our model outperforms the SVM by a margin of 2-3%. With relearning, the gaps are even larger. Syntactic word vectors While most of the work in parsing relies on linear word vectors (Socher et al., 2013; Lei et al., 2014), we consider an alternative vector representation that captures syntactic Representation Arabic English w/o enriching 77.1 85.4 w/ enriching +POS 78.5 86.4 +NextPOS 79.7 87.5 +WordNet+VerbNet 80.4 87.7 w/ enriching+relearning 81.7 88.1 w/ enriching+relearn.+syn. 82.6 88.7 Table 7: PP attachment accuracy when enriching word vectors with part-of-speech tags of the candidate head (POS) and the following word (NextPOS), and with WordNet and VerbNet features. Representation Arabic English Linear 77.1 85.4 Syntactic 79.1 87.1 Syntactic w/ relearning 80.7 87.7 Table 8: PP attachme</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with Compositional Vector Grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiri Stetina</author>
<author>Makoto Nagao</author>
</authors>
<title>Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary. In</title>
<date>1997</date>
<booktitle>Fifth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="6871" citStr="Stetina and Nagao, 1997" startWordPosition="1021" endWordPosition="1024">s for the Stanford parser to improve Arabic PP attachment. In this work, we do isolate PP attachments from other parsing decisions. At the same time, we consider a more realistic scenario where multiple candidate heads are allowed. We also compare against full-scale parsers and show that our model predictions improve a state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored using manually crafted resources (Stetina and Nagao, 1997; Gamallo et al., 2003; Olteanu and Moldovan, 2005; Medimi and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our w</context>
<context position="31966" citStr="Stetina and Nagao (1997)" startWordPosition="5201" endWordPosition="5204">RR dataset Much of the previous work on PP attachment focused on a binary classification scenario (Section 2) and has been evaluated on the RRR dataset (Ratnaparkhi et al., 1994). Such systems cannot be easily evaluated in our setting which allows multiple candidate heads. On the other hand, our full model exploits contextual information that is not available in the RRR dataset. Nevertheless, using a simpler version of our model we obtain an accuracy of 85.6% on the RRR test set.14 This is comparable to much of the previous work (Olteanu and Moldovan, 2005), but still lags behind the 88.1% of Stetina and Nagao (1997), who also used WordNet information. However, our use of WordNet is rather limited compared to theirs, indicating that our enriching method can be improved with other types of information. 6.1 Alternative composition architectures In this section we analyze how different composition architectures (Section 3.2) contribute to the overall performance. To isolate the contribution of the architecture, we focus on standard (linear) word vectors, with no relearning or enriching. As Figure 3 shows, simpler models tend to perform worse than more complex ones. The best variants use different composition</context>
</contexts>
<marker>Stetina, Nagao, 1997</marker>
<rawString>Jiri Stetina and Makoto Nagao. 1997. Corpus Based PP Attachment Ambiguity Resolution with a Semantic Dictionary. In Fifth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon ˇSuster</author>
</authors>
<title>Resolving PP-attachment ambiguity in French with distributional methods.</title>
<date>2012</date>
<tech>Master’s thesis,</tech>
<institution>Universit´e de Lorraine &amp; Rijksuniversiteit Groningen.</institution>
<marker>ˇSuster, 2012</marker>
<rawString>Simon ˇSuster. 2012. Resolving PP-attachment ambiguity in French with distributional methods. Master’s thesis, Universit´e de Lorraine &amp; Rijksuniversiteit Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word Representations: A Simple and General Method for Semi-Supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15907" citStr="Turian et al., 2010" startWordPosition="2587" endWordPosition="2590">ad initial learning rate q = 1.0 (Dyer, n.d.), and minibatch size of 500. Learned parameters are initialized similarly to previous work (Bengio and Glorot, 2010; Socher et al., 2013): composition matrices are set to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias terms b are set to zero; and the weight vector is set to w ∼ U(−1√n, 1√n). 4 Word vector representations Our approach assumes a vector representation for each word. Such representations have gained popularity in recent years, due to the ability to train them from large unlabeled datasets, and their ease of use in a wide variety of tasks (Turian et al., 2010). There are various approaches to training vector representations (Collobert and Weston, 2008; Bengio et al., 2009). Here we chose to focus on the Skip-gram method recently proposed by Mikolov et al. (2013a). The Skip-gram model maximizes the average log-probability of every word generating its context, which is modeled via a neural net architecture, but without the non-linearity. To improve efficiency, this probability is approximated by a hierarchical softmax (Mikolov et al., 2013b) with vocabulary words represented in a binary Huffman tree.3 In the simplest variant of our method, we train t</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word Representations: A Simple and General Method for Semi-Supervised Learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>Combining Unsupervised and Supervised Methods for PP Attachment Disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="7179" citStr="Volk, 2002" startWordPosition="1069" endWordPosition="1070">state-of-the-art dependency parser. Information sources Lexical sparsity associated with disambiguating PP attachments (Figure 1) has spurred researchers to exploit a wide range of information sources. On the one hand, researchers have explored using manually crafted resources (Stetina and Nagao, 1997; Gamallo et al., 2003; Olteanu and Moldovan, 2005; Medimi and Bhattacharyya, 2007). For instance, Agirre et al. (2008) demonstrate that using WordNet semantic classes benefits PP attachment performance. On the other hand, researchers have looked into using co-occurrence statistics from raw text (Volk, 2002; Olteanu and Moldovan, 2005; Gala and Lafourcade, 2007). Such statistics can be translated into word vectors from which a cosine similarity score is calculated (ˇSuster, 2012). We also rely on word vectors, but our model captures more complex relations among them. Algorithmic approach Our work is most similar to recursive neural network parsers (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2010). In 562 particular, Socher et al. (2013) obtain good parsing performance by building compositional representations from word vectors. However, to combat the computational complexity of t</context>
</contexts>
<marker>Volk, 2002</marker>
<rawString>Martin Volk. 2002. Combining Unsupervised and Supervised Methods for PP Attachment Disambiguation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Wager</author>
<author>Sida Wang</author>
<author>Percy Liang</author>
</authors>
<title>Dropout Training as Adaptive Regularization.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="15104" citStr="Wager et al., 2013" startWordPosition="2450" endWordPosition="2453">pagation. For regularization we use Dropout (Hinton et al., 2012), a recent method for preventing co-adaptation of features, where input units to the neural network 564 are randomly dropped. Random dropping occurs independently for each training example and has the effect of creating multiple thinned networks that are trained with shared parameters. In our implementation we dropout input units before each non-linear layer, including the initial word vectors. We do not dropout units after the final non-linear layer. Note that Dropout is known to be especially useful when combined with AdaGrad (Wager et al., 2013). Hyperparameters and initialization We use the following default hyperparameters without further tuning unless noted otherwise: Dropout parameter p = 0.5 (Hinton et al., 2012), AdaGrad initial learning rate q = 1.0 (Dyer, n.d.), and minibatch size of 500. Learned parameters are initialized similarly to previous work (Bengio and Glorot, 2010; Socher et al., 2013): composition matrices are set to W = 0.5[II] + c, where c ∼ U(−1n, 1n); bias terms b are set to zero; and the weight vector is set to w ∼ U(−1√n, 1√n). 4 Word vector representations Our approach assumes a vector representation for eac</context>
</contexts>
<marker>Wager, Wang, Liang, 2013</marker>
<rawString>Stefan Wager, Sida Wang, and Percy Liang. 2013. Dropout Training as Adaptive Regularization. In Proceedings of NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
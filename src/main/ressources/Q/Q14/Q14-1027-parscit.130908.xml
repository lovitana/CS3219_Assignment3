<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.929926">
2-Slave Dual Decomposition for Generalized Higher Order CRFs
</title>
<author confidence="0.995018">
Xian Qian and Yang Liu
</author>
<affiliation confidence="0.9987285">
Computer Science Department
The University of Texas at Dallas
</affiliation>
<email confidence="0.997627">
{qx,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.996638" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9986892">
We show that the decoding problem in
generalized Higher Order Conditional
Random Fields (CRFs) can be decomposed
into two parts: one is a tree labeling
problem that can be solved in linear time
using dynamic programming; the other is
a supermodular quadratic pseudo-Boolean
maximization problem, which can be solved
in cubic time using a minimum cut algorithm.
We use dual decomposition to force their
agreement. Experimental results on Twitter
named entity recognition and sentence
dependency tagging tasks show that our
method outperforms spanning tree based dual
decomposition.
</bodyText>
<sectionHeader confidence="0.998595" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9990126">
Conditional Random Fields (Lafferty et al., 2001)
(CRFs) are popular models for many NLP tasks.
In particular, the linear chain CRFs explore local
structure information for sequence labeling tasks,
such as part-of-speech (POS) tagging, named entity
recognition (NER), and shallow parsing. Recent
studies have shown that the predictive power
of CRFs can be strengthened by breaking the
locality assumption. They either add long distance
dependencies and patterns to linear chains for
improved sequence labeling (Galley, 2006; Finkel
et al., 2005; Kazama and Torisawa, 2007), or
directly use the 4-connected neighborhood lattice
(Ding et al., 2008). The resulting non-local models
generally suffer from exponential time complexity
of inference except some special cases (Sarawagi
and Cohen, 2004; Takhanov and Kolmogorov, 2013;
Kolmogorov and Zabih, 2004).
Approximate decoding algorithms have been
proposed in the past decade, such as reranking
(Collins, 2002b), loopy belief propagation (Sutton
and Mccallum, 2006), tree reweighted belief
propagation (Kolmogorov, 2006). In this paper,
we focus on dual decomposition (DD), which
has attracted much attention recently due to its
simplicity and effectiveness (Rush and Collins,
2012). In short, it decomposes the decoding problem
into several sub-problems. For each sub-problem, an
efficient decoding algorithm is deployed as a slave
solver. Finally a simple method forces agreement
among different slaves. A popular choice is the sub-
gradient algorithm. Martins et al. (2011b) showed
that the success of the sub-gradient algorithm is
strongly tied to the ability of finding a good
decomposition, i.e., one involving few overlapping
slaves. However, for generalized higher order
graphical models, a lightweight decomposition is
not at hand and many overlapping slaves may
be involved. Martins et al. (2011b) showed that
the sub-gradient algorithm exhibits extremely slow
convergence in such cases, and they proposed
the alternating directions method (DD-ADMM) to
tackle these.
In this paper, we propose a 2-slave dual
decomposition approach for efficient decoding in
higher order CRFs. One slave is a tree labeling
model that can be solved in linear time using
dynamic programming. The other is a supermodular
quadratic pseudo-Boolean maximization problem,
which can be solved in cubic time via minimum
</bodyText>
<page confidence="0.993908">
339
</page>
<bodyText confidence="0.7384614">
Transactions of the Association for Computational Linguistics, 2 (2014) 339–350. Action Editor: Kristina Toutanova.
Submitted 11/2013; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
cut. Experimental results on Twitter NER and
sentence dependency tagging tasks demonstrate the
effectiveness of our technique.
</bodyText>
<sectionHeader confidence="0.991914" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.990704">
2.1 Generalized Higher Order CRFs
</subsectionHeader>
<bodyText confidence="0.9995285">
Given an undirected graph G = (V, E) with
N vertices, let x = x1, x2, . . . , xN denote the
observations of the vertices, and each observation
xv is asked to assign one state (or label) in the state
set s E S. The assignment of the graph can be
represented by a binary matrix YN×|S|, where |S |is
the cardinality of S, and the element Yv,s indicates
if xv is assigned state s. In the rest of the paper, we
use Yv[s] instead, and v[s] to denote the vertex v with
state s. The constraint
</bodyText>
<equation confidence="0.9823805">
∑ Yv[s] = 1 (1)
s
</equation>
<bodyText confidence="0.9969422">
is required so that each vertex has exactly one state.
In this paper, we use Y to denote the space of state
assignments.
The decoding problem is to search the optimal
assignment that maximizes the scoring function
</bodyText>
<equation confidence="0.897177">
Y ∗ = arg maxY ∈Y(x)ϕ(x, Y )
</equation>
<bodyText confidence="0.9998965">
where ϕ(x, Y ) is a given scoring function. As x is
constant in this maximization problem, we omit x
for simplicity in the remainder of the paper. The
decoding problem becomes
</bodyText>
<equation confidence="0.937819">
max ϕ(Y ). (2)
Y ∈Y
</equation>
<figureCaption confidence="0.991809">
Figure 1: Pattern c[s] = uv[st] is shown in bold.
</figureCaption>
<bodyText confidence="0.995998">
its members v[s] are selected. For simplicity, in this
paper we use
</bodyText>
<equation confidence="0.956097">
∏Yc[s] = Yv[s]
v[s]∈c[s]
</equation>
<bodyText confidence="0.9797275">
to denote whether pattern c[s] appears in the
assignment. Then the scoring function becomes
</bodyText>
<equation confidence="0.961863">
ϕ(Y ) = ∑ ∑ ϕc[s]Yc[s]. (3)
c s∈c[·]
</equation>
<bodyText confidence="0.9998684">
Many existing CRFs can be represented using
Eq (3). For example, the popular linear chain
CRFs consider two types of patterns: vertices and
edges connecting adjacent vertices, resulting in the
following scoring function
</bodyText>
<equation confidence="0.6156155">
ϕ(Y ) = ∑ ∑ ϕv[s]Yv[s] + ∑ ∑ ϕv(v+1)[st]Yv(v+1)[st]
v s v st∈S2
</equation>
<bodyText confidence="0.9819865">
The optimal Y can be found in linear time using the
Viterbi algorithm.
Another example is the skip-chain CRFs, which
consider the interactions between similar vertices
</bodyText>
<figure confidence="0.995325090909091">
vertex
s
state
t
u v
The scoring function ϕ(Y ) is usually decomposed ∑ ∑ ϕv[s]Yv[s]
into small parts ϕ(Y ) = s
v
∑ ∑ ∏ Yv[s] ∑ ∑ ϕv(v+1)[st]Yv(v+1)[st]
ϕ(Y ) = s∈c[·] ϕc[s] + st∈S2
c v[s]∈c[s] v
</figure>
<bodyText confidence="0.974118384615385">
where c is a subset of vertices, called a factor. c[·]
is the set of all possible assignments of c. For
example, factor c = {u, v} denotes the edge (u, v)
in the graph, and c[·] = S2 is the set of the
|S|2 transitions. A factor c with a specific state
assignment s is called a pattern, denoted as c[s].
For example, v[s] is a pattern of vertex v, and uv[st]
is a pattern of edge (u, v) as shown in Figure 1. Note
that our definition extends of the work of Takhanov
and Kolmogorov where patterns are restricted to the
state sequences of consecutive vertices (Takhanov
and Kolmogorov, 2013). HV[s]EC[1] Yv[s] means a
pattern c[s] is selected in the assignment only if all
</bodyText>
<equation confidence="0.944565">
+ ∑ ∑ ϕuv[ss]Yuv[ss].
u,v are similar s∈S
</equation>
<bodyText confidence="0.999625555555556">
With positive ϕuv[ss], the model encourages similar
vertices u and v to have identical state s, and thus
it yields a more consistent labeling result compared
with linear chain CRFs. Empirically, the use
of complex patterns achieves better performance
but suffers from high computational complexity
of inference, which is generally NP-hard. Hence
an efficient approximate inference algorithm is
required to balance the trade-off.
</bodyText>
<page confidence="0.994659">
340
</page>
<subsectionHeader confidence="0.990682">
2.2 Dual Decomposition
</subsectionHeader>
<bodyText confidence="0.991001692307692">
Dual decomposition is a popular approach due
to its simplicity and effectiveness, and has been
successfully applied to many tasks such as machine
translation, cross sentential POS tagging, joint POS
tagging and parsing.
Briefly, dual decomposition attempts to solve
problems of the following form
The objective function is the sum of several small
components that are tractable in isolation but
whose combination is not. These components
are called slaves. Rather than solving the
problem directly, dual decomposition considers the
equivalent problem
</bodyText>
<equation confidence="0.97915">
max
Y,Z1...ZM
s.t. Zi = Y di
</equation>
<bodyText confidence="0.877067">
Using Lagrangian relaxation to eliminate the
constraint, we get
</bodyText>
<equation confidence="0.996442">
λT (Y − Zi) (4)
</equation>
<bodyText confidence="0.999942186046512">
which provides the upper bound of the original
problem. A is the Lagrange multiplier, which is
typically optimized via sub-gradient algorithms.
Martins et al. (2011b) showed that the success of
sub-gradient algorithms is strongly tied to the ability
of finding a good decomposition, i.e., one involving
few slaves. Finding a concise decomposition is
usually task dependent. For example, Koo et al.
(2010) introduced dual decomposition for parsing
with non-projective head automata. They used
only two slaves: one is the arc-factored model,
and the other is head automata which involves
adjacent siblings and can be solved using dynamic
programming in linear time.
Dual decomposition is especially efficient for
joint learning tasks because a concise decomposition
can be derived naturally where each slave solves one
subtask. For example, Rush et al. (2010) used two
slaves for integrated phrase-structure parsing and
trigram POS tagging task.
However, for generalized higher order CRFs,
a lightweight decomposition may be not at
hand. Martins et al. (2011a) showed that the
sub-gradient algorithms exhibited extremely slow
convergence when handling many slaves. For fast
convergence, they employed alternating directions
dual decomposition (AD3), which relaxes the
agreement constraint via augmented Lagrangian
Relaxation, where an additional quadratic penalty
term was added into the Lagrangian (Eq (4)).
Similarly, Jojic et al. (2010) added a strongly
concave term to the Lagrangian to make it
differentiable, resulting in fast convergence.
The work most closely related to ours is the work
by Komodakis (2011), where dual decomposition
was used for decoding general higher order CRFs.
Komodakis achieved great empirical success even
with the naive decomposition where each slave
processes a single higher order factor. His
result demonstrates the effectiveness of the dual
decomposition framework. Our work improves
Komodakis’ by using a concise decomposition with
only two slaves.
</bodyText>
<subsectionHeader confidence="0.9931405">
2.3 Graph Representable Pseudo-Boolean
Optimization
</subsectionHeader>
<bodyText confidence="0.998601142857143">
One slave in our approach is a graph representable
pseudo-Boolean maximization problem, which
can be reduced to a supermodular quadratic
pseudo-Boolean maximization problem and solved
efficiently using an algorithm for finding a minimal
cut.
A pseudo-Boolean function (PBF) (Boros and
Hammer, 2002) is a multilinear function of binary
variables, that is
where xi E {0, 1}. Maximizing a PBF is usually
NP-hard, such as the maximum cut problem (Boros
and Hammer, 1991).
A pseudo-Boolean function is said to be
supermodular iff
</bodyText>
<equation confidence="0.954239">
f(x) + f(y) ≤ f(x ∧ y) + f(x V y)
</equation>
<bodyText confidence="0.999880666666667">
where x n y, x V y are the element-wise AND and
OR operator of the two vectors respectively. This
is an important concept, because a supermodular
pseudo-Boolean function (SPBF) can be maximized
in O(n6) running time (Orlin, 2009). A necessary
and sufficient condition for identifying a SPBF is
</bodyText>
<equation confidence="0.998751230769231">
∑M
i=1
Oi(Y )
max
Y
∑M
i=1
Oi(Zi)
min max ∑M ∑
A Y,Z1...ZM i=1 Oi(Zi) +
i
f(x) = ∑ ∑aixi + ∑aijxixj + aijkxixjxk + .. .
i i&lt;j i&lt;j&lt;k
</equation>
<page confidence="0.986883">
341
</page>
<bodyText confidence="0.89916325">
that all of its second order derivatives are non-
negative (Nemhauser et al., 1978), i.e., for all i &lt; j,
and Jeavons (2010), and finally solve the slave
problem via graph cuts.
</bodyText>
<equation confidence="0.971806">
∂f &gt; 0
∂xi∂xj
</equation>
<bodyText confidence="0.99904578125">
For example a quadratic PBF is supermodular if its
coefficients of all quadratic terms are non-negative.
Though the general supermodular maximization
algorithm can be used for any SPBF, the special
features of some specific problems allow more
efficient algorithms to be used. For example, it is
well known that the supermodular quadratic pseudo-
Boolean maximization problem can be solved in
cubic time using min-cut (Billionnet and Minoux,
1985; Kolmogorov and Zabih, 2004).
In fact, a subset of SPBFs can be maximized
using a min-cut algorithm. A pseudo-Boolean
function f(x) is called graph representable or
graph expressible if there exists a graph G = (V, E)
with terminals s and t and a subset of vertices V0 =
V − {s, t} = {v1, ... , vry,,, u1,... , um} such that,
for any configuration x1, ... , x, the value of the
function f(x) is equal to a constant plus the cost of
the minimum s-t cut among all cuts, in which vi is
connected with s if xi = 0 and connected with t if
xi = 1.
Our definition extends the work of Kolmogorov
and Zabih (2004) that focused on quadratic and
cubic functions. Vertices u1, ... um correspond to
the extra binary variables that are introduced to
reduce the graph representable PBFs to equivalent
quadratic forms. For example, the positive-negative
PBFs where all terms of degree 2 or more have
positive coefficients are graph representable, and
each non-linear term requires one extra binary
variable to obtain the equivalent quadratic form
(Rhys, 1970).
</bodyText>
<sectionHeader confidence="0.963976" genericHeader="method">
3 The Tree-Cut Decomposition for
</sectionHeader>
<subsectionHeader confidence="0.541395">
Generalized Higher Order CRFs
</subsectionHeader>
<bodyText confidence="0.999958">
We decompose the decoding problem, i.e.,
maximization of Eq (3), into two parts, a tree
labeling problem and a PBF maximization problem.
We show that the PBF can be graph representable
by reparameterizing the scoring function in Eq (3).
Then we reduce these pseudo-Boolean functions to
quadratic forms based on the recent work of ˇZivn´y
</bodyText>
<subsectionHeader confidence="0.999124">
3.1 Fully Connected Pairwise CRFs
</subsectionHeader>
<bodyText confidence="0.999974666666667">
We first describe our idea for a simple case, the
fully connected pairwise CRFs (Kr¨ahenb¨uhl and
Koltun, 2011), which are generalizations of linear-
chain CRFs and skip-chain CRFs. Formally, the
decoding problem in fully connected pairwise CRFs
can be formulated as follows:
</bodyText>
<equation confidence="0.99366">
EE E E
0v[s]Yv[s] +
v s u,v stES2
s.t. Y E Y (5)
</equation>
<bodyText confidence="0.999361333333333">
Note that for any edge (u, v), adding a constant
ψuv to all of its related patterns will not change the
optimal solution of the problem. In other words, the
optimal Y for the following problem is irrelevant to
ψuv
The reparameterization keeps the optimality of the
problem and plays an important role for graph
representation, as we will show later.
By introducing a new variable Z = Y for the
quadratic terms and relaxing the constraint Z =
Y using Lagrangian relaxation, we get the relaxed
problem
</bodyText>
<equation confidence="0.996347">
s.t. Y E Y
Zv[s] E {0,1T, dv, s
</equation>
<bodyText confidence="0.999855">
We split the inner mazY�Z into two subproblems,
and a minimal λ is found using sub-gradient descent
algorithms which repeatedly find a maximizing
assignment for the subproblems individually.
</bodyText>
<figure confidence="0.999540956521739">
max E E 0v[s]Yv[s]
Y v s
(Wuv + 0uv[st])Yuv[st]
s.t. Y E Y
E
stES2
E
+
u,v
min max E E 0v[s]Yv[s]
A Y,Z v s
(Wuv + 0uv[st])Zuv[st]
)
Av[s] (Zv[s] − Yv[s]
+E E
u,v stES2
E
s
+E
v
max
Y
0uv[st]Yuv[st]
</figure>
<page confidence="0.989257">
342
</page>
<bodyText confidence="0.9332104">
A sufficient condition for gλ(Z) to be graph
representable is that coefficients of all non-linear
terms are non-negative (Freedman and Drineas,
2005). Hence, we can set
Let
</bodyText>
<equation confidence="0.966730866666667">
fλ(Y ) = E ϕv[s]Yv[s] − E λv[s]Yv[s]
v,s v,s
E E (ψuv + ϕuv[st])Zuv[st] ψc = − min {ϕc[s]}
gλ(Z) = stES2 sEc[-]
u,v
+E λv[s]Zv[s]
v,s
The two subproblems are
max fλ(Y )
Y
s.t. Y ∈ Y
and
max gλ(Z)
Z
Zv[s] ∈ {0,1}, ∀v, s
</equation>
<bodyText confidence="0.998905833333333">
The first subproblem can be solved in linear
time since all vertices are independent. The
second problem is a binary quadratic programming
problem. As discussed in Section 2.3, gλ(Z) can
be solved using min-cut if the coefficients of the
quadratic terms are non-negative, i.e.
</bodyText>
<equation confidence="0.880148">
ψuv + ϕuv[st] ≥ 0, ∀u, v, s, t
Hence, we can set
ψuv = − min {ϕuv[st]}
stEuv[-]
</equation>
<bodyText confidence="0.99985225">
to guarantee the non-negativity. This supermodular
binary quadratic programming problem can be
solved via the push-relabel algorithm (Goldberg,
2008) in O(|S|N)3 running time.
Though Z may not satisfy the constraint Z E Y
after sub-gradient descent based optimization, Y
must satisfy Y E Y, hence we could use Y as the
final solution if Z and Y disagree.
</bodyText>
<subsectionHeader confidence="0.999362">
3.2 Generalized Higher Order CRFs
</subsectionHeader>
<bodyText confidence="0.9998012">
Now we consider the general case, maximizing
Eq (3). Similar with the pairwise case, we use two
slaves. One is a set of independent vertices, and
the other is a pseudo-Boolean optimization problem.
That is, we can redefine gλ(Z) as
</bodyText>
<equation confidence="0.93700625">
gλ(Z) = E E (ψc + ϕc[s]) Zc[s]
c sEc[-]
+E λv[s]Zv[s]
v,s
</equation>
<bodyText confidence="0.9942525">
to guarantee the non-negativity.
In real applications, higher order patterns are
</bodyText>
<equation confidence="0.464804">
�� « |S||c |(Qian et
sparse, i.e., ��ts E c[��  |ϕc[s]�=0}
</equation>
<bodyText confidence="0.999005388888889">
al., 2009; Ye et al., 2009). Hence we could skip
the patterns with zero weights (ϕc[s] = 0) when
calculating EsEc[·] ϕc[s]Zc[s] for fast inference.
However, the reparameterization described above
may introduce many non-zero terms which destroy
the sparsity. For example, in the NER task, a binary
feature is defined as true if a word subsequence
matches a location name in a gazetteer. Suppose
c =Little York village is such a word subsequence,
then among |S|3 possible assignments of c, only
the one that labels c =Little York village as a
location name has non-zero weight. However, the
reparameterization may add ψc to the other |S|3 − 1
assignments, yielding many new patterns.
Therefore, we use another reparameterization
strategy that exploits the sparsity for efficient
decomposition. We only reparameterize the weights
of edges, i.e., quadratic terms. Let
</bodyText>
<equation confidence="0.9547745">
(ψc + ϕc[s]) Zc[s]
E ϕc[s]Zc[s] + E
|c|≥3
c sEc[-]
</equation>
<bodyText confidence="0.99561725">
The optimal solution is unchanged for any ψ.
In Appendix A, we show that by setting a
sufficiently large ψ, gλ(Z) is graph representable.
Such reparameterization method requires at most
N2|S|2 new patterns ψc,|c|=2 to make gλ(Z) graph
representable. It preserves the sparsity of higher
order patterns, hence is more efficient than the naive
approach.
</bodyText>
<subsectionHeader confidence="0.988221">
3.3 Tree-Cut Decomposition
</subsectionHeader>
<bodyText confidence="0.9999702">
In some cases, the graph is built by adding sparse
global patterns to local models like trees, resulting
in nearly tree-structured CRFs. For example, Sutton
and Mccallum (2006) used skip-chain CRFs for
NER, where skip-edges connecting identical words
</bodyText>
<equation confidence="0.5926419">
E
gλ(Z) =
c
|c|=2
E
sEc[-]
E
+
λv[s]Zv[s]
v,s
</equation>
<page confidence="0.98985">
343
</page>
<bodyText confidence="0.99932475">
were added to linear chain CRFs. Since the skip-
edges are sparse, the resulting graphical models
are nearly linear chains. To handle the edges
in local models efficiently, we reformulate the
decomposition. Let T be a spanning tree of the
graph, if edge (u, v) E T, we put its related patterns
into the first slave, otherwise we put its related
patterns into the second slave.
For clarity, we formulate the tree-cut
decomposition for generalized higher order
CRFs. The first slave involves the patterns covered
by the spanning tree T, and its scoring function is
</bodyText>
<table confidence="0.993167375">
∑ ∑ ∑
fλ(Y ) = ϕv[s]Yv[s] − λv[s]Yv[s]
v s v,s
∑ ∑  
+ sEc[-]  ∑ 
c∈T ψc + ϕc[s] + ϕc′[s′]   Yc[s].
|c|=2 c′[s′]⊇c[s]
|c′|≥3,ϕc′[s′]&lt;0
</table>
<bodyText confidence="0.999133714285714">
The second slave involves the rest patterns. To get
its quadratic form, for each pattern c[s], |c |= 3,
we introduce one extra binary variable uc[s], and for
each pattern c[s], |c |&gt; 4, we introduce |c |− 3 extra
binary variables ukc[s], k = 0, ... , |c |− 4. Let u
denote the vector of all the introduced extra binary
variables. For each pattern c[s], denote
</bodyText>
<equation confidence="0.976569">
∑Zc[s] = Zv[s].
v[s]Ec[s]
</equation>
<bodyText confidence="0.994869">
The scoring function of the second slave is
h(Z, u) = h1(Z) + h2(Z, u) + h3(Z, u) + h4(Z, u)
where
</bodyText>
<equation confidence="0.759545">
h1(Z) = ∑ λv[s]Zv[s]
v,s
</equation>
<table confidence="0.9990492">
Term Number of Variables
h1(Z) N2|S|2
h2(Z, u) ∑ c ∑ s∈c[·] (1 + |c|)
|c|≥3
h3 (Z, u) ∑ c ∑ s∈c[·] (1 + |c|)
|c|=3
ϕc[s]&lt;0
h4(Z, u) ∑ c (2|c |− 3)
∑ s∈c[·]
|c|≥4 ϕc[s]&lt;0
</table>
<tableCaption confidence="0.999818">
Table 1: Number of variables in each part of h(Z, u)
</tableCaption>
<bodyText confidence="0.990598666666667">
h1 involves the edges that are not in T, h2 involves
positive terms of degree 3 or more. h3 involves
negative cubic terms, h4 involves negative terms of
degree 4 or more.
The relaxed problem for generalized higher order
CRFs, i.e., Problem (2) is
</bodyText>
<equation confidence="0.9680388">
min
λ Y,Z,u
max fλ(Y ) + h(Z, u)
s.t. Y ∈ Y (6)
Z, u are binary
</equation>
<subsectionHeader confidence="0.980163">
3.4 Complexity Analysis
</subsectionHeader>
<bodyText confidence="0.9930805">
In this section, we theoretically analyze the time
complexity for each iteration in dual decomposition.
Running time for maxY EY f),(Y) is linear in the
size of the graph, i.e., N x |S|2. Running time for
maxZ�u h(Z, u) is cubic in the number of variables,
which is the sum of variables in function h1 to h4.
h1(Z) has at most N2|S|2 variables; each pattern in
h2(Z, u) requires one extra variable, hence h2(Z, u)
</bodyText>
<equation confidence="0.903518">
has E c E (1 + |c|) vari ables. Similarly,
s∈c[·]
|c|≥3 ϕc[s]≥0
</equation>
<bodyText confidence="0.9531275">
we could count the number of variables in h3 and
h4, as shown in Table 1.
In summary, each pattern in h(Z, u) requires at
most 2|c|−2 variables, so h(Z, u) has no more than
�� sEc[·] (2|c |− 2) variables.
c
Finally, the time complexity for each iteration in
dual decomposition is
</bodyText>
<equation confidence="0.502743769230769">

∑ ∑
N|S|2 + (2|c |− 2)
c sEc[-]
which is cubic in the total length of patterns.
∑
sEc[-]
∑
+
c̸∈T
|c|=2


ϕc′[s′]   Zc[s]

 ∑
ψc + ϕc[s] +
c′[s′]⊇c[s]
|c′|≥3,ϕc′[s′]&lt;0
∑
s∈c[·]
ϕc[s]≥0
∑
h2(Z, u) =
c
|c|≥3
ϕc[s] (Zc[s] − |c |+ 1) uc[s]
∑
s∈c[·]
ϕc[s]&lt;0
∑
h3(Z, u) =
c
|c|=3
��ϕc[s] I uc[s] (Zc[s] − 1)
∑
s∈c[·]
ϕc[s]&lt;0
∑
h4(Z, u) =
c
|c|≥4
(|ϕc[s] |u0 c[s](2Zc[s] − 3)
|c|-4
∑
j=1
+

ujc[s](Zc[s] − j − 2)  .
O (
 3
  
</equation>
<page confidence="0.996803">
344
</page>
<sectionHeader confidence="0.999414" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.986262">
4.1 Named Entity Recognition in Tweets
4.1.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999955212121212">
Our first experiment is named entity recognition
in tweets. Recently, information extraction on
Twitter or Facebook data is attracting much attention
(Ritter et al., 2011). Different from traditional
information extraction for news articles, messages
posted on these social media websites are short
and noisy, making the task more challenging. In
this paper, we use generalized higher order CRFs
for Twitter NER with discriminative training, and
compare our 2-slave dual decomposition approach
with spanning tree based dual decomposition
approach and other decoding algorithms.
So far as we know, there are two publicly
available data sets for Twitter NER. One is the
Ritter’s (Ritter et al., 2011), the other is from
MSM2013 Concept Extraction Challenge (Basave
et al., 2013)1. Note that in Ritter’s work (Ritter et
al., 2011), all of the data are used for evaluating
named entity type classification, and not used
during training. However, our approach requires
discriminative training, which makes our method
not comparable with their results. Therefore we
choose the MSM2013 dataset in our experiment
and compare our system with the MSM2013 official
runs.
The MSM2013 corpus has 4 types of named
entities, person (PER), location (LOC), organization
(ORG), and miscellaneous (MISC). The name
entities are about film/movie, entertainment award
event, political event, programming language,
sporting event and TV show. The data is separated
into a training set containing 2815 tweets, and a test
set containing 1526 tweets.
</bodyText>
<subsectionHeader confidence="0.61987">
4.1.2 Local Features
</subsectionHeader>
<bodyText confidence="0.999791">
We cast the NER task as a structured classification
problem, and adopt BIESO labeling, where for each
multi-word entity of class C, the first word is labeled
as B-C, the words in the entity are labeled as I-C,
and the last word is labeled as E-C, a single word
entity of class C is labeled as S-C, and other words
are labeled as O.
</bodyText>
<footnote confidence="0.869839">
1http://oak.dcs.shef.ac.uk/msm2013/challenge.html
</footnote>
<bodyText confidence="0.99993275">
Our baseline NER is a linear chain CRF. As
the MSM2013 competition allows to use extra
resources, we use several additional datasets to
generate rich features. Specifically, we trained
two POS taggers and two NER taggers using extra
datasets. All the 4 taggers are trained using linear
chain CRFs with perceptron training. One POS
tagger is trained on Brown and Wall Street Journal
corpora in Penn Tree Bank 3, and the other is trained
on ARK Twitter NLP corpus (Gimpel et al., 2011)
with slight modification. One of the NER taggers
is trained on CoNLL 2003 English dataset2, and the
other is trained on Ritter’s dataset.
We used dictionaries in Ark Twitter NLP toolkit3,
Ritter’s Twitter NLP toolkit4 and Moby Words
projects to generate dictionary features. We also
collected film names and TV shows from IMDB
website and musician groups from wikipedia. These
dictionaries are used to detect candidate named
entities in the training and testing datasets using
string matching. Those matched words are assigned
with BIESO style labels which are used as features.
We also used the unsupervised word cluster
features provided by Ark Twitter NLP toolkit, which
has significantly improved the Twitter POS tagging
accuracy (Owoputi et al., 2013). Similar with
previous work, we used prefixes of the cluster bit
strings with lengths ∈ {2, 4, ... ,16} as features.
</bodyText>
<subsubsectionHeader confidence="0.573209">
4.1.3 Global Features
</subsubsectionHeader>
<bodyText confidence="0.999858785714286">
Previous studies showed that the document level
consistency features (same phrases in a document
tend to have the same entity class) are effective for
NER (Kazama and Torisawa, 2007; Finkel et al.,
2005). However, unlike news articles, tweets are
not organized in documents. To use these document
level consistency features, we grouped the tweets
in MSM2013 dataset using single linkage clustering
algorithm where similarity between two tweets is the
number of their overlapped words. If the similarity
is greater than 4, then we put the two tweets into
one group. Unlike standard document clustering,
we did not normalize the length of tweets since all
the tweets are limited to 140 characters. Then we
</bodyText>
<footnote confidence="0.99957925">
2www.cnts.ua.ac.be/conll2003/
3https://code.google.com/p/ark-tweet-nlp/
4http://github.com/aritter/Twitter nlp
5http://icon.shef.ac.uk/Moby/mwords.html
</footnote>
<page confidence="0.998928">
345
</page>
<bodyText confidence="0.9733415">
extracted the group level features as follows. For
any two identical phrases xi ... xi+k, xj ... xj+k in
a group, a binary feature is true if they have the
same label subsequences. The pattern set of this
feature is c = {i, ... , i + k, j, ... , j + k} and
cH = {S|si = sj, ... , si+k = sj+k}.
</bodyText>
<sectionHeader confidence="0.934118" genericHeader="method">
4.1.4 Results
</sectionHeader>
<bodyText confidence="0.985377948717949">
We use two evaluation metrics. One is the micro
averaged F score, which is used in CoNLL2003
shared task. The other is macro averaged F score,
which is used in MSM2013 official evaluation
(Basave et al., 2013).
We compare our approach with two baselines,
integer linear programming (ILP)6 and a naive
dual decomposition method. In naive dual
decomposition, we use three types of slaves: a
linear chain captures unigrams and bigrams, and the
spanning trees cover the skip edges linking identical
words. Identical multi-word phrases yield larger
factors with more than 4 vertices. They could
not be handled efficiently by belief propagation for
spanning trees. Therefore, we create multiple slaves,
each of which covers a pair of identical multi-word
phrases.
To reduce the number of slaves, we use a greedy
algorithm to choose the spanning trees. Each time
we select the spanning tree that covers the most
uncovered edges. This can be done by performing
the maximum spanning tree algorithm on the graph
where each uncovered edge has unit weight. Let x*
denote the most frequent word in a tweet cluster, and
F* is its frequency, then at least (F*−1)/2 spanning
trees are required to cover the complete subgraph
spanned by x*.
For both dual decomposition systems, averaged
perceptron (Collins, 2002a) with 10 iterations is
used for parameter estimation. We follow the work
of Rush et al. (2010) to choose the step size in the
sub-gradient algorithm.
Table 2 shows the comparison results, including
two F scores and total running time (seconds)
for training and testing. Performances of the top
4 official runs are also listed. Different from
our approach, the top performing systems mainly
benefit from rich open resources, such as DBpedia
Ewe use Gurobi as the ILP solver, http://www.gurobi.com/
</bodyText>
<table confidence="0.999622444444444">
System &apos;macro &apos;micro Sec.
Linear chain CRFs 0.657 0.815 98
General CRFs (2-slave DD) 0.680 0.827 214
General CRFs (naive DD) 0.672 0.824 490
General CRFs (ILP) 0.680 0.828 8640
Official 1st 0.670 N/A N/A
Official 2nd 0.662 N/A N/A
Official 3rd 0.658 N/A N/A
Official 4th 0.610 N/A N/A
</table>
<tableCaption confidence="0.9733125">
Table 2: Comparison results on MSM2013 Twitter
NER task.
</tableCaption>
<bodyText confidence="0.999026772727273">
Gazetteer, ANNIE Gazetteer, Yago, Microsoft N-
grams, and external NER system combination,
such as ANNIE, OpenNLP, LingPipe, OpenCalais
(Basave et al., 2013). We can see that general CRFs
with global features are competitive with these top
systems. Our 2-slave DD outperforms naive DD
and achieves competitive performance with exact
inference based on ILP, while is much faster than
ILP.
To compare the convergence speed and optimality
of 2-slave DD and naive DD algorithms, we use
the model trained by ILP, and record the Fmicro
scores, averaged dual objectives per instance (the
lower the tighter), decoding time, and fraction of
optimality certificates across iterations of the two
DD algorithms on test data. Figure 2 shows
the performances of the two algorithms relative to
decoding time. Our method requires 0.0064 seconds
for each iteration on average, about four times
slower than the naive DD. However, our approach
achieves a tighter upper bound and larger fraction of
optimality certificates.
</bodyText>
<subsectionHeader confidence="0.999203">
4.2 Sentence Dependency Tagging
</subsectionHeader>
<bodyText confidence="0.999132">
Our second experiment is sentence dependency
tagging in Question Answering forums task studied
in Qu and Liu’s work (Qu and Liu, 2012). The goal
is to extract the dependency relationships between
sentences for automatic question answering. For
example, from the posts below, we would need to
know that sentence S4 is a comment about sentence
S1 and S2, not an answer to S3.
</bodyText>
<page confidence="0.995269">
346
</page>
<figure confidence="0.967381875">
Dual Objective
50 100 150 200
Decoding Time (sec.)
Decoding Time (sec.)
%certificates of naive DD
%certificates of 2−slave DD
F score of naive DD
F score of 2−slave DD
7300
7200
7100
7000
6900
100 150 200
6800
0 50
naive DD
2−slave DD
Percentage 1
0.9
0.8
0.7
0.6
0.5
</figure>
<figureCaption confidence="0.995022">
Figure 2: Twitter NER: The F..i,ro scores, dual objectives, and fraction of optimality certificates relative to
decoding time.
</figureCaption>
<figure confidence="0.76183">
source
</figure>
<figureCaption confidence="0.999567">
Figure 3: 3-wise CRF for QA sentence dependency
</figureCaption>
<bodyText confidence="0.952170190476191">
tagging. Order-3 factors (e.g., red and blue)
connects the 3 vertices in adjacent edge pairs.
A: [S1]I’m having trouble installing my DVB Card.
[S2]dmesg prints:.. .
[S3]What could I do to resolve this problem?
B: [S4] I’m having similar problems with Ubuntu
For a pair of sentences, the depending sentence
is called the source sentence, and the depended
sentence the target sentence. One source sentence
can potentially depend on many different target
sentences, and one target sentence can also
correspond to multiple sources. Qu and Liu (2012)
casted the task as a binary classification problem,
i.e., whether or not there exists a dependency
relation between a pair of sentences. Formally,
in this task, Y is a N2 × 2 matrix, where N is
the number of sentences, Yi*N+j[1] = 1 if the ith
sentence depends on the jth sentence, otherwise,
Yi*N+j[0] = 1. We use the corpus in Qu and
Liu’s work (Qu and Liu, 2012), where dependencies
between 3,483 sentences in 200 threads were
</bodyText>
<table confidence="0.998527166666667">
System F Sec.
2D CRFs (naive) 0.564 9.2
2D CRFs (2-slave) 0.565 16.4
3-wise CRFs (naive) 0.572 18.7
3-wise CRFs (2-slave) 0.584 17.33
(Qu and Liu, 2012) 0.561 N/A
</table>
<tableCaption confidence="0.8330085">
Table 3: Comparison results on QA sentence
dependency tagging task.
</tableCaption>
<bodyText confidence="0.97753176">
annotated. Following their settings we randomly
split annotated threads into three disjoint sets, and
run a three-fold cross validation. F score is used as
the evaluation metric.
Qu and Liu (2012) used the pairwise CRF with
a 4-connected neighborhood system (2D CRF)
as their graphical model, where each vertex in
the graph represents a sentence pair, and each
edge connects adjacent source sentences or target
sentences. The key observation is that given a
source/target sentence, there is strong dependency
between adjacent target/source sentences. In this
paper, we extend their work by connecting the 3
vertices in adjacent edge pairs, resulting in 3-wise
CRFs, as shown in Figure 3. We use the same
vertex features and edge features as in Qu and
Liu’s work. For a 3-tuple of vertices, we use the
following features: combination of the sentence
types within the tuple, whether the related sentences
are in one post or belong to the same author.
Again, we use perceptron to train the model, and
the max iteration number for dual decomposition is
200. The spanning tree in our decomposition is the
concatenation of all the rows in the graph.
target
</bodyText>
<page confidence="0.676795">
347
</page>
<figure confidence="0.996092208333333">
1
0.8
0.6
0.4
0.2
0
0 1 2 3 4 5
9.25 x 105
naive DD
2−slave DD
9.15
9.1
9.05
9
8.95
0 1 2 3 4 5
Percentage
Dual Objective
%certificates of naive DD
%certificates of 2−slave DD
F score of naive DD
F score of 2−slave DD
9.2
Decoding Time (sec.) Decoding Time (sec.)
</figure>
<figureCaption confidence="0.957908">
Figure 4: QA sentence dependency tagging using 3-wise CRFs: The F scores, dual objectives, and fraction
of optimality certificates relative to decoding time.
</figureCaption>
<bodyText confidence="0.999968611111111">
Table 3 shows the experimental results. For 2D
CRFs, the edges can be covered by 2 spanning
trees (one covers all vertical edges and the other
covers all horizontal edges), hence the naive dual
decomposition has only two slaves. Compared with
naive DD, our 2-slave DD achieved competitive
performance while two times slower. This is
because naive DD adopts dynamic programming
that runs in linear time. However, for 3-wise CRFs,
the naive dual decomposition requires many small
slaves to cover the order-3 factors. Therefore our
2-slave method is more effective. The fraction of
optimality certificates and dual objectives of 3-wise
CRFs relative to decoding time during testing are
shown in Figure 4. For each iteration, our method
requires 0.0049 seconds and the naive DD requires
0.00054 seconds, about 10 times faster than ours,
but our method converges to a lower lower bound.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9997130625">
We proposed a new decomposition approach for
generalized higher order CRFs using only two
slaves. Both permit polynomial decoding time. We
evaluated our method on two different tasks: Twitter
named entity recognition and forum sentence
dependency detection. Experimental results show
that though the compact decomposition requires
more running time for each iteration, it achieves
consistently tighter bounds and outperforms the
naive dual decomposition. The two experiments
demonstrate that our method works for general
graphs, even if the graph can not be decomposed into
a few spanning trees (for example, if the graph has
large complete subgraphs or large factors).
Our code is available at
https://github.com/qxred/higher-order-crf
</bodyText>
<sectionHeader confidence="0.970757" genericHeader="method">
Appendix A
</sectionHeader>
<bodyText confidence="0.999901333333333">
We show that by setting a sufficiently large ψ, g),(Z)
in Section 3.2 is graph representable.
Let
</bodyText>
<equation confidence="0.997941">
9A(Z) = 91(Z) + 92(Z) + 93(Z) + 94(Z)
</equation>
<bodyText confidence="0.829322">
where
</bodyText>
<equation confidence="0.9975045625">
E
91(Z) =
c
ici=2
E
92(Z) =
c
�c�&gt;3
E
93(Z) =
c
�c�=3
E
94(Z) =
c
�c1&gt;4
</equation>
<bodyText confidence="0.9940525">
For g2(Z), since coefficients of all terms are non-
negative, we can use the fact
</bodyText>
<equation confidence="0.916613333333333">
ai = max�� ai − |a |+ 1 I b (7)
bE{0,1} \ /
I:
</equation>
<bodyText confidence="0.965952333333333">
to reduce g2(Z) into an equivalent quadratic
form (Freedman and Drineas, 2005). That is,
maxZ g2(Z) is equivalent to
</bodyText>
<figure confidence="0.96952596">
� �
�E
Oc[s] Zv[s] − |c |+ 1 uc[s]
v[s]Ec[s]
E E Av[s]Zv[s]
sEc[-] (ψc + Oc[s] ) Zc[s] +
v,s
E Oc[s]Zc[s]
sEc[·]
ϕc[s]&gt;0
E Oc[s]Zc[s]
sEc[·]
ϕc[s]&lt;0
E Oc[s]Zc[s]
sEc[·]
ϕc[s]&lt;0
�
i
aiE10,1}
Emax
Z,u
c,�c1&gt;3
ϕc[s]&gt;0
E
sEc[-]
</figure>
<page confidence="0.990803">
348
</page>
<bodyText confidence="0.8356082">
which is graph representable because coefficients of
all the quadratic terms are non-negative.
Coefficients of terms in g3(Z) and g4(Z) are
negative, therefore g3(Z) and g4(Z) are not
supermodular. To make them graph representable,
we use the following fact
Proposition 1 ( ˇZivn´y and Jeavons, 2010)
The pseudo-Bool�eaan function p(x) =
E1≤i&lt;j≤K xixj − 11j 1 xi is graph representable
and can be reduced to the quadratic forms: if
</bodyText>
<equation confidence="0.994920857142857">
K = 3, then
p(x) = max (x1 + x2 + x3 − 1)y (8)
y∈{0,1}
otherwise K &gt; 3,
p(x) = max y0(2
y0∈{0,1}
xi − j − 2) (9)
</equation>
<bodyText confidence="0.810787">
According to Eq (8), for each cubic term in g3(Z),
we have
</bodyText>
<figure confidence="0.531785833333333">
Oc[s] ∏ Zv[s]
v[s]∈c[s]
 
�
=Oc[s] ∏
∑ Zu[s]Zv[t] − Zv[s] 
u[s],v[t]∈c[s] v[s]∈c[s]
− �Oc[s]
��� ∑ Zu[s]Zv[t]
u[s],v[t]∈c[s]
Oc]
[ u.]∈{0,1}
s max uc[s] Zv[s] − 1 
 ∑
v[s]∈c[s]
− �Oc[s]
��� ∑ Zu[s]Zv[t]
u[s],v[t]∈c[s]
</figure>
<bodyText confidence="0.999704375">
The first part on the right hand side is graph
representable since all quadratic terms are non-
negative. The second part is a quadratic function
of Z, and it can be merged into g1(Z). With
sufficiently large ψc in g1(Z), we could guarantee
the non-negativity of all quadratic terms.
Similarly, we could apply Eq (9) to reduce g4(Z)
to graph representable quadratic forms.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999927">
We thank three anonymous reviewers for their
valuable comments. This work is partly supported
by DARPA under Contract No. FA8750-13-2-0041.
Any opinions expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA.
</bodyText>
<sectionHeader confidence="0.982766" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99963047826087">
Amparo Elizabeth Cano Basave, Andrea Varga, Matthew
Rowe, Milan Stankovic, and Aba-Sah Dadzie.
2013. Making sense of microposts (msm2013)
concept extraction challenge (challenge report). In
Proceedings of the Concept Extraction Challenge at
the Workshop on ’Making Sense of Microposts’, pages
1–15.
A. Billionnet and M. Minoux. 1985. Maximizing a
supermodular pseudoboolean function: A polynomial
algorithm for supermodular cubic functions. Discrete
Applied Mathematics, 12(1):1 – 11.
Endre Boros and PeterL. Hammer. 1991. The max-cut
problem and quadratic 0-1 optimization; polyhedral
aspects, relaxations and bounds. Annals of Operations
Research, 33(3):151–180.
Endre Boros and Peter L. Hammer. 2002. Pseudo-
boolean optimization. Discrete Applied Mathematics,
123(1C3):155 – 225.
Michael Collins. 2002a. Discriminative training
methods for hidden markov models: Theory
and experiments with perceptron algorithms. In
Proceedings of EMNLP, pages 1–8.
Michael Collins. 2002b. Ranking algorithms for named
entity extraction: Boosting and the votedperceptron.
In Proceedings of ACL, pages 489–496, July.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to extract
contexts and answers of questions from online forums.
In Proceedings ofACL-08: HLT, pages 710–718, June.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL, pages 363–370, June.
Daniel Freedman and Petros Drineas. 2005. Energy
minimization via graph cuts: Settling what is possible.
In Proceedings of CVPR, pages 939–946. IEEE
Computer Society.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance. In
Proceedings of EMNLP, pages 364–372.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of ACL-HLT, HLT ’11, pages 42–47.
</reference>
<figure confidence="0.8640853">
∑K
i=1
xi − 3)
+ max
binary y
yj(
∑K
i=1
K−4∑
j=1
</figure>
<page confidence="0.988931">
349
</page>
<reference confidence="0.999523469387755">
AndrewV. Goldberg. 2008. The partial augmentcrelabel
algorithm for the maximum flow problem. In Dan
Halperin and Kurt Mehlhorn, editors, Algorithms -
ESA 2008, volume 5193 of Lecture Notes in Computer
Science, pages 466–477. Springer Berlin Heidelberg.
Vladimir Jojic, Stephen Gould, and Daphne Koller.
2010. Accelerated dual decomposition for map
inference. In Proceedings of ICML, pages 503–510.
Omnipress.
Jun’ichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP-CoNLL,
pages 315–324, June.
Vladimir Kolmogorov and Ramin Zabih. 2004. What
energy functions can be minimized via graph cuts?
IEEE Trans. Pattern Anal. Mach. Intell., 26(2):147–
159.
Vladimir Kolmogorov. 2006. Convergent tree-
reweighted message passing for energy minimization.
IEEE Trans. Pattern Anal. Mach. Intell., 28(10):1568–
1583, October.
Nikos Komodakis. 2011. Efficient training for
pairwise or higher order CRFs via dual decomposition.
In Proceedings of CVPR, pages 1841–1848. IEEE
Computer Society.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of EMNLP, pages 1288–
1298, Cambridge, MA, October.
Philipp Kr¨ahenb¨uhl and Vladlen Koltun. 2011. Efficient
inference in fully connected crfs with gaussian edge
potentials. In Proceedings of NIPS, pages 109–117.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proceedings of ICML, pages 282–289.
Andre Martins, Mario Figueiredo, Pedro Aguiar, Noah
Smith, and Eric Xing. 2011a. An augmented
lagrangian approach to constrained map inference. In
Proceedings of ICML, pages 169–176. ACM, June.
Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011b. Dual decomposition with
many overlapping components. In Proceedings of the
EMNLP, pages 238–249, July.
G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978.
An analysis of approximations for maximizing
submodular set functions-I. Mathematical
Programming, 14(1):265–294.
James B. Orlin. 2009. A faster strongly polynomial
time algorithm for submodular function minimization.
Math. Program., 118(2):237–251.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith.
2013. Improved part-of-speech tagging for online
conversational text with word clusters. In Proceedings
of NAACL-HLT, pages 380–390, June.
Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing Huang,
and Lide Wu. 2009. Sparse higher order conditional
random fields for improved sequence labeling. In
Proceedings of ICML, volume 382, page 107. ACM.
Zhonghua Qu and Yang Liu. 2012. Sentence dependency
tagging in online question answering forums. In
Proceedings of ACL, pages 554–562, July.
J. M. W. Rhys. 1970. A selection problem of shared
fixed costs and network flows. Management Science,
17(3):200–207.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An
experimental study. In Proceedings of EMNLP, pages
1524–1534, July.
Alexander M. Rush and Michael Collins. 2012. A
tutorial on dual decomposition and lagrangian
relaxation for inference in natural language
processing. J. Artif. Int. Res., 45(1):305–362,
September.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of EMNLP 2010, pages
1–11, Cambridge, MA, October.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proceedings of NIPS.
Charles Sutton and Andrew Mccallum, 2006.
Introduction to Conditional Random Fields for
Relational Learning. MIT Press.
Rustem Takhanov and Vladimir Kolmogorov. 2013.
Inference algorithms for pattern-based CRFs on
sequence data. In Proceedings of ICML, pages 145–
153.
Stanislav ˇZivn´y and Peter G. Jeavons. 2010. Classes
of submodular constraints expressible by graph cuts.
Constraints, 15(3):430–452, July.
Nan Ye, Wee Sun Lee, Hai Leong Chieu, and Dan
Wu. 2009. Conditional random fields with high-
order features for sequence labeling. In Proceedings
of NIPS, pages 2196–2204.
</reference>
<page confidence="0.997671">
350
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.841005">
<title confidence="0.995582">2-Slave Dual Decomposition for Generalized Higher Order CRFs</title>
<author confidence="0.990587">Qian</author>
<affiliation confidence="0.9807145">Computer Science The University of Texas at</affiliation>
<abstract confidence="0.9927133125">We show that the decoding problem in generalized Higher Order Conditional Random Fields (CRFs) can be decomposed into two parts: one is a tree labeling problem that can be solved in linear time using dynamic programming; the other is a supermodular quadratic pseudo-Boolean maximization problem, which can be solved in cubic time using a minimum cut algorithm. We use dual decomposition to force their agreement. Experimental results on Twitter named entity recognition and sentence dependency tagging tasks show that our method outperforms spanning tree based dual decomposition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amparo Elizabeth Cano Basave</author>
<author>Andrea Varga</author>
<author>Matthew Rowe</author>
<author>Milan Stankovic</author>
<author>Aba-Sah Dadzie</author>
</authors>
<title>Making sense of microposts (msm2013) concept extraction challenge (challenge report).</title>
<date>2013</date>
<booktitle>In Proceedings of the Concept Extraction Challenge at the Workshop on ’Making Sense of Microposts’,</booktitle>
<pages>1--15</pages>
<contexts>
<context position="20595" citStr="Basave et al., 2013" startWordPosition="3470" endWordPosition="3473">r et al., 2011). Different from traditional information extraction for news articles, messages posted on these social media websites are short and noisy, making the task more challenging. In this paper, we use generalized higher order CRFs for Twitter NER with discriminative training, and compare our 2-slave dual decomposition approach with spanning tree based dual decomposition approach and other decoding algorithms. So far as we know, there are two publicly available data sets for Twitter NER. One is the Ritter’s (Ritter et al., 2011), the other is from MSM2013 Concept Extraction Challenge (Basave et al., 2013)1. Note that in Ritter’s work (Ritter et al., 2011), all of the data are used for evaluating named entity type classification, and not used during training. However, our approach requires discriminative training, which makes our method not comparable with their results. Therefore we choose the MSM2013 dataset in our experiment and compare our system with the MSM2013 official runs. The MSM2013 corpus has 4 types of named entities, person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). The name entities are about film/movie, entertainment award event, political event, progra</context>
<context position="24504" citStr="Basave et al., 2013" startWordPosition="4099" endWordPosition="4102">m/p/ark-tweet-nlp/ 4http://github.com/aritter/Twitter nlp 5http://icon.shef.ac.uk/Moby/mwords.html 345 extracted the group level features as follows. For any two identical phrases xi ... xi+k, xj ... xj+k in a group, a binary feature is true if they have the same label subsequences. The pattern set of this feature is c = {i, ... , i + k, j, ... , j + k} and cH = {S|si = sj, ... , si+k = sj+k}. 4.1.4 Results We use two evaluation metrics. One is the micro averaged F score, which is used in CoNLL2003 shared task. The other is macro averaged F score, which is used in MSM2013 official evaluation (Basave et al., 2013). We compare our approach with two baselines, integer linear programming (ILP)6 and a naive dual decomposition method. In naive dual decomposition, we use three types of slaves: a linear chain captures unigrams and bigrams, and the spanning trees cover the skip edges linking identical words. Identical multi-word phrases yield larger factors with more than 4 vertices. They could not be handled efficiently by belief propagation for spanning trees. Therefore, we create multiple slaves, each of which covers a pair of identical multi-word phrases. To reduce the number of slaves, we use a greedy alg</context>
<context position="26586" citStr="Basave et al., 2013" startWordPosition="4434" endWordPosition="4437"> systems mainly benefit from rich open resources, such as DBpedia Ewe use Gurobi as the ILP solver, http://www.gurobi.com/ System &apos;macro &apos;micro Sec. Linear chain CRFs 0.657 0.815 98 General CRFs (2-slave DD) 0.680 0.827 214 General CRFs (naive DD) 0.672 0.824 490 General CRFs (ILP) 0.680 0.828 8640 Official 1st 0.670 N/A N/A Official 2nd 0.662 N/A N/A Official 3rd 0.658 N/A N/A Official 4th 0.610 N/A N/A Table 2: Comparison results on MSM2013 Twitter NER task. Gazetteer, ANNIE Gazetteer, Yago, Microsoft Ngrams, and external NER system combination, such as ANNIE, OpenNLP, LingPipe, OpenCalais (Basave et al., 2013). We can see that general CRFs with global features are competitive with these top systems. Our 2-slave DD outperforms naive DD and achieves competitive performance with exact inference based on ILP, while is much faster than ILP. To compare the convergence speed and optimality of 2-slave DD and naive DD algorithms, we use the model trained by ILP, and record the Fmicro scores, averaged dual objectives per instance (the lower the tighter), decoding time, and fraction of optimality certificates across iterations of the two DD algorithms on test data. Figure 2 shows the performances of the two a</context>
</contexts>
<marker>Basave, Varga, Rowe, Stankovic, Dadzie, 2013</marker>
<rawString>Amparo Elizabeth Cano Basave, Andrea Varga, Matthew Rowe, Milan Stankovic, and Aba-Sah Dadzie. 2013. Making sense of microposts (msm2013) concept extraction challenge (challenge report). In Proceedings of the Concept Extraction Challenge at the Workshop on ’Making Sense of Microposts’, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Billionnet</author>
<author>M Minoux</author>
</authors>
<title>Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions.</title>
<date>1985</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="10879" citStr="Billionnet and Minoux, 1985" startWordPosition="1735" endWordPosition="1738">l of its second order derivatives are nonnegative (Nemhauser et al., 1978), i.e., for all i &lt; j, and Jeavons (2010), and finally solve the slave problem via graph cuts. ∂f &gt; 0 ∂xi∂xj For example a quadratic PBF is supermodular if its coefficients of all quadratic terms are non-negative. Though the general supermodular maximization algorithm can be used for any SPBF, the special features of some specific problems allow more efficient algorithms to be used. For example, it is well known that the supermodular quadratic pseudoBoolean maximization problem can be solved in cubic time using min-cut (Billionnet and Minoux, 1985; Kolmogorov and Zabih, 2004). In fact, a subset of SPBFs can be maximized using a min-cut algorithm. A pseudo-Boolean function f(x) is called graph representable or graph expressible if there exists a graph G = (V, E) with terminals s and t and a subset of vertices V0 = V − {s, t} = {v1, ... , vry,,, u1,... , um} such that, for any configuration x1, ... , x, the value of the function f(x) is equal to a constant plus the cost of the minimum s-t cut among all cuts, in which vi is connected with s if xi = 0 and connected with t if xi = 1. Our definition extends the work of Kolmogorov and Zabih (</context>
</contexts>
<marker>Billionnet, Minoux, 1985</marker>
<rawString>A. Billionnet and M. Minoux. 1985. Maximizing a supermodular pseudoboolean function: A polynomial algorithm for supermodular cubic functions. Discrete Applied Mathematics, 12(1):1 – 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hammer</author>
</authors>
<title>The max-cut problem and quadratic 0-1 optimization; polyhedral aspects, relaxations and bounds.</title>
<date>1991</date>
<journal>Annals of Operations Research,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="9722" citStr="Hammer, 1991" startWordPosition="1530" endWordPosition="1531">framework. Our work improves Komodakis’ by using a concise decomposition with only two slaves. 2.3 Graph Representable Pseudo-Boolean Optimization One slave in our approach is a graph representable pseudo-Boolean maximization problem, which can be reduced to a supermodular quadratic pseudo-Boolean maximization problem and solved efficiently using an algorithm for finding a minimal cut. A pseudo-Boolean function (PBF) (Boros and Hammer, 2002) is a multilinear function of binary variables, that is where xi E {0, 1}. Maximizing a PBF is usually NP-hard, such as the maximum cut problem (Boros and Hammer, 1991). A pseudo-Boolean function is said to be supermodular iff f(x) + f(y) ≤ f(x ∧ y) + f(x V y) where x n y, x V y are the element-wise AND and OR operator of the two vectors respectively. This is an important concept, because a supermodular pseudo-Boolean function (SPBF) can be maximized in O(n6) running time (Orlin, 2009). A necessary and sufficient condition for identifying a SPBF is ∑M i=1 Oi(Y ) max Y ∑M i=1 Oi(Zi) min max ∑M ∑ A Y,Z1...ZM i=1 Oi(Zi) + i f(x) = ∑ ∑aixi + ∑aijxixj + aijkxixjxk + .. . i i&lt;j i&lt;j&lt;k 341 that all of its second order derivatives are nonnegative (Nemhauser et al., 1</context>
</contexts>
<marker>Hammer, 1991</marker>
<rawString>Endre Boros and PeterL. Hammer. 1991. The max-cut problem and quadratic 0-1 optimization; polyhedral aspects, relaxations and bounds. Annals of Operations Research, 33(3):151–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Endre Boros</author>
<author>Peter L Hammer</author>
</authors>
<title>Pseudoboolean optimization.</title>
<date>2002</date>
<journal>Discrete Applied Mathematics, 123(1C3):155 –</journal>
<pages>225</pages>
<contexts>
<context position="9554" citStr="Boros and Hammer, 2002" startWordPosition="1498" endWordPosition="1501"> empirical success even with the naive decomposition where each slave processes a single higher order factor. His result demonstrates the effectiveness of the dual decomposition framework. Our work improves Komodakis’ by using a concise decomposition with only two slaves. 2.3 Graph Representable Pseudo-Boolean Optimization One slave in our approach is a graph representable pseudo-Boolean maximization problem, which can be reduced to a supermodular quadratic pseudo-Boolean maximization problem and solved efficiently using an algorithm for finding a minimal cut. A pseudo-Boolean function (PBF) (Boros and Hammer, 2002) is a multilinear function of binary variables, that is where xi E {0, 1}. Maximizing a PBF is usually NP-hard, such as the maximum cut problem (Boros and Hammer, 1991). A pseudo-Boolean function is said to be supermodular iff f(x) + f(y) ≤ f(x ∧ y) + f(x V y) where x n y, x V y are the element-wise AND and OR operator of the two vectors respectively. This is an important concept, because a supermodular pseudo-Boolean function (SPBF) can be maximized in O(n6) running time (Orlin, 2009). A necessary and sufficient condition for identifying a SPBF is ∑M i=1 Oi(Y ) max Y ∑M i=1 Oi(Zi) min max ∑M </context>
</contexts>
<marker>Boros, Hammer, 2002</marker>
<rawString>Endre Boros and Peter L. Hammer. 2002. Pseudoboolean optimization. Discrete Applied Mathematics, 123(1C3):155 – 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1737" citStr="Collins, 2002" startWordPosition="249" endWordPosition="250">RFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the su</context>
<context position="25590" citStr="Collins, 2002" startWordPosition="4275" endWordPosition="4276">ltiple slaves, each of which covers a pair of identical multi-word phrases. To reduce the number of slaves, we use a greedy algorithm to choose the spanning trees. Each time we select the spanning tree that covers the most uncovered edges. This can be done by performing the maximum spanning tree algorithm on the graph where each uncovered edge has unit weight. Let x* denote the most frequent word in a tweet cluster, and F* is its frequency, then at least (F*−1)/2 spanning trees are required to cover the complete subgraph spanned by x*. For both dual decomposition systems, averaged perceptron (Collins, 2002a) with 10 iterations is used for parameter estimation. We follow the work of Rush et al. (2010) to choose the step size in the sub-gradient algorithm. Table 2 shows the comparison results, including two F scores and total running time (seconds) for training and testing. Performances of the top 4 official runs are also listed. Different from our approach, the top performing systems mainly benefit from rich open resources, such as DBpedia Ewe use Gurobi as the ILP solver, http://www.gurobi.com/ System &apos;macro &apos;micro Sec. Linear chain CRFs 0.657 0.815 98 General CRFs (2-slave DD) 0.680 0.827 214 </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002a. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named entity extraction: Boosting and the votedperceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>489--496</pages>
<contexts>
<context position="1737" citStr="Collins, 2002" startWordPosition="249" endWordPosition="250">RFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the su</context>
<context position="25590" citStr="Collins, 2002" startWordPosition="4275" endWordPosition="4276">ltiple slaves, each of which covers a pair of identical multi-word phrases. To reduce the number of slaves, we use a greedy algorithm to choose the spanning trees. Each time we select the spanning tree that covers the most uncovered edges. This can be done by performing the maximum spanning tree algorithm on the graph where each uncovered edge has unit weight. Let x* denote the most frequent word in a tweet cluster, and F* is its frequency, then at least (F*−1)/2 spanning trees are required to cover the complete subgraph spanned by x*. For both dual decomposition systems, averaged perceptron (Collins, 2002a) with 10 iterations is used for parameter estimation. We follow the work of Rush et al. (2010) to choose the step size in the sub-gradient algorithm. Table 2 shows the comparison results, including two F scores and total running time (seconds) for training and testing. Performances of the top 4 official runs are also listed. Different from our approach, the top performing systems mainly benefit from rich open resources, such as DBpedia Ewe use Gurobi as the ILP solver, http://www.gurobi.com/ System &apos;macro &apos;micro Sec. Linear chain CRFs 0.657 0.815 98 General CRFs (2-slave DD) 0.680 0.827 214 </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002b. Ranking algorithms for named entity extraction: Boosting and the votedperceptron. In Proceedings of ACL, pages 489–496, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shilin Ding</author>
<author>Gao Cong</author>
<author>Chin-Yew Lin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using conditional random fields to extract contexts and answers of questions from online forums.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>710--718</pages>
<contexts>
<context position="1425" citStr="Ding et al., 2008" startWordPosition="204" endWordPosition="207">afferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it </context>
</contexts>
<marker>Ding, Cong, Lin, Zhu, 2008</marker>
<rawString>Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu. 2008. Using conditional random fields to extract contexts and answers of questions from online forums. In Proceedings ofACL-08: HLT, pages 710–718, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="1323" citStr="Finkel et al., 2005" startWordPosition="189" endWordPosition="192"> method outperforms spanning tree based dual decomposition. 1 Introduction Conditional Random Fields (Lafferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted m</context>
<context position="23330" citStr="Finkel et al., 2005" startWordPosition="3909" endWordPosition="3912">ring matching. Those matched words are assigned with BIESO style labels which are used as features. We also used the unsupervised word cluster features provided by Ark Twitter NLP toolkit, which has significantly improved the Twitter POS tagging accuracy (Owoputi et al., 2013). Similar with previous work, we used prefixes of the cluster bit strings with lengths ∈ {2, 4, ... ,16} as features. 4.1.3 Global Features Previous studies showed that the document level consistency features (same phrases in a document tend to have the same entity class) are effective for NER (Kazama and Torisawa, 2007; Finkel et al., 2005). However, unlike news articles, tweets are not organized in documents. To use these document level consistency features, we grouped the tweets in MSM2013 dataset using single linkage clustering algorithm where similarity between two tweets is the number of their overlapped words. If the similarity is greater than 4, then we put the two tweets into one group. Unlike standard document clustering, we did not normalize the length of tweets since all the tweets are limited to 140 characters. Then we 2www.cnts.ua.ac.be/conll2003/ 3https://code.google.com/p/ark-tweet-nlp/ 4http://github.com/aritter/</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL, pages 363–370, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Freedman</author>
<author>Petros Drineas</author>
</authors>
<title>Energy minimization via graph cuts: Settling what is possible.</title>
<date>2005</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>939--946</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="13758" citStr="Freedman and Drineas, 2005" startWordPosition="2242" endWordPosition="2245">g Lagrangian relaxation, we get the relaxed problem s.t. Y E Y Zv[s] E {0,1T, dv, s We split the inner mazY�Z into two subproblems, and a minimal λ is found using sub-gradient descent algorithms which repeatedly find a maximizing assignment for the subproblems individually. max E E 0v[s]Yv[s] Y v s (Wuv + 0uv[st])Yuv[st] s.t. Y E Y E stES2 E + u,v min max E E 0v[s]Yv[s] A Y,Z v s (Wuv + 0uv[st])Zuv[st] ) Av[s] (Zv[s] − Yv[s] +E E u,v stES2 E s +E v max Y 0uv[st]Yuv[st] 342 A sufficient condition for gλ(Z) to be graph representable is that coefficients of all non-linear terms are non-negative (Freedman and Drineas, 2005). Hence, we can set Let fλ(Y ) = E ϕv[s]Yv[s] − E λv[s]Yv[s] v,s v,s E E (ψuv + ϕuv[st])Zuv[st] ψc = − min {ϕc[s]} gλ(Z) = stES2 sEc[-] u,v +E λv[s]Zv[s] v,s The two subproblems are max fλ(Y ) Y s.t. Y ∈ Y and max gλ(Z) Z Zv[s] ∈ {0,1}, ∀v, s The first subproblem can be solved in linear time since all vertices are independent. The second problem is a binary quadratic programming problem. As discussed in Section 2.3, gλ(Z) can be solved using min-cut if the coefficients of the quadratic terms are non-negative, i.e. ψuv + ϕuv[st] ≥ 0, ∀u, v, s, t Hence, we can set ψuv = − min {ϕuv[st]} stEuv[-] </context>
<context position="33140" citStr="Freedman and Drineas, 2005" startWordPosition="5531" endWordPosition="5534"> graph can not be decomposed into a few spanning trees (for example, if the graph has large complete subgraphs or large factors). Our code is available at https://github.com/qxred/higher-order-crf Appendix A We show that by setting a sufficiently large ψ, g),(Z) in Section 3.2 is graph representable. Let 9A(Z) = 91(Z) + 92(Z) + 93(Z) + 94(Z) where E 91(Z) = c ici=2 E 92(Z) = c �c�&gt;3 E 93(Z) = c �c�=3 E 94(Z) = c �c1&gt;4 For g2(Z), since coefficients of all terms are nonnegative, we can use the fact ai = max�� ai − |a |+ 1 I b (7) bE{0,1} \ / I: to reduce g2(Z) into an equivalent quadratic form (Freedman and Drineas, 2005). That is, maxZ g2(Z) is equivalent to � � �E Oc[s] Zv[s] − |c |+ 1 uc[s] v[s]Ec[s] E E Av[s]Zv[s] sEc[-] (ψc + Oc[s] ) Zc[s] + v,s E Oc[s]Zc[s] sEc[·] ϕc[s]&gt;0 E Oc[s]Zc[s] sEc[·] ϕc[s]&lt;0 E Oc[s]Zc[s] sEc[·] ϕc[s]&lt;0 � i aiE10,1} Emax Z,u c,�c1&gt;3 ϕc[s]&gt;0 E sEc[-] 348 which is graph representable because coefficients of all the quadratic terms are non-negative. Coefficients of terms in g3(Z) and g4(Z) are negative, therefore g3(Z) and g4(Z) are not supermodular. To make them graph representable, we use the following fact Proposition 1 ( ˇZivn´y and Jeavons, 2010) The pseudo-Bool�eaan function p(</context>
</contexts>
<marker>Freedman, Drineas, 2005</marker>
<rawString>Daniel Freedman and Petros Drineas. 2005. Energy minimization via graph cuts: Settling what is possible. In Proceedings of CVPR, pages 939–946. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>364--372</pages>
<contexts>
<context position="1302" citStr="Galley, 2006" startWordPosition="187" endWordPosition="188"> show that our method outperforms spanning tree based dual decomposition. 1 Introduction Conditional Random Fields (Lafferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), </context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proceedings of EMNLP, pages 364–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT, HLT ’11,</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: annotation, features, and experiments. In Proceedings of ACL-HLT, HLT ’11, pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Goldberg</author>
</authors>
<title>The partial augmentcrelabel algorithm for the maximum flow problem.</title>
<date>2008</date>
<booktitle>In Dan Halperin and Kurt Mehlhorn, editors, Algorithms -ESA 2008,</booktitle>
<volume>5193</volume>
<pages>466--477</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="14507" citStr="Goldberg, 2008" startWordPosition="2380" endWordPosition="2381">] u,v +E λv[s]Zv[s] v,s The two subproblems are max fλ(Y ) Y s.t. Y ∈ Y and max gλ(Z) Z Zv[s] ∈ {0,1}, ∀v, s The first subproblem can be solved in linear time since all vertices are independent. The second problem is a binary quadratic programming problem. As discussed in Section 2.3, gλ(Z) can be solved using min-cut if the coefficients of the quadratic terms are non-negative, i.e. ψuv + ϕuv[st] ≥ 0, ∀u, v, s, t Hence, we can set ψuv = − min {ϕuv[st]} stEuv[-] to guarantee the non-negativity. This supermodular binary quadratic programming problem can be solved via the push-relabel algorithm (Goldberg, 2008) in O(|S|N)3 running time. Though Z may not satisfy the constraint Z E Y after sub-gradient descent based optimization, Y must satisfy Y E Y, hence we could use Y as the final solution if Z and Y disagree. 3.2 Generalized Higher Order CRFs Now we consider the general case, maximizing Eq (3). Similar with the pairwise case, we use two slaves. One is a set of independent vertices, and the other is a pseudo-Boolean optimization problem. That is, we can redefine gλ(Z) as gλ(Z) = E E (ψc + ϕc[s]) Zc[s] c sEc[-] +E λv[s]Zv[s] v,s to guarantee the non-negativity. In real applications, higher order pa</context>
</contexts>
<marker>Goldberg, 2008</marker>
<rawString>AndrewV. Goldberg. 2008. The partial augmentcrelabel algorithm for the maximum flow problem. In Dan Halperin and Kurt Mehlhorn, editors, Algorithms -ESA 2008, volume 5193 of Lecture Notes in Computer Science, pages 466–477. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Jojic</author>
<author>Stephen Gould</author>
<author>Daphne Koller</author>
</authors>
<title>Accelerated dual decomposition for map inference.</title>
<date>2010</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>503--510</pages>
<publisher>Omnipress.</publisher>
<contexts>
<context position="8655" citStr="Jojic et al. (2010)" startWordPosition="1369" endWordPosition="1372">ubtask. For example, Rush et al. (2010) used two slaves for integrated phrase-structure parsing and trigram POS tagging task. However, for generalized higher order CRFs, a lightweight decomposition may be not at hand. Martins et al. (2011a) showed that the sub-gradient algorithms exhibited extremely slow convergence when handling many slaves. For fast convergence, they employed alternating directions dual decomposition (AD3), which relaxes the agreement constraint via augmented Lagrangian Relaxation, where an additional quadratic penalty term was added into the Lagrangian (Eq (4)). Similarly, Jojic et al. (2010) added a strongly concave term to the Lagrangian to make it differentiable, resulting in fast convergence. The work most closely related to ours is the work by Komodakis (2011), where dual decomposition was used for decoding general higher order CRFs. Komodakis achieved great empirical success even with the naive decomposition where each slave processes a single higher order factor. His result demonstrates the effectiveness of the dual decomposition framework. Our work improves Komodakis’ by using a concise decomposition with only two slaves. 2.3 Graph Representable Pseudo-Boolean Optimization</context>
</contexts>
<marker>Jojic, Gould, Koller, 2010</marker>
<rawString>Vladimir Jojic, Stephen Gould, and Daphne Koller. 2010. Accelerated dual decomposition for map inference. In Proceedings of ICML, pages 503–510. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>A new perceptron algorithm for sequence labeling with nonlocal features.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>315--324</pages>
<contexts>
<context position="1351" citStr="Kazama and Torisawa, 2007" startWordPosition="193" endWordPosition="196">panning tree based dual decomposition. 1 Introduction Conditional Random Fields (Lafferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due t</context>
<context position="23308" citStr="Kazama and Torisawa, 2007" startWordPosition="3905" endWordPosition="3908">d testing datasets using string matching. Those matched words are assigned with BIESO style labels which are used as features. We also used the unsupervised word cluster features provided by Ark Twitter NLP toolkit, which has significantly improved the Twitter POS tagging accuracy (Owoputi et al., 2013). Similar with previous work, we used prefixes of the cluster bit strings with lengths ∈ {2, 4, ... ,16} as features. 4.1.3 Global Features Previous studies showed that the document level consistency features (same phrases in a document tend to have the same entity class) are effective for NER (Kazama and Torisawa, 2007; Finkel et al., 2005). However, unlike news articles, tweets are not organized in documents. To use these document level consistency features, we grouped the tweets in MSM2013 dataset using single linkage clustering algorithm where similarity between two tweets is the number of their overlapped words. If the similarity is greater than 4, then we put the two tweets into one group. Unlike standard document clustering, we did not normalize the length of tweets since all the tweets are limited to 140 characters. Then we 2www.cnts.ua.ac.be/conll2003/ 3https://code.google.com/p/ark-tweet-nlp/ 4http</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. A new perceptron algorithm for sequence labeling with nonlocal features. In Proceedings of EMNLP-CoNLL, pages 315–324, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kolmogorov</author>
<author>Ramin Zabih</author>
</authors>
<title>What energy functions can be minimized via graph cuts?</title>
<date>2004</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>26</volume>
<issue>2</issue>
<pages>159</pages>
<contexts>
<context position="1632" citStr="Kolmogorov and Zabih, 2004" startWordPosition="232" endWordPosition="235"> tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves</context>
<context position="10908" citStr="Kolmogorov and Zabih, 2004" startWordPosition="1739" endWordPosition="1742">ives are nonnegative (Nemhauser et al., 1978), i.e., for all i &lt; j, and Jeavons (2010), and finally solve the slave problem via graph cuts. ∂f &gt; 0 ∂xi∂xj For example a quadratic PBF is supermodular if its coefficients of all quadratic terms are non-negative. Though the general supermodular maximization algorithm can be used for any SPBF, the special features of some specific problems allow more efficient algorithms to be used. For example, it is well known that the supermodular quadratic pseudoBoolean maximization problem can be solved in cubic time using min-cut (Billionnet and Minoux, 1985; Kolmogorov and Zabih, 2004). In fact, a subset of SPBFs can be maximized using a min-cut algorithm. A pseudo-Boolean function f(x) is called graph representable or graph expressible if there exists a graph G = (V, E) with terminals s and t and a subset of vertices V0 = V − {s, t} = {v1, ... , vry,,, u1,... , um} such that, for any configuration x1, ... , x, the value of the function f(x) is equal to a constant plus the cost of the minimum s-t cut among all cuts, in which vi is connected with s if xi = 0 and connected with t if xi = 1. Our definition extends the work of Kolmogorov and Zabih (2004) that focused on quadrat</context>
</contexts>
<marker>Kolmogorov, Zabih, 2004</marker>
<rawString>Vladimir Kolmogorov and Ramin Zabih. 2004. What energy functions can be minimized via graph cuts? IEEE Trans. Pattern Anal. Mach. Intell., 26(2):147– 159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kolmogorov</author>
</authors>
<title>Convergent treereweighted message passing for energy minimization.</title>
<date>2006</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>28</volume>
<issue>10</issue>
<pages>1583</pages>
<contexts>
<context position="1848" citStr="Kolmogorov, 2006" startWordPosition="262" endWordPosition="263"> patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the sub-gradient algorithm is strongly tied to the ability of finding a good decomposition, i.e., one involving few o</context>
</contexts>
<marker>Kolmogorov, 2006</marker>
<rawString>Vladimir Kolmogorov. 2006. Convergent treereweighted message passing for energy minimization. IEEE Trans. Pattern Anal. Mach. Intell., 28(10):1568– 1583, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Komodakis</author>
</authors>
<title>Efficient training for pairwise or higher order CRFs via dual decomposition.</title>
<date>2011</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>1841--1848</pages>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="8831" citStr="Komodakis (2011)" startWordPosition="1400" endWordPosition="1401">ight decomposition may be not at hand. Martins et al. (2011a) showed that the sub-gradient algorithms exhibited extremely slow convergence when handling many slaves. For fast convergence, they employed alternating directions dual decomposition (AD3), which relaxes the agreement constraint via augmented Lagrangian Relaxation, where an additional quadratic penalty term was added into the Lagrangian (Eq (4)). Similarly, Jojic et al. (2010) added a strongly concave term to the Lagrangian to make it differentiable, resulting in fast convergence. The work most closely related to ours is the work by Komodakis (2011), where dual decomposition was used for decoding general higher order CRFs. Komodakis achieved great empirical success even with the naive decomposition where each slave processes a single higher order factor. His result demonstrates the effectiveness of the dual decomposition framework. Our work improves Komodakis’ by using a concise decomposition with only two slaves. 2.3 Graph Representable Pseudo-Boolean Optimization One slave in our approach is a graph representable pseudo-Boolean maximization problem, which can be reduced to a supermodular quadratic pseudo-Boolean maximization problem an</context>
</contexts>
<marker>Komodakis, 2011</marker>
<rawString>Nikos Komodakis. 2011. Efficient training for pairwise or higher order CRFs via dual decomposition. In Proceedings of CVPR, pages 1841–1848. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1288--1298</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="7622" citStr="Koo et al. (2010)" startWordPosition="1221" endWordPosition="1224">d slaves. Rather than solving the problem directly, dual decomposition considers the equivalent problem max Y,Z1...ZM s.t. Zi = Y di Using Lagrangian relaxation to eliminate the constraint, we get λT (Y − Zi) (4) which provides the upper bound of the original problem. A is the Lagrange multiplier, which is typically optimized via sub-gradient algorithms. Martins et al. (2011b) showed that the success of sub-gradient algorithms is strongly tied to the ability of finding a good decomposition, i.e., one involving few slaves. Finding a concise decomposition is usually task dependent. For example, Koo et al. (2010) introduced dual decomposition for parsing with non-projective head automata. They used only two slaves: one is the arc-factored model, and the other is head automata which involves adjacent siblings and can be solved using dynamic programming in linear time. Dual decomposition is especially efficient for joint learning tasks because a concise decomposition can be derived naturally where each slave solves one subtask. For example, Rush et al. (2010) used two slaves for integrated phrase-structure parsing and trigram POS tagging task. However, for generalized higher order CRFs, a lightweight de</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of EMNLP, pages 1288– 1298, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Kr¨ahenb¨uhl</author>
<author>Vladlen Koltun</author>
</authors>
<title>Efficient inference in fully connected crfs with gaussian edge potentials.</title>
<date>2011</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>109--117</pages>
<marker>Kr¨ahenb¨uhl, Koltun, 2011</marker>
<rawString>Philipp Kr¨ahenb¨uhl and Vladlen Koltun. 2011. Efficient inference in fully connected crfs with gaussian edge potentials. In Proceedings of NIPS, pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="828" citStr="Lafferty et al., 2001" startWordPosition="116" endWordPosition="119">ng problem in generalized Higher Order Conditional Random Fields (CRFs) can be decomposed into two parts: one is a tree labeling problem that can be solved in linear time using dynamic programming; the other is a supermodular quadratic pseudo-Boolean maximization problem, which can be solved in cubic time using a minimum cut algorithm. We use dual decomposition to force their agreement. Experimental results on Twitter named entity recognition and sentence dependency tagging tasks show that our method outperforms spanning tree based dual decomposition. 1 Introduction Conditional Random Fields (Lafferty et al., 2001) (CRFs) are popular models for many NLP tasks. In particular, the linear chain CRFs explore local structure information for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). T</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Mario Figueiredo</author>
<author>Pedro Aguiar</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>An augmented lagrangian approach to constrained map inference.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>169--176</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="2301" citStr="Martins et al. (2011" startWordPosition="329" endWordPosition="332">sed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the sub-gradient algorithm is strongly tied to the ability of finding a good decomposition, i.e., one involving few overlapping slaves. However, for generalized higher order graphical models, a lightweight decomposition is not at hand and many overlapping slaves may be involved. Martins et al. (2011b) showed that the sub-gradient algorithm exhibits extremely slow convergence in such cases, and they proposed the alternating directions method (DD-ADMM) to tackle these. In this paper, we propose a 2-slave dual decomposition approach for efficient decoding in higher o</context>
<context position="7382" citStr="Martins et al. (2011" startWordPosition="1184" endWordPosition="1187">ng and parsing. Briefly, dual decomposition attempts to solve problems of the following form The objective function is the sum of several small components that are tractable in isolation but whose combination is not. These components are called slaves. Rather than solving the problem directly, dual decomposition considers the equivalent problem max Y,Z1...ZM s.t. Zi = Y di Using Lagrangian relaxation to eliminate the constraint, we get λT (Y − Zi) (4) which provides the upper bound of the original problem. A is the Lagrange multiplier, which is typically optimized via sub-gradient algorithms. Martins et al. (2011b) showed that the success of sub-gradient algorithms is strongly tied to the ability of finding a good decomposition, i.e., one involving few slaves. Finding a concise decomposition is usually task dependent. For example, Koo et al. (2010) introduced dual decomposition for parsing with non-projective head automata. They used only two slaves: one is the arc-factored model, and the other is head automata which involves adjacent siblings and can be solved using dynamic programming in linear time. Dual decomposition is especially efficient for joint learning tasks because a concise decomposition </context>
</contexts>
<marker>Martins, Figueiredo, Aguiar, Smith, Xing, 2011</marker>
<rawString>Andre Martins, Mario Figueiredo, Pedro Aguiar, Noah Smith, and Eric Xing. 2011a. An augmented lagrangian approach to constrained map inference. In Proceedings of ICML, pages 169–176. ACM, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Mario Figueiredo</author>
<author>Pedro Aguiar</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>238--249</pages>
<contexts>
<context position="2301" citStr="Martins et al. (2011" startWordPosition="329" endWordPosition="332">sed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the sub-gradient algorithm is strongly tied to the ability of finding a good decomposition, i.e., one involving few overlapping slaves. However, for generalized higher order graphical models, a lightweight decomposition is not at hand and many overlapping slaves may be involved. Martins et al. (2011b) showed that the sub-gradient algorithm exhibits extremely slow convergence in such cases, and they proposed the alternating directions method (DD-ADMM) to tackle these. In this paper, we propose a 2-slave dual decomposition approach for efficient decoding in higher o</context>
<context position="7382" citStr="Martins et al. (2011" startWordPosition="1184" endWordPosition="1187">ng and parsing. Briefly, dual decomposition attempts to solve problems of the following form The objective function is the sum of several small components that are tractable in isolation but whose combination is not. These components are called slaves. Rather than solving the problem directly, dual decomposition considers the equivalent problem max Y,Z1...ZM s.t. Zi = Y di Using Lagrangian relaxation to eliminate the constraint, we get λT (Y − Zi) (4) which provides the upper bound of the original problem. A is the Lagrange multiplier, which is typically optimized via sub-gradient algorithms. Martins et al. (2011b) showed that the success of sub-gradient algorithms is strongly tied to the ability of finding a good decomposition, i.e., one involving few slaves. Finding a concise decomposition is usually task dependent. For example, Koo et al. (2010) introduced dual decomposition for parsing with non-projective head automata. They used only two slaves: one is the arc-factored model, and the other is head automata which involves adjacent siblings and can be solved using dynamic programming in linear time. Dual decomposition is especially efficient for joint learning tasks because a concise decomposition </context>
</contexts>
<marker>Martins, Smith, Figueiredo, Aguiar, 2011</marker>
<rawString>Andre Martins, Noah Smith, Mario Figueiredo, and Pedro Aguiar. 2011b. Dual decomposition with many overlapping components. In Proceedings of the EMNLP, pages 238–249, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Nemhauser</author>
<author>L A Wolsey</author>
<author>M L Fisher</author>
</authors>
<title>An analysis of approximations for maximizing submodular set functions-I.</title>
<date>1978</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="10326" citStr="Nemhauser et al., 1978" startWordPosition="1645" endWordPosition="1648">s and Hammer, 1991). A pseudo-Boolean function is said to be supermodular iff f(x) + f(y) ≤ f(x ∧ y) + f(x V y) where x n y, x V y are the element-wise AND and OR operator of the two vectors respectively. This is an important concept, because a supermodular pseudo-Boolean function (SPBF) can be maximized in O(n6) running time (Orlin, 2009). A necessary and sufficient condition for identifying a SPBF is ∑M i=1 Oi(Y ) max Y ∑M i=1 Oi(Zi) min max ∑M ∑ A Y,Z1...ZM i=1 Oi(Zi) + i f(x) = ∑ ∑aixi + ∑aijxixj + aijkxixjxk + .. . i i&lt;j i&lt;j&lt;k 341 that all of its second order derivatives are nonnegative (Nemhauser et al., 1978), i.e., for all i &lt; j, and Jeavons (2010), and finally solve the slave problem via graph cuts. ∂f &gt; 0 ∂xi∂xj For example a quadratic PBF is supermodular if its coefficients of all quadratic terms are non-negative. Though the general supermodular maximization algorithm can be used for any SPBF, the special features of some specific problems allow more efficient algorithms to be used. For example, it is well known that the supermodular quadratic pseudoBoolean maximization problem can be solved in cubic time using min-cut (Billionnet and Minoux, 1985; Kolmogorov and Zabih, 2004). In fact, a subse</context>
</contexts>
<marker>Nemhauser, Wolsey, Fisher, 1978</marker>
<rawString>G.L. Nemhauser, L.A. Wolsey, and M.L. Fisher. 1978. An analysis of approximations for maximizing submodular set functions-I. Mathematical Programming, 14(1):265–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James B Orlin</author>
</authors>
<title>A faster strongly polynomial time algorithm for submodular function minimization.</title>
<date>2009</date>
<journal>Math. Program.,</journal>
<volume>118</volume>
<issue>2</issue>
<contexts>
<context position="10044" citStr="Orlin, 2009" startWordPosition="1589" endWordPosition="1590"> solved efficiently using an algorithm for finding a minimal cut. A pseudo-Boolean function (PBF) (Boros and Hammer, 2002) is a multilinear function of binary variables, that is where xi E {0, 1}. Maximizing a PBF is usually NP-hard, such as the maximum cut problem (Boros and Hammer, 1991). A pseudo-Boolean function is said to be supermodular iff f(x) + f(y) ≤ f(x ∧ y) + f(x V y) where x n y, x V y are the element-wise AND and OR operator of the two vectors respectively. This is an important concept, because a supermodular pseudo-Boolean function (SPBF) can be maximized in O(n6) running time (Orlin, 2009). A necessary and sufficient condition for identifying a SPBF is ∑M i=1 Oi(Y ) max Y ∑M i=1 Oi(Zi) min max ∑M ∑ A Y,Z1...ZM i=1 Oi(Zi) + i f(x) = ∑ ∑aixi + ∑aijxixj + aijkxixjxk + .. . i i&lt;j i&lt;j&lt;k 341 that all of its second order derivatives are nonnegative (Nemhauser et al., 1978), i.e., for all i &lt; j, and Jeavons (2010), and finally solve the slave problem via graph cuts. ∂f &gt; 0 ∂xi∂xj For example a quadratic PBF is supermodular if its coefficients of all quadratic terms are non-negative. Though the general supermodular maximization algorithm can be used for any SPBF, the special features of</context>
</contexts>
<marker>Orlin, 2009</marker>
<rawString>James B. Orlin. 2009. A faster strongly polynomial time algorithm for submodular function minimization. Math. Program., 118(2):237–251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Xiaoqian Jiang</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Sparse higher order conditional random fields for improved sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of ICML,</booktitle>
<volume>382</volume>
<pages>107</pages>
<publisher>ACM.</publisher>
<marker>Qian, Jiang, Zhang, Huang, Wu, 2009</marker>
<rawString>Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Sparse higher order conditional random fields for improved sequence labeling. In Proceedings of ICML, volume 382, page 107. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhonghua Qu</author>
<author>Yang Liu</author>
</authors>
<title>Sentence dependency tagging in online question answering forums.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>554--562</pages>
<contexts>
<context position="27599" citStr="Qu and Liu, 2012" startWordPosition="4594" endWordPosition="4597">bjectives per instance (the lower the tighter), decoding time, and fraction of optimality certificates across iterations of the two DD algorithms on test data. Figure 2 shows the performances of the two algorithms relative to decoding time. Our method requires 0.0064 seconds for each iteration on average, about four times slower than the naive DD. However, our approach achieves a tighter upper bound and larger fraction of optimality certificates. 4.2 Sentence Dependency Tagging Our second experiment is sentence dependency tagging in Question Answering forums task studied in Qu and Liu’s work (Qu and Liu, 2012). The goal is to extract the dependency relationships between sentences for automatic question answering. For example, from the posts below, we would need to know that sentence S4 is a comment about sentence S1 and S2, not an answer to S3. 346 Dual Objective 50 100 150 200 Decoding Time (sec.) Decoding Time (sec.) %certificates of naive DD %certificates of 2−slave DD F score of naive DD F score of 2−slave DD 7300 7200 7100 7000 6900 100 150 200 6800 0 50 naive DD 2−slave DD Percentage 1 0.9 0.8 0.7 0.6 0.5 Figure 2: Twitter NER: The F..i,ro scores, dual objectives, and fraction of optimality c</context>
<context position="28839" citStr="Qu and Liu (2012)" startWordPosition="4802" endWordPosition="4805"> decoding time. source Figure 3: 3-wise CRF for QA sentence dependency tagging. Order-3 factors (e.g., red and blue) connects the 3 vertices in adjacent edge pairs. A: [S1]I’m having trouble installing my DVB Card. [S2]dmesg prints:.. . [S3]What could I do to resolve this problem? B: [S4] I’m having similar problems with Ubuntu For a pair of sentences, the depending sentence is called the source sentence, and the depended sentence the target sentence. One source sentence can potentially depend on many different target sentences, and one target sentence can also correspond to multiple sources. Qu and Liu (2012) casted the task as a binary classification problem, i.e., whether or not there exists a dependency relation between a pair of sentences. Formally, in this task, Y is a N2 × 2 matrix, where N is the number of sentences, Yi*N+j[1] = 1 if the ith sentence depends on the jth sentence, otherwise, Yi*N+j[0] = 1. We use the corpus in Qu and Liu’s work (Qu and Liu, 2012), where dependencies between 3,483 sentences in 200 threads were System F Sec. 2D CRFs (naive) 0.564 9.2 2D CRFs (2-slave) 0.565 16.4 3-wise CRFs (naive) 0.572 18.7 3-wise CRFs (2-slave) 0.584 17.33 (Qu and Liu, 2012) 0.561 N/A Table </context>
</contexts>
<marker>Qu, Liu, 2012</marker>
<rawString>Zhonghua Qu and Yang Liu. 2012. Sentence dependency tagging in online question answering forums. In Proceedings of ACL, pages 554–562, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M W Rhys</author>
</authors>
<title>A selection problem of shared fixed costs and network flows.</title>
<date>1970</date>
<journal>Management Science,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="11923" citStr="Rhys, 1970" startWordPosition="1924" endWordPosition="1925">the minimum s-t cut among all cuts, in which vi is connected with s if xi = 0 and connected with t if xi = 1. Our definition extends the work of Kolmogorov and Zabih (2004) that focused on quadratic and cubic functions. Vertices u1, ... um correspond to the extra binary variables that are introduced to reduce the graph representable PBFs to equivalent quadratic forms. For example, the positive-negative PBFs where all terms of degree 2 or more have positive coefficients are graph representable, and each non-linear term requires one extra binary variable to obtain the equivalent quadratic form (Rhys, 1970). 3 The Tree-Cut Decomposition for Generalized Higher Order CRFs We decompose the decoding problem, i.e., maximization of Eq (3), into two parts, a tree labeling problem and a PBF maximization problem. We show that the PBF can be graph representable by reparameterizing the scoring function in Eq (3). Then we reduce these pseudo-Boolean functions to quadratic forms based on the recent work of ˇZivn´y 3.1 Fully Connected Pairwise CRFs We first describe our idea for a simple case, the fully connected pairwise CRFs (Kr¨ahenb¨uhl and Koltun, 2011), which are generalizations of linearchain CRFs and </context>
</contexts>
<marker>Rhys, 1970</marker>
<rawString>J. M. W. Rhys. 1970. A selection problem of shared fixed costs and network flows. Management Science, 17(3):200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1524--1534</pages>
<contexts>
<context position="19990" citStr="Ritter et al., 2011" startWordPosition="3378" endWordPosition="3381">of patterns. ∑ sEc[-] ∑ + c̸∈T |c|=2   ϕc′[s′]   Zc[s]   ∑ ψc + ϕc[s] + c′[s′]⊇c[s] |c′|≥3,ϕc′[s′]&lt;0 ∑ s∈c[·] ϕc[s]≥0 ∑ h2(Z, u) = c |c|≥3 ϕc[s] (Zc[s] − |c |+ 1) uc[s] ∑ s∈c[·] ϕc[s]&lt;0 ∑ h3(Z, u) = c |c|=3 ��ϕc[s] I uc[s] (Zc[s] − 1) ∑ s∈c[·] ϕc[s]&lt;0 ∑ h4(Z, u) = c |c|≥4 (|ϕc[s] |u0 c[s](2Zc[s] − 3) |c|-4 ∑ j=1 +  ujc[s](Zc[s] − j − 2)  . O (  3    344 4 Experimental Results 4.1 Named Entity Recognition in Tweets 4.1.1 Data Sets Our first experiment is named entity recognition in tweets. Recently, information extraction on Twitter or Facebook data is attracting much attention (Ritter et al., 2011). Different from traditional information extraction for news articles, messages posted on these social media websites are short and noisy, making the task more challenging. In this paper, we use generalized higher order CRFs for Twitter NER with discriminative training, and compare our 2-slave dual decomposition approach with spanning tree based dual decomposition approach and other decoding algorithms. So far as we know, there are two publicly available data sets for Twitter NER. One is the Ritter’s (Ritter et al., 2011), the other is from MSM2013 Concept Extraction Challenge (Basave et al., </context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of EMNLP, pages 1524–1534, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="2010" citStr="Rush and Collins, 2012" startWordPosition="285" endWordPosition="288">ighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the sub-gradient algorithm is strongly tied to the ability of finding a good decomposition, i.e., one involving few overlapping slaves. However, for generalized higher order graphical models, a lightweight decomposition is not at hand and many overlapping slaves may be involved.</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M. Rush and Michael Collins. 2012. A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing. J. Artif. Int. Res., 45(1):305–362, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<pages>1--11</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="8075" citStr="Rush et al. (2010)" startWordPosition="1289" endWordPosition="1292">he ability of finding a good decomposition, i.e., one involving few slaves. Finding a concise decomposition is usually task dependent. For example, Koo et al. (2010) introduced dual decomposition for parsing with non-projective head automata. They used only two slaves: one is the arc-factored model, and the other is head automata which involves adjacent siblings and can be solved using dynamic programming in linear time. Dual decomposition is especially efficient for joint learning tasks because a concise decomposition can be derived naturally where each slave solves one subtask. For example, Rush et al. (2010) used two slaves for integrated phrase-structure parsing and trigram POS tagging task. However, for generalized higher order CRFs, a lightweight decomposition may be not at hand. Martins et al. (2011a) showed that the sub-gradient algorithms exhibited extremely slow convergence when handling many slaves. For fast convergence, they employed alternating directions dual decomposition (AD3), which relaxes the agreement constraint via augmented Lagrangian Relaxation, where an additional quadratic penalty term was added into the Lagrangian (Eq (4)). Similarly, Jojic et al. (2010) added a strongly co</context>
<context position="25686" citStr="Rush et al. (2010)" startWordPosition="4290" endWordPosition="4293">umber of slaves, we use a greedy algorithm to choose the spanning trees. Each time we select the spanning tree that covers the most uncovered edges. This can be done by performing the maximum spanning tree algorithm on the graph where each uncovered edge has unit weight. Let x* denote the most frequent word in a tweet cluster, and F* is its frequency, then at least (F*−1)/2 spanning trees are required to cover the complete subgraph spanned by x*. For both dual decomposition systems, averaged perceptron (Collins, 2002a) with 10 iterations is used for parameter estimation. We follow the work of Rush et al. (2010) to choose the step size in the sub-gradient algorithm. Table 2 shows the comparison results, including two F scores and total running time (seconds) for training and testing. Performances of the top 4 official runs are also listed. Different from our approach, the top performing systems mainly benefit from rich open resources, such as DBpedia Ewe use Gurobi as the ILP solver, http://www.gurobi.com/ System &apos;macro &apos;micro Sec. Linear chain CRFs 0.657 0.815 98 General CRFs (2-slave DD) 0.680 0.827 214 General CRFs (naive DD) 0.672 0.824 490 General CRFs (ILP) 0.680 0.828 8640 Official 1st 0.670 N</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of EMNLP 2010, pages 1–11, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="1572" citStr="Sarawagi and Cohen, 2004" startWordPosition="224" endWordPosition="227">for sequence labeling tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Fin</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W. Cohen. 2004. Semimarkov conditional random fields for information extraction. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew Mccallum</author>
</authors>
<title>Introduction to Conditional Random Fields for Relational Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1793" citStr="Sutton and Mccallum, 2006" startWordPosition="254" endWordPosition="257">ality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agreement among different slaves. A popular choice is the subgradient algorithm. Martins et al. (2011b) showed that the success of the sub-gradient algorithm is strongly tied to the ability of </context>
<context position="16645" citStr="Sutton and Mccallum (2006)" startWordPosition="2731" endWordPosition="2734">quadratic terms. Let (ψc + ϕc[s]) Zc[s] E ϕc[s]Zc[s] + E |c|≥3 c sEc[-] The optimal solution is unchanged for any ψ. In Appendix A, we show that by setting a sufficiently large ψ, gλ(Z) is graph representable. Such reparameterization method requires at most N2|S|2 new patterns ψc,|c|=2 to make gλ(Z) graph representable. It preserves the sparsity of higher order patterns, hence is more efficient than the naive approach. 3.3 Tree-Cut Decomposition In some cases, the graph is built by adding sparse global patterns to local models like trees, resulting in nearly tree-structured CRFs. For example, Sutton and Mccallum (2006) used skip-chain CRFs for NER, where skip-edges connecting identical words E gλ(Z) = c |c|=2 E sEc[-] E + λv[s]Zv[s] v,s 343 were added to linear chain CRFs. Since the skipedges are sparse, the resulting graphical models are nearly linear chains. To handle the edges in local models efficiently, we reformulate the decomposition. Let T be a spanning tree of the graph, if edge (u, v) E T, we put its related patterns into the first slave, otherwise we put its related patterns into the second slave. For clarity, we formulate the tree-cut decomposition for generalized higher order CRFs. The first sl</context>
</contexts>
<marker>Sutton, Mccallum, 2006</marker>
<rawString>Charles Sutton and Andrew Mccallum, 2006. Introduction to Conditional Random Fields for Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rustem Takhanov</author>
<author>Vladimir Kolmogorov</author>
</authors>
<title>Inference algorithms for pattern-based CRFs on sequence data.</title>
<date>2013</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>145--153</pages>
<contexts>
<context position="1603" citStr="Takhanov and Kolmogorov, 2013" startWordPosition="228" endWordPosition="231">s, such as part-of-speech (POS) tagging, named entity recognition (NER), and shallow parsing. Recent studies have shown that the predictive power of CRFs can be strengthened by breaking the locality assumption. They either add long distance dependencies and patterns to linear chains for improved sequence labeling (Galley, 2006; Finkel et al., 2005; Kazama and Torisawa, 2007), or directly use the 4-connected neighborhood lattice (Ding et al., 2008). The resulting non-local models generally suffer from exponential time complexity of inference except some special cases (Sarawagi and Cohen, 2004; Takhanov and Kolmogorov, 2013; Kolmogorov and Zabih, 2004). Approximate decoding algorithms have been proposed in the past decade, such as reranking (Collins, 2002b), loopy belief propagation (Sutton and Mccallum, 2006), tree reweighted belief propagation (Kolmogorov, 2006). In this paper, we focus on dual decomposition (DD), which has attracted much attention recently due to its simplicity and effectiveness (Rush and Collins, 2012). In short, it decomposes the decoding problem into several sub-problems. For each sub-problem, an efficient decoding algorithm is deployed as a slave solver. Finally a simple method forces agr</context>
<context position="5978" citStr="Takhanov and Kolmogorov, 2013" startWordPosition="968" endWordPosition="971">(Y ) = s∈c[·] ϕc[s] + st∈S2 c v[s]∈c[s] v where c is a subset of vertices, called a factor. c[·] is the set of all possible assignments of c. For example, factor c = {u, v} denotes the edge (u, v) in the graph, and c[·] = S2 is the set of the |S|2 transitions. A factor c with a specific state assignment s is called a pattern, denoted as c[s]. For example, v[s] is a pattern of vertex v, and uv[st] is a pattern of edge (u, v) as shown in Figure 1. Note that our definition extends of the work of Takhanov and Kolmogorov where patterns are restricted to the state sequences of consecutive vertices (Takhanov and Kolmogorov, 2013). HV[s]EC[1] Yv[s] means a pattern c[s] is selected in the assignment only if all + ∑ ∑ ϕuv[ss]Yuv[ss]. u,v are similar s∈S With positive ϕuv[ss], the model encourages similar vertices u and v to have identical state s, and thus it yields a more consistent labeling result compared with linear chain CRFs. Empirically, the use of complex patterns achieves better performance but suffers from high computational complexity of inference, which is generally NP-hard. Hence an efficient approximate inference algorithm is required to balance the trade-off. 340 2.2 Dual Decomposition Dual decomposition i</context>
</contexts>
<marker>Takhanov, Kolmogorov, 2013</marker>
<rawString>Rustem Takhanov and Vladimir Kolmogorov. 2013. Inference algorithms for pattern-based CRFs on sequence data. In Proceedings of ICML, pages 145– 153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanislav ˇZivn´y</author>
<author>Peter G Jeavons</author>
</authors>
<title>Classes of submodular constraints expressible by graph cuts.</title>
<date>2010</date>
<journal>Constraints,</journal>
<volume>15</volume>
<issue>3</issue>
<marker>ˇZivn´y, Jeavons, 2010</marker>
<rawString>Stanislav ˇZivn´y and Peter G. Jeavons. 2010. Classes of submodular constraints expressible by graph cuts. Constraints, 15(3):430–452, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Ye</author>
<author>Wee Sun Lee</author>
<author>Hai Leong Chieu</author>
<author>Dan Wu</author>
</authors>
<title>Conditional random fields with highorder features for sequence labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2196--2204</pages>
<contexts>
<context position="15203" citStr="Ye et al., 2009" startWordPosition="2503" endWordPosition="2506">ub-gradient descent based optimization, Y must satisfy Y E Y, hence we could use Y as the final solution if Z and Y disagree. 3.2 Generalized Higher Order CRFs Now we consider the general case, maximizing Eq (3). Similar with the pairwise case, we use two slaves. One is a set of independent vertices, and the other is a pseudo-Boolean optimization problem. That is, we can redefine gλ(Z) as gλ(Z) = E E (ψc + ϕc[s]) Zc[s] c sEc[-] +E λv[s]Zv[s] v,s to guarantee the non-negativity. In real applications, higher order patterns are �� « |S||c |(Qian et sparse, i.e., ��ts E c[�� |ϕc[s]�=0} al., 2009; Ye et al., 2009). Hence we could skip the patterns with zero weights (ϕc[s] = 0) when calculating EsEc[·] ϕc[s]Zc[s] for fast inference. However, the reparameterization described above may introduce many non-zero terms which destroy the sparsity. For example, in the NER task, a binary feature is defined as true if a word subsequence matches a location name in a gazetteer. Suppose c =Little York village is such a word subsequence, then among |S|3 possible assignments of c, only the one that labels c =Little York village as a location name has non-zero weight. However, the reparameterization may add ψc to the o</context>
</contexts>
<marker>Ye, Lee, Chieu, Wu, 2009</marker>
<rawString>Nan Ye, Wee Sun Lee, Hai Leong Chieu, and Dan Wu. 2009. Conditional random fields with highorder features for sequence labeling. In Proceedings of NIPS, pages 2196–2204.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
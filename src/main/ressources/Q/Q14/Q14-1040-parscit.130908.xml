<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.992426">
Predicting the Difficulty of Language Proficiency Tests
</title>
<author confidence="0.970064">
Lisa Beinborn*t, Torsten Zesch§ Iryna Gurevych*t
</author>
<affiliation confidence="0.597327666666667">
o UKP Lab, Technische Universit¨at Darmstadt
‡ UKP Lab, German Institute for Educational Research
§ Language Technology Lab, University of Duisburg-Essen
</affiliation>
<email confidence="0.645496">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.979975" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994286">
Language proficiency tests are used to evaluate and
compare the progress of language learners. We
present an approach for automatic difficulty predic-
tion of C-tests that performs on par with human ex-
perts. On the basis of detailed analysis of newly
collected data, we develop a model for C-test dif-
ficulty introducing four dimensions: solution dif-
ficulty, candidate ambiguity, inter-gap dependency,
and paragraph difficulty. We show that cues from all
four dimensions contribute to C-test difficulty.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989425531915">
In a labor market that is increasingly globalized, knowl-
edge of at least one foreign language is more relevant than
ever before. Due to increased mobility, multilingual skills
are also required for private communication as friend-
ships stretch across geographical and linguistic borders.
In order to provide adequate language learning support,
it is important to frequently evaluate learner progress on
the basis of language proficiency tests that enable a fair
comparison between learners.
The test difficulty needs to match the intended target
group as the test should be challenging for the learner
but not lead to frustration. According to Vygotsky’s zone
of proximal development (Vygotsky, 1978), the range
of suitable material is very small. Thus, creating a test
that fits this narrow target zone is a tedious and time-
consuming task. Teachers predict the difficulty of a test
based on their teaching experience. However, as they al-
ready know the solutions, they cannot always anticipate
the confusion a test might cause for learners. This results
in a subjective difficulty estimation that often lacks the
consistency required for comparing learners over differ-
ent tests.
The underlying principle of most language proficiency
tests is the concept of reduced redundancy testing (Spol-
sky, 1969). It is based on the idea that “natural language
is redundant” and that more advanced learners can be dis-
tinguished from beginners by their ability to deal with re-
duced redundancy. For language testing, redundancy can
be reduced by eliminating words from a text and asking
the learner to fill in the gap, also known as the cloze test.
The C-test is a variant of the cloze test which contains
more gaps but provides part of the solution as a hint and
has been found to be a good estimate for language profi-
ciency (Eckes and Grotjahn, 2006).
We present an approach for determining the difficulty
of C-tests that overcomes the mentioned drawbacks of
subjective evaluation by teachers. Our approach is based
on objective measurable properties and thus produces
consistent results. We show that our approach performs
on par with human experts and analyze to which extent C-
test difficulty is determined by individual gap properties
(micro-level processing) and higher level dependencies
(macro-level processing). On the theoretical level, our
model provides new insights into the factors that affect
difficulty in reduced redundancy testing. On the practical
level, our results may help teachers to precisely evaluate
the difficulty of a test and to foresee challenging parts.
</bodyText>
<sectionHeader confidence="0.99009" genericHeader="method">
2 The C-Test
</sectionHeader>
<bodyText confidence="0.65029725">
The C-test is a form of reduced redundancy testing and
has been established as a standard entrance exam for
many language centers. It usually consists of five coher-
ent paragraphs or short texts. The example below consists
of a single paragraph.
The roots of humanity can be traced back to millions of
years ago. T primary evid comes fr fossils
- skulls, skel and bo fragments. Scien have
ma tools th allow th to ext subtle infor
from anc bones a their enviro settings. Mod
forensic wo in t field a in labora can n
provide a rich understanding of how our ancestors lived.1
</bodyText>
<footnote confidence="0.929913333333333">
1Solutions: The, evidence, from, skeletons, bone, Scientists, made,
that, them, extract, information, ancient, and, environmental, Modern,
work, the, and, laboratories, now
</footnote>
<page confidence="0.913878">
517
</page>
<bodyText confidence="0.9243323">
Transactions of the Association for Computational Linguistics, vol. 2, pp. 517–529, 2014. Action Editor: Chris Callison-Burch.
Submission batch: 4/2014; Revision batch 8/2014; Published 11/2014. c�2014 Association for Computational Linguistics.
After an unaltered introductory sentence, every second
word is transformed into a gap. When the intended num-
ber of gaps is reached (usually 20), the rest of the text
is left intact. For each gap, the smaller half of the word
is provided and the missing part has to be completed by
the learner. Since its introduction, the C-test has been re-
searched from many angles and has been adapted for over
20 languages (see Grotjahn et al. (2002) for an overview).
</bodyText>
<subsectionHeader confidence="0.976891">
2.1 C-Tests vs Cloze Tests
</subsectionHeader>
<bodyText confidence="0.999964122448979">
The C in C-test stands for its origin in the cloze test. In
cloze tests, full words are transformed into gaps accord-
ing to a fixed deletion pattern (e.g. every 7th word).
The main problem with cloze tests is the ambiguity of
the solution. Unless function words are deleted, the gap
allows many alternative solutions such as synonyms and
hypernyms, but also entirely different words that change
the meaning of the text but also fit the context. Language
teachers have proposed two ways of dealing with this
ambiguity: the application of relaxed scoring schemes
and the use of distractors. In relaxed scoring, teachers
accept all tolerable candidates for a gap and not only
the intended solution as in exact scoring. Unfortunately,
this scoring method turned out to be quite subjective and
time-consuming as it is not possible to anticipate all toler-
able solutions (Raatz and Klein-Braley, 2002). The use of
distractors circumvents this open solution space by pro-
viding a closed set of candidates from which the solution
needs to be picked. Several approaches have been pro-
posed for automatic distractor selection (Sakaguchi et al.,
2013; Zesch and Melamud, 2014) to make sure that the
distractors are not too hard nor too easy and are not a
valid solution themselves. However, the presence of the
correct solution in the distractor set enables the option of
random guessing leading to biased results.
In order to overcome this and other weaknesses of the
cloze test, Klein-Braley and Raatz (1984) propose the C-
test as a more stable alternative. Thorough analyses fol-
lowing the principles of test theory indicate advantages of
the C-test over the cloze test regarding empirical valid-
ity, reliability, and correlation with other language tests
(Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur,
1995). For automatic approaches, the following prop-
erties of the C-tests are beneficial: The given prefix re-
stricts the solution space to a single solution (in almost
all cases) which enables automatic scoring without pro-
viding a guessing option. In addition, the prefix hint al-
lows for a narrower deletion pattern (every second gap)
providing more empirical evidence for the students’ abil-
ities on less text.
As the given prefixes reduce the extent to which pro-
ductive skills are required, Cohen (1984) considers the C-
test to be a test of reading ability examining only recogni-
tion. However, Jakschik et al. (2010) transform the C-test
into a true recognition test by providing multiple choice
options and find that this variant is significantly easier
than open C-test gaps. This indicates that C-test solving
requires both, receptive and productive skills, and we re-
flect this in our feature choice.
</bodyText>
<subsectionHeader confidence="0.998805">
2.2 Test Difficulty
</subsectionHeader>
<bodyText confidence="0.999970386363637">
Previous works in the field of educational natural lan-
guage processing approach language proficiency tests
from a generation perspective. The focus is on gener-
ating closed formats such as multiple choice cloze tests
(Mostow and Jang, 2012; Agarwal and Mannem, 2011;
Mitkov et al., 2006), vocabulary exercises (Skory and Es-
kenazi, 2010; Heilman et al., 2007; Brown et al., 2005)
and grammar exercises (Perez-Beltrachini et al., 2012).
The difficulty of these exercises is usually determined by
the choice of distractors as students have to discriminate
the correct answer from a provided set of candidates.
C-tests follow a fixed construction pattern and are
therefore easy to generate. As opposed to closed formats,
the candidate space is only limited by the provided pre-
fix and the length constraint. It is thus harder to deter-
mine the difficulty of a C-test because it is influenced by
a combination of many text- and word-specific factors.
The search for the factors that determine the difficulty of
C-tests is tightly connected to the question of construct
validity: “Which skills does the C-test measure?” While
advocates of the C-test argue that it measures general lan-
guage proficiency involving all levels of language (Eckes
and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985)
others reduce it to a grammar test (Babaii and Ansary,
2001) or rather a vocabulary test (Chapelle, 1994; Single-
ton and Little, 1991).2 In our model, we aim at combining
features touching all levels of language. The earliest anal-
yses of C-test difficulty focused on the paragraph instead
of the gap level. Klein-Braley (1984) performs a linear
regression analysis with only two difficulty indicators –
average sentence length and type-token ratio – obtaining
good results for her target group. Eckes (2011) intend to
calibrate C-test difficulty using a Rasch model in order to
compare different C-tests and build a test pool.3
Kamimoto (1993) was the first to perform classical
item analysis on the gap level. He created a tailored C-test
that only contains selected gaps in order to better discrim-
inate between the students. However, the gap selection is
based on previous test results instead of specific gap fea-
tures and thus cannot be applied on new tests.
Previous work on gap difficulty is based on correlation
analyses. Brown (1989) identifies the word class, the lo-
cal word frequency, and readability measures as factors
correlating with cloze gap difficulty. Sigott (1995) exam-
</bodyText>
<footnote confidence="0.997548333333333">
2It should be noted, that their definition of “vocabulary” is very
wide.
3http://www.ondaf.de
</footnote>
<page confidence="0.964329">
518
</page>
<table confidence="0.9731036">
Error rate
T1 T2 T3 T4
Participants 357 156 147 160
Mean error rate .31 .46 .37 .36
Standard deviation .21 .26 .24 .28
</table>
<tableCaption confidence="0.99988">
Table 1: Analysis of text-level test difficulty
</tableCaption>
<bodyText confidence="0.999821727272727">
ines word frequency, word class, and constituent type of
the gap for the C-test and finds high correlation only for
the word frequency. Klein-Braley (1996) identifies addi-
tional error patterns related to production problems (right
word stem in wrong form) and early closure, i.e. the solu-
tion works locally but not in the larger context. The cited
works focus on the correlation between gap features and
C-test difficulty but did not attempt to actually predict
difficulty. In the following section, we present the results
of our data analysis targeted towards building up a model
for C-test difficulty.
</bodyText>
<sectionHeader confidence="0.985301" genericHeader="method">
3 Data Analysis
</sectionHeader>
<bodyText confidence="0.99962725">
For a better understanding of C-test difficulty, we need to
perform data analysis. As suitable data was not available
in digital form, we conducted a data collection study. In
cooperation with the language center at Technische Uni-
versit¨at Darmstadt, we gathered data from 3 test sessions.
The C-tests are conducted in order to assign students to
courses matching their language proficiency. One test
consists of 5 paragraphs with 20 gaps each.
We created a web interface in which the test had to
be filled. Most students finished before the time limit of
20 minutes was reached. Weaker students left some gaps
unfilled but did not ask for more time. In the first ses-
sion, 357 participants filled in the same C-test (T1). In
the second session, three different test instances (T2, T3,
T4) were assigned randomly to 463 new participants. In
the third session, the tests were composed by randomly
choosing paragraphs from 5 groups, each consisisting of
5 paragraphs. A random combination of 5 paragraphs
(one from each group) was then assigned to 1050 new
participants. All participants are students enrolled at the
university. Our analysis is based on the first two sessions
and we use the data from the third session as test data.
As six paragraphs of the third session had already been
administered before, we remove these from the test data.
</bodyText>
<subsectionHeader confidence="0.997091">
3.1 Text-level Analysis
</subsectionHeader>
<bodyText confidence="0.9999705">
As C-tests are designed mainly for the goal of comparing
students, the difficulty of different tests should be bal-
anced. The difficulty of a C-test is usually measured by
the mean error rate over all gaps. The error rate of a single
gap is the ratio of false answers to all answers. A higher
mean error rate thus indicates higher test difficulty.
As we see in Table 1, the mean error rate varies be-
tween the different tests, although they had been carefully
</bodyText>
<figure confidence="0.44831">
Paragraph
</figure>
<figureCaption confidence="0.999635">
Figure 1: Mean error rate and standard deviation for the para-
graphs 1–5 of the four tests
</figureCaption>
<figure confidence="0.998003333333333">
Standard error 0.12
0.10
0.08
0.06
0.04
0.02
0.00
50 100 150 200 250 300 350
Number of participants
</figure>
<figureCaption confidence="0.992632">
Figure 2: Standard error averaged over all gaps for increasing
numbers of participants
</figureCaption>
<bodyText confidence="0.99982645">
(but manually) designed to be equally difficult. The first
session was generally easier than the second session, and
T2 stood out as particularly difficult. Within this test set-
ting, it is thus not fair to compare students by their overall
score, if they completed different tests. Automatic diffi-
culty prediction prior to the test session could improve
the comparability of test results.
Figure 1 additionally shows the results for each para-
graph. The teachers arrange the five paragraphs of a test
with assumed ascending difficulty. We see that this works
as a general tendency (paragraph 5 is more difficult than
paragraph 1), but a true ordering has not been achieved
for any test. In general, the high standard deviations in-
dicate that the mean error rate is not a very informative
measure, because each test contains very easy and very
difficult gaps. In the extreme case, half of the gaps can
be solved by all learners and the other half by almost no
one. The test is then assigned a medium difficulty, but the
results are not useful for discrimination between learners.
We therefore now analyze the difficulty on the gap-level.
</bodyText>
<subsectionHeader confidence="0.998259">
3.2 Gap-level Analysis
</subsectionHeader>
<bodyText confidence="0.99989975">
Before we can further analyze single gaps, we need to
examine whether the number of participants in our study
was sufficient to obtain reliable error rates on the gap
level. We calculate the standard error for each gap with
</bodyText>
<figure confidence="0.983142625">
P1 – P5 P1 – P5 P1 – P5 P1 – P5
0.8
0.6
0.4
0.2
0
1
T1 T2 T3 T4
</figure>
<page confidence="0.895915">
519
</page>
<construct confidence="0.453415285714286">
The roots of humanity can be traced back to millions of years
ago. T primary evid comes fr fossils - skulls, skel
and bo fragments. Scien have ma tools th allow
th to ext subtle infor from anc bones a their
enviro settings. Mod forensic wo in t field a
in labora can n provide a rich understanding of how
our ancestors lived.
</construct>
<figureCaption confidence="0.999788">
Figure 3: Visualisation of error rates for each gap
</figureCaption>
<bodyText confidence="0.99991190625">
increasing sample sizes.4 Figure 2 shows the results for
the first session (the results for the other three tests are
similar). We see that already with 50 participants, the
standard error is reduced to an acceptable level of 0.05.
As we obtained data from more than 140 participants for
each test, the obtained gap-level error rates are very reli-
able.
Range of error rates In our data, the error rates range
from 0.01 to 0.99 and are almost continuously distributed.
Figure 3 shows an example for the high variance of the
gap difficulty within a single paragraph. The error rates
in the example are indicated by the size of the circles.
Answer variety Even for the difficult gaps, the students
always tried to provide a solution5 because false answers
did not have a negative effect on the result. This behavior
leads to a high answer variety (19 different answers per
gap on average). The number of provided answers cor-
relates with the error rate (Pearson correlation of 0.57).
This indicates that harder gaps trigger more alternatives
and do not provoke the same mistake by everyone.
Spelling errors Many of the false answers are variants
of the correct solution. The students recognize the solu-
tion word but fail to produce it correctly. Unfortunately,
the line between a spelling error and a wrong solution
cannot be clearly drawn. If a plural s is missing we can-
not distinguish between a typo and lack of grammatical
understanding. Spelling errors often also form new words
e.g. of vs. off or then vs. than and we cannot decide
whether it is a spelling error or a wrong word choice. As
the generous time limit allows the students to revise their
solutions for typos, we consider them as normal errors in
line with Raatz and Klein-Braley (2002).
</bodyText>
<sectionHeader confidence="0.984305" genericHeader="method">
4 C-Test Difficulty Model
</sectionHeader>
<bodyText confidence="0.9961235">
Natural languages are complex and constantly develop-
ing constructs that include many exceptions to the rules.
</bodyText>
<footnote confidence="0.99912075">
4For each size, we calculate the error rate based on three randomly
selected samples of participants and report the average result.
5Except for the weakest students who were not able to understand
the texts and left entire paragraphs empty.
</footnote>
<figureCaption confidence="0.999865">
Figure 4: C-Test Difficulty Model
</figureCaption>
<bodyText confidence="0.999984277777778">
Hence, the potential problems for foreign language learn-
ers are manifold and hard to anticipate. We took a closer
look at the false answers in order to gain deeper under-
standing of the dimensions that lead to wrong answers
and therefore to higher difficulty. We find that the diffi-
culty of C-tests is determined by a combination of many
factors.
In order to establish a shared terminology, learner
strategies for C-test solving have been categorized as
micro-level and macro-level processing strategies (Babaii
and Ansary, 2001). Psycholinguistic analyses (Sigott,
2006; Grotjahn and Stemmer, 2002) discuss in detail that
both strategies are required for successful C-test solv-
ing. Therefore, we developed a model for C-test diffi-
culty that incorporates features from both processing lev-
els (see Figure 4).
Micro-level processing only deals with the solution
of the gap and its surrounding micro context. The mi-
cro context consists of the word preceding the solution,
the solution, and the following word. Both, the preced-
ing and the following word are intact (i.e. not mutilated as
gap) and can be used as solution hints by every learner, in-
dependent of the performance on the other gaps. In order
to determine the difficulty of a gap based on micro-level
cues, we estimate two dimensions: the solution difficulty
and the candidate ambiguity.
Macro-level processing takes the wider context into
account and evaluates the gap in relation to other ele-
ments in the sentence and in the whole paragraph. The
difficulty of a gap on the macro-level is determined by
two dimensions: the inter-gap dependency and the para-
graph difficulty.
In the remainder of this section, we elaborate on the in-
dividual dimensions. We provide examples that illustrate
the described phenomena and introduce the features that
operationalize them.
</bodyText>
<subsectionHeader confidence="0.998494">
4.1 Solution Difficulty
</subsectionHeader>
<bodyText confidence="0.999911833333333">
This micro-level dimension comprises features that ap-
proximate whether a learner knows the solution and can
correctly produce it in the context. We identified four im-
portant phenomena that contribute to the solution diffi-
culty: word familiarity, cognateness, inflection, and pho-
netic complexity.
</bodyText>
<page confidence="0.965234">
520
</page>
<bodyText confidence="0.89181975">
Word familiarity If we compare the solutions of the
easiest (example 1) and the most difficult gap (example
2), it is obvious that you is easier because it is more fa-
miliar to the participants than plentiful.6
</bodyText>
<listItem confidence="0.9985035">
1. Ify are looking for new experiences, ... [you]
2. ..., people may try self-employment because the
</listItem>
<bodyText confidence="0.978317727272727">
opportunities seem plen and financing is easy to
get. [plentiful]
The probability that a learner knows a word is usually
estimated by the word frequency; more frequent words
are more likely to be known. We therefore calculate the
frequency of the solution and also its length as more fre-
quent words tend to be shorter in English. In previous
work, Brown (1989) calculates the frequency of the target
word on the basis of the current test text. This is clearly
a biased estimate of the frequency, but it is still identi-
fied as a good indicator for cloze gap difficulty. Sigott
(1995) calculates the frequency of the solution word us-
ing counts from the SUSANNE corpus.7 For our calcula-
tions, we use the larger Web1T corpus (Brants and Franz,
2006) and extract normalized probabilities instead of ab-
solute frequencies for better comparison.
Furthermore, a gap is easier to solve, if the solution
occurs in a very typical context, e.g. in the micro con-
text States o America, the candidate of is clearly fa-
vored, while in the context write o paper, the candidates
on, our and off are more probable. In order to account
for typical phrases, we calculate the normalized trigram
probability of the micro context.
Even if a word seems familiar to a learner, it might
be problematic when used in a compound (e.g. coastline)
because the prefix only provides information about the
first part of the word. In our approach, compounds are
detected using a word splitting algorithm with an English
dictionary.8
Another issue are polysemous words, as learners might
know one sense of a word but not be aware of the exis-
tence of a second sense. Polysemy interferes with fre-
quency, e.g. the word well has a high frequency, but it
occurs only rarely in its sense fountain. In order to ac-
count for polysemy, we count the number of represented
word senses for the solution in the lexical-semantic re-
source UBY (Gurevych et al., 2012).
The two senses of well also differ in their word class.
The word class has been studied as a difficulty indica-
tor by several researchers but with mixed results. Brown
(1989) finds that function words are easier to solve,
while Klein-Braley (1996) claims that prepositions are
often harder for learners. Sigott (1995) could not con-
firm any effect of the word class on C-test difficulty.
</bodyText>
<footnote confidence="0.99819525">
6In all examples, we only highlight a single gap to illustrate a certain
phenomenon.
7http://www.grsampson.net/RSue.html
8http://www.danielnaber.de/jwordsplitter/index en.html
</footnote>
<bodyText confidence="0.999510755555556">
The word class is determined by identifying the part-of-
speech (POS) tag. As additional feature, we calculate the
probability of the POS sequence of the micro context.
Cognateness Frequency is not the only indicator for
word familiarity and can sometimes even be misleading
(Beinborn et al., 2014). Many solution words are cog-
nates, i.e. they are very similar to words in other lan-
guages like information or laboratory. In reading com-
prehension, cognates are known as facilitators because
their meaning can be deducted from the form similarity to
a word in the mother tongue. We therefore assumed that
cognate gaps are easier to solve. However, we observe
that they are more likely to trigger production problems.
In the 20 gaps with the highest answer variety (33 or more
different answers), all solutions have a Latin stem.9 The
20 gaps with the lowest answer variety (5 or less different
answers) are very basic vocabulary.10
The production problems are related to the different
character combinations and the lower frequency of words
with Latin stem. In addition, these words might not
be part of the students active vocabulary and are only
guessed because they occur as cognates in the students
L1. This is supported by the fact that many of the cognate
answers resemble orthographic principles from other lan-
guages, e.g. for skeletons we find *skellets, *skelleton(s),
*skelets, *skelletts, *skeletton(s), *skeltons, *skeletes,
and *skelette(s).11
In order to account for this phenomenon, we estimate
the cognateness of words by gathering data from four
different lists. We retrieve cognates from UBY using
string similarity and from a cognate production algo-
rithm (Beinborn et al., 2013). In addition, we consult the
COCA list of academic words12 and a list of words with
latin roots.13
Inflection Many errors are caused by wrong morpho-
logical inflection as in this example:
And in har times like these, ... [harder]
The base form hard (72) is provided more often than the
correct comparative harder (48), although it is too short.
Other inflection errors are caused by singular/plural and
adjective/adverb confusion.
In order to account for this phenomenon, we test
whether the solution is in lemma form or carries any
inflection markers using a lemmatizer. We also check
whether the word occurs elsewhere in the text in full form
</bodyText>
<construct confidence="0.8269058">
9appropriate, skeletons, tempting, extract, ancient, private, design,
concentrations, state-of-the-art, scientists, modern, examined, constant,
essential, stable, entering, basis, synthetic, cost, demands
10longer, coffee, coffee, in, water, very, give, you, for, people, living,
other, number, water, water, from, over, you, over
</construct>
<footnote confidence="0.985343333333333">
11DE: Skelett, FR: squelette, ES: esqueleto, NL: skelet
12http://www.academicvocabulary.info/
13http://en.wikipedia.org/wiki/List of Latin words with English derivatives
</footnote>
<page confidence="0.994439">
521
</page>
<bodyText confidence="0.999922810810811">
(i.e. not as a gap) because it facilitates the correct pro-
duction for the student. This feature is comparable to the
semantic cache used by Brown (1989).
Phonetic complexity Wrong answer variants for C-test
gaps are often rooted in phonetic problems. The spelling
of a word is more difficult, if it contains a rare sequence
of characters. The word appropriate, for example, trig-
gers 69 different answers, 40 of them were provided only
once. In addition, a spelling error is more likely to oc-
cur, in words with rare grapheme-phoneme mapping as in
Wednesday. We build a character-based language model
that indicates the probability of a character sequence us-
ing BerkeleyLM (Pauls and Klein, 2011). In addition,
we build a phonetic model using phonetisaurus, a sta-
tistical alignment algorithm that maps characters onto
phonemes.14 Both models are trained only on words from
the Basic English list in order to reflect the knowledge of
a language learner.15 Based on this scarce data, the pho-
netic model only learns the most frequent character-to-
phoneme mappings and assigns higher phonetic scores
to less general letter sequences. We use this score as
a feature and additionally calculate the string similarity
between the output and the correct pronunciation in the
CMU dictionary.16
Another source for phonetic problems occurs, if the
prefix boundary splits the word in a way that leads to
another pronunciation pattern compared to the solution
word as in this example.
It is not easy to design and build a mac that is
both, efficient and durable. [machine]
Due to the syllable split, the prefix provokes answers with
the pronunciation [mac] such as macanics, mac(h)anism,
macanical, macbook, macphone, and macro instead of
the original pattern [maf]. A similar issue occurs when
the prefix splits a compound such as greenhouse. We
check if the prefix boundary occurs within a compound
or a syllable using a hyphenation dictionary.17
</bodyText>
<subsectionHeader confidence="0.992895">
4.2 Candidate Ambiguity
</subsectionHeader>
<bodyText confidence="0.999837222222222">
This micro-level dimension examines whether a compet-
ing candidate is more accessible for the learner in the
given context. Even if the learner is familiar with the
solution word, she might still not be able to produce it,
because a competing candidate is stronger. For example,
in 42 gaps in our data, an alternative answer is provided
more frequently than the intended solution.
Some of these gaps actually have more than one possi-
ble solution as the following example:
</bodyText>
<footnote confidence="0.9648692">
14http://code.google.com/p/phonetisaurus/
15http://ogden.basic-english.org
16http://www.speech.cs.cmu.edu/cgi-bin/cmudict
17http://hindson.com.au/info/free/free-english-language-
hyphenation-dictionary/
</footnote>
<bodyText confidence="0.989742622641509">
Scientists have ma tools that allow them to ex-
tract subtle information from ancient bones and their
environmental settings. [many]
Instead of the correct solution many (89), most students
provided made (238) which can also be considered cor-
rect here. These cases had not been anticipated by the
language teachers, they only encoded one solution in the
system.
In other cases, alternative answers seem very probable
to the students but are nevertheless false.
A natural blanket of greenhouse gases in the atmo-
sphere keeps the planet warm enough for life as we
know it at a comfortable 15C today. Human-caused
emissions of greenhouse gases have made the blanket
thi , trapping heat and leading to a global warm-
ing. [thicker]
Instead of the correct solution thicker (12), the students
provided many alternative solutions more often: thinner
(31), thin (19), thick (18), this (14), thing (14). Thinner
fits syntactically but completely changes the semantics of
the sentences as it is the antonym of the correct solution.
The learner needs to apply world knowledge to under-
stand that a thinner blanket would not trap heat. In our
model, we want to account for both cases, as it would
be very helpful if ambiguous gaps could be automatically
detected. This aspect has been neglected in previous work
on C-test difficulty.
In order to account for competing candidates, we first
determine the candidate space and then describe our fea-
tures approximating the probability that a competing can-
didate confuses the student.
Candidate space Prior to the tests, the students are in-
formed about the quite restrictive length constraint. The
given prefix of C-test gaps consists always of the smaller
half of the solution: if 3 characters are provided as pre-
fix, the correct word can only consist of 6 or 7 charac-
ters. This can be a useful indicator for the solution, but
the data reveals that in approx. 40% of the false answers,
the length constraint is not respected. The absolute num-
ber of false length answers is higher for weaker students.
However, the proportion of false length answers relative
to all false answers is higher for stronger (0.45) than for
weaker (0.32) students.
Length violations can be caused by candidates that
seem viable for the context and are more accessible than
the solution or by wrong inflection of the word ending. In
other cases, it is obvious that the student does not find a
proper solution and provides just anything that remotely
fits. It would be interesting to repeat the test with the con-
straint that false answers have a negative influence on the
overall score in order to find out whether the students are
aware of the length violation.
Bresnihan and Ray (1992) show that students perform
</bodyText>
<page confidence="0.986833">
522
</page>
<bodyText confidence="0.999970272727273">
better on the C-test, if the length of the solution is graph-
ically indicated by dashes or dots which supports the as-
sumption that length violations are often not noticed in
the standard C-test. As we want to account for this phe-
nomenon, we decide to relax the length constraint. We
only allow a length tolerance of 1, i.e. for a prefix of
length 3, we consider candidates with 5 to 8 (instead of
6 to 7) characters, as the candidate space would be too
large otherwise.
We noticed that even candidates with wrongly spelled
prefix can be competitors, e.g. some students provided
the answer *demage for the prefix dem instead of the cor-
rect solution demands. The word damage actually fits se-
mantically into the gap, but as the prefix is different, we
currently do not add such cases to the candidate space.
In order to account for candidate ambiguity, we rank
all candidates according to three criteria: the unigram fre-
quency, the trigram frequency of the micro context, and
the parse score. Statistical parsers usually provide parse
scores in order to determine the best variant. This score
cannot be used as an absolute value because it depends
on the sentence length but it helps to distinguish between
candidates. A candidate that produces another parse tree
than the solution is less likely to be correct. For each
ranking, we determine the rank of the solution and the
number of candidates above a fixed threshold.
In addition, we take the intersection of the best candi-
dates from the above rankings, combine them into a set
of top candidates that are likely to compete with the so-
lution and determine its size. Moreover, we calculate the
maximum string similarity of the candidates with the so-
lution in order to capture very close variants (e.g. base
and basis).
</bodyText>
<subsectionHeader confidence="0.986672">
4.3 Inter-gap dependency
</subsectionHeader>
<bodyText confidence="0.9994174375">
This macro-level dimension assesses the dependency of
the current gap on previous gaps: can it be solved, even if
the previous gap has not been solved? In previous work,
Harsch and Hartig (2010) examine dependencies between
individual gaps using a Rasch testlet model and find that
some gaps strongly depend on each other, while others
can be solved independently.
At the same time, fertility is set to fall as women leave
childbirth la and la . [later]
In these gaps, later is repeated which makes it easy to fill
in the second gap, if the first one is solved.
The dependency of a gap is related to its position and
the difficulty of the preceding word. If a gap is preceded
by a very difficult gap, the available context is damaged
which can have an effect on the difficulty of the following
gaps. A gap occurring towards the end of a sentence, is
also more likely to be influenced by limited context. We
thus calculate the position of the gap and the number of
previous gaps in the sentence and in the paragraph. We
check if the same solution also occurs in another gap to
account for repetition. In order to estimate the difficulty
of the previous gap, we calculate its unigram and trigram
probability. If we already had a good difficulty prediction
algorithm, we could perform incremental prediction and
use the difficulty label of the previous gap as a feature for
the current gap, but this is left to future work.
In addition, we check for gaps with the prefix th be-
cause they enable many reference words such as this,
that, there, then, these, those, they, and their. The stu-
dent needs to perform co-reference resolution in order to
select the correct word. These referential gaps usually
cannot be solved on the basis of the micro context.
</bodyText>
<subsectionHeader confidence="0.998814">
4.4 Paragraph difficulty
</subsectionHeader>
<bodyText confidence="0.999982307692308">
This macro-level dimension determines whether the
learner is generally able to understand the text. The over-
all difficulty of a paragraph contributes to the difficulty
of the individual gaps because more complex texts are
harder to parse for language learners, especially when ev-
ery second word is a gap. Thus, the available context for
each gap is assumed to be lower in more difficult para-
graphs. As we have seen in Section 3.1, the difficulty of
the gaps within one paragraph varies strongly. We there-
fore assume that the paragraph difficulty only adds a con-
stant effect to the overall gap difficulty.
The difficulty of a paragraph is inversely related to its
readability. We calculate the following readability fea-
tures for the whole paragraph and for the sentence con-
taining the gap. Average word and sentence length are the
underlying basis of traditional readability measures such
as Flesch-Kincaid and Fry which correlate with cloze test
difficulty according to Brown (1989). We calculate both,
but do not find much variety as the paragraphs in our data
are all of comparable length (64-99 words, 3-7 sentences,
4.85 characters per word).
The type-token ratio, the verb variation, and the pro-
noun ratio are used as indicators for lexical diversity and
referentiality. Klein-Braley (1984) already determined
the type-token ratio as useful cue for paragraph difficulty
prediction. We also use syntactic readability features
such as the number of entity mentions, the number of
certain POS types (e.g. noun, determiner, adjective) and
the number of certain phrase patterns (e.g. verbal phrase,
noun phrase, subordinate phrase).
Having introduced all four dimensions of C-test dif-
ficulty, we now report on the results of the actual diffi-
culty prediction. Difficulty prediction of C-tests has up to
now only been performed on the paragraph level (Klein-
Braley, 1984; Traxel and Dresemann, 2010). In this arti-
cle, we go beyond paragraphs and predict the difficulty of
gaps. We first determine the human performance on the
task and use it as a reference for the performance of the
machine learning approach based on our difficulty model.
</bodyText>
<page confidence="0.997492">
523
</page>
<table confidence="0.996148666666667">
A1 A2 A3 Median A1-A3
Correct Prediction 200 209 192 213
Overestimation 90 99 83 101
Underestimation 107 89 118 84
NA 2 2 6 1
Accuracy 0.50 0.52 0.48 0.53
</table>
<tableCaption confidence="0.99915">
Table 2: Results of the human annotations
</tableCaption>
<sectionHeader confidence="0.937529" genericHeader="method">
5 Human Difficulty Prediction
</sectionHeader>
<bodyText confidence="0.998748428571429">
Due to the high number of participants, we already have
precise gap-level error rates (cf. Figure 2) for our tests.
We now want to determine to what extent human annota-
tors are able to predict these error rates. For this purpose,
we asked three English language teachers to assign a dif-
ficulty category to each gap according to the following
scheme:
</bodyText>
<listItem confidence="0.986976">
1: Very easy gap (error rate &lt; 0.25)
2: Easy gap (0.25 &lt; error rate &lt; 0.5)
3: Medium gap (0.5 &lt; error rate &lt; 0.75)
4: Difficult gap (error rate &gt; 0.75)
</listItem>
<bodyText confidence="0.9999613125">
The annotation was performed on the same 20 texts as de-
scribed in Section 3.1. The teachers were already familiar
with these texts, as they had chosen them for the testing
period. We consider a gap to be correctly annotated, if
the human-assigned class matches the actual error rate.
Given the highly experienced annotators, the predic-
tion accuracy is lower than expected. The three annota-
tors obtain comparable accuracy, each of them correctly
predicts approximately 50% of the gaps (see Table 2).
There is no obvious bias in the annotations, difficulty is
both under- and overestimated. If we combine the human
prediction by taking the median of the three annotators,
53.4% are annotated correctly. These results show that
even experienced teachers are not able to foresee all fac-
tors that influence the difficulty of a gap.
Somewhat surprisingly, the agreement between the an-
notators is also low. The Fleiss’ Kappa for the three anno-
tators is 0.36, the pairwise comparison ranges from 0.31
to 0.39. Only in 38.6% of the gaps, all three annotators
agreed with each other. For only 25.3%, all three annota-
tors agreed with each other and with the actual measured
error rate. This shows that human difficulty prediction is
quite subjective.
The mediocre human performance on the task reveals
the complexity of predicting the elements of language
that cause problems for foreign language learners. How-
ever, this strengthens the need for reliable prediction
methods like the one described in this paper. Note that
the automatic prediction is compared with the actual er-
ror rates, not the human predicated ones. Thus, it is pos-
sible to outperform human performance with automatic
methods and provide a very helpful tool.
</bodyText>
<table confidence="0.9775475">
Classification Regression
P R Fl Pearson’s r RMSE
Majority Baseline .19 .43 .26 .00 .25
Sigott (1995) .23 .40 .28 .34 .24
Our Approach .46 .48 .46 .64 .20
Human Median .56 .53 .54 - -
</table>
<tableCaption confidence="0.9933372">
Table 3: Results for leave-one-out crossvalidation on the train-
ing set for regression and classification prediction (both trained
on support vector machines). Classification results are the
weighted average of precision (P), recall (R) and Fl-measure
over all four classes.
</tableCaption>
<sectionHeader confidence="0.978738" genericHeader="method">
6 Automatic Difficulty Prediction
</sectionHeader>
<bodyText confidence="0.999912714285714">
Our difficulty prediction approach is based on the model
described in the previous section. We extract the features
using tools for natural language processing provided by
DKPro Core (de Castilho and Gurevych, 2014). We then
perform experiments with different datasets and classi-
fiers using Weka (Hall et al., 2009) through the DKPro
TC framework (Daxenberger et al., 2014).18
</bodyText>
<subsectionHeader confidence="0.999111">
6.1 Classification vs Regression
</subsectionHeader>
<bodyText confidence="0.999969586206897">
For the human annotation, we used a classification
scheme because assigning difficulty scores on a fine-
grained numerical scale would be too challenging even
for experienced teachers. However, as the actual error
rates are continuously distributed, gaps that are close to
the class boundaries are more likely to be mislabeled.
Therefore we also test regression prediction using the
actual error rates instead of the artificially determined
classes. We perform leave-one-out testing on the train-
ing set in order to determine the best approach.
We compare our model against the human performance
and two baselines: A naive one that predicts the majority
class for classification and the mean value for regression
and one that only uses the features proposed by Sigott
(1995) (solution probability, word class of solution, and
constituent type of gap).
In Table 3, we report weighted precision, recall and
Fl-measure over all classes for classification and Pearson
correlation and root mean squared error for regression.
It can be seen that our approach clearly outperforms the
baselines in both cases.
For classification, the human median annotation is bet-
ter than our approach. In order to also compare our
regression results to the human annotations, we map
the numerical predictions back into classes according to
the scheme explained in the previous subsection. The
quadratic weighted kappa considers the classes on an or-
dinal scale and thus gives a better impression of the use-
fulness of the prediction. The results in Table 4 show that
</bodyText>
<footnote confidence="0.5418495">
18More information on data and resources can be found at
http://www.ukp.tu-darmstadt.de/data/c-tests.
</footnote>
<page confidence="0.991064">
524
</page>
<table confidence="0.994879772727273">
q.w. κ
Human Median .59
SMO Classification .47
Mapped SMO Regression .58
Feature Group
Micro-level Processing
Micro vs. Macro
Macro-level Processing
w/o Solution Difficulty
w/o Candidate Ambiguity
w/o Inter-Gap Dependency
w/o Paragraph Difficulty
All
# Feat. Pearson’s r
51 .50**
37 .24**
50 .42**
74 .54**
79 .62*
59 .59**
87 .64
Dimensions
</table>
<tableCaption confidence="0.996433">
Table 4: Quadratic weighted κ of difficulty class predictions
</tableCaption>
<figureCaption confidence="0.999200333333333">
Figure 5: Regression results for leave-one-out testing on the
training data. The symbols indicate the difficulty class that was
annotated by the human experts.
</figureCaption>
<bodyText confidence="0.9999075">
the performance of the regression approach is almost on
the same level as the median of the human prediction.
Therefore, we will focus on regression prediction for the
remainder of the paper.
For a better understanding of the behaviour of human
and automatic predictions, the plot in Figure 5 combines
the two results. The position in the plot indicates the re-
lation between the true error rate and the prediction and
the symbols show the corresponding human annotation.
The plot reveals that the regression equation predicts the
right tendency but tends to slightly underestimate difficult
gaps and overestimate easy gaps. The human prediction
performs well for the easiest gaps (class 1, green circle)
while the other three classes are confused quite often.
</bodyText>
<subsectionHeader confidence="0.999491">
6.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.999815875">
For a deeper analysis of our difficulty model, we now
compare different feature groups.
Processing Levels The results in the first two rows
show that the gap difficulty is mainly determined by the
features representing micro-level processing. This is not
surprising, as these features are calculated for each gap,
while most of the macro-level features are constant for all
gaps in the paragraph. The predictive power on the micro-
</bodyText>
<tableCaption confidence="0.992964666666667">
Table 5: Regression results for different feature groups. Signifi-
cant differences to the result with all features are indicated with
*(p&lt;0.05) and **(p&lt;0.01).
</tableCaption>
<bodyText confidence="0.999539621621622">
level of our approach is a strong improvement over previ-
ous prediction approaches that only attempted to predict
paragraph difficulty.
Dimensions The middle part of Table 5 shows that the
prediction results decrease significantly, if we exclude
features from one dimension. The effect is particularly
strong, if we exclude the features estimating the diffi-
culty of the solution, while the effect of the inter-item
dependency features is quite small. This supports previ-
ous theoretical research claiming that the solution word
itself and its micro context are most relevant for the solv-
ing processes. The dimensions candidate ambiguity and
inter-item dependency have been newly introduced, while
many of the features for solution and paragraph difficulty
are well established. We therefore assume that future
work on improved feature development for these dimen-
sions could lead to even better prediction results.
Selected Features As the results for the individual di-
mensions might be related to the number of features, we
additionally perform feature selection and reduce the set
to 21 features.19
The selection shows that the probability of the word,
the phrase and the character sequence play a major role
for prediction. However, it might be the case that the con-
tinuous values of these features are simply more suitable
for regression approaches than boolean features such as
the word class of the solution. In addition, the number
of available candidates plays an important role but prim-
ing effects also need to be considered (whether the solu-
tion occurs previously in the text or mutilated as another
gap). For the paragraph difficulty, the number of verbs
and embedded sentences seems to be a good indicator of
difficulty.20
Interestingly, features from all four dimensions are in-
cluded in the selection as can be seen in Table 7. This
indicates that the dimensions in our model represent the
factors that have an influence on the C-test difficulty quite
</bodyText>
<footnote confidence="0.99514175">
19We use the WrapperSubsetEval-evaluator with SMOreg and
BestFirst-search as implemented in Weka.
20The term CoverSentence in Table 6 refers to the sentence contain-
ing the gap.
</footnote>
<page confidence="0.99011">
525
</page>
<table confidence="0.996688863636364">
Dimensions Selected Features
SolutionDifficulty IsAdverb
IsPlural
CharacterLMProbabilityOfPrefix
UnigramProbability
LeftBigramProbability
RightBigramProbability
TrigramProbability
SolutionOccursAsText
CandidateAmbiguity NrOfCandidates
NrOfParseCandidates
RankOfSolutionInParseCandidates
MaxLCSRofCandidatesAndSolution
Inter-Item Dependency SolutionOccursInAnotherGap
PrefixIsTh
NumberOfPreviousGapsInCoverSentence
PositionOfGap
Paragraph Difficulty AvgWordLength
NounsPerSentence
VerbsPerSentence
VerbVariation
SBarsInCoverSentence
</table>
<tableCaption confidence="0.829771">
Table 6: Selected Features
</tableCaption>
<table confidence="0.999845">
All Features Selected Features
Solution Difficulty 37 (43%) 8 (38%)
Candidate Ambiguity 13 (15%) 4 (19%)
Inter-Gap Dependency 8 ( 9%) 4 (19%)
Paragraph Difficulty 28 (33%) 5 (24%)
Sum 87 21
</table>
<tableCaption confidence="0.989653">
Table 7: Proportion of dimensions in selected features and all
features
</tableCaption>
<bodyText confidence="0.9996546">
well. However, the solution difficulty dimension is by
far the most important one, while the other three dimen-
sions contribute fewer features. We include the predic-
tion results for the selected features in Table 8 which is
discussed in the following section.
</bodyText>
<subsectionHeader confidence="0.999714">
6.3 Test results
</subsectionHeader>
<bodyText confidence="0.999961611111111">
In order to evaluate our model on unseen data, we test
it on a set of 375 additional gaps. The results on the
test set are substantially worse than in the leave-one-out
(LOOCV Train) setting. If we merge the two sets and per-
form leave-one-out testing on the whole data (LOOCV
All), the results get close to our training set again. This
indicates that the test set contains characteristics, that
have not been observed during training. It is also inter-
esting that using only the selected features yields better
results on the smaller training set, while the full model is
better on larger data. In order to support the assumption
that our model performs better with more data, we plot
a learning curve (see Figure 6). We calculated the Pear-
son correlation for increasing sample sizes of randomly
selected instances and average the results over 100 runs.
The anomaly for smaller sample sizes can be explained
by very high standard deviations. Starting from a sample
size of about 70 instances, the learning curve proceeds as
</bodyText>
<table confidence="0.999292">
# LOOCV Train Train-Test LOOCV All
Mean Baseline 1 .00 .00 .00
Sigott (1995) 7 .34 .38 .36
Full Model 87 .64 .32 .60
Selected Features 21 .68 .44 .57
</table>
<tableCaption confidence="0.999795">
Table 8: Results on the train and the test set
</tableCaption>
<bodyText confidence="0.9412615">
expected and highlights the importance of a larger train-
ing set.
</bodyText>
<figure confidence="0.523577">
Sample Size
</figure>
<figureCaption confidence="0.997281">
Figure 6: Learning curve for 10-fold cross-validation with in-
creasing size of training data, results are averaged over 100 runs
</figureCaption>
<subsectionHeader confidence="0.626354">
6.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.956135666666667">
Figure 7 shows that our prediction approach produces
a few strong outliers for the test data. In particular, it
strongly underestimates the error rate for some very easy
gaps. We perform an error analysis on the 9 outliers.
Underestimation In two underestimated gaps, the so-
lution requires an apostrophe (Earth’s, world’s). This has
not been seen in the training data, and therefore we can-
not predict that the students have difficulties here. It is
debatable whether punctuation should be included into
the solution but the language teachers insisted on the im-
portance of these gaps. In two other cases, the students
systematically favour a wrong solution—one is due to
</bodyText>
<figureCaption confidence="0.993092666666667">
Figure 7: The prediction for the train-test setting produces more
outliers. Instances with an absolute difference of predicted and
actual error rate ≥ 0.5 are coloured red.
</figureCaption>
<figure confidence="0.999483">
0.6
Mean
Stdev
0 200 400 600
Pearson Correlation 0.4
0.2
0
</figure>
<page confidence="0.995403">
526
</page>
<bodyText confidence="0.999968333333333">
spelling (of instead of off) and the other due to refer-
entiality (the instead of this)—which our approach did
not anticipate correctly. The last two outliers occur in a
phrase that is very frequent for native speakers but nev-
ertheless unknown to the participants (cause untold dam-
age, the continental United States21).
Overestimation One of the items for which the error
rate is strongly underestimated is the compound carbon-
free. It can be seen, that the teachers deviated from the
original length constraint here and applied it only on the
second part of the component. As these kind of com-
pounds have not been seen in the training set, our ap-
proach estimates the difficulty for providing carbon-free,
while it should rather consider only free. The second
overestimation is due to the named entity Deutsche Bahn,
which is unlikely to occur in English text but very com-
mon for students living in Germany. The third overesti-
mated outlier is simply due to an unfortunate combina-
tion of a long word (dangerous) at the end of a difficult
sentence that is nevertheless easy for the students.
The errors due to apostrophes and hyphenated com-
pounds can be minimized by adapting the processing. In
order to also anticipate the other outlier phenomena, we
need more training data.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.985336348837209">
We introduce the first model for the automatic prediction
of gap-level difficulty of C-tests. We collected data from
real learners and find that the gap-level error rates are
quite stable. The prediction results of our approach are
on the same level as the performance of human experts.
The learning curve indicates that even better results are
possible with more training data. A higher number of
instances makes it easier to learn the nuances for the pre-
diction and this can help to improve the features.
Our work also sheds light on the question what C-tests
measure. The difficulty of a C-test gap is determined by
a combination of many factors. Our experiments have
shown that both, micro- and macro-level cues, contribute
to the gap difficulty: i) problems related to the solution
such as spelling, phonetic difficulties and morphological
derivation, ii) problems caused by competing candidates,
iii) problems caused by dependencies between gaps, and
iv) readability problems caused by text complexity. Even
the reduced set of selected features comprises features
from all introduced dimensions which shows that our
conclusions drawn from the data analysis led to a very
suitable model. However, the features measuring the dif-
ficulty of the solution and the probability of the micro
context seem most relevant. As a next step, we need to
improve the feature extraction for compound nouns and
named entities.
21The students provided only continent instead.
Our approach has already raised interest in language
teachers who see strong practical benefits. The automatic
difficulty prediction facilitates test selection, as teachers
can run our approach on a corpus and only inspect tests
with adequate difficulty. The system could also be tuned
towards the prediction of potentially ambiguous gaps so
that teachers become aware of the alternative solutions.
In addition, our approach can also be used productively
for the automatic test generation in platforms for self-
directed language learning.
Our model has been developed for the difficulty pre-
diction of English C-tests. However, it can also be gener-
alized to other languages and to test variants of reduced
redundancy testing. In future work, we aim at adapting
the difficulty of a given text by varying the gap place-
ment.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999953">
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Klaus Tschira Foun-
dation under project No. 00.133.2008. We thank the
anonymous reviewers for their very helpful comments.
</bodyText>
<sectionHeader confidence="0.998033" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997914366666666">
Manish Agarwal and Prashanth Mannem. 2011. Automatic
Gap-fill Question Generation from Text Books. Proceedings
of the 6th Workshop on Innovative Use of NLP for Building
Educational Applications, pages 56–64.
Esmat Babaii and Hasan Ansary. 2001. The C-test: a valid
operationalization of reduced redundancy principle? System,
29(2):209–219.
Lisa Beinborn, Torsten Zesch, and Iryna Gurevych. 2013. Cog-
nate Production using Character-based Machine Translation.
In Proceedings of the Sixth International Joint Conference on
Natural Language Processing, pages 883–891. Asian Feder-
ation of Natural Language Processing.
Lisa Beinborn, Torsten Zesch, and Iryna Gurevych. 2014.
Readability for foreign language learning: The importance
of cognates. International Journal of Applied Linguistics,
pages 136–162.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram corpus
version 1.1. Linguistic Data Consortium.
Brian Bresnihan and Stratton Ray. 1992. C-tests and the use-
fulness of non-linguistic instructions. In R¨udiger Grotjahn,
editor, Der C-Test. Theoretische Grundlagen und praktische
Anwendungen 1, pages 193–216. Brockmeyer, Bochum.
Jonathan C Brown, Gwen A Frishkoff, and Maxine Eskenazi.
2005. Automatic Question Generation for Vocabulary As-
sessment. In HLT ’05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods in Natu-
ral Language Processing, pages 819–826, Morristown, NJ,
USA. Association for Computational Linguistics.
James Dean Brown. 1989. Cloze item difficulty. JALTjournal,
11:46–67.
</reference>
<page confidence="0.971215">
527
</page>
<reference confidence="0.999843">
C. A. Chapelle. 1994. Are C-tests valid measures for L2 vocab-
ulary research? Second Language Research, 10(2):157–187,
June.
Andrew D. Cohen. 1984. The C-Test in Hebrew. Language
Testing, 1(2):221–225.
Johannes Daxenberger, Oliver Ferschke, Iryna Gurevych, and
Torsten Zesch. 2014. DKPro TC: A Java-based Framework
for Supervised Learning Experiments on Textual Data. In
Proceedings of 52nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations, pages
61–66. Association for Computational Linguistics.
Richard Eckart de Castilho and Iryna Gurevych. 2014. A
broad-coverage collection of portable NLP components for
building shareable analysis pipelines. In Proceedings of the
Workshop on Open Infrastructures and Analysis Frameworks
for HLT (OIAF4HLT) at COLING 2014, pages 1–11, August.
Thomas Eckes and R¨udiger Grotjahn. 2006. A closer look
at the construct validity of C-tests. Language Testing,
23(3):290–325, July.
Thomas Eckes. 2011. Item banking for C-tests: A polytomous
Rasch modeling approach. Psychological Test and Assess-
ment Modeling, 53(4):414–439.
R¨udiger Grotjahn and Brigitte Stemmer. 2002. C-Tests and
language processing. In James A. Coleman, R¨udiger Grot-
jahn, and Ulrich Raatz, editors, University language testing
and the C-Test, pages 115–130. AKS-Verlag, Bochum.
R¨udiger Grotjahn, Christine Klein-Braley, and Ulrich Raatz.
2002. C-Tests: an overview. In James A. Coleman, R¨udiger
Grotjahn, and Ulrich Raatz, editors, University language
testing and the C-Test, pages 93–114. AKS-Verlag, Bochum.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann,
Michael Matuschek, Christian M Meyer, and Christian
Wirth. 2012. A Large-Scale Unified Lexical-Semantic Re-
source Based on LMF. Proceedings of the 13th Conference
of the European Chapter of the Association for Computa-
tional Linguistics, pages 580–590.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer,
Peter Reutemann, and Ian H. Witten. 2009. The WEKA data
mining software: an update. 11(1).
Claudia Harsch and Johannes Hartig. 2010. Empirische und
inhaltliche Analyse lokaler Abh¨angigkeiten im C-Test. In
R¨udiger Grotjahn, editor, Der C-Test: Beitr¨age aus der ak-
tuellen Forschung, pages 193–204. Peter Lang.
Michael J. Heilman, Kevyn Collins-Thompson, Jamie Callan,
and Maxine Eskenazi. 2007. Combining lexical and gram-
matical features to improve readability measures for first
and second language texts. In Proceedings of NAACL-HLT,
pages 460–467.
A. Jafarpur. 1995. Is C-testing superior to cloze? Language
Testing, 12(2):194–216, July.
Gerhard Jakschik, Hella Klemmert, and Dorothea Klinck.
2010. Computergest¨utzter Multiple Choice C-Test in der
Bundesagentur f¨ur Arbeit: Bundesweite Erprobung und
Einf¨uhrung. In R¨udiger Grotjahn, editor, Der C-Test:
Beitr¨age aus der aktuellen Forschung The C-Test: Contri-
butions from Current Research, pages 231–264. Peter Lang
International Academic Publishers.
Tadamitsu Kamimoto. 1993. Tailoring the Test to Fit the Stu-
dents: Improvement of the C-Test through Classical Item
Analysis. Language Laboratory, 30:47–61, November.
Christine Klein-Braley and Ulrich Raatz. 1984. A survey of re-
search on the C-Test. Language Testing, 1(2):134–146, De-
cember.
Christine Klein-Braley. 1984. Advance Prediction of Difficulty
with C-Tests. In Terry Culhane, Christine Klein-Braley, and
Douglas K. Stevenson, editors, Practice and problems in lan-
guage testing, volume 7, pages 97–112.
Christine Klein-Braley. 1985. A cloze-up on the C-Test: a
study in the construct validation of authentic tests. Language
Testing, 2(1):76–104.
Christine Klein-Braley. 1996. Towards a theory of C-Test pro-
cessing. In R¨udiger Grotjahn, editor, Der C-Test. Theoretis-
che Grundlagen und praktische Anwendungen 3, pages 23–
94. Brockmeyer, Bochum.
Christine Klein-Braley. 1997. C-Tests in the context of re-
duced redundancy testing: an appraisal. Language Testing,
14(1):47–84, March.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 2006. A
computer-aided environment for generating multiple-choice
test items. Natural Language Engineering, 12(2):177–194,
May.
Jack Mostow and Hyeju Jang. 2012. Generating Diagnostic
Multiple Choice Comprehension Cloze Questions. In Pro-
ceedings of the Seventh Workshop on Building Educational
Applications Using NLP, pages 136–146. Association for
Computational Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and Smaller N-Gram
Language Models. In Proceedings of the 49th Annual Meet-
ing of the ACL: Human Language Technologies, volume 1,
pages 258–267. Association for Computational Linguistics.
Laura Perez-Beltrachini, Claire Gardent, and German
Kruszewski. 2012. Generating Grammar Exercises.
pages 147–156.
Ulrich Raatz and Christine Klein-Braley. 2002. Introduction to
language testing and to C-Tests. University language testing
and the C-test, pages 75–91.
Keisuke Sakaguchi, Yuki Arase, and Mamoru Komachi. 2013.
Discriminative Approach to Fill-in-the-Blank Quiz Genera-
tion for Language Learners.
G¨unther Sigott. 1995. The C-test: some factors of difficulty.
AAA. Arbeiten aus Anglistik und Amerikanistik, 20(1):43–54.
G¨unther Sigott. 2006. How fluid is the C-Test construct. In
R¨udiger Grotjahn and G¨unther Sigott, editors, Der C-Test:
Theorie, Empirie, Anwendungen The C-Test: Theory, Empir-
ical Research, Applications, pages 139–146. Peter Lang.
David Singleton and David Little. 1991. The second lan-
guage lexicon: some evidence from university-level learners
of French and German. Second Language Research, 7:61–
81.
Adam Skory and Maxine Eskenazi. 2010. Predicting Cloze
Task Quality for Vocabulary Training. In The 5th Workshop
on Innovative Use of NLP for Building Educational Applica-
tions (NAACL-HLT).
Bernard Spolsky. 1969. Reduced Redundancy as a Language
Testing Tool. In G.E. Perren and J.L.M. Trim, editors, Appli-
cations of linguistics, pages 383–390. Cambridge University
Press, Cambridge, August.
Oliver Traxel and Bettina Dresemann. 2010. Collect, callibrate,
compare: A practical approach to estimating the difficulty
</reference>
<page confidence="0.976311">
528
</page>
<reference confidence="0.998875">
of C-Test items. In R¨udiger Grotjahn, editor, Der C-Test:
Beitr¨age aus der aktuellen Forschung The C-Test: Contribu-
tions from Current Research, pages 57–69. Peter Lang Inter-
national Academic Publishers, Frankfurt a.M.
Lev Vygotsky. 1978. Mind in society: The development of
higher psychological processes. Harvard University Press.
Torsten Zesch and Oren Melamud. 2014. Automatic genera-
tion of challenging distractors using context-sensitive infer-
ence rules. In Proceedings of the Ninth Workshop on In-
novative Use of NLP for Building Educational Applications,
pages 143–148. Association for Computational Linguistics.
</reference>
<page confidence="0.999169">
529
530
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.401144">
<title confidence="0.999704">Predicting the Difficulty of Language Proficiency Tests</title>
<author confidence="0.80013">Torsten</author>
<affiliation confidence="0.627321">Lab, Technische Universit¨at Lab, German Institute for Educational Technology Lab, University of</affiliation>
<web confidence="0.985384">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.997885181818182">Language proficiency tests are used to evaluate and compare the progress of language learners. We present an approach for automatic difficulty prediction of C-tests that performs on par with human experts. On the basis of detailed analysis of newly collected data, we develop a model for C-test difficulty introducing four dimensions: solution difficulty, candidate ambiguity, inter-gap dependency, and paragraph difficulty. We show that cues from all four dimensions contribute to C-test difficulty.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Manish Agarwal</author>
<author>Prashanth Mannem</author>
</authors>
<title>Automatic Gap-fill Question Generation from Text Books.</title>
<date>2011</date>
<booktitle>Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>56--64</pages>
<contexts>
<context position="7857" citStr="Agarwal and Mannem, 2011" startWordPosition="1244" endWordPosition="1247">ng only recognition. However, Jakschik et al. (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test beca</context>
</contexts>
<marker>Agarwal, Mannem, 2011</marker>
<rawString>Manish Agarwal and Prashanth Mannem. 2011. Automatic Gap-fill Question Generation from Text Books. Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications, pages 56–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esmat Babaii</author>
<author>Hasan Ansary</author>
</authors>
<title>The C-test: a valid operationalization of reduced redundancy principle?</title>
<date>2001</date>
<journal>System,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="6638" citStr="Babaii and Ansary, 2001" startWordPosition="1050" endWordPosition="1053"> and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995). For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option. In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students’ abilities on less text. As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining on</context>
<context position="8940" citStr="Babaii and Ansary, 2001" startWordPosition="1417" endWordPosition="1420">didate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: “Which skills does the C-test measure?” While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to </context>
<context position="17583" citStr="Babaii and Ansary, 2001" startWordPosition="2882" endWordPosition="2885">ble to understand the texts and left entire paragraphs empty. Figure 4: C-Test Difficulty Model Hence, the potential problems for foreign language learners are manifold and hard to anticipate. We took a closer look at the false answers in order to gain deeper understanding of the dimensions that lead to wrong answers and therefore to higher difficulty. We find that the difficulty of C-tests is determined by a combination of many factors. In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001). Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving. Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4). Micro-level processing only deals with the solution of the gap and its surrounding micro context. The micro context consists of the word preceding the solution, the solution, and the following word. Both, the preceding and the following word are intact (i.e. not mutilated as gap) and can be used as solution hints by eve</context>
</contexts>
<marker>Babaii, Ansary, 2001</marker>
<rawString>Esmat Babaii and Hasan Ansary. 2001. The C-test: a valid operationalization of reduced redundancy principle? System, 29(2):209–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Beinborn</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Cognate Production using Character-based Machine Translation.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>883--891</pages>
<contexts>
<context position="23679" citStr="Beinborn et al., 2013" startWordPosition="3872" endWordPosition="3875">e words might not be part of the students active vocabulary and are only guessed because they occur as cognates in the students L1. This is supported by the fact that many of the cognate answers resemble orthographic principles from other languages, e.g. for skeletons we find *skellets, *skelleton(s), *skelets, *skelletts, *skeletton(s), *skeltons, *skeletes, and *skelette(s).11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists. We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013). In addition, we consult the COCA list of academic words12 and a list of words with latin roots.13 Inflection Many errors are caused by wrong morphological inflection as in this example: And in har times like these, ... [harder] The base form hard (72) is provided more often than the correct comparative harder (48), although it is too short. Other inflection errors are caused by singular/plural and adjective/adverb confusion. In order to account for this phenomenon, we test whether the solution is in lemma form or carries any inflection markers using a lemmatizer. We also check whether the wo</context>
</contexts>
<marker>Beinborn, Zesch, Gurevych, 2013</marker>
<rawString>Lisa Beinborn, Torsten Zesch, and Iryna Gurevych. 2013. Cognate Production using Character-based Machine Translation. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 883–891. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Beinborn</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Readability for foreign language learning: The importance of cognates.</title>
<date>2014</date>
<journal>International Journal of Applied Linguistics,</journal>
<pages>136--162</pages>
<contexts>
<context position="22282" citStr="Beinborn et al., 2014" startWordPosition="3649" endWordPosition="3652">ley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty. 6In all examples, we only highlight a single gap to illustrate a certain phenomenon. 7http://www.grsampson.net/RSue.html 8http://www.danielnaber.de/jwordsplitter/index en.html The word class is determined by identifying the part-ofspeech (POS) tag. As additional feature, we calculate the probability of the POS sequence of the micro context. Cognateness Frequency is not the only indicator for word familiarity and can sometimes even be misleading (Beinborn et al., 2014). Many solution words are cognates, i.e. they are very similar to words in other languages like information or laboratory. In reading comprehension, cognates are known as facilitators because their meaning can be deducted from the form similarity to a word in the mother tongue. We therefore assumed that cognate gaps are easier to solve. However, we observe that they are more likely to trigger production problems. In the 20 gaps with the highest answer variety (33 or more different answers), all solutions have a Latin stem.9 The 20 gaps with the lowest answer variety (5 or less different answer</context>
</contexts>
<marker>Beinborn, Zesch, Gurevych, 2014</marker>
<rawString>Lisa Beinborn, Torsten Zesch, and Iryna Gurevych. 2014. Readability for foreign language learning: The importance of cognates. International Journal of Applied Linguistics, pages 136–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram corpus version 1.1. Linguistic Data Consortium.</title>
<date>2006</date>
<contexts>
<context position="20256" citStr="Brants and Franz, 2006" startWordPosition="3319" endWordPosition="3322">stimated by the word frequency; more frequent words are more likely to be known. We therefore calculate the frequency of the solution and also its length as more frequent words tend to be shorter in English. In previous work, Brown (1989) calculates the frequency of the target word on the basis of the current test text. This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty. Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison. Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g. in the micro context States o America, the candidate of is clearly favored, while in the context write o paper, the candidates on, our and off are more probable. In order to account for typical phrases, we calculate the normalized trigram probability of the micro context. Even if a word seems familiar to a learner, it might be problematic when used in a compound (e.g. coastline) because the prefix only provide</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram corpus version 1.1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Bresnihan</author>
<author>Stratton Ray</author>
</authors>
<title>C-tests and the usefulness of non-linguistic instructions.</title>
<date>1992</date>
<booktitle>Der C-Test. Theoretische Grundlagen und praktische Anwendungen 1,</booktitle>
<pages>193--216</pages>
<editor>In R¨udiger Grotjahn, editor,</editor>
<location>Brockmeyer, Bochum.</location>
<contexts>
<context position="30146" citStr="Bresnihan and Ray (1992)" startWordPosition="4888" endWordPosition="4891">th answers relative to all false answers is higher for stronger (0.45) than for weaker (0.32) students. Length violations can be caused by candidates that seem viable for the context and are more accessible than the solution or by wrong inflection of the word ending. In other cases, it is obvious that the student does not find a proper solution and provides just anything that remotely fits. It would be interesting to repeat the test with the constraint that false answers have a negative influence on the overall score in order to find out whether the students are aware of the length violation. Bresnihan and Ray (1992) show that students perform 522 better on the C-test, if the length of the solution is graphically indicated by dashes or dots which supports the assumption that length violations are often not noticed in the standard C-test. As we want to account for this phenomenon, we decide to relax the length constraint. We only allow a length tolerance of 1, i.e. for a prefix of length 3, we consider candidates with 5 to 8 (instead of 6 to 7) characters, as the candidate space would be too large otherwise. We noticed that even candidates with wrongly spelled prefix can be competitors, e.g. some students </context>
</contexts>
<marker>Bresnihan, Ray, 1992</marker>
<rawString>Brian Bresnihan and Stratton Ray. 1992. C-tests and the usefulness of non-linguistic instructions. In R¨udiger Grotjahn, editor, Der C-Test. Theoretische Grundlagen und praktische Anwendungen 1, pages 193–216. Brockmeyer, Bochum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan C Brown</author>
<author>Gwen A Frishkoff</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Automatic Question Generation for Vocabulary Assessment. In</title>
<date>2005</date>
<booktitle>HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>819--826</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7970" citStr="Brown et al., 2005" startWordPosition="1263" endWordPosition="1266">ltiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that de</context>
</contexts>
<marker>Brown, Frishkoff, Eskenazi, 2005</marker>
<rawString>Jonathan C Brown, Gwen A Frishkoff, and Maxine Eskenazi. 2005. Automatic Question Generation for Vocabulary Assessment. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 819–826, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Dean Brown</author>
</authors>
<title>Cloze item difficulty.</title>
<date>1989</date>
<pages>11--46</pages>
<contexts>
<context position="9919" citStr="Brown (1989)" startWordPosition="1578" endWordPosition="1579">nd type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C-test that only contains selected gaps in order to better discriminate between the students. However, the gap selection is based on previous test results instead of specific gap features and thus cannot be applied on new tests. Previous work on gap difficulty is based on correlation analyses. Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty. Sigott (1995) exam2It should be noted, that their definition of “vocabulary” is very wide. 3http://www.ondaf.de 518 Error rate T1 T2 T3 T4 Participants 357 156 147 160 Mean error rate .31 .46 .37 .36 Standard deviation .21 .26 .24 .28 Table 1: Analysis of text-level test difficulty ines word frequency, word class, and constituent type of the gap for the C-test and finds high correlation only for the word frequency. Klein-Braley (1996) identifies additional error patt</context>
<context position="19871" citStr="Brown (1989)" startWordPosition="3253" endWordPosition="3254"> (example 1) and the most difficult gap (example 2), it is obvious that you is easier because it is more familiar to the participants than plentiful.6 1. Ify are looking for new experiences, ... [you] 2. ..., people may try self-employment because the opportunities seem plen and financing is easy to get. [plentiful] The probability that a learner knows a word is usually estimated by the word frequency; more frequent words are more likely to be known. We therefore calculate the frequency of the solution and also its length as more frequent words tend to be shorter in English. In previous work, Brown (1989) calculates the frequency of the target word on the basis of the current test text. This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty. Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison. Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g. in the micro context States </context>
<context position="21597" citStr="Brown (1989)" startWordPosition="3553" endWordPosition="3554">English dictionary.8 Another issue are polysemous words, as learners might know one sense of a word but not be aware of the existence of a second sense. Polysemy interferes with frequency, e.g. the word well has a high frequency, but it occurs only rarely in its sense fountain. In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012). The two senses of well also differ in their word class. The word class has been studied as a difficulty indicator by several researchers but with mixed results. Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty. 6In all examples, we only highlight a single gap to illustrate a certain phenomenon. 7http://www.grsampson.net/RSue.html 8http://www.danielnaber.de/jwordsplitter/index en.html The word class is determined by identifying the part-ofspeech (POS) tag. As additional feature, we calculate the probability of the POS sequence of the micro context. Cognateness Frequency is not the only indica</context>
<context position="24981" citStr="Brown (1989)" startWordPosition="4065" endWordPosition="4066">ncient, private, design, concentrations, state-of-the-art, scientists, modern, examined, constant, essential, stable, entering, basis, synthetic, cost, demands 10longer, coffee, coffee, in, water, very, give, you, for, people, living, other, number, water, water, from, over, you, over 11DE: Skelett, FR: squelette, ES: esqueleto, NL: skelet 12http://www.academicvocabulary.info/ 13http://en.wikipedia.org/wiki/List of Latin words with English derivatives 521 (i.e. not as a gap) because it facilitates the correct production for the student. This feature is comparable to the semantic cache used by Brown (1989). Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems. The spelling of a word is more difficult, if it contains a rare sequence of characters. The word appropriate, for example, triggers 69 different answers, 40 of them were provided only once. In addition, a spelling error is more likely to occur, in words with rare grapheme-phoneme mapping as in Wednesday. We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011). In addition, we build a phonetic model using phonetisaur</context>
<context position="34726" citStr="Brown (1989)" startWordPosition="5676" endWordPosition="5677">n more difficult paragraphs. As we have seen in Section 3.1, the difficulty of the gaps within one paragraph varies strongly. We therefore assume that the paragraph difficulty only adds a constant effect to the overall gap difficulty. The difficulty of a paragraph is inversely related to its readability. We calculate the following readability features for the whole paragraph and for the sentence containing the gap. Average word and sentence length are the underlying basis of traditional readability measures such as Flesch-Kincaid and Fry which correlate with cloze test difficulty according to Brown (1989). We calculate both, but do not find much variety as the paragraphs in our data are all of comparable length (64-99 words, 3-7 sentences, 4.85 characters per word). The type-token ratio, the verb variation, and the pronoun ratio are used as indicators for lexical diversity and referentiality. Klein-Braley (1984) already determined the type-token ratio as useful cue for paragraph difficulty prediction. We also use syntactic readability features such as the number of entity mentions, the number of certain POS types (e.g. noun, determiner, adjective) and the number of certain phrase patterns (e.g</context>
</contexts>
<marker>Brown, 1989</marker>
<rawString>James Dean Brown. 1989. Cloze item difficulty. JALTjournal, 11:46–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Chapelle</author>
</authors>
<title>Are C-tests valid measures for L2 vocabulary research?</title>
<date>1994</date>
<journal>Second Language Research,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="8984" citStr="Chapelle, 1994" startWordPosition="1426" endWordPosition="1427">d the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: “Which skills does the C-test measure?” While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap l</context>
</contexts>
<marker>Chapelle, 1994</marker>
<rawString>C. A. Chapelle. 1994. Are C-tests valid measures for L2 vocabulary research? Second Language Research, 10(2):157–187, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew D Cohen</author>
</authors>
<title>The C-Test in Hebrew.</title>
<date>1984</date>
<journal>Language Testing,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="7173" citStr="Cohen (1984)" startWordPosition="1137" endWordPosition="1138">liability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995). For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option. In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students’ abilities on less text. As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition. However, Jakschik et al. (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats </context>
</contexts>
<marker>Cohen, 1984</marker>
<rawString>Andrew D. Cohen. 1984. The C-Test in Hebrew. Language Testing, 1(2):221–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Daxenberger</author>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>DKPro TC: A Java-based Framework for Supervised Learning Experiments on Textual Data.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>61--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="39197" citStr="Daxenberger et al., 2014" startWordPosition="6413" endWordPosition="6416">e training set for regression and classification prediction (both trained on support vector machines). Classification results are the weighted average of precision (P), recall (R) and Fl-measure over all four classes. 6 Automatic Difficulty Prediction Our difficulty prediction approach is based on the model described in the previous section. We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014). We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18 6.1 Classification vs Regression For the human annotation, we used a classification scheme because assigning difficulty scores on a finegrained numerical scale would be too challenging even for experienced teachers. However, as the actual error rates are continuously distributed, gaps that are close to the class boundaries are more likely to be mislabeled. Therefore we also test regression prediction using the actual error rates instead of the artificially determined classes. We perform leave-one-out testing on the training set in order to determine the best approach. We compare our model </context>
</contexts>
<marker>Daxenberger, Ferschke, Gurevych, Zesch, 2014</marker>
<rawString>Johannes Daxenberger, Oliver Ferschke, Iryna Gurevych, and Torsten Zesch. 2014. DKPro TC: A Java-based Framework for Supervised Learning Experiments on Textual Data. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 61–66. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Eckart de Castilho</author>
<author>Iryna Gurevych</author>
</authors>
<title>A broad-coverage collection of portable NLP components for building shareable analysis pipelines.</title>
<date>2014</date>
<booktitle>In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014,</booktitle>
<pages>1--11</pages>
<marker>de Castilho, Gurevych, 2014</marker>
<rawString>Richard Eckart de Castilho and Iryna Gurevych. 2014. A broad-coverage collection of portable NLP components for building shareable analysis pipelines. In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014, pages 1–11, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Eckes</author>
<author>R¨udiger Grotjahn</author>
</authors>
<title>A closer look at the construct validity of C-tests.</title>
<date>2006</date>
<journal>Language Testing,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2651" citStr="Eckes and Grotjahn, 2006" startWordPosition="409" endWordPosition="412">language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969). It is based on the idea that “natural language is redundant” and that more advanced learners can be distinguished from beginners by their ability to deal with reduced redundancy. For language testing, redundancy can be reduced by eliminating words from a text and asking the learner to fill in the gap, also known as the cloze test. The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006). We present an approach for determining the difficulty of C-tests that overcomes the mentioned drawbacks of subjective evaluation by teachers. Our approach is based on objective measurable properties and thus produces consistent results. We show that our approach performs on par with human experts and analyze to which extent Ctest difficulty is determined by individual gap properties (micro-level processing) and higher level dependencies (macro-level processing). On the theoretical level, our model provides new insights into the factors that affect difficulty in reduced redundancy testing. On</context>
<context position="8844" citStr="Eckes and Grotjahn, 2006" startWordPosition="1402" endWordPosition="1405">d construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: “Which skills does the C-test measure?” While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model</context>
</contexts>
<marker>Eckes, Grotjahn, 2006</marker>
<rawString>Thomas Eckes and R¨udiger Grotjahn. 2006. A closer look at the construct validity of C-tests. Language Testing, 23(3):290–325, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Eckes</author>
</authors>
<title>Item banking for C-tests: A polytomous Rasch modeling approach.</title>
<date>2011</date>
<journal>Psychological Test and Assessment Modeling,</journal>
<volume>53</volume>
<issue>4</issue>
<contexts>
<context position="9386" citStr="Eckes (2011)" startWordPosition="1489" endWordPosition="1490">proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C-test that only contains selected gaps in order to better discriminate between the students. However, the gap selection is based on previous test results instead of specific gap features and thus cannot be applied on new tests. Previous work on gap difficulty is based on correlation analyses. Brown (1989) identifies the word class, the local word frequency, and readabili</context>
</contexts>
<marker>Eckes, 2011</marker>
<rawString>Thomas Eckes. 2011. Item banking for C-tests: A polytomous Rasch modeling approach. Psychological Test and Assessment Modeling, 53(4):414–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R¨udiger Grotjahn</author>
<author>Brigitte Stemmer</author>
</authors>
<title>C-Tests and language processing.</title>
<date>2002</date>
<booktitle>University language testing and the C-Test,</booktitle>
<pages>115--130</pages>
<editor>In James A. Coleman, R¨udiger Grotjahn, and Ulrich Raatz, editors,</editor>
<publisher>AKS-Verlag,</publisher>
<location>Bochum.</location>
<contexts>
<context position="17653" citStr="Grotjahn and Stemmer, 2002" startWordPosition="2890" endWordPosition="2893">e 4: C-Test Difficulty Model Hence, the potential problems for foreign language learners are manifold and hard to anticipate. We took a closer look at the false answers in order to gain deeper understanding of the dimensions that lead to wrong answers and therefore to higher difficulty. We find that the difficulty of C-tests is determined by a combination of many factors. In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001). Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving. Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4). Micro-level processing only deals with the solution of the gap and its surrounding micro context. The micro context consists of the word preceding the solution, the solution, and the following word. Both, the preceding and the following word are intact (i.e. not mutilated as gap) and can be used as solution hints by every learner, independent of the performance on the other gaps. In order</context>
</contexts>
<marker>Grotjahn, Stemmer, 2002</marker>
<rawString>R¨udiger Grotjahn and Brigitte Stemmer. 2002. C-Tests and language processing. In James A. Coleman, R¨udiger Grotjahn, and Ulrich Raatz, editors, University language testing and the C-Test, pages 115–130. AKS-Verlag, Bochum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R¨udiger Grotjahn</author>
<author>Christine Klein-Braley</author>
<author>Ulrich Raatz</author>
</authors>
<title>C-Tests: an overview.</title>
<date>2002</date>
<booktitle>University language testing and the C-Test,</booktitle>
<pages>93--114</pages>
<editor>In James A. Coleman, R¨udiger Grotjahn, and Ulrich Raatz, editors,</editor>
<publisher>AKS-Verlag,</publisher>
<location>Bochum.</location>
<contexts>
<context position="4827" citStr="Grotjahn et al. (2002)" startWordPosition="753" endWordPosition="756">cs, vol. 2, pp. 517–529, 2014. Action Editor: Chris Callison-Burch. Submission batch: 4/2014; Revision batch 8/2014; Published 11/2014. c�2014 Association for Computational Linguistics. After an unaltered introductory sentence, every second word is transformed into a gap. When the intended number of gaps is reached (usually 20), the rest of the text is left intact. For each gap, the smaller half of the word is provided and the missing part has to be completed by the learner. Since its introduction, the C-test has been researched from many angles and has been adapted for over 20 languages (see Grotjahn et al. (2002) for an overview). 2.1 C-Tests vs Cloze Tests The C in C-test stands for its origin in the cloze test. In cloze tests, full words are transformed into gaps according to a fixed deletion pattern (e.g. every 7th word). The main problem with cloze tests is the ambiguity of the solution. Unless function words are deleted, the gap allows many alternative solutions such as synonyms and hypernyms, but also entirely different words that change the meaning of the text but also fit the context. Language teachers have proposed two ways of dealing with this ambiguity: the application of relaxed scoring sc</context>
</contexts>
<marker>Grotjahn, Klein-Braley, Raatz, 2002</marker>
<rawString>R¨udiger Grotjahn, Christine Klein-Braley, and Ulrich Raatz. 2002. C-Tests: an overview. In James A. Coleman, R¨udiger Grotjahn, and Ulrich Raatz, editors, University language testing and the C-Test, pages 93–114. AKS-Verlag, Bochum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>A Large-Scale Unified Lexical-Semantic Resource Based on LMF.</title>
<date>2012</date>
<booktitle>Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>580--590</pages>
<contexts>
<context position="21422" citStr="Gurevych et al., 2012" startWordPosition="3520" endWordPosition="3523">ompound (e.g. coastline) because the prefix only provides information about the first part of the word. In our approach, compounds are detected using a word splitting algorithm with an English dictionary.8 Another issue are polysemous words, as learners might know one sense of a word but not be aware of the existence of a second sense. Polysemy interferes with frequency, e.g. the word well has a high frequency, but it occurs only rarely in its sense fountain. In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012). The two senses of well also differ in their word class. The word class has been studied as a difficulty indicator by several researchers but with mixed results. Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty. 6In all examples, we only highlight a single gap to illustrate a certain phenomenon. 7http://www.grsampson.net/RSue.html 8http://www.danielnaber.de/jwordsplitter/index en.html The word class is determined by iden</context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M Meyer, and Christian Wirth. 2012. A Large-Scale Unified Lexical-Semantic Resource Based on LMF. Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 580–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<pages>11--1</pages>
<contexts>
<context position="39139" citStr="Hall et al., 2009" startWordPosition="6404" endWordPosition="6407"> 3: Results for leave-one-out crossvalidation on the training set for regression and classification prediction (both trained on support vector machines). Classification results are the weighted average of precision (P), recall (R) and Fl-measure over all four classes. 6 Automatic Difficulty Prediction Our difficulty prediction approach is based on the model described in the previous section. We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych, 2014). We then perform experiments with different datasets and classifiers using Weka (Hall et al., 2009) through the DKPro TC framework (Daxenberger et al., 2014).18 6.1 Classification vs Regression For the human annotation, we used a classification scheme because assigning difficulty scores on a finegrained numerical scale would be too challenging even for experienced teachers. However, as the actual error rates are continuously distributed, gaps that are close to the class boundaries are more likely to be mislabeled. Therefore we also test regression prediction using the actual error rates instead of the artificially determined classes. We perform leave-one-out testing on the training set in o</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Harsch</author>
<author>Johannes Hartig</author>
</authors>
<title>Empirische und inhaltliche Analyse lokaler Abh¨angigkeiten im C-Test.</title>
<date>2010</date>
<booktitle>Der C-Test: Beitr¨age aus der aktuellen Forschung,</booktitle>
<pages>193--204</pages>
<editor>In R¨udiger Grotjahn, editor,</editor>
<publisher>Peter Lang.</publisher>
<contexts>
<context position="32164" citStr="Harsch and Hartig (2010)" startWordPosition="5235" endWordPosition="5238">n and the number of candidates above a fixed threshold. In addition, we take the intersection of the best candidates from the above rankings, combine them into a set of top candidates that are likely to compete with the solution and determine its size. Moreover, we calculate the maximum string similarity of the candidates with the solution in order to capture very close variants (e.g. base and basis). 4.3 Inter-gap dependency This macro-level dimension assesses the dependency of the current gap on previous gaps: can it be solved, even if the previous gap has not been solved? In previous work, Harsch and Hartig (2010) examine dependencies between individual gaps using a Rasch testlet model and find that some gaps strongly depend on each other, while others can be solved independently. At the same time, fertility is set to fall as women leave childbirth la and la . [later] In these gaps, later is repeated which makes it easy to fill in the second gap, if the first one is solved. The dependency of a gap is related to its position and the difficulty of the preceding word. If a gap is preceded by a very difficult gap, the available context is damaged which can have an effect on the difficulty of the following </context>
</contexts>
<marker>Harsch, Hartig, 2010</marker>
<rawString>Claudia Harsch and Johannes Hartig. 2010. Empirische und inhaltliche Analyse lokaler Abh¨angigkeiten im C-Test. In R¨udiger Grotjahn, editor, Der C-Test: Beitr¨age aus der aktuellen Forschung, pages 193–204. Peter Lang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Heilman</author>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Combining lexical and grammatical features to improve readability measures for first and second language texts.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>460--467</pages>
<contexts>
<context position="7949" citStr="Heilman et al., 2007" startWordPosition="1259" endWordPosition="1262">n test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search fo</context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>Michael J. Heilman, Kevyn Collins-Thompson, Jamie Callan, and Maxine Eskenazi. 2007. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of NAACL-HLT, pages 460–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jafarpur</author>
</authors>
<title>Is C-testing superior to cloze?</title>
<date>1995</date>
<journal>Language Testing,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="6675" citStr="Jafarpur, 1995" startWordPosition="1056" endWordPosition="1057">tractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995). For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option. In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students’ abilities on less text. As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition. However, Jakschik et </context>
</contexts>
<marker>Jafarpur, 1995</marker>
<rawString>A. Jafarpur. 1995. Is C-testing superior to cloze? Language Testing, 12(2):194–216, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Jakschik</author>
<author>Hella Klemmert</author>
<author>Dorothea Klinck</author>
</authors>
<title>Computergest¨utzter Multiple Choice C-Test in der Bundesagentur f¨ur Arbeit: Bundesweite Erprobung und Einf¨uhrung.</title>
<date>2010</date>
<booktitle>Der C-Test: Beitr¨age aus der aktuellen Forschung The C-Test: Contributions from Current Research,</booktitle>
<pages>231--264</pages>
<editor>In R¨udiger Grotjahn, editor,</editor>
<publisher>Peter Lang International Academic Publishers.</publisher>
<contexts>
<context position="7285" citStr="Jakschik et al. (2010)" startWordPosition="1155" endWordPosition="1158">arpur, 1995). For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option. In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students’ abilities on less text. As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition. However, Jakschik et al. (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), voca</context>
</contexts>
<marker>Jakschik, Klemmert, Klinck, 2010</marker>
<rawString>Gerhard Jakschik, Hella Klemmert, and Dorothea Klinck. 2010. Computergest¨utzter Multiple Choice C-Test in der Bundesagentur f¨ur Arbeit: Bundesweite Erprobung und Einf¨uhrung. In R¨udiger Grotjahn, editor, Der C-Test: Beitr¨age aus der aktuellen Forschung The C-Test: Contributions from Current Research, pages 231–264. Peter Lang International Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadamitsu Kamimoto</author>
</authors>
<title>Tailoring the Test to Fit the Students: Improvement of the C-Test through Classical Item Analysis. Language Laboratory,</title>
<date>1993</date>
<pages>30--47</pages>
<contexts>
<context position="9522" citStr="Kamimoto (1993)" startWordPosition="1511" endWordPosition="1512">ar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C-test that only contains selected gaps in order to better discriminate between the students. However, the gap selection is based on previous test results instead of specific gap features and thus cannot be applied on new tests. Previous work on gap difficulty is based on correlation analyses. Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty. Sigott (1995) exam2It should be noted, that their definition of “vocabular</context>
</contexts>
<marker>Kamimoto, 1993</marker>
<rawString>Tadamitsu Kamimoto. 1993. Tailoring the Test to Fit the Students: Improvement of the C-Test through Classical Item Analysis. Language Laboratory, 30:47–61, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Klein-Braley</author>
<author>Ulrich Raatz</author>
</authors>
<title>A survey of research on the C-Test.</title>
<date>1984</date>
<journal>Language Testing,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="6368" citStr="Klein-Braley and Raatz (1984)" startWordPosition="1008" endWordPosition="1011">atz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995). For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option. In addition, the prefix hint allows for a narrower </context>
</contexts>
<marker>Klein-Braley, Raatz, 1984</marker>
<rawString>Christine Klein-Braley and Ulrich Raatz. 1984. A survey of research on the C-Test. Language Testing, 1(2):134–146, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Klein-Braley</author>
</authors>
<title>Advance Prediction of Difficulty with C-Tests.</title>
<date>1984</date>
<booktitle>Practice and problems in language testing,</booktitle>
<volume>7</volume>
<pages>97--112</pages>
<editor>In Terry Culhane, Christine Klein-Braley, and Douglas K. Stevenson, editors,</editor>
<contexts>
<context position="9205" citStr="Klein-Braley (1984)" startWordPosition="1462" endWordPosition="1463">iculty of C-tests is tightly connected to the question of construct validity: “Which skills does the C-test measure?” While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C-test that only contains selected gaps in order to better discriminate between the students. However, the gap selection is based on previous test results instead of specific gap features and t</context>
<context position="35039" citStr="Klein-Braley (1984)" startWordPosition="5726" endWordPosition="5727">We calculate the following readability features for the whole paragraph and for the sentence containing the gap. Average word and sentence length are the underlying basis of traditional readability measures such as Flesch-Kincaid and Fry which correlate with cloze test difficulty according to Brown (1989). We calculate both, but do not find much variety as the paragraphs in our data are all of comparable length (64-99 words, 3-7 sentences, 4.85 characters per word). The type-token ratio, the verb variation, and the pronoun ratio are used as indicators for lexical diversity and referentiality. Klein-Braley (1984) already determined the type-token ratio as useful cue for paragraph difficulty prediction. We also use syntactic readability features such as the number of entity mentions, the number of certain POS types (e.g. noun, determiner, adjective) and the number of certain phrase patterns (e.g. verbal phrase, noun phrase, subordinate phrase). Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction. Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010</context>
</contexts>
<marker>Klein-Braley, 1984</marker>
<rawString>Christine Klein-Braley. 1984. Advance Prediction of Difficulty with C-Tests. In Terry Culhane, Christine Klein-Braley, and Douglas K. Stevenson, editors, Practice and problems in language testing, volume 7, pages 97–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Klein-Braley</author>
</authors>
<title>A cloze-up on the C-Test: a study in the construct validation of authentic tests.</title>
<date>1985</date>
<journal>Language Testing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="8879" citStr="Klein-Braley, 1985" startWordPosition="1408" endWordPosition="1409"> easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: “Which skills does the C-test measure?” While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-te</context>
</contexts>
<marker>Klein-Braley, 1985</marker>
<rawString>Christine Klein-Braley. 1985. A cloze-up on the C-Test: a study in the construct validation of authentic tests. Language Testing, 2(1):76–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Klein-Braley</author>
</authors>
<title>Towards a theory of C-Test processing.</title>
<date>1996</date>
<booktitle>Der C-Test. Theoretische Grundlagen und praktische Anwendungen 3,</booktitle>
<pages>23--94</pages>
<editor>In R¨udiger Grotjahn, editor,</editor>
<location>Brockmeyer, Bochum.</location>
<contexts>
<context position="10486" citStr="Klein-Braley (1996)" startWordPosition="1670" endWordPosition="1671">culty is based on correlation analyses. Brown (1989) identifies the word class, the local word frequency, and readability measures as factors correlating with cloze gap difficulty. Sigott (1995) exam2It should be noted, that their definition of “vocabulary” is very wide. 3http://www.ondaf.de 518 Error rate T1 T2 T3 T4 Participants 357 156 147 160 Mean error rate .31 .46 .37 .36 Standard deviation .21 .26 .24 .28 Table 1: Analysis of text-level test difficulty ines word frequency, word class, and constituent type of the gap for the C-test and finds high correlation only for the word frequency. Klein-Braley (1996) identifies additional error patterns related to production problems (right word stem in wrong form) and early closure, i.e. the solution works locally but not in the larger context. The cited works focus on the correlation between gap features and C-test difficulty but did not attempt to actually predict difficulty. In the following section, we present the results of our data analysis targeted towards building up a model for C-test difficulty. 3 Data Analysis For a better understanding of C-test difficulty, we need to perform data analysis. As suitable data was not available in digital form, </context>
<context position="21670" citStr="Klein-Braley (1996)" startWordPosition="3564" endWordPosition="3565">rs might know one sense of a word but not be aware of the existence of a second sense. Polysemy interferes with frequency, e.g. the word well has a high frequency, but it occurs only rarely in its sense fountain. In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012). The two senses of well also differ in their word class. The word class has been studied as a difficulty indicator by several researchers but with mixed results. Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty. 6In all examples, we only highlight a single gap to illustrate a certain phenomenon. 7http://www.grsampson.net/RSue.html 8http://www.danielnaber.de/jwordsplitter/index en.html The word class is determined by identifying the part-ofspeech (POS) tag. As additional feature, we calculate the probability of the POS sequence of the micro context. Cognateness Frequency is not the only indicator for word familiarity and can sometimes even be misleading (Beinborn e</context>
</contexts>
<marker>Klein-Braley, 1996</marker>
<rawString>Christine Klein-Braley. 1996. Towards a theory of C-Test processing. In R¨udiger Grotjahn, editor, Der C-Test. Theoretische Grundlagen und praktische Anwendungen 3, pages 23– 94. Brockmeyer, Bochum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Klein-Braley</author>
</authors>
<title>C-Tests in the context of reduced redundancy testing: an appraisal.</title>
<date>1997</date>
<journal>Language Testing,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="6658" citStr="Klein-Braley, 1997" startWordPosition="1054" endWordPosition="1055">ke sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995). For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option. In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students’ abilities on less text. As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition. Howe</context>
</contexts>
<marker>Klein-Braley, 1997</marker>
<rawString>Christine Klein-Braley. 1997. C-Tests in the context of reduced redundancy testing: an appraisal. Language Testing, 14(1):47–84, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Le An Ha, and Nikiforos Karamanis.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<marker>Mitkov, 2006</marker>
<rawString>Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 2006. A computer-aided environment for generating multiple-choice test items. Natural Language Engineering, 12(2):177–194, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jack Mostow</author>
<author>Hyeju Jang</author>
</authors>
<title>Generating Diagnostic Multiple Choice Comprehension Cloze Questions.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>136--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7831" citStr="Mostow and Jang, 2012" startWordPosition="1240" endWordPosition="1243">reading ability examining only recognition. However, Jakschik et al. (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the d</context>
</contexts>
<marker>Mostow, Jang, 2012</marker>
<rawString>Jack Mostow and Hyeju Jang. 2012. Generating Diagnostic Multiple Choice Comprehension Cloze Questions. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 136–146. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and Smaller N-Gram Language Models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies,</booktitle>
<volume>1</volume>
<pages>258--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25523" citStr="Pauls and Klein, 2011" startWordPosition="4151" endWordPosition="4154">e student. This feature is comparable to the semantic cache used by Brown (1989). Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems. The spelling of a word is more difficult, if it contains a rare sequence of characters. The word appropriate, for example, triggers 69 different answers, 40 of them were provided only once. In addition, a spelling error is more likely to occur, in words with rare grapheme-phoneme mapping as in Wednesday. We build a character-based language model that indicates the probability of a character sequence using BerkeleyLM (Pauls and Klein, 2011). In addition, we build a phonetic model using phonetisaurus, a statistical alignment algorithm that maps characters onto phonemes.14 Both models are trained only on words from the Basic English list in order to reflect the knowledge of a language learner.15 Based on this scarce data, the phonetic model only learns the most frequent character-tophoneme mappings and assigns higher phonetic scores to less general letter sequences. We use this score as a feature and additionally calculate the string similarity between the output and the correct pronunciation in the CMU dictionary.16 Another sourc</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and Smaller N-Gram Language Models. In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies, volume 1, pages 258–267. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Perez-Beltrachini</author>
<author>Claire Gardent</author>
<author>German Kruszewski</author>
</authors>
<title>Generating Grammar Exercises.</title>
<date>2012</date>
<pages>147--156</pages>
<contexts>
<context position="8025" citStr="Perez-Beltrachini et al., 2012" startWordPosition="1270" endWordPosition="1273">riant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected </context>
</contexts>
<marker>Perez-Beltrachini, Gardent, Kruszewski, 2012</marker>
<rawString>Laura Perez-Beltrachini, Claire Gardent, and German Kruszewski. 2012. Generating Grammar Exercises. pages 147–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Raatz</author>
<author>Christine Klein-Braley</author>
</authors>
<title>Introduction to language testing and to C-Tests. University language testing and the C-test,</title>
<date>2002</date>
<pages>75--91</pages>
<contexts>
<context position="5766" citStr="Raatz and Klein-Braley, 2002" startWordPosition="908" endWordPosition="911"> deleted, the gap allows many alternative solutions such as synonyms and hypernyms, but also entirely different words that change the meaning of the text but also fit the context. Language teachers have proposed two ways of dealing with this ambiguity: the application of relaxed scoring schemes and the use of distractors. In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring. Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (198</context>
<context position="16642" citStr="Raatz and Klein-Braley (2002)" startWordPosition="2733" endWordPosition="2736"> answers are variants of the correct solution. The students recognize the solution word but fail to produce it correctly. Unfortunately, the line between a spelling error and a wrong solution cannot be clearly drawn. If a plural s is missing we cannot distinguish between a typo and lack of grammatical understanding. Spelling errors often also form new words e.g. of vs. off or then vs. than and we cannot decide whether it is a spelling error or a wrong word choice. As the generous time limit allows the students to revise their solutions for typos, we consider them as normal errors in line with Raatz and Klein-Braley (2002). 4 C-Test Difficulty Model Natural languages are complex and constantly developing constructs that include many exceptions to the rules. 4For each size, we calculate the error rate based on three randomly selected samples of participants and report the average result. 5Except for the weakest students who were not able to understand the texts and left entire paragraphs empty. Figure 4: C-Test Difficulty Model Hence, the potential problems for foreign language learners are manifold and hard to anticipate. We took a closer look at the false answers in order to gain deeper understanding of the di</context>
</contexts>
<marker>Raatz, Klein-Braley, 2002</marker>
<rawString>Ulrich Raatz and Christine Klein-Braley. 2002. Introduction to language testing and to C-Tests. University language testing and the C-test, pages 75–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keisuke Sakaguchi</author>
<author>Yuki Arase</author>
<author>Mamoru Komachi</author>
</authors>
<title>Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners.</title>
<date>2013</date>
<contexts>
<context position="6008" citStr="Sakaguchi et al., 2013" startWordPosition="947" endWordPosition="950">ty: the application of relaxed scoring schemes and the use of distractors. In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring. Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language </context>
</contexts>
<marker>Sakaguchi, Arase, Komachi, 2013</marker>
<rawString>Keisuke Sakaguchi, Yuki Arase, and Mamoru Komachi. 2013. Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unther Sigott</author>
</authors>
<title>The C-test: some factors of difficulty.</title>
<date>1995</date>
<journal>AAA. Arbeiten aus Anglistik und Amerikanistik,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="8858" citStr="Sigott, 1995" startWordPosition="1406" endWordPosition="1407"> are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: “Which skills does the C-test measure?” While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to c</context>
<context position="20093" citStr="Sigott (1995)" startWordPosition="3293" endWordPosition="3294">ry self-employment because the opportunities seem plen and financing is easy to get. [plentiful] The probability that a learner knows a word is usually estimated by the word frequency; more frequent words are more likely to be known. We therefore calculate the frequency of the solution and also its length as more frequent words tend to be shorter in English. In previous work, Brown (1989) calculates the frequency of the target word on the basis of the current test text. This is clearly a biased estimate of the frequency, but it is still identified as a good indicator for cloze gap difficulty. Sigott (1995) calculates the frequency of the solution word using counts from the SUSANNE corpus.7 For our calculations, we use the larger Web1T corpus (Brants and Franz, 2006) and extract normalized probabilities instead of absolute frequencies for better comparison. Furthermore, a gap is easier to solve, if the solution occurs in a very typical context, e.g. in the micro context States o America, the candidate of is clearly favored, while in the context write o paper, the candidates on, our and off are more probable. In order to account for typical phrases, we calculate the normalized trigram probability</context>
<context position="21740" citStr="Sigott (1995)" startWordPosition="3574" endWordPosition="3575">ond sense. Polysemy interferes with frequency, e.g. the word well has a high frequency, but it occurs only rarely in its sense fountain. In order to account for polysemy, we count the number of represented word senses for the solution in the lexical-semantic resource UBY (Gurevych et al., 2012). The two senses of well also differ in their word class. The word class has been studied as a difficulty indicator by several researchers but with mixed results. Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty. 6In all examples, we only highlight a single gap to illustrate a certain phenomenon. 7http://www.grsampson.net/RSue.html 8http://www.danielnaber.de/jwordsplitter/index en.html The word class is determined by identifying the part-ofspeech (POS) tag. As additional feature, we calculate the probability of the POS sequence of the micro context. Cognateness Frequency is not the only indicator for word familiarity and can sometimes even be misleading (Beinborn et al., 2014). Many solution words are cognates, i.e. they are very sim</context>
<context position="38433" citStr="Sigott (1995)" startWordPosition="6295" endWordPosition="6296">ty prediction is quite subjective. The mediocre human performance on the task reveals the complexity of predicting the elements of language that cause problems for foreign language learners. However, this strengthens the need for reliable prediction methods like the one described in this paper. Note that the automatic prediction is compared with the actual error rates, not the human predicated ones. Thus, it is possible to outperform human performance with automatic methods and provide a very helpful tool. Classification Regression P R Fl Pearson’s r RMSE Majority Baseline .19 .43 .26 .00 .25 Sigott (1995) .23 .40 .28 .34 .24 Our Approach .46 .48 .46 .64 .20 Human Median .56 .53 .54 - - Table 3: Results for leave-one-out crossvalidation on the training set for regression and classification prediction (both trained on support vector machines). Classification results are the weighted average of precision (P), recall (R) and Fl-measure over all four classes. 6 Automatic Difficulty Prediction Our difficulty prediction approach is based on the model described in the previous section. We extract the features using tools for natural language processing provided by DKPro Core (de Castilho and Gurevych,</context>
<context position="40005" citStr="Sigott (1995)" startWordPosition="6540" endWordPosition="6541">or experienced teachers. However, as the actual error rates are continuously distributed, gaps that are close to the class boundaries are more likely to be mislabeled. Therefore we also test regression prediction using the actual error rates instead of the artificially determined classes. We perform leave-one-out testing on the training set in order to determine the best approach. We compare our model against the human performance and two baselines: A naive one that predicts the majority class for classification and the mean value for regression and one that only uses the features proposed by Sigott (1995) (solution probability, word class of solution, and constituent type of gap). In Table 3, we report weighted precision, recall and Fl-measure over all classes for classification and Pearson correlation and root mean squared error for regression. It can be seen that our approach clearly outperforms the baselines in both cases. For classification, the human median annotation is better than our approach. In order to also compare our regression results to the human annotations, we map the numerical predictions back into classes according to the scheme explained in the previous subsection. The quad</context>
<context position="47153" citStr="Sigott (1995)" startWordPosition="7629" endWordPosition="7630"> features yields better results on the smaller training set, while the full model is better on larger data. In order to support the assumption that our model performs better with more data, we plot a learning curve (see Figure 6). We calculated the Pearson correlation for increasing sample sizes of randomly selected instances and average the results over 100 runs. The anomaly for smaller sample sizes can be explained by very high standard deviations. Starting from a sample size of about 70 instances, the learning curve proceeds as # LOOCV Train Train-Test LOOCV All Mean Baseline 1 .00 .00 .00 Sigott (1995) 7 .34 .38 .36 Full Model 87 .64 .32 .60 Selected Features 21 .68 .44 .57 Table 8: Results on the train and the test set expected and highlights the importance of a larger training set. Sample Size Figure 6: Learning curve for 10-fold cross-validation with increasing size of training data, results are averaged over 100 runs 6.4 Error Analysis Figure 7 shows that our prediction approach produces a few strong outliers for the test data. In particular, it strongly underestimates the error rate for some very easy gaps. We perform an error analysis on the 9 outliers. Underestimation In two underest</context>
</contexts>
<marker>Sigott, 1995</marker>
<rawString>G¨unther Sigott. 1995. The C-test: some factors of difficulty. AAA. Arbeiten aus Anglistik und Amerikanistik, 20(1):43–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unther Sigott</author>
</authors>
<title>How fluid is the C-Test construct.</title>
<date>2006</date>
<booktitle>Anwendungen The C-Test: Theory, Empirical Research, Applications,</booktitle>
<pages>139--146</pages>
<editor>In R¨udiger Grotjahn and G¨unther Sigott, editors, Der C-Test: Theorie, Empirie,</editor>
<publisher>Peter Lang.</publisher>
<contexts>
<context position="17624" citStr="Sigott, 2006" startWordPosition="2888" endWordPosition="2889">s empty. Figure 4: C-Test Difficulty Model Hence, the potential problems for foreign language learners are manifold and hard to anticipate. We took a closer look at the false answers in order to gain deeper understanding of the dimensions that lead to wrong answers and therefore to higher difficulty. We find that the difficulty of C-tests is determined by a combination of many factors. In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001). Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving. Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4). Micro-level processing only deals with the solution of the gap and its surrounding micro context. The micro context consists of the word preceding the solution, the solution, and the following word. Both, the preceding and the following word are intact (i.e. not mutilated as gap) and can be used as solution hints by every learner, independent of the performanc</context>
</contexts>
<marker>Sigott, 2006</marker>
<rawString>G¨unther Sigott. 2006. How fluid is the C-Test construct. In R¨udiger Grotjahn and G¨unther Sigott, editors, Der C-Test: Theorie, Empirie, Anwendungen The C-Test: Theory, Empirical Research, Applications, pages 139–146. Peter Lang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Singleton</author>
<author>David Little</author>
</authors>
<title>The second language lexicon: some evidence from university-level learners of French and German.</title>
<date>1991</date>
<journal>Second Language Research,</journal>
<volume>7</volume>
<contexts>
<context position="9013" citStr="Singleton and Little, 1991" startWordPosition="1428" endWordPosition="1432">straint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: “Which skills does the C-test measure?” While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991).2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicators – average sentence length and type-token ratio – obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool.3 Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C</context>
</contexts>
<marker>Singleton, Little, 1991</marker>
<rawString>David Singleton and David Little. 1991. The second language lexicon: some evidence from university-level learners of French and German. Second Language Research, 7:61– 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Skory</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Predicting Cloze Task Quality for Vocabulary Training.</title>
<date>2010</date>
<booktitle>In The 5th Workshop on Innovative Use of NLP for Building Educational Applications (NAACL-HLT).</booktitle>
<contexts>
<context position="7927" citStr="Skory and Eskenazi, 2010" startWordPosition="1254" endWordPosition="1258">est into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice. 2.2 Test Difficulty Previous works in the field of educational natural language processing approach language proficiency tests from a generation perspective. The focus is on generating closed formats such as multiple choice cloze tests (Mostow and Jang, 2012; Agarwal and Mannem, 2011; Mitkov et al., 2006), vocabulary exercises (Skory and Eskenazi, 2010; Heilman et al., 2007; Brown et al., 2005) and grammar exercises (Perez-Beltrachini et al., 2012). The difficulty of these exercises is usually determined by the choice of distractors as students have to discriminate the correct answer from a provided set of candidates. C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text- and word-specific </context>
</contexts>
<marker>Skory, Eskenazi, 2010</marker>
<rawString>Adam Skory and Maxine Eskenazi. 2010. Predicting Cloze Task Quality for Vocabulary Training. In The 5th Workshop on Innovative Use of NLP for Building Educational Applications (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Spolsky</author>
</authors>
<title>Reduced Redundancy as a Language Testing Tool.</title>
<date>1969</date>
<booktitle>Applications of linguistics,</booktitle>
<pages>383--390</pages>
<editor>In G.E. Perren and J.L.M. Trim, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, August.</location>
<contexts>
<context position="2113" citStr="Spolsky, 1969" startWordPosition="312" endWordPosition="314">Vygotsky, 1978), the range of suitable material is very small. Thus, creating a test that fits this narrow target zone is a tedious and timeconsuming task. Teachers predict the difficulty of a test based on their teaching experience. However, as they already know the solutions, they cannot always anticipate the confusion a test might cause for learners. This results in a subjective difficulty estimation that often lacks the consistency required for comparing learners over different tests. The underlying principle of most language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969). It is based on the idea that “natural language is redundant” and that more advanced learners can be distinguished from beginners by their ability to deal with reduced redundancy. For language testing, redundancy can be reduced by eliminating words from a text and asking the learner to fill in the gap, also known as the cloze test. The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006). We present an approach for determining the difficulty of C-t</context>
</contexts>
<marker>Spolsky, 1969</marker>
<rawString>Bernard Spolsky. 1969. Reduced Redundancy as a Language Testing Tool. In G.E. Perren and J.L.M. Trim, editors, Applications of linguistics, pages 383–390. Cambridge University Press, Cambridge, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Traxel</author>
<author>Bettina Dresemann</author>
</authors>
<title>Collect, callibrate, compare: A practical approach to estimating the difficulty of C-Test items.</title>
<date>2010</date>
<booktitle>Der C-Test: Beitr¨age aus der aktuellen Forschung The C-Test: Contributions from Current Research,</booktitle>
<pages>57--69</pages>
<editor>In R¨udiger Grotjahn, editor,</editor>
<publisher>Peter Lang International Academic Publishers,</publisher>
<location>Frankfurt a.M.</location>
<contexts>
<context position="35640" citStr="Traxel and Dresemann, 2010" startWordPosition="5816" endWordPosition="5819">ality. Klein-Braley (1984) already determined the type-token ratio as useful cue for paragraph difficulty prediction. We also use syntactic readability features such as the number of entity mentions, the number of certain POS types (e.g. noun, determiner, adjective) and the number of certain phrase patterns (e.g. verbal phrase, noun phrase, subordinate phrase). Having introduced all four dimensions of C-test difficulty, we now report on the results of the actual difficulty prediction. Difficulty prediction of C-tests has up to now only been performed on the paragraph level (KleinBraley, 1984; Traxel and Dresemann, 2010). In this article, we go beyond paragraphs and predict the difficulty of gaps. We first determine the human performance on the task and use it as a reference for the performance of the machine learning approach based on our difficulty model. 523 A1 A2 A3 Median A1-A3 Correct Prediction 200 209 192 213 Overestimation 90 99 83 101 Underestimation 107 89 118 84 NA 2 2 6 1 Accuracy 0.50 0.52 0.48 0.53 Table 2: Results of the human annotations 5 Human Difficulty Prediction Due to the high number of participants, we already have precise gap-level error rates (cf. Figure 2) for our tests. We now want</context>
</contexts>
<marker>Traxel, Dresemann, 2010</marker>
<rawString>Oliver Traxel and Bettina Dresemann. 2010. Collect, callibrate, compare: A practical approach to estimating the difficulty of C-Test items. In R¨udiger Grotjahn, editor, Der C-Test: Beitr¨age aus der aktuellen Forschung The C-Test: Contributions from Current Research, pages 57–69. Peter Lang International Academic Publishers, Frankfurt a.M.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Vygotsky</author>
</authors>
<title>Mind in society: The development of higher psychological processes.</title>
<date>1978</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="1514" citStr="Vygotsky, 1978" startWordPosition="218" endWordPosition="219"> language is more relevant than ever before. Due to increased mobility, multilingual skills are also required for private communication as friendships stretch across geographical and linguistic borders. In order to provide adequate language learning support, it is important to frequently evaluate learner progress on the basis of language proficiency tests that enable a fair comparison between learners. The test difficulty needs to match the intended target group as the test should be challenging for the learner but not lead to frustration. According to Vygotsky’s zone of proximal development (Vygotsky, 1978), the range of suitable material is very small. Thus, creating a test that fits this narrow target zone is a tedious and timeconsuming task. Teachers predict the difficulty of a test based on their teaching experience. However, as they already know the solutions, they cannot always anticipate the confusion a test might cause for learners. This results in a subjective difficulty estimation that often lacks the consistency required for comparing learners over different tests. The underlying principle of most language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969).</context>
</contexts>
<marker>Vygotsky, 1978</marker>
<rawString>Lev Vygotsky. 1978. Mind in society: The development of higher psychological processes. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Oren Melamud</author>
</authors>
<title>Automatic generation of challenging distractors using context-sensitive inference rules.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>143--148</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6034" citStr="Zesch and Melamud, 2014" startWordPosition="951" endWordPosition="954">elaxed scoring schemes and the use of distractors. In relaxed scoring, teachers accept all tolerable candidates for a gap and not only the intended solution as in exact scoring. Unfortunately, this scoring method turned out to be quite subjective and time-consuming as it is not possible to anticipate all tolerable solutions (Raatz and Klein-Braley, 2002). The use of distractors circumvents this open solution space by providing a closed set of candidates from which the solution needs to be picked. Several approaches have been proposed for automatic distractor selection (Sakaguchi et al., 2013; Zesch and Melamud, 2014) to make sure that the distractors are not too hard nor too easy and are not a valid solution themselves. However, the presence of the correct solution in the distractor set enables the option of random guessing leading to biased results. In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, </context>
</contexts>
<marker>Zesch, Melamud, 2014</marker>
<rawString>Torsten Zesch and Oren Melamud. 2014. Automatic generation of challenging distractors using context-sensitive inference rules. In Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 143–148. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000845">
<title confidence="0.990083">
Improved CCG Parsing with Semi-supervised Supertagging
</title>
<author confidence="0.993103">
Mike Lewis
</author>
<affiliation confidence="0.997798">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.986923">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.999247">
mike.lewis@ed.ac.uk
</email>
<author confidence="0.997838">
Mark Steedman
</author>
<affiliation confidence="0.998087">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.986921">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.999254">
steedman@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999612043478261">
Current supervised parsers are limited by the
size of their labelled training data, making
improving them with unlabelled data an im-
portant goal. We show how a state-of-the-
art CCG parser can be enhanced, by pre-
dicting lexical categories using unsupervised
vector-space embeddings of words. The use
of word embeddings enables our model to
better generalize from the labelled data, and
allows us to accurately assign lexical cate-
gories without depending on a POS-tagger.
Our approach leads to substantial improve-
ments in dependency parsing results over the
standard supervised CCG parser when evalu-
ated on Wall Street Journal (0.8%), Wikipedia
(1.8%) and biomedical (3.4%) text. We com-
pare the performance of two recently proposed
approaches for classification using a wide va-
riety of word embeddings. We also give a de-
tailed error analysis demonstrating where us-
ing embeddings outperforms traditional fea-
ture sets, and showing how including POS fea-
tures can decrease accuracy.
</bodyText>
<sectionHeader confidence="0.99912" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999679127659575">
Combinatory Categorial Grammar (CCG) is widely
used in natural language semantics (Bos, 2008;
Kwiatkowski et al., 2010; Krishnamurthy and
Mitchell, 2012; Lewis and Steedman, 2013a; Lewis
and Steedman, 2013b; Kwiatkowski et al., 2013),
largely because of its direct linkage of syntax and
semantics. However, this connection means that
performance on semantic applications is highly de-
pendent on the quality of the syntactic parse. Al-
though CCG parsers perform at state-of-the-art lev-
els (Rimell et al., 2009; Nivre et al., 2010), full-
sentence accuracy is just 25.6% on Wikipedia text,
which gives a low upper bound on logical inference
approaches to question-answering and textual entail-
ment.
Supertags are rich lexical categories that go be-
yond POS tags by encoding information about
predicate-argument structure. Supertagging is “al-
most parsing”, and is used by parsers based on
strongly lexicalized formalisms such as CCG and
TAG to improve accuracy and efficiency, by dele-
gating many of the parsing decisions to finite-state
models (Bangalore and Joshi, 1999). A disadvan-
tage of this approach is that larger sets of lexical
categories mean increased sparsity, decreasing tag-
ging accuracy. As large amounts of labelled data are
unlikely to be made available, recent work has ex-
plored using unlabelled data to improve parser lex-
icons (Thomforde and Steedman, 2011; Deoskar et
al., 2011; Deoskar et al., 2014). However, existing
work has failed to improve the overall accuracy of
state-of-the-art supervised parsers in-domain.
Another strand of recent work has explored us-
ing unsupervised word embeddings as features in
supervised models (Turian et al., 2010; Collobert et
al., 2011b), largely motivated as a simpler and more
general alternative to standard feature sets. We apply
similar techniques to CCG supertagging, hypothe-
sising that words which are close in the embedding
space will have similar supertags. Most existing
work has focused on flat tagging tasks, and has not
produced state-of-the-art results on structured pre-
diction tasks like parsing (Collobert, 2011; Andreas
and Klein, 2014). CCG’s lexicalized nature provides
a simple and elegant solution to treating parsing as
a flat tagging task, as the lexical categories encode
information about hierarchical structure.
</bodyText>
<page confidence="0.987945">
327
</page>
<bodyText confidence="0.925979666666667">
Transactions of the Association for Computational Linguistics, 2 (2014) 327–338. Action Editor: Ryan McDonald.
Submitted 4/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics.
As well as improving parsing accuracy, our model
has a number of advantages over current CCG pars-
ing work. Our supertagger does not make use of a
POS-tagger, a fact which simplifies the model archi-
tecture, reduces the number of parameters, and elim-
inates errors caused by a pipeline approach. Also,
learning word embeddings is an active area of re-
search, and future developments may directly lead
to better parsing accuracy, with no change required
to our model.
</bodyText>
<sectionHeader confidence="0.992349" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.861183">
2.1 CCG Parsing
</subsectionHeader>
<bodyText confidence="0.999903823529412">
The widely-used C&amp;C parser (Clark and Curran,
2007) for CCG takes a pipeline approach, where
first sentences are POS-tagged, then supertagged,
and then parsed. The supertagger outputs a distri-
bution over tags for each word, and a beam is used
to aggressively prune supertags to reduce the parser
search space. If the parser is unable to find a parse
with a given set of supertags, the beam is relaxed.
This approach is known as adaptive supertagging.
The pipeline approach has two major drawbacks.
Firstly, the use of a POS-tagger can overly prune
the search space for the supertagger. Whilst POS-
taggers have an accuracy of around 97% in domain,
this drops to just 93.4% on biomedical text (Rimell
and Clark, 2009), meaning that most sentences
will contain an erroneous POS-tag. The supertag-
ger model is overly dependent on POS-features—in
Section 4.6 we show that supertagger performance
drops dramatically on words which have been as-
signed an incorrect POS-tag.
Secondly, both the POS-tagger and supertagger
are highly reliant on lexical features, meaning that
performance drops both on unknown words, and
words used differently from the training data. Many
common words do not appear at all in the train-
ing data of the Penn Treebank, such as ten, mili-
tants, insight, and teenager. Many others are not
seen with all their possible uses—for example Eu-
ropean only occurs as an adjective, never a noun,
meaning that the C&amp;C parser is unable to analyse
simple sentences like The director of the IMF is tra-
ditionally a European. These problems are particu-
larly acute when parsing other domains (Rimell and
Clark, 2009).
</bodyText>
<subsectionHeader confidence="0.994166">
2.2 Semi Supervised NLP using Word
Embeddings
</subsectionHeader>
<bodyText confidence="0.9998476">
Recent work has explored using vector space em-
beddings for words as features in supervised mod-
els for a variety of tasks, such as POS-tagging,
chunking, named-entity recognition, semantic role
labelling, and phrase structure parsing (Turian et
al., 2010; Collobert et al., 2011b; Collobert, 2011;
Socher et al., 2013). The major motivation for us-
ing these techniques has been to minimize the level
of task-specific feature engineering required, as the
same feature set can lead to good results on a variety
of tasks. Performance varies between tasks, but any
gains over state-of-the-art traditional features have
been small. A variety of techniques have been used
for learning such embeddings from large unlabelled
corpora, such as neural-network language models.
</bodyText>
<sectionHeader confidence="0.996481" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.9999612">
We introduce models for predicting CCG lexi-
cal categories based on vector-space embeddings.
The models can then be used to replace the POS-
tagging and supertagging stages used by existing
CCG parsers. We experiment with the neural net-
work model proposed by Collobert et al. (2011b),
and conditional random field (CRF) model used by
Turian et al. (2010). We only use features that can be
expected to work well out-of-domain—in particular,
we use no lexical or POS features.
</bodyText>
<subsectionHeader confidence="0.965666">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999282">
Our features are similar to those used by Collobert
et al. (2011b) for POS-tagging. For every word in a
context window, we add features for the embedding
of the word, its 2-character suffix, and whether or
not it is capitalised. We expect such features to gen-
eralize well to other domains—and in Section 4.5
we show that adding traditional POS-tag and lexical
features does not help.
To further reduce sparsity, we apply some simple
preprocessing techniques. Words are lower-cased1,
and all digits are replaced with 0. If an unknown
word is hyphenated, we first try backing-off to the
substring after the hyphen.
</bodyText>
<footnote confidence="0.998645">
1For embeddings that include separate entries for the same word
with different capitalization, we take the most frequently occur-
ring version in the unlabelled corpus.
</footnote>
<page confidence="0.995557">
328
</page>
<figureCaption confidence="0.9793134">
Figure 1: Collobert et al. (2011b)’s Window approach network, applied to CCG supertagging. Each context
word Ci is connected to one hidden node per dimension of the embedding Eij with weight Wij, and ad-
ditional hidden nodes representing capitalization and suffix features. The weights Wij are initialized using
pre-trained word embeddings, but can be modified during supervised training. The additional hidden layer
uses a hard-tanh activation function, and the output layer uses a softmax activation function.
</figureCaption>
<figure confidence="0.997956266666667">
green
ideas
Lookup Tables
for Embeddings
and Discrete
Features
Output CCG
Categories
NP
Context Words
Colourless
S\NP
N
Additional
Hidden Layer
</figure>
<bodyText confidence="0.99794">
Words which do not have an entry in the word em-
beddings share an ‘unknown’ embedding. Different
‘unknown’ vectors are used for capitalized and un-
capitalized words, and non-alphabetic symbols. We
also add entries for context words which are before
the start and after the end of the sentence. All of
these were initialized to the ‘unknown’ vector in the
pre-trained embeddings (or with Gaussian noise if
not available).
</bodyText>
<subsectionHeader confidence="0.979933">
3.2 Neural Network Model
</subsectionHeader>
<bodyText confidence="0.999969970588235">
We predict word supertags with the neural network
classifier used by Collobert et al. (2011b) for POS-
tagging, as shown in Figure 1. Each feature is imple-
mented as a lookup table, which maps context words
onto vectors. The same lookup table parameters are
used wherever a word appears in the context win-
dow.
Word embeddings are implemented with a lookup
table W ∈ RV ×D, where V is the size of the vo-
cabulary, and D is the dimension of the word em-
beddings. The parameters of the lookup table are
initialized using unsupervised embeddings, but are
modified during supervised training.
As in Collobert et al. (2011b), non-embedding
features (2-character suffixes and capitalization) are
also each represented with lookup tables, which map
each feature onto a K dimensional vector (as in Col-
lobert et al. (2011b), we use K = 5). Lookup table
parameters for non-embeddings features are initial-
ized with Gaussian noise.
The first hidden layer therefore contains C ×(D+
KF) nodes, where F is the number of discrete fea-
tures and C is the size of the context window. We
also experimented adding an additional hidden layer,
with a hard-tanh activation function, which makes
the classifier non-linear. Finally, a logistic softmax
layer is used for classifying output categories.
The model is trained using stochastic gradient de-
scent, with a learning rate of 0.01, optimizing for
cross-entropy. We use early-stopping as an alterna-
tive to regularization—after each iteration the model
is evaluated for accuracy on held-out data, and we
use the best performing model. Training was run
until performance decreased on held-out data.
</bodyText>
<subsectionHeader confidence="0.957275">
3.3 CRF Model
</subsectionHeader>
<bodyText confidence="0.999922333333333">
The neural network model treats the probability of
each supertag as being conditionally independent.
However, conditioning on surrounding supertags
</bodyText>
<page confidence="0.9985">
329
</page>
<table confidence="0.999608166666667">
Embeddings Model Dimensionality Training Words Training Domain
Collobert&amp;Weston NNLM 50 660M Wikipedia
Skip-Gram Skip Gram 300 100B Google News
Turian NNLM 25, 50, 100, 200 37M Newswire
HLBL HLBL 50, 100 37M Newswire
Mikolov RNNLM 80, 640 320M Broadcast News
</table>
<tableCaption confidence="0.970955666666667">
Table 1: Embeddings used in our experiments. Dimensionality is the set of dimensions of the word em-
bedding space that we experimented with, and Training Words refers to the size of the unlabelled corpus the
embeddings were trained on.
</tableCaption>
<bodyText confidence="0.999477705882353">
may be very useful—for example, a noun is much
more likely to follow an adjective than a verb. Cur-
ran et al. (2006) report a large improvement using a
maximum-entropy Markov model for supertagging,
conditioned on the surrounding supertags.
We follow Turian et al. (2010) in using a linear
chain CRF model for sequence classification using
embeddings as features. This model does not al-
low supervised training to fine-tune the embeddings,
though it would be possible to build a CRF/NN hy-
brid that enabled this. We use the same feature set
as with the neural network model—so the probabil-
ity of a category depends on embeddings, capital-
ization and suffix features—as well as the previous
category. The model is trained using the averaged-
perceptron algorithm (Collins, 2002), again using
early-stopping based on development data accuracy.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997126">
4.1 Domains
</subsectionHeader>
<bodyText confidence="0.999356">
We experiment with three domains:
</bodyText>
<listItem confidence="0.994487">
• CCGBank (Hockenmaier and Steedman,
2007), which is a conversion of the Penn Tree-
bank (Marcus et al., 1993) to CCG. Section 23
is used for evaluation.
• Wikipedia, using the corpus of 200 sentences
annotated with CCG derivations by Honnibal et
al. (2009). As the text is out-of-domain, parsing
accuracy drops substantially on this corpus.
• Biomedical text, which is even less related to
the newswire text than Wikipedia, due to large
numbers of unseen words and different writ-
ing styles, causing low parsing accuracy. For
</listItem>
<bodyText confidence="0.9945044">
parsing experiments, we use the Bioinfer cor-
pus (Pyysalo et al., 2007) as a test set. For
measuring supertagging accuracy, we use the
CCG annotation produced by Rimell and Clark
(2008).
</bodyText>
<subsectionHeader confidence="0.664491">
4.2 Neural Network Model Parameters
</subsectionHeader>
<bodyText confidence="0.9998607">
In this section, we explore how adjusting the pa-
rameters of our neural network model2 affects 1-best
lexical category accuracy on the Section 00 of CCG-
Bank (all development was done on this data). The
C&amp;C supertagger achieves 91.5% accuracy on this
task. The models were trained on Sections 02-21
of CCGBank, and the reported numbers are the best
accuracy achieved on Section 00. As in Clark and
Curran (2007), all models use only the 425 most fre-
quent categories in CCGBank.
</bodyText>
<subsectionHeader confidence="0.445772">
4.2.1 Embeddings
</subsectionHeader>
<bodyText confidence="0.999921375">
A number of word embeddings have recently been
released, aiming to capture a variety of syntactic and
semantic phenomena, based on neural network lan-
guage models (NNLMs) (Turian et al., 2010; Col-
lobert et al., 2011b), recurrent neural network lan-
guage models (Mikolov, 2012), the hierarchical log
bilinear model (HLBL) (Mnih and Hinton, 2008),
and Mikolov et al. (2013)’s Skip Gram model. How-
ever, there has been a lack of experiments comparing
which embeddings provide the most effective fea-
tures for downstream tasks.
First, we investigated the performance of several
publicly available embeddings, to find which was
most effective for supertagging. The embeddings
we used are summarized in Table 1. For efficiency,
we used our simplest architecture, with no additional
</bodyText>
<footnote confidence="0.867309">
2Implemented using the Torch7 library (Collobert et al., 2011a)
</footnote>
<page confidence="0.98626">
330
</page>
<table confidence="0.999398307692308">
Embeddings Category Category
Accuracy Accuracy
(window=5) (window=7)
Collobert&amp;Weston 90.0% 89.6%
Skip Gram 90.9% 91.0%
Turian-25 91.0% 91.1%
Turian-50 91.1% 91.3%
Turian-100 91.0% 91.1%
Turian-200 90.8% 90.7%
HLBL-50 90.9% 91.2%
HLBL-100 91.1% 91.3%
Mikolov-80 87.2% 88.1%
Mikolov-640 87.9% 88.4%
</table>
<tableCaption confidence="0.995849">
Table 2: Comparison of different embeddings and
</tableCaption>
<bodyText confidence="0.994559451612903">
context windows on Section 00 of CCGBank. Ab-
breviations such as Turian-50 refer to the Turian em-
beddings with a 50-dimensional embedding space.
hidden layer. We also investigate which size context
window is most effective.
Results are shown in Table 2, and show that the
choice of embeddings is crucial to performance on
this task. The performance of the Turian and HLBL
embeddings is surprisingly high given the relatively
small amount of unlabelled data, suggesting that pa-
rameters other than the size of the corpus are more
important. Of course, we have not performed a grid-
search of the parameter space, and it is possible that
other embeddings would perform better with differ-
ent training data, dimensionality, or model architec-
tures. The Mikolov embeddings may suffer from be-
ing trained on broadcast news, which has no punc-
tuation and different language use. Using a context
window of 7 words generally outperformed using a
window of 5 words (we also experimented with a
9 word window, but found performance decreased
slightly to 91.2% for the Turian-50 embeddings).
There is no clear trend on the optimal dimension of
the embedding space, and it is likely to vary with
training methods and corpus size.
Next, we experimented with the size of the addi-
tional hidden layer—for efficiency, using the Turian-
50 embeddings with a 5-word context window. Re-
sults are shown in Table 3, and suggest that a hid-
den layer is not useful for this task—possibly due to
over-fitting.
</bodyText>
<table confidence="0.998925142857143">
Embeddings Additional Hidden Accuracy
Layer Size
Turian-50 0 91.1%
Turian-50 100 90.9%
Turian-50 300 90.6%
Turian-50 500 90.9%
Turian-50 1000 90.5%
</table>
<tableCaption confidence="0.994593">
Table 3: Comparison of different model architec-
</tableCaption>
<bodyText confidence="0.800097">
tures, using the Turian embeddings and a 5-word
context window. A size of 0 means no additional
hidden layer was used.
In all subsequent experiments we used a context
window of 7 words, no additional hidden layer, and
the Turian-50 embeddings.
</bodyText>
<subsectionHeader confidence="0.965746">
4.3 CRF Model
</subsectionHeader>
<bodyText confidence="0.999965571428571">
We also experimented with the CRF model for
supertagging3. Training these models took far
longer than our neural-network model, due to the
need to use the forward-backward algorithm with
a 425×425-dimensional transition matrix during
training (rather than considering each word’s cate-
gory independently). Consequently, we only exper-
imented with the Turian-50 embeddings with a 7-
word context window, which attained the best per-
formance using the neural network.
We found that using the Turian-50 embeddings
gave a surprisingly weak performance of just 90.3%
(compared to 91.3% for the neural network model).
We hypothesised that one reason for this result
could be that the model is unable to modify the
embeddings during supervised training (in contrast
to the neural-network model). Consequently, we
built a new set of embeddings, using the weight-
matrix learned by our best neural network model.
A new CRF model was then trained using the tuned
embeddings. Performance then improved dramati-
cally to 91.5%, and slightly outperformed the neu-
ral network—showing that while there is a small
advantage to using sequence information, it is cru-
cial to allow supervised training to modify the em-
beddings. These results help explain why Collobert
et al. (2011b)’s neural network models outperform
Turian et al. (2010)’s sequence models—but greater
</bodyText>
<footnote confidence="0.875577">
3Implemented using CRFSuite (Okazaki, )
</footnote>
<page confidence="0.996415">
331
</page>
<bodyText confidence="0.999938428571428">
improvements may be possible with the combined
approach we introduce here, which allows the model
to both tune the embeddings and exploit sequence
information. However, tagging with this model was
considerably slower than the neural network (again,
due to the cost of decoding), so we used the neural
network architecture in the remaining experiments.
</bodyText>
<subsectionHeader confidence="0.999549">
4.4 Multitagging Accuracy
</subsectionHeader>
<bodyText confidence="0.999981454545455">
The C&amp;C parser takes a set of supertags per word
as input, which is used to prune the search space. If
no parse is found, the sentence is supertagged again
with a wider beam. The effectiveness of the pruning
therefore depends on the accuracy of the supertagger
at a given level of ambiguity.
We experimented with the accuracy of different
supertaggers at different levels of ambiguity. For the
C&amp;C supertagger, we vary the number of categories
per word using the same back-off beam settings re-
ported in Clark and Curran (2007). For our supertag-
ger, we vary recall by adjusting the a variable-width
beam, which removes tags whose probability is less
than β times that of the most likely tag.
Results are shown in Figure 2. The supertag-
gers based on embeddings consistently match or out-
perform the C&amp;C supertagger at all levels of recall
across all domains. While performance is similar
with a small number of tags per word, our supertag-
gers perform better with a more relaxed beam—
perhaps representing cases which are challenging
for the C&amp;C model, such as POS-tag errors.
</bodyText>
<subsectionHeader confidence="0.99805">
4.5 Parsing Accuracy
</subsectionHeader>
<bodyText confidence="0.999900083333333">
We investigate whether our supertagger improves
the performance of the C&amp;C parser, by replacing
the standard C&amp;C supertagger with our model. This
evaluation is somewhat indirect, as the parser does
not make use of the supertagger probabilities for cat-
egories, but instead simply uses it to prune the search
space. However, we show that better pruning leads
directly to better parsing accuracies.
C&amp;C parser results on CCGBank and Wikipedia
are reported using Clark and Curran (2007)’s best
performing hybrid model4 (trained on Sections 02-
21), with automatic POS-tags, and the parameters
</bodyText>
<footnote confidence="0.971294">
4This model is not publicly available, so we re-trained it follow-
ing the instructions at http://aclweb.org/aclwiki/
index.php?title=Training_the_C&amp;C_Parser
</footnote>
<figure confidence="0.9216295">
CCGBank Section 00
Average Categories Per Word
Wikipedia
Average Categories Per Word
Biomedical
Average Categories Per Word
</figure>
<figureCaption confidence="0.990866">
Figure 2: Ambiguity vs. Accuracy for different su-
pertaggers across different domains. Datapoints for
the C&amp;C parser use its standard back-off parameters.
</figureCaption>
<figure confidence="0.996883730769231">
1 2 3 4
C&amp;C
Turian-50
Oracle Category Accuracy (%)
100
99
98
97
96
1 2 3 4
C&amp;C
Turian-50
Oracle Category Accuracy (%)
100
98
96
94
1 2 3 4 5
C&amp;C
Turian-50
Oracle Category Accuracy (%)
100
98
96
94
92
</figure>
<page confidence="0.973836">
332
</page>
<table confidence="0.999743125">
Supertagger F1 CCGBank F1 F1 Wikipedia F1 F1 Bioinfer F1
(cov) COV (all) (cov) COV (all) (cov) COV (all)
C&amp;C 85.47 99.6 85.30 81.19 99.0 80.64 76.08 97.2 74.88
Honnibal et al. (2009) 85.19 99.8 - 81.75 99.4 - - - -
Brown Clusters 85.27 99.9 85.21 80.89 100.0 80.89 76.06 100.0 76.06
Turian-50 Embeddings 86.11 100.0 86.11 82.30 100.0 82.30 78.41 99.8 78.28
Turian-50 + POS tags 85.62 99.9 85.55 81.77 100.0 81.77 77.05 100.0 77.05
Turian-50 + Frequent words 86.04 100.0 86.04 82.44 100.0 82.44 78.10 100.0 78.10
</table>
<tableCaption confidence="0.9908">
Table 4: Parsing F1-scores for labelled dependencies across a range of domains, using the C&amp;C parser with
</tableCaption>
<bodyText confidence="0.977987169230769">
different supertaggers. Embeddings models used a context window of 7 words, and no additional hidden
layer. Following previous CCG parsing work, we report F1-scores on the subset of sentences where the
parser is able to produce a parse (F1-cov), and the parser’s coverage (COV). Where available we also report
overall scores (F1-all), including parser failures, which we believe gives a more realistic assessment.
used in the published results. Biomedical results
use the publicly available parsing model, setting the
‘parser beam ratio’ parameter to 10−4, which im-
proved results on development data. To achieve full
coverage on the Wikipedia corpus, we increased the
‘max supercats’ parameter to 107. C&amp;C accuracies
differ very slightly from previously reported results,
due to differences in the retrained models.
As in Clark and Curran (2007), we use a variable-
width beam 0 that prunes categories whose prob-
ability is less than 0 times that of the most likely
category. For simplicity, our supertaggers use the
same 0 back-off parameters as are used by the C&amp;C
parser, though it is possible that further improve-
ments could be gained by carefully tuning these pa-
rameters.5 In contrast to the C&amp;C supertaggers, we
do not make use a tag-dictionaries.
Results are shown in Table 4, and our supertag-
gers consistently lead to improvements over the
baseline parser across all domains, with larger im-
provements out-of-domain. Our best model also
outperforms Honnibal et al. (2009)’s self-training
approach to domain adaptation on Wikipedia (which
lowers performance on CCGBank).
Our results show that word embeddings are an ef-
fective way of adding distributional information into
CCG supertagging. A popular alternative approach
5We briefly experimented setting the β parameters to match the
ambiguity of the C&amp;C supertagger on Section 00 of CCGBank,
which caused the F1-score using the Turian-50 embeddings to
drop slightly from 86.11 to 85.95.
for semi-supervised learning is to use Brown clus-
ters (Brown et al., 1992). To ensure a fair com-
parison with the Turian embeddings, we use clus-
ters trained on the same corpus, and use a com-
parable feature set (clusters, capitalization, and 2-
character suffixes—all implemented as sparse bi-
nary features). Brown clusters are hierarchical, and
following Koo et al. (2008), we incorporate Brown
clusters features at multiple levels of granularity—
using 64 coarse clusters (loosely analogous to POS-
tags) and 1000 fine-grained clusters. Results show
slightly lower performance than C&amp;C in domain, but
higher performance out of domain. However, they
are substantially lower than results using the Turian-
50 embeddings.
We also experimented with adding traditional
word and POS features, which were implemented
as sparse vectors for each word in the context win-
dow. We found that including POS features (de-
rived from the C&amp;C POS-tagger) reduced accuracy
across all domains. One reason is that POS tags are
highly discriminative features, therefore errors can
be hard to recover from. Adding lexical features for
the most frequent 250 words had little impact on re-
sults, showing that the embeddings already represent
this information.
For infrequent words, the C&amp;C parser uses a hard
constraint that only certain POS-tag/supertag com-
binations are allowed. This constraint means that
the parser may be particularly vulnerable to POS-
</bodyText>
<page confidence="0.996501">
333
</page>
<figure confidence="0.99917915">
Oracle Category Accuracy (%)
Words with incorrect POS Tag (3%)
1 2 3 4
C&amp;C
Turian-50
Words only seen with other categories (2%)
1 2 3 4
C&amp;C
Turian-50
Oracle Category Accuracy (%)
100
80
60
40
100
90
80
70
60
Average Categories Per Word
</figure>
<figureCaption confidence="0.9289035">
Figure 3: Ambiguity vs. Accuracy for different su-
pertaggers, on words with incorrect POS tags.
</figureCaption>
<bodyText confidence="0.996837666666667">
tag errors, as the model cannot override the hard-
constraint. We therefore also ran the model allowing
any POS-tag/supertag combination. We found that
parsing accuracy was 0.02 higher on development
data (and much slower), suggesting that the model
itself is overly reliant on POS-features.
</bodyText>
<subsectionHeader confidence="0.892338">
4.6 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999968142857143">
We have demonstrated that word embeddings are
highly effective for CCG supertagging. In this sec-
tion, we investigate several cases in which they
are particularly helpful—by measuring supertagger
performance when the POS tagger made mistakes,
when words were unseen in the labelled data, and
when the labelled data only contains the word with
a different category. Our supertaggers show substan-
tial improvements over more complex existing mod-
els.
Figure 3 shows performance when the POS-
tagger assigns the wrong tag to a word. Both sys-
tems show dramatically lower performance on these
cases—the embeddings supertagger does not use
POS features, but POS errors are likely to represent
generally difficult examples. However, the embed-
dings supertagger is almost 15% more accurate on
this subset than the C&amp;C supertagger, and with a re-
laxed beam reaches 96% accuracy, showing the ad-
vantages of avoiding a pipeline approach. In con-
trast, the C&amp;C tagger is not robust to POS tag-
</bodyText>
<figure confidence="0.552953">
Average Categories Per Word
</figure>
<figureCaption confidence="0.62243825">
Figure 4: Ambiguity vs. Accuracy for different su-
pertaggers, on words that do occur in the training
data, but not with the category required in the test
data.
</figureCaption>
<bodyText confidence="0.999593">
ger errors, and asymptotes at just 82% accuracy.
An alternative way of mitigating POS errors is to
use a distribution over POS tags as features in the
supertagger—Curran et al. (2006) show that this
technique improves supertagging accuracy by 0.4%
over the C&amp;C baseline, but do not report the impact
on parsing.
Figure 4 shows performance when the a word has
been seen in the training data, but only with a dif-
ferent category from the instance in the test data (for
example, European only occurs as a adjective in the
training data, but it may occur as a noun in the test
data). Performance is even worse on these cases,
which appear to be extremely difficult for existing
models. The accuracy of the embeddings supertag-
ger converges at just 80%, suggesting that our model
has overfit the labelled data. However, it still out-
performs the C&amp;C supertagger by 22% with a beam
allowing 2 tags per word. The large jump in C&amp;C
supertagger performance for the final back-off level
is due to a change in the word frequency threshold at
which the C&amp;C parser only considers word/category
pairs that occur in the labelled data.
Figure 5 gives results for cases where the word
is unseen in the labelled data. The C&amp;C supertag-
ger performance is surprisingly good on such cases,
suggesting that the morphological and context used
</bodyText>
<page confidence="0.996173">
334
</page>
<figure confidence="0.9438435">
Unseen Words (4%)
Average Categories Per Word
</figure>
<figureCaption confidence="0.976287333333333">
Figure 5: Ambiguity vs. Accuracy for different
supertaggers, on words which are unseen in the la-
belled data.
</figureCaption>
<bodyText confidence="0.999752535714286">
is normally sufficient for inferring categories for un-
seen words. However, our supertagger still clearly
outperforms the C&amp;C supertagger, suggesting that
the large vocabulary of the unsupervised embed-
dings helps it to generalize from the labelled data.
We also investigated supertagging accuracy on
different types of word—Table 5 shows several in-
teresting cases. While performance on nouns is sim-
ilar, our supertagger is substantially better on verbs.
Verbs can have many different CCG categories, de-
pending on the number of arguments and tense, and
not all valid word/category combinations will be
seen in the labelled data. Our embeddings allow
the supertagger to learn generalizations, such as that
transitive verbs can also often have intransitive uses.
Similarly, wh-words can have many possible cate-
gories in complex constructions like relative clauses
and pied-piping—and our embeddings may help the
model generalize from having seen a category for
which to one for whom. On the other hand, the C&amp;C
supertagger performs much better on prepositions.
Prepositions have different categories when appear-
ing as arguments or adjuncts, and the distinction in
the gold-standard was made using somewhat arbi-
trary heuristics (Hockenmaier and Steedman, 2007).
It seems our embeddings have failed to capture these
subtleties. Future work should explore methods for
combining the strengths of each model.
</bodyText>
<table confidence="0.999848857142857">
Word Type C&amp;C Turian-50
Accuracy Embeddings
Accuracy
Verbs 93.9% 94.8%
Nouns 97.7% 97.3%
WH-words 90.1% 93.4%
Prepositions 94.8% 91.2%
</table>
<tableCaption confidence="0.994626">
Table 5: Supertagging accuracy on different types
</tableCaption>
<bodyText confidence="0.9984378">
of words, with an ambiguity of 1.27 tags per word
(corresponding to the C&amp;C’s initial beam setting).
Overall performance with this beam is almost iden-
tical. Words types were identified using gold POS
tags, using IN for prepositions.
</bodyText>
<subsectionHeader confidence="0.907815">
4.7 Discussion
</subsectionHeader>
<bodyText confidence="0.999970142857143">
With a narrow supertagger beam, our method gives
similar results to the C&amp;C supertagger. However,
it gains by being more accurate on difficult cases,
due to not relying on lexical or POS features. These
improvements lead directly to parser improvements.
We identify two cases where our supertagger greatly
outperforms the C&amp;C parser: where the POS-tag is
incorrect, and where the word-category pair is un-
seen in the labelled data. Our approach achieves
larger improvements out-of-domain than in-domain,
suggesting that the large vocabulary of embeddings
built by the unsupervised pre-training allows it to
better generalize from the labelled data.
Interestingly, the best-performing Turian-50 em-
beddings were trained on just 37M words of text
(compared to 100B words for the Skip-gram embed-
dings), suggesting that further improvements may
well be possible using larger unlabelled corpora. Fu-
ture work should investigate whether the models and
embeddings that work well for supertagging gener-
alize to other tasks.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999838125">
Many methods have recently been proposed for im-
proving supervised parsers with unlabelled data.
Most of these are orthogonal to our work, and larger
improvements may be possible by combining them.
Thomforde and Steedman (2011) extends a CCG
lexicon by inferring categories for unseen words,
based on the likely categories of surrounding words.
Unlike our method, this approach is able to learn
</bodyText>
<figure confidence="0.907138875">
1 2 3 4
C&amp;C
Turian-50
Oracle Category Accuracy (%)
100
98
96
94
</figure>
<page confidence="0.998326">
335
</page>
<bodyText confidence="0.999979560606061">
categories which were unseen in the labelled data,
which is shown to be useful for parsing a corpus
of questions. Deoskar et al. (2011) and Deoskar et
al. (2014) use Viterbi-EM to learn new lexical en-
tries by running a generative parser over a large un-
labelled corpus. They show good improvements in
accuracy on unseen words, but not overall parsing
improvements in-domain. Their parsing model aims
to capture non-local information about word usage,
which would not be possible for the local context
windows used to learn our embeddings.
Self-training is another popular method for do-
main adaptation, and was used successfully by Hon-
nibal et al. (2009) to improve CCG parser perfor-
mance on Wikipedia. However, it caused a decrease
in performance on the in-domain data, and our
method achieves better performance across all do-
mains. McClosky et al. (2006) improve a Penn Tree-
bank parser in-domain using self-training, but other
work has failed to improve performance out-of-
domain using self training (Dredze et al., 2007). In a
similar spirit to our work, Koo et al. (2008) improve
parsing accuracy using unsupervised word clus-
ter features—we have shown that word-embeddings
outperform Brown clusters for CCG supertagging.
An alternative approach to domain adaptation is
to annotate a small corpus of out-of-domain text.
Rimell and Clark (2008) argue that this annotation
is simpler for lexicalized formalisms such as CCG,
as large improvements can be gained from annotat-
ing lexical categories, rather than full syntax trees.
They achieve higher parsing accuracies than us on
biomedical text, but our unsupervised method re-
quires no annotation. It seems likely that our method
could be further improved by incorporating out-of-
domain labelled data (where available).
The best reported CCG parsing results have been
achieved with a model that integrates supertagging
and parsing (Auli and Lopez, 2011a; Auli and
Lopez, 2011b). This work still uses the same fea-
ture set as the C&amp;C parser, suggesting further im-
provements may be possible by using our embed-
dings features. Auli and Lopez POS-tag the sentence
before parsing, but using our features would allow
us to fully eliminate the current pipeline approach to
CCG parsing.
Our work also builds on approaches to semi-
supervised NLP using neural embeddings (Turian et
al., 2010; Collobert et al., 2011b). Existing work
has mainly focused on ‘flat’ tagging problems, with-
out hierarchical structure. Collobert (2011) gives
a model for parsing using embeddings features, by
treating each level of the parse tree as a sequence
classification problem. Socher et al. (2013) in-
troduce a model in which context-free grammar
parses are reranked based on compositional distri-
butional representations for each node. Andreas
and Klein (2014) experiment with a number of ap-
proaches to improving the Berkeley parser with
word embeddings. Such work has not improved over
state-of-the-art existing feature sets for constituency
parsing—although Bansal et al. (2014) achieve good
results for dependency parsing using embeddings.
CCG categories contain much of the hierarchical
structure needed for parsing, giving a simpler way
to improve a parser using embeddings.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999859692307693">
We have shown that CCG parsing can be signif-
icantly improved by predicting lexical categories
based on unsupervised word embeddings. The re-
sulting parsing pipeline is simpler, and has improved
performance both in and out of domain. We ex-
pect further improvements to follow as better word
embeddings are developed, without other changes
to our model. Our approach reduces the problem
of sparsity caused by the large number of CCG
categories, suggesting that finer-grained categories
could be created for CCGBank (in the spirit of Hon-
nibal et al. (2010)), which lead to improved perfor-
mance in downstream semantic parsers. Future work
should also explore domain-adaptation, either using
unsupervised embeddings trained on out-of-domain
text, or using supervised training on out-of-domain
corpora. Our results also have implications for other
NLP tasks—suggesting that using word embeddings
features may be particularly useful out-of-domain,
in pipelines that currently rely on POS taggers, and
in tasks which suffer from sparsity in the labelled
data.
Code for our supertagger is released as part
of the EASYCCG parser (Lewis and Steedman,
2014), available from: https://github.com/
mikelewis0/easyccg
</bodyText>
<page confidence="0.998384">
336
</page>
<sectionHeader confidence="0.998331" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999802375">
We would like to thank Tejaswini Deoskar, Bharat
Ram Ambati, Michael Roth and the anonymous
reviewers for helpful feedback on an earlier ver-
sion of this paper. We also thank Rahul Jha for
sharing his re-implementation of Collobert et al.
(2011b)’s model, and Stephen Clark, Laura Rimell
and Matthew Honnibal for making their out-of-
domain CCG corpora available.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998651882978723">
Jacob Andreas and Dan Klein. 2014. How much do word
embeddings encode about syntax. In Proceedings of
ACL.
Michael Auli and Adam Lopez. 2011a. A comparison
of loopy belief propagation and dual decomposition
for integrated CCG supertagging and parsing. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies-Volume 1, pages 470–480. Association
for Computational Linguistics.
Michael Auli and Adam Lopez. 2011b. Training a log-
linear parser with loss functions via softmax-margin.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 333–343.
Association for Computational Linguistics.
Srinivas Bangalore and Aravind K Joshi. 1999. Su-
pertagging: An approach to almost parsing. Compu-
tational Linguistics, 25(2):237–265.
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring continuous word representations for depen-
dency parsing. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277–286. College Publications.
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and
J.C. Lai. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18(4):467–479.
Stephen Clark and James R Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the ACL-02 conference on Empirical methods in natu-
ral language processing-Volume 10, pages 1–8. Asso-
ciation for Computational Linguistics.
Ronan Collobert, Koray Kavukcuoglu, and Cl´ement
Farabet. 2011a. Torch7: A Matlab-like environment
for machine learning. In BigLearn, NIPS Workshop.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011b.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.
Ronan Collobert. 2011. Deep learning for effi-
cient discriminative parsing. In Geoffrey J. Gordon,
David B. Dunson, and Miroslav Dudk, editors, AIS-
TATS, volume 15 of JMLR Proceedings, pages 224–
232. JMLR.org.
James R Curran, Stephen Clark, and David Vadas. 2006.
Multi-tagging for lexicalized-grammar parsing. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
pages 697–704. Association for Computational Lin-
guistics.
Tejaswini Deoskar, Markos Mylonakis, and Khalil
Sima’an. 2011. Learning structural dependencies
of words in the zipfian tail. In Proceedings of the
12th International Conference on Parsing Technolo-
gies, pages 80–91. Association for Computational Lin-
guistics.
Tejaswini Deoskar, Christos Christodoulopoulos,
Alexandra Birch, and Mark Steedman. 2014. Gener-
alizing a Strongly Lexicalized Parser using Unlabeled
Data. In Proceedings of the 14th Conference of the
European Chapter of the Association for Compu-
tational Linguistics. Association for Computational
Linguistics.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joao Graca, and Fernando Pereira.
2007. Frustratingly hard domain adaptation for depen-
dency parsing. In EMNLP-CoNLL, pages 1051–1055.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355–396.
Matthew Honnibal, Joel Nothman, and James R Cur-
ran. 2009. Evaluating a statistical CCG parser on
Wikipedia. In Proceedings of the 2009 Workshop on
The People’s Web Meets NLP: Collaboratively Con-
structed Semantic Resources, pages 38–41. Associa-
tion for Computational Linguistics.
M. Honnibal, J.R. Curran, and J. Bos. 2010. Rebanking
CCGbank for improved NP interpretation. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 207–215. Associa-
tion for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing.
</reference>
<page confidence="0.981008">
337
</page>
<reference confidence="0.999785877551021">
Jayant Krishnamurthy and Tom M. Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL ’12, pages 754–765. Association for Compu-
tational Linguistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilistic
CCG grammars from logical form with higher-order
unification. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’10, pages 1223–1233. Association for Com-
putational Linguistics.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with on-
the-fly ontology matching. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1545–1556, Seattle, Wash-
ington, USA, October. Association for Computational
Linguistics.
Mike Lewis and Mark Steedman. 2013a. Combined
Distributional and Logical Semantics. Transactions of
the Association for Computational Linguistics, 1:179–
192.
Mike Lewis and Mark Steedman. 2013b. Unsupervised
induction of cross-lingual semantic relations. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 681–692,
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Mike Lewis and Mark Steedman. 2014. A* CCG Pars-
ing with a Supertag-factored Model. In Proceedings of
the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP), Doha, Qatar, Octo-
ber.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313–330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the main conference on human language tech-
nology conference of the North American Chapter of
the Association of Computational Linguistics, pages
152–159. Association for Computational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representa-
tions in vector space. arXiv preprint arXiv:1301.3781.
Tom´aˇs Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Ph. D. the-
sis, Brno University of Technology.
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In NIPS,
pages 1081–1088.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
G´omez Rodr´ıguez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 833–841, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Naoaki Okazaki. CRFsuite: a fast implementation of
conditional random fields (CRFs).
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC bioinfor-
matics, 8(1):50.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 475–484. As-
sociation for Computational Linguistics.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, 42(5):852–865.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 2-Volume 2, pages 813–821. Association for
Computational Linguistics.
Richard Socher, John Bauer, Christopher D Manning, and
Andrew Y Ng. 2013. Parsing with compositional vec-
tor grammars. In In Proceedings of the ACL confer-
ence. Citeseer.
Emily Thomforde and Mark Steedman. 2011. Semi-
supervised CCG lexicon extension. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1246–1256. Association for
Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 384–394. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.998372">
338
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.364235">
<title confidence="0.997495">Improved CCG Parsing with Semi-supervised Supertagging</title>
<author confidence="0.995487">Mike</author>
<affiliation confidence="0.843855">School of University of Edinburgh, EH8 9AB,</affiliation>
<email confidence="0.994745">mike.lewis@ed.ac.uk</email>
<author confidence="0.99538">Mark</author>
<affiliation confidence="0.9992835">School of University of</affiliation>
<address confidence="0.716346">Edinburgh, EH8 9AB,</address>
<email confidence="0.997869">steedman@inf.ed.ac.uk</email>
<abstract confidence="0.998811">Current supervised parsers are limited by the size of their labelled training data, making improving them with unlabelled data an important goal. We show how a state-of-theart CCG parser can be enhanced, by predicting lexical categories using unsupervised vector-space embeddings of words. The use of word embeddings enables our model to better generalize from the labelled data, and allows us to accurately assign lexical categories without depending on a POS-tagger. Our approach leads to substantial improvements in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>How much do word embeddings encode about syntax.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3360" citStr="Andreas and Klein, 2014" startWordPosition="506" endWordPosition="509"> of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Collobert et al., 2011b), largely motivated as a simpler and more general alternative to standard feature sets. We apply similar techniques to CCG supertagging, hypothesising that words which are close in the embedding space will have similar supertags. Most existing work has focused on flat tagging tasks, and has not produced state-of-the-art results on structured prediction tasks like parsing (Collobert, 2011; Andreas and Klein, 2014). CCG’s lexicalized nature provides a simple and elegant solution to treating parsing as a flat tagging task, as the lexical categories encode information about hierarchical structure. 327 Transactions of the Association for Computational Linguistics, 2 (2014) 327–338. Action Editor: Ryan McDonald. Submitted 4/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. As well as improving parsing accuracy, our model has a number of advantages over current CCG parsing work. Our supertagger does not make use of a POS-tagger, a fact which simplifies the model archi</context>
<context position="34027" citStr="Andreas and Klein (2014)" startWordPosition="5412" endWordPosition="5415">ow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for constituency parsing—although Bansal et al. (2014) achieve good results for dependency parsing using embeddings. CCG categories contain much of the hierarchical structure needed for parsing, giving a simpler way to improve a parser using embeddings. 6 Conclusions We have shown that CCG parsing can be significantly improved by predicting lexical categories based on unsupervised word embeddings. The resulting parsing pipeline is</context>
</contexts>
<marker>Andreas, Klein, 2014</marker>
<rawString>Jacob Andreas and Dan Klein. 2014. How much do word embeddings encode about syntax. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>470--480</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33149" citStr="Auli and Lopez, 2011" startWordPosition="5272" endWordPosition="5275">tate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gained from annotating lexical categories, rather than full syntax trees. They achieve higher parsing accuracies than us on biomedical text, but our unsupervised method requires no annotation. It seems likely that our method could be further improved by incorporating out-ofdomain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&amp;C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embe</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011a. A comparison of loopy belief propagation and dual decomposition for integrated CCG supertagging and parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 470–480. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>Training a loglinear parser with loss functions via softmax-margin.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>333--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="33149" citStr="Auli and Lopez, 2011" startWordPosition="5272" endWordPosition="5275">tate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gained from annotating lexical categories, rather than full syntax trees. They achieve higher parsing accuracies than us on biomedical text, but our unsupervised method requires no annotation. It seems likely that our method could be further improved by incorporating out-ofdomain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&amp;C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embe</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011b. Training a loglinear parser with loss functions via softmax-margin. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 333–343. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2322" citStr="Bangalore and Joshi, 1999" startWordPosition="346" endWordPosition="349">gh CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. Supertags are rich lexical categories that go beyond POS tags by encoding information about predicate-argument structure. Supertagging is “almost parsing”, and is used by parsers based on strongly lexicalized formalisms such as CCG and TAG to improve accuracy and efficiency, by delegating many of the parsing decisions to finite-state models (Bangalore and Joshi, 1999). A disadvantage of this approach is that larger sets of lexical categories mean increased sparsity, decreasing tagging accuracy. As large amounts of labelled data are unlikely to be made available, recent work has explored using unlabelled data to improve parser lexicons (Thomforde and Steedman, 2011; Deoskar et al., 2011; Deoskar et al., 2014). However, existing work has failed to improve the overall accuracy of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Co</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind K Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34247" citStr="Bansal et al. (2014)" startWordPosition="5444" endWordPosition="5447"> focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for constituency parsing—although Bansal et al. (2014) achieve good results for dependency parsing using embeddings. CCG categories contain much of the hierarchical structure needed for parsing, giving a simpler way to improve a parser using embeddings. 6 Conclusions We have shown that CCG parsing can be significantly improved by predicting lexical categories based on unsupervised word embeddings. The resulting parsing pipeline is simpler, and has improved performance both in and out of domain. We expect further improvements to follow as better word embeddings are developed, without other changes to our model. Our approach reduces the problem of </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics,</booktitle>
<pages>277--286</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="1352" citStr="Bos, 2008" startWordPosition="199" endWordPosition="200">pproach leads to substantial improvements in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy. 1 Introduction Combinatory Categorial Grammar (CCG) is widely used in natural language semantics (Bos, 2008; Kwiatkowski et al., 2010; Krishnamurthy and Mitchell, 2012; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Kwiatkowski et al., 2013), largely because of its direct linkage of syntax and semantics. However, this connection means that performance on semantic applications is highly dependent on the quality of the syntactic parse. Although CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. </context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V Desouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="23302" citStr="Brown et al., 1992" startWordPosition="3699" endWordPosition="3702">rovements out-of-domain. Our best model also outperforms Honnibal et al. (2009)’s self-training approach to domain adaptation on Wikipedia (which lowers performance on CCGBank). Our results show that word embeddings are an effective way of adding distributional information into CCG supertagging. A popular alternative approach 5We briefly experimented setting the β parameters to match the ambiguity of the C&amp;C supertagger on Section 00 of CCGBank, which caused the F1-score using the Turian-50 embeddings to drop slightly from 86.11 to 85.95. for semi-supervised learning is to use Brown clusters (Brown et al., 1992). To ensure a fair comparison with the Turian embeddings, we use clusters trained on the same corpus, and use a comparable feature set (clusters, capitalization, and 2- character suffixes—all implemented as sparse binary features). Brown clusters are hierarchical, and following Koo et al. (2008), we incorporate Brown clusters features at multiple levels of granularity— using 64 coarse clusters (loosely analogous to POStags) and 1000 fine-grained clusters. Results show slightly lower performance than C&amp;C in domain, but higher performance out of domain. However, they are substantially lower than</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra, and J.C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="4306" citStr="Clark and Curran, 2007" startWordPosition="649" endWordPosition="652">ed 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. As well as improving parsing accuracy, our model has a number of advantages over current CCG parsing work. Our supertagger does not make use of a POS-tagger, a fact which simplifies the model architecture, reduces the number of parameters, and eliminates errors caused by a pipeline approach. Also, learning word embeddings is an active area of research, and future developments may directly lead to better parsing accuracy, with no change required to our model. 2 Background 2.1 CCG Parsing The widely-used C&amp;C parser (Clark and Curran, 2007) for CCG takes a pipeline approach, where first sentences are POS-tagged, then supertagged, and then parsed. The supertagger outputs a distribution over tags for each word, and a beam is used to aggressively prune supertags to reduce the parser search space. If the parser is unable to find a parse with a given set of supertags, the beam is relaxed. This approach is known as adaptive supertagging. The pipeline approach has two major drawbacks. Firstly, the use of a POS-tagger can overly prune the search space for the supertagger. Whilst POStaggers have an accuracy of around 97% in domain, this </context>
<context position="13388" citStr="Clark and Curran (2007)" startWordPosition="2117" endWordPosition="2120">experiments, we use the Bioinfer corpus (Pyysalo et al., 2007) as a test set. For measuring supertagging accuracy, we use the CCG annotation produced by Rimell and Clark (2008). 4.2 Neural Network Model Parameters In this section, we explore how adjusting the parameters of our neural network model2 affects 1-best lexical category accuracy on the Section 00 of CCGBank (all development was done on this data). The C&amp;C supertagger achieves 91.5% accuracy on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to capture a variety of syntactic and semantic phenomena, based on neural network language models (NNLMs) (Turian et al., 2010; Collobert et al., 2011b), recurrent neural network language models (Mikolov, 2012), the hierarchical log bilinear model (HLBL) (Mnih and Hinton, 2008), and Mikolov et al. (2013)’s Skip Gram model. However, there has been a lack of experiments comparing which embeddings provide the most effective features for downstream tas</context>
<context position="18874" citStr="Clark and Curran (2007)" startWordPosition="2986" endWordPosition="2989"> we used the neural network architecture in the remaining experiments. 4.4 Multitagging Accuracy The C&amp;C parser takes a set of supertags per word as input, which is used to prune the search space. If no parse is found, the sentence is supertagged again with a wider beam. The effectiveness of the pruning therefore depends on the accuracy of the supertagger at a given level of ambiguity. We experimented with the accuracy of different supertaggers at different levels of ambiguity. For the C&amp;C supertagger, we vary the number of categories per word using the same back-off beam settings reported in Clark and Curran (2007). For our supertagger, we vary recall by adjusting the a variable-width beam, which removes tags whose probability is less than β times that of the most likely tag. Results are shown in Figure 2. The supertaggers based on embeddings consistently match or outperform the C&amp;C supertagger at all levels of recall across all domains. While performance is similar with a small number of tags per word, our supertaggers perform better with a more relaxed beam— perhaps representing cases which are challenging for the C&amp;C model, such as POS-tag errors. 4.5 Parsing Accuracy We investigate whether our super</context>
<context position="22132" citStr="Clark and Curran (2007)" startWordPosition="3511" endWordPosition="3514">oduce a parse (F1-cov), and the parser’s coverage (COV). Where available we also report overall scores (F1-all), including parser failures, which we believe gives a more realistic assessment. used in the published results. Biomedical results use the publicly available parsing model, setting the ‘parser beam ratio’ parameter to 10−4, which improved results on development data. To achieve full coverage on the Wikipedia corpus, we increased the ‘max supercats’ parameter to 107. C&amp;C accuracies differ very slightly from previously reported results, due to differences in the retrained models. As in Clark and Curran (2007), we use a variablewidth beam 0 that prunes categories whose probability is less than 0 times that of the most likely category. For simplicity, our supertaggers use the same 0 back-off parameters as are used by the C&amp;C parser, though it is possible that further improvements could be gained by carefully tuning these parameters.5 In contrast to the C&amp;C supertaggers, we do not make use a tag-dictionaries. Results are shown in Table 4, and our supertaggers consistently lead to improvements over the baseline parser across all domains, with larger improvements out-of-domain. Our best model also outp</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12108" citStr="Collins, 2002" startWordPosition="1909" endWordPosition="1910">m-entropy Markov model for supertagging, conditioned on the surrounding supertags. We follow Turian et al. (2010) in using a linear chain CRF model for sequence classification using embeddings as features. This model does not allow supervised training to fine-tune the embeddings, though it would be possible to build a CRF/NN hybrid that enabled this. We use the same feature set as with the neural network model—so the probability of a category depends on embeddings, capitalization and suffix features—as well as the previous category. The model is trained using the averagedperceptron algorithm (Collins, 2002), again using early-stopping based on development data accuracy. 4 Experiments 4.1 Domains We experiment with three domains: • CCGBank (Hockenmaier and Steedman, 2007), which is a conversion of the Penn Treebank (Marcus et al., 1993) to CCG. Section 23 is used for evaluation. • Wikipedia, using the corpus of 200 sentences annotated with CCG derivations by Honnibal et al. (2009). As the text is out-of-domain, parsing accuracy drops substantially on this corpus. • Biomedical text, which is even less related to the newswire text than Wikipedia, due to large numbers of unseen words and different w</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Koray Kavukcuoglu</author>
<author>Cl´ement Farabet</author>
</authors>
<title>Torch7: A Matlab-like environment for machine learning.</title>
<date>2011</date>
<booktitle>In BigLearn, NIPS Workshop.</booktitle>
<contexts>
<context position="2942" citStr="Collobert et al., 2011" startWordPosition="443" endWordPosition="446">9). A disadvantage of this approach is that larger sets of lexical categories mean increased sparsity, decreasing tagging accuracy. As large amounts of labelled data are unlikely to be made available, recent work has explored using unlabelled data to improve parser lexicons (Thomforde and Steedman, 2011; Deoskar et al., 2011; Deoskar et al., 2014). However, existing work has failed to improve the overall accuracy of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Collobert et al., 2011b), largely motivated as a simpler and more general alternative to standard feature sets. We apply similar techniques to CCG supertagging, hypothesising that words which are close in the embedding space will have similar supertags. Most existing work has focused on flat tagging tasks, and has not produced state-of-the-art results on structured prediction tasks like parsing (Collobert, 2011; Andreas and Klein, 2014). CCG’s lexicalized nature provides a simple and elegant solution to treating parsing as a flat tagging task, as the lexical categories encode information about hierarchical structur</context>
<context position="6195" citStr="Collobert et al., 2011" startWordPosition="959" endWordPosition="962">their possible uses—for example European only occurs as an adjective, never a noun, meaning that the C&amp;C parser is unable to analyse simple sentences like The director of the IMF is traditionally a European. These problems are particularly acute when parsing other domains (Rimell and Clark, 2009). 2.2 Semi Supervised NLP using Word Embeddings Recent work has explored using vector space embeddings for words as features in supervised models for a variety of tasks, such as POS-tagging, chunking, named-entity recognition, semantic role labelling, and phrase structure parsing (Turian et al., 2010; Collobert et al., 2011b; Collobert, 2011; Socher et al., 2013). The major motivation for using these techniques has been to minimize the level of task-specific feature engineering required, as the same feature set can lead to good results on a variety of tasks. Performance varies between tasks, but any gains over state-of-the-art traditional features have been small. A variety of techniques have been used for learning such embeddings from large unlabelled corpora, such as neural-network language models. 3 Models We introduce models for predicting CCG lexical categories based on vector-space embeddings. The models c</context>
<context position="7992" citStr="Collobert et al. (2011" startWordPosition="1249" endWordPosition="1252"> whether or not it is capitalised. We expect such features to generalize well to other domains—and in Section 4.5 we show that adding traditional POS-tag and lexical features does not help. To further reduce sparsity, we apply some simple preprocessing techniques. Words are lower-cased1, and all digits are replaced with 0. If an unknown word is hyphenated, we first try backing-off to the substring after the hyphen. 1For embeddings that include separate entries for the same word with different capitalization, we take the most frequently occurring version in the unlabelled corpus. 328 Figure 1: Collobert et al. (2011b)’s Window approach network, applied to CCG supertagging. Each context word Ci is connected to one hidden node per dimension of the embedding Eij with weight Wij, and additional hidden nodes representing capitalization and suffix features. The weights Wij are initialized using pre-trained word embeddings, but can be modified during supervised training. The additional hidden layer uses a hard-tanh activation function, and the output layer uses a softmax activation function. green ideas Lookup Tables for Embeddings and Discrete Features Output CCG Categories NP Context Words Colourless S\NP N A</context>
<context position="9670" citStr="Collobert et al. (2011" startWordPosition="1521" endWordPosition="1524">work Model We predict word supertags with the neural network classifier used by Collobert et al. (2011b) for POStagging, as shown in Figure 1. Each feature is implemented as a lookup table, which maps context words onto vectors. The same lookup table parameters are used wherever a word appears in the context window. Word embeddings are implemented with a lookup table W ∈ RV ×D, where V is the size of the vocabulary, and D is the dimension of the word embeddings. The parameters of the lookup table are initialized using unsupervised embeddings, but are modified during supervised training. As in Collobert et al. (2011b), non-embedding features (2-character suffixes and capitalization) are also each represented with lookup tables, which map each feature onto a K dimensional vector (as in Collobert et al. (2011b), we use K = 5). Lookup table parameters for non-embeddings features are initialized with Gaussian noise. The first hidden layer therefore contains C ×(D+ KF) nodes, where F is the number of discrete features and C is the size of the context window. We also experimented adding an additional hidden layer, with a hard-tanh activation function, which makes the classifier non-linear. Finally, a logistic </context>
<context position="13686" citStr="Collobert et al., 2011" startWordPosition="2166" endWordPosition="2170"> affects 1-best lexical category accuracy on the Section 00 of CCGBank (all development was done on this data). The C&amp;C supertagger achieves 91.5% accuracy on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to capture a variety of syntactic and semantic phenomena, based on neural network language models (NNLMs) (Turian et al., 2010; Collobert et al., 2011b), recurrent neural network language models (Mikolov, 2012), the hierarchical log bilinear model (HLBL) (Mnih and Hinton, 2008), and Mikolov et al. (2013)’s Skip Gram model. However, there has been a lack of experiments comparing which embeddings provide the most effective features for downstream tasks. First, we investigated the performance of several publicly available embeddings, to find which was most effective for supertagging. The embeddings we used are summarized in Table 1. For efficiency, we used our simplest architecture, with no additional 2Implemented using the Torch7 library (Col</context>
<context position="17837" citStr="Collobert et al. (2011" startWordPosition="2821" endWordPosition="2824">ne reason for this result could be that the model is unable to modify the embeddings during supervised training (in contrast to the neural-network model). Consequently, we built a new set of embeddings, using the weightmatrix learned by our best neural network model. A new CRF model was then trained using the tuned embeddings. Performance then improved dramatically to 91.5%, and slightly outperformed the neural network—showing that while there is a small advantage to using sequence information, it is crucial to allow supervised training to modify the embeddings. These results help explain why Collobert et al. (2011b)’s neural network models outperform Turian et al. (2010)’s sequence models—but greater 3Implemented using CRFSuite (Okazaki, ) 331 improvements may be possible with the combined approach we introduce here, which allows the model to both tune the embeddings and exploit sequence information. However, tagging with this model was considerably slower than the neural network (again, due to the cost of decoding), so we used the neural network architecture in the remaining experiments. 4.4 Multitagging Accuracy The C&amp;C parser takes a set of supertags per word as input, which is used to prune the sea</context>
<context position="33599" citStr="Collobert et al., 2011" startWordPosition="5348" endWordPosition="5351">domain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&amp;C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for co</context>
</contexts>
<marker>Collobert, Kavukcuoglu, Farabet, 2011</marker>
<rawString>Ronan Collobert, Koray Kavukcuoglu, and Cl´ement Farabet. 2011a. Torch7: A Matlab-like environment for machine learning. In BigLearn, NIPS Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="2942" citStr="Collobert et al., 2011" startWordPosition="443" endWordPosition="446">9). A disadvantage of this approach is that larger sets of lexical categories mean increased sparsity, decreasing tagging accuracy. As large amounts of labelled data are unlikely to be made available, recent work has explored using unlabelled data to improve parser lexicons (Thomforde and Steedman, 2011; Deoskar et al., 2011; Deoskar et al., 2014). However, existing work has failed to improve the overall accuracy of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Collobert et al., 2011b), largely motivated as a simpler and more general alternative to standard feature sets. We apply similar techniques to CCG supertagging, hypothesising that words which are close in the embedding space will have similar supertags. Most existing work has focused on flat tagging tasks, and has not produced state-of-the-art results on structured prediction tasks like parsing (Collobert, 2011; Andreas and Klein, 2014). CCG’s lexicalized nature provides a simple and elegant solution to treating parsing as a flat tagging task, as the lexical categories encode information about hierarchical structur</context>
<context position="6195" citStr="Collobert et al., 2011" startWordPosition="959" endWordPosition="962">their possible uses—for example European only occurs as an adjective, never a noun, meaning that the C&amp;C parser is unable to analyse simple sentences like The director of the IMF is traditionally a European. These problems are particularly acute when parsing other domains (Rimell and Clark, 2009). 2.2 Semi Supervised NLP using Word Embeddings Recent work has explored using vector space embeddings for words as features in supervised models for a variety of tasks, such as POS-tagging, chunking, named-entity recognition, semantic role labelling, and phrase structure parsing (Turian et al., 2010; Collobert et al., 2011b; Collobert, 2011; Socher et al., 2013). The major motivation for using these techniques has been to minimize the level of task-specific feature engineering required, as the same feature set can lead to good results on a variety of tasks. Performance varies between tasks, but any gains over state-of-the-art traditional features have been small. A variety of techniques have been used for learning such embeddings from large unlabelled corpora, such as neural-network language models. 3 Models We introduce models for predicting CCG lexical categories based on vector-space embeddings. The models c</context>
<context position="7992" citStr="Collobert et al. (2011" startWordPosition="1249" endWordPosition="1252"> whether or not it is capitalised. We expect such features to generalize well to other domains—and in Section 4.5 we show that adding traditional POS-tag and lexical features does not help. To further reduce sparsity, we apply some simple preprocessing techniques. Words are lower-cased1, and all digits are replaced with 0. If an unknown word is hyphenated, we first try backing-off to the substring after the hyphen. 1For embeddings that include separate entries for the same word with different capitalization, we take the most frequently occurring version in the unlabelled corpus. 328 Figure 1: Collobert et al. (2011b)’s Window approach network, applied to CCG supertagging. Each context word Ci is connected to one hidden node per dimension of the embedding Eij with weight Wij, and additional hidden nodes representing capitalization and suffix features. The weights Wij are initialized using pre-trained word embeddings, but can be modified during supervised training. The additional hidden layer uses a hard-tanh activation function, and the output layer uses a softmax activation function. green ideas Lookup Tables for Embeddings and Discrete Features Output CCG Categories NP Context Words Colourless S\NP N A</context>
<context position="9670" citStr="Collobert et al. (2011" startWordPosition="1521" endWordPosition="1524">work Model We predict word supertags with the neural network classifier used by Collobert et al. (2011b) for POStagging, as shown in Figure 1. Each feature is implemented as a lookup table, which maps context words onto vectors. The same lookup table parameters are used wherever a word appears in the context window. Word embeddings are implemented with a lookup table W ∈ RV ×D, where V is the size of the vocabulary, and D is the dimension of the word embeddings. The parameters of the lookup table are initialized using unsupervised embeddings, but are modified during supervised training. As in Collobert et al. (2011b), non-embedding features (2-character suffixes and capitalization) are also each represented with lookup tables, which map each feature onto a K dimensional vector (as in Collobert et al. (2011b), we use K = 5). Lookup table parameters for non-embeddings features are initialized with Gaussian noise. The first hidden layer therefore contains C ×(D+ KF) nodes, where F is the number of discrete features and C is the size of the context window. We also experimented adding an additional hidden layer, with a hard-tanh activation function, which makes the classifier non-linear. Finally, a logistic </context>
<context position="13686" citStr="Collobert et al., 2011" startWordPosition="2166" endWordPosition="2170"> affects 1-best lexical category accuracy on the Section 00 of CCGBank (all development was done on this data). The C&amp;C supertagger achieves 91.5% accuracy on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to capture a variety of syntactic and semantic phenomena, based on neural network language models (NNLMs) (Turian et al., 2010; Collobert et al., 2011b), recurrent neural network language models (Mikolov, 2012), the hierarchical log bilinear model (HLBL) (Mnih and Hinton, 2008), and Mikolov et al. (2013)’s Skip Gram model. However, there has been a lack of experiments comparing which embeddings provide the most effective features for downstream tasks. First, we investigated the performance of several publicly available embeddings, to find which was most effective for supertagging. The embeddings we used are summarized in Table 1. For efficiency, we used our simplest architecture, with no additional 2Implemented using the Torch7 library (Col</context>
<context position="17837" citStr="Collobert et al. (2011" startWordPosition="2821" endWordPosition="2824">ne reason for this result could be that the model is unable to modify the embeddings during supervised training (in contrast to the neural-network model). Consequently, we built a new set of embeddings, using the weightmatrix learned by our best neural network model. A new CRF model was then trained using the tuned embeddings. Performance then improved dramatically to 91.5%, and slightly outperformed the neural network—showing that while there is a small advantage to using sequence information, it is crucial to allow supervised training to modify the embeddings. These results help explain why Collobert et al. (2011b)’s neural network models outperform Turian et al. (2010)’s sequence models—but greater 3Implemented using CRFSuite (Okazaki, ) 331 improvements may be possible with the combined approach we introduce here, which allows the model to both tune the embeddings and exploit sequence information. However, tagging with this model was considerably slower than the neural network (again, due to the cost of decoding), so we used the neural network architecture in the remaining experiments. 4.4 Multitagging Accuracy The C&amp;C parser takes a set of supertags per word as input, which is used to prune the sea</context>
<context position="33599" citStr="Collobert et al., 2011" startWordPosition="5348" endWordPosition="5351">domain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&amp;C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for co</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011b. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning for efficient discriminative parsing.</title>
<date>2011</date>
<booktitle>of JMLR Proceedings,</booktitle>
<volume>15</volume>
<pages>224--232</pages>
<editor>In Geoffrey J. Gordon, David B. Dunson, and Miroslav Dudk, editors, AISTATS,</editor>
<contexts>
<context position="3334" citStr="Collobert, 2011" startWordPosition="504" endWordPosition="505"> overall accuracy of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Collobert et al., 2011b), largely motivated as a simpler and more general alternative to standard feature sets. We apply similar techniques to CCG supertagging, hypothesising that words which are close in the embedding space will have similar supertags. Most existing work has focused on flat tagging tasks, and has not produced state-of-the-art results on structured prediction tasks like parsing (Collobert, 2011; Andreas and Klein, 2014). CCG’s lexicalized nature provides a simple and elegant solution to treating parsing as a flat tagging task, as the lexical categories encode information about hierarchical structure. 327 Transactions of the Association for Computational Linguistics, 2 (2014) 327–338. Action Editor: Ryan McDonald. Submitted 4/2014; Revised 6/2014; Published 10/2014. c�2014 Association for Computational Linguistics. As well as improving parsing accuracy, our model has a number of advantages over current CCG parsing work. Our supertagger does not make use of a POS-tagger, a fact which </context>
<context position="6213" citStr="Collobert, 2011" startWordPosition="963" endWordPosition="964">xample European only occurs as an adjective, never a noun, meaning that the C&amp;C parser is unable to analyse simple sentences like The director of the IMF is traditionally a European. These problems are particularly acute when parsing other domains (Rimell and Clark, 2009). 2.2 Semi Supervised NLP using Word Embeddings Recent work has explored using vector space embeddings for words as features in supervised models for a variety of tasks, such as POS-tagging, chunking, named-entity recognition, semantic role labelling, and phrase structure parsing (Turian et al., 2010; Collobert et al., 2011b; Collobert, 2011; Socher et al., 2013). The major motivation for using these techniques has been to minimize the level of task-specific feature engineering required, as the same feature set can lead to good results on a variety of tasks. Performance varies between tasks, but any gains over state-of-the-art traditional features have been small. A variety of techniques have been used for learning such embeddings from large unlabelled corpora, such as neural-network language models. 3 Models We introduce models for predicting CCG lexical categories based on vector-space embeddings. The models can then be used to</context>
<context position="33712" citStr="Collobert (2011)" startWordPosition="5365" endWordPosition="5366">grates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&amp;C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for constituency parsing—although Bansal et al. (2014) achieve good results for dependency parsing using embeddings. CC</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep learning for efficient discriminative parsing. In Geoffrey J. Gordon, David B. Dunson, and Miroslav Dudk, editors, AISTATS, volume 15 of JMLR Proceedings, pages 224– 232. JMLR.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
<author>David Vadas</author>
</authors>
<title>Multi-tagging for lexicalized-grammar parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>697--704</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11452" citStr="Curran et al. (2006)" startWordPosition="1802" endWordPosition="1806">ertags 329 Embeddings Model Dimensionality Training Words Training Domain Collobert&amp;Weston NNLM 50 660M Wikipedia Skip-Gram Skip Gram 300 100B Google News Turian NNLM 25, 50, 100, 200 37M Newswire HLBL HLBL 50, 100 37M Newswire Mikolov RNNLM 80, 640 320M Broadcast News Table 1: Embeddings used in our experiments. Dimensionality is the set of dimensions of the word embedding space that we experimented with, and Training Words refers to the size of the unlabelled corpus the embeddings were trained on. may be very useful—for example, a noun is much more likely to follow an adjective than a verb. Curran et al. (2006) report a large improvement using a maximum-entropy Markov model for supertagging, conditioned on the surrounding supertags. We follow Turian et al. (2010) in using a linear chain CRF model for sequence classification using embeddings as features. This model does not allow supervised training to fine-tune the embeddings, though it would be possible to build a CRF/NN hybrid that enabled this. We use the same feature set as with the neural network model—so the probability of a category depends on embeddings, capitalization and suffix features—as well as the previous category. The model is traine</context>
<context position="26637" citStr="Curran et al. (2006)" startWordPosition="4238" endWordPosition="4241">the embeddings supertagger is almost 15% more accurate on this subset than the C&amp;C supertagger, and with a relaxed beam reaches 96% accuracy, showing the advantages of avoiding a pipeline approach. In contrast, the C&amp;C tagger is not robust to POS tagAverage Categories Per Word Figure 4: Ambiguity vs. Accuracy for different supertaggers, on words that do occur in the training data, but not with the category required in the test data. ger errors, and asymptotes at just 82% accuracy. An alternative way of mitigating POS errors is to use a distribution over POS tags as features in the supertagger—Curran et al. (2006) show that this technique improves supertagging accuracy by 0.4% over the C&amp;C baseline, but do not report the impact on parsing. Figure 4 shows performance when the a word has been seen in the training data, but only with a different category from the instance in the test data (for example, European only occurs as a adjective in the training data, but it may occur as a noun in the test data). Performance is even worse on these cases, which appear to be extremely difficult for existing models. The accuracy of the embeddings supertagger converges at just 80%, suggesting that our model has overfi</context>
</contexts>
<marker>Curran, Clark, Vadas, 2006</marker>
<rawString>James R Curran, Stephen Clark, and David Vadas. 2006. Multi-tagging for lexicalized-grammar parsing. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 697–704. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tejaswini Deoskar</author>
<author>Markos Mylonakis</author>
<author>Khalil Sima’an</author>
</authors>
<title>Learning structural dependencies of words in the zipfian tail.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>80--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Deoskar, Mylonakis, Sima’an, 2011</marker>
<rawString>Tejaswini Deoskar, Markos Mylonakis, and Khalil Sima’an. 2011. Learning structural dependencies of words in the zipfian tail. In Proceedings of the 12th International Conference on Parsing Technologies, pages 80–91. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tejaswini Deoskar</author>
</authors>
<institution>Christos Christodoulopoulos,</institution>
<marker>Deoskar, </marker>
<rawString>Tejaswini Deoskar, Christos Christodoulopoulos,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Mark Steedman</author>
</authors>
<title>Generalizing a Strongly Lexicalized Parser using Unlabeled Data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<marker>Birch, Steedman, 2014</marker>
<rawString>Alexandra Birch, and Mark Steedman. 2014. Generalizing a Strongly Lexicalized Parser using Unlabeled Data. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Partha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira.</title>
<date>2007</date>
<pages>1051--1055</pages>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for dependency parsing. In EMNLP-CoNLL, pages 1051–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="12275" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1930" endWordPosition="1933">r sequence classification using embeddings as features. This model does not allow supervised training to fine-tune the embeddings, though it would be possible to build a CRF/NN hybrid that enabled this. We use the same feature set as with the neural network model—so the probability of a category depends on embeddings, capitalization and suffix features—as well as the previous category. The model is trained using the averagedperceptron algorithm (Collins, 2002), again using early-stopping based on development data accuracy. 4 Experiments 4.1 Domains We experiment with three domains: • CCGBank (Hockenmaier and Steedman, 2007), which is a conversion of the Penn Treebank (Marcus et al., 1993) to CCG. Section 23 is used for evaluation. • Wikipedia, using the corpus of 200 sentences annotated with CCG derivations by Honnibal et al. (2009). As the text is out-of-domain, parsing accuracy drops substantially on this corpus. • Biomedical text, which is even less related to the newswire text than Wikipedia, due to large numbers of unseen words and different writing styles, causing low parsing accuracy. For parsing experiments, we use the Bioinfer corpus (Pyysalo et al., 2007) as a test set. For measuring supertagging accur</context>
<context position="29184" citStr="Hockenmaier and Steedman, 2007" startWordPosition="4650" endWordPosition="4653">a. Our embeddings allow the supertagger to learn generalizations, such as that transitive verbs can also often have intransitive uses. Similarly, wh-words can have many possible categories in complex constructions like relative clauses and pied-piping—and our embeddings may help the model generalize from having seen a category for which to one for whom. On the other hand, the C&amp;C supertagger performs much better on prepositions. Prepositions have different categories when appearing as arguments or adjuncts, and the distinction in the gold-standard was made using somewhat arbitrary heuristics (Hockenmaier and Steedman, 2007). It seems our embeddings have failed to capture these subtleties. Future work should explore methods for combining the strengths of each model. Word Type C&amp;C Turian-50 Accuracy Embeddings Accuracy Verbs 93.9% 94.8% Nouns 97.7% 97.3% WH-words 90.1% 93.4% Prepositions 94.8% 91.2% Table 5: Supertagging accuracy on different types of words, with an ambiguity of 1.27 tags per word (corresponding to the C&amp;C’s initial beam setting). Overall performance with this beam is almost identical. Words types were identified using gold POS tags, using IN for prepositions. 4.7 Discussion With a narrow supertag</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Honnibal</author>
<author>Joel Nothman</author>
<author>James R Curran</author>
</authors>
<title>Evaluating a statistical CCG parser on Wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>38--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12488" citStr="Honnibal et al. (2009)" startWordPosition="1968" endWordPosition="1971">ture set as with the neural network model—so the probability of a category depends on embeddings, capitalization and suffix features—as well as the previous category. The model is trained using the averagedperceptron algorithm (Collins, 2002), again using early-stopping based on development data accuracy. 4 Experiments 4.1 Domains We experiment with three domains: • CCGBank (Hockenmaier and Steedman, 2007), which is a conversion of the Penn Treebank (Marcus et al., 1993) to CCG. Section 23 is used for evaluation. • Wikipedia, using the corpus of 200 sentences annotated with CCG derivations by Honnibal et al. (2009). As the text is out-of-domain, parsing accuracy drops substantially on this corpus. • Biomedical text, which is even less related to the newswire text than Wikipedia, due to large numbers of unseen words and different writing styles, causing low parsing accuracy. For parsing experiments, we use the Bioinfer corpus (Pyysalo et al., 2007) as a test set. For measuring supertagging accuracy, we use the CCG annotation produced by Rimell and Clark (2008). 4.2 Neural Network Model Parameters In this section, we explore how adjusting the parameters of our neural network model2 affects 1-best lexical </context>
<context position="20851" citStr="Honnibal et al. (2009)" startWordPosition="3308" endWordPosition="3311">Word Wikipedia Average Categories Per Word Biomedical Average Categories Per Word Figure 2: Ambiguity vs. Accuracy for different supertaggers across different domains. Datapoints for the C&amp;C parser use its standard back-off parameters. 1 2 3 4 C&amp;C Turian-50 Oracle Category Accuracy (%) 100 99 98 97 96 1 2 3 4 C&amp;C Turian-50 Oracle Category Accuracy (%) 100 98 96 94 1 2 3 4 5 C&amp;C Turian-50 Oracle Category Accuracy (%) 100 98 96 94 92 332 Supertagger F1 CCGBank F1 F1 Wikipedia F1 F1 Bioinfer F1 (cov) COV (all) (cov) COV (all) (cov) COV (all) C&amp;C 85.47 99.6 85.30 81.19 99.0 80.64 76.08 97.2 74.88 Honnibal et al. (2009) 85.19 99.8 - 81.75 99.4 - - - - Brown Clusters 85.27 99.9 85.21 80.89 100.0 80.89 76.06 100.0 76.06 Turian-50 Embeddings 86.11 100.0 86.11 82.30 100.0 82.30 78.41 99.8 78.28 Turian-50 + POS tags 85.62 99.9 85.55 81.77 100.0 81.77 77.05 100.0 77.05 Turian-50 + Frequent words 86.04 100.0 86.04 82.44 100.0 82.44 78.10 100.0 78.10 Table 4: Parsing F1-scores for labelled dependencies across a range of domains, using the C&amp;C parser with different supertaggers. Embeddings models used a context window of 7 words, and no additional hidden layer. Following previous CCG parsing work, we report F1-scores</context>
<context position="22762" citStr="Honnibal et al. (2009)" startWordPosition="3617" endWordPosition="3620"> a variablewidth beam 0 that prunes categories whose probability is less than 0 times that of the most likely category. For simplicity, our supertaggers use the same 0 back-off parameters as are used by the C&amp;C parser, though it is possible that further improvements could be gained by carefully tuning these parameters.5 In contrast to the C&amp;C supertaggers, we do not make use a tag-dictionaries. Results are shown in Table 4, and our supertaggers consistently lead to improvements over the baseline parser across all domains, with larger improvements out-of-domain. Our best model also outperforms Honnibal et al. (2009)’s self-training approach to domain adaptation on Wikipedia (which lowers performance on CCGBank). Our results show that word embeddings are an effective way of adding distributional information into CCG supertagging. A popular alternative approach 5We briefly experimented setting the β parameters to match the ambiguity of the C&amp;C supertagger on Section 00 of CCGBank, which caused the F1-score using the Turian-50 embeddings to drop slightly from 86.11 to 85.95. for semi-supervised learning is to use Brown clusters (Brown et al., 1992). To ensure a fair comparison with the Turian embeddings, we</context>
<context position="31904" citStr="Honnibal et al. (2009)" startWordPosition="5078" endWordPosition="5082">n the labelled data, which is shown to be useful for parsing a corpus of questions. Deoskar et al. (2011) and Deoskar et al. (2014) use Viterbi-EM to learn new lexical entries by running a generative parser over a large unlabelled corpus. They show good improvements in accuracy on unseen words, but not overall parsing improvements in-domain. Their parsing model aims to capture non-local information about word usage, which would not be possible for the local context windows used to learn our embeddings. Self-training is another popular method for domain adaptation, and was used successfully by Honnibal et al. (2009) to improve CCG parser performance on Wikipedia. However, it caused a decrease in performance on the in-domain data, and our method achieves better performance across all domains. McClosky et al. (2006) improve a Penn Treebank parser in-domain using self-training, but other work has failed to improve performance out-ofdomain using self training (Dredze et al., 2007). In a similar spirit to our work, Koo et al. (2008) improve parsing accuracy using unsupervised word cluster features—we have shown that word-embeddings outperform Brown clusters for CCG supertagging. An alternative approach to dom</context>
</contexts>
<marker>Honnibal, Nothman, Curran, 2009</marker>
<rawString>Matthew Honnibal, Joel Nothman, and James R Curran. 2009. Evaluating a statistical CCG parser on Wikipedia. In Proceedings of the 2009 Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 38–41. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Honnibal</author>
<author>J R Curran</author>
<author>J Bos</author>
</authors>
<title>Rebanking CCGbank for improved NP interpretation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>207--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35012" citStr="Honnibal et al. (2010)" startWordPosition="5563" endWordPosition="5567">ving a simpler way to improve a parser using embeddings. 6 Conclusions We have shown that CCG parsing can be significantly improved by predicting lexical categories based on unsupervised word embeddings. The resulting parsing pipeline is simpler, and has improved performance both in and out of domain. We expect further improvements to follow as better word embeddings are developed, without other changes to our model. Our approach reduces the problem of sparsity caused by the large number of CCG categories, suggesting that finer-grained categories could be created for CCGBank (in the spirit of Honnibal et al. (2010)), which lead to improved performance in downstream semantic parsers. Future work should also explore domain-adaptation, either using unsupervised embeddings trained on out-of-domain text, or using supervised training on out-of-domain corpora. Our results also have implications for other NLP tasks—suggesting that using word embeddings features may be particularly useful out-of-domain, in pipelines that currently rely on POS taggers, and in tasks which suffer from sparsity in the labelled data. Code for our supertagger is released as part of the EASYCCG parser (Lewis and Steedman, 2014), availa</context>
</contexts>
<marker>Honnibal, Curran, Bos, 2010</marker>
<rawString>M. Honnibal, J.R. Curran, and J. Bos. 2010. Rebanking CCGbank for improved NP interpretation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 207–215. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<date>2008</date>
<note>Simple semi-supervised dependency parsing.</note>
<contexts>
<context position="23598" citStr="Koo et al. (2008)" startWordPosition="3747" endWordPosition="3750"> popular alternative approach 5We briefly experimented setting the β parameters to match the ambiguity of the C&amp;C supertagger on Section 00 of CCGBank, which caused the F1-score using the Turian-50 embeddings to drop slightly from 86.11 to 85.95. for semi-supervised learning is to use Brown clusters (Brown et al., 1992). To ensure a fair comparison with the Turian embeddings, we use clusters trained on the same corpus, and use a comparable feature set (clusters, capitalization, and 2- character suffixes—all implemented as sparse binary features). Brown clusters are hierarchical, and following Koo et al. (2008), we incorporate Brown clusters features at multiple levels of granularity— using 64 coarse clusters (loosely analogous to POStags) and 1000 fine-grained clusters. Results show slightly lower performance than C&amp;C in domain, but higher performance out of domain. However, they are substantially lower than results using the Turian50 embeddings. We also experimented with adding traditional word and POS features, which were implemented as sparse vectors for each word in the context window. We found that including POS features (derived from the C&amp;C POS-tagger) reduced accuracy across all domains. On</context>
<context position="32324" citStr="Koo et al. (2008)" startWordPosition="5149" endWordPosition="5152"> would not be possible for the local context windows used to learn our embeddings. Self-training is another popular method for domain adaptation, and was used successfully by Honnibal et al. (2009) to improve CCG parser performance on Wikipedia. However, it caused a decrease in performance on the in-domain data, and our method achieves better performance across all domains. McClosky et al. (2006) improve a Penn Treebank parser in-domain using self-training, but other work has failed to improve performance out-ofdomain using self training (Dredze et al., 2007). In a similar spirit to our work, Koo et al. (2008) improve parsing accuracy using unsupervised word cluster features—we have shown that word-embeddings outperform Brown clusters for CCG supertagging. An alternative approach to domain adaptation is to annotate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gained from annotating lexical categories, rather than full syntax trees. They achieve higher parsing accuracies than us on biomedical text, but our unsupervised method requires no annotation. It seems likely that our met</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>754--765</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1412" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="205" endWordPosition="208">ents in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy. 1 Introduction Combinatory Categorial Grammar (CCG) is widely used in natural language semantics (Bos, 2008; Kwiatkowski et al., 2010; Krishnamurthy and Mitchell, 2012; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Kwiatkowski et al., 2013), largely because of its direct linkage of syntax and semantics. However, this connection means that performance on semantic applications is highly dependent on the quality of the syntactic parse. Although CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. Supertags are rich lexical categories that go beyond POS tag</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom M. Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 754–765. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higher-order unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1223--1233</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1378" citStr="Kwiatkowski et al., 2010" startWordPosition="201" endWordPosition="204">ds to substantial improvements in dependency parsing results over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy. 1 Introduction Combinatory Categorial Grammar (CCG) is widely used in natural language semantics (Bos, 2008; Kwiatkowski et al., 2010; Krishnamurthy and Mitchell, 2012; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Kwiatkowski et al., 2013), largely because of its direct linkage of syntax and semantics. However, this connection means that performance on semantic applications is highly dependent on the quality of the syntactic parse. Although CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. Supertags are rich lexical</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higher-order unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1223–1233. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with onthe-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1545--1556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1493" citStr="Kwiatkowski et al., 2013" startWordPosition="217" endWordPosition="220">d on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy. 1 Introduction Combinatory Categorial Grammar (CCG) is widely used in natural language semantics (Bos, 2008; Kwiatkowski et al., 2010; Krishnamurthy and Mitchell, 2012; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Kwiatkowski et al., 2013), largely because of its direct linkage of syntax and semantics. However, this connection means that performance on semantic applications is highly dependent on the quality of the syntactic parse. Although CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. Supertags are rich lexical categories that go beyond POS tags by encoding information about predicate-argument structure. Supertagging is “al</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with onthe-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combined Distributional and Logical Semantics.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<pages>192</pages>
<contexts>
<context position="1438" citStr="Lewis and Steedman, 2013" startWordPosition="209" endWordPosition="212"> over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy. 1 Introduction Combinatory Categorial Grammar (CCG) is widely used in natural language semantics (Bos, 2008; Kwiatkowski et al., 2010; Krishnamurthy and Mitchell, 2012; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Kwiatkowski et al., 2013), largely because of its direct linkage of syntax and semantics. However, this connection means that performance on semantic applications is highly dependent on the quality of the syntactic parse. Although CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. Supertags are rich lexical categories that go beyond POS tags by encoding information </context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013a. Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics, 1:179– 192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Unsupervised induction of cross-lingual semantic relations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>681--692</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1438" citStr="Lewis and Steedman, 2013" startWordPosition="209" endWordPosition="212"> over the standard supervised CCG parser when evaluated on Wall Street Journal (0.8%), Wikipedia (1.8%) and biomedical (3.4%) text. We compare the performance of two recently proposed approaches for classification using a wide variety of word embeddings. We also give a detailed error analysis demonstrating where using embeddings outperforms traditional feature sets, and showing how including POS features can decrease accuracy. 1 Introduction Combinatory Categorial Grammar (CCG) is widely used in natural language semantics (Bos, 2008; Kwiatkowski et al., 2010; Krishnamurthy and Mitchell, 2012; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Kwiatkowski et al., 2013), largely because of its direct linkage of syntax and semantics. However, this connection means that performance on semantic applications is highly dependent on the quality of the syntactic parse. Although CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. Supertags are rich lexical categories that go beyond POS tags by encoding information </context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013b. Unsupervised induction of cross-lingual semantic relations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 681–692, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>A* CCG Parsing with a Supertag-factored Model.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Doha, Qatar,</location>
<marker>Lewis, Steedman, 2014</marker>
<rawString>Mike Lewis and Mark Steedman. 2014. A* CCG Parsing with a Supertag-factored Model. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="12341" citStr="Marcus et al., 1993" startWordPosition="1943" endWordPosition="1946">llow supervised training to fine-tune the embeddings, though it would be possible to build a CRF/NN hybrid that enabled this. We use the same feature set as with the neural network model—so the probability of a category depends on embeddings, capitalization and suffix features—as well as the previous category. The model is trained using the averagedperceptron algorithm (Collins, 2002), again using early-stopping based on development data accuracy. 4 Experiments 4.1 Domains We experiment with three domains: • CCGBank (Hockenmaier and Steedman, 2007), which is a conversion of the Penn Treebank (Marcus et al., 1993) to CCG. Section 23 is used for evaluation. • Wikipedia, using the corpus of 200 sentences annotated with CCG derivations by Honnibal et al. (2009). As the text is out-of-domain, parsing accuracy drops substantially on this corpus. • Biomedical text, which is even less related to the newswire text than Wikipedia, due to large numbers of unseen words and different writing styles, causing low parsing accuracy. For parsing experiments, we use the Bioinfer corpus (Pyysalo et al., 2007) as a test set. For measuring supertagging accuracy, we use the CCG annotation produced by Rimell and Clark (2008)</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>152--159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="32106" citStr="McClosky et al. (2006)" startWordPosition="5112" endWordPosition="5115">rser over a large unlabelled corpus. They show good improvements in accuracy on unseen words, but not overall parsing improvements in-domain. Their parsing model aims to capture non-local information about word usage, which would not be possible for the local context windows used to learn our embeddings. Self-training is another popular method for domain adaptation, and was used successfully by Honnibal et al. (2009) to improve CCG parser performance on Wikipedia. However, it caused a decrease in performance on the in-domain data, and our method achieves better performance across all domains. McClosky et al. (2006) improve a Penn Treebank parser in-domain using self-training, but other work has failed to improve performance out-ofdomain using self training (Dredze et al., 2007). In a similar spirit to our work, Koo et al. (2008) improve parsing accuracy using unsupervised word cluster features—we have shown that word-embeddings outperform Brown clusters for CCG supertagging. An alternative approach to domain adaptation is to annotate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gai</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics, pages 152–159. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="13841" citStr="Mikolov et al. (2013)" startWordPosition="2190" endWordPosition="2193">on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to capture a variety of syntactic and semantic phenomena, based on neural network language models (NNLMs) (Turian et al., 2010; Collobert et al., 2011b), recurrent neural network language models (Mikolov, 2012), the hierarchical log bilinear model (HLBL) (Mnih and Hinton, 2008), and Mikolov et al. (2013)’s Skip Gram model. However, there has been a lack of experiments comparing which embeddings provide the most effective features for downstream tasks. First, we investigated the performance of several publicly available embeddings, to find which was most effective for supertagging. The embeddings we used are summarized in Table 1. For efficiency, we used our simplest architecture, with no additional 2Implemented using the Torch7 library (Collobert et al., 2011a) 330 Embeddings Category Category Accuracy Accuracy (window=5) (window=7) Collobert&amp;Weston 90.0% 89.6% Skip Gram 90.9% 91.0% Turian-25</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>Ph.D. thesis, Ph. D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="13746" citStr="Mikolov, 2012" startWordPosition="2177" endWordPosition="2178">nk (all development was done on this data). The C&amp;C supertagger achieves 91.5% accuracy on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to capture a variety of syntactic and semantic phenomena, based on neural network language models (NNLMs) (Turian et al., 2010; Collobert et al., 2011b), recurrent neural network language models (Mikolov, 2012), the hierarchical log bilinear model (HLBL) (Mnih and Hinton, 2008), and Mikolov et al. (2013)’s Skip Gram model. However, there has been a lack of experiments comparing which embeddings provide the most effective features for downstream tasks. First, we investigated the performance of several publicly available embeddings, to find which was most effective for supertagging. The embeddings we used are summarized in Table 1. For efficiency, we used our simplest architecture, with no additional 2Implemented using the Torch7 library (Collobert et al., 2011a) 330 Embeddings Category Category Accur</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´aˇs Mikolov. 2012. Statistical language models based on neural networks. Ph.D. thesis, Ph. D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="13814" citStr="Mnih and Hinton, 2008" startWordPosition="2185" endWordPosition="2188">ger achieves 91.5% accuracy on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to capture a variety of syntactic and semantic phenomena, based on neural network language models (NNLMs) (Turian et al., 2010; Collobert et al., 2011b), recurrent neural network language models (Mikolov, 2012), the hierarchical log bilinear model (HLBL) (Mnih and Hinton, 2008), and Mikolov et al. (2013)’s Skip Gram model. However, there has been a lack of experiments comparing which embeddings provide the most effective features for downstream tasks. First, we investigated the performance of several publicly available embeddings, to find which was most effective for supertagging. The embeddings we used are summarized in Table 1. For efficiency, we used our simplest architecture, with no additional 2Implemented using the Torch7 library (Collobert et al., 2011a) 330 Embeddings Category Category Accuracy Accuracy (window=5) (window=7) Collobert&amp;Weston 90.0% 89.6% Skip</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos G´omez Rodr´ıguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>833--841</pages>
<location>Beijing, China,</location>
<marker>Nivre, Rimell, McDonald, Rodr´ıguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos G´omez Rodr´ıguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 833–841, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of conditional random fields (CRFs).</title>
<marker>Okazaki, </marker>
<rawString>Naoaki Okazaki. CRFsuite: a fast implementation of conditional random fields (CRFs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Filip Ginter</author>
<author>Juho Heimonen</author>
<author>Jari Bj¨orne</author>
<author>Jorma Boberg</author>
<author>Jouni J¨arvinen</author>
<author>Tapio Salakoski</author>
</authors>
<title>Bioinfer: a corpus for information extraction in the biomedical domain.</title>
<date>2007</date>
<journal>BMC bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<marker>Pyysalo, Ginter, Heimonen, Bj¨orne, Boberg, J¨arvinen, Salakoski, 2007</marker>
<rawString>Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bj¨orne, Jorma Boberg, Jouni J¨arvinen, and Tapio Salakoski. 2007. Bioinfer: a corpus for information extraction in the biomedical domain. BMC bioinformatics, 8(1):50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Adapting a lexicalized-grammar parser to contrasting domains.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>475--484</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12941" citStr="Rimell and Clark (2008)" startWordPosition="2042" endWordPosition="2045">k (Marcus et al., 1993) to CCG. Section 23 is used for evaluation. • Wikipedia, using the corpus of 200 sentences annotated with CCG derivations by Honnibal et al. (2009). As the text is out-of-domain, parsing accuracy drops substantially on this corpus. • Biomedical text, which is even less related to the newswire text than Wikipedia, due to large numbers of unseen words and different writing styles, causing low parsing accuracy. For parsing experiments, we use the Bioinfer corpus (Pyysalo et al., 2007) as a test set. For measuring supertagging accuracy, we use the CCG annotation produced by Rimell and Clark (2008). 4.2 Neural Network Model Parameters In this section, we explore how adjusting the parameters of our neural network model2 affects 1-best lexical category accuracy on the Section 00 of CCGBank (all development was done on this data). The C&amp;C supertagger achieves 91.5% accuracy on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to ca</context>
<context position="32595" citStr="Rimell and Clark (2008)" startWordPosition="5187" endWordPosition="5190">a decrease in performance on the in-domain data, and our method achieves better performance across all domains. McClosky et al. (2006) improve a Penn Treebank parser in-domain using self-training, but other work has failed to improve performance out-ofdomain using self training (Dredze et al., 2007). In a similar spirit to our work, Koo et al. (2008) improve parsing accuracy using unsupervised word cluster features—we have shown that word-embeddings outperform Brown clusters for CCG supertagging. An alternative approach to domain adaptation is to annotate a small corpus of out-of-domain text. Rimell and Clark (2008) argue that this annotation is simpler for lexicalized formalisms such as CCG, as large improvements can be gained from annotating lexical categories, rather than full syntax trees. They achieve higher parsing accuracies than us on biomedical text, but our unsupervised method requires no annotation. It seems likely that our method could be further improved by incorporating out-ofdomain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still use</context>
</contexts>
<marker>Rimell, Clark, 2008</marker>
<rawString>Laura Rimell and Stephen Clark. 2008. Adapting a lexicalized-grammar parser to contrasting domains. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 475–484. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Porting a lexicalized-grammar parser to the biomedical domain.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>42</volume>
<issue>5</issue>
<contexts>
<context position="4969" citStr="Rimell and Clark, 2009" startWordPosition="762" endWordPosition="765">e first sentences are POS-tagged, then supertagged, and then parsed. The supertagger outputs a distribution over tags for each word, and a beam is used to aggressively prune supertags to reduce the parser search space. If the parser is unable to find a parse with a given set of supertags, the beam is relaxed. This approach is known as adaptive supertagging. The pipeline approach has two major drawbacks. Firstly, the use of a POS-tagger can overly prune the search space for the supertagger. Whilst POStaggers have an accuracy of around 97% in domain, this drops to just 93.4% on biomedical text (Rimell and Clark, 2009), meaning that most sentences will contain an erroneous POS-tag. The supertagger model is overly dependent on POS-features—in Section 4.6 we show that supertagger performance drops dramatically on words which have been assigned an incorrect POS-tag. Secondly, both the POS-tagger and supertagger are highly reliant on lexical features, meaning that performance drops both on unknown words, and words used differently from the training data. Many common words do not appear at all in the training data of the Penn Treebank, such as ten, militants, insight, and teenager. Many others are not seen with </context>
</contexts>
<marker>Rimell, Clark, 2009</marker>
<rawString>Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. Journal of Biomedical Informatics, 42(5):852–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded dependency recovery for parser evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>813--821</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1766" citStr="Rimell et al., 2009" startWordPosition="260" endWordPosition="263">tperforms traditional feature sets, and showing how including POS features can decrease accuracy. 1 Introduction Combinatory Categorial Grammar (CCG) is widely used in natural language semantics (Bos, 2008; Kwiatkowski et al., 2010; Krishnamurthy and Mitchell, 2012; Lewis and Steedman, 2013a; Lewis and Steedman, 2013b; Kwiatkowski et al., 2013), largely because of its direct linkage of syntax and semantics. However, this connection means that performance on semantic applications is highly dependent on the quality of the syntactic parse. Although CCG parsers perform at state-of-the-art levels (Rimell et al., 2009; Nivre et al., 2010), fullsentence accuracy is just 25.6% on Wikipedia text, which gives a low upper bound on logical inference approaches to question-answering and textual entailment. Supertags are rich lexical categories that go beyond POS tags by encoding information about predicate-argument structure. Supertagging is “almost parsing”, and is used by parsers based on strongly lexicalized formalisms such as CCG and TAG to improve accuracy and efficiency, by delegating many of the parsing decisions to finite-state models (Bangalore and Joshi, 1999). A disadvantage of this approach is that la</context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency recovery for parser evaluation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 813–821. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="6235" citStr="Socher et al., 2013" startWordPosition="965" endWordPosition="968">nly occurs as an adjective, never a noun, meaning that the C&amp;C parser is unable to analyse simple sentences like The director of the IMF is traditionally a European. These problems are particularly acute when parsing other domains (Rimell and Clark, 2009). 2.2 Semi Supervised NLP using Word Embeddings Recent work has explored using vector space embeddings for words as features in supervised models for a variety of tasks, such as POS-tagging, chunking, named-entity recognition, semantic role labelling, and phrase structure parsing (Turian et al., 2010; Collobert et al., 2011b; Collobert, 2011; Socher et al., 2013). The major motivation for using these techniques has been to minimize the level of task-specific feature engineering required, as the same feature set can lead to good results on a variety of tasks. Performance varies between tasks, but any gains over state-of-the-art traditional features have been small. A variety of techniques have been used for learning such embeddings from large unlabelled corpora, such as neural-network language models. 3 Models We introduce models for predicting CCG lexical categories based on vector-space embeddings. The models can then be used to replace the POStaggin</context>
<context position="33865" citStr="Socher et al. (2013)" startWordPosition="5388" endWordPosition="5391">ng further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art existing feature sets for constituency parsing—although Bansal et al. (2014) achieve good results for dependency parsing using embeddings. CCG categories contain much of the hierarchical structure needed for parsing, giving a simpler way to improve a parser using embeddings. 6 Conclusions We h</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Thomforde</author>
<author>Mark Steedman</author>
</authors>
<title>Semisupervised CCG lexicon extension.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1246--1256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2624" citStr="Thomforde and Steedman, 2011" startWordPosition="395" endWordPosition="398">o beyond POS tags by encoding information about predicate-argument structure. Supertagging is “almost parsing”, and is used by parsers based on strongly lexicalized formalisms such as CCG and TAG to improve accuracy and efficiency, by delegating many of the parsing decisions to finite-state models (Bangalore and Joshi, 1999). A disadvantage of this approach is that larger sets of lexical categories mean increased sparsity, decreasing tagging accuracy. As large amounts of labelled data are unlikely to be made available, recent work has explored using unlabelled data to improve parser lexicons (Thomforde and Steedman, 2011; Deoskar et al., 2011; Deoskar et al., 2014). However, existing work has failed to improve the overall accuracy of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Collobert et al., 2011b), largely motivated as a simpler and more general alternative to standard feature sets. We apply similar techniques to CCG supertagging, hypothesising that words which are close in the embedding space will have similar supertags. Most existing work has focused on flat tagging tas</context>
<context position="31016" citStr="Thomforde and Steedman (2011)" startWordPosition="4929" endWordPosition="4932">rom the labelled data. Interestingly, the best-performing Turian-50 embeddings were trained on just 37M words of text (compared to 100B words for the Skip-gram embeddings), suggesting that further improvements may well be possible using larger unlabelled corpora. Future work should investigate whether the models and embeddings that work well for supertagging generalize to other tasks. 5 Related Work Many methods have recently been proposed for improving supervised parsers with unlabelled data. Most of these are orthogonal to our work, and larger improvements may be possible by combining them. Thomforde and Steedman (2011) extends a CCG lexicon by inferring categories for unseen words, based on the likely categories of surrounding words. Unlike our method, this approach is able to learn 1 2 3 4 C&amp;C Turian-50 Oracle Category Accuracy (%) 100 98 96 94 335 categories which were unseen in the labelled data, which is shown to be useful for parsing a corpus of questions. Deoskar et al. (2011) and Deoskar et al. (2014) use Viterbi-EM to learn new lexical entries by running a generative parser over a large unlabelled corpus. They show good improvements in accuracy on unseen words, but not overall parsing improvements i</context>
</contexts>
<marker>Thomforde, Steedman, 2011</marker>
<rawString>Emily Thomforde and Mark Steedman. 2011. Semisupervised CCG lexicon extension. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1246–1256. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2918" citStr="Turian et al., 2010" startWordPosition="439" endWordPosition="442">galore and Joshi, 1999). A disadvantage of this approach is that larger sets of lexical categories mean increased sparsity, decreasing tagging accuracy. As large amounts of labelled data are unlikely to be made available, recent work has explored using unlabelled data to improve parser lexicons (Thomforde and Steedman, 2011; Deoskar et al., 2011; Deoskar et al., 2014). However, existing work has failed to improve the overall accuracy of state-of-the-art supervised parsers in-domain. Another strand of recent work has explored using unsupervised word embeddings as features in supervised models (Turian et al., 2010; Collobert et al., 2011b), largely motivated as a simpler and more general alternative to standard feature sets. We apply similar techniques to CCG supertagging, hypothesising that words which are close in the embedding space will have similar supertags. Most existing work has focused on flat tagging tasks, and has not produced state-of-the-art results on structured prediction tasks like parsing (Collobert, 2011; Andreas and Klein, 2014). CCG’s lexicalized nature provides a simple and elegant solution to treating parsing as a flat tagging task, as the lexical categories encode information abo</context>
<context position="6171" citStr="Turian et al., 2010" startWordPosition="955" endWordPosition="958">re not seen with all their possible uses—for example European only occurs as an adjective, never a noun, meaning that the C&amp;C parser is unable to analyse simple sentences like The director of the IMF is traditionally a European. These problems are particularly acute when parsing other domains (Rimell and Clark, 2009). 2.2 Semi Supervised NLP using Word Embeddings Recent work has explored using vector space embeddings for words as features in supervised models for a variety of tasks, such as POS-tagging, chunking, named-entity recognition, semantic role labelling, and phrase structure parsing (Turian et al., 2010; Collobert et al., 2011b; Collobert, 2011; Socher et al., 2013). The major motivation for using these techniques has been to minimize the level of task-specific feature engineering required, as the same feature set can lead to good results on a variety of tasks. Performance varies between tasks, but any gains over state-of-the-art traditional features have been small. A variety of techniques have been used for learning such embeddings from large unlabelled corpora, such as neural-network language models. 3 Models We introduce models for predicting CCG lexical categories based on vector-space </context>
<context position="11607" citStr="Turian et al. (2010)" startWordPosition="1825" endWordPosition="1828">Turian NNLM 25, 50, 100, 200 37M Newswire HLBL HLBL 50, 100 37M Newswire Mikolov RNNLM 80, 640 320M Broadcast News Table 1: Embeddings used in our experiments. Dimensionality is the set of dimensions of the word embedding space that we experimented with, and Training Words refers to the size of the unlabelled corpus the embeddings were trained on. may be very useful—for example, a noun is much more likely to follow an adjective than a verb. Curran et al. (2006) report a large improvement using a maximum-entropy Markov model for supertagging, conditioned on the surrounding supertags. We follow Turian et al. (2010) in using a linear chain CRF model for sequence classification using embeddings as features. This model does not allow supervised training to fine-tune the embeddings, though it would be possible to build a CRF/NN hybrid that enabled this. We use the same feature set as with the neural network model—so the probability of a category depends on embeddings, capitalization and suffix features—as well as the previous category. The model is trained using the averagedperceptron algorithm (Collins, 2002), again using early-stopping based on development data accuracy. 4 Experiments 4.1 Domains We exper</context>
<context position="13662" citStr="Turian et al., 2010" startWordPosition="2162" endWordPosition="2165">neural network model2 affects 1-best lexical category accuracy on the Section 00 of CCGBank (all development was done on this data). The C&amp;C supertagger achieves 91.5% accuracy on this task. The models were trained on Sections 02-21 of CCGBank, and the reported numbers are the best accuracy achieved on Section 00. As in Clark and Curran (2007), all models use only the 425 most frequent categories in CCGBank. 4.2.1 Embeddings A number of word embeddings have recently been released, aiming to capture a variety of syntactic and semantic phenomena, based on neural network language models (NNLMs) (Turian et al., 2010; Collobert et al., 2011b), recurrent neural network language models (Mikolov, 2012), the hierarchical log bilinear model (HLBL) (Mnih and Hinton, 2008), and Mikolov et al. (2013)’s Skip Gram model. However, there has been a lack of experiments comparing which embeddings provide the most effective features for downstream tasks. First, we investigated the performance of several publicly available embeddings, to find which was most effective for supertagging. The embeddings we used are summarized in Table 1. For efficiency, we used our simplest architecture, with no additional 2Implemented using</context>
<context position="17895" citStr="Turian et al. (2010)" startWordPosition="2829" endWordPosition="2832">to modify the embeddings during supervised training (in contrast to the neural-network model). Consequently, we built a new set of embeddings, using the weightmatrix learned by our best neural network model. A new CRF model was then trained using the tuned embeddings. Performance then improved dramatically to 91.5%, and slightly outperformed the neural network—showing that while there is a small advantage to using sequence information, it is crucial to allow supervised training to modify the embeddings. These results help explain why Collobert et al. (2011b)’s neural network models outperform Turian et al. (2010)’s sequence models—but greater 3Implemented using CRFSuite (Okazaki, ) 331 improvements may be possible with the combined approach we introduce here, which allows the model to both tune the embeddings and exploit sequence information. However, tagging with this model was considerably slower than the neural network (again, due to the cost of decoding), so we used the neural network architecture in the remaining experiments. 4.4 Multitagging Accuracy The C&amp;C parser takes a set of supertags per word as input, which is used to prune the search space. If no parse is found, the sentence is supertagg</context>
<context position="33575" citStr="Turian et al., 2010" startWordPosition="5344" endWordPosition="5347"> incorporating out-ofdomain labelled data (where available). The best reported CCG parsing results have been achieved with a model that integrates supertagging and parsing (Auli and Lopez, 2011a; Auli and Lopez, 2011b). This work still uses the same feature set as the C&amp;C parser, suggesting further improvements may be possible by using our embeddings features. Auli and Lopez POS-tag the sentence before parsing, but using our features would allow us to fully eliminate the current pipeline approach to CCG parsing. Our work also builds on approaches to semisupervised NLP using neural embeddings (Turian et al., 2010; Collobert et al., 2011b). Existing work has mainly focused on ‘flat’ tagging problems, without hierarchical structure. Collobert (2011) gives a model for parsing using embeddings features, by treating each level of the parse tree as a sequence classification problem. Socher et al. (2013) introduce a model in which context-free grammar parses are reranked based on compositional distributional representations for each node. Andreas and Klein (2014) experiment with a number of approaches to improving the Berkeley parser with word embeddings. Such work has not improved over state-of-the-art exis</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
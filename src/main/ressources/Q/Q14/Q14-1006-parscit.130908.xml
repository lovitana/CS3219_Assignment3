<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.9120915">
From image descriptions to visual denotations:
New similarity metrics for semantic inference over event descriptions
</title>
<author confidence="0.987181">
Peter Young Alice Lai Micah Hodosh Julia Hockenmaier
</author>
<affiliation confidence="0.99867">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.990093">
{pyoung2, aylai2, mhodosh2, juliahmr}@illinois.edu
</email>
<sectionHeader confidence="0.996577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999122181818182">
We propose to use the visual denotations of
linguistic expressions (i.e. the set of images
they describe) to define novel denotational
similarity metrics, which we show to be at
least as beneficial as distributional similarities
for two tasks that require semantic inference.
To compute these denotational similarities, we
construct a denotation graph, i.e. a subsump-
tion hierarchy over constituents and their de-
notations, based on a large corpus of 30K im-
ages and 150K descriptive captions.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991125">
The ability to draw inferences from text is a prereq-
uisite for language understanding. These inferences
are what makes it possible for even brief descrip-
tions of everyday scenes to evoke rich mental im-
ages. For example, we would expect an image of
people shopping in a supermarket to depict aisles
of produce or other goods, and we would expect
most of these people to be customers who are either
standing or walking around. But such inferences
require a great deal of commonsense world knowl-
edge. Standard distributional approaches to lexical
similarity (Section 2.1) are very effective at iden-
tifying which words are related to the same topic,
and can provide useful features for systems that per-
form semantic inferences (Mirkin et al., 2009), but
are not suited to capture precise entailments between
complex expressions. In this paper, we propose a
novel approach for the automatic acquisition of de-
notational similarities between descriptions of ev-
eryday situations (Section 2). We define the (visual)
denotation of a linguistic expression as the set of im-
ages it describes. We create a corpus of images of
everyday activities (each paired with multiple cap-
tions; Section 3) to construct a large scale visual de-
notation graph which associates image descriptions
with their denotations (Section 4). The algorithm
that constructs the denotation graph uses purely syn-
tactic and lexical rules to produce simpler captions
(which have a larger denotation). But since each
image is originally associated with several captions,
the graph can also capture similarities between syn-
tactically and lexically unrelated descriptions. We
apply these similarities to two different tasks (Sec-
tions 6 and 7): an approximate entailment recogni-
tion task for our domain, where the goal is to decide
whether the hypothesis (a brief image caption) refers
to the same image as the premises (four longer cap-
tions), and the recently introduced Semantic Textual
Similarity task (Agirre et al., 2012), which can be
viewed as a graded (rather than binary) version of
paraphrase detection. Both tasks require semantic
inference, and our results indicate that denotational
similarities are at least as effective as standard ap-
proaches to similarity. Our code and data set, as
well as the denotation graph itself and the lexical
similarities we define over it are available for re-
search purposes at http://nlp.cs.illinois.edu/
Denotation.html.
</bodyText>
<sectionHeader confidence="0.954903" genericHeader="introduction">
2 Towards Denotational Similarities
</sectionHeader>
<subsectionHeader confidence="0.840338">
2.1 Distributional Similarities
</subsectionHeader>
<bodyText confidence="0.9995045">
The distributional hypothesis posits that linguistic
expressions that appear in similar contexts have a
</bodyText>
<page confidence="0.99385">
67
</page>
<note confidence="0.846474076923077">
Transactions of the Association for Computational Linguistics, 2 (2014) 67–78. Action Editor: Lillian Lee.
Submitted 6/2013; Revised 10/2013; Published 2/2014. c�2014 Association for Computational Linguistics.
Gray haired man in black suit and yellow tie working in a financial environment.
A graying man in a suit is perplexed at a business meeting.
A businessman in a yellow tie gives a frustrated look.
A man in a yellow tie is rubbing the back of his neck.
A man with a yellow tie looks concerned.
A butcher cutting an animal to sell.
A green-shirted man with a butcher’s apron uses a knife to carve out the hanging carcass of a cow.
A man at work, butchering a cow.
A man in a green t-shirt and long tan apron hacks apart the carcass of a cow
while another man hoses away the blood.
Two men work in a butcher shop; one cuts the meat from a butchered cow, while the other hoses the floor.
</note>
<figureCaption confidence="0.954587">
Figure 1: Two images from our data set and their five captions
</figureCaption>
<bodyText confidence="0.9999269">
similar meaning (Harris, 1954). This has led to the
definition of vector-based distributional similarities,
which represent each word w as a vector w derived
from counts of w’s co-occurrence with other words.
These vectors can be used directly to compute the
lexical similarities of words, either via the cosine
of the angle between them, or via other, more com-
plex metrics (Lin, 1998). More recently, asymmetric
similarities have been proposed as more suitable for
semantic inference tasks such as entailment (Weeds
and Weir, 2003; Szpektor and Dagan, 2008; Clarke,
2009; Kotlerman et al., 2010). Distributional word
vectors can also be used to define the compositional
similarity of longer strings (Mitchell and Lapata,
2010). To compute the similarity of two strings, the
lexical vectors of the words in each string are first
combined into a single vector (e.g. by element-wise
addition or multiplication), and then an appropriate
vector similarity (e.g. cosine) is applied to the re-
sulting pair of vectors.
</bodyText>
<subsectionHeader confidence="0.991624">
2.2 Visual Denotations
</subsectionHeader>
<bodyText confidence="0.9933155">
Our approach is inspired by truth-conditional se-
mantic theories in which the denotation of a declar-
ative sentence is assumed to be the set of all situa-
tions or possible worlds in which the sentence is true
(Montague, 1974; Dowty et al., 1981; Barwise and
Perry, 1980). Restricting our attention to visually
descriptive sentences, i.e. non-negative, episodic
(Carlson, 2005) sentences that can be used to de-
scribe an image (Figure 1), we propose to instantiate
the abstract notions of possible worlds or situations
with concrete sets of images. The interpretation
function J�K maps sentences to their visual denota-
tions JsK, which is the set of images i E U$ C_ U in
a ‘universe’ of images U that s describes:
</bodyText>
<equation confidence="0.911361">
JsK = {i E U  |s is a truthful description of i} (1)
</equation>
<bodyText confidence="0.999926">
Similarly, we map nouns and noun phrases to the
set of images that depict the objects they describe,
and verbs and verb phrases to the set of images that
depict the events they describe.
</bodyText>
<subsectionHeader confidence="0.997375">
2.3 Denotation Graphs
</subsectionHeader>
<bodyText confidence="0.999894555555556">
Denotations induce a partial ordering over descrip-
tions: if s (e.g. “a poodle runs on the beach”) en-
tails a description s&apos; (e.g. “a dog runs”), its denota-
tion is a subset of the denotation of s&apos; (JsK C_ Js&apos;K),
and we say that s&apos; subsumes the more specific s
(s&apos; C s). In our domain of descriptive sentences,
we can obtain more generic descriptions by simple
syntactic and lexical operations ω E O C S x S
that preserve upward entailment, so that if ω(s) =
s&apos;, JsK C_ Js&apos;K. We consider three types of oper-
ations: the removal of optional material (e.g PPs
like on the beach), the extraction of simpler con-
stituents (NPs, VPs, or simple Ss), and lexical sub-
stitutions of nouns by their hypernyms (poodle -+
dog). These operations are akin to the atomic ed-
its of MacCartney and Manning (2008)’s NatLog
system, and allow us to construct large subsump-
tion hierarchies over image descriptions, which we
call denotation graphs. Given a set of (upward
entailment-preserving) operations O C S x S, the
denotation graph DG = (E, V ) of a set of images I
and a set of strings S represents a subsumption hier-
archy in which each node V = (s, JsK) corresponds
to a string s E S and its denotation JsK C_ I. Di-
rected edges e = (s, s&apos;) E E C_ V x V indicate a
subsumption relations C s&apos; between a more generic
expression s and its child s&apos;. An edge from s to s&apos;
</bodyText>
<page confidence="0.998891">
68
</page>
<bodyText confidence="0.999491333333333">
exists if there is an operation w E O that reduces the
string s&apos; to s (i.e. w(s&apos;) = s) and its inverse w−1
expands the string s to s&apos; (i.e. w−1(s) = s&apos;).
</bodyText>
<subsectionHeader confidence="0.996053">
2.4 Denotational Similarities
</subsectionHeader>
<bodyText confidence="0.9995171">
Given a denotation graph over N images, we esti-
mate the denotational probability of an expressions
with a denotation of size |JsK |as PQ➢(s) = |JsK|/N,
and the joint probability of two expressions analo-
gously as PQ➢(s, s&apos;) = |JsK n Js&apos;K|/N. The condi-
tional probability PQ➢(s  |s&apos;) indicates how likely
s is to be true when s&apos; holds, and yields a simple
directed denotational similarity. The (normalized)
pointwise mutual information (PMI) (Church and
Hanks, 1990) defines a symmetric similarity:
</bodyText>
<equation confidence="0.801565333333333">
( PJK(S,S�) )
log PJK(S)PJK(S�)
− log(PQ➢(s, s&apos;))
We set PQ➢(s|s) = nPMI Q➢(s, s) = 1, and, if s or
s&apos; are not in the denotation graph, nPMI Q➢(s, s&apos;) =
PQ➢(s, s&apos;) = 0.
</equation>
<sectionHeader confidence="0.958274" genericHeader="method">
3 Our Data Set
</sectionHeader>
<bodyText confidence="0.997901769230769">
Our data set (Figure 1) consists of 31,783 pho-
tographs of everyday activities, events and scenes
(all harvested from Flickr) and 158,915 captions
(obtained via crowdsourcing). It contains and ex-
tends Hodosh et al. (2013)’s corpus of 8,092 im-
ages. We followed Hodosh et al. (2013)’s approach
to collect images. We also use their annotation
guidelines, and use similar quality controls to cor-
rect spelling mistakes, eliminate ungrammatical or
non-descriptive sentences. Almost all of the im-
ages that we add to those collected by Hodosh et
al. (2013) have been made available under a Cre-
ative Commons license. Each image is described in-
dependently by five annotators who are not familiar
with the specific entities and circumstances depicted
in them, resulting in captions such as “Three people
setting up a tent”, rather than the kind of captions
people provide for their own images (“Our trip to
the Olympic Peninsula”). Moreover, different an-
notators use different levels of specificity, from de-
scribing the overall situation (performing a musical
piece) to specific actions (bowing on a violin). This
variety of descriptions associated with the same im-
age is what allows us to induce denotational similari-
ties between expressions that are not trivially related
by syntactic rewrite rules.
</bodyText>
<sectionHeader confidence="0.931703" genericHeader="method">
4 Constructing the Denotation Graph
</sectionHeader>
<bodyText confidence="0.999961441860465">
The construction of the denotation graph consists
of the following steps: preprocessing and linguistic
analysis of the captions, identification of applicable
transformations, and generation of the graph itself.
Preprocessing and Linguistic Analysis We use
the Linux spell checker, the OpenNLP tok-
enizer, POS tagger and chunker (http://opennlp.
apache.org), and the Malt parser (Nivre et al.,
2006) to analyze the captions. Since the vocabulary
of our corpus differs significantly from the data these
tools are trained on, we resort to a number of heuris-
tics to improve the analyses they provide. Since
some heuristics require us to identify different entity
types, we developed a lexicon of the most common
entity types in our domain (people, clothing, bodily
appearance (e.g. hair or body parts), containers of
liquids, food items and vehicles).
After spell-checking, we normalize certain words
and compounds with several spelling variations, e.g.
barbecue (barbeque, BBQ), gray (grey), waterski
(water ski), brown-haired (brown haired), and to-
kenize the captions using the OpenNLP tokenizer.
The OpenNLP POS tagger makes a number of sys-
tematic errors on our corpus (e.g. mistagging main
verbs as nouns). Since these errors are highly sys-
tematic, we are able to correct them automatically
by applying deterministic rules (e.g. climbs is never
a noun in our corpus, stand is a noun if it is pre-
ceded by vegetable but a verb when preceded by a
noun that refers to people). These fixes apply to
27,784 (17% of the 158,915 image captions). Next,
we use the OpenNLP chunker to create a shallow
parse. Fixing its (systematic) errors affects 28,587
captions. We then analyze the structure of each
NP chunk to identify heads, determiners and pre-
nominal modifiers. The head may include more than
a single token if WordNet (or our hypernym lexi-
con, described below) contains a corresponding en-
try (e.g. little girl). Determiners include phrases
such as a couple or a few. Although we use the
Malt parser (Nivre et al., 2006) to identify subject-
verb-object dependencies, we have found it more ac-
curate to develop deterministic heuristics and lexi-
</bodyText>
<equation confidence="0.772386">
nPMI Q➢(s, s&apos;) =
</equation>
<page confidence="0.98443">
69
</page>
<bodyText confidence="0.999631057142857">
cal rules to identify the boundaries of complex (e.g.
conjoined) NPs, allowing us to treat “a man with red
shoes and a white hat” as an NP followed by a sin-
gle PP, but “a man with red shoes and a white-haired
woman” as two NPs, and to transform e.g. “stand-
ing by a man and a woman” into “standing” and not
“standing and a woman” when dropping the PP.
Hypernym Lexicon We use our corpus and Word-
Net to construct a hypernym lexicon that allows us
to replace head nouns with more generic terms. We
only consider hypernyms that occur themselves with
sufficient frequency in the original captions (replac-
ing “adult” with “person”, but not with “organ-
ism”). Since the language in our corpus is very
concrete, each noun tends to have a single sense, al-
lowing us to always replace it with the same hyper-
nyms.1 But since WordNet provides us with mul-
tiple senses for most nouns, we first have to iden-
tify which sense is used in our corpus. To do this,
we use the heuristic cross-caption coreference algo-
rithm of Hodosh et al. (2010) to identify coreferent
NP chunks among the original five captions of each
image.2 For each ambiguous head noun, we con-
sider every non-singleton coreference chains it ap-
pears in, and reduce its synsets to those that stand
in a hypernym-hyponym relation with at least one
other head noun in the chain. Finally, we apply a
greedy majority voting algorithm to iteratively nar-
row down each term’s senses to a single synset that
is compatible with the largest number of coreference
chains it occurs in.
Caption Normalization In order to increase the
recall of the denotations we capture, we drop all
punctuation marks, and lemmatize nouns, verbs, and
adjectives that end in “-ed” or “-ing” before gener-
</bodyText>
<subsectionHeader confidence="0.607574">
1Descriptions of people that refer to both age and gen-
</subsectionHeader>
<bodyText confidence="0.967796708333333">
der (e.g. “man”) can have multiple distinct hypernyms
(“adult”/’“male”). Because our annotators never describe
young children or babies as “persons”, we only allow terms
that are likely to describe adults or teenagers (including occu-
pations) to be replaced by the term “person”. This means that
the term “girl” has two senses: a female child (the default) or a
younger woman. We distinguish the two senses in a preprocess-
ing step: if the other captions of the same image do not mention
children, but refer to teenaged or adult women, we assign girl
the woman-sense. Some nouns that end in -er (e.g. “diner”,
“pitcher” also violate our monosemy assumption.
2Coreference resolution has also been used for word sense
disambiguation by Preiss (2001) and Hu and Liu (2011).
ating the denotation graph. In order to distinguish
between frequently occurring homonyms where the
noun is unrelated to the verb, we change all forms of
the verb dress to dressed, all forms of the verb stand
to standing and all forms of the verb park to park-
ing. Finally, we drop sentence-initial there/here/this
is/are (as in there is a dog splashing in the water),
and normalize the expressions in X and dressed (up)
in X (where X is an article of clothing or a color) to
wear X. We reduce plural determiners to {two, three,
some}, and drop singular determiners except for no.
</bodyText>
<subsectionHeader confidence="0.998216">
4.1 Rule Templates
</subsectionHeader>
<bodyText confidence="0.997127666666667">
The denotation graph contains a directed edge from
s to s&apos; if there is a rule ω that reduces s&apos; to s, with an
inverse ω−1 that expands s to s&apos;. Reduction rules can
drop optional material, extract simpler constituents,
or perform lexical substitutions.
Drop Pre-Nominal Modifiers: “red shirt” →
“shirt” In an NP of the form “X Y Z”, where
X and Y both modify the head Z, we only allow
X and Y to be dropped separately if “X Z” and
“Y Z” both occur elsewhere in the corpus. Since
“white building” and “stone building” occur else-
where in the corpus, we generate both “white build-
ing” and “stone building” from the NP “white stone
building”. But since “ice player” is not used,
we replace “ice hockey player” only with “hockey
player” (which does occur) and then “player”.
Drop Other Modifiers “run quickly” → “run”
We drop ADVP chunks and adverbs in VP chunks.
We also allow a prepositional phrase (a preposi-
tion followed by a possibly conjoined NP chunk)
to be dropped if the preposition is locational
(“in”, “on”, “above”, etc.), directional (“towards”,
“through”, “across”, etc.), or instrumental (“by”,
“for”, “with”). Similarly, we also allow the drop-
ping of all “wear NP” constructions. Since the dis-
tinction between particles and prepositions is often
difficult, we also use a predefined list of phrasal
verbs that commonly occur in our corpus to identify
constructions such as “climb up a mountain”, which
is transformed into “climb a mountain” or “walk
down a street”, which is transformed into “walk”.
Replace Nouns by Hypernyms: “red shirt” →
“red clothing” We iteratively use our hypernym
</bodyText>
<page confidence="0.888732">
70
</page>
<equation confidence="0.918046538461538">
GENERATEGRAPH():
Q, Captions, Rules +- 0
for all c E ImageCorpus do
Rules(c) +- GenerateRules(s.)
pushAll(Q, {c} x RootNodes(s., Rules(c)))
while-empty(Q) do
(c, s) +- pop(Q)
Captions(s) +- Captions(s) U {c}
if |Captions(s) |= 2 then
for all c� E Captions(s) do
pushAll(Q, {c�} x Children(s, Rules(c�)))
else if |Captions(s) |&gt; 2 then
pushAll(Q, {c} x Children(s, Rules(c)))
</equation>
<figureCaption confidence="0.993019">
Figure 2: Generating the graph
</figureCaption>
<bodyText confidence="0.997806964285714">
lexicon to make head nouns more generic. We only
allow head nouns to be replaced by their hypernyms
if any age based modifiers have already been re-
moved: “toddler” can be replaced with “child”, but
not “older toddler” with “older child”.
Handle Partitive NPs: cup of tea -+ “cup”, “tea”
In most partitive NP1-of-NP2 constructions (“cup of
tea”, “a team offootball players”) the correspond-
ing entity can be referred to by both the first or the
second NP. Exceptions include the phrase “body of
water”, and expressions such as “a kind/type/sort
of”, which we treat similar to determiners.
Handle VP1-to-VP2 Cases Depending on the first
verb, we replace VPs of the form X to Y with both X
and Y if X is a movement or posture (jump to catch,
etc.). Otherwise we distinguish between cases we
can only replace with X (wait to jump) and those we
can only replace with Y (seem to jump).
Extract Simpler Constituents Any noun phrase
or verb phrase can also be used as a node in the
graph and simplified further. We use the Malt de-
pendencies (and the person terms in the entity type
lexicon) to identify and extract subject-verb-object
chunks which correspond to simpler sentences that
we would otherwise not be able to obtain: from
“man laugh(s) while drink(ing)”, we extract “man
laugh” and “man drink”, and then further split those
into “man”, “laugh(s)”, and “drink”.
</bodyText>
<subsectionHeader confidence="0.99547">
4.2 Graph Generation
</subsectionHeader>
<bodyText confidence="0.999981242424242">
The naive approach to graph generation would be to
generate all possible strings for each caption. How-
ever, this would produce far more strings than can be
processed in a reasonable amount of time, and most
of these strings would have uninformative denota-
tions, consisting of only a single image. To make
graph generation tractable, we use a top-down al-
gorithm which generates the graph from the most
generic (root) nodes, and stops at nodes that have a
singleton denotation (Figure 2). We first identify the
set of rules that can apply to each original caption
(GenerateRules). These rules are then used to re-
duce each caption as much as possible. The resulting
(maximally generic) strings are added as root nodes
to the graph (RootNodes), and added to the queue
Q. Q keeps track of all currently possible node ex-
pansions. It contains items (c, s), which pair the ID
of an original caption and its image (c) with a string
(s) that corresponds to an existing node in the graph
and can be derived from c’s caption. When (c, s) is
processed, we check how many captions have gen-
erated s so far (Captions(s)). If s has more than a
single caption, we use each of the applicable rewrite
rules of c’s caption to create new strings s&apos; that cor-
respond to the children of s in the graph, and push
all resulting (c, s&apos;) onto Q. If c is the second caption
of s, we also use all of the applicable rewrite rules
from the first caption c&apos; to create its children.
A post-processing step (not shown in Figure 2)
attaches each original caption to all leaf nodes of the
graph to which it can be reduced. Finally, we obtain
the denotation of each node s from the set of images
whose captions are in Captions(s).
</bodyText>
<sectionHeader confidence="0.994697" genericHeader="method">
5 The Denotation Graph
</sectionHeader>
<bodyText confidence="0.9998474">
Size and Coverage On our corpus of 158,439
unique captions and 31,783 images, the denotation
graph contains 1,749,097 captions, out of which
230,811 describe more than a single image. Ta-
ble 1 provides the distribution of the size of deno-
tations. It is perhaps surprising that the 161 cap-
tions which describe each over 1,000 images do
not just consist of nouns such as person, but also
contain simple sentences such as woman standing,
adult work, person walk street, or person play in-
strument. Since the graph is derived from the origi-
nal captions by very simple syntactic operations, the
denotations it captures are most likely incomplete:
Jsoccer playerK contains 251 images, Jplay soccerK
contains 234 images, and Jsoccer gameK contains
</bodyText>
<page confidence="0.987701">
71
</page>
<table confidence="0.9857205">
Size of denotations |JsK |≥ 1 |JsK |≥ 2 |JsK |≥ 5 |JsK |≥ 10 |JsK |≥ 100 |JsK |≥ 1000
Nr. of captions 1,749,096 230,811 53,341 22,683 1,921 161
</table>
<tableCaption confidence="0.998871">
Table 1: Distribution of the size of denotations in our graph
</tableCaption>
<bodyText confidence="0.964886">
119 images. We have not yet attempted to iden-
tify variants in word order (“stick tongue out” vs.
“stick out tongue”) or equivalent choices of prepo-
sition (“look into mirror” vs. “look in mirror”). De-
spite this brittleness, the current graph already gives
us a large number of semantic associations.
</bodyText>
<table confidence="0.526588625">
play baseball
nPM7JK E
0.674 tag him 0.859 play softball
0.637 hold bat 0.782 play game
0.616 try to tag 0.768 play ball
0.569 slide into base 0.741 play catch
0.516 pitch ball 0.739 play cricket
play football
</table>
<bodyText confidence="0.9802632">
Denotational Similarities The following exam-
ples of the similarities found by nPMI JK and PJK
show that denotational similarities do not simply
find topically related events, but instead find events
that are related by entailment:
</bodyText>
<table confidence="0.98832875">
nPM7JK E
0.623 tackle person 0.826 play game
0.597 hold football 0.817 play rugby
0.545 run down field 0.811 play soccer
0.519 wear white jersey 0.796 play on field
0.487 avoid 0.773 play ball
PJK(x|y) x y
0.962 sit eat lunch
0.846 play guitar strum
0.811 surf catch wave
0.800 ride horse rope calf
0.700 listen sit in classroom
</table>
<bodyText confidence="0.9858651">
If someone is eating lunch, it is likely that they
are sitting, and people who sit in a classroom are
likely to be listening to somebody. These entail-
ments can be very precise: “walk up stair” entails
“ascend”, but not “descend”; the reverse is true for
“walk down stair”:
nPMI JK captures paraphrases as well as closely
related events: people look in a mirror when shav-
ing their face, and baseball players may try to tag
someone who is sliding into base:
</bodyText>
<table confidence="0.563736666666667">
nPM7JK x y
0.835 open present unwrap
0.826 lasso try to rope
0.791 get ready to kick run towards ball
0.785 try to tag slide into base
0.777 shave face look in mirror
</table>
<bodyText confidence="0.999630857142857">
Comparing the expressions that are most similar
to “play baseball” or “play football” according to
the denotational nPMI JK and the compositional Σ
similarities reveals that the denotational similarity
finds a number of actions that are part of the partic-
ular sport, while the compositional similarity finds
events that are similar to playing baseball (football):
</bodyText>
<sectionHeader confidence="0.987553" genericHeader="method">
6 Task 1: Approximate Entailment
</sectionHeader>
<bodyText confidence="0.999466387096774">
A caption never provides a complete description of
the depicted scene, but commonsense knowledge
often allows us to draw implicit inferences: when
somebody mentions a bride, it is quite likely that the
picture shows a woman in a wedding dress; a pic-
ture of a parent most likely also has a child or baby,
etc. In order to compare the utility of denotational
and distributional similarities for drawing these in-
ferences, we apply them to an approximate entail-
ment task, which is loosely modeled after the Rec-
ognizing Textual Entailment problem (Dagan et al.,
2006), and consists of deciding whether a brief cap-
tion h (the hypothesis) can describe the same image
as a set of captions P = {p1, ..., pNI known to de-
scribe the same image (the premises).
Data We generate positive and negative items
(P, h, f) (Figure 3) as follows: Given an image,
any subset of four of its captions form a set of
premises. A hypothesis is either a short verb phrase
or sentence that corresponds to a node in the deno-
tation graph. By focusing on short hypotheses, we
minimize the possibility that they contain extrane-
ous details that cannot be inferred from the premises.
Positive examples are generated by choosing a node
h as hypothesis and an image i E Qhs such that ex-
actly one caption of i generates h and the other four
captions of i are not descendants of h and hence
do not trivially entail h, giving an unfair advantage
to denotational approaches. Negative examples are
generated by choosing a node h as hypothesis and
selecting four of the captions of an image i E� Qhs.
</bodyText>
<figure confidence="0.923414176470588">
PJK(x|y)
x =ascend x =descend
y =walk up stair
y =walk down stair
32.0 0.0
0.0 30.8
72
Premises: A woman with dark hair in bending, open mouthed, towards the back of a dark headed toddler’s head.
A dark-haired woman has her mouth open and is hugging a little girl while sitting on a red blanket.
A grown lady is snuggling on the couch with a young girl and the lady has a frightened look.
A mom holding her child on a red sofa while they are both having fun.
VP Hypothesis: make face
Premises: A man editing a black and white photo at a computer with a pencil in his ear.
A man in a white shirt is working at a computer.
A guy in white t-shirt on a mac computer.
A young main is using an Apple computer.
S Hypothesis: man sit
</figure>
<figureCaption confidence="0.999539">
Figure 3: Positive examples from the Approximate Entailment tasks.
</figureCaption>
<bodyText confidence="0.998569712121212">
Since our items are created automatically, a posi-
tive hypothesis is not necessarily logically entailed
by its premises. We have performed a small-scale
human evaluation on 300 items (200 positive, 100
negative), each judged independently by the same
three judges (inter-annotator agreement: Fleiss-K =
0.74). Our results indicate that over half (55%) of
the positive hypotheses can be inferred from their
premises alone without looking at the original im-
age, while almost none of the negative hypotheses
(100% for sentences, 96% for verb phrases) can be
inferred from their premises. The training items are
generated from the captions of 25,000 images, and
the test items are generated from a disjoint set of
3,000 images. The VP data set consists of 290,000
training items and 16,000 test items, while the S data
set consists of 400,000 training items and 22,000 test
items. Half of the items in each set are positive, and
the other half are negative.
Models All of our models are binary MaxEnt clas-
sifiers, trained using MALLET (McCallum, 2002).
We have two baseline models: a plain bag-of-words
model (BOW) and a bag-of-words model where we
add all hypernyms in our lexicon to the captions be-
fore computing their overlap (BOW-H). This is in-
tended to minimize the advantage the denotational
features obtain from the hypernym lexicon used to
construct the denotation graph. In both cases, a
global BOW feature captures the fraction of tokens
in the hypothesis that are contained in the premises.
Word-specific BOW features capture the product of
the frequencies of each word in h and P. All other
models extend the BOW-H model.
Denotational Similarity Features We compute
denotational similarities nPMITi and PTi (Sec-
tion 2.4) over the pairs of nodes in a denotation
graph that is restricted to the training images. We
only consider pairs of nodes n, n&apos; if their denota-
tions contain at least 10 images and their intersection
contains at least 2 images.
To map an item (P, h) to denotational simi-
larity features, we represent the premises as the
set of all nodes P that are ancestors of its cap-
tions. A sentential hypothesis is represented as
the set of nodes H = {hS, hsbj, hVP, hv, hdobj}
that correspond to the sentence (h itself), its sub-
ject, its VP and its direct object. A VP hypothe-
sis has only the nodes H = {hVP, hv, hdobj}. In
both cases, hdobj may be empty. Both of the de-
notational similarities nPMI Ti(h, p) and PTi(h|p)
for h E H, p E P lead to two constituent-
specific features, sumx and maxx, (e.g. sumsbj =
Ep sim(hsbj, p), maxdobj = maxp sim(hdobj, p))
and two global features sump,h = Ep,h sim(h, p)
and maxp,h = maxp,h sim(h, p). Each constituent
type also has a set of node-specific sumx,, and
maxx,, features that are on when constituent x in
h is equal to the string s and whose value is equal
to the constituent-based feature. For PTi, each con-
stituent (and each constituent-node pair) has an ad-
ditional feature P(h|P) = 1 − Hn(1 − PTi(h|pn))
that estimates the probability that h is generated by
some node in the premise.
Lexical Similarity Features We use two sym-
metric lexical similarities: standard cosine distance
(cos), and Lin (1998)’s similarity (Lin):
</bodyText>
<equation confidence="0.9602292">
cos(w, w&apos;)
�i:w(i)&gt;0∧w0(i)&gt;0 w(i)+w0(i)
Lin(w, w&apos;) = � i w(i)+� i w0(i)
= w·w0
IIwIIIIw0II
</equation>
<page confidence="0.992206">
73
</page>
<bodyText confidence="0.98345155">
We use two directed lexical similarities: Clarke
(2009)’s similarity (Clk), and Szpektor and Dagan
(2008)’s balanced precision (Bal), which builds on
Lin and on Weeds and Weir (2003)’s similarity (W):
Ei w(i)
We also use two publicly available resources that
provide precomputed similarities, Kotlerman et al.
(2010)’s DIRECT noun and verb rules and Chklovski
and Pantel (2004)’s VERBOCEAN rules. Both are
motivated by the need for numerically quantifiable
semantic inferences between predicates. We only
use entries that correspond to single tokens (ignor-
ing e.g. phrasal verbs).
Each lexical similarity results in the follow-
ing features: words in the output are represented
by a max-sim,,, feature which captures its max-
imum similarity with any word in the premises
(max-sim,,, = max,,,&apos;EP sim(w, w&apos;)) and by a
sum-sim,,, feature which captures the sum of its sim-
ilarities to the words in the premises (sum-sim,,, =
</bodyText>
<equation confidence="0.762331">
E
</equation>
<bodyText confidence="0.983306454545454">
,,,&apos;EP sim(w, w&apos;)). Global max sim and sum sim
features capture the maximal (resp. total) similarity
of any word in the hypothesis to the premise.
We compute distributional and compositional
similarities (cos, Lin, Bal, Clk, Σ, Π) on our im-
age captions (“cap”), the BNC and Gigaword. For
each corpus C, we map each word w that appears
at least 10 times in C to a vector we of the non-
negative normalized pointwise mutual information
scores (Section 2.4) of w and the 1,000 words (ex-
cluding stop words) that occur in the most sentences
of C. We generally define P(w) (and P(w, w&apos;)) as
the fraction of sentences in C in which w (and w&apos;)
occur. To allow a direct comparison between dis-
tributional and denotational similarities, we first de-
fine P(w) (and P(w, w&apos;)) over individual captions
(“cap”), and then, to level the playing field, we rede-
fine P(w) (and P(w, w&apos;)) as the fraction of images
in whose captions w (and w&apos;) occur (“img”), and
then we use our lexicon to augment captions with
all hypernyms (“+hyp”). Finally, we include BNC
and Gigaword similarity features (“all”).
</bodyText>
<table confidence="0.99952835">
VP task S task
Baseline 1: BoW 58.7 71.2
Baseline 2: BoW-H 59.0 73.6
External 1: DIRECT 59.2 73.5
External 2: VerbOcean 60.8 74.0
Cap All Cap All
Distributional cos 67.5 71.9 76.1 78.9
Distributional Lin 62.6 70.2 75.4 77.8
Distributional Bal 62.3 69.6 74.7 75.3
Distributional Clk 62.4 69.2 75.4 77.5
Compositional H 68.4 70.3 75.3 77.3
Compositional E 67.8 71.4 76.9 79.2
Compositional H, E 69.8 72.7 77.0 79.6
Denotational nPMIJK 74.9 80.2
Denotational PJK 73.8 79.5
nPMIJK, PJK 75.5 81.2
Combined cos, H, E 71.1 72.6 77.4 79.2
nPMIJK, PJK, H, E 75.6 75.9 80.2 80.7
nPMIJK, PJK, cos 75.6 75.7 80.2 81.2
nPMIJK, PJK, cos, H, E 75.8 75.9 81.2 80.5
</table>
<tableCaption confidence="0.996592">
Table 2: Test accuracy on Approximate Entailment.
</tableCaption>
<bodyText confidence="0.979314166666667">
Compositional Similarity Features We use two
standard compositional baselines to combine the
word vectors of a sentence into a single vector: ad-
dition (sE = w1 + ... + w,,,, which can be inter-
preted as a disjunctive operation), and element-wise
(Hadamard) multiplication (srJ = w1 O ... (D w,,,,
which can be seen as a conjunctive operation). In
both cases, we represent the premises (which con-
sist of four captions) as a the sum of each caption’s
vector p = p1 + ...p4. This gives two composi-
tional similarity features: Σ = cos(pE, hE), and
Π = cos(pn, hn).
</bodyText>
<subsectionHeader confidence="0.998489">
6.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999685">
Table 2 provides the test accuracy of our mod-
els on the VP and S tasks. Adding hypernyms
(BOW-H) yields a slight improvement over the ba-
sic BOW model. Among the external resources,
VERBOCEAN is more beneficial than DIRECT, but
neither help as much as in-domain distributional
similarities (this may be due to sparsity).
Table 2 shows only the simplest (“Cap”) and
the most complex (“all”) distributional and com-
positional models, but Table 3 provides accuracies
of these models as we go from standard sentence-
based co-occurrence counts towards more denota-
tion graph-like co-occurrence counts that are based
on all captions describing the same image (“Img”),
</bodyText>
<equation confidence="0.999677333333333">
Clk(w  |w0) = Ei:w(i)&gt;0∧w&apos;(i)&gt;0 min(w(i),w0(i))
Ei w(i)
�
Bal(w  |w0) = W(w  |w0) × Lin(//w, w0)
Ei:w(i)&gt;0∧w&apos;(i)&gt;0 w(i)
W(w  |w0) = /
</equation>
<page confidence="0.996866">
74
</page>
<table confidence="0.99987825">
VP task S task
Cap Img +Hyp All Cap Img +Hyp All
cos 67.5 69.3 69.8 71.9 76.1 76.8 77.5 78.9
Lin 62.6 63.4 61.3 70.0 75.4 74.8 75.2 77.8
Bal 62.3 61.9 62.8 69.6 74.7 75.5 75.1 75.3
Clk 62.4 67.3 68.0 69.2 75.4 75.5 76.0 77.5
H 68.4 70.5 70.5 70.3 75.3 76.6 77.1 77.3
E 67.8 71.4 71.6 71.4 76.9 78.1 79.1 79.2
H, E 69.8 72.7 72.9 72.7 77.0 78.6 79.3 79.6
nPMIJK 74.9 80.2
PJK 73.8 79.5
nPMI JK, PJK 75.5 81.2
</table>
<tableCaption confidence="0.913710833333333">
Table 3: Accuracy on hypotheses as various additions are
made to the vector corpora. Cap is the image corpus with
caption co-occurrence. Img is the image corpus with im-
age co-occurrence. +Hyp augments the image corpus
with hypernyms and uses image co-occurrence. All adds
the BNC and Gigaword corpora to +Hyp.
</tableCaption>
<table confidence="0.999957142857143">
VP task S task
Words in h 1 2 3+ 2 3 4+
% of items 72.8 13.9 13.3 65.3 22.8 11.9
BoW-H 52.0 75.0 80.1 69.1 80.8 84.4
cos (All) 68.8 79.4 81.1 75.9 83.9 85.7
E (All) 68.1 80.8 79.5 76.5 83.9 85.1
nPMIJK 72.0 82.9 82.2 77.3 85.4 86.2
</table>
<tableCaption confidence="0.999868">
Table 4: Accuracy on hypotheses of varying length.
</tableCaption>
<bodyText confidence="0.999955954545454">
include hypernyms (“+Hyp”), and add informa-
tion from other corpora (“All”). The “+Hyp” col-
umn in Table 3 shows that the denotational metrics
clearly outperform any distributional metric when
both have access to the same information. Al-
though the distributional models benefit from the
BNC and Gigaword-based similarities (“All”), their
performance is still below that of the denotational
models. Among the distributional model, the simple
cos performs better than Lin, or the directed Clk and
Bal similarities. In all cases, giving models access to
different similarity features improves performance.
Table 4 shows the results by hypothesis length.
As the length of h increases, classifiers that use sim-
ilarities between pairs of words (BOW-H and cos)
continue to improve in performance relative to the
classifiers that use similarities between phrases and
sentences (E and nPMI®). Most likely, this is due
to the lexical similarities having a larger set of fea-
tures to work with for longer h. nPMI ® does espe-
cially well on shorter h, likely due to the shorter h
having larger denotations.
</bodyText>
<sectionHeader confidence="0.590096" genericHeader="method">
7 Task 2: Semantic Textual Similarity
</sectionHeader>
<bodyText confidence="0.99997715">
To assess how the denotational similarities perform
on a more established task and domain, we apply
them to the 1500 sentence pairs from the MSR Video
Description Corpus (Chen and Dolan, 2011) that
were annotated for the SemEval 2012 Semantic Tex-
tual Similarity (STS) task (Agirre et al., 2012). The
goal of this task is to assign scores between 0 and 5
to a pair of sentences, where 5 indicates equivalence,
and 0 unrelatedness. Since this is a symmetric task,
we do not consider directed similarities. And be-
cause the goal of this experiment is not to achieve
the best possible performance on this task, but to
compare the effectiveness of denotational and more
established similarities, we only compare the impact
of denotational similarities with compositional sim-
ilarities computed on our own corpus. Since the
MSR Video corpus associates each video with mul-
tiple sentences, it is in principle also amenable to a
denotational treatment, but the STS task description
explicitly forbids its use.
</bodyText>
<subsectionHeader confidence="0.996712">
7.1 Models
</subsectionHeader>
<bodyText confidence="0.999763416666667">
Baseline and Compositional Features Our start-
ing point is B¨ar et al. (2013)’s DKPro Similarity,
one of the top-performing models from the 2012
STS shared task, which is available and easily mod-
ified. It consists of a log-linear regression model
trained on multiple text features (word and charac-
ter n-grams, longest common substring and longest
common subsequence, Gabrilovich and Markovitch
(2007)’s Explicit Semantic Analysis, and Resnik
(1995)’s WordNet-based similarity). We investigate
the effects of adding compositional (computed on
the vectors obtained from the image-caption train-
ing data) and denotational similarity features to this
state-of-the-art system.
Denotational Features Since the STS task is
symmetric, we only consider nPMI ® similari-
ties. We again represent each sentence s by fea-
tures based on 5 types of constituents: S =
{sS, ssbj, sV P, sv, sdobj1. Since sentences might be
complex, they might contain multiple constituents
of the same type, and we therefore think of each
feature as a feature over sets of nodes. For each
constituent C we consider two sets of nodes in the
denotation graph: C itself (typically leaf nodes),
</bodyText>
<page confidence="0.996693">
75
</page>
<table confidence="0.959792">
DKPro +Σ, Π (img) +nPMI [] +both
Pearson r 0.868 0.880 0.888 0.890
</table>
<tableCaption confidence="0.825508">
Table 5: Performance on the STS MSRvid task: DKPro
(B¨ar et al., 2013) plus compositional (E, II) and/or deno-
tational similarities (nPMI Qj) from our corpus
</tableCaption>
<bodyText confidence="0.91991025">
and Canc, their parents and grandparents. For
each pair of sentences, C-C similarities compute
the similarity of the constituents of the same type,
while C-all similarities compute the similarity of
a C constituent in one sentence against all con-
stituents in the other sentence. For each pair of
constituents we consider three similarity features:
sim(C1, C2), max(sim(C1Canc
</bodyText>
<equation confidence="0.96394">
2 ), sim(Canc
1 , C2)),
sim(Canc
1 , Canc
</equation>
<bodyText confidence="0.9961664">
2 ). The similarity of two sets of
nodes is determined by the maximal similarity
of any pair of their elements: sim(C1, C2) =
maxc1∈C1,c2∈C2 nPMIQl(c1,c2). This gives us 15
C-C features and 15 C-all features.
</bodyText>
<subsectionHeader confidence="0.834842">
7.2 Experiments
</subsectionHeader>
<bodyText confidence="0.99998419047619">
We use the STS 2012 train/test data, normalized in
the same way as the image captions for the deno-
tation graph (i.e. we re-tokenize, lemmatize, and
remove determiners). Table 5 shows experimental
results for four models: DKPro is the off-the-shelf
DKProSimilarity model (B¨ar et al., 2013). From
our corpus, we either add additive and multiplicative
compositional features (E, II) from Section 6 (img),
the C-C and C-All denotational features based on
nPMI Ql, or both compositional and denotational
features. Systems are evaluated by the Pearson cor-
relation (r) of their predicted similarity scores to the
human-annotated ones. We see that the denotational
similarities outperform the compositional similari-
ties, and that including compositional similarity fea-
tures in addition to denotational similarity features
has little effect. For additional comparison, the
published numbers for the TakeLab Semantic Text
Similarity System (ˇSari´c et al., 2012), another top-
performing model from the 2012 shared task, are
r = 0.880 on this dataset.
</bodyText>
<sectionHeader confidence="0.998273" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999807765957447">
Summary of Contributions We have defined
novel denotational metrics of linguistic similarity
(Section 2), and have shown them to be at least
competitive with, if not superior to, distributional
similarities for two tasks that require simple se-
mantic inferences (Sections 6, 7), even though our
current method of computing them is somewhat
brittle (Section 5). We have also introduced two
new resources: a large data set of images paired
with descriptive captions, and a denotation graph
that pairs generalized versions of these captions
with their visual denotations, i.e. the sets of im-
ages they describe. Both of these resources are
freely available (http://nlp.cs.illinois.edu/
Denotation.html) Although the aim of this paper
is to show their utility for a purely linguistic task,
we believe that they should also be of great interest
for people who aim to build systems that automat-
ically associate image with sentences that describe
them (Farhadi et al., 2010; Kulkarni et al., 2011; Li
et al., 2011; Yang et al., 2011; Mitchell et al., 2012;
Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh
et al., 2013).
Related Work and Resources We believe that the
work reported in this paper has the potential to open
up promising new research directions. There are
other data sets that pair images or video with de-
scriptive language, but we have not yet applied our
approach to them. Chen and Dolan (2011)’s MSR
Video Description Corpus (of which the STS data
is a subset) is most similar to ours, but its curated
part is significantly smaller. Instead of several in-
dependent captions, Grubinger et al. (2006)’s IAPR
TC-12 data set contains longer descriptions. Or-
donez et al. (2011) harvested 1 million images and
their user-generated captions from Flickr to create
the SBU Captioned Photo Dataset. These captions
tend to be less descriptive of the image. The de-
notation graph is similar to Berant et al. (2012)’s
‘entailment graph’, but differs from it in two ways:
first, entailment relations in the denotation graph
are defined extensionally in terms of the images de-
scribed by the expressions at each node, and sec-
ond, nodes in Berant et al.’s entailment graph corre-
spond to generic propositional templates (X treats
Y), whereas nodes in our denotation graph corre-
spond to complete propositions (a dog runs).
</bodyText>
<page confidence="0.978921">
76
</page>
<sectionHeader confidence="0.996411" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996388111111111">
We gratefully acknowledge the support of the
National Science Foundation under NSF awards
0803603 “INT2-Medium: Understanding the mean-
ing of images”, 1053856 “CAREER: Bayesian Mod-
els for Lexicalized Grammars”, and 1205627 “CI-
P:Collaborative Research: Visual entailment data
set and challenge for the Language and Vision Com-
munity”, as well as via an NSF Graduate Research
Fellowship to Alice Lai.
</bodyText>
<sectionHeader confidence="0.997641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999387544444444">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 task 6: a pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, SemEval ’12, pages 385–393.
Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro Similarity: An Open Source Framework for
Text Similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics: System Demonstrations, pages 121–126, Sofia,
Bulgaria, August.
Jon Barwise and John Perry. 1980. Situations and atti-
tudes. Journal of Philosophy, 78:668–691.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2012. Learning entailment relations by global graph
structure optimization. Computational Linguistics,
38(1):73–111.
Greg Carlson, 2005. The Encyclopedia of Language and
Linguistics, chapter Generics, Habituals and Iteratives.
Elsevier, 2nd edition.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 190–200, Portland, Oregon, USA,
June.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 33–40, Barcelona, Spain, July.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1):22–29.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural Lan-
guage Semantics, pages 112–119, Athens, Greece,
March.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
challenge. In Machine Learning Challenges, volume
3944 of Lecture Notes in Computer Science, pages
177–190. Springer.
David Dowty, Robert Wall, and Stanley Peters. 1981. In-
troduction to Montague Semantics. Reidel, Dordrecht.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,
Peter Young, Cyrus Rashtchian, Julia Hockenmaier,
and David Forsyth. 2010. Every picture tells a
story: Generating sentences from images. In Proceed-
ings of the European Conference on Computer Vision
(ECCV), Part IV, pages 15–29, Heraklion, Greece,
September.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
international joint conference on Artifical intelligence,
IJCAI’07, pages 1606–1611.
Michael Grubinger, Paul Clough, Henning M¨uller, and
Thomas Deselaers. 2006. The IAPR benchmark: A
new evaluation resource for visual information sys-
tems. In OntoImage 2006, Workshop on Language
Resources for Content-based Image Retrieval during
LREC 2006, pages 13–23, Genoa, Italy, May.
Ankush Gupta, Yashaswi Verma, and C. Jawahar. 2012.
Choosing linguistics over vision to describe images.
In Proceedings of the Twenty-Sixth AAAI Conference
on Artificial Intelligence, Toronto, Ontario, Canada,
July.
Zellig S Harris. 1954. Distributional structure. Word,
10:146–162.
Micah Hodosh, Peter Young, Cyrus Rashtchian, and Julia
Hockenmaier. 2010. Cross-caption coreference reso-
lution for automatic image understanding. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 162–171, Uppsala,
Sweden, July.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Arti-
ficial Intelligence Research (JAIR), 47:853–899.
Shangfeng Hu and Chengfei Liu. 2011. Incorporating
coreference resolution into word sense disambigua-
tion. In Alexander F. Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing, volume
6608 of Lecture Notes in Computer Science, pages
265–276. Springer Berlin Heidelberg.
</reference>
<page confidence="0.979431">
77
</page>
<reference confidence="0.999899774193549">
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(4):359–389.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and generat-
ing simple image descriptions. In Proceedings of the
2011 IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 1601–1608.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gener-
ation of natural image descriptions. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), pages
359–368, Jeju Island, Korea, July.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
220–228, Portland, OR, USA, June.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning (ICML),
pages 296–304, Madison, WI, USA, July.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 521–528, Manchester, UK,
August.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009.
Evaluating the inferential utility of lexical-semantic
resources. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
558–566, Athens, Greece, March.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388–1429.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume III. 2012.
Midge: Generating image descriptions from computer
vision detections. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL), pages 747–756,
Avignon, France, April.
Richard Montague. 1974. Formal philosophy: papers
of Richard Montague. Yale University Press, New
Haven. Edited by Richmond H. Thomason.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In Proceedings of the International Confer-
ence on Language Resources and Evaluation (LREC),
pages 2216–2219.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural Infor-
mation Processing Systems 24, pages 1143–1151.
Judita Preiss. 2001. Anaphora resolution with word
sense disambiguation. In Proceedings of SENSEVAL-
2 Second International Workshop on Evaluating
Word Sense Disambiguation Systems, pages 143–146,
Toulouse, France, July.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxonomy. In Proceedings
of the 14th international joint conference on Artificial
intelligence - Volume 1, IJCAI’95, pages 448–453.
Idan Szpektor and Ido Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008), pages 849–856, Manchester, UK,
August. Coling 2008 Organizing Committee.
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441–448,
Montr´eal, Canada, 7-8 June.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 81–88.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 444–454, Edin-
burgh, UK, July.
</reference>
<page confidence="0.998827">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440219">
<title confidence="0.9892705">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
<author confidence="0.999932">Peter Young Alice Lai Micah Hodosh Julia</author>
<affiliation confidence="0.9976205">Department of Computer University of Illinois at Urbana-Champaign</affiliation>
<email confidence="0.471821">aylai2,mhodosh2,</email>
<abstract confidence="0.995539833333333">propose to use the denotations linguistic expressions (i.e. the set of images describe) to define novel which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we a i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 task 6: a pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>385--393</pages>
<contexts>
<context position="2788" citStr="Agirre et al., 2012" startWordPosition="427" endWordPosition="430">s purely syntactic and lexical rules to produce simpler captions (which have a larger denotation). But since each image is originally associated with several captions, the graph can also capture similarities between syntactically and lexically unrelated descriptions. We apply these similarities to two different tasks (Sections 6 and 7): an approximate entailment recognition task for our domain, where the goal is to decide whether the hypothesis (a brief image caption) refers to the same image as the premises (four longer captions), and the recently introduced Semantic Textual Similarity task (Agirre et al., 2012), which can be viewed as a graded (rather than binary) version of paraphrase detection. Both tasks require semantic inference, and our results indicate that denotational similarities are at least as effective as standard approaches to similarity. Our code and data set, as well as the denotation graph itself and the lexical similarities we define over it are available for research purposes at http://nlp.cs.illinois.edu/ Denotation.html. 2 Towards Denotational Similarities 2.1 Distributional Similarities The distributional hypothesis posits that linguistic expressions that appear in similar cont</context>
<context position="35464" citStr="Agirre et al., 2012" startWordPosition="6053" endWordPosition="6056">classifiers that use similarities between phrases and sentences (E and nPMI®). Most likely, this is due to the lexical similarities having a larger set of features to work with for longer h. nPMI ® does especially well on shorter h, likely due to the shorter h having larger denotations. 7 Task 2: Semantic Textual Similarity To assess how the denotational similarities perform on a more established task and domain, we apply them to the 1500 sentence pairs from the MSR Video Description Corpus (Chen and Dolan, 2011) that were annotated for the SemEval 2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012). The goal of this task is to assign scores between 0 and 5 to a pair of sentences, where 5 indicates equivalence, and 0 unrelatedness. Since this is a symmetric task, we do not consider directed similarities. And because the goal of this experiment is not to achieve the best possible performance on this task, but to compare the effectiveness of denotational and more established similarities, we only compare the impact of denotational similarities with compositional similarities computed on our own corpus. Since the MSR Video corpus associates each video with multiple sentences, it is in princ</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 task 6: a pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>DKPro Similarity: An Open Source Framework for Text Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>121--126</pages>
<location>Sofia, Bulgaria,</location>
<marker>B¨ar, Zesch, Gurevych, 2013</marker>
<rawString>Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013. DKPro Similarity: An Open Source Framework for Text Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121–126, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Barwise</author>
<author>John Perry</author>
</authors>
<title>Situations and attitudes.</title>
<date>1980</date>
<journal>Journal of Philosophy,</journal>
<pages>78--668</pages>
<contexts>
<context position="5660" citStr="Barwise and Perry, 1980" startWordPosition="896" endWordPosition="899">rity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the set of all situations or possible worlds in which the sentence is true (Montague, 1974; Dowty et al., 1981; Barwise and Perry, 1980). Restricting our attention to visually descriptive sentences, i.e. non-negative, episodic (Carlson, 2005) sentences that can be used to describe an image (Figure 1), we propose to instantiate the abstract notions of possible worlds or situations with concrete sets of images. The interpretation function J�K maps sentences to their visual denotations JsK, which is the set of images i E U$ C_ U in a ‘universe’ of images U that s describes: JsK = {i E U |s is a truthful description of i} (1) Similarly, we map nouns and noun phrases to the set of images that depict the objects they describe, and v</context>
</contexts>
<marker>Barwise, Perry, 1980</marker>
<rawString>Jon Barwise and John Perry. 1980. Situations and attitudes. Journal of Philosophy, 78:668–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Learning entailment relations by global graph structure optimization.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="41168" citStr="Berant et al. (2012)" startWordPosition="6968" endWordPosition="6971">mages or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et al. (2011) harvested 1 million images and their user-generated captions from Flickr to create the SBU Captioned Photo Dataset. These captions tend to be less descriptive of the image. The denotation graph is similar to Berant et al. (2012)’s ‘entailment graph’, but differs from it in two ways: first, entailment relations in the denotation graph are defined extensionally in terms of the images described by the expressions at each node, and second, nodes in Berant et al.’s entailment graph correspond to generic propositional templates (X treats Y), whereas nodes in our denotation graph correspond to complete propositions (a dog runs). 76 Acknowledgements We gratefully acknowledge the support of the National Science Foundation under NSF awards 0803603 “INT2-Medium: Understanding the meaning of images”, 1053856 “CAREER: Bayesian Mo</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2012</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure optimization. Computational Linguistics, 38(1):73–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Carlson</author>
</authors>
<title>The Encyclopedia of Language and Linguistics, chapter Generics, Habituals and Iteratives. Elsevier, 2nd edition.</title>
<date>2005</date>
<contexts>
<context position="5766" citStr="Carlson, 2005" startWordPosition="910" endWordPosition="911"> the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the set of all situations or possible worlds in which the sentence is true (Montague, 1974; Dowty et al., 1981; Barwise and Perry, 1980). Restricting our attention to visually descriptive sentences, i.e. non-negative, episodic (Carlson, 2005) sentences that can be used to describe an image (Figure 1), we propose to instantiate the abstract notions of possible worlds or situations with concrete sets of images. The interpretation function J�K maps sentences to their visual denotations JsK, which is the set of images i E U$ C_ U in a ‘universe’ of images U that s describes: JsK = {i E U |s is a truthful description of i} (1) Similarly, we map nouns and noun phrases to the set of images that depict the objects they describe, and verbs and verb phrases to the set of images that depict the events they describe. 2.3 Denotation Graphs Den</context>
</contexts>
<marker>Carlson, 2005</marker>
<rawString>Greg Carlson, 2005. The Encyclopedia of Language and Linguistics, chapter Generics, Habituals and Iteratives. Elsevier, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chen</author>
<author>William Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>190--200</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="35362" citStr="Chen and Dolan, 2011" startWordPosition="6036" endWordPosition="6039">similarities between pairs of words (BOW-H and cos) continue to improve in performance relative to the classifiers that use similarities between phrases and sentences (E and nPMI®). Most likely, this is due to the lexical similarities having a larger set of features to work with for longer h. nPMI ® does especially well on shorter h, likely due to the shorter h having larger denotations. 7 Task 2: Semantic Textual Similarity To assess how the denotational similarities perform on a more established task and domain, we apply them to the 1500 sentence pairs from the MSR Video Description Corpus (Chen and Dolan, 2011) that were annotated for the SemEval 2012 Semantic Textual Similarity (STS) task (Agirre et al., 2012). The goal of this task is to assign scores between 0 and 5 to a pair of sentences, where 5 indicates equivalence, and 0 unrelatedness. Since this is a symmetric task, we do not consider directed similarities. And because the goal of this experiment is not to achieve the best possible performance on this task, but to compare the effectiveness of denotational and more established similarities, we only compare the impact of denotational similarities with compositional similarities computed on ou</context>
<context position="40661" citStr="Chen and Dolan (2011)" startWordPosition="6884" endWordPosition="6887"> we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et al. (2011) harvested 1 million images and their user-generated captions from Flickr to create the SBU Captioned Photo Dataset. These captions tend to be less descriptive of the image. The denotation graph is similar to Berant et al. (2012)’s ‘entailment graph’, but differs from it in two ways: first, entailment relations in the de</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190–200, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>33--40</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="29346" citStr="Chklovski and Pantel (2004)" startWordPosition="4996" endWordPosition="4999">e in the premise. Lexical Similarity Features We use two symmetric lexical similarities: standard cosine distance (cos), and Lin (1998)’s similarity (Lin): cos(w, w&apos;) �i:w(i)&gt;0∧w0(i)&gt;0 w(i)+w0(i) Lin(w, w&apos;) = � i w(i)+� i w0(i) = w·w0 IIwIIIIw0II 73 We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): Ei w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s DIRECT noun and verb rules and Chklovski and Pantel (2004)’s VERBOCEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between predicates. We only use entries that correspond to single tokens (ignoring e.g. phrasal verbs). Each lexical similarity results in the following features: words in the output are represented by a max-sim,,, feature which captures its maximum similarity with any word in the premises (max-sim,,, = max,,,&apos;EP sim(w, w&apos;)) and by a sum-sim,,, feature which captures the sum of its similarities to the words in the premises (sum-sim,,, = E ,,,&apos;EP sim(w, w&apos;)). Global max sim and sum sim features c</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 33–40, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="8357" citStr="Church and Hanks, 1990" startWordPosition="1391" endWordPosition="1394">peration w E O that reduces the string s&apos; to s (i.e. w(s&apos;) = s) and its inverse w−1 expands the string s to s&apos; (i.e. w−1(s) = s&apos;). 2.4 Denotational Similarities Given a denotation graph over N images, we estimate the denotational probability of an expressions with a denotation of size |JsK |as PQ➢(s) = |JsK|/N, and the joint probability of two expressions analogously as PQ➢(s, s&apos;) = |JsK n Js&apos;K|/N. The conditional probability PQ➢(s |s&apos;) indicates how likely s is to be true when s&apos; holds, and yields a simple directed denotational similarity. The (normalized) pointwise mutual information (PMI) (Church and Hanks, 1990) defines a symmetric similarity: ( PJK(S,S�) ) log PJK(S)PJK(S�) − log(PQ➢(s, s&apos;)) We set PQ➢(s|s) = nPMI Q➢(s, s) = 1, and, if s or s&apos; are not in the denotation graph, nPMI Q➢(s, s&apos;) = PQ➢(s, s&apos;) = 0. 3 Our Data Set Our data set (Figure 1) consists of 31,783 photographs of everyday activities, events and scenes (all harvested from Flickr) and 158,915 captions (obtained via crowdsourcing). It contains and extends Hodosh et al. (2013)’s corpus of 8,092 images. We followed Hodosh et al. (2013)’s approach to collect images. We also use their annotation guidelines, and use similar quality controls</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: an overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>112--119</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="4930" citStr="Clarke, 2009" startWordPosition="779" endWordPosition="780">rom our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the se</context>
<context position="29024" citStr="Clarke (2009)" startWordPosition="4949" endWordPosition="4950"> features that are on when constituent x in h is equal to the string s and whose value is equal to the constituent-based feature. For PTi, each constituent (and each constituent-node pair) has an additional feature P(h|P) = 1 − Hn(1 − PTi(h|pn)) that estimates the probability that h is generated by some node in the premise. Lexical Similarity Features We use two symmetric lexical similarities: standard cosine distance (cos), and Lin (1998)’s similarity (Lin): cos(w, w&apos;) �i:w(i)&gt;0∧w0(i)&gt;0 w(i)+w0(i) Lin(w, w&apos;) = � i w(i)+� i w0(i) = w·w0 IIwIIIIw0II 73 We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): Ei w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s DIRECT noun and verb rules and Chklovski and Pantel (2004)’s VERBOCEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between predicates. We only use entries that correspond to single tokens (ignoring e.g. phrasal verbs). Each lexical similarity results in the following features: words in the ou</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 112–119, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="23901" citStr="Dagan et al., 2006" startWordPosition="4040" endWordPosition="4043">ng baseball (football): 6 Task 1: Approximate Entailment A caption never provides a complete description of the depicted scene, but commonsense knowledge often allows us to draw implicit inferences: when somebody mentions a bride, it is quite likely that the picture shows a woman in a wedding dress; a picture of a parent most likely also has a child or baby, etc. In order to compare the utility of denotational and distributional similarities for drawing these inferences, we apply them to an approximate entailment task, which is loosely modeled after the Recognizing Textual Entailment problem (Dagan et al., 2006), and consists of deciding whether a brief caption h (the hypothesis) can describe the same image as a set of captions P = {p1, ..., pNI known to describe the same image (the premises). Data We generate positive and negative items (P, h, f) (Figure 3) as follows: Given an image, any subset of four of its captions form a set of premises. A hypothesis is either a short verb phrase or sentence that corresponds to a node in the denotation graph. By focusing on short hypotheses, we minimize the possibility that they contain extraneous details that cannot be inferred from the premises. Positive exam</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment challenge. In Machine Learning Challenges, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
<author>Robert Wall</author>
<author>Stanley Peters</author>
</authors>
<title>Introduction to Montague Semantics.</title>
<date>1981</date>
<location>Reidel, Dordrecht.</location>
<contexts>
<context position="5634" citStr="Dowty et al., 1981" startWordPosition="892" endWordPosition="895">compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the set of all situations or possible worlds in which the sentence is true (Montague, 1974; Dowty et al., 1981; Barwise and Perry, 1980). Restricting our attention to visually descriptive sentences, i.e. non-negative, episodic (Carlson, 2005) sentences that can be used to describe an image (Figure 1), we propose to instantiate the abstract notions of possible worlds or situations with concrete sets of images. The interpretation function J�K maps sentences to their visual denotations JsK, which is the set of images i E U$ C_ U in a ‘universe’ of images U that s describes: JsK = {i E U |s is a truthful description of i} (1) Similarly, we map nouns and noun phrases to the set of images that depict the ob</context>
</contexts>
<marker>Dowty, Wall, Peters, 1981</marker>
<rawString>David Dowty, Robert Wall, and Stanley Peters. 1981. Introduction to Montague Semantics. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: Generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Computer Vision (ECCV), Part IV,</booktitle>
<pages>15--29</pages>
<location>Heraklion, Greece,</location>
<contexts>
<context position="40221" citStr="Farhadi et al., 2010" startWordPosition="6805" endWordPosition="6808">le (Section 5). We have also introduced two new resources: a large data set of images paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several i</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: Generating sentences from images. In Proceedings of the European Conference on Computer Vision (ECCV), Part IV, pages 15–29, Heraklion, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="36579" citStr="Gabrilovich and Markovitch (2007)" startWordPosition="6231" endWordPosition="6234">s computed on our own corpus. Since the MSR Video corpus associates each video with multiple sentences, it is in principle also amenable to a denotational treatment, but the STS task description explicitly forbids its use. 7.1 Models Baseline and Compositional Features Our starting point is B¨ar et al. (2013)’s DKPro Similarity, one of the top-performing models from the 2012 STS shared task, which is available and easily modified. It consists of a log-linear regression model trained on multiple text features (word and character n-grams, longest common substring and longest common subsequence, Gabrilovich and Markovitch (2007)’s Explicit Semantic Analysis, and Resnik (1995)’s WordNet-based similarity). We investigate the effects of adding compositional (computed on the vectors obtained from the image-caption training data) and denotational similarity features to this state-of-the-art system. Denotational Features Since the STS task is symmetric, we only consider nPMI ® similarities. We again represent each sentence s by features based on 5 types of constituents: S = {sS, ssbj, sV P, sv, sdobj1. Since sentences might be complex, they might contain multiple constituents of the same type, and we therefore think of eac</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI’07, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Grubinger</author>
<author>Paul Clough</author>
<author>Henning M¨uller</author>
<author>Thomas Deselaers</author>
</authors>
<title>The IAPR benchmark: A new evaluation resource for visual information systems.</title>
<date>2006</date>
<booktitle>In OntoImage 2006, Workshop on Language Resources for Content-based Image Retrieval during LREC</booktitle>
<pages>13--23</pages>
<location>Genoa, Italy,</location>
<marker>Grubinger, Clough, M¨uller, Deselaers, 2006</marker>
<rawString>Michael Grubinger, Paul Clough, Henning M¨uller, and Thomas Deselaers. 2006. The IAPR benchmark: A new evaluation resource for visual information systems. In OntoImage 2006, Workshop on Language Resources for Content-based Image Retrieval during LREC 2006, pages 13–23, Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ankush Gupta</author>
<author>Yashaswi Verma</author>
<author>C Jawahar</author>
</authors>
<title>Choosing linguistics over vision to describe images.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,</booktitle>
<location>Toronto, Ontario, Canada,</location>
<contexts>
<context position="40348" citStr="Gupta et al., 2012" startWordPosition="6829" endWordPosition="6832">otation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et al. (2011) harveste</context>
</contexts>
<marker>Gupta, Verma, Jawahar, 2012</marker>
<rawString>Ankush Gupta, Yashaswi Verma, and C. Jawahar. 2012. Choosing linguistics over vision to describe images. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, Toronto, Ontario, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--146</pages>
<contexts>
<context position="4389" citStr="Harris, 1954" startWordPosition="693" endWordPosition="694">low tie gives a frustrated look. A man in a yellow tie is rubbing the back of his neck. A man with a yellow tie looks concerned. A butcher cutting an animal to sell. A green-shirted man with a butcher’s apron uses a knife to carve out the hanging carcass of a cow. A man at work, butchering a cow. A man in a green t-shirt and long tan apron hacks apart the carcass of a cow while another man hoses away the blood. Two men work in a butcher shop; one cuts the meat from a butchered cow, while the other hoses the floor. Figure 1: Two images from our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. Word, 10:146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Cross-caption coreference resolution for automatic image understanding.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>162--171</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="13078" citStr="Hodosh et al. (2010)" startWordPosition="2181" endWordPosition="2184">onstruct a hypernym lexicon that allows us to replace head nouns with more generic terms. We only consider hypernyms that occur themselves with sufficient frequency in the original captions (replacing “adult” with “person”, but not with “organism”). Since the language in our corpus is very concrete, each noun tends to have a single sense, allowing us to always replace it with the same hypernyms.1 But since WordNet provides us with multiple senses for most nouns, we first have to identify which sense is used in our corpus. To do this, we use the heuristic cross-caption coreference algorithm of Hodosh et al. (2010) to identify coreferent NP chunks among the original five captions of each image.2 For each ambiguous head noun, we consider every non-singleton coreference chains it appears in, and reduce its synsets to those that stand in a hypernym-hyponym relation with at least one other head noun in the chain. Finally, we apply a greedy majority voting algorithm to iteratively narrow down each term’s senses to a single synset that is compatible with the largest number of coreference chains it occurs in. Caption Normalization In order to increase the recall of the denotations we capture, we drop all punct</context>
</contexts>
<marker>Hodosh, Young, Rashtchian, Hockenmaier, 2010</marker>
<rawString>Micah Hodosh, Peter Young, Cyrus Rashtchian, and Julia Hockenmaier. 2010. Cross-caption coreference resolution for automatic image understanding. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 162–171, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing image description as a ranking task: Data, models and evaluation metrics.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>47--853</pages>
<contexts>
<context position="8794" citStr="Hodosh et al. (2013)" startWordPosition="1470" endWordPosition="1473"> indicates how likely s is to be true when s&apos; holds, and yields a simple directed denotational similarity. The (normalized) pointwise mutual information (PMI) (Church and Hanks, 1990) defines a symmetric similarity: ( PJK(S,S�) ) log PJK(S)PJK(S�) − log(PQ➢(s, s&apos;)) We set PQ➢(s|s) = nPMI Q➢(s, s) = 1, and, if s or s&apos; are not in the denotation graph, nPMI Q➢(s, s&apos;) = PQ➢(s, s&apos;) = 0. 3 Our Data Set Our data set (Figure 1) consists of 31,783 photographs of everyday activities, events and scenes (all harvested from Flickr) and 158,915 captions (obtained via crowdsourcing). It contains and extends Hodosh et al. (2013)’s corpus of 8,092 images. We followed Hodosh et al. (2013)’s approach to collect images. We also use their annotation guidelines, and use similar quality controls to correct spelling mistakes, eliminate ungrammatical or non-descriptive sentences. Almost all of the images that we add to those collected by Hodosh et al. (2013) have been made available under a Creative Commons license. Each image is described independently by five annotators who are not familiar with the specific entities and circumstances depicted in them, resulting in captions such as “Three people setting up a tent”, rather t</context>
<context position="40370" citStr="Hodosh et al., 2013" startWordPosition="6833" endWordPosition="6836">airs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et al. (2011) harvested 1 million images and</context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research (JAIR), 47:853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shangfeng Hu</author>
<author>Chengfei Liu</author>
</authors>
<title>Incorporating coreference resolution into word sense disambiguation.</title>
<date>2011</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>6608</volume>
<pages>265--276</pages>
<editor>In Alexander F. Gelbukh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="14596" citStr="Hu and Liu (2011)" startWordPosition="2433" endWordPosition="2436">we only allow terms that are likely to describe adults or teenagers (including occupations) to be replaced by the term “person”. This means that the term “girl” has two senses: a female child (the default) or a younger woman. We distinguish the two senses in a preprocessing step: if the other captions of the same image do not mention children, but refer to teenaged or adult women, we assign girl the woman-sense. Some nouns that end in -er (e.g. “diner”, “pitcher” also violate our monosemy assumption. 2Coreference resolution has also been used for word sense disambiguation by Preiss (2001) and Hu and Liu (2011). ating the denotation graph. In order to distinguish between frequently occurring homonyms where the noun is unrelated to the verb, we change all forms of the verb dress to dressed, all forms of the verb stand to standing and all forms of the verb park to parking. Finally, we drop sentence-initial there/here/this is/are (as in there is a dog splashing in the water), and normalize the expressions in X and dressed (up) in X (where X is an article of clothing or a color) to wear X. We reduce plural determiners to {two, three, some}, and drop singular determiners except for no. 4.1 Rule Templates</context>
</contexts>
<marker>Hu, Liu, 2011</marker>
<rawString>Shangfeng Hu and Chengfei Liu. 2011. Incorporating coreference resolution into word sense disambiguation. In Alexander F. Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 6608 of Lecture Notes in Computer Science, pages 265–276. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="4955" citStr="Kotlerman et al., 2010" startWordPosition="781" endWordPosition="784">et and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the set of all situations or po</context>
<context position="29285" citStr="Kotlerman et al. (2010)" startWordPosition="4986" endWordPosition="4989">estimates the probability that h is generated by some node in the premise. Lexical Similarity Features We use two symmetric lexical similarities: standard cosine distance (cos), and Lin (1998)’s similarity (Lin): cos(w, w&apos;) �i:w(i)&gt;0∧w0(i)&gt;0 w(i)+w0(i) Lin(w, w&apos;) = � i w(i)+� i w0(i) = w·w0 IIwIIIIw0II 73 We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): Ei w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s DIRECT noun and verb rules and Chklovski and Pantel (2004)’s VERBOCEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between predicates. We only use entries that correspond to single tokens (ignoring e.g. phrasal verbs). Each lexical similarity results in the following features: words in the output are represented by a max-sim,,, feature which captures its maximum similarity with any word in the premises (max-sim,,, = max,,,&apos;EP sim(w, w&apos;)) and by a sum-sim,,, feature which captures the sum of its similarities to the words in the premises (sum-sim,,, </context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>1601--1608</pages>
<contexts>
<context position="40244" citStr="Kulkarni et al., 2011" startWordPosition="6809" endWordPosition="6812">e also introduced two new resources: a large data set of images paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Gr</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1601–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander Berg</author>
<author>Tamara Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>359--368</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="40328" citStr="Kuznetsova et al., 2012" startWordPosition="6825" endWordPosition="6828">ptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander Berg, Tamara Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 359–368, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>220--228</pages>
<location>Portland, OR, USA,</location>
<contexts>
<context position="40261" citStr="Li et al., 2011" startWordPosition="6813" endWordPosition="6816">ew resources: a large data set of images paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2</context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning (CoNLL), pages 220–228, Portland, OR, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning (ICML),</booktitle>
<pages>296--304</pages>
<location>Madison, WI, USA,</location>
<contexts>
<context position="4744" citStr="Lin, 1998" startWordPosition="752" endWordPosition="753">carcass of a cow while another man hoses away the blood. Two men work in a butcher shop; one cuts the meat from a butchered cow, while the other hoses the floor. Figure 1: Two images from our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the r</context>
<context position="28854" citStr="Lin (1998)" startWordPosition="4923" endWordPosition="4924">p sim(hdobj, p)) and two global features sump,h = Ep,h sim(h, p) and maxp,h = maxp,h sim(h, p). Each constituent type also has a set of node-specific sumx,, and maxx,, features that are on when constituent x in h is equal to the string s and whose value is equal to the constituent-based feature. For PTi, each constituent (and each constituent-node pair) has an additional feature P(h|P) = 1 − Hn(1 − PTi(h|pn)) that estimates the probability that h is generated by some node in the premise. Lexical Similarity Features We use two symmetric lexical similarities: standard cosine distance (cos), and Lin (1998)’s similarity (Lin): cos(w, w&apos;) �i:w(i)&gt;0∧w0(i)&gt;0 w(i)+w0(i) Lin(w, w&apos;) = � i w(i)+� i w0(i) = w·w0 IIwIIIIw0II 73 We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): Ei w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s DIRECT noun and verb rules and Chklovski and Pantel (2004)’s VERBOCEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning (ICML), pages 296–304, Madison, WI, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>521--528</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="7151" citStr="MacCartney and Manning (2008)" startWordPosition="1165" endWordPosition="1168">is a subset of the denotation of s&apos; (JsK C_ Js&apos;K), and we say that s&apos; subsumes the more specific s (s&apos; C s). In our domain of descriptive sentences, we can obtain more generic descriptions by simple syntactic and lexical operations ω E O C S x S that preserve upward entailment, so that if ω(s) = s&apos;, JsK C_ Js&apos;K. We consider three types of operations: the removal of optional material (e.g PPs like on the beach), the extraction of simpler constituents (NPs, VPs, or simple Ss), and lexical substitutions of nouns by their hypernyms (poodle -+ dog). These operations are akin to the atomic edits of MacCartney and Manning (2008)’s NatLog system, and allow us to construct large subsumption hierarchies over image descriptions, which we call denotation graphs. Given a set of (upward entailment-preserving) operations O C S x S, the denotation graph DG = (E, V ) of a set of images I and a set of strings S represents a subsumption hierarchy in which each node V = (s, JsK) corresponds to a string s E S and its denotation JsK C_ I. Directed edges e = (s, s&apos;) E E C_ V x V indicate a subsumption relations C s&apos; between a more generic expression s and its child s&apos;. An edge from s to s&apos; 68 exists if there is an operation w E O th</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 521–528, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://www.cs.umass.edu/ mccallum/mallet.</note>
<contexts>
<context position="26736" citStr="McCallum, 2002" startWordPosition="4549" endWordPosition="4550">king at the original image, while almost none of the negative hypotheses (100% for sentences, 96% for verb phrases) can be inferred from their premises. The training items are generated from the captions of 25,000 images, and the test items are generated from a disjoint set of 3,000 images. The VP data set consists of 290,000 training items and 16,000 test items, while the S data set consists of 400,000 training items and 22,000 test items. Half of the items in each set are positive, and the other half are negative. Models All of our models are binary MaxEnt classifiers, trained using MALLET (McCallum, 2002). We have two baseline models: a plain bag-of-words model (BOW) and a bag-of-words model where we add all hypernyms in our lexicon to the captions before computing their overlap (BOW-H). This is intended to minimize the advantage the denotational features obtain from the hypernym lexicon used to construct the denotation graph. In both cases, a global BOW feature captures the fraction of tokens in the hypothesis that are contained in the premises. Word-specific BOW features capture the product of the frequencies of each word in h and P. All other models extend the BOW-H model. Denotational Simi</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://www.cs.umass.edu/ mccallum/mallet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Eyal Shnarch</author>
</authors>
<title>Evaluating the inferential utility of lexical-semantic resources.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>558--566</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="1555" citStr="Mirkin et al., 2009" startWordPosition="235" endWordPosition="238">ssible for even brief descriptions of everyday scenes to evoke rich mental images. For example, we would expect an image of people shopping in a supermarket to depict aisles of produce or other goods, and we would expect most of these people to be customers who are either standing or walking around. But such inferences require a great deal of commonsense world knowledge. Standard distributional approaches to lexical similarity (Section 2.1) are very effective at identifying which words are related to the same topic, and can provide useful features for systems that perform semantic inferences (Mirkin et al., 2009), but are not suited to capture precise entailments between complex expressions. In this paper, we propose a novel approach for the automatic acquisition of denotational similarities between descriptions of everyday situations (Section 2). We define the (visual) denotation of a linguistic expression as the set of images it describes. We create a corpus of images of everyday activities (each paired with multiple captions; Section 3) to construct a large scale visual denotation graph which associates image descriptions with their denotations (Section 4). The algorithm that constructs the denotat</context>
</contexts>
<marker>Mirkin, Dagan, Shnarch, 2009</marker>
<rawString>Shachar Mirkin, Ido Dagan, and Eyal Shnarch. 2009. Evaluating the inferential utility of lexical-semantic resources. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 558–566, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="5086" citStr="Mitchell and Lapata, 2010" startWordPosition="800" endWordPosition="803">ities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the set of all situations or possible worlds in which the sentence is true (Montague, 1974; Dowty et al., 1981; Barwise and Perry, 1980). Restricting our attentio</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Kota Yamaguchi</author>
<author>Karl Stratos</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Alex Berg</author>
<author>Tamara Berg</author>
<author>Hal Daume</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>747--756</pages>
<location>Avignon, France,</location>
<contexts>
<context position="40303" citStr="Mitchell et al., 2012" startWordPosition="6821" endWordPosition="6824">ages paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer</context>
</contexts>
<marker>Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch, Berg, Berg, Daume, 2012</marker>
<rawString>Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Xufeng Han, Alyssa Mensch, Alex Berg, Tamara Berg, and Hal Daume III. 2012. Midge: Generating image descriptions from computer vision detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 747–756, Avignon, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Formal philosophy: papers of Richard Montague.</title>
<date>1974</date>
<publisher>Yale University Press,</publisher>
<location>New</location>
<note>Edited by</note>
<contexts>
<context position="5614" citStr="Montague, 1974" startWordPosition="890" endWordPosition="891">d to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assumed to be the set of all situations or possible worlds in which the sentence is true (Montague, 1974; Dowty et al., 1981; Barwise and Perry, 1980). Restricting our attention to visually descriptive sentences, i.e. non-negative, episodic (Carlson, 2005) sentences that can be used to describe an image (Figure 1), we propose to instantiate the abstract notions of possible worlds or situations with concrete sets of images. The interpretation function J�K maps sentences to their visual denotations JsK, which is the set of images i E U$ C_ U in a ‘universe’ of images U that s describes: JsK = {i E U |s is a truthful description of i} (1) Similarly, we map nouns and noun phrases to the set of image</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Richard Montague. 1974. Formal philosophy: papers of Richard Montague. Yale University Press, New Haven. Edited by Richmond H. Thomason.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="10297" citStr="Nivre et al., 2006" startWordPosition="1701" endWordPosition="1704">riety of descriptions associated with the same image is what allows us to induce denotational similarities between expressions that are not trivially related by syntactic rewrite rules. 4 Constructing the Denotation Graph The construction of the denotation graph consists of the following steps: preprocessing and linguistic analysis of the captions, identification of applicable transformations, and generation of the graph itself. Preprocessing and Linguistic Analysis We use the Linux spell checker, the OpenNLP tokenizer, POS tagger and chunker (http://opennlp. apache.org), and the Malt parser (Nivre et al., 2006) to analyze the captions. Since the vocabulary of our corpus differs significantly from the data these tools are trained on, we resort to a number of heuristics to improve the analyses they provide. Since some heuristics require us to identify different entity types, we developed a lexicon of the most common entity types in our domain (people, clothing, bodily appearance (e.g. hair or body parts), containers of liquids, food items and vehicles). After spell-checking, we normalize certain words and compounds with several spelling variations, e.g. barbecue (barbeque, BBQ), gray (grey), waterski </context>
<context position="11915" citStr="Nivre et al., 2006" startWordPosition="1969" endWordPosition="1972">ded by vegetable but a verb when preceded by a noun that refers to people). These fixes apply to 27,784 (17% of the 158,915 image captions). Next, we use the OpenNLP chunker to create a shallow parse. Fixing its (systematic) errors affects 28,587 captions. We then analyze the structure of each NP chunk to identify heads, determiners and prenominal modifiers. The head may include more than a single token if WordNet (or our hypernym lexicon, described below) contains a corresponding entry (e.g. little girl). Determiners include phrases such as a couple or a few. Although we use the Malt parser (Nivre et al., 2006) to identify subjectverb-object dependencies, we have found it more accurate to develop deterministic heuristics and lexinPMI Q➢(s, s&apos;) = 69 cal rules to identify the boundaries of complex (e.g. conjoined) NPs, allowing us to treat “a man with red shoes and a white hat” as an NP followed by a single PP, but “a man with red shoes and a white-haired woman” as two NPs, and to transform e.g. “standing by a man and a woman” into “standing” and not “standing and a woman” when dropping the PP. Hypernym Lexicon We use our corpus and WordNet to construct a hypernym lexicon that allows us to replace hea</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems 24,</booktitle>
<pages>1143--1151</pages>
<contexts>
<context position="40939" citStr="Ordonez et al. (2011)" startWordPosition="6929" endWordPosition="6933"> al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 data set contains longer descriptions. Ordonez et al. (2011) harvested 1 million images and their user-generated captions from Flickr to create the SBU Captioned Photo Dataset. These captions tend to be less descriptive of the image. The denotation graph is similar to Berant et al. (2012)’s ‘entailment graph’, but differs from it in two ways: first, entailment relations in the denotation graph are defined extensionally in terms of the images described by the expressions at each node, and second, nodes in Berant et al.’s entailment graph correspond to generic propositional templates (X treats Y), whereas nodes in our denotation graph correspond to compl</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Advances in Neural Information Processing Systems 24, pages 1143–1151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
</authors>
<title>Anaphora resolution with word sense disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>143--146</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="14574" citStr="Preiss (2001)" startWordPosition="2430" endWordPosition="2431">ies as “persons”, we only allow terms that are likely to describe adults or teenagers (including occupations) to be replaced by the term “person”. This means that the term “girl” has two senses: a female child (the default) or a younger woman. We distinguish the two senses in a preprocessing step: if the other captions of the same image do not mention children, but refer to teenaged or adult women, we assign girl the woman-sense. Some nouns that end in -er (e.g. “diner”, “pitcher” also violate our monosemy assumption. 2Coreference resolution has also been used for word sense disambiguation by Preiss (2001) and Hu and Liu (2011). ating the denotation graph. In order to distinguish between frequently occurring homonyms where the noun is unrelated to the verb, we change all forms of the verb dress to dressed, all forms of the verb stand to standing and all forms of the verb park to parking. Finally, we drop sentence-initial there/here/this is/are (as in there is a dog splashing in the water), and normalize the expressions in X and dressed (up) in X (where X is an article of clothing or a color) to wear X. We reduce plural determiners to {two, three, some}, and drop singular determiners except for </context>
</contexts>
<marker>Preiss, 2001</marker>
<rawString>Judita Preiss. 2001. Anaphora resolution with word sense disambiguation. In Proceedings of SENSEVAL2 Second International Workshop on Evaluating Word Sense Disambiguation Systems, pages 143–146, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th international joint conference on Artificial intelligence -</booktitle>
<volume>1</volume>
<pages>448--453</pages>
<contexts>
<context position="36627" citStr="Resnik (1995)" startWordPosition="6239" endWordPosition="6240">each video with multiple sentences, it is in principle also amenable to a denotational treatment, but the STS task description explicitly forbids its use. 7.1 Models Baseline and Compositional Features Our starting point is B¨ar et al. (2013)’s DKPro Similarity, one of the top-performing models from the 2012 STS shared task, which is available and easily modified. It consists of a log-linear regression model trained on multiple text features (word and character n-grams, longest common substring and longest common subsequence, Gabrilovich and Markovitch (2007)’s Explicit Semantic Analysis, and Resnik (1995)’s WordNet-based similarity). We investigate the effects of adding compositional (computed on the vectors obtained from the image-caption training data) and denotational similarity features to this state-of-the-art system. Denotational Features Since the STS task is symmetric, we only consider nPMI ® similarities. We again represent each sentence s by features based on 5 types of constituents: S = {sS, ssbj, sV P, sv, sdobj1. Since sentences might be complex, they might contain multiple constituents of the same type, and we therefore think of each feature as a feature over sets of nodes. For e</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI’95, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>849--856</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="4916" citStr="Szpektor and Dagan, 2008" startWordPosition="775" endWordPosition="778">or. Figure 1: Two images from our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a declarative sentence is assume</context>
<context position="29074" citStr="Szpektor and Dagan (2008)" startWordPosition="4954" endWordPosition="4957"> x in h is equal to the string s and whose value is equal to the constituent-based feature. For PTi, each constituent (and each constituent-node pair) has an additional feature P(h|P) = 1 − Hn(1 − PTi(h|pn)) that estimates the probability that h is generated by some node in the premise. Lexical Similarity Features We use two symmetric lexical similarities: standard cosine distance (cos), and Lin (1998)’s similarity (Lin): cos(w, w&apos;) �i:w(i)&gt;0∧w0(i)&gt;0 w(i)+w0(i) Lin(w, w&apos;) = � i w(i)+� i w0(i) = w·w0 IIwIIIIw0II 73 We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): Ei w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s DIRECT noun and verb rules and Chklovski and Pantel (2004)’s VERBOCEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between predicates. We only use entries that correspond to single tokens (ignoring e.g. phrasal verbs). Each lexical similarity results in the following features: words in the output are represented by a max-sim,,, feature which</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 849–856, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>441--448</pages>
<location>Montr´eal,</location>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 441–448, Montr´eal, Canada, 7-8 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>81--88</pages>
<contexts>
<context position="4890" citStr="Weeds and Weir, 2003" startWordPosition="771" endWordPosition="774">he other hoses the floor. Figure 1: Two images from our data set and their five captions similar meaning (Harris, 1954). This has led to the definition of vector-based distributional similarities, which represent each word w as a vector w derived from counts of w’s co-occurrence with other words. These vectors can be used directly to compute the lexical similarities of words, either via the cosine of the angle between them, or via other, more complex metrics (Lin, 1998). More recently, asymmetric similarities have been proposed as more suitable for semantic inference tasks such as entailment (Weeds and Weir, 2003; Szpektor and Dagan, 2008; Clarke, 2009; Kotlerman et al., 2010). Distributional word vectors can also be used to define the compositional similarity of longer strings (Mitchell and Lapata, 2010). To compute the similarity of two strings, the lexical vectors of the words in each string are first combined into a single vector (e.g. by element-wise addition or multiplication), and then an appropriate vector similarity (e.g. cosine) is applied to the resulting pair of vectors. 2.2 Visual Denotations Our approach is inspired by truth-conditional semantic theories in which the denotation of a decl</context>
<context position="29151" citStr="Weeds and Weir (2003)" startWordPosition="4967" endWordPosition="4970">d feature. For PTi, each constituent (and each constituent-node pair) has an additional feature P(h|P) = 1 − Hn(1 − PTi(h|pn)) that estimates the probability that h is generated by some node in the premise. Lexical Similarity Features We use two symmetric lexical similarities: standard cosine distance (cos), and Lin (1998)’s similarity (Lin): cos(w, w&apos;) �i:w(i)&gt;0∧w0(i)&gt;0 w(i)+w0(i) Lin(w, w&apos;) = � i w(i)+� i w0(i) = w·w0 IIwIIIIw0II 73 We use two directed lexical similarities: Clarke (2009)’s similarity (Clk), and Szpektor and Dagan (2008)’s balanced precision (Bal), which builds on Lin and on Weeds and Weir (2003)’s similarity (W): Ei w(i) We also use two publicly available resources that provide precomputed similarities, Kotlerman et al. (2010)’s DIRECT noun and verb rules and Chklovski and Pantel (2004)’s VERBOCEAN rules. Both are motivated by the need for numerically quantifiable semantic inferences between predicates. We only use entries that correspond to single tokens (ignoring e.g. phrasal verbs). Each lexical similarity results in the following features: words in the output are represented by a max-sim,,, feature which captures its maximum similarity with any word in the premises (max-sim,,, = </context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
<author>Ching Teo</author>
<author>Hal Daume</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Corpus-guided sentence generation of natural images.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>444--454</pages>
<location>Edinburgh, UK,</location>
<contexts>
<context position="40280" citStr="Yang et al., 2011" startWordPosition="6817" endWordPosition="6820">arge data set of images paired with descriptive captions, and a denotation graph that pairs generalized versions of these captions with their visual denotations, i.e. the sets of images they describe. Both of these resources are freely available (http://nlp.cs.illinois.edu/ Denotation.html) Although the aim of this paper is to show their utility for a purely linguistic task, we believe that they should also be of great interest for people who aim to build systems that automatically associate image with sentences that describe them (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2012; Gupta et al., 2012; Hodosh et al., 2013). Related Work and Resources We believe that the work reported in this paper has the potential to open up promising new research directions. There are other data sets that pair images or video with descriptive language, but we have not yet applied our approach to them. Chen and Dolan (2011)’s MSR Video Description Corpus (of which the STS data is a subset) is most similar to ours, but its curated part is significantly smaller. Instead of several independent captions, Grubinger et al. (2006)’s IAPR TC-12 d</context>
</contexts>
<marker>Yang, Teo, Daume, Aloimonos, 2011</marker>
<rawString>Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 444–454, Edinburgh, UK, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
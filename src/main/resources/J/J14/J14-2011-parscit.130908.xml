<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025486">
<title confidence="0.893360666666667">
Book Reviews
Semi-Supervised Learning and Domain Adaptation in
Natural Language Processing
</title>
<author confidence="0.765624">
Anders Søgaard
</author>
<affiliation confidence="0.641013">
University of Copenhagen
</affiliation>
<figure confidence="0.5494884">
Morgan &amp; Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 21), 2013, x+93 pp; paperbound, ISBN 978-1-60845-985-8, $40.00;
e-book, ISBN 978-1-60845-986-5, $30.00 or by subscription
Reviewed by
George Foster
</figure>
<subsectionHeader confidence="0.621814">
National Research Council Canada
</subsectionHeader>
<bodyText confidence="0.999743333333333">
Classical machine learning makes at least two assumptions that are at odds with its
application to natural language. First, it implicitly assumes there are enough data. This
is rarely the case in NLP, where sparse data is the norm, especially for intermediate
tasks like parsing that require artificial labeling. Second, it assumes that all examples
are drawn from the same distribution. Language is of course not like this: rather than
being an ordered landscape, it is a wildly varying one, rich with strange growths and
prone to sudden monstrous blooms like micro-blogging.
Both these traits can cause a classically trained NLP system to behave poorly. To
cope with data sparsity, a common strategy is semi-supervised learning, in which a
small labeled data set is augmented by a larger amount of (typically more abundant)
unlabeled data. To cope with domain differences between training and test data, adap-
tation techniques can be used to mitigate training data bias by exploiting whatever is
known of the test domain. The link between these two topics is that what is known of the
test domain often comes in the form of an unlabeled sample, and hence semi-supervised
techniques constitute an important class of adaptation strategies.
Søgaard’s book has at its core the intersection of these two important topics,
although it also covers semi-supervised techniques without considering data bias, and
techniques for handling bias that are not semi-supervised.
Before proceeding, I should declare a bias of another sort, which is that of a machine
translation (MT) researcher toward a book that cites only two MT papers related to semi-
supervised learning or domain adaptation (Habash 2008; Daum´e III and Jagarlamudi
2011), neither of which is highly representative of the fairly substantial MT work on
these topics. I have done my best to apply my neural adaptation faculty to the material
in this book; non-MT readers might find it helpful to do the same with this review.
The book begins with two chapters of introductory material on machine learning
and NLP that occupy half of its 80 pages (exclusive of bibliography). The main topics
of semi-supervised learning and adaptation are then presented in approximately equal-
sized portions, with adaptation split into two chapters covering techniques for known
and unknown test domains. A short final chapter deals with evaluation in the presence
of domain shift.
</bodyText>
<footnote confidence="0.656601666666667">
doi:10.1162/COLI r 00188
© 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
</footnote>
<bodyText confidence="0.99996428">
The introductory chapter plunges somewhat abruptly into a sketch of NLP as a
machine learning problem, then emerges to motivate the core material of the book.
Faced with a domain mismatch between training and test data, we are given the option
of performing semi-supervised learning with an unlabeled test-domain sample, using
either standard algorithms or specialized ones that address domain shift; or, if nothing
is known about the test domain, using a learning approach that encourages robustness.
Given the focus of the book, this catalog is entirely appropriate, but the scenarios
it contemplates are special cases of a more general one in which both labeled and
unlabeled training data are available for a collection of domains that may or may not
include the test domain. (Incidentally, the 20-Newsgroups data used in the book for text-
classification examples falls more naturally into this heterogeneous case than into the
binary train/test split to which it is cast.) This section would have benefitted from an at-
tempt to situate the approaches considered here within the more general setting, which
is barely acknowledged throughout the book. Pointers to representative relevant work,
both in NLP (Daum´e III 2007; Finkel and Manning 2009) and machine learning (Ben-
David et al. 2010; Dredze, Kulesza, and Crammer 2010), would also have been an asset.
Chapter 2 is a lengthy chapter that lays out basic techniques in a bid to make
the book self-contained. This is always a tricky proposition, as it must tread a fine
line between boring the expert and baffling the beginner. The author strikes a good
balance here by emphasizing practical advice over theoretical completeness, and pro-
viding experimental results to underscore various points. Although a few passages
would probably cause beginners to stumble, for instance one that invokes the concept
of a generative story when explaining hidden Markov models (HMMs), with only a
reference to Brown et al. (1993) for explanation, most of the material is very accessible,
and there are many links into the literature. The expert reader will also not be unduly
bored, and will find this chapter a quick and probably productive read. Although it
clearly does not present all basic techniques in NLP, the chapter manages to cover a lot
of ground while remaining coherent and for the most part not superfluous with respect
to the remainder of the book.
To summarize the contents of this chapter briefly, after a discussion of some relevant
assumptions about data (smoothness, i.i.d., coherence), and related empirical tests, three
basic classification techniques—nearest neighbor, naive Bayes, and perceptron—are
described and compared experimentally on a 20-Newsgroups text-classification task.
Next, weighted versions of these algorithms are given (omitting how the weights get
set). Then a section on unsupervised learning presents hierarchical and k-means clus-
tering, along with a somewhat elliptical account of generalized EM. Finally, structured
learning is introduced through HMM-based POS tagging—with a nod to conditional
random fields and structured perceptrons—as well as transition- and graph-based de-
pendency parsing.
Chapter 3 describes a variety of semi-supervised learning algorithms. A section
on wrapper methods traces two branches of work that refine self-training, in which a
learner labels some of the unlabeled material and then trains on its own output. One
branch is based on co-training, where two learners trained on complementary features
label data for each other. This generalizes to three learners in tri-training, and ultimately
to an arbitrary number in multi-view approaches (Ganchev et al. 2008), although these
latter are not mentioned in the book. The other branch requires a learner that can assign
probabilities to outcomes, and exploits this to iterate soft labeling and training on
weighted examples in an expectation maximization (EM)-like (or just plain EM) pro-
cedure. (The section that describes this method includes “CO-EM” in the title, but the
text never seems to make it to that tantalizing destination.) Another briefly mentioned
</bodyText>
<page confidence="0.971491">
520
</page>
<subsectionHeader confidence="0.970136">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99997076">
wrapper-type technique is to exploit a clustering of the unlabeled data in order to
generate additional features for supervised learning; this is related to recent work that
uses neural nets to learn embeddings from unlabeled data (Collobert et al. 2011).
A final group of semi-supervised algorithms is specific to nearest-neighbor
classification. Label propagation (Zhu and Ghahramani 2002) is a well-known iterative
graph-based algorithm where neighbors vote on each node’s label, with votes weighted
by distance. A similar voting takes place in editing and condensation, which are
methods for identifying a subset of prototypical data points (similar to support vectors
in support vector machines) in order to speed up nearest-neighbor search. Unlabeled
data can be used to improve this process by essentially providing greater resolution.
Chapter 4 is the first to grapple explicitly with domain mismatch, and begins
by making a standard distinction between conditional and marginal distributions for
inputs and outputs (Jiang and Zhai 2007). Here we are clearly limited to considering
biased inputs, because there is only an unlabeled sample from the test domain. One
strategy for exploiting this is to apply the semi-supervised methods from the previous
chapter, which will work as-is if the mismatch between training and test domains is not
too great. Otherwise, we can downweight instances in the labeled training set whose
inputs are not close to those in the test-domain data; techniques for measuring distance
include LMs and KL divergence. A similar approach can be applied to features, by
comparing the training-data values of a feature across all examples (ignoring labels)
to its test-domain values. A more sophisticated extension of this idea is structured
correspondence learning (Blitzer, McDonald, and Pereira 2006), which automatically
places test-domain features in correspondence with training-domain features.
Chapter 5 tackles the ambitious goal of learning “defensively” so as to be robust to
domain shifts in the absence of any prior information about the test domain. A common
effect is the out-of-vocabulary (OOV) problem, in which features do not appear in a
new test domain. If these have high weights, test-domain performance can degrade
badly, to the point where it would have been better to have left them out of training in
the first place, allowing other features to take up the slack. (Note that for some versions
of the OOV problem—such as in MT—things aren’t this easy.) An interesting technique
for countering feature OOVs, recently introduced to NLP by the author (Søgaard
and Johannsen 2012), is adversarial learning, in which random subsets of features
are removed during training. The chapter ends with a discussion of more traditional
ensemble-based methods (voting, product of experts, stacking, meta-learning) that
combine predictions from a set of diverse base classifiers in order to decrease variance.
The final chapter gives a short treatment of the problem of how to predict the perfor-
mance of systems on new domains. The main, and most interesting, suggestion is meta-
analysis, a technique widely used in fields such as medicine and psychology. The idea is
to extrapolate from many experimental trials (typically in previously published work)
that use the same method but different data, while calibrating for differences among the
trials and making use of their internal estimates of variance. I am not sure this is a perfect
fit with NLP for many reasons, such as that many papers do not include estimates of
variance, and the tendency of NLP systems to be sensitive to undocumented “minor”
configuration changes; but it is definitely a direction that bears further investigation.
To summarize, this is a monograph that focuses on the problem of optimizing
an NLP system trained on a single domain for a test domain that is either unknown
or represented by an unlabeled sample. When the unlabeled sample comes from the
training domain, the task is plain semi-supervised learning, a case that is dealt with
extensively. The book is written in an informal style, and sticks to basic machine learn-
ing concepts, which are illustrated with many examples involving text classification,
</bodyText>
<page confidence="0.990416">
521
</page>
<note confidence="0.54806">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.997462583333333">
POS tagging, and dependency parsing. In my opinion, the most interesting chapters are
the shorter ones toward the end. I recommend these to any NLP researcher interested
in the perennial problem of making do with not enough of the right kind of data.
The printed version of the book has some imperfections that are mostly due to
production quality. Many equations and graphs are fuzzy, and some graphs (e.g.,
Fig 3.3) are completely indecipherable due to their having been intended for color
printing. An index would have been a boon, though its absence is partly compensated
for by an extensive back-referenced bibliography. Finally, the text contains many large
blocks of Python code, which are sometimes used in lieu of pseudo-code for describing
algorithms. Although having real code is a bonus, it is harder to read than pseudo-code,
and would have been much more useful to readers had it been moved out of the book
and made available separately on-line.
</bodyText>
<sectionHeader confidence="0.995307" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.998597961038961">
Ben-David, Shai, John Blitzer, Koby
Crammer, Alex Kulesza, Fernando Pereira,
and Jennifer Wortman Vaughan. 2010.
A theory of learning from different
domains. Machine Learning, 79:151–175.
Blitzer, John, Ryan McDonald, and Fernando
Pereira. 2006. Domain adaptation with
structural correspondence learning.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 120–128, Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent Della J. Pietra, and Robert L.
Mercer. 1993. The mathematics of machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263–312.
Collobert, Ronan, Jason Weston, L´eon Bottou,
Michael Karlen, Koray Kavukcuoglu, and
Pavel Kuksa. 2011. Natural language
processing (almost) from scratch.
Journal of Machine Learning Research,
12:2493–2537.
Daum´e III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 256–263,
Prague.
Daum´e III, Hal and Jagadeesh Jagarlamudi.
2011. Domain adaptation for machine
translation by mining unseen words.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 407–412, Portland, OR.
Dredze, Mark, Alex Kulesza, and Koby
Crammer. 2010. Multi-domain learning
This book review was edited by Pierre Isabelle.
by confidence-weighted parameter
combination. Machine Learning,
79:123–149.
Finkel, Jenny Rose and Christopher D.
Manning. 2009. Hierarchical Bayesian
domain adaptation. In Proceedings of
Human Language Technologies: The 2009
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, pages 602–610, Boulder, CO.
Ganchev, K., J. Grac¸a, J. Blitzer, and B. Taskar.
2008. Multi-view learning over structured
and non-identical outputs. In Proceedings
of the 24th Conference on Uncertainty in
Artificial Intelligence, pages 204–211,
Helsinki.
Habash, Nizar. 2008. Four techniques for
online handling of out-of-vocabulary
words in Arabic-English statistical
machine translation. In Proceedings of
ACL-08: HLT, Short Papers, pages 57–60,
Columbus, OH.
Jiang, Jing and ChengXiang Zhai. 2007.
Instance weighting for domain adaptation
in NLP. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 264–271, Prague.
Søgaard, Anders and Anders Johannsen.
2012. Robust learning in random
subspaces: Equipping NLP for OOV
effects. In Proceedings of COLING 2012:
Posters, pages 1,171–1,180, Mumbai.
Zhu, Xiaojin and Zoubin Ghahramani. 2002.
Learning from labeled and unlabeled data
with label propagation. Technical Report
CMU-CALD-02-107, Carnegie Mellon
University, Pittsburgh, PA.
George Foster is a Senior Researcher in the Multilingual Text Processing Group at the National
Research Council of Canada. His recent research has focused on adaptation and parameter
estimation for machine translation. Foster’s e-mail address is george.foster@nrc-cnrc.gc.ca.
</reference>
<page confidence="0.997416">
522
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000082">
<title confidence="0.99354">Book Reviews Semi-Supervised Learning and Domain Adaptation in Natural Language Processing</title>
<author confidence="0.981889">Anders Søgaard</author>
<affiliation confidence="0.947396">University of Copenhagen</affiliation>
<author confidence="0.634379">Morgan</author>
<author confidence="0.634379">Claypool</author>
<note confidence="0.831912333333333">Graeme Hirst, volume 21), 2013, x+93 pp; paperbound, ISBN 978-1-60845-985-8, $40.00; e-book, ISBN 978-1-60845-986-5, $30.00 or by subscription Reviewed by</note>
<author confidence="0.922703">George Foster</author>
<affiliation confidence="0.93079">National Research Council Canada</affiliation>
<abstract confidence="0.992749825503356">Classical machine learning makes at least two assumptions that are at odds with its application to natural language. First, it implicitly assumes there are enough data. This is rarely the case in NLP, where sparse data is the norm, especially for intermediate tasks like parsing that require artificial labeling. Second, it assumes that all examples are drawn from the same distribution. Language is of course not like this: rather than being an ordered landscape, it is a wildly varying one, rich with strange growths and prone to sudden monstrous blooms like micro-blogging. Both these traits can cause a classically trained NLP system to behave poorly. To cope with data sparsity, a common strategy is semi-supervised learning, in which a small labeled data set is augmented by a larger amount of (typically more abundant) unlabeled data. To cope with domain differences between training and test data, adaptation techniques can be used to mitigate training data bias by exploiting whatever is known of the test domain. The link between these two topics is that what is known of the test domain often comes in the form of an unlabeled sample, and hence semi-supervised techniques constitute an important class of adaptation strategies. Søgaard’s book has at its core the intersection of these two important topics, although it also covers semi-supervised techniques without considering data bias, and techniques for handling bias that are not semi-supervised. Before proceeding, I should declare a bias of another sort, which is that of a machine translation (MT) researcher toward a book that cites only two MT papers related to semisupervised learning or domain adaptation (Habash 2008; Daum´e III and Jagarlamudi 2011), neither of which is highly representative of the fairly substantial MT work on these topics. I have done my best to apply my neural adaptation faculty to the material in this book; non-MT readers might find it helpful to do the same with this review. The book begins with two chapters of introductory material on machine learning and NLP that occupy half of its 80 pages (exclusive of bibliography). The main topics of semi-supervised learning and adaptation are then presented in approximately equalsized portions, with adaptation split into two chapters covering techniques for known and unknown test domains. A short final chapter deals with evaluation in the presence of domain shift. doi:10.1162/COLI r 00188 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 The introductory chapter plunges somewhat abruptly into a sketch of NLP as a machine learning problem, then emerges to motivate the core material of the book. Faced with a domain mismatch between training and test data, we are given the option of performing semi-supervised learning with an unlabeled test-domain sample, using either standard algorithms or specialized ones that address domain shift; or, if nothing is known about the test domain, using a learning approach that encourages robustness. Given the focus of the book, this catalog is entirely appropriate, but the scenarios it contemplates are special cases of a more general one in which both labeled and unlabeled training data are available for a collection of domains that may or may not include the test domain. (Incidentally, the 20-Newsgroups data used in the book for textclassification examples falls more naturally into this heterogeneous case than into the binary train/test split to which it is cast.) This section would have benefitted from an attempt to situate the approaches considered here within the more general setting, which is barely acknowledged throughout the book. Pointers to representative relevant work, both in NLP (Daum´e III 2007; Finkel and Manning 2009) and machine learning (Ben- David et al. 2010; Dredze, Kulesza, and Crammer 2010), would also have been an asset. Chapter 2 is a lengthy chapter that lays out basic techniques in a bid to make the book self-contained. This is always a tricky proposition, as it must tread a fine line between boring the expert and baffling the beginner. The author strikes a good balance here by emphasizing practical advice over theoretical completeness, and providing experimental results to underscore various points. Although a few passages would probably cause beginners to stumble, for instance one that invokes the concept of a generative story when explaining hidden Markov models (HMMs), with only a reference to Brown et al. (1993) for explanation, most of the material is very accessible, and there are many links into the literature. The expert reader will also not be unduly bored, and will find this chapter a quick and probably productive read. Although it does not present techniques in NLP, the chapter manages to cover a lot of ground while remaining coherent and for the most part not superfluous with respect to the remainder of the book. To summarize the contents of this chapter briefly, after a discussion of some relevant assumptions about data (smoothness, i.i.d., coherence), and related empirical tests, three basic classification techniques—nearest neighbor, naive Bayes, and perceptron—are described and compared experimentally on a 20-Newsgroups text-classification task. Next, weighted versions of these algorithms are given (omitting how the weights get Then a section on unsupervised learning presents hierarchical and clustering, along with a somewhat elliptical account of generalized EM. Finally, structured learning is introduced through HMM-based POS tagging—with a nod to conditional random fields and structured perceptrons—as well as transitionand graph-based dependency parsing. Chapter 3 describes a variety of semi-supervised learning algorithms. A section on wrapper methods traces two branches of work that refine self-training, in which a learner labels some of the unlabeled material and then trains on its own output. One branch is based on co-training, where two learners trained on complementary features label data for each other. This generalizes to three learners in tri-training, and ultimately to an arbitrary number in multi-view approaches (Ganchev et al. 2008), although these latter are not mentioned in the book. The other branch requires a learner that can assign probabilities to outcomes, and exploits this to iterate soft labeling and training on weighted examples in an expectation maximization (EM)-like (or just plain EM) procedure. (The section that describes this method includes “CO-EM” in the title, but the text never seems to make it to that tantalizing destination.) Another briefly mentioned 520 Book Reviews wrapper-type technique is to exploit a clustering of the unlabeled data in order to generate additional features for supervised learning; this is related to recent work that uses neural nets to learn embeddings from unlabeled data (Collobert et al. 2011). A final group of semi-supervised algorithms is specific to nearest-neighbor classification. Label propagation (Zhu and Ghahramani 2002) is a well-known iterative graph-based algorithm where neighbors vote on each node’s label, with votes weighted by distance. A similar voting takes place in editing and condensation, which are methods for identifying a subset of prototypical data points (similar to support vectors in support vector machines) in order to speed up nearest-neighbor search. Unlabeled data can be used to improve this process by essentially providing greater resolution. Chapter 4 is the first to grapple explicitly with domain mismatch, and begins by making a standard distinction between conditional and marginal distributions for inputs and outputs (Jiang and Zhai 2007). Here we are clearly limited to considering biased inputs, because there is only an unlabeled sample from the test domain. One strategy for exploiting this is to apply the semi-supervised methods from the previous chapter, which will work as-is if the mismatch between training and test domains is not too great. Otherwise, we can downweight instances in the labeled training set whose inputs are not close to those in the test-domain data; techniques for measuring distance include LMs and KL divergence. A similar approach can be applied to features, by comparing the training-data values of a feature across all examples (ignoring labels) to its test-domain values. A more sophisticated extension of this idea is structured correspondence learning (Blitzer, McDonald, and Pereira 2006), which automatically places test-domain features in correspondence with training-domain features. Chapter 5 tackles the ambitious goal of learning “defensively” so as to be robust to domain shifts in the absence of any prior information about the test domain. A common effect is the out-of-vocabulary (OOV) problem, in which features do not appear in a new test domain. If these have high weights, test-domain performance can degrade badly, to the point where it would have been better to have left them out of training in the first place, allowing other features to take up the slack. (Note that for some versions of the OOV problem—such as in MT—things aren’t this easy.) An interesting technique for countering feature OOVs, recently introduced to NLP by the author (Søgaard and Johannsen 2012), is adversarial learning, in which random subsets of features are removed during training. The chapter ends with a discussion of more traditional ensemble-based methods (voting, product of experts, stacking, meta-learning) that combine predictions from a set of diverse base classifiers in order to decrease variance. The final chapter gives a short treatment of the problem of how to predict the performance of systems on new domains. The main, and most interesting, suggestion is metaanalysis, a technique widely used in fields such as medicine and psychology. The idea is to extrapolate from many experimental trials (typically in previously published work) that use the same method but different data, while calibrating for differences among the trials and making use of their internal estimates of variance. I am not sure this is a perfect fit with NLP for many reasons, such as that many papers do not include estimates of variance, and the tendency of NLP systems to be sensitive to undocumented “minor” configuration changes; but it is definitely a direction that bears further investigation. To summarize, this is a monograph that focuses on the problem of optimizing an NLP system trained on a single domain for a test domain that is either unknown or represented by an unlabeled sample. When the unlabeled sample comes from the training domain, the task is plain semi-supervised learning, a case that is dealt with extensively. The book is written in an informal style, and sticks to basic machine learning concepts, which are illustrated with many examples involving text classification, 521 Computational Linguistics Volume 40, Number 2 POS tagging, and dependency parsing. In my opinion, the most interesting chapters are the shorter ones toward the end. I recommend these to any NLP researcher interested in the perennial problem of making do with not enough of the right kind of data. The printed version of the book has some imperfections that are mostly due to production quality. Many equations and graphs are fuzzy, and some graphs (e.g., Fig 3.3) are completely indecipherable due to their having been intended for color printing. An index would have been a boon, though its absence is partly compensated for by an extensive back-referenced bibliography. Finally, the text contains many large blocks of Python code, which are sometimes used in lieu of pseudo-code for describing algorithms. Although having real code is a bonus, it is harder to read than pseudo-code, and would have been much more useful to readers had it been moved out of the book and made available separately on-line.</abstract>
<title confidence="0.530083">References</title>
<note confidence="0.918539142857143">A theory of learning from different 79:151–175. Blitzer, John, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. of the 2006 Conference on Empirical Methods in Natural Language pages 120–128, Sydney. Brown, Peter F., Stephen A. Della Pietra, Vincent Della J. Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. 19(2):263–312. Collobert, Ronan, Jason Weston, L´eon Bottou,</note>
<author confidence="0.9092855">Natural language</author>
<abstract confidence="0.900321454545454">processing (almost) from scratch. of Machine Learning 12:2493–2537. Daum´e III, Hal. 2007. Frustratingly easy adaptation. In of the 45th Annual Meeting of the Association of pages 256–263, Prague. Daum´e III, Hal and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words.</abstract>
<title confidence="0.471622333333333">of the 49th Annual Meeting of the Association for Computational Human Language</title>
<note confidence="0.8099402">pages 407–412, Portland, OR. Dredze, Mark, Alex Kulesza, and Koby Crammer. 2010. Multi-domain learning This book review was edited by Pierre Isabelle. by confidence-weighted parameter 79:123–149. Finkel, Jenny Rose and Christopher D. Manning. 2009. Hierarchical Bayesian adaptation. In of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational pages 602–610, Boulder, CO. K., J. J. Blitzer, and B. Taskar. 2008. Multi-view learning over structured</note>
<abstract confidence="0.859671">non-identical outputs. In of the 24th Conference on Uncertainty in pages 204–211, Helsinki. Habash, Nizar. 2008. Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical translation. In of HLT, Short pages 57–60,</abstract>
<address confidence="0.606549">Columbus, OH.</address>
<note confidence="0.935149736842105">Jiang, Jing and ChengXiang Zhai. 2007. Instance weighting for domain adaptation NLP. In of the 45th Annual Meeting of the Association of Computational pages 264–271, Prague. Søgaard, Anders and Anders Johannsen. 2012. Robust learning in random subspaces: Equipping NLP for OOV In of COLING 2012: pages 1,171–1,180, Mumbai. Zhu, Xiaojin and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, Pittsburgh, PA. Foster a Senior Researcher in the Multilingual Text Processing Group at the National Research Council of Canada. His recent research has focused on adaptation and parameter for machine translation. Foster’s e-mail address is 522</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--151</pages>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2010</marker>
<rawString>Ben-David, Shai, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79:151–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<location>Sydney.</location>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>Blitzer, John, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent Della J Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4919" citStr="Brown et al. (1993)" startWordPosition="766" endWordPosition="769">n asset. Chapter 2 is a lengthy chapter that lays out basic techniques in a bid to make the book self-contained. This is always a tricky proposition, as it must tread a fine line between boring the expert and baffling the beginner. The author strikes a good balance here by emphasizing practical advice over theoretical completeness, and providing experimental results to underscore various points. Although a few passages would probably cause beginners to stumble, for instance one that invokes the concept of a generative story when explaining hidden Markov models (HMMs), with only a reference to Brown et al. (1993) for explanation, most of the material is very accessible, and there are many links into the literature. The expert reader will also not be unduly bored, and will find this chapter a quick and probably productive read. Although it clearly does not present all basic techniques in NLP, the chapter manages to cover a lot of ground while remaining coherent and for the most part not superfluous with respect to the remainder of the book. To summarize the contents of this chapter briefly, after a discussion of some relevant assumptions about data (smoothness, i.i.d., coherence), and related empirical</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent Della J. Pietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="7351" citStr="Collobert et al. 2011" startWordPosition="1134" endWordPosition="1137">n assign probabilities to outcomes, and exploits this to iterate soft labeling and training on weighted examples in an expectation maximization (EM)-like (or just plain EM) procedure. (The section that describes this method includes “CO-EM” in the title, but the text never seems to make it to that tantalizing destination.) Another briefly mentioned 520 Book Reviews wrapper-type technique is to exploit a clustering of the unlabeled data in order to generate additional features for supervised learning; this is related to recent work that uses neural nets to learn embeddings from unlabeled data (Collobert et al. 2011). A final group of semi-supervised algorithms is specific to nearest-neighbor classification. Label propagation (Zhu and Ghahramani 2002) is a well-known iterative graph-based algorithm where neighbors vote on each node’s label, with votes weighted by distance. A similar voting takes place in editing and condensation, which are methods for identifying a subset of prototypical data points (similar to support vectors in support vector machines) in order to speed up nearest-neighbor search. Unlabeled data can be used to improve this process by essentially providing greater resolution. Chapter 4 i</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Collobert, Ronan, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<location>Prague.</location>
<marker>Hal, 2007</marker>
<rawString>Daum´e III, Hal. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>407--412</pages>
<location>Portland, OR.</location>
<marker>Hal, Jagarlamudi, 2011</marker>
<rawString>Daum´e III, Hal and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 407–412, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Alex Kulesza</author>
<author>Koby Crammer</author>
</authors>
<title>Multi-domain learning This book review was edited by Pierre Isabelle. by confidence-weighted parameter combination.</title>
<date>2010</date>
<booktitle>Machine Learning,</booktitle>
<pages>79--123</pages>
<marker>Dredze, Kulesza, Crammer, 2010</marker>
<rawString>Dredze, Mark, Alex Kulesza, and Koby Crammer. 2010. Multi-domain learning This book review was edited by Pierre Isabelle. by confidence-weighted parameter combination. Machine Learning, 79:123–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical Bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>602--610</pages>
<location>Boulder, CO.</location>
<contexts>
<context position="4197" citStr="Finkel and Manning 2009" startWordPosition="648" endWordPosition="651"> more general one in which both labeled and unlabeled training data are available for a collection of domains that may or may not include the test domain. (Incidentally, the 20-Newsgroups data used in the book for textclassification examples falls more naturally into this heterogeneous case than into the binary train/test split to which it is cast.) This section would have benefitted from an attempt to situate the approaches considered here within the more general setting, which is barely acknowledged throughout the book. Pointers to representative relevant work, both in NLP (Daum´e III 2007; Finkel and Manning 2009) and machine learning (BenDavid et al. 2010; Dredze, Kulesza, and Crammer 2010), would also have been an asset. Chapter 2 is a lengthy chapter that lays out basic techniques in a bid to make the book self-contained. This is always a tricky proposition, as it must tread a fine line between boring the expert and baffling the beginner. The author strikes a good balance here by emphasizing practical advice over theoretical completeness, and providing experimental results to underscore various points. Although a few passages would probably cause beginners to stumble, for instance one that invokes t</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Finkel, Jenny Rose and Christopher D. Manning. 2009. Hierarchical Bayesian domain adaptation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 602–610, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>J Blitzer</author>
<author>B Taskar</author>
</authors>
<title>Multi-view learning over structured and non-identical outputs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>204--211</pages>
<location>Helsinki.</location>
<marker>Ganchev, Grac¸a, Blitzer, Taskar, 2008</marker>
<rawString>Ganchev, K., J. Grac¸a, J. Blitzer, and B. Taskar. 2008. Multi-view learning over structured and non-identical outputs. In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence, pages 204–211, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>57--60</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2103" citStr="Habash 2008" startWordPosition="321" endWordPosition="322">of the test domain often comes in the form of an unlabeled sample, and hence semi-supervised techniques constitute an important class of adaptation strategies. Søgaard’s book has at its core the intersection of these two important topics, although it also covers semi-supervised techniques without considering data bias, and techniques for handling bias that are not semi-supervised. Before proceeding, I should declare a bias of another sort, which is that of a machine translation (MT) researcher toward a book that cites only two MT papers related to semisupervised learning or domain adaptation (Habash 2008; Daum´e III and Jagarlamudi 2011), neither of which is highly representative of the fairly substantial MT work on these topics. I have done my best to apply my neural adaptation faculty to the material in this book; non-MT readers might find it helpful to do the same with this review. The book begins with two chapters of introductory material on machine learning and NLP that occupy half of its 80 pages (exclusive of bibliography). The main topics of semi-supervised learning and adaptation are then presented in approximately equalsized portions, with adaptation split into two chapters covering</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Habash, Nizar. 2008. Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation. In Proceedings of ACL-08: HLT, Short Papers, pages 57–60, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>264--271</pages>
<location>Prague.</location>
<contexts>
<context position="8142" citStr="Jiang and Zhai 2007" startWordPosition="1249" endWordPosition="1252">ased algorithm where neighbors vote on each node’s label, with votes weighted by distance. A similar voting takes place in editing and condensation, which are methods for identifying a subset of prototypical data points (similar to support vectors in support vector machines) in order to speed up nearest-neighbor search. Unlabeled data can be used to improve this process by essentially providing greater resolution. Chapter 4 is the first to grapple explicitly with domain mismatch, and begins by making a standard distinction between conditional and marginal distributions for inputs and outputs (Jiang and Zhai 2007). Here we are clearly limited to considering biased inputs, because there is only an unlabeled sample from the test domain. One strategy for exploiting this is to apply the semi-supervised methods from the previous chapter, which will work as-is if the mismatch between training and test domains is not too great. Otherwise, we can downweight instances in the labeled training set whose inputs are not close to those in the test-domain data; techniques for measuring distance include LMs and KL divergence. A similar approach can be applied to features, by comparing the training-data values of a fea</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jiang, Jing and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Anders Johannsen</author>
</authors>
<title>Robust learning in random subspaces: Equipping NLP for OOV effects.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1--171</pages>
<location>Mumbai.</location>
<contexts>
<context position="9729" citStr="Søgaard and Johannsen 2012" startWordPosition="1499" endWordPosition="1502">be robust to domain shifts in the absence of any prior information about the test domain. A common effect is the out-of-vocabulary (OOV) problem, in which features do not appear in a new test domain. If these have high weights, test-domain performance can degrade badly, to the point where it would have been better to have left them out of training in the first place, allowing other features to take up the slack. (Note that for some versions of the OOV problem—such as in MT—things aren’t this easy.) An interesting technique for countering feature OOVs, recently introduced to NLP by the author (Søgaard and Johannsen 2012), is adversarial learning, in which random subsets of features are removed during training. The chapter ends with a discussion of more traditional ensemble-based methods (voting, product of experts, stacking, meta-learning) that combine predictions from a set of diverse base classifiers in order to decrease variance. The final chapter gives a short treatment of the problem of how to predict the performance of systems on new domains. The main, and most interesting, suggestion is metaanalysis, a technique widely used in fields such as medicine and psychology. The idea is to extrapolate from many</context>
</contexts>
<marker>Søgaard, Johannsen, 2012</marker>
<rawString>Søgaard, Anders and Anders Johannsen. 2012. Robust learning in random subspaces: Equipping NLP for OOV effects. In Proceedings of COLING 2012: Posters, pages 1,171–1,180, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107,</tech>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="7488" citStr="Zhu and Ghahramani 2002" startWordPosition="1151" endWordPosition="1154">mization (EM)-like (or just plain EM) procedure. (The section that describes this method includes “CO-EM” in the title, but the text never seems to make it to that tantalizing destination.) Another briefly mentioned 520 Book Reviews wrapper-type technique is to exploit a clustering of the unlabeled data in order to generate additional features for supervised learning; this is related to recent work that uses neural nets to learn embeddings from unlabeled data (Collobert et al. 2011). A final group of semi-supervised algorithms is specific to nearest-neighbor classification. Label propagation (Zhu and Ghahramani 2002) is a well-known iterative graph-based algorithm where neighbors vote on each node’s label, with votes weighted by distance. A similar voting takes place in editing and condensation, which are methods for identifying a subset of prototypical data points (similar to support vectors in support vector machines) in order to speed up nearest-neighbor search. Unlabeled data can be used to improve this process by essentially providing greater resolution. Chapter 4 is the first to grapple explicitly with domain mismatch, and begins by making a standard distinction between conditional and marginal dist</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Zhu, Xiaojin and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>George Foster</author>
</authors>
<title>is a Senior Researcher in the Multilingual Text Processing Group at the National Research Council of Canada. His recent research has focused on adaptation and parameter estimation for machine translation. Foster’s e-mail address is george.foster@nrc-cnrc.gc.ca.</title>
<marker>Foster, </marker>
<rawString>George Foster is a Senior Researcher in the Multilingual Text Processing Group at the National Research Council of Canada. His recent research has focused on adaptation and parameter estimation for machine translation. Foster’s e-mail address is george.foster@nrc-cnrc.gc.ca.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
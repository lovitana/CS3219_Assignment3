<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.979425">
Unsupervised Event Coreference Resolution
</title>
<author confidence="0.722053">
Cosmin Adrian Bejan*
</author>
<affiliation confidence="0.637051">
Vanderbilt University
</affiliation>
<author confidence="0.944514">
Sanda Harabagiu**
</author>
<affiliation confidence="0.867186">
University of Texas at Dallas
</affiliation>
<bodyText confidence="0.99651356">
The task of event coreference resolution plays a critical role in many natural language pro-
cessing applications such as information extraction, question answering, and topic detection
and tracking. In this article, we describe a new class of unsupervised, nonparametric Bayesian
models with the purpose of probabilistically inferring coreference clusters of event mentions
from a collection of unlabeled documents. In order to infer these clusters, we automatically
extract various lexical, syntactic, and semantic features for each event mention from the doc-
ument collection. Extracting a rich set of features for each event mention allows us to cast
event coreference resolution as the task of grouping together the mentions that share the same
features (they have the same participating entities, share the same location, happen at the same
time, etc.).
Some of the most important challenges posed by the resolution of event coreference in an
unsupervised way stem from (a) the choice of representing event mentions through a rich set
offeatures and (b) the ability of modeling events described both within the same document and
across multiple documents. Our first unsupervised model that addresses these challenges is a
generalization of the hierarchical Dirichlet process. This new extension presents the hierarchi-
cal Dirichlet process’s ability to capture the uncertainty regarding the number of clustering
components and, additionally, takes into account any finite number offeatures associated with
each event mention. Furthermore, to overcome some of the limitations of this extension, we
devised a new hybrid model, which combines an infinite latent class model with a discrete time
series model. The main advantage of this hybrid model stands in its capability to automatically
infer the number of features associated with each event mention from data and, at the same
time, to perform an automatic selection of the most informative features for the task of event
coreference. The evaluation performed for solving both within- and cross-document event coref-
erence shows significant improvements of these models when compared against two baselines for
this task.
</bodyText>
<note confidence="0.986251222222222">
* Department of Biomedical Informatics, School of Medicine, Vanderbilt University, 400 Eskind Biomedical
Library, 2209 Garland Avenue, Nashville, TN 37232, USA. E-mail: adi.bejan®vanderbilt.edu.
** Human Language Technology Research Institute, Department of Computer Science, University of Texas
at Dallas, 800 West Campbell Road, Richardson, TX 75080, USA. E-mail: sanda®hlt.utdallas.edu.
Submission received: 6 February 2012; revised submission received: 9 May 2013; accepted for publication:
28 June 2013.
doi:10.1162/COLI a 00174
© 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
</note>
<sectionHeader confidence="0.988811" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999961083333333">
Event coreference resolution consists of grouping together the text expressions that refer
to real-world events (also called event mentions) into a set of clusters such that all the
mentions from the same cluster correspond to a unique event. The problem of event
coreference is not new. It was originally studied in philosophy, where researchers tried
to determine when two events are identical and when they are different. One relevant
theory in this direction was proposed by Davidson (1969), who argued that two events
are identical if they have the same causes and effects. Later on, a different theory was
proposed by Quine (1985), who considered that each event is associated with a physical
object (which is well defined in space and time), and therefore, two events are identical
if their corresponding objects have the same spatiotemporal location. According to
Malpas (2009), in the same year, Davidson abandoned his suggestion to embrace the
Quinean theory on event identity (Davidson 1985).
Resolving event coreference is an essential requirement for many natural language
processing (NLP) applications. For instance, in topic detection and tracking, event
coreference resolution is required in order to identify new seminal events in broadcast
news that have not been mentioned before (Allan et al. 1998). In information extraction,
event coreference information was used for filling predefined template structures from
text documents (Humphreys, Gaizauskas, and Azzam 1997). In question answering, a
novel method of mapping event structures was used in order to provide answer justi-
fication (Narayanan and Harabagiu 2004). The same idea of mapping event structures
was used in a graph-matching approach for enhancing textual entailment (Haghighi,
Ng, and Manning 2005). Event coreference information was also used for detecting
contradictions in text (de Marneffe, Rafferty, and Manning 2008).
Previous NLP approaches for solving event coreference relied on supervised learn-
ing methods that explore various linguistic properties in order to decide if a pair of
event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga
and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite
of being successful for a particular labeled corpus, in general, these pairwise models
are dependent on the domain or language that they are trained on. For instance, in
order to adapt a supervised system to run over a collection of documents written in
a different language or belonging to a different domain of interest, at least a minimal
annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these
models are dependent on local pairwise decisions, they are unable to capture a global
event distribution at the topic- or document-collection level.
To address these limitations, we departed from the idea of using supervised ap-
proaches for event coreference resolution and explored how a new class of unsuper-
vised, nonparametric Bayesian models can be used to probabilistically infer coreference
clusters of event mentions from a collection of unlabeled documents. In addition,
because an event can be mentioned multiple times in a document collection and its
mentions may occur both in the same document or across multiple documents, we
designed our unsupervised models to solve the two subproblems of within-document
and cross-document event coreference resolution. In order to evaluate the unsupervised
models for these two subproblems, we annotated a new data set encoding both within-
and cross-document event coreference information.
Besides our contribution of using unsupervised methods to solve within- and
cross-document event coreference, in this article we present novel Bayesian models
that provide a more flexible framework for representing data than current models. By
starting from the generic problem of clustering observable linguistic objects (i.e., event
</bodyText>
<page confidence="0.989766">
312
</page>
<note confidence="0.939383">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.998763652173913">
mentions) encoded into a large collection of text documents where the clusters (i.e.,
events) can be shared across documents, we devised our unsupervised models such
that they provide solutions to the following four desiderata:
1) We prefer the number of clusters (denoted by K) to be probabilistically inferred
from data rather than to be assigned to an a priori fixed value. This desideratum
of allowing K to be a free parameter in the Bayesian models devised for our
problem constitutes a more realistic approach because, in general, document
collections encode an unspecified number of latent linguistic structures.
2) We redefine the task of finding clusters of mentions that refer to the same events
as the task of identifying those mentions that share the same event participants
and the same event properties. For example, the same entity must participate in
all the event mentions that are coreferential; also, all the coreferential mentions
must have the same spatiotemporal location. These characteristics extracted for
each event mention from text are also called linguistic features and, in general,
the event mentions corresponding to each of these clusters are characterized by
a large set of features. Because of this, we desire that the generative process
associated with each Bayesian model to automatically adapt every time a new
feature is added in the feature extraction phase.
3) Although each event mention is represented as a feature-rich linguistic object,
there is no guarantee that all the features that describe event mentions have a
positive impact for the task of event coreference. Some of these features may be
redundant or may increase the complexity of the Bayesian models solving this
task and, consequently, they may contribute to lowering the overall performance
of event coreference. To address these problems, we wish to incorporate into the
Bayesian models a feature selection mechanism that is able to automatically build
a set of the most salient features from the initial feature set such that only these
salient features will participate in the process of clustering event mentions. In this
regard, we assume that a feature is salient if it corresponds to a large number of
samples in the generative process. We denote the size of the salient feature set
by M. Furthermore, in spite of the fact that the initial feature space describing
event mentions can have an unbounded number of features, we want the set of
salient features to be finite (i.e., M–finite) at any given point in time during the
generative process corresponding to each Bayesian model.
4) Finally, we also want our Bayesian models to capture the structural dependencies
of the observable objects. In this way, the models can take advantage of the se-
quential order in which the event mentions are generated inside each document.
We believe that these four desiderata constitute a more natural approach for clustering
complex linguistic objects from a large collection of documents and relax many of the
constraints imposed in the current clustering tasks.
It is worth pointing out that the generic problem described here can be instanti-
ated by tasks not only from the area of computational linguistics, but also from other
research areas as well. For instance, in biomedical informatics, clinical researchers can
use the new Bayesian models to perform studies over various cohorts of patients. In
this configuration, the observations to be clustered correspond to patients, and the
features associated with the patients can be extracted from clinical reports or can be
represented by structured clinical information (e.g., white blood cells, temperature,
</bodyText>
<page confidence="0.997867">
313
</page>
<note confidence="0.830888">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.9989078">
heart rate, respiratory rate, sputum culture). Another instance of the generic problem
described here is from data mining. In this domain, clustering tasks can be performed
over structured information stored in large tables (e.g., products, restaurants, hotels).
For this type of problem, each object is associated with a row in a table and the features
correspond to table columns.
</bodyText>
<sectionHeader confidence="0.99984" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.999895619047619">
Unlike entity coreference resolution, event coreference resolution is a relatively less-
studied task. One rationale is that events are expressed in many more varied linguistic
constructs. For example, event mentions are typically predications that require more
complex lexico-semantic processing, and furthermore, the capability of extracting fea-
tures that characterize them has been available only since semantic parsers based on
PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and
Lowe 1998) corpora have been developed. In contrast, entity coreference resolution
has been intensively studied and many successful techniques for identifying mention
clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009;
Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and
Ng 2011).
Even if entity coreference resolution has received much attention from the compu-
tational linguistic researchers, there is only limited work that incorporates event-related
information to solve entity coreference, typically by considering the verbs that are
present in the context of a referring entity as features. For instance, Haghighi and Klein
(2010) include the governor of the head of nominal mentions as features in their model.
Rahman and Ng (2011) used event-related information by looking at which semantic
role the entity mentions can have and the verb pairs of their predicates. More recently,
Lee et al. (2012) proposed an approach to jointly model event and entity coreference by
allowing information from event coreference to help entity coreference, and the other
way around. Their supervised method uses a high-precision entity resolution method
based on a collection of deterministic models (called sieves) to produce both entity and
event clusters that are optimally merged using linear regression. A similar technique
that treated entity and event coreference resolution jointly was reported in He (2007)
using narrative clinical data.
Research that aimed at resolving only event coreference was initiated by the
template merging task required in MUC evaluations and was primarily focused on
scenario-specific events (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin
1999). More recently, various supervised approaches using a mention-pair probabilistic
framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel–
based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference.
Tree kernel–based methods have also been used to solve a special case of event coref-
erence resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and
Zhou 2011). To the best of our knowledge, the framework for solving event coreference
presented in this article, extending the approach reported in Bejan and colleagues (Bejan
et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference
resolution that uses fully unsupervised methods and is based on Bayesian models.
Over the past years, Bayesian models have been extensively used for the purpose
of solving similar problems or subproblems of the generic problem presented in the
previous section. In 2003, Blei, Ng, and Jordan proposed a parametric approach, called
latent Dirichlet allocation (LDA), for automatically learning probability distributions
of words corresponding to a specific number of latent classes (or topics) from a large
</bodyText>
<page confidence="0.995557">
314
</page>
<note confidence="0.93909">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.99996228">
collection of text documents. In this latent class model, documents are expressed as
probabilistic mixtures of topics, while each topic has assigned a multinomial distribu-
tion over the words from the entire document collection. This approach also uses an
exchangeability assumption by modeling the documents as bags of words. The LDA
model and variations of it have been used in many applications such as topic modeling
(Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation
(Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images
(Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005),
discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu
2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009).
The LDA model, although attractive, has the disadvantage of requiring a priori knowl-
edge regarding the number of latent classes.
A more suitable approach for solving our problem is the hierarchical Dirichlet
process (HDP) model described in Teh et al. (2006). Like LDA, this model considers
problems that involve groups of data, where each observable object is sampled from
a mixture model and each mixture component is shared across groups. However, the
HDP mixture model is a nonparametric generalization of LDA that is also able to
automatically infer the number of clustering components K (the first desideratum for
our problem). It consists of a set of Dirichlet processes (DPs) (Ferguson 1973), in which
each DP is associated with a group of data. In addition, these DPs are coupled through
a common random base measure which is itself distributed according to a DP. Due to
the fact that a DP provides a nonparametric prior for the number of classes K, the HDP
setting allows for this number to be unbounded in each group. More recently, various
other applications have been proposed to improve the existing HDP inference algo-
rithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used
in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth,
and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval
(Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event
coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation
(Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free
grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007).
Although infinite latent class models like HDP have the advantage of automatically
inferring the number of categorical outcomes K, they are still limited in representing
feature-rich objects. Specifically, in their original form, they are not able to model the
data such that each observable object can be generated from a combination of multiple
features. For example, in HDP, each data point is represented only by its corresponding
word. For this reason, we built new Bayesian models on top of already-existing models
with the main goal of providing a more flexible framework for representing data.
The first model extends the HDP model such that it takes into account additional
linguistic features associated with event mentions. This extension is performed by
using a conditional independence assumption between the observed random variables
corresponding to object features. Thus, instead of considering as features only the words
that express the event mentions (which is the way an observable object is represented
in the original HDP model), we devised an HDP extension that is also able to represent
features such as location, time, and agent for each event mention. This extension was
inspired from the fully generative Bayesian model proposed by Haghighi and Klein
(2007). However, Haghighi and Klein’s model was strictly customized for the task of
entity coreference resolution. As also noted in Ng (2008) and Poon and Domingos
(2008), whenever new features need to be considered in Haghighi and Klein’s model,
the extension becomes a challenging task. Also, Daum´e III and Marcu (2005) performed
</bodyText>
<page confidence="0.996978">
315
</page>
<note confidence="0.830952">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.999815125">
related work in this direction by proposing a generative model for solving supervised
clustering problems.
As an alternative to the HDP model, an important extension of latent class models
that are able to represent feature-rich objects is the Indian buffet process (IBP) model
presented in Griffiths and Ghahramani (2005). The IBP model defines a distribution
over infinite binary sparse matrices that can be used as a nonparametric prior on the
features associated with observable objects. Moreover, extensions of this model were
considered in order to provide a more flexible approach for modeling the data. For
example, the Markov Indian buffet process (mIBP) (Van Gael, Teh, and Ghahramani
2008) was defined as a distribution over an unbounded set of binary Markov chains,
where each chain can be associated with a binary latent feature that evolves over time
according to Markov dynamics. Also, the phylogenetic Indian buffet process (pIBP)
(Miller, Griffiths, and Jordan 2008) was created as a non-exchangeable, nonparametric
prior for latent feature models, where the dependencies between objects were expressed
as tree structures. Examples of applications that utilized these models are: identification
of protein complexes (Chu et al. 2006), modeling of dyadic data (Meeds et al. 2006),
modeling of choice behavior (G¨or¨ur, J¨akel, and Rasmussen 2006), and event coreference
resolution (Bejan et al. 2009; Bejan and Harabagiu 2010).
Our extension of the HDP model still does not fulfill all the desiderata for the
generic problem introduced in Section 1. It still requires a mechanism to automati-
cally select a finite set of salient features that will be used in the clustering process
(third desideratum) as well as a mechanism for capturing the structural dependencies
between objects (fourth desideratum). To overcome these limitations, we created two
additional models. First, we incorporated the mIBP framework into our HDP extension
to create the mIBP–HDP model. And second, we coupled an infinite latent feature
model with an infinite latent class model into a new discrete time series model. For
the infinite latent feature model, we chose the infinite factorial hidden Markov model
(iFHMM) (Van Gael, Teh, and Ghahramani 2008) coupled with the mIBP mechanism
in order to represent the latent features as an infinite set of parallel Markov chains; for
the infinite latent class model, we chose the infinite hidden Markov model (iHMM)
(Beal, Ghahramani, and Rasmussen 2002). We call this new hybrid the iFHMM–iHMM
model.
</bodyText>
<subsectionHeader confidence="0.988043">
2.1 Contribution
</subsectionHeader>
<bodyText confidence="0.999540285714286">
This article represents an extension of our previous work on unsupervised event
coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). In this work, we
present more details on the problem of solving both within- and cross-document event
coreference as well as describe a generic framework for solving this type of problem
in an unsupervised way. As data sets, we consider three different resources, including
our own corpus (which is the only corpus available that encodes event coreference
annotations across and within documents). In the next section, we provide additional
information on how we performed the annotation of this corpus. Another major contri-
bution of this article is an extended description of the unsupervised models for solving
event coreference. In particular, we focused on providing further explanations about
the implementation of the mIBP framework as well as its integration into the HDP and
iHMM models. Finally, in this work, we significantly extended the experimental results
section, which also includes a novel set of experiments performed over the OntoNotes
English corpus (LDC-ON 2007).
</bodyText>
<page confidence="0.998598">
316
</page>
<note confidence="0.82852">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<sectionHeader confidence="0.92144" genericHeader="method">
3. Event Coreference Data Sets
</sectionHeader>
<bodyText confidence="0.999838641025641">
Because our nonparametric Bayesian models are also unsupervised, they do not require
the data set(s) on which they are trained to be annotated with event coreference infor-
mation. The only requirement for them to infer coreference clusters of event mentions
is to have the observable objects (i.e., the event mentions) identified in the order they
occur in the documents as well as to have all the linguistic features associated with
these objects extracted. However, in order to see how well these models perform, we
need to compare their results with manually annotated clusters of event mentions. For
this purpose, we evaluated our models on three different data sets annotated with event
coreference information.
The first data set was used for the event coreference evaluations performed in the
automatic content extraction (ACE) task (LDC-ACE 2005). This resource contains only a
restricted set of event types such as LIFE, BUSINESS, CONFLICT, and JUSTICE. As a second
data set, we used the OntoNotes English corpus (release 2.0), a more diverse resource
that provides a larger coverage of event (and entity) annotations. The utilization of
the ACE and OntoNotes corpora for evaluating our event coreference models is, how-
ever, limited because these resources provide only within-document event coreference
annotations. For this reason, as a third data set, we created the EventCorefBank (ECB)
corpus1 to increase the diversity of event types and to be able to evaluate our models
for both within- and cross-document event coreference resolution. Recently, Lee et al.
(2012) extended the EventCorefBank corpus with entity coreference information and
additional annotations of event coreference.
One important step in the creation process of the ECB corpus consists of finding
sets of related documents that describe the same seminal event2 such that the an-
notation of coreferential event mentions across documents is possible. In this regard,
we searched the Google News archive3 for various topics whose description contains
keywords such as commercial transaction, attack, death, sports, announcement, terrorist act,
election, arrest, natural disaster, and so on, and manually selected sets of Web documents
describing the same seminal event for each of these topics. In a subsequent step, for
every Web document, we automatically tokenized and split the textual content into
sentences, and saved the preprocessed data in a uniquely identified text file. Next,
we manually annotated a limited set of events in each text file in accordance with the
TimeML specification (Pustejovsky et al. 2003a). To mark the event mentions and the
coreferential relations between them we utilized the Callisto4 and Tango5 annotation
tools, respectively. Additional details regarding the annotation process for creating the
ECB resource are described in Bejan and Harabagiu (2008a).
Several annotation fragments from ECB are shown in Example (1). In this example,
event mentions are annotated at the sentence level, sentences are grouped into docu-
ments, and the documents describing the same seminal event are organized into topics.
The topics shown in Example (1) describe the seminal event of arresting sea pirates by a
</bodyText>
<footnote confidence="0.672732333333333">
1 The ECB corpus is available at http://www.hlt.utdallas.edu/∼ady/data/ECB1.0.tar.gz.
2 A seminal event in a document is the event that triggers the topic of the document and has
interconnections with the majority of events from its surrounding textual context. Furthermore, the
set of documents describing the same seminal event defines a topic. A more detailed description
of seminal events can be found in topic detection and tracking literature (Allan 2002).
3 http://news.google.com.
4 http://callisto.mitre.org.
5 Tango is a tool designed for annotating relations between the event mentions encoded in the TimeML
format and is available at http://timeml.org/site/tango/tool.html.
</footnote>
<page confidence="0.996776">
317
</page>
<figure confidence="0.991195892857143">
Computational Linguistics Volume 40, Number 2
Topic 12
Document 3
s1: In another anti-piracy operation, Navy warship on Saturday repulsed an attack on a merchant
vessel in the Gulf of Aden and [nabbed]em1 23 Somali and Yemeni sea brigands.
Topic 43
Document 3
s4: AMD agreed to [buy]em2 Markham, Ontario-based ATIfor around $5.4 billion in cash and stock,
the companies announced Monday.
s5: The [acquisition]em3 would turn AMD into one of the world’s largest providers of graphics
chips.
Topic 44
Document 2
s1: Hewlett-Packard is negotiating to [buy]em4 technology services provider Electronic Data Sys-
tems.
· · ·
s8: With a market value of about $115 billion, HP could easily use its own stock to finance the
[purchase]em5.
s9: If the [deal]em6 is completed, it would be HP’s biggest [acquisition]em7 since it [bought]em8
Compaq Computer Corp. for $19 billion in 2002.
Document 5
s2: Industry sources have confirmed to eWEEK that Hewlett-Packard will [acquire]em9 Electronic
Data Systems for about $13 billion.
Topic 55
Document 2
s2: Despite his [arrest]em10 on suspicion of driving under the influence yesterday, Chargers receiver
Vincent Jackson will play in Sunday’s AFC divisional playoff game at Pittsburgh.
Document 3
</figure>
<reference confidence="0.410537">
s2: San Diego Chargers receiver Vincent Jackson was [arrested]em11 on suspicion of drunk driving
on Tuesday morning, five days before a key NFL playoff game.
s3: Police [apprehended]em12 Jackson in San Diego at 2:30 a.m. and booked him for the misdemeanor
before his release.
</reference>
<subsectionHeader confidence="0.866447">
Example 1
Examples of event mention annotations.
</subsectionHeader>
<bodyText confidence="0.999700875">
Navy warship (topic 12), the event of buying ATI by AMD (topic 43), the event of buying
EDS by HP (topic 44), and the event of arresting a reputed football player (topic 55).
When taken out of context, the event mentions annotated in this example refer only to
two generic events: arrest and buy. On the other hand, when these mentions are contex-
tually associated with the event properties expressed in Example (1), five individuated
events can be distinguished: e1={em2, em3}, e2={em4−7, em9}, e3={em8}, e4={em1}, and
e5={em10, em11, em12}. For example, em4−7 are event mentions referring to the same real
event (of buying EDS by HP), whereas em2 (buy) and em4(buy) correspond to different
</bodyText>
<page confidence="0.995828">
318
</page>
<note confidence="0.644408">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.999627458333333">
individuated events because they have a different AGENT (i.e., BUYER(em2)=AMD is dif-
ferent from BUYER(em4)=HP). Similarly, the mentions em1(nabbed) and em12 (apprehended)
do not corefer because they correspond to different spatial and temporal locations (e.g.,
LOCATION(em1)=Gulf of Aden is different from LOCATION(em12)=San Diego).
This organization of event mentions leads to the idea of creating an event hier-
archy as the one illustrated in Figure 1. Specifically, this figure depicts the hierarchy
of the events described in Example (1). In this hierarchy, the nodes on the first level
correspond to event mentions (e.g., em11 corresponds to arrested), the nodes on the
second level correspond to individuated events (e.g., e5 subsumes all the event mention
nodes that refer to the arrest of Vincent Jackson), and, finally, the nodes on the third
level correspond to generic events (e.g., the node arrest contains all possible arrest
events). In this article, our focus is to discover the nodes on the second level of this
hierarchy.
As can be seen from Example (1), solving the event coreference problem poses
many interesting challenges. For instance, in order to solve the coreference chain of
event mentions that refer to the event e2, we need to take into account the following
issues: (i) a coreference chain can encode both within- and cross-document coreference
information; (ii) two mentions from the same chain can have different word classes (e.g.,
em4(buy)–verb, em5(purchase)–noun); (iii) not all the mentions from the same chain are
synonymous (e.g., em4(buy) and em9(acquire)), although a semantic relation might exist
between them (e.g., in WordNet [Fellbaum 1998], the genus of buy is acquire); (iv) not
all the properties associated with an event mention are expressed in text (e.g., all the
properties of em5(purchase) are omitted). In Section 7, we discuss additional challenges
of the event coreference problem that are not observed in Example (1).
</bodyText>
<sectionHeader confidence="0.908156" genericHeader="method">
4. Linguistic Features for Event Coreference Resolution
</sectionHeader>
<bodyText confidence="0.999889818181818">
The main idea for solving event coreference is to identify the event mentions (from the
same or different documents) that share the same characteristics (e.g., all the mentions
in a cluster convey the same meaning in text, have the same participants, and happen
in the same space and temporal location). Moreover, finding clusters of event mentions
that share the same characteristics is identical to finding clusters of mention features
that correspond to the same real event. For instance, Figure 2 depicts five clusters of
linguistic features that characterize the five individuated events from Example (1). As
can be observed, each individuated event corresponds to a subset of features that are
usually common to all the mentions referring to it. For this purpose, we extracted var-
ious linguistic features associated with each event mention from the ACE, OntoNotes,
and ECB corpora.
</bodyText>
<equation confidence="0.642945266666667">
event
generic
events
individuated
events
event em2 em3 em4 em5 em6 emq em9 em8 em1
mentions
e1
buy
e2
e3 e4
arrest
em10
em11
e5
</equation>
<page confidence="0.851166">
em12
</page>
<figureCaption confidence="0.673683">
Figure 1
Fragment from the event hierarchy.
</figureCaption>
<page confidence="0.993509">
319
</page>
<note confidence="0.338332">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.999017777777778">
Before describing in detail all the categories of linguistic features considered for
solving event coreference, we would like to emphasize that we make a clear distinction
between the notions of feature type and feature value throughout this article. A feature
type is represented by a characteristic that can be extracted with a specific methodology
and is associated with at least two feature values. For instance, the feature values
corresponding to the feature type WORD consist of all the distinct words extracted from
a given data set. In order to differentiate between the same values of different feature
types, we inserted to the notation of each feature value the name of its corresponding
feature type (e.g., WORD:play).
</bodyText>
<subsectionHeader confidence="0.997417">
4.1 Lexical Features (LF)
</subsectionHeader>
<bodyText confidence="0.999699833333333">
We capture the lexical context of an event mention by extracting the following features:
the head word (HW), the lemmatized head word (HL), the lemmatized left and right
words surrounding the mention (LHL, RHL), and the HL features corresponding to
the left and right mentions (LHE, RHE). For instance, the lexical features extracted
for the event mention em8(bought) from our example are HW:bought, HL:buy, LHL:it,
RHL:Compaq, LHE:acquisition, and RHE:acquire.
</bodyText>
<subsectionHeader confidence="0.994753">
4.2 Class Features (CF)
</subsectionHeader>
<bodyText confidence="0.9998563">
This category of features aims to group mentions into several types of classes: the part-
of-speech of the HW feature (POS), the word class of the HW feature (HWC), and the event
class of the mention (EC). The HWC feature type is associated with the following four fea-
ture values: VERB, NOUN, ADJECTIVE, and OTHER. As feature values for the EC feature
type, we consider the seven event classes defined in the TimeML specification language
(Pustejovsky et al. 2003a): OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE,
I ACTION, and I STATE. To extract all these event classes for all the event mentions, we
used an event identifier trained on the TimeBank corpus (Pustejovsky et al. 2003b), a
linguistic resource encoding temporal elements such as events, time expressions, and
temporal relations. More details about this event identifier are described in Bejan (2007).
</bodyText>
<subsectionHeader confidence="0.998934">
4.3 WordNet Features (WF)
</subsectionHeader>
<bodyText confidence="0.999891333333333">
In our efforts to create clusters of attributes corresponding to event mentions as close
as possible to the true attribute clusters of the individuated events, we built two sets
of word clusters using the entire lexical information from the WordNet database. After
</bodyText>
<figure confidence="0.540201888888889">
Event properties:
BUYER: AMD
SELLER: ATI
MONEY: $5.4 billion
Event properties:
BUYER: HP
SELLER: EDS
MONEY: $13 billion
e1
</figure>
<page confidence="0.337894">
e2
</page>
<note confidence="0.3449746">
e3 Event properties: e4 Event properties: e5 Event properties:
BUYER: HP SUSPECT: sea brigands SUSPECT: Vincent Jackson
SELLER: Compaq AUTHORITY: Navy warship AUTHORITY: police
MONEY: $19 billion LOCATION: Gulf of Aden LOCATION: San Diego
TIME: 2002 TIME: Saturday TIME: Tuesday
</note>
<figureCaption confidence="0.823653">
Figure 2
</figureCaption>
<bodyText confidence="0.68043">
Linguistic features associated with the individuated events encoded in Example (1).
</bodyText>
<page confidence="0.988027">
320
</page>
<note confidence="0.63957">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.9996925">
creating these sets of clusters, we associated each event mention with only one cluster
from each set. For the first set, we used the transitive closure of the WordNet SYNONY-
MOUS relation to form clusters with all the words from WordNet (WNS). For instance,
the verbs buy and purchase correspond to the same cluster ID because there exist a chain
of SYNONYMOUS relations between them in WordNet. For the second set, we considered
as grouping criteria the categorization of words from the WordNet lexicographer’s files
(WNL). In addition, for each word that is not represented in WordNet, we created a new
cluster ID in each set of clusters.
</bodyText>
<subsectionHeader confidence="0.998105">
4.4 Semantic Features (SF)
</subsectionHeader>
<bodyText confidence="0.999942235294118">
To extract features that characterize participants and properties of event mentions,
we used the semantic parser described in Bejan and Hathaway (2007). One category
of semantic features that we identified for event mentions is the predicate argument
structures encoded in the PropBank annotations (Palmer, Gildea, and Kingsbury 2005).
The predicate argument structures in PropBank are represented by events (or verbs)
and by the semantic roles (or predicate arguments) associated with these events. For
example, ARG0 annotates a specific type of semantic role which represents the AGENT,
DOER, or ACTOR of a specific event. Another argument is ARG1, which plays the role
of the PATIENT, THEME, or EXPERIENCER of an event. In Example (1), for instance,
the predicate arguments associated with the event mention em8(bought) are ARG0:[it],
ARG1:[Compaq Computer Corp.], ARG3:[ for $19 billion], and ARG-TMP:[in 2002].
Event mentions are not only expressed as verbs in text, but they also can occur as
nouns and adjectives. Therefore, for a better coverage of semantic features, we also used
the semantic annotations encoded in the FrameNet corpus (Baker, Fillmore, and Lowe
1998). FrameNet annotates word expressions capable of evoking conceptual structures,
or semantic frames, which describe specific situations, objects, or events. The semantic
roles associated with a word in FrameNet, or frame elements, are locally defined for
the semantic frame evoked by the word. In general, the words annotated in FrameNet
are expressed as verbs, nouns, and adjectives.
To preserve the consistency of the semantic role features, we aligned the frame
elements to the predicate arguments by running the PropBank semantic parser on the
manual annotations from FrameNet as well as running the FrameNet parser on the
PropBank annotations. Moreover, to obtain a better alignment for each semantic role, we
ran both parsers on a large amount of unlabeled text. The result of this process is a map
with all frame elements statistically aligned to all predicate arguments. For instance,
in 99.7% of the cases the frame element BUYER of the semantic frame COMMERCE BUY
is mapped to ARG0, and in the remaining 0.3% of the cases to ARG1. Additionally, we
used this map to create a more general semantic feature that assigns a frame element
label to each predicate argument. Examples of semantic features for the em8 mention
are ARG0:BUYER, ARG1:GOODS, ARG3:MONEY, and ARG-TMP:TIME.
Another two semantic features used in our experiments are: (1) the semantic frame
(FR) evoked by every mention in the data set, since in general, frames are able to capture
properties of generic events (Lowe, Baker, and Fillmore 1997); and (2) the WNS feature
applied to the head word of every semantic role (e.g., WSARG0, WSARG1).
</bodyText>
<subsectionHeader confidence="0.95263">
4.5 Feature Combinations (FC)
</subsectionHeader>
<bodyText confidence="0.99833">
We also explored various combinations of the given features. For instance, the feature
resulting from the combination of the HW and HWC feature types for em8(bought) in
</bodyText>
<page confidence="0.983409">
321
</page>
<note confidence="0.305673">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.832835">
Example (1) is HW+HWC:bought+VERB. Examples of additional feature combinations we
experimented with are HL+FR, HW+POS, FR+POS+EC, FE+ARG1, and so forth.
</bodyText>
<sectionHeader confidence="0.955828" genericHeader="method">
5. Finite Feature Models
</sectionHeader>
<bodyText confidence="0.999933173913043">
In this section, we first present HDP, a nonparametric Bayesian model that is capable
of clustering objects based on one feature type (i.e., WORD); then, we introduce a novel
extension of this model that describes an algorithm for clustering objects characterized
by multiple feature types.
The HDP models take as input a collection of I documents, where each document
i has Ji event mentions. Each event mention is characterized by L feature types (FT),
and each feature type is represented by a finite vocabulary of feature values (fv). For
example, the feature values extracted from an event coreference data set and associated
with the feature type HW constitute all possible head words of the event mentions
annotated in the data set. Therefore, we can represent the observable properties of an
event mention as a vector of pairs ((FT1:fv1i), ... , (FTL :fvLi)), where each feature value
index i ranges in the feature value space of its corresponding feature type. In the
description of these models, we also consider Z: the set of indicator random variables
for indices of events (i.e., an array of size equal with the number of event mentions in the
document collection where Zi,j represents the event index of the event mention j from
the document i); φz: the set of parameters associated with an event z; φ: a notation for all
model parameters; and X: a notation for all random variables that represent observable
features. As already introduced in Section 1, we denote by K the total number of latent
events.
Given a document collection annotated with event mentions, the goal is to find
the best assignment of event indices Z∗, which maximize the posterior probability
P(Z|X). In a Bayesian approach, this probability is computed by integrating out all
model parameters:
</bodyText>
<equation confidence="0.994245">
� �
P(Z|X) _ P(Z,φ|X)dφ _ P(Z|X,φ)P(φ|X)dφ (2)
</equation>
<subsectionHeader confidence="0.971523">
5.1 The HDP1f Model
</subsectionHeader>
<bodyText confidence="0.999979615384615">
The one feature model, denoted here as HDP1f, constitutes the simplest representation
of an HDP model. In this model, depicted graphically in Figure 3(a), the observable com-
ponents are characterized by only one feature type (e.g., the head lemma corresponding
to each event mention). The distribution over events associated with each document,
0, is generated by a Dirichlet process with a concentration parameter α&gt;0. Because
this setting enables a clustering of event mentions at the document level, it is desirable
that events be shared across documents and the number of events, K, be inferred from
data. To ensure this flexibility, a global nonparametric DP prior with a hyperparameter
γ and a global base measure H can be considered for 0 (Teh et al. 2006). The global
distribution drawn from this DP prior, denoted as 00 in Figure 3(a), encodes the event
mixing weights. Thus, the same global events are used for each document, but each
event has a document specific distribution 0i that is drawn from a DP prior centered
on 00.
</bodyText>
<page confidence="0.977537">
322
</page>
<figure confidence="0.999304433333334">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
γ
α
HL;
J;
(a)
H
Zi
β0
0C
β
0C
I
φ
0C
H
γ
β0
0C
α
β
0C
φ
0C
Zi
HL; FR;
J;
I
(b)
γ
α
β0
0C
0C
Zi
β
φ
0C
Xi
L
J&apos; I
H
(c)
H
γ
α
β0
0C
β
0C
Zi
φ
0C
θ
HL;
POS,
FR;
J;
I
(d)
</figure>
<figureCaption confidence="0.978246">
Figure 3
</figureCaption>
<bodyText confidence="0.976902571428571">
Graphical representation of four HDP models. Each node corresponds to a random variable. In
particular, shaded nodes denote observable variables. Each rectangle captures the replication of
the structure it contains. The number of replications is indicated in the bottom-right corner of the
rectangle. The model depicted in (a) is an HDP model using one feature type; the model in (b)
employs the HL and FR feature types; (c) illustrates a flat representation of a limited number of
feature types in a generalized framework (henceforth, HDPflat); and (d) captures a simple
example of structured network topology of three feature types (henceforth, HDPstruct).
To infer the true posterior probability of P(Z|X), we followed Teh et al. (2006)
and used a Gibbs sampling algorithm (Geman and Geman 1984) based on the direct
assignment sampling scheme. In this sampling scheme, the β and φ parameters are
integrated out analytically. The formula for sampling an event index for mention j from
document i, Zi,j, is given by:6
P(Zi,j  |Z−i,j, HL) a P(Zi,j  |Z−i,j)P(HLi,j  |Z, HL−i,j) (3)
where HLi,j is the head lemma of event mention j from document i.
</bodyText>
<page confidence="0.676383">
6 Z−i,j represents a notation for Z − {Zi,j}.
323
</page>
<note confidence="0.328221">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.999217">
First, in the generative process of an event mention, an event index z is sam-
pled by using a mechanism that facilitates sampling from a prior for infinite mixture
models called the Chinese restaurant franchise (CRF) representation, as reported in
(Teh et al. 2006):
</bodyText>
<equation confidence="0.961681">
� αβu if z = znew
P(Zi,j = z  |Z−i,j, β0) ∝ 0, (4)
nz + αβz 0, otherwise
</equation>
<bodyText confidence="0.999916285714286">
In this formula, nz is the number of event mentions with event index z, znew is a new
event index not used already in Z−i,j, βz0 are the global mixing proportions associated
with the K events, and βu0 is the weight for the unknown mixture component.
Next, to generate the mention head lemma (in this model, X = hHLi), the event z is
associated with a multinomial emission distribution over the HL feature values having
the parameters φ=hφhlZi. We assume that this emission distribution is drawn from a
symmetric Dirichlet distribution with concentration λHL:
</bodyText>
<equation confidence="0.966436">
P(HLi,j = hl  |Z, HL−i,j) ∝ nhl,z + λHL (5)
</equation>
<bodyText confidence="0.999685">
where HLi,j is the head lemma of mention j from document i, and nhl,z is the number of
times the feature value hl has been associated with the event index z in (Z, HL−i,j).
</bodyText>
<subsectionHeader confidence="0.995758">
5.2 The HDPflat Model
</subsectionHeader>
<bodyText confidence="0.999852384615385">
A model in which observable components are represented only by one feature type has
the tendency to cluster these components based on their corresponding feature values.
This model may produce good results for tasks such as topic discovery where the
linguistic objects rely only on lexical information. Because event coreference involves
clustering complex objects characterized by a large number of features, it is desirable to
extend the HDP1f model with a generalized model where additional feature types can
be easily incorporated. Moreover, this extension should allow multiple feature types to
be added simultaneously.
To facilitate this extension, we assume that the feature variables are conditionally in-
dependent given Z. This assumption considerably reduces the complexity of computing
P(Z|X). For example, if we want to incorporate into the previous model the feature
type associated with the semantic frame evoked by every event mention (i.e., FR), the
formula becomes:
</bodyText>
<equation confidence="0.90388">
P(Zi,j |HL, FR) ∝ P(Zi,j)P(HLi,j, FRi,j |Z)
(6)
∝ P(Zi,j)P(HLi,j |Z)P(FRi,j |Z)
</equation>
<bodyText confidence="0.9957195">
In this formula, we omit the conditioning components of Z, HL, and FR for the sake
of clarity. The graphical representation corresponding to this model is illustrated in
Figure 3(b). In general, if X consists of L feature variables, the inference formula for
the Gibbs sampler is defined as:
</bodyText>
<equation confidence="0.9211275">
P(Zi,j |X) ∝ P(Zi,j) � P(FTi,j |Z) (7)
FT∈X
</equation>
<page confidence="0.974435">
324
</page>
<note confidence="0.565392">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.999685333333333">
The graphical model for this general setting is depicted in Figure 3(c). Drawing an
analogy, the graphical representation involving Z and feature variables resembles the
graphical representation of a naive Bayes classifier.
</bodyText>
<subsectionHeader confidence="0.993233">
5.3 The HDPstruct Model
</subsectionHeader>
<bodyText confidence="0.9999746">
When dependencies between feature type variables exist (e.g., in our case, frame
elements are dependent on the semantic frames that define them, and frames are
dependent on the words that evoke them), various global distributions are involved
for computing P(Z|X). For the model depicted in Figure 3(d), for instance, the posterior
probability is given by:
</bodyText>
<equation confidence="0.9515535">
P(Zi,j |X) a P(Zi,j)P(FRi,j|HLi,j, 0) � P(FTi,j |Z) (8)
FT∈X
</equation>
<bodyText confidence="0.999959230769231">
In this model, P(FRi,j |HLi,j, 0) is a global distribution parameterized by 0, and FT is
a feature type variable from the set X=(HL, POS, FR). However, one limitation of
this particular model is that it requires domain knowledge in order to establish the
dependencies between the feature type variables.
For all the HDP extended models, we computed the prior and likelihood factors
as described in the HDP1f model. In the inference mechanism, we assigned soft counts
to those likelihood factors whose corresponding feature values cannot be extracted for
a given event mention (e.g., unspecified predicate arguments). It is worth noting that
there exist event mentions for which not all the features can be extracted. For instance,
the feature types corresponding to the left and right lemmatized head words (denoted in
Section 4 as LHE and RHE, respectively) are missing for the first and last event mentions
in a document. Also, many semantic roles can be absent for an event mention in a given
context.
</bodyText>
<sectionHeader confidence="0.992844" genericHeader="method">
6. Infinite Feature Models
</sectionHeader>
<bodyText confidence="0.999948833333333">
One of the main limitations of the HDP extensions presented in the previous section
is that these models have limited capabilities in representing the observable objects
characterized by a large number of feature types. This is because, in order to sample
the event indices into the set of indicator random variables Z, the HDP models need to
store in memory large matrices that encode the significant statistics for the observable
components associated with each cluster. More specifically, in order to compute the
likelihood factors in Equation (5), for each feature type FTi, i = 1... L, we assigned a
counting matrix having the number of rows equal with the number of distinct feature
values corresponding to FTi and K + 1 columns, where K represents the number of
inferred events. For instance, the counting matrix corresponding to the head lemma
feature type (HL) stores the number of times each feature value of the HL feature type
has been associated with each event index during the HDP generative process. The
number nhl,z in Equation (5), for example, is stored in a cell of this matrix.
Just to have an idea of how much memory the HDP models require to infer the
events from OntoNotes, we made the following calculation. In OntoNotes, we automat-
ically identified a total number of 81,938 event mentions for which we extracted 454,170
distinct feature values. For all data sets, we considered L = 132 feature types, which
means that, on average, each feature type is associated with approximately 3,440 feature
</bodyText>
<page confidence="0.987524">
325
</page>
<note confidence="0.327122">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.999943892857143">
values. Because K is bounded by the total number of event mentions considered (i.e.,
the case when each event mention is associated with a different event), the maximum
value that it can reach when inferring the event indices from OntoNotes is 81,938. If we
consider that each cell from the counting matrices associated with each feature type is
represented into the memory by one byte, the total space required to store only one such
matrix is, on average, 81,938x3,440 bytes. By a simple computation, the total amount of
memory to store all 132 matrices is ∼ 34.6 gigabytes (GB). Furthermore, by adding more
data, the amount of memory needed by the HDP models increases considerably. For
instance, if we consider all three data sets (with a total number of 148,402 event mentions
and 832,611 distinct feature values), the memory space required increases to 115 GB.
Because in our implementation we used the int type (4 bytes) to represent the counting
matrices, the total amount of memory required by the HDP extensions to infer the event
indices from OntoNotes and all three data sets when considering all 132 feature types is
in fact 4 x 34.6 = 138.4 GB and 4 x 115 = 460.3 GB, respectively.
Due to this limitation, the HDP extensions will be able to run only using a restricted,
manually selected set of feature types.7 Therefore, the existence of a novel methodology
that is able to consider a much smaller subset of representative feature values from the
entire feature space is necessary. For this purpose, we devised two novel approaches
that provide a more flexible representation of the data by modeling event mentions
with an infinite number of features and by using a mechanism to automatically select
a finite set of the most salient features for each mention in the inference process. The
first approach uses the Markov Indian buffet process (mIBP) to represent each object
as a sparse subset of a potentially unbounded set of latent features (Griffiths and
Ghahramani 2006; Ghahramani, Griffiths, and Sollich 2007; Van Gael et al. 2008), and
combines it with the HDP extension presented in the previous section. We call this
hybrid the mIBP–HDP model. The second approach uses the infinite factorial hidden
Markov model (iFHMM), which is an extension of mIBP, and combines it with the
infinite hidden Markov model (iHMM) to form the iFHMM–iHMM model.
</bodyText>
<subsectionHeader confidence="0.994213">
6.1 The mIBP–HDP Model
</subsectionHeader>
<bodyText confidence="0.919508647058824">
In this section, we describe a model that is able to represent event mentions charac-
terized by an unbounded set of feature values into the HDP framework. Although the
feature space describing event mentions is unbounded, this approach is able to model
the uncertainty in the number of feature values M that will be used for clustering event
mentions and, at the same time, is able to guarantee that this number is finite at any
point in time during the generative process. First, we use mIBP to describe a mechanism
for assigning to each event mention a sparse subset of feature values from the set of M
observed feature values used in the clustering process. We will use the set of notations
introduced in this description when presenting both mIBP–HDP and iFHMM–iHMM
models. Then, we will show how this mechanism is coupled into the HDP framework.
6.1.1 The Markov Indian Buffet Process. The Markov Indian buffet process (Van Gael, Teh,
and Ghahramani 2008) defines a distribution over an unbounded set of independent
hidden Markov chains, where each chain is associated with a binary latent feature value
7 Because, in general, most of the counts corresponding to each feature value are assigned to a single
cluster, a partial solution for this problem would be an efficient way of managing the sparsity in
the counting matrices. However, the main issue of representing the entire set of features into the
HDP models remains unaddressed.
</bodyText>
<page confidence="0.984438">
326
</page>
<note confidence="0.571126">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.999916791666666">
that evolves over time according to Markov dynamics. Specifically, if we denote by M
the total number of Markov chains associated with the latent feature values and by T the
number of observations, mIBP defines a probability distribution over a binary matrix F
with an unbounded number of rows M (M→oo) and T columns.
In our framework, we use mIBP to incrementally build the set of M observed feature
values that will be used for clustering event mentions (denoted as {f1,f2,...,fM}), as
well as to determine which of these feature values will be selected to explain each event
mention. The sequence of observations is associated with the sequence of event men-
tions, y1, y2, . . ., yT, and each latent feature value in the mIBP framework is associated
with one observed feature value from the unbounded set of features that characterize
our event mentions. It is worth mentioning that, at any given time point during the
mIBP generative process, from the unbounded set of observed features, we index only
these M observed feature values that correspond to the set of hidden feature values.
The selection of the observed feature values which will represent each event men-
tion in the clustering process is determined by the indicator random variables of the
binary matrix F. For instance, the selection of the observed feature value f i for the event
mention yt is indicated by an assignment of the binary random variable Fit to 1 in the
mIBP generative process. More specifically, the set of observed feature values that will
represent the event mention yt is indicated in the matrix by the column vector of binary
random variables Ft =(F1t , F2t , ... , FMt ). Therefore, F decomposes the event mentions and
represents them as feature value factors, which can then be associated with hidden
variables in an iFHMM model as described in Van Gael, Teh, and Ghahramani (2008).
The transition probabilities of the binary Markov chain associated with a latent
feature value, Fm =(Fm1 , Fm 2 , ... , FmT), are given by the following transition matrix:
</bodyText>
<equation confidence="0.9967424">
�1 − am am/
W(m) =
−
1
bm bm (9)
</equation>
<bodyText confidence="0.9857534">
where W(m)
ij =P(Fmt+1=j|Fmt =i), the parameters am ∼Beta(α′/M, 1) and bm ∼Beta(γ′, b′),
and the initial state Fm0 =0. In the mIBP process, the hidden variable associated with
an observed feature value fm and an event mention yt is generated from the following
Bernoulli distribution:
</bodyText>
<subsectionHeader confidence="0.915736">
Fmt ∼ Bernoulli(am F` 1 bm 1) (10)
</subsectionHeader>
<bodyText confidence="0.999829909090909">
Based on these definitions, we computed the probability of the feature matrix F8 (in
which the parameters a and b are integrated out analytically) by recording the number
of 0→0, 0→1, 1→0, and 1→1 transitions for each binary chain m into the counting
variables c00m , c01m , c10m , and c11m , respectively. For example, the c11m associated with the
feature value representing the VERB class (fm = HWC:VERB) counts how many times
this feature value was assigned to the event mention yt when it was also assigned to the
previous event mention yt−1 during the generative process.
The stochastic process that derives the probability distribution in terms of these
variables is defined as follows. In the first step, the process assigns a value of 1 to a
number of Poisson(α′) latent features for the first component. In our implementation,
this statement is equivalent with the process of randomly selecting for the first event
</bodyText>
<footnote confidence="0.357762">
8 Technical details for computing this probability are described in Van Gael, Teh, and Ghahramani (2008).
</footnote>
<page confidence="0.972208">
327
</page>
<note confidence="0.533655">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.996720333333333">
mention a number of Poisson(α′) observed feature values. In the general case, the sam-
pling of the binary variable from the mth Markov chain and associated with the tth event
mention depends on the value assigned to the hidden variable in the previous t —1 step:
</bodyText>
<equation confidence="0.875033">
c11m + δ′ (11)
P(Fm t = 1|Fm t−1=1) = γ′ + δ′ + c10 m + c11 m
P(Fm t= 1|Fm t−1 =0) = c01 m
c00 m + c01 m
</equation>
<bodyText confidence="0.999144066666667">
As a result, in our implementation, the observed feature value fm is selected for the tth
event mention according to the probabilities presented in Equation (11). For example,
in order to select the feature value which indicates that the tth event mention has the
OCCURRENCE event class (i.e., fm = EC:OCCURRENCE), we need to determine whether
or not the event mention t — 1 from the document collection selected this feature value.
In the cases when EC:OCCURRENCE was previously selected for the event mention t — 1
(Fmt−1=1), we select this feature value according to P(Fmt = 1|Fmt−1=1). Otherwise, the
selection is determined according to P(Fmt = 1|Fmt−1 =0). Furthermore, in the tth step of
the generative process, the same sampling mechanism is repeated until all M latent
feature values are generated. After sampling all these feature values for the tth event
mention, an additional number of Poisson(α′/t) new feature values are assigned to this
mention, and M gets incremented accordingly.
As an observation regarding the mIBP generative process, it has been shown that
M grows logarithmically with the number of observed components (in our case, event
mentions) (Ghahramani, Griffiths, and Sollich 2007; Doshi-Velez 2009). This type of
growth is desirable because it provides a scalable solution for our models to work in
an efficient way on fairly large data sets.
6.1.2 Integration of mIBP into HDP. One direct application of the mIBP model is to
integrate it into the framework of the HDP extension model described in the previous
section. In this way, the new nonparametric extension will have the benefits of capturing
the uncertainty regarding the number of mixture components that are characterized by
a potentially infinite number of feature values. However, to make this hybrid work, we
have to devise a mechanism in which only a finite set of relevant feature values will be
selected to explain each observation (i.e., event mention) in the HDP inference process.
Our idea of selecting a finite set of representative feature values for each event
mention is based on a heuristic approach that is used after running the mIBP generative
process. Specifically, by considering the event mention yt, fm one of the feature values
that characterizes yt, qm the number of times fm was selected for all mentions during
mIBP, and vt a threshold variable for yt such that vt—Uniform(1,maxfqm  |Fmt =1}), we
define the finite set of feature values Bt corresponding to the observation yt as:
</bodyText>
<equation confidence="0.922232">
Bt=ffm  |Fmt = 1 ∧ qm &gt; vt} (12)
</equation>
<bodyText confidence="0.9985166">
A pictorial representation of this idea is illustrated in Figure 4, where only the
feature values fm with the corresponding counts qm above the threshold indicated by
vt are selected in Bt. The finiteness of this feature set is based on the observation that,
at any time point during the generative process of the mIBP model, only a finite set of
latent features have assigned a value of 1 for an event mention. Furthermore, based on
</bodyText>
<page confidence="0.974239">
328
</page>
<figure confidence="0.95185475">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
qm
vt
f1 f2 f3 f4 . . .fm−1 fm fm+1 fM
</figure>
<figureCaption confidence="0.976123">
Figure 4
</figureCaption>
<bodyText confidence="0.969202742857143">
Graphical representation of the mechanism for filtering the feature values associated with the
event mention yt. After this mechanism is applied, yt will be represented only by the feature
values fm for which their corresponding counts qm are above the threshold variable vt.
the assumption that the more a feature value is selected during the mIBP generative
process the more relevant it is for the event coreference task, each set Bt contains the
most informative feature values that are able to explain its corresponding event mention
yt. This last property is ensured by the second constraint imposed when building each
set Bt (i.e., qm &gt; vt). Due to the fact that the threshold variables are sampled using a
uniform distribution, we denote this model as mIBP–HDPuniform.
The feature values selected by this mechanism are used to represent the event men-
tions in the clustering process of the HDP. The main difference from the original imple-
mentation of the HDP extensions is that, in this new model, instead of representing the
event mentions by the entire set of feature values from the initial feature space (which
can be as large as possible), only a restricted subset of these feature values is considered.
Furthermore, due to the random process of selecting the feature values, the number of
feature values associated with each event mention can vary significantly. We adapted
the implementation of the HDP framework to this modification by truncating all count-
ing matrices such that they will represent only the feature values selected in mIBP. More
specifically, we removed from each counting matrix the rows corresponding to all the
feature values that were not selected during the mIBP generative process. Because M
grows as O(log T), it now becomes feasible for the HDP extension models to represent
event mentions using the entire set of feature types. It is important to mention that this
modification does not affect the implementation of the Gibbs sampler in the HDP frame-
work because we always normalize the probabilities corresponding to the likelihood
factors in Equation (5) when computing the posterior distribution over event indices.
Moreover, using the assumption that the relevance of a feature value is proportional
with the number of times it was selected during the mIBP generative process, we
explored additional heuristics for building the sets of feature values Bt for each event
mention. In general, we chose these new heuristics to be biased towards selecting more
relevant feature values fm for each event mention yt (i.e., their counts qm to be closer to
max{qm  |Fmt =1}). One such heuristic is based on the method that considers for each
event mention yt all feature values fm with the counts qm &gt; 1 (i.e., vt = 1). In this case,
each set Bt contains all the observed feature values selected for each event mention yt
during the mIBP process, and therefore it represents a subset of the set of observed fea-
ture values {f1,f2, ... ,fM}. It is worth mentioning that all the subsets of {f1, f2, ... , fM}
</bodyText>
<page confidence="0.991625">
329
</page>
<note confidence="0.542604">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.999766416666667">
are finite due to the fact that M is finite at any given point in time during mIBP. In
consequence, all the Bt sets derived using this heuristic are finite. Because no feature
value is filtered out after it was assigned to an event mention during mIBP, we denote
the model implementing this heuristic as mIBP–HDPunfiltered. Starting from the distri-
bution of the counting variables qm corresponding to those feature values fm selected
during the mIBP generative process for an event mention yt, another heuristic considers
for building each set Bt only the feature values with the counts above the median of
this distribution (mIBP–HDPmedian). Finally, the last heuristic we experimented with is
based on the idea of sampling the threshold variables vt directly from the distribution
of the counting variables associated with each event mention yt (mIBP–HDPdiscrete). The
implementation of these three heuristics is possible due to the observation that in the
mIBP–HDP framework the size of each set Bt is not required to be known in advance.
</bodyText>
<subsectionHeader confidence="0.987416">
6.2 The iFHMM–iHMM Model
</subsectionHeader>
<bodyText confidence="0.909331416666667">
Over the years, the hidden Markov model (HMM) (Rabiner 1989) has proven to be one
of the most commonly used statistical tools for modeling time series data. Due to the
efficiency in estimating its parameters, various HMM generalizations were proposed
for a better representation of the latent structure encoded in this type of data. Figure 5
illustrates a hierarchy of HMM extensions whose main criteria of expansion is based on
relaxing the constraints on the parameters M (the number of state chains) and K (the
number of clustering components). In the factorial hidden Markov model (FHMM),
Ghahramani and Jordan (1997) introduced the idea of factoring the hidden state space
into a finite number of state variables, in which each of these variables has its own
Markovian dynamics. Later on, Van Gael, Teh, and Ghahramani (2008) introduced the
Figure 5
Extensions of the hidden Markov model.
</bodyText>
<figure confidence="0.987000597402597">
8
(Van Gael et al., 2008)
F2T
8
M
K
F2 F2 F2
0 1 2
S0 S1 S2
ST
FM0
FM FM
1 2
FMT
Y1 Y2
YT
HMM
M = 1
K − finite
F20
F10
F21
F11
F22
F12
F2T
F1T
(Rabiner, 1989)
S0 S1 S2
ST
Y2
(Ghahramani and Jordan, 1987)
YT
Y1
FHMM
M − finite
Y1 Y2
YT
K − finite
iHMM
M = 1
(Beal et al., 2002)
FM0
F20
F10
FM FM
1 2
F21
F11
Y1
F22
F12
Y2
FMT
F2T
F1T
YT
iFHMM
M
K − finite
iFHMM − iHMM
M M M
F0 F1 F2
S0 S1 S2
FTM
ST
F10
F11
F12
F1T
Y1
Y2
YT
K
8
8
K
</figure>
<page confidence="0.934025">
330
</page>
<note confidence="0.758682">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.999692976744186">
infinite factorial hidden Markov model (iFHMM) with the purpose of allowing the
number of parallel Markov chains M to be learned from data. Although the iFHMM
provides a more flexible representation of the latent structure, it cannot be used as a
framework where the number of clustering components K is infinite. In this direction,
Beal, Ghahramani, and Rasmussen (2002) proposed the infinite hidden Markov model
(iHMM) in order to perform inferences with an infinite number of states K. To further
increase the representational power for modeling discrete time series data, we introduce
a novel nonparametric extension that combines the best of the iFHMM and iHMM
models (denoted as iFHMM–iHMM) and lets both parameters M and K to be learned
from data.
As shown in Figure 5, the graphical representation of this new model consists
of a sequence of hidden state variables, (s1, ... , sT), that corresponds to the sequence
of event mentions (y1, ... , yT). Each hidden state st can be assigned to one of the K
latent events, st EJ1,. . . , K}, and each mention yt is represented by a column vector of
binary random variables (F1 t ,F2 t ,...,FM t ). One element of the transition probability 7t is
defined as 7tij =P(st =j|st−1=i), and a mention yt is generated according to a likelihood
model F that is parameterized by a state-dependent parameter φst (yt |st —F(φst )). The
observation parameters φ are independent and identically distributed drawn from a
prior base distribution H.
6.2.1 Inference. The main idea of the inference mechanism corresponding to this new
model is illustrated in Figure 6. As depicted in this figure, each step in the generative
process of the new hybrid model is performed in two consecutive phases. In the first
phase, the binary random variables associated with each feature value from the iFHMM
framework are sampled using the mIBP mechanism, and consequently, the most salient
feature values are selected for each event mention (Figure 6: Phase I). Of note, the Bt
sets of feature values associated with each event mention yt are determined using the
same set of heuristics as described in Section 6.1. In the second phase, the feature values
sampled so far, which become observable during this phase, are used in an adapted
beam sampling algorithm (Van Gael et al. 2008) to infer the clustering components or
latent events (Figure 6: Phase II).
Because we utilized the same mechanism for determining the sets of relevant fea-
ture values for each event mention (as described in Section 6.1), in this section we focus
on describing our implementation of the beam sampling algorithm. The beam sampling
algorithm (Van Gael et al. 2008) combines the ideas of slice sampling (Neal 2003) and
dynamic programming for an efficient sampling of state trajectories. Because in time
series models the transition probabilities have independent priors (Beal, Ghahramani,
and Rasmussen 2002), Van Gael et al. (2008) also used the HDP mechanism to allow
couplings across transitions. For sampling the whole hidden state trajectory s, this
algorithm uses a forward filtering-backward sampling technique.
As described in Van Gael et al. (2008), in the forward step, an auxiliary variable ut
is sampled for each mention yt, ut —Uniform(0, 7tst−1st). The auxiliary variables u are used
to filter only those trajectories s for which 7tst−1st &gt;ut, for all t. Also, in this step, for all t,
the probabilities P(st |y1:t, u1:t) are computed as follows:
</bodyText>
<equation confidence="0.520402">
P(st  |y1:t,u1:t) oc P(yt  |st) E P(st−1  |y1:t−1, u1:t−1) (13)
</equation>
<bodyText confidence="0.7673345">
st−1:ut&lt;πst−1st
In this formula, the dependencies involving parameters 7t and φ are omitted for clarity.
</bodyText>
<page confidence="0.993572">
331
</page>
<figure confidence="0.857810333333333">
Computational Linguistics Volume 40, Number 2
Phase I
Phase II
</figure>
<equation confidence="0.942752">
FM FM FM FM
0 1 2 T
2 2 2 2
F0 F1 F2 FT
1 1 1 1
F0 F1 F2 FT
FM FM FM FM
0 1 2 T
F2 F2 F2 F2
0 1 2 T
F1 F1 F1 F1
0 1 2 T
S0 S1 S2 ST
S0 S1 S2 ST
Y1 Y2 YT
Y1 Y2 YT
</equation>
<figureCaption confidence="0.888108">
Figure 6
</figureCaption>
<bodyText confidence="0.997172166666667">
A step in the generative process of the iFHMM–iHMM model is performed in two phases:
(Phase I) sample the feature values for each event mention, and (Phase II) sample the
latent events.
In the backward step, first, the event for the last state sT is directly sampled from
P(sT |y1:T, u1:T) and then, for all t : T − 1, 1, each state st given st+1 is sampled using the
following formula:
</bodyText>
<equation confidence="0.839983">
P(st  |st+1, y1:T, u1:T) « P(st  |y1:t, u1:t)P(st+1  |st, ut+1) (14)
</equation>
<bodyText confidence="0.94080525">
To sample the emission distribution φ efficiently and to ensure that each mention is
characterized by a finite set of representative features, in our implementation of the
beam sampling algorithm, we set the base distribution H to be conjugate with the
data distribution F in a Dirichlet-multinomial model with the multinomial parameters
(o1, ... , oK) defined as:
nmk (15)
where nmk counts how many times the feature value fm was assigned in the generative
process to event k, and Bt stores a finite set of feature values for yt as defined in
</bodyText>
<equation confidence="0.952302666666667">
T
E
t=1
ok =
E
f m∈Bt
</equation>
<page confidence="0.993303">
332
</page>
<note confidence="0.789105">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.9981746">
Section 6.1. As can be noticed, the multinomial parameters defined here are finite due to
the fact that each set of feature values Bt is finite and the number of event mentions T is
fixed. This allows us to define a proper emission distribution for the new hybrid model.
In a similar manner to the notations of the mIBP–HDP model, we make notations of the
iFHMM–iHMM model according to the heuristic used for selecting the feature values.
</bodyText>
<sectionHeader confidence="0.924244" genericHeader="evaluation">
7. Evaluation
</sectionHeader>
<bodyText confidence="0.999862666666667">
In this section, we present the evaluation framework of the Bayesian models for
both within-document (WD) and cross-document (CD) coreference resolution. We start
by briefly describing the experimental set-up and coreference evaluation measures,
and then continue by showing the experimental results on the ACE, OntoNotes, and
EventCorefBank data sets. Finally, we conclude with an analysis of the most common
errors made by the Bayesian models.
</bodyText>
<subsectionHeader confidence="0.995883">
7.1 The Experimental Set-up
</subsectionHeader>
<bodyText confidence="0.999904">
In the data processing phase, we extracted the linguistic features described in Section 4
for each event mention annotated in the three data sets. As a result of this phase, in
the ACE corpus, we identified 6,553 event mentions grouped into 4,946 events, and in
the OntoNotes corpus, we identified 11,433 event mentions grouped into 3,393 events.
Likewise, in the new ECB corpus, we distinguished 1,744 event mentions, 1,302 within-
document events, 339 cross-document events, and 43 seminal events (or topics). Table 1
lists additional statistics extracted from these three data sets after performing this phase.
It is also worth mentioning that for processing OntoNotes we devoted additional
efforts. This is because, in spite of the fact that OntoNotes provides coreference
annotations for both entity and event mentions, the annotations from this data set do not
specify which of the mentions refer to entities and which of them refer to events. There-
fore, in order to identify only the event mentions from OntoNotes, we first ran our event
identifier (Bejan 2007) and then marked as event mentions only those mentions anno-
tated in this data set that overlap with the mentions extracted by the event identifier.
</bodyText>
<tableCaption confidence="0.84781">
Table 1
Statistics of the ACE, OntoNotes, and ECB corpora.
</tableCaption>
<table confidence="0.9882685">
ACE OntoNotes ECB
Number of true mentions 6,553 11,433 1,744
Number of system mentions 45,289 81,938 21,175
Number of within-document events 4,946 3,393 1,302
Number of cross-document events – – 339
Number of documents 745 1,540 482
Number of seminal events – – 43
Average number of true mentions/within-document event 1.32 3.37 1.34
Average number of true mentions/document 8.79 7.42 3.62
Average number of true mentions/seminal event – – 40.55
Average number of system mentions/document 60.79 53.2 43.93
Average number of within-document events/document 6.63 2.20 2.70
Average number of within-document events/seminal event – – 30.27
Average number of cross-document events/seminal event – – 7.88
Average number of documents/seminal event – – 11.20
Number of distinct feature values for system mentions 391,798 454,170 237,197
</table>
<page confidence="0.970972">
333
</page>
<note confidence="0.529905">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.999704">
Using this procedure, we marked a number of 4,940 mentions as event mentions
from the total number of 67,500 mentions annotated in OntoNotes. In a second step of
processing OntoNotes, we extended the number of event mentions to 11,433 by marking
all the mentions that share the same cluster with at least one event mention from the set
of 4,940 previously identified event mentions. From the 6,493 event mentions marked in
this step, the majority of them correspond to nouns (4,707) and to the it pronoun (767).
Although only a small subset of event mentions was manually annotated with event
coreference information in the three data sets (also called the set of true or gold event
mentions), during the generative process, we considered all possible event mentions
that are expressed in the data sets for every specific event. We believe this is a more
realistic approach, in spite of the fact that we evaluated only the manually annotated
events. For this purpose, we ran the event identifier described in Bejan (2007) on the
ACE, OntoNotes, and ECB corpora, and extracted 45,289, 81,938, and 21,175 event men-
tions, respectively. It is also worth mentioning that the set of event mentions obtained
from running the event identifier (also called the set of system event mentions) on
ACE and ECB includes more than 98% from the set of true event mentions. In terms of
feature space dimensionality over the two data sets, we performed experiments with a
set of 132 feature types, where each feature type consists, on average, of 6,300 distinct
feature values.
In the evaluation phase, we considered only the true mentions from the ACE test
data set and from the test sets of a five-fold cross validation scheme on the OntoNotes
and ECB data sets. For evaluating the cross-document coreference annotations from
EventCorefBank, we adopted the same approach as described in Bagga and Baldwin
(1999) by merging all the documents from the same topic into a meta-document and
then scoring this document as performed for within-document evaluation. To compute
the final results of our experiments, we averaged the results over five runs of the
generative models.
</bodyText>
<subsectionHeader confidence="0.997539">
7.2 Coreference Resolution Metrics
</subsectionHeader>
<bodyText confidence="0.9999876">
Because there is no agreement on the best coreference resolution metric, we used four
metrics for our evaluation: the link-based MUC metric (Vilain et al. 1995), the mention-
based B3 metric (Bagga and Baldwin 1998), the entity-based CEAF metric (Luo 2005), and
the pairwise (PW) metric. These metrics report results in terms of recall (R), precision
(P), and F-score (F) by comparing the true set of coreference chains T (i.e., the manually
annotated coreference chains) against the set of chains predicted by a coreference res-
olution system S. Here, a coreference link represents a pair of coreferential mentions
whereas a coreference chain represents all the event mentions from the same cluster
with coreference links between consecutive mentions.
The MUC recall computes the number of common coreference links in T and S
divided by the number of links in T, and the MUC precision computes the number of
common links in T and S divided by the number of links in S. As was previously noted
(Luo et al. 2004; Denis and Baldridge 2008; Finkel and Manning 2008), this metric favors
the systems that group mentions into smaller number of clusters (or, in other words,
systems that predict large coreference chains) and does not take into account single
mention clusters. For instance, a system that groups all entity mentions into the same
cluster achieves a MUC score that surpasses any published results of known systems
developed for the task of entity coreference resolution.
The B3 metric was designed to overcome some of the MUC metric’s shortcomings.
This metric computes the recall and precision for each mention and then estimates the
</bodyText>
<page confidence="0.99575">
334
</page>
<note confidence="0.789298">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.999988181818182">
overall score by averaging over all mention scores. For a given mention m, the scorer
compares the true coreference chain that contains the mention m (Tm) against the system
chain that contains the same mention m (Sm). Thus, the recall for m is the ratio of the
number of common elements in Sm and Tm over the number of elements in Tm. Similarly,
the precision corresponding to the mention m is the ratio of the number of common
elements in Sm and Tm over the number of elements in Sm. Because this metric computes
the precision and recall for each mention, it will penalize in precision the systems that
predict a small number of clusters. Because of the same reason, this metric includes
single mention clusters in the evaluation.
The Constrained Entity-Alignment F-Measure (CEAF) scorer finds the best align-
ment between the set of true coreference chains T and the set of predicted coreference
chains S. This is equivalent to finding the best mapping in a weighted bipartite graph.
We computed the weight of a pair of coreference chains (Ti, Sj), with Ti E T and Sj E S,
by using the φ4 similarity measure described in Luo et al. (2004). Therefore, the CEAF
recall and precision measures are computed as the overall similarity score of the best
alignment divided by the self-similarity score of the coreference links in T and S,
respectively.
The last coreference metric that we considered, the PW metric, finds correspon-
dences between all mentions pairs (mi, mj) from the true and system chains with the
coreference chains linking the mentions mi and mj in the system and true chains,
respectively. As can be noticed, this metric overpenalizes those systems that predict too
many or too few clusters when compared with the number of true clusters.
</bodyText>
<subsectionHeader confidence="0.991118">
7.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99799925">
Tables 2, 3, 4, and 5 list the results performed by our proposed baselines (rows 1–2),
by the HDP models (rows 3–8), by the mIBP–HDP model (row 9), and by the iFHMM–
iHMM model (rows 10–13). We discuss the performance achieved by these models in
the remaining part of this section.
</bodyText>
<tableCaption confidence="0.904363">
Table 2
Results for WD coreference resolution on the ACE data set.
</tableCaption>
<table confidence="0.9575226">
Model configuration R MUC F R B3 F R CEAF F R PW F
P P P P
1 BLeclass 94.3 33.1 49.0 97.9 25.0 39.9 14.7 64.4 24.0 93.5 8.2 15.2
2 BLsyn 71.9 29.6 41.5 89.3 36.7 52.1 25.1 64.8 36.2 63.8 10.5 18.1
3 HDP1f (HL) 62.2 43.1 50.9 86.0 70.6 77.5 62.3 76.4 68.6 50.5 27.7 35.8
4 HDPflat (LF) 52.5 51.1 51.8 82.9 82.6 82.7 74.9 75.8 75.3 42.4 41.9 42.1
5 (LF+CF) 48.9 53.1 51.0 82.0 84.9 83.4 77.8 75.3 76.6 37.9 45.1 41.2
6 (LF+CF+WF) 53.8 53.9 53.9 83.3 83.6 83.4 76.3 76.2 76.3 42.2 43.9 43.0
7 (LF+CF+WF+SF) 53.5 54.2 53.9 83.4 84.2 83.8 76.9 76.5 76.7 43.3 47.1 45.1
8 HDPstruct (HL→FR→FEA) 61.9 49.0 54.7 86.2 76.9 81.3 69.0 77.5 73.0 53.2 38.1 44.4
9 mIBP-HDPunfiltered 48.7 41.9 45.1 81.7 76.4 79.0 68.8 73.8 71.2 37.4 28.9 32.6
10 iFHMM-iHMMunfiltered 50.7 52.0 51.4 82.8 83.6 83.2 75.8 75.0 75.4 41.4 42.6 42.0
11 iFHMM-iHMMdiscrete 52.5 50.2 51.3 83.1 81.5 82.3 73.7 75.1 74.4 41.9 40.1 41.0
12 iFHMM-iHMMmedian 52.8 49.6 51.1 83.0 81.3 82.1 73.2 75.2 74.2 40.7 39.0 39.8
13 iFHMM-iHMMuniform 48.7 48.8 48.7 81.9 82.2 82.1 74.6 74.5 74.5 37.2 39.0 38.1
</table>
<page confidence="0.704111">
335
</page>
<note confidence="0.327475">
Computational Linguistics Volume 40, Number 2
</note>
<tableCaption confidence="0.8710275">
Table 3
Results for WD coreference resolution on the OntoNotes data set.
</tableCaption>
<table confidence="0.9180221">
Model configuration R MUC F R B3 F R CEAF F R PW F
P P P P
1 BLeclass 77.6 68.3 72.7 71.2 50.3 58.9 38.1 56.1 45.4 57.1 42.1 47.9
2 BLsyn 54.3 61.8 57.8 52.9 63.2 57.6 50.7 39.5 44.4 30.6 42.5 34.7
3 HDP1f (HL) 78.3 72.2 75.1 77.1 54.1 63.6 40.4 50.4 44.9 67.4 44.0 52.9
4 HDPflat (LF) 70.3 77.6 73.8 81.6 58.3 67.9 40.0 42.6 41.2 81.2 41.5 54.5
5 (LF+CF) 72.1 76.4 74.2 74.8 62.7 68.2 43.4 38.3 40.7 72.8 43.7 54.5
6 (LF+CF+WF) 79.6 77.1 78.2 81.7 57.3 67.1 39.6 43.1 41.1 81.0 41.9 54.6
7 (LF+CF+WF+SF) 72.1 77.6 74.7 74.9 64.0 68.8 39.4 50.7 44.3 74.5 44.1 55.0
8 HDPstruct (HL→FR→FEA) 84.7 78.1 81.2 85.6 55.2 67.1 67.4 37.3 48.0 79.1 40.1 51.4
</table>
<subsubsectionHeader confidence="0.415583">
7.3.1 Baseline Results. A simple baseline for event coreference, which was proposed by
</subsubsectionHeader>
<bodyText confidence="0.921193076923077">
Ahn (2006), consists of grouping event mentions by their event classes (BLeclass). To com-
pute this baseline, we grouped mentions into clusters according to their corresponding
EC feature value. In consequence, this baseline categorizes events into a small number
of clusters, since the event identifier for extracting the EC features is trained to predict
the seven event classes annotated in TimeBank. A second baseline that we implemented
groups two event mentions if there is a (transitive) SYNONYMOUS relation between their
corresponding head lemmas (BLsyn). To implement this baseline, we used the clusters
built over the WordNet SYNONYMOUS relations as described in Section 4. Similarly to
the MUC results reported for entity coreference resolution, the baselines that group event
mentions into very few clusters are overestimated by the MUC metric (e.g., the MUC
F-scores of BLeclass in Table 5).
7.3.2 HDP Results. Due to memory limitations, we evaluated the HDP models on a
restricted set of manually selected feature types. For the HDP1f model, which plays
</bodyText>
<tableCaption confidence="0.9069685">
Table 4
Results for WD coreference resolution on the ECB data set.
</tableCaption>
<table confidence="0.970650333333333">
Model configuration R MUC F R B3 F R CEAF F R PW F
P P P P
1 BLeclass 92.2 39.8 55.6 97.7 55.8 71.0 44.5 80.1 57.2 93.7 25.4 39.8
2 BLsyn 75.0 34.3 47.0 91.5 57.4 70.5 45.7 75.9 57.0 65.3 21.9 32.6
3 HDP1f (HL) 46.9 54.8 50.4 84.3 89.0 86.5 83.4 79.6 81.4 36.6 53.4 42.6
4 HDPflat (LF) 34.7 85.7 49.4 81.4 98.2 89.0 92.7 77.2 84.2 24.7 82.8 37.7
5 (LF+CF) 36.1 83.4 50.1 81.5 98.0 89.0 92.8 77.9 84.7 24.6 80.7 37.4
6 (LF+CF+WF) 38.0 90.2 53.2 82.0 98.9 89.6 93.7 78.4 85.3 26.8 89.9 41.0
7 (LF+CF+WF+SF) 37.8 92.9 53.4 82.1 99.2 89.8 93.9 78.2 85.3 27.0 92.4 41.3
8 HDPstruct (HL→FR→FEA) 47.4 82.7 60.1 84.3 97.1 90.2 92.7 81.1 86.5 34.4 83.0 48.6
9 mIBP-HDPunfiltered 38.2 68.8 48.9 82.1 95.3 88.2 90.3 78.5 84.0 26.5 67.9 37.7
10 iFHMM-iHMMunfiltered 38.9 84.4 52.9 82.6 97.7 89.5 92.7 78.5 85.0 28.5 82.4 41.8
11 iFHMM-iHMMdiscrete 40.2 85.2 54.6 82.6 98.1 89.7 93.2 79.0 85.5 29.7 85.4 44.0
12 iFHMM-iHMMmedian 39.5 84.3 53.6 82.6 97.8 89.5 92.9 78.8 85.3 29.3 83.7 43.0
13 iFHMM-iHMMuniform 39.5 85.2 53.9 82.5 98.1 89.6 93.1 78.8 85.3 29.4 86.6 43.7
</table>
<page confidence="0.957022">
336
</page>
<note confidence="0.879574">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<tableCaption confidence="0.8820025">
Table 5
Results for CD coreference resolution on the ECB data set.
</tableCaption>
<table confidence="0.949542466666667">
Model configuration R MUC F R B3 F R CEAF F R PW F
P P P P
1 BLeclass 90.5 61.1 72.9 93.8 49.6 64.9 36.6 72.7 48.7 90.7 28.6 43.3
2 BLsyn 80.9 55.1 65.5 84.6 48.1 61.3 32.8 63.6 43.3 66.2 26.0 37.3
3 HDP1f (HL) 47.7 70.5 56.8 67.0 86.2 75.3 76.2 57.1 65.2 34.9 58.9 43.5
4 HDPflat (LF) 41.1 90.5 56.5 63.8 97.3 77.0 84.9 54.3 66.1 27.2 88.5 41.5
5 (LF+CF) 43.8 90.7 59.0 64.6 97.3 77.6 85.3 55.6 67.2 27.6 88.7 42.0
6 (LF+CF+WF) 46.2 93.0 61.6 65.8 98.0 78.7 86.7 57.1 68.8 29.6 93.0 44.8
7 (LF+CF+WF+SF) 44.4 95.3 60.5 65.0 98.7 78.3 86.9 56.0 68.0 29.2 95.1 44.4
8 HDPstruct (HL→FR→FEA) 51.9 89.5 65.7 69.3 95.8 80.4 86.2 60.1 70.8 37.5 85.6 52.1
9 mIBP-HDPunfiltered 40.0 79.8 53.2 63.1 94.1 75.5 82.7 54.6 65.7 26.1 77.0 38.9
10 iFHMM-iHMMunfiltered 48.2 89.8 62.7 67.2 96.4 79.1 85.6 58.0 69.1 32.5 87.7 47.2
11 iFHMM-iHMMdiscrete 47.0 88.4 61.3 66.2 96.2 78.4 84.8 57.2 68.3 32.2 88.1 47.1
12 iFHMM-iHMMmedian 48.3 89.6 62.7 67.0 96.5 79.0 86.1 58.3 69.5 33.1 88.1 47.9
13 iFHMM-iHMMuniform 48.4 89.0 62.7 67.0 96.4 79.0 85.5 58.0 69.1 33.3 88.3 48.2
</table>
<bodyText confidence="0.944213642857143">
the role of baseline for the HDPflat and HDPstruct models, we considered HL as the
most representative feature type for performing the clustering of event mentions. In
this configuration, the HDP1f model outperforms the BLeclass and BLsyn baselines. For
the HDPflat models (rows 4–7 in Tables 2–5), we classified the experiments according
to the set of manually selected feature types. We found that the best configuration of
features for this model consists of a combination of feature types from all the categories
of features described in Section 4 (row 7 in Tables 2–5). For the experiments of the
HDPstruct model, we considered the set of features of the best HDPflat experiment as
well as the conditional dependencies between the HL, FR, and FEA feature types.
In general, the HDPflat model achieved the best performance results on the ACE
test data set (the results in Table 2), whereas the HDPstruct model, which also encounters
dependencies between feature types, proved to be more effective on the ECB data set
for both within- and cross-document event coreference evaluation (as shown in Tables 4
and 5). On the OntoNotes data set, as listed in Table 3, HDPflat shows better results
than HDPstruct when considering the B3 and PW metrics, whereas HDPstruct outperforms
HDPflat when considering the MUC and CEAF metrics. Moreover, the results of the
HDPflat and HDPstruct models show an F-score increase by 4–10 percentage points over
the HDP1f model, and therefore prove that the HDP extensions provide a more flexible
representation for clustering objects characterized by rich properties than the original
HDP model.
We also plot the evolution of the generative process associated with an HDP model.
For instance, Figure 7 shows that the HDPflat model corresponding to the experiment
from row 7 in Table 2 converges in 350 iteration steps to a posterior distribution over
event mentions from the ACE corpus with around 2,000 latent events.
7.3.3 mIBP–HDP Results. In spite of its advantage of working with a potentially infinite
number of features in an HDP framework, the mIBP–HDP model (row 9 in Tables 2,
4, and 5) did not achieve a satisfactory performance in comparison with the other pro-
posed models. However, the results were obtained by automatically selecting only 2% of
</bodyText>
<page confidence="0.984891">
337
</page>
<figure confidence="0.95384">
Computational Linguistics Volume 40, Number 2
Number of iterations
</figure>
<figureCaption confidence="0.999136">
Figure 7
</figureCaption>
<bodyText confidence="0.969487275862069">
Evolution of the number of categories and the log of the posterior probability for the
HDPflat model.
distinct feature values from the entire set of values extracted from both corpora. When
compared with the restricted set of features considered by the HDPflat and HDPstruct
models, the percentage of values selected by the mIBP–HDP model is only 6%.
7.3.4 iFHMM–iHMM Results. The results achieved by the iFHMM–iHMM model using
automatic selection of feature values remain competitive against the results of the
HDP models, where the feature types were manually tuned. When comparing the
strategies for filtering feature values in the iFHMM–iHMM framework, we could not
find a distinct separation between the results obtained by the iFHMM–iHMMunfiltered,
iFHMM–iHMMdiscrete, iFHMM–iHMMmedian, and iFHMM–iHMMuniform models. As ob-
served from Tables 2, 4, and 5, most of the iFHMM–iHMM results fall in between the
HDPflat and HDPstruct results. Moreover, the results listed in these tables indicate that the
iFHMM–iHMM model is a better framework than the HDP framework for capturing
the event mention dependencies simulated by the mIBP feature sampling scheme.
A study of the impact on the performance results of the parameter α′ that controls
the number of feature values selected in the iFHMM–iHMM framework is presented
in Figure 8. The results plotted in this figure show a small variation in performance for
different values of α′ indicating that the iFHMM–iHMM model is able to successfully
handle the feature values that introduce additional noise in the data. Figure 8 also
shows that the iFHMM–iHMM model achieves the best results on the ACE data set for a
relative small value of α′ (α′ = 10), which corresponds to 0.05% feature values sampled
from the total number of feature values considered. However, because the number of
event mentions in the ECB corpus is smaller than the number of mentions in the ACE
corpus, the iFHMM–iHMM model utilizes a larger number of features values (0.91% of
feature values selected for α′ = 150) extracted from the new corpus in order to obtain
most of its best results.
The experiments depicted in Figure 8 were performed by using the unfiltered
heuristic for selecting feature values in the iFHMM–iHMM model. Similar results were
</bodyText>
<figure confidence="0.995674227272727">
2200
2000
1800
1600
1400
1200
1000
0 50 100 150 200 250 300 350
Number of iterations
x 105
−2.8
−3
−3.2
−3.4
−3.6
−3.8
−4
−4.2
−4.4
0 50 100 150 200 250 300 350
Number of categories (K)
Log posterior probability
</figure>
<page confidence="0.84592">
338
</page>
<note confidence="0.785299">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<figureCaption confidence="0.931072">
Figure 8
</figureCaption>
<bodyText confidence="0.656827">
Performance results and feature reduction bars of the iFHMM–iHMM models for various α′.
</bodyText>
<page confidence="0.991572">
339
</page>
<note confidence="0.640649">
Computational Linguistics Volume 40, Number 2
</note>
<bodyText confidence="0.9999702">
obtained when considering the rest of the heuristics integrated in the iFHMM–iHMM
framework. Also, the iFHMM–iHMM experiments using the unfiltered, discrete, median,
and uniform heuristics from Tables 2–5 were performed by setting α′ to 10, 100, 100, and
50, respectively. For the parameters γ′ and b′, we considered a default value of 0.5.
To gain a deeper insight into the behavior of the iFHMM–iHMM model, we show
in Figure 9 the performance results obtained by this model for different sets of feature
types. For this purpose, we ran the iFHMM–iHMMuniform model with a fixed value of
the α′ parameter (α′ = 50) on increasing fractions of feature types.9 The results confirm
the fact that the sampling scheme of the feature values used in the iFHMM–iHMM
framework does not guarantee the selection of the most salient features. However, the
constant trend in the performance values shown in Figure 9 proves that iFHMM–iHMM
is a robust generative model for handling noisy and redundant features. For instance,
noisy features for our problem can be generated from errors in semantic parsing, event
class extraction, POS tagging, and disambiguation of polysemous semantic frames.
To strengthen this statement, we also compare in Table 6 the results obtained by an
iFHMM–iHMM model that considers all the feature values associated with an observ-
able object (iFHMM–iHMMall) against the iFHMM–iHMM models that use the mIBP
sampling scheme and the unfiltered, discrete, median, and uniform heuristics. Because
of the memory limitation constraints, we performed the experiments listed in Table 6
by selecting only a subset of feature types from the ones that proved to be salient in
the HDP experiments. As listed in Table 6, all the iFHMM–iHMM models that used a
heuristic approach for selecting feature values significantly outperform the iFHMM–
iHMMall model; therefore, this proves that all the feature selection approaches consid-
ered in the iFHMM–iHMM framework are able to successfully filter out a significant
number of noisy and redundant feature values.
</bodyText>
<subsectionHeader confidence="0.964386">
7.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.965992263157895">
We performed an error analysis by manually inspecting both system and gold-
annotated data in order to track the most common errors made by our models. One
frequent error occurs when a more complex form of semantic inference is needed to
find a correspondence between two event mentions of the same individuated event.
For instance, because all properties and participants of em3(acquisition) are omitted in
Example (1), and no common features exist between em2(buy) and em3(acquisition) to
indicate a similarity between these mentions, they will most probably be assigned to
different clusters. This example also suggests the need for a better modeling of the
discourse salience for event mentions.
Another common error is made when matching the semantic roles corresponding
to coreferential event mentions. Although we simulated entity coreference by using
various semantic features, the task of matching participants and properties associ-
ated with coreferential event mentions is not completely solved. This is because, in
many coreferential cases, partonomic relations between semantic roles need to be in-
ferred.10 Examples of such relations extracted from ECB are Israeli forces PARTOF
−−−−→Israel,
an Indian warship PART OF
−−−−→the Indian navy, his cellPART OF −−−−→Sicilian jail. Similarly for event
properties, many coreferential examples do not specify a clear location and time interval
</bodyText>
<footnote confidence="0.616142333333333">
9 The selection of features into the increasing fractions of feature types was randomly performed. The
fraction corresponding to the 100% experiment in Figure 9 contains all 132 feature types.
10 This observation was also reported in Hasler and Orasan (2009).
</footnote>
<page confidence="0.98487">
340
</page>
<note confidence="0.7047655">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
ACE  |WD ACE  |WD
</note>
<figure confidence="0.998892609375">
1
0.9
0.18 0.19 0.20 0.21 0.21 0.20 0.22 0.22 0.21 0.21
10 20 30 40 50 60 70 80 90 100
Percent of feature types
ECB  |WD
10 20 30 40 50 60 70 80 90 100
Percent of feature types
Percent of feature values
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Percent of feature values
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.23 0.25
0.30 0.31 0.32 0.32 0.34 0.32 0.33 0.32
F1−measure
70
60
50
40
30
90
80
52.74
41.91
MUC B3 CEAF PW
10 20 30 40 50 60 70 80 90 100
83.01
76.04
Percent of feature types
ECB  |WD
70
F1−measure
55.34
45.63
MUC B3 CEAF PW
10 20 30 40 50 60 70 80 90 100
90
80
89.88
85.66
60
50
40
Percent of feature types
ECB  |CD ECB  |CD
Percent of feature types Percent of feature types
</figure>
<figureCaption confidence="0.957032">
Figure 9
</figureCaption>
<bodyText confidence="0.563539">
Performance results and feature reduction bars of the iFHMM–iHMM models for various sets
of features.
</bodyText>
<figure confidence="0.994467529411765">
10 20 30 40 50 60 70 80 90 100
1
90
85
Percent of feature values
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
69.55
79.32
70
65
60
63.31
55
F1−measure
49.32
50
45
MUC B3 CEAF PW
10 20 30 40 50 60 70 80 90 100
80
75
40
35
0.23 0.25
0.30 0.31 0.32 0.32 0.34 0.32 0.33 0.32
</figure>
<page confidence="0.862056">
341
</page>
<note confidence="0.470182">
Computational Linguistics Volume 40, Number 2
</note>
<tableCaption confidence="0.982859">
Table 6
</tableCaption>
<table confidence="0.982962952380953">
Feature non-sampling vs. feature sampling in iFHMM–iHMM models.
Model configuration R MUC F R B3 F R CEAF F R PW F
P P P P
ACE (within-document event coreference)
1 iFHMM-iHMMall 72.4 30.9 43.3 89.3 39.8 55.0 30.2 68.8 42.0 62.7 9.1 15.9
2 iFHMM-iHMMunfiltered 53.5 45.8 49.4 83.3 77.7 80.4 70.6 75.9 73.2 42.1 34.6 38.0
3 iFHMM-iHMMdiscrete 54.3 50.0 52.0 83.8 80.7 82.2 73.0 75.8 74.4 43.9 39.1 41.4
4 iFHMM-iHMMmedian 53.8 48.9 51.2 83.5 80.2 81.8 72.2 75.3 73.7 42.7 38.2 40.3
5 iFHMM-iHMMuniform 51.5 47.8 49.6 82.8 80.7 81.7 72.8 75.2 73.9 41.4 39.3 40.3
ECB (within-document event coreference)
6 iFHMM-iHMMall 64.6 34.0 44.5 89.5 62.5 73.6 53.3 76.5 62.8 60.7 22.9 33.2
7 iFHMM-iHMMunfiltered 40.0 76.9 52.4 82.6 96.6 89.0 92.0 79.1 85.1 28.4 75.6 41.0
8 iFHMM-iHMMdiscrete 41.7 77.2 54.0 83.1 96.7 89.4 91.6 79.2 84.9 30.5 79.0 43.9
9 iFHMM-iHMMmedian 39.0 80.0 52.5 82.5 97.3 89.3 92.8 78.9 85.3 29.2 78.8 42.0
10 iFHMM-iHMMuniform 40.4 73.4 51.8 82.7 96.0 88.9 91.1 79.0 84.6 29.3 74.9 41.6
ECB (cross-document event coreference)
11 iFHMM-iHMMall 70.4 54.7 61.5 79.3 54.4 64.5 43.3 61.3 50.7 59.6 26.2 36.4
12 iFHMM-iHMMunfiltered 49.3 84.3 62.1 67.2 94.5 78.5 84.7 59.2 69.6 32.8 82.5 46.8
13 iFHMM-iHMMdiscrete 48.8 84.8 61.8 67.6 94.8 78.9 83.8 58.3 68.8 34.3 85.3 48.9
14 iFHMM-iHMMmedian 47.6 86.2 61.4 66.7 95.2 78.4 84.5 57.7 68.5 32.2 83.7 46.3
15 iFHMM-iHMMuniform 49.9 82.9 62.2 67.7 93.6 78.4 83.6 59.2 69.2 33.6 79.5 46.9
</table>
<bodyText confidence="0.771374166666667">
(e.g., Jabaliya refugee camp PART OF
−−−−→Gaza, Tuesday PART OF
−−−−→this week). In future work, we plan
to build relevant clusters using partonomies and taxonomies such as the WordNet
hierarchies built from MERONYMY/HOLONYMY and HYPERNYMY/HYPONYMY rela-
tions, respectively.11
</bodyText>
<sectionHeader confidence="0.595235" genericHeader="conclusions">
8. Conclusion
</sectionHeader>
<bodyText confidence="0.9665245">
We have described a new class of unsupervised, nonparametric Bayesian models
designed for the purpose of solving the problem of event coreference resolution. Spe-
cifically, we have shown how already existing models can be extended in order to
relax some of their limitations and how to better represent the event mentions from
a particular document collection. In this regard, we have focused on devising models
for which the number of clusters and the number of feature values corresponding to
event mentions can be automatically inferred from data.
Our experimental results for solving the problem of event coreference proved that
these models are able to successfully handle such types of requirements on a real data
application. Based on these results, we also demonstrated that the new HDP extension,
which is able to model observable objects characterized by multiple properties, is a
better fit for this type of problem than the original HDP model. Moreover, we believe
that the HDP extension can be used for solving clustering problems that involve a small
number of feature types and a priori known facts about the salience of these feature
11 This task is not trivial, because if applying the transitive closure on these relations, all words will end up
being part of the same cluster with entity for instance.
</bodyText>
<page confidence="0.984826">
342
</page>
<note confidence="0.830165">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<bodyText confidence="0.999507285714286">
types. On the other hand, when no such prior information is known with respect to the
number of feature types, or the total number of features is relatively large, we believe
that the iFHMM–iHMM model is a more suitable choice. The main reason is because the
new hybrid model is able to perform an automatic selection of feature values. As shown
in our experiments, this model was capable of achieving competitive results even when
only 2% of feature values were selected from the entire set of features encoded in the
ACE, OntoNotes, and ECB data sets.
</bodyText>
<sectionHeader confidence="0.997548" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98985425">
The authors would like to thank the
anonymous reviewers, whose insightful
comments and suggestions considerably
improved the quality of this article.
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997828989010989">
Ahn, David. 2006. The stages of event
extraction. In Proceedings of the Workshop
on Annotating and Reasoning about Time
and Events, pages 1–8, Sydney.
Allan, James, editor. 2002. Topic Detection
and Tracking: Event-Based Information
Organization. Kluwer Academic
Publishers.
Allan, James, Jaime Carbonell, George
Doddington, Jonathan Yamron, and
Yiming Yang. 1998. Topic detection
and tracking pilot study: Final report.
In Proceedings of the Broadcast News
Understanding and Transcription Workshop,
pages 194–218, Lansdowne, VA.
Bagga, Amit and Breck Baldwin. 1998.
Algorithms for scoring coreference chains.
In Proceedings of the 1st International
Conference on Language Resources and
Evaluation (LREC-1998), pages 563–566,
Granada.
Bagga, Amit and Breck Baldwin. 1999.
Cross-document event coreference:
Annotations, experiments, and
observations. In Proceedings of the ACL
Workshop on Coreference and its Applications,
pages 1–8, College Park, MD.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics (COLING-ACL), pages 86–90,
Montreal.
Beal, Matthew J., Zoubin Ghahramani,
and Carl Edward Rasmussen. 2002.
The infinite hidden Markov model.
In Advances in Neural Information
Processing Systems 14 (NIPS),
pages 577–584, Vancouver.
Bejan, Cosmin Adrian. 2007. Deriving
chronological information from texts
through a graph-based algorithm. In
Proceedings of the 20th Florida Artificial
Intelligence Research Society International
Conference (FLAIRS), Applied Natural
Language Processing Track, pages 259–260,
Key West, FL.
Bejan, Cosmin Adrian. 2008. Unsupervised
discovery of event scenarios from texts.
In Proceedings of the 21st Florida Artificial
Intelligence Research Society International
Conference (FLAIRS), Applied Natural
Language Processing Track, pages 124–129,
Coconut Grove, FL.
Bejan, Cosmin Adrian and Sanda Harabagiu.
2008a. A linguistic resource for
discovering event structures and resolving
event coreference. In Proceedings of the
Sixth International Conference on Language
Resources and Evaluation (LREC),
pages 2,881–2,887, Marrakech.
Bejan, Cosmin Adrian and Sanda Harabagiu.
2008b. Using clustering methods
for discovering event structures.
In Proceedings of the Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1,776–1,777, Chicago, IL.
Bejan, Cosmin Adrian and Sanda Harabagiu.
2010. Unsupervised event coreference
resolution with rich linguistic features.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 1,412–1,422, Uppsala.
Bejan, Cosmin Adrian and Chris Hathaway.
2007. UTD-SRL: A pipeline architecture
for extracting frame semantic structures.
In Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval), pages 460–463, Prague.
Bejan, Cosmin Adrian, Matthew Titsworth,
Andrew Hickl, and Sanda Harabagiu.
2009. Nonparametric Bayesian models for
unsupervised event coreference resolution.
In Advances in Neural Information Processing
Systems 23 (NIPS), pages 73–81, Vancouver.
Blei, David, Andrew Ng, and Michael
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
pages 993–1,022.
</reference>
<page confidence="0.997308">
343
</page>
<note confidence="0.749121">
Computational Linguistics Volume 40, Number 2
</note>
<reference confidence="0.981348559322034">
Boyd-Graber, Jordan, David Blei, and
Xiaojin Zhu. 2007. A topic model for word
sense disambiguation. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 1,024–1,033,
Prague.
Bryant, Michael and Erik B. Sudderth. 2012.
Truly nonparametric online variational
inference for hierarchical Dirichlet
processes. In Advances in Neural
Information Processing Systems 25 (NIPS),
pages 2,708–2,716, Lake Tahoe, NV.
Cardie, Claire and Kiri Wagstaf. 1999.
Noun phrase coreference as clustering.
In Proceedings of the 1999 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 82–89,
College Park, MD.
Chen, Bin, Jian Su, and Chew Lim Tan.
2010a. A twin-candidate based approach
for event pronoun resolution using
composite kernel. In Proceedings of
the 23rd International Conference on
Computational Linguistics (COLING),
pages 188–196, Beijing.
Chen, Bin, Jian Su, and Chew Lim Tan.
2010b. Resolving event noun phrases to
their verbal mentions. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 872–881, Cambridge, MA.
Chen, Zheng and Heng Ji. 2009. Graph-based
event coreference resolution. In Proceedings
of the 2009 Workshop on Graph-based
Methods for Natural Language Processing
(TextGraphs-4), pages 54–57, Singapore.
Chu, Wei, Zoubin Ghahramani, Roland
Krause, and David Wild. 2006. Identifying
protein complexes in high-throughput
protein interaction screens using an
infinite latent feature model. In Pacific
Symposium on Biocomputing (PSB-11),
pages 231–242, Maui, HI.
Cowans, Philip. 2004. Information retrieval
using hierarchical Dirichlet processes.
In Proceedings of the 27th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 564–565, Sheffield.
Daum´e III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
45th Annual Meeting of the Association
of Computational Linguistics (ACL),
pages 256–263, Prague.
Daum´e III, Hal and Daniel Marcu. 2005.
A Bayesian model for supervised
clustering with the Dirichlet process
prior. Journal of Machine Learning Research
(JMLR), 6:1551–1577.
Davidson, Donald, 1969. The individuation
of events, pages 216–234. In N. Rescher
et al., editors, Essays in Honor of Carl G.
Hempel. Dordrecht: Reidel. Reprinted in
D. Davidson, editor, Essays on Actions and
Events. 2001, Oxford: Clarendon Press.
Davidson, Donald, 1985. Reply to Quine on
Events. In E. LePore and B. McLaughlin,
eds., Actions and Events: Perspectives on the
Philosophy of Donald Davidson. Blackwell,
Oxford, pages 172–176.
de Marneffe, Marie-Catherine, Anna N.
Rafferty, and Christopher D. Manning.
2008. Finding contradictions in text. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics:
Human Language Technologies (ACL-HLT),
pages 1,039–1,047, Columbus, OH.
Denis, Pascal and Jason Baldridge. 2008.
Specialized models and ranking for
coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP’08),
pages 660–669, Honolulu, HI.
Doshi-Velez, Finale. 2009. The Indian Buffet
Process: Scalable Inference and Extensions.
Ph.D. thesis, Department of Engineering,
University of Cambridge.
Fellbaum, Christiane. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Ferguson, Thomas S. 1973. A Bayesian
analysis of some nonparametric problems.
The Annals of Statistics, 1(2):209–230.
Finkel, Jenny Rose, Trond Grenager, and
Christopher Manning. 2007. The infinite
tree. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics, pages 272–279, Prague.
Finkel, Jenny Rose and Christopher
Manning. 2008. Enforcing transitivity in
coreference resolution. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies (ACL-HLT), Short Papers,
pages 45–48, Columbus, OH.
Fox, E. B., E. B. Sudderth, and A. S. Willsky.
2007. Hierarchical Dirichlet processes
for tracking maneuvering targets.
In Proceedings of International Conference
on Information Fusion, pages 1,415–1,422,
Quebec.
Geman, Stuart and Donald Geman. 1984.
Stochastic relaxation, Gibbs distributions
and the Bayesian restoration of images.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721–741.
</reference>
<page confidence="0.997977">
344
</page>
<note confidence="0.945036">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<reference confidence="0.995974288135594">
Ghahramani, Zoubin, T. L. Griffiths,
and Peter Sollich, 2007. Bayesian
Nonparametric Latent Feature
Models. In Bayesian Statistics 8, edited
by J. M. Bernardo et al., pages 201–225.
Oxford University Press.
Ghahramani, Zoubin and Michael Jordan.
1997. Factorial hidden Markov models.
Machine Learning, 29:245–273.
Goldwater, Sharon, Thomas L. Griffiths,
and Mark Johnson. 2006. Contextual
dependencies in unsupervised word
segmentation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 673–680, Sydney.
G¨or¨ur, Dilan, Frank J¨akel, and Carl Edward
Rasmussen. 2006. A choice model
with infinitely many latent features.
In Proceedings of the 23rd Annual
International Conference on Machine Learning
(ICML), pages 361–368, Pittsburgh, PA.
Griffiths, Thomas and Mark Steyvers. 2004.
Finding scientific topics. In Proceedings
of the National Academy of Sciences,
pages 5,228–5,235.
Griffiths, Tom and Zoubin Ghahramani. 2005.
Infinite latent feature models and the Indian
buffet process. Technical Report 2005-10,
Gatsby Computational Neuroscience Unit,
University College London.
Griffiths, Tom and Zoubin Ghahramani.
2006. Infinite latent feature models and the
indian buffet process. In Advances in Neural
Information Processing Systems 18 (NIPS),
pages 475–482, Vancouver.
Haghighi, Aria and Dan Klein. 2007.
Unsupervised coreference resolution
in a nonparametric Bayesian model.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 848–855, Prague.
Haghighi, Aria and Dan Klein. 2009. Simple
coreference resolution with rich syntactic
and semantic features. In Proceedings of the
2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 1,152–1,161, Singapore.
Haghighi, Aria and Dan Klein. 2010.
Coreference resolution in a modular,
entity-centered model. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 385–393, Los Angeles, CA.
Haghighi, Aria, Andrew Ng, and
Christopher Manning. 2005. Robust
textual inference via graph matching.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (HLT-EMNLP), pages 387–394,
Vancouver.
Hasler, Laura and Constantin Orasan. 2009.
Do coreferential arguments make event
mentions coreferential? In Proceedings of
the 7th Discourse Anaphora and Anaphor
Resolution Colloquium (DAARC 2009),
pages 151–163, Goa.
He, Tian. 2007. Coreference resolution on
entities and events for hospital discharge
summaries. Master’s thesis, Department
of Electrical Engineering and Computer
Science, Massachusetts Institute of
Technology.
Humphreys, Kevin, Robert Gaizauskas, and
Saliha Azzam. 1997. Event coreference for
information extraction. In Proceedings of the
Workshop on Operational Factors in Practical,
Robust Anaphora Resolution for Unrestricted
Texts, 35th Meeting of ACL, pages 75–81,
Madrid.
Kong, Fang and Guodong Zhou. 2011.
Improve tree kernel-based event pronoun
resolution with competitive information.
In Proceedings of the Twenty-Second
International Joint Conference on Artificial
Intelligence (IJCAI), pages 1,814–1,819,
Barcelona.
LDC-ACE. 2005. ACE (Automatic Content
Extraction) English Annotation Guidelines
for Events, version 5.4.3 2005.07.01. LDC
Catalog Number: LDC2006T06.
LDC-ON. 2007.OntoNotes Release 2.0. LDC
Catalog Number: LDC2008T04.
Lee, Heeyoung, Marta Recasens, Angel
Chang, Mihai Surdeanu, and Dan Jurafsky.
2012. Joint entity and event coreference
resolution across documents. In Proceedings
of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning,
pages 489–500, Jeju Island.
Li, Fei-Fei and Pietro Perona. 2005. A
Bayesian hierarchical model for learning
natural scene categories. In Proceedings of
the 2005 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition
(CVPR) - Volume 2, pages 524–531,
San Diego, CA.
Liang, Percy, Slav Petrov, Michael Jordan,
and Dan Klein. 2007. The infinite PCFG
using hierarchical Dirichlet processes.
In Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP/CoNLL),
pages 688–697, Prague.
</reference>
<page confidence="0.970479">
345
</page>
<reference confidence="0.994895369747899">
Computational Linguistics Volume 40, Number 2
Lowe, John B., Collin F. Baker, and Charles J.
Fillmore. 1997. A frame-semantic approach
to semantic annotation. In Proceedings of the
SIGLEX Workshop on Tagging Text with
Lexical Semantics: Why, What, and How?,
pages 18–24, Washington, DC.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2005), pages 25–32,
Vancouver.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan
Jing, Nanda Kambhatla, and Salim
Roukos. 2004. A mention-synchronous
coreference resolution algorithm based
on the bell tree. In Proceedings of the
42nd Meeting of the Association for
Computational Linguistics (ACL’04),
Main Volume, pages 135–142, Barcelona.
Malpas, Jeff. 2009. Donald Davidson. In the
Stanford Encyclopedia of Philosophy (Fall
2009 Edition), edited by Edward N. Zalta.
Available at http://plato.stanford.edu/
archives/fall2009/entries/davidson/.
Meeds, Edward, Zoubin Ghahramani,
Radford Neal, and Sam Roweis. 2006.
Modeling dyadic data with binary
latent factors. In Advances in Neural
Information Processing Systems 19 (NIPS),
pages 977–984, Vancouver.
Miller, Kurt, Thomas Griffiths, and Michael
Jordan. 2008. The phylogenetic Indian
buffet process: A non-exchangeable
nonparametric prior for latent features.
In Proceedings of the Twenty-Fourth
Conference on Uncertainty in Artificial
Intelligence (UAI), pages 403–410, Helsinki.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings
of the 20th International Conference on
Computational Linguistics (COLING),
pages 693–701, Geneva.
Neal, Radford M. 2003. Slice Sampling.
The Annals of Statistics, 31:705–741.
Ng, Vincent. 2008. Unsupervised models for
coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 640–649, Honolulu, HI.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71–105.
Poon, Hoifung and Pedro Domingos. 2008.
Joint unsupervised coreference resolution
with Markov logic. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 650–659, Honolulu, HI.
Pustejovsky, James, Jose Castano, Bob Ingria,
Roser Sauri, Rob Gaizauskas, Andrea
Setzer, and Graham Katz. 2003a. TimeML:
Robust specification of event and temporal
expressions in text. In Proceedings of
the Fifth International Workshop on
Computational Semantics (IWCS),
pages 337–353, Tilburg.
Pustejovsky, James, Patrick Hanks, Roser
Sauri, Andrew See, Robert Gaizauskas,
Andrea Setzer, Dragomir Radev, Beth
Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TimeBank
Corpus. In Corpus Linguistics,
pages 647–656.
Quine, W. V. O., 1985. Events and Reification.
In E. LePore and B. P. McLaughlin,
editors, Actions and Events: Perspectives
on the Philosophy of Donald Davidson.
Blackwell, Oxford, pages 162–171.
Reprinted in R. Casati and A. C. Varzi,
editors, Events. 1996, Aldershot,
Dartmouth, pages 107–116.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition. In
Proceedings of the IEEE, pages 257–286.
Raghunathan, Karthik, Heeyoung Lee,
Sudarshan Rangarajan, Nate Chambers,
Mihai Surdeanu, Dan Jurafsky, and
Christopher Manning. 2010. A multi-pass
sieve for coreference resolution.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 492–501,
Cambridge, MA.
Rahman, Altaf and Vincent Ng. 2011.
Coreference resolution with world
knowledge. In Proceedings of the 49th
Annual Meeting of the Association for
Computational Linguistics: Human
Language Technologies, pages 814–824,
Portland, OR.
Reisinger, Joseph and Marius Pas¸ca. 2009.
Latent variable models of concept-attribute
attachment. In Proceedings of the Joint
Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 620–628, Singapore.
Sivic, Josef, Bryan Russell, Alexei Efros,
Andrew Zisserman, and William Freeman.
2005. Discovering object categories in
image collections. In Proceedings of the 10th
IEEE International Conference on Computer
Vision (ICCV), pages 370–377, Beijing.
</reference>
<page confidence="0.993822">
346
</page>
<note confidence="0.808643">
Bejan and Harabagiu Unsupervised Event Coreference Resolution
</note>
<reference confidence="0.99779286">
Sivic, Josef, Bryan Russell, Andrew
Zisserman, William Freeman, and
Alexei Efros. 2008. Unsupervised
discovery of visual object class hierarchies.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition,
pages 1–8, Anchorage, AK.
Stoyanov, Veselin, Nathan Gilbert,
Claire Cardie, and Ellen Riloff. 2009.
Conundrums in noun phrase coreference
resolution: Making sense of the state
of the art. In Proceedings of the
Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 656–664,
Singapore.
Sudderth, Erik B., Antonio Torralba,
William T. Freeman, and Alan S. Willsky.
2008. Describing visual scenes using
transformed objects and parts.
International Journal of Computer
Vision, 77:291–330.
Teh, Yee Whye, Michael Jordan, Matthew
Beal, and David Blei. 2006. Hierarchical
Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566–1581.
Van Gael, Jurgen, Y. Saatci, Yee Whye Teh,
and Zoubin Ghahramani. 2008. Beam
sampling for the infinite hidden Markov
model. In Proceedings of the 25th Annual
International Conference on Machine Learning
(ICML), pages 1,088–1,095, Helsinki.
Van Gael, Jurgen, Yee Whye Teh, and Zoubin
Ghahramani. 2008. The infinite factorial
hidden Markov model. In Advances in
Neural Information Processing Systems 21
(NIPS), pages 1697–1704, Vancouver.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of MUC-6,
pages 45–52, Columbia, MD.
Wang, Chong, John Paisley, and David Blei.
2011. Online variational inference for
the hierarchical Dirichlet process.
In Proceedings of the 14th International
Conference on Artificial Intelligence and
Statistics (AISTATS), pages 752–760, Ft.
Lauderdale, FL.
</reference>
<page confidence="0.998527">
347
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416105">
<title confidence="0.997998">Unsupervised Event Coreference Resolution</title>
<author confidence="0.996215">Adrian</author>
<affiliation confidence="0.9971915">Vanderbilt University University of Texas at Dallas</affiliation>
<abstract confidence="0.99916664">The task of event coreference resolution plays a critical role in many natural language processing applications such as information extraction, question answering, and topic detection and tracking. In this article, we describe a new class of unsupervised, nonparametric Bayesian models with the purpose of probabilistically inferring coreference clusters of event mentions from a collection of unlabeled documents. In order to infer these clusters, we automatically extract various lexical, syntactic, and semantic features for each event mention from the document collection. Extracting a rich set of features for each event mention allows us to cast event coreference resolution as the task of grouping together the mentions that share the same features (they have the same participating entities, share the same location, happen at the same time, etc.). Some of the most important challenges posed by the resolution of event coreference in an unsupervised way stem from (a) the choice of representing event mentions through a rich set offeatures and (b) the ability of modeling events described both within the same document and across multiple documents. Our first unsupervised model that addresses these challenges is a generalization of the hierarchical Dirichlet process. This new extension presents the hierarchical Dirichlet process’s ability to capture the uncertainty regarding the number of clustering components and, additionally, takes into account any finite number offeatures associated with each event mention. Furthermore, to overcome some of the limitations of this extension, we devised a new hybrid model, which combines an infinite latent class model with a discrete time series model. The main advantage of this hybrid model stands in its capability to automatically infer the number of features associated with each event mention from data and, at the same time, to perform an automatic selection of the most informative features for the task of event coreference. The evaluation performed for solving both withinand cross-document event coreference shows significant improvements of these models when compared against two baselines for this task.</abstract>
<note confidence="0.810393444444444">of Biomedical Informatics, School of Medicine, Vanderbilt University, 400 Eskind Biomedical 2209 Garland Avenue, Nashville, TN 37232, USA. E-mail: Language Technology Research Institute, Department of Computer Science, University of Texas Dallas, 800 West Campbell Road, Richardson, TX 75080, USA. E-mail: Submission received: 6 February 2012; revised submission received: 9 May 2013; accepted for publication: 28 June 2013. doi:10.1162/COLI a 00174 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>s2: San Diego Chargers receiver Vincent Jackson was [arrested]em11 on suspicion of drunk driving on Tuesday morning, five days before a key NFL playoff game.</title>
<marker></marker>
<rawString>s2: San Diego Chargers receiver Vincent Jackson was [arrested]em11 on suspicion of drunk driving on Tuesday morning, five days before a key NFL playoff game.</rawString>
</citation>
<citation valid="false">
<title>s3: Police [apprehended]em12 Jackson in San Diego at 2:30 a.m. and booked him for the misdemeanor before his release.</title>
<marker></marker>
<rawString>s3: Police [apprehended]em12 Jackson in San Diego at 2:30 a.m. and booked him for the misdemeanor before his release.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ahn</author>
</authors>
<title>The stages of event extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Annotating and Reasoning about Time and Events,</booktitle>
<pages>1--8</pages>
<location>Sydney.</location>
<contexts>
<context position="5139" citStr="Ahn 2006" startWordPosition="754" endWordPosition="755">ovide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in general, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of interest, at least a minimal annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at the topic</context>
<context position="13430" citStr="Ahn 2006" startWordPosition="2020" endWordPosition="2021">ministic models (called sieves) to produce both entity and event clusters that are optimally merged using linear regression. A similar technique that treated entity and event coreference resolution jointly was reported in He (2007) using narrative clinical data. Research that aimed at resolving only event coreference was initiated by the template merging task required in MUC evaluations and was primarily focused on scenario-specific events (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999). More recently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of event coreference resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and Zhou 2011). To the best of our knowledge, the framework for solving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference resolution that u</context>
</contexts>
<marker>Ahn, 2006</marker>
<rawString>Ahn, David. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 1–8, Sydney.</rawString>
</citation>
<citation valid="true">
<title>Topic Detection and Tracking: Event-Based Information Organization.</title>
<date>2002</date>
<editor>Allan, James, editor.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>2002</marker>
<rawString>Allan, James, editor. 2002. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic detection and tracking pilot study: Final report.</title>
<date>1998</date>
<booktitle>In Proceedings of the Broadcast News Understanding and Transcription Workshop,</booktitle>
<pages>194--218</pages>
<location>Lansdowne, VA.</location>
<contexts>
<context position="4270" citStr="Allan et al. 1998" startWordPosition="626" endWordPosition="629">bject (which is well defined in space and time), and therefore, two events are identical if their corresponding objects have the same spatiotemporal location. According to Malpas (2009), in the same year, Davidson abandoned his suggestion to embrace the Quinean theory on event identity (Davidson 1985). Resolving event coreference is an essential requirement for many natural language processing (NLP) applications. For instance, in topic detection and tracking, event coreference resolution is required in order to identify new seminal events in broadcast news that have not been mentioned before (Allan et al. 1998). In information extraction, event coreference information was used for filling predefined template structures from text documents (Humphreys, Gaizauskas, and Azzam 1997). In question answering, a novel method of mapping event structures was used in order to provide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous </context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>Allan, James, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study: Final report. In Proceedings of the Broadcast News Understanding and Transcription Workshop, pages 194–218, Lansdowne, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC-1998),</booktitle>
<pages>563--566</pages>
<location>Granada.</location>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC-1998), pages 563–566, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Cross-document event coreference: Annotations, experiments, and observations.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop on Coreference and its Applications,</booktitle>
<pages>1--8</pages>
<location>College Park, MD.</location>
<contexts>
<context position="5129" citStr="Bagga and Baldwin 1999" startWordPosition="750" endWordPosition="753"> was used in order to provide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in general, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of interest, at least a minimal annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at</context>
<context position="13328" citStr="Bagga and Baldwin 1999" startWordPosition="2006" endWordPosition="2009">er way around. Their supervised method uses a high-precision entity resolution method based on a collection of deterministic models (called sieves) to produce both entity and event clusters that are optimally merged using linear regression. A similar technique that treated entity and event coreference resolution jointly was reported in He (2007) using narrative clinical data. Research that aimed at resolving only event coreference was initiated by the template merging task required in MUC evaluations and was primarily focused on scenario-specific events (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999). More recently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of event coreference resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and Zhou 2011). To the best of our knowledge, the framework for solving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al</context>
</contexts>
<marker>Bagga, Baldwin, 1999</marker>
<rawString>Bagga, Amit and Breck Baldwin. 1999. Cross-document event coreference: Annotations, experiments, and observations. In Proceedings of the ACL Workshop on Coreference and its Applications, pages 1–8, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL),</booktitle>
<pages>86--90</pages>
<location>Montreal.</location>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL), pages 86–90, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
<author>Zoubin Ghahramani</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>The infinite hidden Markov model.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 14 (NIPS),</booktitle>
<pages>577--584</pages>
<location>Vancouver.</location>
<marker>Beal, Ghahramani, Rasmussen, 2002</marker>
<rawString>Beal, Matthew J., Zoubin Ghahramani, and Carl Edward Rasmussen. 2002. The infinite hidden Markov model. In Advances in Neural Information Processing Systems 14 (NIPS), pages 577–584, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
</authors>
<title>Deriving chronological information from texts through a graph-based algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th Florida Artificial Intelligence Research Society International Conference (FLAIRS), Applied Natural Language Processing Track,</booktitle>
<pages>259--260</pages>
<location>Key West, FL.</location>
<marker>Bejan, 2007</marker>
<rawString>Bejan, Cosmin Adrian. 2007. Deriving chronological information from texts through a graph-based algorithm. In Proceedings of the 20th Florida Artificial Intelligence Research Society International Conference (FLAIRS), Applied Natural Language Processing Track, pages 259–260, Key West, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
</authors>
<title>Unsupervised discovery of event scenarios from texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the 21st Florida Artificial Intelligence Research Society International Conference (FLAIRS), Applied Natural Language Processing Track,</booktitle>
<pages>124--129</pages>
<location>Coconut Grove, FL.</location>
<contexts>
<context position="15326" citStr="Bejan 2008" startWordPosition="2311" endWordPosition="2312">opic has assigned a multinomial distribution over the words from the entire document collection. This approach also uses an exchangeability assumption by modeling the documents as bags of words. The LDA model and variations of it have been used in many applications such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared across groups. However, the HDP mixture model is a nonparametri</context>
</contexts>
<marker>Bejan, 2008</marker>
<rawString>Bejan, Cosmin Adrian. 2008. Unsupervised discovery of event scenarios from texts. In Proceedings of the 21st Florida Artificial Intelligence Research Society International Conference (FLAIRS), Applied Natural Language Processing Track, pages 124–129, Coconut Grove, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>A linguistic resource for discovering event structures and resolving event coreference.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>2--881</pages>
<location>Marrakech.</location>
<contexts>
<context position="15352" citStr="Bejan and Harabagiu 2008" startWordPosition="2313" endWordPosition="2316">igned a multinomial distribution over the words from the entire document collection. This approach also uses an exchangeability assumption by modeling the documents as bags of words. The LDA model and variations of it have been used in many applications such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared across groups. However, the HDP mixture model is a nonparametric generalization of LDA th</context>
<context position="25469" citStr="Bejan and Harabagiu (2008" startWordPosition="3870" endWordPosition="3873">vent for each of these topics. In a subsequent step, for every Web document, we automatically tokenized and split the textual content into sentences, and saved the preprocessed data in a uniquely identified text file. Next, we manually annotated a limited set of events in each text file in accordance with the TimeML specification (Pustejovsky et al. 2003a). To mark the event mentions and the coreferential relations between them we utilized the Callisto4 and Tango5 annotation tools, respectively. Additional details regarding the annotation process for creating the ECB resource are described in Bejan and Harabagiu (2008a). Several annotation fragments from ECB are shown in Example (1). In this example, event mentions are annotated at the sentence level, sentences are grouped into documents, and the documents describing the same seminal event are organized into topics. The topics shown in Example (1) describe the seminal event of arresting sea pirates by a 1 The ECB corpus is available at http://www.hlt.utdallas.edu/∼ady/data/ECB1.0.tar.gz. 2 A seminal event in a document is the event that triggers the topic of the document and has interconnections with the majority of events from its surrounding textual cont</context>
</contexts>
<marker>Bejan, Harabagiu, 2008</marker>
<rawString>Bejan, Cosmin Adrian and Sanda Harabagiu. 2008a. A linguistic resource for discovering event structures and resolving event coreference. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC), pages 2,881–2,887, Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Using clustering methods for discovering event structures.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI),</booktitle>
<pages>1--776</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="15352" citStr="Bejan and Harabagiu 2008" startWordPosition="2313" endWordPosition="2316">igned a multinomial distribution over the words from the entire document collection. This approach also uses an exchangeability assumption by modeling the documents as bags of words. The LDA model and variations of it have been used in many applications such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared across groups. However, the HDP mixture model is a nonparametric generalization of LDA th</context>
<context position="25469" citStr="Bejan and Harabagiu (2008" startWordPosition="3870" endWordPosition="3873">vent for each of these topics. In a subsequent step, for every Web document, we automatically tokenized and split the textual content into sentences, and saved the preprocessed data in a uniquely identified text file. Next, we manually annotated a limited set of events in each text file in accordance with the TimeML specification (Pustejovsky et al. 2003a). To mark the event mentions and the coreferential relations between them we utilized the Callisto4 and Tango5 annotation tools, respectively. Additional details regarding the annotation process for creating the ECB resource are described in Bejan and Harabagiu (2008a). Several annotation fragments from ECB are shown in Example (1). In this example, event mentions are annotated at the sentence level, sentences are grouped into documents, and the documents describing the same seminal event are organized into topics. The topics shown in Example (1) describe the seminal event of arresting sea pirates by a 1 The ECB corpus is available at http://www.hlt.utdallas.edu/∼ady/data/ECB1.0.tar.gz. 2 A seminal event in a document is the event that triggers the topic of the document and has interconnections with the majority of events from its surrounding textual cont</context>
</contexts>
<marker>Bejan, Harabagiu, 2008</marker>
<rawString>Bejan, Cosmin Adrian and Sanda Harabagiu. 2008b. Using clustering methods for discovering event structures. In Proceedings of the Association for the Advancement of Artificial Intelligence (AAAI), pages 1,776–1,777, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Unsupervised event coreference resolution with rich linguistic features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--412</pages>
<location>Uppsala.</location>
<contexts>
<context position="13961" citStr="Bejan and Harabagiu 2010" startWordPosition="2104" endWordPosition="2107">ecently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of event coreference resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and Zhou 2011). To the best of our knowledge, the framework for solving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference resolution that uses fully unsupervised methods and is based on Bayesian models. Over the past years, Bayesian models have been extensively used for the purpose of solving similar problems or subproblems of the generic problem presented in the previous section. In 2003, Blei, Ng, and Jordan proposed a parametric approach, called latent Dirichlet allocation (LDA), for automatically learning probability distributions of words corresponding to a specific number of latent classes (or topics) from a large 314 Bejan and Harabagiu Unsupervised Event</context>
<context position="16979" citStr="Bejan and Harabagiu 2010" startWordPosition="2573" endWordPosition="2576">etric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of multiple features. For example, in HDP, each data point is represented only by its corresp</context>
<context position="20254" citStr="Bejan and Harabagiu 2010" startWordPosition="3070" endWordPosition="3073"> a binary latent feature that evolves over time according to Markov dynamics. Also, the phylogenetic Indian buffet process (pIBP) (Miller, Griffiths, and Jordan 2008) was created as a non-exchangeable, nonparametric prior for latent feature models, where the dependencies between objects were expressed as tree structures. Examples of applications that utilized these models are: identification of protein complexes (Chu et al. 2006), modeling of dyadic data (Meeds et al. 2006), modeling of choice behavior (G¨or¨ur, J¨akel, and Rasmussen 2006), and event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). Our extension of the HDP model still does not fulfill all the desiderata for the generic problem introduced in Section 1. It still requires a mechanism to automatically select a finite set of salient features that will be used in the clustering process (third desideratum) as well as a mechanism for capturing the structural dependencies between objects (fourth desideratum). To overcome these limitations, we created two additional models. First, we incorporated the mIBP framework into our HDP extension to create the mIBP–HDP model. And second, we coupled an infinite latent feature model with a</context>
<context position="21516" citStr="Bejan and Harabagiu 2010" startWordPosition="3268" endWordPosition="3271">new discrete time series model. For the infinite latent feature model, we chose the infinite factorial hidden Markov model (iFHMM) (Van Gael, Teh, and Ghahramani 2008) coupled with the mIBP mechanism in order to represent the latent features as an infinite set of parallel Markov chains; for the infinite latent class model, we chose the infinite hidden Markov model (iHMM) (Beal, Ghahramani, and Rasmussen 2002). We call this new hybrid the iFHMM–iHMM model. 2.1 Contribution This article represents an extension of our previous work on unsupervised event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). In this work, we present more details on the problem of solving both within- and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents). In the next section, we provide additional information on how we performed the annotation of this corpus. Another major contribution of this article is an extended description of the unsupervised mode</context>
</contexts>
<marker>Bejan, Harabagiu, 2010</marker>
<rawString>Bejan, Cosmin Adrian and Sanda Harabagiu. 2010. Unsupervised event coreference resolution with rich linguistic features. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1,412–1,422, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Chris Hathaway</author>
</authors>
<title>UTD-SRL: A pipeline architecture for extracting frame semantic structures.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval),</booktitle>
<pages>460--463</pages>
<location>Prague.</location>
<marker>Bejan, Hathaway, 2007</marker>
<rawString>Bejan, Cosmin Adrian and Chris Hathaway. 2007. UTD-SRL: A pipeline architecture for extracting frame semantic structures. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval), pages 460–463, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
<author>Matthew Titsworth</author>
<author>Andrew Hickl</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Nonparametric Bayesian models for unsupervised event coreference resolution.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 23 (NIPS),</booktitle>
<pages>73--81</pages>
<location>Vancouver.</location>
<contexts>
<context position="13934" citStr="Bejan et al. 2009" startWordPosition="2100" endWordPosition="2103">ldwin 1999). More recently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of event coreference resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and Zhou 2011). To the best of our knowledge, the framework for solving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference resolution that uses fully unsupervised methods and is based on Bayesian models. Over the past years, Bayesian models have been extensively used for the purpose of solving similar problems or subproblems of the generic problem presented in the previous section. In 2003, Blei, Ng, and Jordan proposed a parametric approach, called latent Dirichlet allocation (LDA), for automatically learning probability distributions of words corresponding to a specific number of latent classes (or topics) from a large 314 Bejan and H</context>
<context position="16952" citStr="Bejan et al. 2009" startWordPosition="2569" endWordPosition="2572">provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of multiple features. For example, in HDP, each data point is repr</context>
<context position="20227" citStr="Bejan et al. 2009" startWordPosition="3066" endWordPosition="3069"> be associated with a binary latent feature that evolves over time according to Markov dynamics. Also, the phylogenetic Indian buffet process (pIBP) (Miller, Griffiths, and Jordan 2008) was created as a non-exchangeable, nonparametric prior for latent feature models, where the dependencies between objects were expressed as tree structures. Examples of applications that utilized these models are: identification of protein complexes (Chu et al. 2006), modeling of dyadic data (Meeds et al. 2006), modeling of choice behavior (G¨or¨ur, J¨akel, and Rasmussen 2006), and event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). Our extension of the HDP model still does not fulfill all the desiderata for the generic problem introduced in Section 1. It still requires a mechanism to automatically select a finite set of salient features that will be used in the clustering process (third desideratum) as well as a mechanism for capturing the structural dependencies between objects (fourth desideratum). To overcome these limitations, we created two additional models. First, we incorporated the mIBP framework into our HDP extension to create the mIBP–HDP model. And second, we coupled an infinite </context>
<context position="21489" citStr="Bejan et al. 2009" startWordPosition="3264" endWordPosition="3267">class model into a new discrete time series model. For the infinite latent feature model, we chose the infinite factorial hidden Markov model (iFHMM) (Van Gael, Teh, and Ghahramani 2008) coupled with the mIBP mechanism in order to represent the latent features as an infinite set of parallel Markov chains; for the infinite latent class model, we chose the infinite hidden Markov model (iHMM) (Beal, Ghahramani, and Rasmussen 2002). We call this new hybrid the iFHMM–iHMM model. 2.1 Contribution This article represents an extension of our previous work on unsupervised event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). In this work, we present more details on the problem of solving both within- and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents). In the next section, we provide additional information on how we performed the annotation of this corpus. Another major contribution of this article is an extended descripti</context>
</contexts>
<marker>Bejan, Titsworth, Hickl, Harabagiu, 2009</marker>
<rawString>Bejan, Cosmin Adrian, Matthew Titsworth, Andrew Hickl, and Sanda Harabagiu. 2009. Nonparametric Bayesian models for unsupervised event coreference resolution. In Advances in Neural Information Processing Systems 23 (NIPS), pages 73–81, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>993--1</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David, Andrew Ng, and Michael Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, pages 993–1,022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1--024</pages>
<location>Prague.</location>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1,024–1,033, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bryant</author>
<author>Erik B Sudderth</author>
</authors>
<title>Truly nonparametric online variational inference for hierarchical Dirichlet processes.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25 (NIPS),</booktitle>
<pages>2--708</pages>
<location>Lake Tahoe, NV.</location>
<contexts>
<context position="16631" citStr="Bryant and Sudderth 2012" startWordPosition="2522" endWordPosition="2525">lustering components K (the first desideratum for our problem). It consists of a set of Dirichlet processes (DPs) (Ferguson 1973), in which each DP is associated with a group of data. In addition, these DPs are coupled through a common random base measure which is itself distributed according to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatical</context>
</contexts>
<marker>Bryant, Sudderth, 2012</marker>
<rawString>Bryant, Michael and Erik B. Sudderth. 2012. Truly nonparametric online variational inference for hierarchical Dirichlet processes. In Advances in Neural Information Processing Systems 25 (NIPS), pages 2,708–2,716, Lake Tahoe, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Kiri Wagstaf</author>
</authors>
<title>Noun phrase coreference as clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>82--89</pages>
<location>College Park, MD.</location>
<contexts>
<context position="11808" citStr="Cardie and Wagstaf 1999" startWordPosition="1778" endWordPosition="1781">ionale is that events are expressed in many more varied linguistic constructs. For example, event mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related info</context>
</contexts>
<marker>Cardie, Wagstaf, 1999</marker>
<rawString>Cardie, Claire and Kiri Wagstaf. 1999. Noun phrase coreference as clustering. In Proceedings of the 1999 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 82–89, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Chen</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>A twin-candidate based approach for event pronoun resolution using composite kernel.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>188--196</pages>
<location>Beijing.</location>
<marker>Chen, Su, Tan, 2010</marker>
<rawString>Chen, Bin, Jian Su, and Chew Lim Tan. 2010a. A twin-candidate based approach for event pronoun resolution using composite kernel. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 188–196, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Chen</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Resolving event noun phrases to their verbal mentions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>872--881</pages>
<location>Cambridge, MA.</location>
<marker>Chen, Su, Tan, 2010</marker>
<rawString>Chen, Bin, Jian Su, and Chew Lim Tan. 2010b. Resolving event noun phrases to their verbal mentions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 872–881, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Chen</author>
<author>Heng Ji</author>
</authors>
<title>Graph-based event coreference resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing (TextGraphs-4),</booktitle>
<pages>54--57</pages>
<contexts>
<context position="5157" citStr="Chen and Ji 2009" startWordPosition="756" endWordPosition="759">er justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite of being successful for a particular labeled corpus, in general, these pairwise models are dependent on the domain or language that they are trained on. For instance, in order to adapt a supervised system to run over a collection of documents written in a different language or belonging to a different domain of interest, at least a minimal annotation effort needs to be performed (Daum´e III 2007). Furthermore, because these models are dependent on local pairwise decisions, they are unable to capture a global event distribution at the topic- or document-coll</context>
<context position="13476" citStr="Chen and Ji 2009" startWordPosition="2025" endWordPosition="2028">duce both entity and event clusters that are optimally merged using linear regression. A similar technique that treated entity and event coreference resolution jointly was reported in He (2007) using narrative clinical data. Research that aimed at resolving only event coreference was initiated by the template merging task required in MUC evaluations and was primarily focused on scenario-specific events (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999). More recently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of event coreference resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and Zhou 2011). To the best of our knowledge, the framework for solving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference resolution that uses fully unsupervised methods and is based on</context>
</contexts>
<marker>Chen, Ji, 2009</marker>
<rawString>Chen, Zheng and Heng Ji. 2009. Graph-based event coreference resolution. In Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing (TextGraphs-4), pages 54–57, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Chu</author>
<author>Zoubin Ghahramani</author>
<author>Roland Krause</author>
<author>David Wild</author>
</authors>
<title>Identifying protein complexes in high-throughput protein interaction screens using an infinite latent feature model.</title>
<date>2006</date>
<booktitle>In Pacific Symposium on Biocomputing (PSB-11),</booktitle>
<pages>231--242</pages>
<location>Maui, HI.</location>
<contexts>
<context position="20062" citStr="Chu et al. 2006" startWordPosition="3041" endWordPosition="3044"> Indian buffet process (mIBP) (Van Gael, Teh, and Ghahramani 2008) was defined as a distribution over an unbounded set of binary Markov chains, where each chain can be associated with a binary latent feature that evolves over time according to Markov dynamics. Also, the phylogenetic Indian buffet process (pIBP) (Miller, Griffiths, and Jordan 2008) was created as a non-exchangeable, nonparametric prior for latent feature models, where the dependencies between objects were expressed as tree structures. Examples of applications that utilized these models are: identification of protein complexes (Chu et al. 2006), modeling of dyadic data (Meeds et al. 2006), modeling of choice behavior (G¨or¨ur, J¨akel, and Rasmussen 2006), and event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). Our extension of the HDP model still does not fulfill all the desiderata for the generic problem introduced in Section 1. It still requires a mechanism to automatically select a finite set of salient features that will be used in the clustering process (third desideratum) as well as a mechanism for capturing the structural dependencies between objects (fourth desideratum). To overcome these limitations,</context>
</contexts>
<marker>Chu, Ghahramani, Krause, Wild, 2006</marker>
<rawString>Chu, Wei, Zoubin Ghahramani, Roland Krause, and David Wild. 2006. Identifying protein complexes in high-throughput protein interaction screens using an infinite latent feature model. In Pacific Symposium on Biocomputing (PSB-11), pages 231–242, Maui, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Cowans</author>
</authors>
<title>Information retrieval using hierarchical Dirichlet processes.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>564--565</pages>
<location>Sheffield.</location>
<contexts>
<context position="16837" citStr="Cowans 2004" startWordPosition="2555" endWordPosition="2556">through a common random base measure which is itself distributed according to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each obser</context>
</contexts>
<marker>Cowans, 2004</marker>
<rawString>Cowans, Philip. 2004. Information retrieval using hierarchical Dirichlet processes. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 564–565, Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>256--263</pages>
<location>Prague.</location>
<marker>Hal, 2007</marker>
<rawString>Daum´e III, Hal. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), pages 256–263, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daum´e Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A Bayesian model for supervised clustering with the Dirichlet process prior.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>6--1551</pages>
<marker>Hal, Marcu, 2005</marker>
<rawString>Daum´e III, Hal and Daniel Marcu. 2005. A Bayesian model for supervised clustering with the Dirichlet process prior. Journal of Machine Learning Research (JMLR), 6:1551–1577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>The individuation of events,</title>
<date>1969</date>
<booktitle>Essays in Honor of Carl</booktitle>
<pages>216--234</pages>
<editor>In N. Rescher et al., editors,</editor>
<publisher>Clarendon Press.</publisher>
<location>Oxford:</location>
<contexts>
<context position="3446" citStr="Davidson (1969)" startWordPosition="501" endWordPosition="502">I a 00174 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 1. Introduction Event coreference resolution consists of grouping together the text expressions that refer to real-world events (also called event mentions) into a set of clusters such that all the mentions from the same cluster correspond to a unique event. The problem of event coreference is not new. It was originally studied in philosophy, where researchers tried to determine when two events are identical and when they are different. One relevant theory in this direction was proposed by Davidson (1969), who argued that two events are identical if they have the same causes and effects. Later on, a different theory was proposed by Quine (1985), who considered that each event is associated with a physical object (which is well defined in space and time), and therefore, two events are identical if their corresponding objects have the same spatiotemporal location. According to Malpas (2009), in the same year, Davidson abandoned his suggestion to embrace the Quinean theory on event identity (Davidson 1985). Resolving event coreference is an essential requirement for many natural language processi</context>
</contexts>
<marker>Davidson, 1969</marker>
<rawString>Davidson, Donald, 1969. The individuation of events, pages 216–234. In N. Rescher et al., editors, Essays in Honor of Carl G. Hempel. Dordrecht: Reidel. Reprinted in D. Davidson, editor, Essays on Actions and Events. 2001, Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>Reply to Quine on Events.</title>
<date>1985</date>
<booktitle>Actions and Events: Perspectives on the Philosophy of Donald Davidson.</booktitle>
<pages>172--176</pages>
<editor>In E. LePore and B. McLaughlin, eds.,</editor>
<publisher>Blackwell,</publisher>
<location>Oxford,</location>
<contexts>
<context position="3954" citStr="Davidson 1985" startWordPosition="582" endWordPosition="583">entical and when they are different. One relevant theory in this direction was proposed by Davidson (1969), who argued that two events are identical if they have the same causes and effects. Later on, a different theory was proposed by Quine (1985), who considered that each event is associated with a physical object (which is well defined in space and time), and therefore, two events are identical if their corresponding objects have the same spatiotemporal location. According to Malpas (2009), in the same year, Davidson abandoned his suggestion to embrace the Quinean theory on event identity (Davidson 1985). Resolving event coreference is an essential requirement for many natural language processing (NLP) applications. For instance, in topic detection and tracking, event coreference resolution is required in order to identify new seminal events in broadcast news that have not been mentioned before (Allan et al. 1998). In information extraction, event coreference information was used for filling predefined template structures from text documents (Humphreys, Gaizauskas, and Azzam 1997). In question answering, a novel method of mapping event structures was used in order to provide answer justificat</context>
</contexts>
<marker>Davidson, 1985</marker>
<rawString>Davidson, Donald, 1985. Reply to Quine on Events. In E. LePore and B. McLaughlin, eds., Actions and Events: Perspectives on the Philosophy of Donald Davidson. Blackwell, Oxford, pages 172–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Finding contradictions in text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT),</booktitle>
<pages>1--039</pages>
<location>Columbus, OH.</location>
<marker>de Marneffe, Rafferty, Manning, 2008</marker>
<rawString>de Marneffe, Marie-Catherine, Anna N. Rafferty, and Christopher D. Manning. 2008. Finding contradictions in text. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 1,039–1,047, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP’08),</booktitle>
<pages>660--669</pages>
<location>Honolulu, HI.</location>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Denis, Pascal and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP’08), pages 660–669, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finale Doshi-Velez</author>
</authors>
<title>The Indian Buffet Process: Scalable Inference and Extensions.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Engineering, University of Cambridge.</institution>
<marker>Doshi-Velez, 2009</marker>
<rawString>Doshi-Velez, Finale. 2009. The Indian Buffet Process: Scalable Inference and Extensions. Ph.D. thesis, Department of Engineering, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>The Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="16135" citStr="Ferguson 1973" startWordPosition="2438" endWordPosition="2439">dge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared across groups. However, the HDP mixture model is a nonparametric generalization of LDA that is also able to automatically infer the number of clustering components K (the first desideratum for our problem). It consists of a set of Dirichlet processes (DPs) (Ferguson 1973), in which each DP is associated with a group of data. In addition, these DPs are coupled through a common random base measure which is itself distributed according to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Ferguson, Thomas S. 1973. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>The infinite tree.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>272--279</pages>
<location>Prague.</location>
<marker>Finkel, Grenager, Manning, 2007</marker>
<rawString>Finkel, Jenny Rose, Trond Grenager, and Christopher Manning. 2007. The infinite tree. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 272–279, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), Short Papers,</booktitle>
<pages>45--48</pages>
<location>Columbus, OH.</location>
<marker>Finkel, Manning, 2008</marker>
<rawString>Finkel, Jenny Rose and Christopher Manning. 2008. Enforcing transitivity in coreference resolution. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), Short Papers, pages 45–48, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E B Fox</author>
<author>E B Sudderth</author>
<author>A S Willsky</author>
</authors>
<title>Hierarchical Dirichlet processes for tracking maneuvering targets.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on Information Fusion,</booktitle>
<pages>1--415</pages>
<location>Quebec.</location>
<marker>Fox, Sudderth, Willsky, 2007</marker>
<rawString>Fox, E. B., E. B. Sudderth, and A. S. Willsky. 2007. Hierarchical Dirichlet processes for tracking maneuvering targets. In Proceedings of International Conference on Information Fusion, pages 1,415–1,422, Quebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<marker>Geman, Geman, 1984</marker>
<rawString>Geman, Stuart and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>T L Griffiths</author>
<author>Peter Sollich</author>
</authors>
<title>Bayesian Nonparametric Latent Feature Models. In Bayesian Statistics 8, edited by</title>
<date>2007</date>
<pages>201--225</pages>
<publisher>Oxford University Press.</publisher>
<marker>Ghahramani, Griffiths, Sollich, 2007</marker>
<rawString>Ghahramani, Zoubin, T. L. Griffiths, and Peter Sollich, 2007. Bayesian Nonparametric Latent Feature Models. In Bayesian Statistics 8, edited by J. M. Bernardo et al., pages 201–225. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael Jordan</author>
</authors>
<date>1997</date>
<booktitle>Factorial hidden Markov models. Machine Learning,</booktitle>
<pages>29--245</pages>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Ghahramani, Zoubin and Michael Jordan. 1997. Factorial hidden Markov models. Machine Learning, 29:245–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<location>Sydney.</location>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Goldwater, Sharon, Thomas L. Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673–680, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilan G¨or¨ur</author>
<author>Frank J¨akel</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>A choice model with infinitely many latent features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd Annual International Conference on Machine Learning (ICML),</booktitle>
<pages>361--368</pages>
<location>Pittsburgh, PA.</location>
<marker>G¨or¨ur, J¨akel, Rasmussen, 2006</marker>
<rawString>G¨or¨ur, Dilan, Frank J¨akel, and Carl Edward Rasmussen. 2006. A choice model with infinitely many latent features. In Proceedings of the 23rd Annual International Conference on Machine Learning (ICML), pages 361–368, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In Proceedings of the National Academy of Sciences,</booktitle>
<pages>5--228</pages>
<contexts>
<context position="15061" citStr="Griffiths and Steyvers 2004" startWordPosition="2271" endWordPosition="2274">f words corresponding to a specific number of latent classes (or topics) from a large 314 Bejan and Harabagiu Unsupervised Event Coreference Resolution collection of text documents. In this latent class model, documents are expressed as probabilistic mixtures of topics, while each topic has assigned a multinomial distribution over the words from the entire document collection. This approach also uses an exchangeability assumption by modeling the documents as bags of words. The LDA model and variations of it have been used in many applications such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) mod</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Griffiths, Thomas and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy of Sciences, pages 5,228–5,235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Infinite latent feature models and the Indian buffet process.</title>
<date>2005</date>
<tech>Technical Report 2005-10,</tech>
<institution>Gatsby Computational Neuroscience Unit, University College London.</institution>
<contexts>
<context position="19134" citStr="Griffiths and Ghahramani (2005)" startWordPosition="2901" endWordPosition="2904">e task of entity coreference resolution. As also noted in Ng (2008) and Poon and Domingos (2008), whenever new features need to be considered in Haghighi and Klein’s model, the extension becomes a challenging task. Also, Daum´e III and Marcu (2005) performed 315 Computational Linguistics Volume 40, Number 2 related work in this direction by proposing a generative model for solving supervised clustering problems. As an alternative to the HDP model, an important extension of latent class models that are able to represent feature-rich objects is the Indian buffet process (IBP) model presented in Griffiths and Ghahramani (2005). The IBP model defines a distribution over infinite binary sparse matrices that can be used as a nonparametric prior on the features associated with observable objects. Moreover, extensions of this model were considered in order to provide a more flexible approach for modeling the data. For example, the Markov Indian buffet process (mIBP) (Van Gael, Teh, and Ghahramani 2008) was defined as a distribution over an unbounded set of binary Markov chains, where each chain can be associated with a binary latent feature that evolves over time according to Markov dynamics. Also, the phylogenetic Indi</context>
</contexts>
<marker>Griffiths, Ghahramani, 2005</marker>
<rawString>Griffiths, Tom and Zoubin Ghahramani. 2005. Infinite latent feature models and the Indian buffet process. Technical Report 2005-10, Gatsby Computational Neuroscience Unit, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Griffiths</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Infinite latent feature models and the indian buffet process.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18 (NIPS),</booktitle>
<pages>475--482</pages>
<location>Vancouver.</location>
<marker>Griffiths, Ghahramani, 2006</marker>
<rawString>Griffiths, Tom and Zoubin Ghahramani. 2006. Infinite latent feature models and the indian buffet process. In Advances in Neural Information Processing Systems 18 (NIPS), pages 475–482, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric Bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>848--855</pages>
<location>Prague.</location>
<contexts>
<context position="16893" citStr="Haghighi and Klein 2007" startWordPosition="2560" endWordPosition="2563"> itself distributed according to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of mult</context>
<context position="18435" citStr="Haghighi and Klein (2007)" startWordPosition="2794" endWordPosition="2797">into account additional linguistic features associated with event mentions. This extension is performed by using a conditional independence assumption between the observed random variables corresponding to object features. Thus, instead of considering as features only the words that express the event mentions (which is the way an observable object is represented in the original HDP model), we devised an HDP extension that is also able to represent features such as location, time, and agent for each event mention. This extension was inspired from the fully generative Bayesian model proposed by Haghighi and Klein (2007). However, Haghighi and Klein’s model was strictly customized for the task of entity coreference resolution. As also noted in Ng (2008) and Poon and Domingos (2008), whenever new features need to be considered in Haghighi and Klein’s model, the extension becomes a challenging task. Also, Daum´e III and Marcu (2005) performed 315 Computational Linguistics Volume 40, Number 2 related work in this direction by proposing a generative model for solving supervised clustering problems. As an alternative to the HDP model, an important extension of latent class models that are able to represent feature</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Haghighi, Aria and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric Bayesian model. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 848–855, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1--152</pages>
<contexts>
<context position="11833" citStr="Haghighi and Klein 2009" startWordPosition="1782" endWordPosition="1785"> expressed in many more varied linguistic constructs. For example, event mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at whi</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Haghighi, Aria and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1,152–1,161, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>385--393</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="11880" citStr="Haghighi and Klein 2010" startWordPosition="1790" endWordPosition="1793">ructs. For example, event mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the entity mentions can have a</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Haghighi, Aria and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 385–393, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Haghighi, Aria, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP),</booktitle>
<pages>387--394</pages>
<location>Vancouver.</location>
<marker></marker>
<rawString>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP), pages 387–394, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Hasler</author>
<author>Constantin Orasan</author>
</authors>
<title>Do coreferential arguments make event mentions coreferential?</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC</booktitle>
<pages>151--163</pages>
<location>Goa.</location>
<marker>Hasler, Orasan, 2009</marker>
<rawString>Hasler, Laura and Constantin Orasan. 2009. Do coreferential arguments make event mentions coreferential? In Proceedings of the 7th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 2009), pages 151–163, Goa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tian He</author>
</authors>
<title>Coreference resolution on entities and events for hospital discharge summaries.</title>
<date>2007</date>
<tech>Master’s thesis,</tech>
<institution>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology.</institution>
<contexts>
<context position="13052" citStr="He (2007)" startWordPosition="1969" endWordPosition="1970">c role the entity mentions can have and the verb pairs of their predicates. More recently, Lee et al. (2012) proposed an approach to jointly model event and entity coreference by allowing information from event coreference to help entity coreference, and the other way around. Their supervised method uses a high-precision entity resolution method based on a collection of deterministic models (called sieves) to produce both entity and event clusters that are optimally merged using linear regression. A similar technique that treated entity and event coreference resolution jointly was reported in He (2007) using narrative clinical data. Research that aimed at resolving only event coreference was initiated by the template merging task required in MUC evaluations and was primarily focused on scenario-specific events (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999). More recently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of e</context>
</contexts>
<marker>He, 2007</marker>
<rawString>He, Tian. 2007. Coreference resolution on entities and events for hospital discharge summaries. Master’s thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Humphreys</author>
<author>Robert Gaizauskas</author>
<author>Saliha Azzam</author>
</authors>
<title>Event coreference for information extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, 35th Meeting of ACL,</booktitle>
<pages>75--81</pages>
<location>Madrid.</location>
<marker>Humphreys, Gaizauskas, Azzam, 1997</marker>
<rawString>Humphreys, Kevin, Robert Gaizauskas, and Saliha Azzam. 1997. Event coreference for information extraction. In Proceedings of the Workshop on Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts, 35th Meeting of ACL, pages 75–81, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Guodong Zhou</author>
</authors>
<title>Improve tree kernel-based event pronoun resolution with competitive information.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>1--814</pages>
<location>Barcelona.</location>
<contexts>
<context position="13757" citStr="Kong and Zhou 2011" startWordPosition="2072" endWordPosition="2075">e was initiated by the template merging task required in MUC evaluations and was primarily focused on scenario-specific events (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999). More recently, various supervised approaches using a mention-pair probabilistic framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel– based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference. Tree kernel–based methods have also been used to solve a special case of event coreference resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and Zhou 2011). To the best of our knowledge, the framework for solving event coreference presented in this article, extending the approach reported in Bejan and colleagues (Bejan et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference resolution that uses fully unsupervised methods and is based on Bayesian models. Over the past years, Bayesian models have been extensively used for the purpose of solving similar problems or subproblems of the generic problem presented in the previous section. In 2003, Blei, Ng, and Jordan proposed a parametric approach, called latent Dirich</context>
</contexts>
<marker>Kong, Zhou, 2011</marker>
<rawString>Kong, Fang and Guodong Zhou. 2011. Improve tree kernel-based event pronoun resolution with competitive information. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence (IJCAI), pages 1,814–1,819, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC-ACE</author>
</authors>
<title>ACE (Automatic Content Extraction) English Annotation Guidelines for Events, version 5.4.3 2005.07.01.</title>
<date>2005</date>
<booktitle>LDC Catalog Number: LDC2006T06.</booktitle>
<contexts>
<context position="23453" citStr="LDC-ACE 2005" startWordPosition="3568" endWordPosition="3569">f event mentions is to have the observable objects (i.e., the event mentions) identified in the order they occur in the documents as well as to have all the linguistic features associated with these objects extracted. However, in order to see how well these models perform, we need to compare their results with manually annotated clusters of event mentions. For this purpose, we evaluated our models on three different data sets annotated with event coreference information. The first data set was used for the event coreference evaluations performed in the automatic content extraction (ACE) task (LDC-ACE 2005). This resource contains only a restricted set of event types such as LIFE, BUSINESS, CONFLICT, and JUSTICE. As a second data set, we used the OntoNotes English corpus (release 2.0), a more diverse resource that provides a larger coverage of event (and entity) annotations. The utilization of the ACE and OntoNotes corpora for evaluating our event coreference models is, however, limited because these resources provide only within-document event coreference annotations. For this reason, as a third data set, we created the EventCorefBank (ECB) corpus1 to increase the diversity of event types and t</context>
</contexts>
<marker>LDC-ACE, 2005</marker>
<rawString>LDC-ACE. 2005. ACE (Automatic Content Extraction) English Annotation Guidelines for Events, version 5.4.3 2005.07.01. LDC Catalog Number: LDC2006T06.</rawString>
</citation>
<citation valid="false">
<authors>
<author>LDC-ON</author>
</authors>
<booktitle>2007.OntoNotes Release 2.0. LDC Catalog Number: LDC2008T04.</booktitle>
<marker>LDC-ON, </marker>
<rawString>LDC-ON. 2007.OntoNotes Release 2.0. LDC Catalog Number: LDC2008T04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>489--500</pages>
<location>Jeju Island.</location>
<contexts>
<context position="12551" citStr="Lee et al. (2012)" startWordPosition="1894" endWordPosition="1897">f entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. More recently, Lee et al. (2012) proposed an approach to jointly model event and entity coreference by allowing information from event coreference to help entity coreference, and the other way around. Their supervised method uses a high-precision entity resolution method based on a collection of deterministic models (called sieves) to produce both entity and event clusters that are optimally merged using linear regression. A similar technique that treated entity and event coreference resolution jointly was reported in He (2007) using narrative clinical data. Research that aimed at resolving only event coreference was initiat</context>
<context position="24179" citStr="Lee et al. (2012)" startWordPosition="3679" endWordPosition="3682">As a second data set, we used the OntoNotes English corpus (release 2.0), a more diverse resource that provides a larger coverage of event (and entity) annotations. The utilization of the ACE and OntoNotes corpora for evaluating our event coreference models is, however, limited because these resources provide only within-document event coreference annotations. For this reason, as a third data set, we created the EventCorefBank (ECB) corpus1 to increase the diversity of event types and to be able to evaluate our models for both within- and cross-document event coreference resolution. Recently, Lee et al. (2012) extended the EventCorefBank corpus with entity coreference information and additional annotations of event coreference. One important step in the creation process of the ECB corpus consists of finding sets of related documents that describe the same seminal event2 such that the annotation of coreferential event mentions across documents is possible. In this regard, we searched the Google News archive3 for various topics whose description contains keywords such as commercial transaction, attack, death, sports, announcement, terrorist act, election, arrest, natural disaster, and so on, and manu</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Lee, Heeyoung, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 489–500, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei-Fei Li</author>
<author>Pietro Perona</author>
</authors>
<title>A Bayesian hierarchical model for learning natural scene categories.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) -</booktitle>
<volume>2</volume>
<pages>524--531</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="15264" citStr="Li and Perona 2005" startWordPosition="2300" endWordPosition="2303">cuments are expressed as probabilistic mixtures of topics, while each topic has assigned a multinomial distribution over the words from the entire document collection. This approach also uses an exchangeability assumption by modeling the documents as bags of words. The LDA model and variations of it have been used in many applications such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared a</context>
</contexts>
<marker>Li, Perona, 2005</marker>
<rawString>Li, Fei-Fei and Pietro Perona. 2005. A Bayesian hierarchical model for learning natural scene categories. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) - Volume 2, pages 524–531, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Slav Petrov</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>The infinite PCFG using hierarchical Dirichlet processes.</title>
<date>2007</date>
<booktitle>In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL),</booktitle>
<pages>688--697</pages>
<location>Prague.</location>
<contexts>
<context position="17149" citStr="Liang et al. 2007" startWordPosition="2595" endWordPosition="2598">prove the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of multiple features. For example, in HDP, each data point is represented only by its corresponding word. For this reason, we built new Bayesian models on top of already-existing models with the main goal of providing a more flexible framework for representing da</context>
</contexts>
<marker>Liang, Petrov, Jordan, Klein, 2007</marker>
<rawString>Liang, Percy, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL), pages 688–697, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John B Lowe</author>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
</authors>
<title>A frame-semantic approach to semantic annotation.</title>
<date>1997</date>
<booktitle>In Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?,</booktitle>
<pages>18--24</pages>
<location>Washington, DC.</location>
<marker>Lowe, Baker, Fillmore, 1997</marker>
<rawString>Lowe, John B., Collin F. Baker, and Charles J. Fillmore. 1997. A frame-semantic approach to semantic annotation. In Proceedings of the SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?, pages 18–24, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP-2005),</booktitle>
<pages>25--32</pages>
<location>Vancouver.</location>
<marker>Luo, 2005</marker>
<rawString>Luo, Xiaoqiang. 2005. On coreference resolution performance metrics. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP-2005), pages 25–32, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>135--142</pages>
<location>Barcelona.</location>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the bell tree. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 135–142, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Malpas</author>
</authors>
<title>Donald Davidson. In the Stanford Encyclopedia of Philosophy (Fall</title>
<date>2009</date>
<note>Edition), edited by</note>
<contexts>
<context position="3837" citStr="Malpas (2009)" startWordPosition="564" endWordPosition="565">nce is not new. It was originally studied in philosophy, where researchers tried to determine when two events are identical and when they are different. One relevant theory in this direction was proposed by Davidson (1969), who argued that two events are identical if they have the same causes and effects. Later on, a different theory was proposed by Quine (1985), who considered that each event is associated with a physical object (which is well defined in space and time), and therefore, two events are identical if their corresponding objects have the same spatiotemporal location. According to Malpas (2009), in the same year, Davidson abandoned his suggestion to embrace the Quinean theory on event identity (Davidson 1985). Resolving event coreference is an essential requirement for many natural language processing (NLP) applications. For instance, in topic detection and tracking, event coreference resolution is required in order to identify new seminal events in broadcast news that have not been mentioned before (Allan et al. 1998). In information extraction, event coreference information was used for filling predefined template structures from text documents (Humphreys, Gaizauskas, and Azzam 19</context>
</contexts>
<marker>Malpas, 2009</marker>
<rawString>Malpas, Jeff. 2009. Donald Davidson. In the Stanford Encyclopedia of Philosophy (Fall 2009 Edition), edited by Edward N. Zalta. Available at http://plato.stanford.edu/ archives/fall2009/entries/davidson/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Meeds</author>
<author>Zoubin Ghahramani</author>
<author>Radford Neal</author>
<author>Sam Roweis</author>
</authors>
<title>Modeling dyadic data with binary latent factors.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 19 (NIPS),</booktitle>
<pages>977--984</pages>
<location>Vancouver.</location>
<contexts>
<context position="20107" citStr="Meeds et al. 2006" startWordPosition="3049" endWordPosition="3052">h, and Ghahramani 2008) was defined as a distribution over an unbounded set of binary Markov chains, where each chain can be associated with a binary latent feature that evolves over time according to Markov dynamics. Also, the phylogenetic Indian buffet process (pIBP) (Miller, Griffiths, and Jordan 2008) was created as a non-exchangeable, nonparametric prior for latent feature models, where the dependencies between objects were expressed as tree structures. Examples of applications that utilized these models are: identification of protein complexes (Chu et al. 2006), modeling of dyadic data (Meeds et al. 2006), modeling of choice behavior (G¨or¨ur, J¨akel, and Rasmussen 2006), and event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). Our extension of the HDP model still does not fulfill all the desiderata for the generic problem introduced in Section 1. It still requires a mechanism to automatically select a finite set of salient features that will be used in the clustering process (third desideratum) as well as a mechanism for capturing the structural dependencies between objects (fourth desideratum). To overcome these limitations, we created two additional models. First, we </context>
</contexts>
<marker>Meeds, Ghahramani, Neal, Roweis, 2006</marker>
<rawString>Meeds, Edward, Zoubin Ghahramani, Radford Neal, and Sam Roweis. 2006. Modeling dyadic data with binary latent factors. In Advances in Neural Information Processing Systems 19 (NIPS), pages 977–984, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Miller</author>
<author>Thomas Griffiths</author>
<author>Michael Jordan</author>
</authors>
<title>The phylogenetic Indian buffet process: A non-exchangeable nonparametric prior for latent features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI),</booktitle>
<pages>403--410</pages>
<location>Helsinki.</location>
<marker>Miller, Griffiths, Jordan, 2008</marker>
<rawString>Miller, Kurt, Thomas Griffiths, and Michael Jordan. 2008. The phylogenetic Indian buffet process: A non-exchangeable nonparametric prior for latent features. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI), pages 403–410, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>693--701</pages>
<location>Geneva.</location>
<contexts>
<context position="4588" citStr="Narayanan and Harabagiu 2004" startWordPosition="670" endWordPosition="673">olving event coreference is an essential requirement for many natural language processing (NLP) applications. For instance, in topic detection and tracking, event coreference resolution is required in order to identify new seminal events in broadcast news that have not been mentioned before (Allan et al. 1998). In information extraction, event coreference information was used for filling predefined template structures from text documents (Humphreys, Gaizauskas, and Azzam 1997). In question answering, a novel method of mapping event structures was used in order to provide answer justification (Narayanan and Harabagiu 2004). The same idea of mapping event structures was used in a graph-matching approach for enhancing textual entailment (Haghighi, Ng, and Manning 2005). Event coreference information was also used for detecting contradictions in text (de Marneffe, Rafferty, and Manning 2008). Previous NLP approaches for solving event coreference relied on supervised learning methods that explore various linguistic properties in order to decide if a pair of event mentions is coreferential or not (Humphreys, Gaizauskas, and Azzam 1997; Bagga and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In </context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Narayanan, Srini and Sanda Harabagiu. 2004. Question answering based on semantic structures. In Proceedings of the 20th International Conference on Computational Linguistics (COLING), pages 693–701, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice Sampling. The Annals of Statistics,</title>
<date>2003</date>
<pages>31--705</pages>
<marker>Neal, 2003</marker>
<rawString>Neal, Radford M. 2003. Slice Sampling. The Annals of Statistics, 31:705–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>640--649</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="16903" citStr="Ng 2008" startWordPosition="2564" endWordPosition="2565">ding to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able to model the data such that each observable object can be generated from a combination of multiple featu</context>
<context position="18570" citStr="Ng (2008)" startWordPosition="2817" endWordPosition="2818">between the observed random variables corresponding to object features. Thus, instead of considering as features only the words that express the event mentions (which is the way an observable object is represented in the original HDP model), we devised an HDP extension that is also able to represent features such as location, time, and agent for each event mention. This extension was inspired from the fully generative Bayesian model proposed by Haghighi and Klein (2007). However, Haghighi and Klein’s model was strictly customized for the task of entity coreference resolution. As also noted in Ng (2008) and Poon and Domingos (2008), whenever new features need to be considered in Haghighi and Klein’s model, the extension becomes a challenging task. Also, Daum´e III and Marcu (2005) performed 315 Computational Linguistics Volume 40, Number 2 related work in this direction by proposing a generative model for solving supervised clustering problems. As an alternative to the HDP model, an important extension of latent class models that are able to represent feature-rich objects is the Indian buffet process (IBP) model presented in Griffiths and Ghahramani (2005). The IBP model defines a distributi</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>Ng, Vincent. 2008. Unsupervised models for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 640–649, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>650--659</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="18599" citStr="Poon and Domingos (2008)" startWordPosition="2820" endWordPosition="2823">served random variables corresponding to object features. Thus, instead of considering as features only the words that express the event mentions (which is the way an observable object is represented in the original HDP model), we devised an HDP extension that is also able to represent features such as location, time, and agent for each event mention. This extension was inspired from the fully generative Bayesian model proposed by Haghighi and Klein (2007). However, Haghighi and Klein’s model was strictly customized for the task of entity coreference resolution. As also noted in Ng (2008) and Poon and Domingos (2008), whenever new features need to be considered in Haghighi and Klein’s model, the extension becomes a challenging task. Also, Daum´e III and Marcu (2005) performed 315 Computational Linguistics Volume 40, Number 2 related work in this direction by proposing a generative model for solving supervised clustering problems. As an alternative to the HDP model, an important extension of latent class models that are able to represent feature-rich objects is the Indian buffet process (IBP) model presented in Griffiths and Ghahramani (2005). The IBP model defines a distribution over infinite binary spars</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Poon, Hoifung and Pedro Domingos. 2008. Joint unsupervised coreference resolution with Markov logic. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 650–659, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jose Castano</author>
<author>Bob Ingria</author>
<author>Roser Sauri</author>
<author>Rob Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
</authors>
<title>TimeML: Robust specification of event and temporal expressions in text.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS),</booktitle>
<pages>337--353</pages>
<location>Tilburg.</location>
<contexts>
<context position="25200" citStr="Pustejovsky et al. 2003" startWordPosition="3832" endWordPosition="3835">rchive3 for various topics whose description contains keywords such as commercial transaction, attack, death, sports, announcement, terrorist act, election, arrest, natural disaster, and so on, and manually selected sets of Web documents describing the same seminal event for each of these topics. In a subsequent step, for every Web document, we automatically tokenized and split the textual content into sentences, and saved the preprocessed data in a uniquely identified text file. Next, we manually annotated a limited set of events in each text file in accordance with the TimeML specification (Pustejovsky et al. 2003a). To mark the event mentions and the coreferential relations between them we utilized the Callisto4 and Tango5 annotation tools, respectively. Additional details regarding the annotation process for creating the ECB resource are described in Bejan and Harabagiu (2008a). Several annotation fragments from ECB are shown in Example (1). In this example, event mentions are annotated at the sentence level, sentences are grouped into documents, and the documents describing the same seminal event are organized into topics. The topics shown in Example (1) describe the seminal event of arresting sea p</context>
</contexts>
<marker>Pustejovsky, Castano, Ingria, Sauri, Gaizauskas, Setzer, Katz, 2003</marker>
<rawString>Pustejovsky, James, Jose Castano, Bob Ingria, Roser Sauri, Rob Gaizauskas, Andrea Setzer, and Graham Katz. 2003a. TimeML: Robust specification of event and temporal expressions in text. In Proceedings of the Fifth International Workshop on Computational Semantics (IWCS), pages 337–353, Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Marcia Lazo</author>
</authors>
<title>The TimeBank Corpus. In Corpus Linguistics,</title>
<date>2003</date>
<pages>647--656</pages>
<contexts>
<context position="25200" citStr="Pustejovsky et al. 2003" startWordPosition="3832" endWordPosition="3835">rchive3 for various topics whose description contains keywords such as commercial transaction, attack, death, sports, announcement, terrorist act, election, arrest, natural disaster, and so on, and manually selected sets of Web documents describing the same seminal event for each of these topics. In a subsequent step, for every Web document, we automatically tokenized and split the textual content into sentences, and saved the preprocessed data in a uniquely identified text file. Next, we manually annotated a limited set of events in each text file in accordance with the TimeML specification (Pustejovsky et al. 2003a). To mark the event mentions and the coreferential relations between them we utilized the Callisto4 and Tango5 annotation tools, respectively. Additional details regarding the annotation process for creating the ECB resource are described in Bejan and Harabagiu (2008a). Several annotation fragments from ECB are shown in Example (1). In this example, event mentions are annotated at the sentence level, sentences are grouped into documents, and the documents describing the same seminal event are organized into topics. The topics shown in Example (1) describe the seminal event of arresting sea p</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>Pustejovsky, James, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003b. The TimeBank Corpus. In Corpus Linguistics, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W V O Quine</author>
</authors>
<title>Events and Reification.</title>
<date>1985</date>
<booktitle>Actions and Events: Perspectives on the Philosophy of Donald Davidson.</booktitle>
<pages>162--171</pages>
<editor>In E. LePore and B. P. McLaughlin, editors,</editor>
<publisher>Blackwell,</publisher>
<location>Oxford,</location>
<note>Reprinted in</note>
<contexts>
<context position="3588" citStr="Quine (1985)" startWordPosition="526" endWordPosition="527">tion consists of grouping together the text expressions that refer to real-world events (also called event mentions) into a set of clusters such that all the mentions from the same cluster correspond to a unique event. The problem of event coreference is not new. It was originally studied in philosophy, where researchers tried to determine when two events are identical and when they are different. One relevant theory in this direction was proposed by Davidson (1969), who argued that two events are identical if they have the same causes and effects. Later on, a different theory was proposed by Quine (1985), who considered that each event is associated with a physical object (which is well defined in space and time), and therefore, two events are identical if their corresponding objects have the same spatiotemporal location. According to Malpas (2009), in the same year, Davidson abandoned his suggestion to embrace the Quinean theory on event identity (Davidson 1985). Resolving event coreference is an essential requirement for many natural language processing (NLP) applications. For instance, in topic detection and tracking, event coreference resolution is required in order to identify new semina</context>
</contexts>
<marker>Quine, 1985</marker>
<rawString>Quine, W. V. O., 1985. Events and Reification. In E. LePore and B. P. McLaughlin, editors, Actions and Events: Perspectives on the Philosophy of Donald Davidson. Blackwell, Oxford, pages 162–171. Reprinted in R. Casati and A. C. Varzi, editors, Events. 1996, Aldershot, Dartmouth, pages 107–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, Lawrence R. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nate Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multi-pass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>492--501</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="11905" citStr="Raghunathan et al. 2010" startWordPosition="1794" endWordPosition="1797"> mentions are typically predications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of thei</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Raghunathan, Karthik, Heeyoung Lee, Sudarshan Rangarajan, Nate Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multi-pass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 492–501, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>814--824</pages>
<location>Portland, OR.</location>
<contexts>
<context position="11926" citStr="Rahman and Ng 2011" startWordPosition="1798" endWordPosition="1801">redications that require more complex lexico-semantic processing, and furthermore, the capability of extracting features that characterize them has been available only since semantic parsers based on PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and Lowe 1998) corpora have been developed. In contrast, entity coreference resolution has been intensively studied and many successful techniques for identifying mention clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009; Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and Ng 2011). Even if entity coreference resolution has received much attention from the computational linguistic researchers, there is only limited work that incorporates event-related information to solve entity coreference, typically by considering the verbs that are present in the context of a referring entity as features. For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Rahman and Ng (2011) used event-related information by looking at which semantic role the entity mentions can have and the verb pairs of their predicates. More re</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Rahman, Altaf and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 814–824, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Latent variable models of concept-attribute attachment.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>620--628</pages>
<marker>Reisinger, Pas¸ca, 2009</marker>
<rawString>Reisinger, Joseph and Marius Pas¸ca. 2009. Latent variable models of concept-attribute attachment. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 620–628, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Bryan Russell</author>
<author>Alexei Efros</author>
<author>Andrew Zisserman</author>
<author>William Freeman</author>
</authors>
<title>Discovering object categories in image collections.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th IEEE International Conference on Computer Vision (ICCV),</booktitle>
<pages>370--377</pages>
<location>Beijing.</location>
<contexts>
<context position="15192" citStr="Sivic et al. 2005" startWordPosition="2290" endWordPosition="2293">esolution collection of text documents. In this latent class model, documents are expressed as probabilistic mixtures of topics, while each topic has assigned a multinomial distribution over the words from the entire document collection. This approach also uses an exchangeability assumption by modeling the documents as bags of words. The LDA model and variations of it have been used in many applications such as topic modeling (Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable objec</context>
</contexts>
<marker>Sivic, Russell, Efros, Zisserman, Freeman, 2005</marker>
<rawString>Sivic, Josef, Bryan Russell, Alexei Efros, Andrew Zisserman, and William Freeman. 2005. Discovering object categories in image collections. In Proceedings of the 10th IEEE International Conference on Computer Vision (ICCV), pages 370–377, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Bryan Russell</author>
<author>Andrew Zisserman</author>
<author>William Freeman</author>
<author>Alexei Efros</author>
</authors>
<title>Unsupervised discovery of visual object class hierarchies.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1--8</pages>
<location>Anchorage, AK.</location>
<marker>Sivic, Russell, Zisserman, Freeman, Efros, 2008</marker>
<rawString>Sivic, Josef, Bryan Russell, Andrew Zisserman, William Freeman, and Alexei Efros. 2008. Unsupervised discovery of visual object class hierarchies. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8, Anchorage, AK.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Veselin Stoyanov</author>
</authors>
<location>Nathan Gilbert,</location>
<marker>Stoyanov, </marker>
<rawString>Stoyanov, Veselin, Nathan Gilbert,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the state of the art.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>656--664</pages>
<marker>Cardie, Riloff, 2009</marker>
<rawString>Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the state of the art. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 656–664, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik B Sudderth</author>
<author>Antonio Torralba</author>
<author>William T Freeman</author>
<author>Alan S Willsky</author>
</authors>
<title>Describing visual scenes using transformed objects and parts.</title>
<date>2008</date>
<journal>International Journal of Computer Vision,</journal>
<pages>77--291</pages>
<contexts>
<context position="16800" citStr="Sudderth et al. 2008" startWordPosition="2549" endWordPosition="2552">p of data. In addition, these DPs are coupled through a common random base measure which is itself distributed according to a DP. Due to the fact that a DP provides a nonparametric prior for the number of classes K, the HDP setting allows for this number to be unbounded in each group. More recently, various other applications have been proposed to improve the existing HDP inference algorithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth, and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval (Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation (Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007). Although infinite latent class models like HDP have the advantage of automatically inferring the number of categorical outcomes K, they are still limited in representing feature-rich objects. Specifically, in their original form, they are not able t</context>
</contexts>
<marker>Sudderth, Torralba, Freeman, Willsky, 2008</marker>
<rawString>Sudderth, Erik B., Antonio Torralba, William T. Freeman, and Alan S. Willsky. 2008. Describing visual scenes using transformed objects and parts. International Journal of Computer Vision, 77:291–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael Jordan</author>
<author>Matthew Beal</author>
<author>David Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="15694" citStr="Teh et al. (2006)" startWordPosition="2366" endWordPosition="2369">ambiguation (Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images (Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005), discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu 2008b), and attachment of attributes to a concept ontology (Reisinger and Pas¸ca 2009). The LDA model, although attractive, has the disadvantage of requiring a priori knowledge regarding the number of latent classes. A more suitable approach for solving our problem is the hierarchical Dirichlet process (HDP) model described in Teh et al. (2006). Like LDA, this model considers problems that involve groups of data, where each observable object is sampled from a mixture model and each mixture component is shared across groups. However, the HDP mixture model is a nonparametric generalization of LDA that is also able to automatically infer the number of clustering components K (the first desideratum for our problem). It consists of a set of Dirichlet processes (DPs) (Ferguson 1973), in which each DP is associated with a group of data. In addition, these DPs are coupled through a common random base measure which is itself distributed acco</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Teh, Yee Whye, Michael Jordan, Matthew Beal, and David Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Y Saatci</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Beam sampling for the infinite hidden Markov model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th Annual International Conference on Machine Learning (ICML),</booktitle>
<pages>1--088</pages>
<location>Helsinki.</location>
<marker>Van Gael, Saatci, Teh, Ghahramani, 2008</marker>
<rawString>Van Gael, Jurgen, Y. Saatci, Yee Whye Teh, and Zoubin Ghahramani. 2008. Beam sampling for the infinite hidden Markov model. In Proceedings of the 25th Annual International Conference on Machine Learning (ICML), pages 1,088–1,095, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Van Gael</author>
<author>Yee Whye Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite factorial hidden Markov model.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 21 (NIPS),</booktitle>
<pages>1697--1704</pages>
<location>Vancouver.</location>
<marker>Van Gael, Teh, Ghahramani, 2008</marker>
<rawString>Van Gael, Jurgen, Yee Whye Teh, and Zoubin Ghahramani. 2008. The infinite factorial hidden Markov model. In Advances in Neural Information Processing Systems 21 (NIPS), pages 1697–1704, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC-6,</booktitle>
<pages>45--52</pages>
<location>Columbia, MD.</location>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings of MUC-6, pages 45–52, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chong Wang</author>
<author>John Paisley</author>
<author>David Blei</author>
</authors>
<title>Online variational inference for the hierarchical Dirichlet process.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),</booktitle>
<pages>752--760</pages>
<location>Ft. Lauderdale, FL.</location>
<marker>Wang, Paisley, Blei, 2011</marker>
<rawString>Wang, Chong, John Paisley, and David Blei. 2011. Online variational inference for the hierarchical Dirichlet process. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), pages 752–760, Ft. Lauderdale, FL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.074070">
<title confidence="0.9996205">
Optimizing Features in Active Machine Learning for Complex
Qualitative Content Analysis
</title>
<author confidence="0.997065">
Jasy Liew Suet Yan
</author>
<affiliation confidence="0.902273">
School of Information Studies
Syracuse University, USA
</affiliation>
<email confidence="0.997856">
jliewsue@syr.edu
</email>
<author confidence="0.996114">
Nancy McCracken
</author>
<affiliation confidence="0.901869">
School of Information Studies
Syracuse University, USA
</affiliation>
<email confidence="0.99769">
njmccrac@syr.edu
</email>
<author confidence="0.99458">
Shichun Zhou
</author>
<affiliation confidence="0.925467666666667">
College of Engineering and
Computer Science
Syracuse University, USA
</affiliation>
<email confidence="0.996518">
szhou02@syr.edu
</email>
<author confidence="0.990462">
Kevin Crowston
</author>
<affiliation confidence="0.9439115">
National Science
Foundation, USA
</affiliation>
<email confidence="0.994334">
crowston@syr.edu
</email>
<sectionHeader confidence="0.99382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896538461539">
We propose a semi-automatic approach for
content analysis that leverages machine learn-
ing (ML) being initially trained on a small set
of hand-coded data to perform a first pass in
coding, and then have human annotators cor-
rect machine annotations in order to produce
more examples to retrain the existing model
incrementally for better performance. In this
“active learning” approach, it is equally im-
portant to optimize the creation of the initial
ML model given less training data so that the
model is able to capture most if not all posi-
tive examples, and filter out as many negative
examples as possible for human annotators to
correct. This paper reports our attempt to op-
timize the initial ML model through feature
exploration in a complex content analysis
project that uses a multidimensional coding
scheme, and contains codes with sparse posi-
tive examples. While different codes respond
optimally to different combinations of fea-
tures, we show that it is possible to create an
optimal initial ML model using only a single
combination of features for codes with at
least 100 positive examples in the gold stand-
ard corpus.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991154877192983">
Content analysis, a technique for finding evi-
dence of concepts of theoretical interest through
text, is an increasingly popular technique social
scientists use in their research investigations. In
the process commonly known as “coding”, social
scientists often have to painstakingly comb
through large quantities of natural language cor-
pora to annotate text segments (e.g., phrase, sen-
tence, and paragraphs) with codes exhibiting the
concepts of interest (Miles &amp; Huberman, 1994).
Analyzing textual data is very labor-intensive,
time-consuming, and is often limited to the capa-
bilities of individual researchers (W. Evans,
1996). The coding process becomes even more
demanding as the complexity of the project in-
creases especially in the case of attempting to
apply a multidimensional coding scheme with a
significant number of codes (Dönmez, Rosé,
Stegmann, Weinberger, &amp; Fischer, 2005).
With the proliferation and availability of dig-
ital texts, it is challenging, if not impossible, for
human coders to manually analyze torrents of
text to help advance social scientists’ understand-
ing of the practices of different populations of
interest through textual data. Therefore, compu-
tational methods offer significant benefits to help
augment human capabilities to explore massive
amounts of text in more complex ways for theory
generation and theory testing. Content analysis
can be framed as a text classification problem,
where each text segment is labeled based on a
predetermined set of categories or codes.
Full automation of content analysis is still far
from being perfect (Grimmer &amp; Stewart, 2013).
The accuracy of current automatic approaches on
the best performing codes in social science re-
search ranges from 60-90% (Broadwell et al.,
2013; Crowston, Allen, &amp; Heckman, 2012; M.
Evans, McIntosh, Lin, &amp; Cates, 2007; Ishita,
Oard, Fleischmann, Cheng, &amp; Templeton, 2010;
Zhu, Kraut, Wang, &amp; Kittur, 2011). While the
potential of automatic content analysis is promis-
ing, computational methods should not be
viewed as a replacement for the role of the pri-
mary researcher in the careful interpretation of
text. Rather, the computers’ pattern recognition
capabilities can be leveraged to seek out the most
likely examples for each code of interest, thus
reducing the amount of texts researchers have to
read and process.
We propose a semi-automatic method that
promotes a close human-computer partnership
for content analysis. Machine learning (ML) is
used to perform the first pass of coding on the
unlabeled texts. Human annotators then have to
correct only what the ML model identifies as
positive examples of each code. The initial ML
</bodyText>
<page confidence="0.99078">
44
</page>
<note confidence="0.508563">
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 44–48,
</note>
<page confidence="0.345896">
Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics
</page>
<bodyText confidence="0.99301788">
model needs to learn only from a small set of
hand-coded examples (i.e., gold standard data),
and will evolve and improve as machine annota-
tions that are verified by human annotators are
used to incrementally retrain the model. In con-
trast to conventional machine learning, this “ac-
tive learning” approach will significantly reduce
the amount of training data needed upfront from
the human annotators. However, it is still equally
important to optimize the creation of the initial
ML model given less training data so that the
model is able to capture most if not all positive
examples, and filter out as many negative exam-
ples as possible for human annotators to correct.
To effectively implement the active learning
approach for coding qualitative data, we have to
first understand the nature and complexity of
content analysis projects in social science re-
search. Our pilot case study, an investigation of
leadership behaviors exhibited in emails from a
FLOSS development project (Misiolek, Crow-
ston, &amp; Seymour, 2012), reveals that it is com-
mon for researchers to use a multidimensional
coding scheme consisting of a significant number
of codes in their research inquiry. Previous work
has shown that not all dimensions in a multidi-
mensional coding scheme could be applied fully
automatically with acceptable level of accuracy
(Dönmez et al., 2005) but little is known if it is
possible at all to train an optimal model for all
codes using the same combination of features.
Also, the distribution of codes is often times un-
even with some rarely occurring codes having
only few positive examples in the gold standard
corpus.
This paper presents our attempt in optimiz-
ing the initial ML model through feature explora-
tion using gold standard data created from a mul-
tidimensional coding scheme, including codes
that suffer from sparseness of positive examples.
Specifically, our study is guided by two research
questions:
a) How can features for an initial machine
learning model be optimized for all codes in
a text classification problem based on multi-
dimensional coding schemes? Is it possible
to train a one-size-fits-all model for all codes
using a single combination of features?
b) Are certain features better suited for codes
with sparse positive examples?
</bodyText>
<sectionHeader confidence="0.986895" genericHeader="method">
2 Machine Learning Experiments
</sectionHeader>
<bodyText confidence="0.999480214285714">
To optimize the initial machine learning model,
we systematically ran multiple experiments using
a gold standard corpus of emails from a
free/libre/open-source software (FLOSS) devel-
opment project coded for leadership behaviors
(Misiolek et al., 2012). The coding scheme con-
tained six dimensions: 1) social/relationship, 2)
task process, 3) task substance, 4) dual process
and substance, 5) change behaviors, and 6) net-
working. The number of codes for each dimen-
sion ranged from 1 to 14. There were a total of
35 codes in the coding scheme. Each sentence
could be assigned more than one code. Framing
the problem as a multi-label classification task,
we trained a binary classification model for each
code using support vector machine (SVM) with
ten-fold cross-validation. This gold standard cor-
pus consisted of 3,728 hand-coded sentences
from 408 email messages.
For the active learning setup, we tune the ini-
tial ML model for high recall since having the
annotators pick out positive examples that have
been incorrectly classified by the model is pref-
erable to missing machine-annotated positive
examples to be presented to human annotators
for verification (Liew, McCracken, &amp; Crowston,
2014). Therefore, the initial ML model with low
precision is acceptable.
</bodyText>
<table confidence="0.997154833333333">
Category Features
Content Unigram, bigram, pruning,
tagging, lowercase, stop-
words, stemming, part-of-
speech (POS) tags
Syntactic Token count
Orthographic Capitalization of first letter of
a word, capitalization of entire
word
Word list Subjectivity words
Semantic Role of sender (software de-
veloper or not)
</table>
<tableCaption confidence="0.990404">
Table 1. Features for ML model.
</tableCaption>
<bodyText confidence="0.999755538461539">
As shown in Table 1, we have selected gen-
eral candidate features that have proven to work
well across various text classification tasks, as
well as one semantic feature specific to the con-
text of FLOSS development projects. For content
features, techniques that we have incorporated to
reduce the feature space include pruning, substi-
tuting certain tokens with more generic tags,
converting all tokens to lowercase, excluding
stopwords, and stemming. Using the wrapper
approach (Kohavi &amp; John, 1997), the same clas-
sifier is used to test the prediction performance
of various feature combinations listed in Table 1.
</bodyText>
<page confidence="0.996044">
45
</page>
<table confidence="0.999854916666667">
Model SINGLE MULTIPLE
Measure Mean Recall Mean Precision Mean Recall Mean Precision
Overall
All (35) 0.690 0.065 0.877 0.068
Dimension
Change (1) 0.917 0.011 1.000 0.016
Dual Process and 0.675 0.069 0.852 0.067
Substance (13)
Networking (1) 0.546 0.010 0.843 0.020
Process (3) 0.445 0.006 0.944 0.024
Relationship (14) 0.742 0.083 0.872 0.089
Substance (3) 0.735 0.061 0.919 0.051
</table>
<tableCaption confidence="0.999706">
Table 2. Comparison of mean recall and mean precision between SINGLE and MULTIPLE models.
</tableCaption>
<figureCaption confidence="0.999489">
Figure 1. Recall and precision for each code (grouped by dimension).
</figureCaption>
<figure confidence="0.999801139534884">
0.8
0.6
0.4
0.2
0
1
Best Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)
Change Dual Networking Process Relationship Substance
External Monitoring
Informing
Issue Directive
Correction
Offer/Provide Assistance
Approval
Request/Invite
Commit/Assume Responsibility
Confirm/Clarify
Objection/Disagreement
Query/Question
Update
Suggest/Recommend
Explanation
Networking/Boundary Spanning
Remind
Procedure
Schedule
Criticism
Proactive Informing
Apology
Consulting
Humor
Appreciation
Self-disclosure
Vocative
Agreement
Emotional Expression
Phatics/Salutations
Inclusive Reference
Opinion/Preference
Acronym/Jargon
Generate New Idea
Evaluation/Feedback
Provide Information
</figure>
<sectionHeader confidence="0.998071" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999659411764706">
We ran 343 experiments with different combina-
tions of the 13 features in Table 1. We first com-
pare the performance of the best one-size-fits-all
initial machine learning model that produces the
highest recall using a single combination of fea-
tures for all codes (SINGLE) with an “ensemble”
model that uses different combinations of fea-
tures to produce the highest recall for each code
(MULTIPLE). The SINGLE model combines
content (unigram + bigram + POS tags + lower-
case + stopwords) with syntactic, orthographic,
and semantic features. None of the best feature
combination for each code in the MULTIPLE
model coincides with the feature combination in
the SINGLE model. For example, the best fea-
ture combination for code “Phatics/Salutations”
consists of only 2 out of the 13 features (unigram
+ bigram).
The best feature combination for each code
in the MULTIPLE model varies with only some
regularity noted in a few codes within the Dual
and Substance dimensions. However, these pat-
terns are not consistent across all codes in a sin-
gle dimension indicating that the pertinent lin-
guistic features for codes belonging to the same
dimension may differ despite their conceptual
similarities, and even fitting an optimal model
for all codes within a single dimension may
prove to be difficult especially when the distribu-
tion of codes is uneven, and positive examples
for certain codes are sparse. There are also no
consistent feature patterns observed from the
codes with sparse positive examples in the
MULTIPLE model.
</bodyText>
<page confidence="0.998519">
46
</page>
<figure confidence="0.99982230952381">
0.8
0.6
0.4
0.2
0
1
Remind (4)
Generate New Idea (4)
Best Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)
Criticism (5)
Proactive Informing (5)
Informing (7)
Procedure (7)
External Monitoring (8)
Issue Directive (8)
Schedule (8)
Apology (8)
Evaluation/Feedback (9)
Correction (11)
Offer/Provide Assistance (11)
Approval (12)
Consulting (13)
Request/Invite (14)
Commit/Assume Responsibility (17)
Networking/Boundary Spanning (18)
Acronym/Jargon (19)
Humor (21)
Appreciation (23)
Self-disclosure (37)
Vocative (41)
Confirm/Clarify (45)
Objection/Disagreement (51)
Agreement (67)
Emotional Expression (108)
Query/Question (115)
Phatics/Salutations (116)
Update (119)
Suggest/Recommend (138)
Inclusive Reference (146)
Opinion/Preference (215)
Provide Information (312)
Explanation (327)
</figure>
<figureCaption confidence="0.999988">
Figure 2. Recall and precision for each code (sorted by gold frequency)
</figureCaption>
<bodyText confidence="0.99995304">
The comparison between the two models in
Table 2 further demonstrates that the MULTI-
PLE model outperforms the SINGLE model both
in the overall mean recall of all 35 codes, as well
as the mean recall for each dimension. Figure 1
(codes grouped by dimensions) illustrates that
the feature combination on the SINGLE model is
ill-suited for the Process codes, and half the Dual
Process and Substance codes. Recall for each
code for the SINGLE model are mostly below or
at par with the recall for each code in the MUL-
TIPLE model. Thus, creating a one-size-fits-all
initial model may not be optimal when training
data is limited. Figure 2 (codes sorted based on
gold frequency as shown beside the code names
in the x-axis) exhibits that the SINGLE model is
able to achieve similar recall to the MULTIPLE
model for codes with over 100 positive examples
in the training data. Precision for these codes are
also higher compared to codes with sparse posi-
tive examples. This finding is promising because
it implies that creating a one-size-fits-all initial
ML model may be possible even for a multidi-
mensional coding scheme if there are more than
100 positive examples for each code.
</bodyText>
<sectionHeader confidence="0.997415" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999964735294118">
We conclude that creating an optimal initial one-
size-fits-all ML model for all codes in a multi-
dimensional coding scheme using only a single
feature combination is not possible when codes
with sparse positive examples are present, and
training data is limited, which may be common
in real world content analysis projects in social
science research. However, our findings also
show that the potential of using a one-size-fits-all
model increases when the size of positive exam-
ples for each code in the gold standard corpus are
above 100. For social scientists who may not
possess the technical skills needed for feature
selection to optimize the initial ML model, this
discovery confirms that we can create a “canned”
model using a single combination of features that
would work well in text classification for a wide
range of codes with the condition that research-
ers must be able to provide sufficient positive
examples above a certain threshold to train the
initial model. This would make the application of
machine learning for qualitative content analysis
more accessible to social scientists.
The initial ML model with low precision
means that the model is over-predicting. As a
result, human annotators will have to correct
more false positives in the machine annotations.
For future work, we plan to experiment with dif-
ferent sampling strategies to pick the most “prof-
itable” machine annotations to be corrected by
human annotators. We will also work on design-
ing an interactive and adaptive user interface to
promote greater understanding of machine learn-
ing outputs for our target users.
</bodyText>
<page confidence="0.99915">
47
</page>
<sectionHeader confidence="0.998329" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999915666666667">
This material is based upon work supported
by the National Science Foundation under Grant
No. IIS-1111107. Kevin Crowston is supported
by the National Science Foundation. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
author(s) and do not necessarily reflect the views
of the National Science Foundation. The authors
wish to thank Janet Marsden for assisting with
the feature testing experiments, and gratefully
acknowledge helpful suggestions by the review-
ers.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941666666667">
Broadwell, G. A., Stromer-Galley, J., Strzalkowski,
T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia,
A., Jiao, L., Webb, N. (2013). Modeling Soci-
ocultural phenomena in discourse. Natural Lan-
guage Engineering, 19(02), 213–257.
Crowston, K., Allen, E. E., &amp; Heckman, R. (2012).
Using natural language processing technology for
qualitative data analysis. International Journal of
Social Research Methodology, 15(6), 523–543.
Dönmez, P., Rosé, C., Stegmann, K., Weinberger, A.,
&amp; Fischer, F. (2005). Supporting CSCL with au-
tomatic corpus analysis technology. In Proceed-
ings of 2005 Conference on Computer Support
for Collaborative Learning (pp. 125–134).
Evans, M., McIntosh, W., Lin, J., &amp; Cates, C. (2007).
Recounting the courts? Applying automated con-
tent analysis to enhance empirical legal research.
Journal of Empirical Legal Studies, 4(4), 1007–
1039.
Evans, W. (1996). Computer-supported content anal-
ysis: Trends, tools, and techniques. Social Sci-
ence Computer Review, 14(3), 269–279.
Grimmer, J., &amp; Stewart, B. M. (2013). Text as data:
The promise and pitfalls of automatic content
analysis methods for political texts. Political
Analysis, 21(3), 267–297.
Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng,
A.-S., &amp; Templeton, T. C. (2010). Investigating
multi-label classification for human values. Pro-
ceedings of the American Society for Information
Science and Technology, 47(1), 1–4.
Liew, J. S. Y., McCracken, N., &amp; Crowston, K.
(2014). Semi-automatic content analysis of quali-
tative data. In iConference 2014 Proceedings (pp.
1128–1132).
Miles, M. B., &amp; Huberman, A. M. (1994). Qualitative
data analysis: An expanded sourcebook (2nd
ed.). Sage.
Misiolek, N., Crowston, K., &amp; Seymour, J. (2012).
Team dynamics in long-standing technology-
supported virtual teams. Presented at the Acade-
my of Management Annual Meeting, Organiza-
tional Behavior Division, Boston, MA.
Zhu, H., Kraut, R. E., Wang, Y.-C., &amp; Kittur, A.
(2011). Identifying shared leadership in Wikipe-
dia. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems (pp.
3431–3434). New York, NY, USA.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.707174">
<title confidence="0.9998925">Optimizing Features in Active Machine Learning for Qualitative Content Analysis</title>
<author confidence="0.999593">Jasy Liew Suet Yan</author>
<affiliation confidence="0.986604">School of Information Studies Syracuse University, USA</affiliation>
<email confidence="0.999129">jliewsue@syr.edu</email>
<author confidence="0.999652">Nancy McCracken</author>
<affiliation confidence="0.9910605">School of Information Studies Syracuse University, USA</affiliation>
<email confidence="0.999029">njmccrac@syr.edu</email>
<author confidence="0.99823">Shichun Zhou</author>
<affiliation confidence="0.988578333333333">College of Engineering Computer Syracuse University, USA</affiliation>
<email confidence="0.998774">szhou02@syr.edu</email>
<author confidence="0.999527">Kevin Crowston</author>
<affiliation confidence="0.999805">National Science</affiliation>
<address confidence="0.862757">Foundation, USA</address>
<email confidence="0.999663">crowston@syr.edu</email>
<abstract confidence="0.995983962962963">We propose a semi-automatic approach for content analysis that leverages machine learning (ML) being initially trained on a small set of hand-coded data to perform a first pass in coding, and then have human annotators correct machine annotations in order to produce more examples to retrain the existing model incrementally for better performance. In this learning” it is equally important to optimize the creation of the initial ML model given less training data so that the model is able to capture most if not all positive examples, and filter out as many negative examples as possible for human annotators to correct. This paper reports our attempt to optimize the initial ML model through feature exploration in a complex content analysis project that uses a multidimensional coding scheme, and contains codes with sparse positive examples. While different codes respond optimally to different combinations of features, we show that it is possible to create an optimal initial ML model using only a single combination of features for codes with at least 100 positive examples in the gold standard corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G A Broadwell</author>
<author>J Stromer-Galley</author>
<author>T Strzalkowski</author>
<author>S Shaikh</author>
<author>S Taylor</author>
<author>T Liu</author>
<author>U Boz</author>
<author>A Elia</author>
<author>L Jiao</author>
<author>N Webb</author>
</authors>
<title>Modeling Sociocultural phenomena in discourse. Natural Language Engineering,</title>
<date>2013</date>
<pages>213--257</pages>
<contexts>
<context position="3331" citStr="Broadwell et al., 2013" startWordPosition="502" endWordPosition="505">ons of interest through textual data. Therefore, computational methods offer significant benefits to help augment human capabilities to explore massive amounts of text in more complex ways for theory generation and theory testing. Content analysis can be framed as a text classification problem, where each text segment is labeled based on a predetermined set of categories or codes. Full automation of content analysis is still far from being perfect (Grimmer &amp; Stewart, 2013). The accuracy of current automatic approaches on the best performing codes in social science research ranges from 60-90% (Broadwell et al., 2013; Crowston, Allen, &amp; Heckman, 2012; M. Evans, McIntosh, Lin, &amp; Cates, 2007; Ishita, Oard, Fleischmann, Cheng, &amp; Templeton, 2010; Zhu, Kraut, Wang, &amp; Kittur, 2011). While the potential of automatic content analysis is promising, computational methods should not be viewed as a replacement for the role of the primary researcher in the careful interpretation of text. Rather, the computers’ pattern recognition capabilities can be leveraged to seek out the most likely examples for each code of interest, thus reducing the amount of texts researchers have to read and process. We propose a semi-automat</context>
</contexts>
<marker>Broadwell, Stromer-Galley, Strzalkowski, Shaikh, Taylor, Liu, Boz, Elia, Jiao, Webb, 2013</marker>
<rawString>Broadwell, G. A., Stromer-Galley, J., Strzalkowski, T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, A., Jiao, L., Webb, N. (2013). Modeling Sociocultural phenomena in discourse. Natural Language Engineering, 19(02), 213–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crowston</author>
<author>E E Allen</author>
<author>R Heckman</author>
</authors>
<title>Using natural language processing technology for qualitative data analysis.</title>
<date>2012</date>
<journal>International Journal of Social Research Methodology,</journal>
<volume>15</volume>
<issue>6</issue>
<pages>523--543</pages>
<contexts>
<context position="3365" citStr="Crowston, Allen, &amp; Heckman, 2012" startWordPosition="506" endWordPosition="510">textual data. Therefore, computational methods offer significant benefits to help augment human capabilities to explore massive amounts of text in more complex ways for theory generation and theory testing. Content analysis can be framed as a text classification problem, where each text segment is labeled based on a predetermined set of categories or codes. Full automation of content analysis is still far from being perfect (Grimmer &amp; Stewart, 2013). The accuracy of current automatic approaches on the best performing codes in social science research ranges from 60-90% (Broadwell et al., 2013; Crowston, Allen, &amp; Heckman, 2012; M. Evans, McIntosh, Lin, &amp; Cates, 2007; Ishita, Oard, Fleischmann, Cheng, &amp; Templeton, 2010; Zhu, Kraut, Wang, &amp; Kittur, 2011). While the potential of automatic content analysis is promising, computational methods should not be viewed as a replacement for the role of the primary researcher in the careful interpretation of text. Rather, the computers’ pattern recognition capabilities can be leveraged to seek out the most likely examples for each code of interest, thus reducing the amount of texts researchers have to read and process. We propose a semi-automatic method that promotes a close hu</context>
</contexts>
<marker>Crowston, Allen, Heckman, 2012</marker>
<rawString>Crowston, K., Allen, E. E., &amp; Heckman, R. (2012). Using natural language processing technology for qualitative data analysis. International Journal of Social Research Methodology, 15(6), 523–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dönmez</author>
<author>C Rosé</author>
<author>K Stegmann</author>
<author>A Weinberger</author>
<author>F Fischer</author>
</authors>
<title>Supporting CSCL with automatic corpus analysis technology.</title>
<date>2005</date>
<booktitle>In Proceedings of 2005 Conference on Computer Support for Collaborative Learning</booktitle>
<pages>125--134</pages>
<contexts>
<context position="2469" citStr="Dönmez, Rosé, Stegmann, Weinberger, &amp; Fischer, 2005" startWordPosition="366" endWordPosition="372">, social scientists often have to painstakingly comb through large quantities of natural language corpora to annotate text segments (e.g., phrase, sentence, and paragraphs) with codes exhibiting the concepts of interest (Miles &amp; Huberman, 1994). Analyzing textual data is very labor-intensive, time-consuming, and is often limited to the capabilities of individual researchers (W. Evans, 1996). The coding process becomes even more demanding as the complexity of the project increases especially in the case of attempting to apply a multidimensional coding scheme with a significant number of codes (Dönmez, Rosé, Stegmann, Weinberger, &amp; Fischer, 2005). With the proliferation and availability of digital texts, it is challenging, if not impossible, for human coders to manually analyze torrents of text to help advance social scientists’ understanding of the practices of different populations of interest through textual data. Therefore, computational methods offer significant benefits to help augment human capabilities to explore massive amounts of text in more complex ways for theory generation and theory testing. Content analysis can be framed as a text classification problem, where each text segment is labeled based on a predetermined set </context>
<context position="5777" citStr="Dönmez et al., 2005" startWordPosition="888" endWordPosition="891">coding qualitative data, we have to first understand the nature and complexity of content analysis projects in social science research. Our pilot case study, an investigation of leadership behaviors exhibited in emails from a FLOSS development project (Misiolek, Crowston, &amp; Seymour, 2012), reveals that it is common for researchers to use a multidimensional coding scheme consisting of a significant number of codes in their research inquiry. Previous work has shown that not all dimensions in a multidimensional coding scheme could be applied fully automatically with acceptable level of accuracy (Dönmez et al., 2005) but little is known if it is possible at all to train an optimal model for all codes using the same combination of features. Also, the distribution of codes is often times uneven with some rarely occurring codes having only few positive examples in the gold standard corpus. This paper presents our attempt in optimizing the initial ML model through feature exploration using gold standard data created from a multidimensional coding scheme, including codes that suffer from sparseness of positive examples. Specifically, our study is guided by two research questions: a) How can features for an ini</context>
</contexts>
<marker>Dönmez, Rosé, Stegmann, Weinberger, Fischer, 2005</marker>
<rawString>Dönmez, P., Rosé, C., Stegmann, K., Weinberger, A., &amp; Fischer, F. (2005). Supporting CSCL with automatic corpus analysis technology. In Proceedings of 2005 Conference on Computer Support for Collaborative Learning (pp. 125–134).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Evans</author>
<author>W McIntosh</author>
<author>J Lin</author>
<author>C Cates</author>
</authors>
<title>Recounting the courts? Applying automated content analysis to enhance empirical legal research.</title>
<date>2007</date>
<journal>Journal of Empirical Legal Studies,</journal>
<volume>4</volume>
<issue>4</issue>
<pages>1039</pages>
<contexts>
<context position="3405" citStr="Evans, McIntosh, Lin, &amp; Cates, 2007" startWordPosition="512" endWordPosition="517">l methods offer significant benefits to help augment human capabilities to explore massive amounts of text in more complex ways for theory generation and theory testing. Content analysis can be framed as a text classification problem, where each text segment is labeled based on a predetermined set of categories or codes. Full automation of content analysis is still far from being perfect (Grimmer &amp; Stewart, 2013). The accuracy of current automatic approaches on the best performing codes in social science research ranges from 60-90% (Broadwell et al., 2013; Crowston, Allen, &amp; Heckman, 2012; M. Evans, McIntosh, Lin, &amp; Cates, 2007; Ishita, Oard, Fleischmann, Cheng, &amp; Templeton, 2010; Zhu, Kraut, Wang, &amp; Kittur, 2011). While the potential of automatic content analysis is promising, computational methods should not be viewed as a replacement for the role of the primary researcher in the careful interpretation of text. Rather, the computers’ pattern recognition capabilities can be leveraged to seek out the most likely examples for each code of interest, thus reducing the amount of texts researchers have to read and process. We propose a semi-automatic method that promotes a close human-computer partnership for content ana</context>
</contexts>
<marker>Evans, McIntosh, Lin, Cates, 2007</marker>
<rawString>Evans, M., McIntosh, W., Lin, J., &amp; Cates, C. (2007). Recounting the courts? Applying automated content analysis to enhance empirical legal research. Journal of Empirical Legal Studies, 4(4), 1007– 1039.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Evans</author>
</authors>
<title>Computer-supported content analysis: Trends, tools, and techniques.</title>
<date>1996</date>
<journal>Social Science Computer Review,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>269--279</pages>
<contexts>
<context position="2211" citStr="Evans, 1996" startWordPosition="331" endWordPosition="332">, a technique for finding evidence of concepts of theoretical interest through text, is an increasingly popular technique social scientists use in their research investigations. In the process commonly known as “coding”, social scientists often have to painstakingly comb through large quantities of natural language corpora to annotate text segments (e.g., phrase, sentence, and paragraphs) with codes exhibiting the concepts of interest (Miles &amp; Huberman, 1994). Analyzing textual data is very labor-intensive, time-consuming, and is often limited to the capabilities of individual researchers (W. Evans, 1996). The coding process becomes even more demanding as the complexity of the project increases especially in the case of attempting to apply a multidimensional coding scheme with a significant number of codes (Dönmez, Rosé, Stegmann, Weinberger, &amp; Fischer, 2005). With the proliferation and availability of digital texts, it is challenging, if not impossible, for human coders to manually analyze torrents of text to help advance social scientists’ understanding of the practices of different populations of interest through textual data. Therefore, computational methods offer significant benefits to h</context>
</contexts>
<marker>Evans, 1996</marker>
<rawString>Evans, W. (1996). Computer-supported content analysis: Trends, tools, and techniques. Social Science Computer Review, 14(3), 269–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grimmer</author>
<author>B M Stewart</author>
</authors>
<title>Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis,</title>
<date>2013</date>
<volume>21</volume>
<issue>3</issue>
<pages>267--297</pages>
<contexts>
<context position="3186" citStr="Grimmer &amp; Stewart, 2013" startWordPosition="479" endWordPosition="482">ible, for human coders to manually analyze torrents of text to help advance social scientists’ understanding of the practices of different populations of interest through textual data. Therefore, computational methods offer significant benefits to help augment human capabilities to explore massive amounts of text in more complex ways for theory generation and theory testing. Content analysis can be framed as a text classification problem, where each text segment is labeled based on a predetermined set of categories or codes. Full automation of content analysis is still far from being perfect (Grimmer &amp; Stewart, 2013). The accuracy of current automatic approaches on the best performing codes in social science research ranges from 60-90% (Broadwell et al., 2013; Crowston, Allen, &amp; Heckman, 2012; M. Evans, McIntosh, Lin, &amp; Cates, 2007; Ishita, Oard, Fleischmann, Cheng, &amp; Templeton, 2010; Zhu, Kraut, Wang, &amp; Kittur, 2011). While the potential of automatic content analysis is promising, computational methods should not be viewed as a replacement for the role of the primary researcher in the careful interpretation of text. Rather, the computers’ pattern recognition capabilities can be leveraged to seek out the </context>
</contexts>
<marker>Grimmer, Stewart, 2013</marker>
<rawString>Grimmer, J., &amp; Stewart, B. M. (2013). Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis, 21(3), 267–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ishita</author>
<author>D W Oard</author>
<author>K R Fleischmann</author>
<author>A-S Cheng</author>
<author>T C Templeton</author>
</authors>
<title>Investigating multi-label classification for human values.</title>
<date>2010</date>
<booktitle>Proceedings of the American Society for Information Science and Technology,</booktitle>
<volume>47</volume>
<issue>1</issue>
<pages>1--4</pages>
<contexts>
<context position="3458" citStr="Ishita, Oard, Fleischmann, Cheng, &amp; Templeton, 2010" startWordPosition="518" endWordPosition="524">to help augment human capabilities to explore massive amounts of text in more complex ways for theory generation and theory testing. Content analysis can be framed as a text classification problem, where each text segment is labeled based on a predetermined set of categories or codes. Full automation of content analysis is still far from being perfect (Grimmer &amp; Stewart, 2013). The accuracy of current automatic approaches on the best performing codes in social science research ranges from 60-90% (Broadwell et al., 2013; Crowston, Allen, &amp; Heckman, 2012; M. Evans, McIntosh, Lin, &amp; Cates, 2007; Ishita, Oard, Fleischmann, Cheng, &amp; Templeton, 2010; Zhu, Kraut, Wang, &amp; Kittur, 2011). While the potential of automatic content analysis is promising, computational methods should not be viewed as a replacement for the role of the primary researcher in the careful interpretation of text. Rather, the computers’ pattern recognition capabilities can be leveraged to seek out the most likely examples for each code of interest, thus reducing the amount of texts researchers have to read and process. We propose a semi-automatic method that promotes a close human-computer partnership for content analysis. Machine learning (ML) is used to perform the f</context>
</contexts>
<marker>Ishita, Oard, Fleischmann, Cheng, Templeton, 2010</marker>
<rawString>Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng, A.-S., &amp; Templeton, T. C. (2010). Investigating multi-label classification for human values. Proceedings of the American Society for Information Science and Technology, 47(1), 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Y Liew</author>
<author>N McCracken</author>
<author>K Crowston</author>
</authors>
<title>Semi-automatic content analysis of qualitative data.</title>
<date>2014</date>
<booktitle>In iConference 2014 Proceedings</booktitle>
<pages>1128--1132</pages>
<contexts>
<context position="7915" citStr="Liew, McCracken, &amp; Crowston, 2014" startWordPosition="1226" endWordPosition="1230">be assigned more than one code. Framing the problem as a multi-label classification task, we trained a binary classification model for each code using support vector machine (SVM) with ten-fold cross-validation. This gold standard corpus consisted of 3,728 hand-coded sentences from 408 email messages. For the active learning setup, we tune the initial ML model for high recall since having the annotators pick out positive examples that have been incorrectly classified by the model is preferable to missing machine-annotated positive examples to be presented to human annotators for verification (Liew, McCracken, &amp; Crowston, 2014). Therefore, the initial ML model with low precision is acceptable. Category Features Content Unigram, bigram, pruning, tagging, lowercase, stopwords, stemming, part-ofspeech (POS) tags Syntactic Token count Orthographic Capitalization of first letter of a word, capitalization of entire word Word list Subjectivity words Semantic Role of sender (software developer or not) Table 1. Features for ML model. As shown in Table 1, we have selected general candidate features that have proven to work well across various text classification tasks, as well as one semantic feature specific to the context </context>
</contexts>
<marker>Liew, McCracken, Crowston, 2014</marker>
<rawString>Liew, J. S. Y., McCracken, N., &amp; Crowston, K. (2014). Semi-automatic content analysis of qualitative data. In iConference 2014 Proceedings (pp. 1128–1132).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Miles</author>
<author>A M Huberman</author>
</authors>
<title>Qualitative data analysis: An expanded sourcebook (2nd ed.).</title>
<date>1994</date>
<publisher>Sage.</publisher>
<contexts>
<context position="2062" citStr="Miles &amp; Huberman, 1994" startWordPosition="308" endWordPosition="311"> ML model using only a single combination of features for codes with at least 100 positive examples in the gold standard corpus. 1 Introduction Content analysis, a technique for finding evidence of concepts of theoretical interest through text, is an increasingly popular technique social scientists use in their research investigations. In the process commonly known as “coding”, social scientists often have to painstakingly comb through large quantities of natural language corpora to annotate text segments (e.g., phrase, sentence, and paragraphs) with codes exhibiting the concepts of interest (Miles &amp; Huberman, 1994). Analyzing textual data is very labor-intensive, time-consuming, and is often limited to the capabilities of individual researchers (W. Evans, 1996). The coding process becomes even more demanding as the complexity of the project increases especially in the case of attempting to apply a multidimensional coding scheme with a significant number of codes (Dönmez, Rosé, Stegmann, Weinberger, &amp; Fischer, 2005). With the proliferation and availability of digital texts, it is challenging, if not impossible, for human coders to manually analyze torrents of text to help advance social scientists’ under</context>
</contexts>
<marker>Miles, Huberman, 1994</marker>
<rawString>Miles, M. B., &amp; Huberman, A. M. (1994). Qualitative data analysis: An expanded sourcebook (2nd ed.). Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Misiolek</author>
<author>K Crowston</author>
<author>J Seymour</author>
</authors>
<title>Team dynamics in long-standing technologysupported virtual teams. Presented at the Academy of Management Annual Meeting, Organizational Behavior Division,</title>
<date>2012</date>
<location>Boston, MA.</location>
<contexts>
<context position="5445" citStr="Misiolek, Crowston, &amp; Seymour, 2012" startWordPosition="833" endWordPosition="838">human annotators. However, it is still equally important to optimize the creation of the initial ML model given less training data so that the model is able to capture most if not all positive examples, and filter out as many negative examples as possible for human annotators to correct. To effectively implement the active learning approach for coding qualitative data, we have to first understand the nature and complexity of content analysis projects in social science research. Our pilot case study, an investigation of leadership behaviors exhibited in emails from a FLOSS development project (Misiolek, Crowston, &amp; Seymour, 2012), reveals that it is common for researchers to use a multidimensional coding scheme consisting of a significant number of codes in their research inquiry. Previous work has shown that not all dimensions in a multidimensional coding scheme could be applied fully automatically with acceptable level of accuracy (Dönmez et al., 2005) but little is known if it is possible at all to train an optimal model for all codes using the same combination of features. Also, the distribution of codes is often times uneven with some rarely occurring codes having only few positive examples in the gold standard </context>
<context position="6972" citStr="Misiolek et al., 2012" startWordPosition="1076" endWordPosition="1079">How can features for an initial machine learning model be optimized for all codes in a text classification problem based on multidimensional coding schemes? Is it possible to train a one-size-fits-all model for all codes using a single combination of features? b) Are certain features better suited for codes with sparse positive examples? 2 Machine Learning Experiments To optimize the initial machine learning model, we systematically ran multiple experiments using a gold standard corpus of emails from a free/libre/open-source software (FLOSS) development project coded for leadership behaviors (Misiolek et al., 2012). The coding scheme contained six dimensions: 1) social/relationship, 2) task process, 3) task substance, 4) dual process and substance, 5) change behaviors, and 6) networking. The number of codes for each dimension ranged from 1 to 14. There were a total of 35 codes in the coding scheme. Each sentence could be assigned more than one code. Framing the problem as a multi-label classification task, we trained a binary classification model for each code using support vector machine (SVM) with ten-fold cross-validation. This gold standard corpus consisted of 3,728 hand-coded sentences from 408 ema</context>
</contexts>
<marker>Misiolek, Crowston, Seymour, 2012</marker>
<rawString>Misiolek, N., Crowston, K., &amp; Seymour, J. (2012). Team dynamics in long-standing technologysupported virtual teams. Presented at the Academy of Management Annual Meeting, Organizational Behavior Division, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhu</author>
<author>R E Kraut</author>
<author>Y-C Wang</author>
<author>A Kittur</author>
</authors>
<title>Identifying shared leadership in Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</booktitle>
<pages>3431--3434</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="3492" citStr="Zhu, Kraut, Wang, &amp; Kittur, 2011" startWordPosition="525" endWordPosition="530"> amounts of text in more complex ways for theory generation and theory testing. Content analysis can be framed as a text classification problem, where each text segment is labeled based on a predetermined set of categories or codes. Full automation of content analysis is still far from being perfect (Grimmer &amp; Stewart, 2013). The accuracy of current automatic approaches on the best performing codes in social science research ranges from 60-90% (Broadwell et al., 2013; Crowston, Allen, &amp; Heckman, 2012; M. Evans, McIntosh, Lin, &amp; Cates, 2007; Ishita, Oard, Fleischmann, Cheng, &amp; Templeton, 2010; Zhu, Kraut, Wang, &amp; Kittur, 2011). While the potential of automatic content analysis is promising, computational methods should not be viewed as a replacement for the role of the primary researcher in the careful interpretation of text. Rather, the computers’ pattern recognition capabilities can be leveraged to seek out the most likely examples for each code of interest, thus reducing the amount of texts researchers have to read and process. We propose a semi-automatic method that promotes a close human-computer partnership for content analysis. Machine learning (ML) is used to perform the first pass of coding on the unlabel</context>
</contexts>
<marker>Zhu, Kraut, Wang, Kittur, 2011</marker>
<rawString>Zhu, H., Kraut, R. E., Wang, Y.-C., &amp; Kittur, A. (2011). Identifying shared leadership in Wikipedia. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 3431–3434). New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
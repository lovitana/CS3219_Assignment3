<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001181">
<title confidence="0.999521">
A Natural Language Instructor for pedestrian navigation based in
generation by selection
</title>
<author confidence="0.910398">
Santiago Avalos
</author>
<affiliation confidence="0.79234">
LIIS Group, FaMAF
</affiliation>
<address confidence="0.692959">
Universidad Nacional de C´ordoba
C´ordoba, Argentina
</address>
<email confidence="0.998432">
santiagoe.avalos@gmail.com
</email>
<sectionHeader confidence="0.993934" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999340111111111">
In this paper we describe a method for
developing a virtual instructor for pedes-
trian navigation based on real interactions
between a human instructor and a human
pedestrian. A virtual instructor is an agent
capable of fulfilling the role of a human
instructor, and its goal is to assist a pedes-
trian in the accomplishment of different
tasks within the context of a real city.
The instructor decides what to say using
a generation by selection algorithm, based
on a corpus of real interactions generated
within the world of interest. The instructor
is able to react to different requests by the
pedestrian. It is also aware of the pedes-
trian position with a certain degree of un-
certainty, and it can use different city land-
marks to guide him.
</bodyText>
<sectionHeader confidence="0.976666" genericHeader="categories and subject descriptors">
1 Introduction and previous work
</sectionHeader>
<bodyText confidence="0.99981455">
Virtual instructors are conversational agents that
help a user perform a task. These agents can be
useful for many purposes, such as language learn-
ing (Nunan, 2004), training in simulated envi-
ronments (Kim et al., 2009) and entertainment
(Dignum, 2012; Jan et al., 2009).
Navigation agents generate verbal route direc-
tions for users to go from point A to point B in
a given world. The wide variety of techniques to
accomplish this task, range from giving complete
route directions (all route information in a single
instruction), to full interactive dialogue systems
which give incremental instructions based on the
position of the pedestrian. Although it can recog-
nize pre-established written requests, the instruc-
tor presented in this work is not able to interpret
utterances from the pedestrian, leaving it unable to
generate a full dialogue. The instructor’s decisions
are based on the pedestrian actual task, his posi-
tion in the world, and the previous behavior from
</bodyText>
<note confidence="0.73923275">
Luciana Benotti
LIIS Group, FaMAF
Universidad Nacional de C´ordoba
C´ordoba, Argentina
</note>
<email confidence="0.966693">
luciana.benotti@gmail.com
</email>
<bodyText confidence="0.999910975609756">
different human instructors. In order to guide a
user while performing a task, an effective instruc-
tor must know how to describe what needs to be
done in a way that accounts for the nuances of
the virtual world and that is enough to engage the
trainee or gamer in the activity.
There are two main approaches toward automat-
ically producing instructions. One is the selection
approach, in which the task is to pick the appropri-
ate output from a corpus of possible outputs. The
other is the composition approach, in which the
output is dynamically assembled using some com-
position procedure, e.g. grammar rules.
The natural language generation algorithm used
in this work is a modified version of the generation
by selection method described in (Benotti and De-
nis, 2011).
The advantages of generation by selection are
many: it affords the use of complex and human-
like sentences, the system is not bound to use writ-
ten instructions (it may easily use recorded audio
clips, for example), and finally, no rule writing by
a dialogue expert or manual annotations is needed.
The disadvantage of generation by selection is that
the resulting dialogue may not be fully coherent
(Shawar and Atwell, 2003; Shawar and Atwell,
2005; Gandhe and Traum, 2007).
In previous work, the selection approach to
generation has been used in non task-oriented
conversational agents such as negotiating agents
(Gandhe and Traum, 2007), question answering
characters (Leuski et al., 2006) and virtual pa-
tients (Kenny et al., 2007). In the work pre-
sented in this paper, the conversational agent is
task-oriented.
In Section 2 we introduce the framework used
in the interaction between the navigation agent and
the human pedestrians. We discuss the creation of
the human interaction corpus and the method for
natural language generation in Section 3; And in
Section 4 we explain the evaluation methods and
</bodyText>
<page confidence="0.992744">
33
</page>
<note confidence="0.7319155">
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 33–37,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.85524">
the expected results.
</bodyText>
<sectionHeader confidence="0.849836" genericHeader="method">
2 The GRUVE framework
</sectionHeader>
<bodyText confidence="0.999793972222222">
One of the major problems in developing systems
that generate navigation instructions for pedestri-
ans is evaluating them with real users in the real
world. This evaluations are expensive, time con-
suming, and need to be carried out not just at the
end of the project but also during the development
cycle.
Consequently, there is a need for a common
platform to effectively compare the performances
of several verbal navigation systems developed by
different teams using a variety of techniques.
The GIVE challenge developed a 3D virtual in-
door environment for development and evaluation
of indoor pedestrian navigation instruction sys-
tems (Byron et al., 2007; Koller et al., 2007).
In this framework, users walk through a building
with rooms and corridors, and interact with the
world by pressing buttons. The user is guided by
a navigation system that generates route instruc-
tions.
The GRUVE framework presented in (Ja-
narthanam et al., 2012) is a web-based environ-
ment containing a simulated real world in which
users can simulate walking on the streets of real
cities whilst interacting with different navigation
systems. This system focus on providing a simu-
lated environment where people can look at land-
marks and navigate based on spatial and visual in-
structions provided to them. GRUVE also pro-
vides a embedded navigation agent, the Buddy
System, which can be used to test the framework.
Apart from the virtual environment in which they
are based an important difference between GIVE
and GRUVE is that, in GRUVE, there is a cer-
tain degree of uncertainty about the position of the
user.
</bodyText>
<figureCaption confidence="0.999384">
Figure 1: Snapshot of the GRUVE web-client.
</figureCaption>
<bodyText confidence="0.999941352941177">
GRUVE presents navigation tasks in a game-
world overlaid on top of the simulated real world.
The main task consists of a treasure hunting simi-
lar to the one presented in GIVE. In our work, we
use a modified version of the original framework,
in which the main task has been replaced by a set
of navigation tasks.
The web-client (see Figure 1) includes an inter-
action panel that lets the user interact with his nav-
igation system. In addition to user location infor-
mation, users can also interact with the navigation
system using a fixed set or written utterances. The
interaction panel provided to the user consists of a
GUI panel with buttons and drop-lists which can
be used to construct and send requests to the sys-
tem in form of abstract semantic representations
(dialogue actions).
</bodyText>
<sectionHeader confidence="0.960934" genericHeader="method">
3 The virtual instructor
</sectionHeader>
<bodyText confidence="0.999984615384615">
The virtual instructor is a natural language agent
that must help users reach a desired destination
within the virtual world. Our method for devel-
oping an instructor consists of two phases: an an-
notation phase and a selection phase. In Section
3.1 we describe the annotation phase. This is per-
formed only once, when the instructor is created,
and it consists of automatically generating a cor-
pus formed by associations between each instruc-
tion and the reaction to it. In Section 3.2 we de-
scribe how the utterance selection is performed ev-
ery time the virtual instructor generates an instruc-
tion.
</bodyText>
<subsectionHeader confidence="0.996062">
3.1 Annotation
</subsectionHeader>
<bodyText confidence="0.999977235294117">
As described in (Benotti and Denis, 2011), the cor-
pus consists in recorded interactions between two
people in two different roles: the Direction Giver
(DG), who has knowledge of how to perform the
task, and creates the instructions, and the Direc-
tion Follower (DF), who travels through the envi-
ronment following those instructions.
The representation of the virtual world is given
by a graph of nodes, each one representing an in-
tersection between two streets in the city. GRUVE
provides a planner that can calculate the optimal
path from any starting point to a selected desti-
nation (this plan consists in the list of nodes the
user must travel to reach the desired destination).
As the DF user walks through the environment, he
cannot change the world that surrounds him. This
simplifies the automatic annotation process, and
</bodyText>
<page confidence="0.995006">
34
</page>
<bodyText confidence="0.893845">
the logged atoms are:
</bodyText>
<listItem confidence="0.98477125">
• user position: latitude and longitude, indicat-
ing position relative to the world.
• user orientation: angle between 0-360, indi-
cating rotation of the point of view.
</listItem>
<bodyText confidence="0.999062888888889">
In order to define the reaction associated to each
utterance, it is enough to consider the position to
which the user arrives after an instruction has been
given, and before another one is requested. Nine
destinations within the city of Edinburgh were se-
lected to be the tasks to complete (the task is to
arrive to each destination, from a common starting
point, see Figure 2). Each pair of DG and DF had
to complete all tasks and record their progress.
</bodyText>
<figureCaption confidence="0.997479">
Figure 2: The 9 selected tasks .
</figureCaption>
<bodyText confidence="0.9994165">
For the creation of the corpus, a slightly mod-
ified version of the GRUVE wizards-desk was
used. This tool is connected to the GRUVE web-
client, and allows a human user to act as DF, gen-
erating instructions to assist the user in the com-
pletion of the task and monitoring his progression.
Each instruction generated by a DG was numbered
in order, in relation to each task. For example: if
the fifth instruction given by the third DG, while
performing the second task, was ”Go forward and
cross the square”, then that instruction was num-
bered as follows:
</bodyText>
<subsubsectionHeader confidence="0.985647">
5.3.2 − ”Go forward and cross the square”.
</subsubsectionHeader>
<bodyText confidence="0.999987814814815">
This notation was included to maintain the gener-
ation order between instructions (as the tasks were
given in an arbitrary specific order for each DG).
With last-generated, we refer to the instructions
that were generated in the last 3 runs of each DG.
This notion is needed to evaluate the effect of the
increasing knowledge of the city (this metric is ex-
plained in Section 4).
As discussed in (Benotti and Denis, 2011) mis-
interpreted instructions and corrections result in
clearly inappropriate instruction-reaction associa-
tions. Since we want to avoid any manual anno-
tation, but we also want to minimize the quantity
of errors inside the corpus, we decided to create
a first corpus in which the same person portraits
the roles of DG and DF. This allows us to elim-
inate the ambiguity of the instruction interpreta-
tion on the DF side, and eliminates correction in-
structions (instructions that are of no use for guid-
ance, but were made to correct a previous error
from the DG, or a wrong action from the DF).
Later on, each instruction in this corpus was per-
formed upon the virtual world by various others
users, their reactions compared to the original re-
action, and scored. For each task, only the instruc-
tions whose score exceeded an acceptance thresh-
old remained in the final corpus.
</bodyText>
<subsectionHeader confidence="0.996038">
3.2 Instruction selection
</subsectionHeader>
<bodyText confidence="0.999428875">
The instruction selection algorithm, displayed in
Algorithm 1 consists in finding in the corpus the
set of candidate utterances C for the current task
plan P, which is the sequence of actions that needs
to be executed in the current state of the virtual
world in order to complete the task. We use the
planner included in GRUVE to create P. We de-
fine:
</bodyText>
<equation confidence="0.972609">
C = {U E Corpus  |P starts with U.Reaction}
</equation>
<bodyText confidence="0.9999725">
In other words, an utterance U belongs to C if the
first action of the current plan P exactly matches
the reaction associated to the utterance U. When-
ever the plan P changes, as a result of the actions
of the DF, we call the selection algorithm in order
to regenerate the set of candidate utterances C.
</bodyText>
<equation confidence="0.79010825">
Algorithm 1 Selection Algorithm
C +— 0
action +— nextAction(currentObjective)
for all Utterance U E Corpus do
if action = U.Reaction then
C +— C U U
end if
end for
</equation>
<bodyText confidence="0.99964">
All the utterances that pass this test are consid-
ered paraphrases and hence suitable in the current
context. Given a set of candidate paraphrases, one
has to consider two cases: the most frequent case
when there are several candidates and the possible
case when there is no candidate.
</bodyText>
<page confidence="0.995945">
35
</page>
<listItem confidence="0.9302864">
• No candidate available: If no instruction is
selected because the current plan cannot be
matched with any existing reaction, a default,
neutral, instruction ”go” is uttered.
• Multiple candidates available: When multi-
ple paraphrases are available, the agent must
select one to transmit to the user. In this case,
the algorithm selects one from the set of the
last-generated instructions for the task (see
Section 3.1).
</listItem>
<sectionHeader confidence="0.923058" genericHeader="evaluation">
4 Evaluation and expected results
</sectionHeader>
<bodyText confidence="0.9998728">
Is this section we present the metrics and evalua-
tion process that will be performed to test the vir-
tual instructor presented in Section 3, which was
generated using the dialogue model algorithm in-
troduced in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.985493">
4.1 Objective metrics
</subsectionHeader>
<bodyText confidence="0.994708">
The objective metrics are summarized below:
</bodyText>
<listItem confidence="0.9999818">
• Task success: successful runs.
• Canceled: runs not finished.
• Lost: runs finished but failed.
• Time (sec): average for successful runs.
• Utterances: average per successful run.
</listItem>
<bodyText confidence="0.9992698">
With this metrics, we will compare 3 systems:
agents A, B and C.
Agent A is the GRUVE buddy system, which
is provider by the GRUVE Challenge organizers
as a baseline. Agent B consists of our virtual in-
structor, configured to select a random instruction
when presented with multiple candidates (see Sec-
tion 3.1). Agent C is also our virtual instructor, but
when presented with several candidates, C selects
a candidate who is also part of the last-generated
set. As each task was completed in different or-
der by each DG when the corpus was created, it
is expected that in every set of candidates, the
most late-generated instructions were created with
greater knowledge of the city.
</bodyText>
<subsectionHeader confidence="0.99272">
4.2 Subjective metrics
</subsectionHeader>
<bodyText confidence="0.860591086956522">
The subjective measures will be obtained from re-
sponses to a questionnaire given to each user at the
end of the evaluation, based partially on the GIVE-
2 Challenge questionnaire (Koller et al., 2010). It
ask users to rate different statements about the sys-
tem using a 0 to 10 scale.
The questionnaire will include 19 subjective
metrics presented below:
Q1: The system used words and phrases that
were easy to understand.
Q2: I had to re-read instructions to understand
what I needed to do.
Q3: The system gave me useful feedback about my
progress.
Q4: I was confused about what to do next.
Q5: I was confused about which direction to go
in.
Q6: I had no difficulty with identifying the objects
the system described for me.
Q7: The system gave me a lot of unnecessary
Information.
Q8: The system gave me too much information all
at once.
</bodyText>
<reference confidence="0.9652483125">
Q9: The system immediately offered help when I
was in trouble.
Q10: The system sent instructions too late.
Q11: The systems instructions were delivered too
early.
Q12: The systems instructions were clearly
worded.
Q13: The systems instructions sounded robotic.
Q14: The systems instructions were repetitive.
Q15: I lost track of time while solving the overall
task.
Q16: I enjoyed solving the overall task.
Q17: Interacting with the system was really
annoying.
Q18: The system was very friendly.
Q19: I felt I could trust the systems instructions.
</reference>
<bodyText confidence="0.9982456">
Metrics Q1 to Q12 assess the effectiveness and
reliability of instructions, while metrics Q13 to
Q19 are intended to assess the naturalness of the
instructions, as well as the immersion and engage-
ment of the interaction.
</bodyText>
<subsectionHeader confidence="0.995456">
4.3 Expected results
</subsectionHeader>
<bodyText confidence="0.99995025">
Based on the results obtained by (Benotti and De-
nis, 2011) in the GIVE-2 Challenge, we expect a
good rate of successful runs for the agent. Further-
more, the most interesting part of the evaluation
resides in the comparison between agents B and C.
We expect that the different selection methods of
this agents, when presented with multiple instruc-
tion candidates, can provide information about the
form in which the level of knowledge of the vir-
tual world or environment modifies the capacity
of a Direction Giver to create correct, and useful,
instructions.
</bodyText>
<page confidence="0.997512">
36
</page>
<sectionHeader confidence="0.990195" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999740887323944">
Luciana Benotti and Alexandre Denis. 2011. Giv-
ing instructions in virtual environments by corpus
based selection. In Proceedings of the SIGDIAL
2011 Conference, SIGDIAL ’11, pages 68–77. As-
sociation for Computational Linguistics.
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating instructions in vir-
tual environments (give): A challenge and evalua-
tion testbed for nlg. In Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation
in Natural Language Generation.
Frank Dignum. 2012. Agents for games and simula-
tions. Autonomous Agents and Multi-Agent Systems,
24(2):217–220, March.
S. Gandhe and D. Traum. 2007. First steps toward
dialogue modelling from an un-annotated human-
human corpus. In IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systemss.
Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie,
and David Traum. 2009. A virtual tour guide for
virtual worlds. In Proceedings of the 9th Interna-
tional Conference on Intelligent Virtual Agents, IVA
’09, pages 372–378, Berlin, Heidelberg. Springer-
Verlag.
Srinivasan Janarthanam, Oliver Lemon, and Xingkun
Liu. 2012. A web-based evaluation framework for
spatial instruction-giving systems. In Proceedings
of the ACL 2012 System Demonstrations, ACL ’12,
pages 49–54. Association for Computational Lin-
guistics.
Patrick Kenny, Thomas D. Parsons, Jonathan Gratch,
Anton Leuski, and Albert A. Rizzo. 2007. Vir-
tual patients for clinical therapist skills training. In
Proceedings of the 7th International Conference on
Intelligent Virtual Agents, IVA ’07, pages 197–210,
Berlin, Heidelberg. Springer-Verlag.
Julia M. Kim, Randall W. Hill, Jr., Paula J. Durlach,
H. Chad Lane, Eric Forbell, Mark Core, Stacy
Marsella, David Pynadath, and John Hart. 2009. Bi-
lat: A game-based environment for practicing nego-
tiation in a cultural context. Int. J. Artif. Intell. Ed.,
19(3):289–308, August.
A. Koller, J. Moore, B. Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared task proposal: Instruction giving in virtual
worlds. In In Workshop on Shared Tasks and Com-
parative Evaluation in Natural Language Genera-
tion.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the
second nlg challenge on generating instructions in
virtual environments (give-2). In Proceedings of
the 6th International Natural Language Generation
Conference, INLG ’10, pages 243–250. Association
for Computational Linguistics.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue, Sig-
DIAL ’06, pages 18–27. Association for Computa-
tional Linguistics.
David Nunan. 2004. Task-based language teaching.
University Press, Cambridge.
B.A. Shawar and E. Atwell. 2003. Using dialogue
corpora to retrain a chatbot system. In Proceedings
of the Corpus Linguistics Conference, pages 681–
690.
B.A. Shawar and E. Atwell. 2005. Using corpora
in machine-learning chatbot systems. International
Journal of Corpus Linguistics, 10:489–516.
</reference>
<page confidence="0.999611">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523475">
<title confidence="0.991648">A Natural Language Instructor for pedestrian navigation based generation by selection</title>
<author confidence="0.981635">Santiago</author>
<affiliation confidence="0.863007666666667">LIIS Group, Universidad Nacional de C´ordoba,</affiliation>
<email confidence="0.9999">santiagoe.avalos@gmail.com</email>
<abstract confidence="0.994541947368421">In this paper we describe a method for developing a virtual instructor for pedestrian navigation based on real interactions between a human instructor and a human pedestrian. A virtual instructor is an agent capable of fulfilling the role of a human instructor, and its goal is to assist a pedestrian in the accomplishment of different tasks within the context of a real city. The instructor decides what to say using a generation by selection algorithm, based on a corpus of real interactions generated within the world of interest. The instructor is able to react to different requests by the pedestrian. It is also aware of the pedestrian position with a certain degree of uncertainty, and it can use different city landmarks to guide him.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Q9: The system immediately offered help when I was in trouble.</title>
<marker></marker>
<rawString>Q9: The system immediately offered help when I was in trouble.</rawString>
</citation>
<citation valid="false">
<title>Q10: The system sent instructions too late. Q11: The systems instructions were delivered too early.</title>
<marker></marker>
<rawString>Q10: The system sent instructions too late. Q11: The systems instructions were delivered too early.</rawString>
</citation>
<citation valid="false">
<title>Q12: The systems instructions were clearly worded.</title>
<marker></marker>
<rawString>Q12: The systems instructions were clearly worded.</rawString>
</citation>
<citation valid="false">
<title>Q13: The systems instructions sounded robotic. Q14: The systems instructions were repetitive. Q15: I lost track of time while solving the overall task.</title>
<marker></marker>
<rawString>Q13: The systems instructions sounded robotic. Q14: The systems instructions were repetitive. Q15: I lost track of time while solving the overall task.</rawString>
</citation>
<citation valid="false">
<title>Q16: I enjoyed solving the overall task. Q17: Interacting with the system was really annoying.</title>
<marker></marker>
<rawString>Q16: I enjoyed solving the overall task. Q17: Interacting with the system was really annoying.</rawString>
</citation>
<citation valid="false">
<title>Q18: The system was very friendly. Q19: I felt I could trust the systems instructions.</title>
<marker></marker>
<rawString>Q18: The system was very friendly. Q19: I felt I could trust the systems instructions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciana Benotti</author>
<author>Alexandre Denis</author>
</authors>
<title>Giving instructions in virtual environments by corpus based selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGDIAL 2011 Conference, SIGDIAL ’11,</booktitle>
<pages>68--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2840" citStr="Benotti and Denis, 2011" startWordPosition="453" endWordPosition="457">be done in a way that accounts for the nuances of the virtual world and that is enough to engage the trainee or gamer in the activity. There are two main approaches toward automatically producing instructions. One is the selection approach, in which the task is to pick the appropriate output from a corpus of possible outputs. The other is the composition approach, in which the output is dynamically assembled using some composition procedure, e.g. grammar rules. The natural language generation algorithm used in this work is a modified version of the generation by selection method described in (Benotti and Denis, 2011). The advantages of generation by selection are many: it affords the use of complex and humanlike sentences, the system is not bound to use written instructions (it may easily use recorded audio clips, for example), and finally, no rule writing by a dialogue expert or manual annotations is needed. The disadvantage of generation by selection is that the resulting dialogue may not be fully coherent (Shawar and Atwell, 2003; Shawar and Atwell, 2005; Gandhe and Traum, 2007). In previous work, the selection approach to generation has been used in non task-oriented conversational agents such as nego</context>
<context position="7265" citStr="Benotti and Denis, 2011" startWordPosition="1186" endWordPosition="1189">natural language agent that must help users reach a desired destination within the virtual world. Our method for developing an instructor consists of two phases: an annotation phase and a selection phase. In Section 3.1 we describe the annotation phase. This is performed only once, when the instructor is created, and it consists of automatically generating a corpus formed by associations between each instruction and the reaction to it. In Section 3.2 we describe how the utterance selection is performed every time the virtual instructor generates an instruction. 3.1 Annotation As described in (Benotti and Denis, 2011), the corpus consists in recorded interactions between two people in two different roles: the Direction Giver (DG), who has knowledge of how to perform the task, and creates the instructions, and the Direction Follower (DF), who travels through the environment following those instructions. The representation of the virtual world is given by a graph of nodes, each one representing an intersection between two streets in the city. GRUVE provides a planner that can calculate the optimal path from any starting point to a selected destination (this plan consists in the list of nodes the user must tr</context>
<context position="9744" citStr="Benotti and Denis, 2011" startWordPosition="1616" endWordPosition="1619"> fifth instruction given by the third DG, while performing the second task, was ”Go forward and cross the square”, then that instruction was numbered as follows: 5.3.2 − ”Go forward and cross the square”. This notation was included to maintain the generation order between instructions (as the tasks were given in an arbitrary specific order for each DG). With last-generated, we refer to the instructions that were generated in the last 3 runs of each DG. This notion is needed to evaluate the effect of the increasing knowledge of the city (this metric is explained in Section 4). As discussed in (Benotti and Denis, 2011) misinterpreted instructions and corrections result in clearly inappropriate instruction-reaction associations. Since we want to avoid any manual annotation, but we also want to minimize the quantity of errors inside the corpus, we decided to create a first corpus in which the same person portraits the roles of DG and DF. This allows us to eliminate the ambiguity of the instruction interpretation on the DF side, and eliminates correction instructions (instructions that are of no use for guidance, but were made to correct a previous error from the DG, or a wrong action from the DF). Later on, e</context>
</contexts>
<marker>Benotti, Denis, 2011</marker>
<rawString>Luciana Benotti and Alexandre Denis. 2011. Giving instructions in virtual environments by corpus based selection. In Proceedings of the SIGDIAL 2011 Conference, SIGDIAL ’11, pages 68–77. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Byron</author>
<author>A Koller</author>
<author>J Oberlander</author>
<author>L Stoia</author>
<author>K Striegnitz</author>
</authors>
<title>Generating instructions in virtual environments (give): A challenge and evaluation testbed for nlg.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation.</booktitle>
<contexts>
<context position="4825" citStr="Byron et al., 2007" startWordPosition="771" endWordPosition="774"> systems that generate navigation instructions for pedestrians is evaluating them with real users in the real world. This evaluations are expensive, time consuming, and need to be carried out not just at the end of the project but also during the development cycle. Consequently, there is a need for a common platform to effectively compare the performances of several verbal navigation systems developed by different teams using a variety of techniques. The GIVE challenge developed a 3D virtual indoor environment for development and evaluation of indoor pedestrian navigation instruction systems (Byron et al., 2007; Koller et al., 2007). In this framework, users walk through a building with rooms and corridors, and interact with the world by pressing buttons. The user is guided by a navigation system that generates route instructions. The GRUVE framework presented in (Janarthanam et al., 2012) is a web-based environment containing a simulated real world in which users can simulate walking on the streets of real cities whilst interacting with different navigation systems. This system focus on providing a simulated environment where people can look at landmarks and navigate based on spatial and visual ins</context>
</contexts>
<marker>Byron, Koller, Oberlander, Stoia, Striegnitz, 2007</marker>
<rawString>D. Byron, A. Koller, J. Oberlander, L. Stoia, and K. Striegnitz. 2007. Generating instructions in virtual environments (give): A challenge and evaluation testbed for nlg. In Proceedings of the Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Dignum</author>
</authors>
<title>Agents for games and simulations.</title>
<date>2012</date>
<journal>Autonomous Agents and Multi-Agent Systems,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="1239" citStr="Dignum, 2012" startWordPosition="197" endWordPosition="198">ng a generation by selection algorithm, based on a corpus of real interactions generated within the world of interest. The instructor is able to react to different requests by the pedestrian. It is also aware of the pedestrian position with a certain degree of uncertainty, and it can use different city landmarks to guide him. 1 Introduction and previous work Virtual instructors are conversational agents that help a user perform a task. These agents can be useful for many purposes, such as language learning (Nunan, 2004), training in simulated environments (Kim et al., 2009) and entertainment (Dignum, 2012; Jan et al., 2009). Navigation agents generate verbal route directions for users to go from point A to point B in a given world. The wide variety of techniques to accomplish this task, range from giving complete route directions (all route information in a single instruction), to full interactive dialogue systems which give incremental instructions based on the position of the pedestrian. Although it can recognize pre-established written requests, the instructor presented in this work is not able to interpret utterances from the pedestrian, leaving it unable to generate a full dialogue. The i</context>
</contexts>
<marker>Dignum, 2012</marker>
<rawString>Frank Dignum. 2012. Agents for games and simulations. Autonomous Agents and Multi-Agent Systems, 24(2):217–220, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gandhe</author>
<author>D Traum</author>
</authors>
<title>First steps toward dialogue modelling from an un-annotated humanhuman corpus.</title>
<date>2007</date>
<booktitle>In IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systemss.</booktitle>
<contexts>
<context position="3314" citStr="Gandhe and Traum, 2007" startWordPosition="533" endWordPosition="536">al language generation algorithm used in this work is a modified version of the generation by selection method described in (Benotti and Denis, 2011). The advantages of generation by selection are many: it affords the use of complex and humanlike sentences, the system is not bound to use written instructions (it may easily use recorded audio clips, for example), and finally, no rule writing by a dialogue expert or manual annotations is needed. The disadvantage of generation by selection is that the resulting dialogue may not be fully coherent (Shawar and Atwell, 2003; Shawar and Atwell, 2005; Gandhe and Traum, 2007). In previous work, the selection approach to generation has been used in non task-oriented conversational agents such as negotiating agents (Gandhe and Traum, 2007), question answering characters (Leuski et al., 2006) and virtual patients (Kenny et al., 2007). In the work presented in this paper, the conversational agent is task-oriented. In Section 2 we introduce the framework used in the interaction between the navigation agent and the human pedestrians. We discuss the creation of the human interaction corpus and the method for natural language generation in Section 3; And in Section 4 we e</context>
</contexts>
<marker>Gandhe, Traum, 2007</marker>
<rawString>S. Gandhe and D. Traum. 2007. First steps toward dialogue modelling from an un-annotated humanhuman corpus. In IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systemss.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dusan Jan</author>
<author>Antonio Roque</author>
<author>Anton Leuski</author>
<author>Jacki Morie</author>
<author>David Traum</author>
</authors>
<title>A virtual tour guide for virtual worlds.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Virtual Agents, IVA ’09,</booktitle>
<pages>372--378</pages>
<publisher>SpringerVerlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1258" citStr="Jan et al., 2009" startWordPosition="199" endWordPosition="202">n by selection algorithm, based on a corpus of real interactions generated within the world of interest. The instructor is able to react to different requests by the pedestrian. It is also aware of the pedestrian position with a certain degree of uncertainty, and it can use different city landmarks to guide him. 1 Introduction and previous work Virtual instructors are conversational agents that help a user perform a task. These agents can be useful for many purposes, such as language learning (Nunan, 2004), training in simulated environments (Kim et al., 2009) and entertainment (Dignum, 2012; Jan et al., 2009). Navigation agents generate verbal route directions for users to go from point A to point B in a given world. The wide variety of techniques to accomplish this task, range from giving complete route directions (all route information in a single instruction), to full interactive dialogue systems which give incremental instructions based on the position of the pedestrian. Although it can recognize pre-established written requests, the instructor presented in this work is not able to interpret utterances from the pedestrian, leaving it unable to generate a full dialogue. The instructor’s decisio</context>
</contexts>
<marker>Jan, Roque, Leuski, Morie, Traum, 2009</marker>
<rawString>Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie, and David Traum. 2009. A virtual tour guide for virtual worlds. In Proceedings of the 9th International Conference on Intelligent Virtual Agents, IVA ’09, pages 372–378, Berlin, Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>A web-based evaluation framework for spatial instruction-giving systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations, ACL ’12,</booktitle>
<pages>49--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5109" citStr="Janarthanam et al., 2012" startWordPosition="817" endWordPosition="821">y, there is a need for a common platform to effectively compare the performances of several verbal navigation systems developed by different teams using a variety of techniques. The GIVE challenge developed a 3D virtual indoor environment for development and evaluation of indoor pedestrian navigation instruction systems (Byron et al., 2007; Koller et al., 2007). In this framework, users walk through a building with rooms and corridors, and interact with the world by pressing buttons. The user is guided by a navigation system that generates route instructions. The GRUVE framework presented in (Janarthanam et al., 2012) is a web-based environment containing a simulated real world in which users can simulate walking on the streets of real cities whilst interacting with different navigation systems. This system focus on providing a simulated environment where people can look at landmarks and navigate based on spatial and visual instructions provided to them. GRUVE also provides a embedded navigation agent, the Buddy System, which can be used to test the framework. Apart from the virtual environment in which they are based an important difference between GIVE and GRUVE is that, in GRUVE, there is a certain degr</context>
</contexts>
<marker>Janarthanam, Lemon, Liu, 2012</marker>
<rawString>Srinivasan Janarthanam, Oliver Lemon, and Xingkun Liu. 2012. A web-based evaluation framework for spatial instruction-giving systems. In Proceedings of the ACL 2012 System Demonstrations, ACL ’12, pages 49–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Kenny</author>
<author>Thomas D Parsons</author>
<author>Jonathan Gratch</author>
<author>Anton Leuski</author>
<author>Albert A Rizzo</author>
</authors>
<title>Virtual patients for clinical therapist skills training.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Conference on Intelligent Virtual Agents, IVA ’07,</booktitle>
<pages>197--210</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="3574" citStr="Kenny et al., 2007" startWordPosition="572" endWordPosition="575">stem is not bound to use written instructions (it may easily use recorded audio clips, for example), and finally, no rule writing by a dialogue expert or manual annotations is needed. The disadvantage of generation by selection is that the resulting dialogue may not be fully coherent (Shawar and Atwell, 2003; Shawar and Atwell, 2005; Gandhe and Traum, 2007). In previous work, the selection approach to generation has been used in non task-oriented conversational agents such as negotiating agents (Gandhe and Traum, 2007), question answering characters (Leuski et al., 2006) and virtual patients (Kenny et al., 2007). In the work presented in this paper, the conversational agent is task-oriented. In Section 2 we introduce the framework used in the interaction between the navigation agent and the human pedestrians. We discuss the creation of the human interaction corpus and the method for natural language generation in Section 3; And in Section 4 we explain the evaluation methods and 33 Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 33–37, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics the expected results. 2 The GRUVE framework One of</context>
</contexts>
<marker>Kenny, Parsons, Gratch, Leuski, Rizzo, 2007</marker>
<rawString>Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, Anton Leuski, and Albert A. Rizzo. 2007. Virtual patients for clinical therapist skills training. In Proceedings of the 7th International Conference on Intelligent Virtual Agents, IVA ’07, pages 197–210, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia M Kim</author>
<author>Randall W Hill</author>
<author>Paula J Durlach</author>
<author>H Chad Lane</author>
<author>Eric Forbell</author>
<author>Mark Core</author>
<author>Stacy Marsella</author>
<author>David Pynadath</author>
<author>John Hart</author>
</authors>
<title>Bilat: A game-based environment for practicing negotiation in a cultural context.</title>
<date>2009</date>
<journal>Int. J. Artif. Intell. Ed.,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="1207" citStr="Kim et al., 2009" startWordPosition="191" endWordPosition="194">he instructor decides what to say using a generation by selection algorithm, based on a corpus of real interactions generated within the world of interest. The instructor is able to react to different requests by the pedestrian. It is also aware of the pedestrian position with a certain degree of uncertainty, and it can use different city landmarks to guide him. 1 Introduction and previous work Virtual instructors are conversational agents that help a user perform a task. These agents can be useful for many purposes, such as language learning (Nunan, 2004), training in simulated environments (Kim et al., 2009) and entertainment (Dignum, 2012; Jan et al., 2009). Navigation agents generate verbal route directions for users to go from point A to point B in a given world. The wide variety of techniques to accomplish this task, range from giving complete route directions (all route information in a single instruction), to full interactive dialogue systems which give incremental instructions based on the position of the pedestrian. Although it can recognize pre-established written requests, the instructor presented in this work is not able to interpret utterances from the pedestrian, leaving it unable to</context>
</contexts>
<marker>Kim, Hill, Durlach, Lane, Forbell, Core, Marsella, Pynadath, Hart, 2009</marker>
<rawString>Julia M. Kim, Randall W. Hill, Jr., Paula J. Durlach, H. Chad Lane, Eric Forbell, Mark Core, Stacy Marsella, David Pynadath, and John Hart. 2009. Bilat: A game-based environment for practicing negotiation in a cultural context. Int. J. Artif. Intell. Ed., 19(3):289–308, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>J Moore</author>
<author>B Eugenio</author>
<author>J Lester</author>
<author>L Stoia</author>
<author>D Byron</author>
<author>J Oberlander</author>
<author>K Striegnitz</author>
</authors>
<title>Shared task proposal: Instruction giving in virtual worlds.</title>
<date>2007</date>
<booktitle>In In Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation.</booktitle>
<contexts>
<context position="4847" citStr="Koller et al., 2007" startWordPosition="775" endWordPosition="778">te navigation instructions for pedestrians is evaluating them with real users in the real world. This evaluations are expensive, time consuming, and need to be carried out not just at the end of the project but also during the development cycle. Consequently, there is a need for a common platform to effectively compare the performances of several verbal navigation systems developed by different teams using a variety of techniques. The GIVE challenge developed a 3D virtual indoor environment for development and evaluation of indoor pedestrian navigation instruction systems (Byron et al., 2007; Koller et al., 2007). In this framework, users walk through a building with rooms and corridors, and interact with the world by pressing buttons. The user is guided by a navigation system that generates route instructions. The GRUVE framework presented in (Janarthanam et al., 2012) is a web-based environment containing a simulated real world in which users can simulate walking on the streets of real cities whilst interacting with different navigation systems. This system focus on providing a simulated environment where people can look at landmarks and navigate based on spatial and visual instructions provided to </context>
</contexts>
<marker>Koller, Moore, Eugenio, Lester, Stoia, Byron, Oberlander, Striegnitz, 2007</marker>
<rawString>A. Koller, J. Moore, B. Eugenio, J. Lester, L. Stoia, D. Byron, J. Oberlander, and K. Striegnitz. 2007. Shared task proposal: Instruction giving in virtual worlds. In In Workshop on Shared Tasks and Comparative Evaluation in Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Andrew Gargett</author>
<author>Donna Byron</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
<author>Jon Oberlander</author>
</authors>
<title>Report on the second nlg challenge on generating instructions in virtual environments (give-2).</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference, INLG ’10,</booktitle>
<pages>243--250</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13617" citStr="Koller et al., 2010" startWordPosition="2280" endWordPosition="2283">andidates (see Section 3.1). Agent C is also our virtual instructor, but when presented with several candidates, C selects a candidate who is also part of the last-generated set. As each task was completed in different order by each DG when the corpus was created, it is expected that in every set of candidates, the most late-generated instructions were created with greater knowledge of the city. 4.2 Subjective metrics The subjective measures will be obtained from responses to a questionnaire given to each user at the end of the evaluation, based partially on the GIVE2 Challenge questionnaire (Koller et al., 2010). It ask users to rate different statements about the system using a 0 to 10 scale. The questionnaire will include 19 subjective metrics presented below: Q1: The system used words and phrases that were easy to understand. Q2: I had to re-read instructions to understand what I needed to do. Q3: The system gave me useful feedback about my progress. Q4: I was confused about what to do next. Q5: I was confused about which direction to go in. Q6: I had no difficulty with identifying the objects the system described for me. Q7: The system gave me a lot of unnecessary Information. Q8: The system gave</context>
</contexts>
<marker>Koller, Striegnitz, Gargett, Byron, Cassell, Dale, Moore, Oberlander, 2010</marker>
<rawString>Alexander Koller, Kristina Striegnitz, Andrew Gargett, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2010. Report on the second nlg challenge on generating instructions in virtual environments (give-2). In Proceedings of the 6th International Natural Language Generation Conference, INLG ’10, pages 243–250. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>Ronakkumar Patel</author>
<author>David Traum</author>
<author>Brandon Kennedy</author>
</authors>
<title>Building effective question answering characters.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, SigDIAL ’06,</booktitle>
<pages>18--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3532" citStr="Leuski et al., 2006" startWordPosition="564" endWordPosition="567"> of complex and humanlike sentences, the system is not bound to use written instructions (it may easily use recorded audio clips, for example), and finally, no rule writing by a dialogue expert or manual annotations is needed. The disadvantage of generation by selection is that the resulting dialogue may not be fully coherent (Shawar and Atwell, 2003; Shawar and Atwell, 2005; Gandhe and Traum, 2007). In previous work, the selection approach to generation has been used in non task-oriented conversational agents such as negotiating agents (Gandhe and Traum, 2007), question answering characters (Leuski et al., 2006) and virtual patients (Kenny et al., 2007). In the work presented in this paper, the conversational agent is task-oriented. In Section 2 we introduce the framework used in the interaction between the navigation agent and the human pedestrians. We discuss the creation of the human interaction corpus and the method for natural language generation in Section 3; And in Section 4 we explain the evaluation methods and 33 Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 33–37, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics the expe</context>
</contexts>
<marker>Leuski, Patel, Traum, Kennedy, 2006</marker>
<rawString>Anton Leuski, Ronakkumar Patel, David Traum, and Brandon Kennedy. 2006. Building effective question answering characters. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, SigDIAL ’06, pages 18–27. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nunan</author>
</authors>
<title>Task-based language teaching.</title>
<date>2004</date>
<publisher>University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1152" citStr="Nunan, 2004" startWordPosition="184" endWordPosition="185">fferent tasks within the context of a real city. The instructor decides what to say using a generation by selection algorithm, based on a corpus of real interactions generated within the world of interest. The instructor is able to react to different requests by the pedestrian. It is also aware of the pedestrian position with a certain degree of uncertainty, and it can use different city landmarks to guide him. 1 Introduction and previous work Virtual instructors are conversational agents that help a user perform a task. These agents can be useful for many purposes, such as language learning (Nunan, 2004), training in simulated environments (Kim et al., 2009) and entertainment (Dignum, 2012; Jan et al., 2009). Navigation agents generate verbal route directions for users to go from point A to point B in a given world. The wide variety of techniques to accomplish this task, range from giving complete route directions (all route information in a single instruction), to full interactive dialogue systems which give incremental instructions based on the position of the pedestrian. Although it can recognize pre-established written requests, the instructor presented in this work is not able to interpr</context>
</contexts>
<marker>Nunan, 2004</marker>
<rawString>David Nunan. 2004. Task-based language teaching. University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Shawar</author>
<author>E Atwell</author>
</authors>
<title>Using dialogue corpora to retrain a chatbot system.</title>
<date>2003</date>
<booktitle>In Proceedings of the Corpus Linguistics Conference,</booktitle>
<pages>681--690</pages>
<contexts>
<context position="3264" citStr="Shawar and Atwell, 2003" startWordPosition="525" endWordPosition="528">mposition procedure, e.g. grammar rules. The natural language generation algorithm used in this work is a modified version of the generation by selection method described in (Benotti and Denis, 2011). The advantages of generation by selection are many: it affords the use of complex and humanlike sentences, the system is not bound to use written instructions (it may easily use recorded audio clips, for example), and finally, no rule writing by a dialogue expert or manual annotations is needed. The disadvantage of generation by selection is that the resulting dialogue may not be fully coherent (Shawar and Atwell, 2003; Shawar and Atwell, 2005; Gandhe and Traum, 2007). In previous work, the selection approach to generation has been used in non task-oriented conversational agents such as negotiating agents (Gandhe and Traum, 2007), question answering characters (Leuski et al., 2006) and virtual patients (Kenny et al., 2007). In the work presented in this paper, the conversational agent is task-oriented. In Section 2 we introduce the framework used in the interaction between the navigation agent and the human pedestrians. We discuss the creation of the human interaction corpus and the method for natural langu</context>
</contexts>
<marker>Shawar, Atwell, 2003</marker>
<rawString>B.A. Shawar and E. Atwell. 2003. Using dialogue corpora to retrain a chatbot system. In Proceedings of the Corpus Linguistics Conference, pages 681– 690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Shawar</author>
<author>E Atwell</author>
</authors>
<title>Using corpora in machine-learning chatbot systems.</title>
<date>2005</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>10--489</pages>
<contexts>
<context position="3289" citStr="Shawar and Atwell, 2005" startWordPosition="529" endWordPosition="532"> grammar rules. The natural language generation algorithm used in this work is a modified version of the generation by selection method described in (Benotti and Denis, 2011). The advantages of generation by selection are many: it affords the use of complex and humanlike sentences, the system is not bound to use written instructions (it may easily use recorded audio clips, for example), and finally, no rule writing by a dialogue expert or manual annotations is needed. The disadvantage of generation by selection is that the resulting dialogue may not be fully coherent (Shawar and Atwell, 2003; Shawar and Atwell, 2005; Gandhe and Traum, 2007). In previous work, the selection approach to generation has been used in non task-oriented conversational agents such as negotiating agents (Gandhe and Traum, 2007), question answering characters (Leuski et al., 2006) and virtual patients (Kenny et al., 2007). In the work presented in this paper, the conversational agent is task-oriented. In Section 2 we introduce the framework used in the interaction between the navigation agent and the human pedestrians. We discuss the creation of the human interaction corpus and the method for natural language generation in Section</context>
</contexts>
<marker>Shawar, Atwell, 2005</marker>
<rawString>B.A. Shawar and E. Atwell. 2005. Using corpora in machine-learning chatbot systems. International Journal of Corpus Linguistics, 10:489–516.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
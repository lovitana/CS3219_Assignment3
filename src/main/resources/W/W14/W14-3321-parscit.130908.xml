<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.98424">
Machine Translation and Monolingual Postediting:
The AFRL WMT-14 System
</title>
<author confidence="0.920699">
Lane O.B. Schwartz
</author>
<affiliation confidence="0.904394">
Air Force Research Laboratory
</affiliation>
<email confidence="0.954908">
lane.schwartz@us.af.mil
</email>
<author confidence="0.864159">
Jeremy Gwinnup
</author>
<affiliation confidence="0.678177">
SRA International†
</affiliation>
<email confidence="0.983218">
jeremy.gwinnup.ctr@us.af.mil
</email>
<sectionHeader confidence="0.979092" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996686875">
This paper describes the AFRL sta-
tistical MT system and the improve-
ments that were developed during the
WMT14 evaluation campaign. As part
of these efforts we experimented with
a number of extensions to the stan-
dard phrase-based model that improve
performance on Russian to English
and Hindi to English translation tasks.
In addition, we describe our efforts
to make use of monolingual English
speakers to correct the output of ma-
chine translation, and present the re-
sults of monolingual postediting of the
entire 3003 sentences of the WMT14
Russian-English test set.
</bodyText>
<sectionHeader confidence="0.996306" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999992055555556">
As part of the 2014 Workshop on Machine
Translation (WMT14) shared translation task,
the human language technology team at the
Air Force Research Laboratory participated
in two language pairs: Russian-English and
Hindi-English. Our machine translation sys-
tem represents enhancements to our system
from IWSLT 2013 (Kazi et al., 2013). In this
paper, we focus on enhancements to our pro-
cedures with regard to data processing and the
handling of unknown words.
In addition, we describe our efforts to make
use of monolingual English speakers to correct
the output of machine translation, and present
the results of monolingual postediting of the
entire 3003 sentences of the WMT14 Russian-
English test set. Using a binary adequacy clas-
sification, we evaluate the entire postedited
</bodyText>
<footnote confidence="0.777069">
†This work is sponsored by the Air Force Research
Laboratory under Air Force contract FA-8650-09-D-
6939-029.
</footnote>
<author confidence="0.577807">
Timothy Anderson
</author>
<affiliation confidence="0.518087">
Air Force Research Laboratory
</affiliation>
<email confidence="0.631552">
timothy.anderson.20@us.af.mil
</email>
<author confidence="0.617423">
Katherine M. Young
</author>
<affiliation confidence="0.411509">
N-Space Analysis LLC†
</affiliation>
<email confidence="0.867476">
katherine.young.1.ctr@us.af.mil
</email>
<bodyText confidence="0.999913444444445">
test set for correctness against the reference
translations. Using bilingual judges, we fur-
ther evaluate a substantial subset of the post-
edited test set using a more fine-grained ade-
quacy metric; using this metric, we show that
monolingual posteditors can successfully pro-
duce postedited translations that convey all or
most of the meaning of the original source sen-
tence in up to 87.8% of sentences.
</bodyText>
<sectionHeader confidence="0.968647" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999968142857143">
We submitted systems for the Russian-to-
English and Hindi-to-English MT shared
tasks. In all submitted systems, we use the
phrase-based moses decoder (Koehn et al.,
2007). We used only the constrained data sup-
plied by the evaluation for each language pair
for training our systems.
</bodyText>
<subsectionHeader confidence="0.825728">
2.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999850714285714">
Before training our systems, a cleaning pass
was performed on all data. Unicode charac-
ters in the unallocated and private use ranges
were all removed, along with C0 and C1 con-
trol characters, zero-width and non-breaking
spaces and joiners, directionality and para-
graph markers.
</bodyText>
<subsubsectionHeader confidence="0.874115">
2.1.1 Hindi Processing
</subsubsectionHeader>
<bodyText confidence="0.999087083333333">
The HindEnCorp corpus (Bojar et al., 2014)
is distributed in tokenized form; in order to
ensure a uniform tokenization standard across
all of our data, we began by detokenized this
data using the Moses detokenization scripts.
In addition to normalizing various extended
Latin punctuation marks to their Basic Latin
equivalents, following Bojar et al. (2010) we
normalized DEVANAGARI DANDA (U+0964),
DOUBLE DANDA (U+0965), and ABBREVIA-
TION SIGN (U+0970) punctuation marks to
Latin FULL STOP (U+002E), any DEVANA-
</bodyText>
<page confidence="0.980729">
186
</page>
<note confidence="0.7155235">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 186–194,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999388">
GARI DIGIT to the equivalent ASCII DIGIT,
and decomposed all Hindi data into Unicode
Normalization Form D (Davis and Whistler,
2013) using charlint.1 In addition, we per-
formed Hindi diacritic and vowel normaliza-
tion, following Larkey et al. (2003).
Since no Hindi-English development test
set was provided in WMT14, we randomly
sampled 1500 sentence pairs from the Hindi-
English parallel training data to serve this pur-
pose. Upon discovering duplicate sentences in
the corpus, 552 sentences that overlapped with
the training portion were removed from the
sample, leaving a development test set of 948
sentences.
</bodyText>
<subsubsectionHeader confidence="0.958827">
2.1.2 Russian Processing
</subsubsectionHeader>
<bodyText confidence="0.99967509375">
The Russian sentences contained many exam-
ples of mixed-character spelling, in which both
Latin and Cyrillic characters are used in a sin-
gle word, relying on the visual similarity of the
characters. For example, although the first
letter and last letter in the word cейчас ap-
pear visually indistinguishable, we find that
the former is U+0063 LATIN SMALL LETTER
C and the latter is U+0441 CYRILLIC SMALL
LETTER Es. We created a spelling normal-
ization program to convert these words to all
Cyrillic or all Latin characters, with a pref-
erence for all-Cyrillic conversion if possible.
Normalization also removes U+0301 COMBIN-
ING ACUTE ACCENT (̲́) and converts U+00F2
LATIN SMALL LETTER O WITH GRAVE (ò)
and U+00F3 LATIN SMALL LETTER O WITH
ACUTE (ó) to the unaccented U+043E CYRIL-
LIC SMALL LETTER O (о).
The Russian-English Common Crawl par-
allel corpus (Smith et al., 2013) is relatively
noisy. A number of Russian source sentences
are incorrectly encoded using characters in the
Latin-1 supplement block; we correct these
sentences by shifting these characters ahead
by 350hex code points into the correct Cyrillic
character range.2
We examine the Common Crawl parallel
sentences and mark for removal any non-
Russian source sentences and non-English tar-
get sentences. Target sentences were marked
as non-English if more than half of the charac-
</bodyText>
<footnote confidence="0.964925333333333">
1http://www.w3.org/International/charlint
2For example: “Rïðàâêà ïî ãîðîäàì Ðîññèè è ìèðà.”
becomes “Справка по городам России и мира.”
</footnote>
<bodyText confidence="0.999812304347826">
ters in the sentence were non-Latin, or if more
than half of the words were unknown to the
aspell English spelling correction program,
not counting short words, which frequently
occur as (possibly false) cognates across lan-
guages (English die vs. German die, English
on vs. French on, for example). Because
aspell does not recognize some proper names,
brand names, and borrowed words as known
English words, this method incorrectly flags
for removal some English sentences which have
a high proportion of these types of words.
Source sentences were marked as non-
Russian if less than one-third of the charac-
ters were within the Russian Cyrillic range, or
if non-Russian characters equal or outnumber
Russian characters and the sentence contains
no contiguous sequence of at least three Rus-
sian characters. Some portions of the Cyrillic
character set are not used in typical Russian
text; source sentences were therefore marked
for removal if they contained Cyrillic exten-
sion characters UKRAINIAN I (і І), YI(ї Ї),
GHE WITH UPTURN (ґ Ґ) or IE (є Є) in ei-
ther upper- or lowercase, with exceptions for
U+0406 UKRAINIAN I (І) in Roman numerals
and for U+0491 GHE WITH UPTURN (ґ) when
it occurred as an encoding error artifact.3
Sentence pairs where the source was identi-
fied as non-Russian or the target was identified
as non-English were removed from the parallel
corpus. Overall, 12% of the parallel sentences
were excluded based on a non-Russian source
sentence (94k instances) or a non-English tar-
get sentence (11.8k instances).
Our Russian-English parallel training data
includes a parallel corpus extracted from
Wikipedia headlines (Ammar et al., 2013),
provided as part of the WMT14 shared trans-
lation task. Two files in this parallel cor-
pus (wiki.ru-en and guessed-names.ru-en)
contained some overlapping data. We re-
moved 6415 duplicate lines within wiki.ru-en
(about 1.4%), and removed 94 lines of
guessed-names.ru-en that were already
present in wiki.ru-en (about 0.17%).
</bodyText>
<footnote confidence="0.9775804">
3Specifically, we allowed lines containing ґ where it
appears as an encoding error in place of an apostro-
phe within English words. For example: “Песня The
Kelly Family Iґm So Happy представлена вам Lyrics-
Keeper.”
</footnote>
<page confidence="0.994025">
187
</page>
<subsectionHeader confidence="0.978256">
2.2 Machine Translation
</subsectionHeader>
<bodyText confidence="0.9998395">
Our baseline system is a variant of the MIT-
LL/AFRL IWSLT 2013 system (Kazi et al.,
2013) with some modifications to the training
and decoding processes.
</bodyText>
<subsubsectionHeader confidence="0.551543">
2.2.1 Phrase Table Training
</subsubsectionHeader>
<bodyText confidence="0.997990833333333">
For our Russian-English system, we trained
a phrase table using the Moses Experiment
Management System (Koehn, 2010b), with
mgiza (Gao and Vogel, 2008) as the word
aligner; this phrase table was trained using the
Russian-English Common Crawl, News Com-
mentary, Yandex (Bojar et al., 2013), and
Wikipedia headlines parallel corpora.
The phrase table for our Hindi-English sys-
tem was trained using a similar in-house train-
ing pipeline, making use of the HindEnCorp
and Wikipedia headlines parallel corpora.
</bodyText>
<subsubsectionHeader confidence="0.844618">
2.2.2 Language Model Training
</subsubsectionHeader>
<bodyText confidence="0.999954419354839">
During the training process we built n-gram
language models (LMs) for use in decoding
and rescoring using the KenLM language mod-
elling toolkit (Heafield et al., 2013). Class-
based language models (Brown et al., 1992)
were also trained, for later use in n-best list
rescoring, using the SRILM language mod-
elling toolkit (Stolcke, 2002).We trained a 6-
gram language model from the LDC English
Gigaword Fifth Edition, for use in both the
Hindi-English and Russian-English systems.
All language models were binarized in order
to reduce model disk usage and loading time.
For the Russian-to-English task, we concate-
nated the English portion of the parallel train-
ing data for the WMT 2014 shared transla-
tion task (Common Crawl, News Commen-
tary, Wiki Headlines and Yandex corpora) in
addition to the shared task English monolin-
gual training data (Europarl, News Commen-
tary and News Crawl corpora) into a training
set for a large 6-gram language model using
KenLM. We denote this model as “BigLM”. In-
dividual 6-gram models were also constructed
from each respective corpus.
For the Hindi-to-English task, individual 6-
gram models were constructed from the re-
spective English portions of the HindEnCorp
and Wikipedia headlines parallel corpora, and
from the monolingual English sections of the
Europarl and News Crawl corpora.
</bodyText>
<table confidence="0.995077625">
Decoding Features
P(f  |e)
P(e |f)
Pw(f  |e)
Pw(e  |f)
Phrase Penalty
Lexical Backoff
Word Penalty
Distortion Model
Unknown Word Penalty
Lexicalized Reordering Model
Operation Sequence Model
Rescoring Features
Pclass(E) – 7-gram class-based LM
Plex(F  |E) – sentence-level averaged
lexical translation score
</table>
<tableCaption confidence="0.58367275">
Table 1: Models used in log-linear combina-
tion
2.2.3 Decoding, n-best List Rescoring,
and Optimization
</tableCaption>
<bodyText confidence="0.99988825">
We decode using the phrase-based moses de-
coder (Koehn et al., 2007), choosing the best
translation for each source sentence according
to a linear combination of decoding features:
</bodyText>
<equation confidence="0.962497">
∑
E� = arg max arhr(E, F) (1)
E br
</equation>
<bodyText confidence="0.942916866666667">
We make use of a standard set of decoding
features, listed in Table 1. In contrast to our
IWSLT 2013 system, all experiments submit-
ted to this year’s WMT evaluation made use
of version 2.1 of moses, and incorporated ad-
ditional decoding features, namely the Oper-
ation Sequence Model (Durrani et al., 2011)
and Lexicalized Reordering Model (Tillman,
2004; Galley and Manning, 2008).
Following Shen et al. (2006), we use
the word-level lexical translation probabili-
ties Pw(fj  |ei) to obtain a sentence-level aver-
aged lexical translation score (Eq. 2), which is
added as an additional feature to each n-best
list entry.
</bodyText>
<equation confidence="0.892761">
∏
Plex(F  |E) =
jE1...J
</equation>
<listItem confidence="0.65049525">
(2)
Shen et al. (2006) use the term “IBM model 1
score” to describe the value calculated in Eq.
2. While the lexical probability distribution
</listItem>
<figure confidence="0.588366666666667">
1 ∑ Pw(fj  |ei)
I + 1
iE1...I
</figure>
<page confidence="0.982885">
188
</page>
<bodyText confidence="0.9999423">
from IBM Model 1 (Brown et al., 1993) could
in fact be used as the P,,,(fj I ez) in Eq. 2, in
practice we use a variant of P,,,(fj I ez) defined
by Koehn et al. (2003).
We also add a 7-gram class language model
score P,ass(E) (Brown et al., 1992) as an ad-
ditional feature of each n-best list entry. After
adding these features to each translation in an
n-best list, Eq. 1 is applied, rescoring the en-
tries to extract new 1-best translations.
To optimize system performance we train
scaling factors, A,., for both decoding and
rescoring features so as to minimize an ob-
jective error criterion. In our systems we use
DREM (Kazi et al., 2013) or PRO (Hopkins
and May, 2011) to perform this optimization.
For development data during optimization,
we used newstest2013 for the Russian-to-
English task and newsdev2014 for the Hindi-
to-English task supplied by WMT14.
</bodyText>
<subsubsectionHeader confidence="0.853763">
2.2.4 Unknown Words
</subsubsectionHeader>
<bodyText confidence="0.999847857142857">
For the Hindi-to-English task, unknown words
were marked during the decoding process and
were transliterated by the icu4j Devanagari-
to-Latin transliterator.4
For the Russian-to-English task, we selec-
tively stemmed and inflected input words not
found in the phrase table. Each input sentence
was examined to identify any source words
which did not occur as a phrase of length 1
in the phrase table. For each such unknown
word, we used treetagger (Schmid, 1994;
Schmid, 1995) to identify the part of speech,
and then we removed inflectional endings to
derive a stem. We applied all possible Rus-
sian inflectional endings for the given part of
speech; if an inflected form of the unknown
word could be found as a stand-alone phrase
in the phrase table, that form was used to re-
place the unknown word in the original Rus-
sian file. If multiple candidates were found,
we used the one with the highest frequency of
occurrence in the training data. This process
replaces words that we know we cannot trans-
late with semantically similar words that we
can translate, replacing unknown words like
c)OTOHOM “photon” (instrumental case) with
a known morphological variant c)OTOH “pho-
ton” (nominative case) that is found in the
</bodyText>
<footnote confidence="0.987854">
4http://site.icu-project.org
</footnote>
<table confidence="0.999428166666667">
BLEU BLEU-cased
System 1 hi-en 13.1 12.1
2 ru-en 32.0 30.8
3 ru-en 32.2 31.0
4 ru-en 31.5 30.3
5 ru-en 33.0 31.1
</table>
<tableCaption confidence="0.985972">
Table 2: Translation results, as measured by
BLEU (Papineni et al., 2002).
</tableCaption>
<bodyText confidence="0.997297888888889">
phrase table. Selective stemming of just the
unknown words allows us to retain informa-
tion that would be lost if we applied stemming
to all the data.
Any remaining unknown words were
transliterated as a post-process, using a
simple letter-mapping from Cyrillic characters
to Latin characters representing their typical
sounds.
</bodyText>
<subsectionHeader confidence="0.86184">
2.3 MT Results
</subsectionHeader>
<bodyText confidence="0.965248233333333">
Our best Hindi-English system for
newstest2014 is listed in Table 2 as System
1. This system uses a combination of 6-gram
language models built from HindEnCorp,
News Commentary, Europarl, and News
Crawl corpora. Transliteration of unknown
words was performed after decoding but
before n-best list rescoring.
System 2 is Russian-English, and handles
unknown words following §2.2.4. We used as
independent decoder features separate 6-gram
LMs trained respectively on Common Crawl,
Europarl, News Crawl, Wiki headlines and
Yandex corpora. This system was optimized
with DREM. No rescoring was performed. We
also tested a variant of System 2 which did
perform rescoring. That variant (not listed in
Table 2) performed worse than System 2, with
scores of 31.2 BLEU and 30.1 BLEU-cased.
System 3, our best Russian-English system
for newstest2014, used the BigLM and Giga-
word language models (see §2.2.2) as indepen-
dent decoder features and was optimized with
DREM. Rescoring was performed after de-
coding. Instead of following §2.2.4, unknown
words were dropped to maximize BLEU score.
We note that the optimizer assigned weights of
0.314 and 0.003 to the BigLM and Gigaword
models, respectively, suggesting that the opti-
mizer found the BigLM to be much more use-
</bodyText>
<page confidence="0.996625">
189
</page>
<figureCaption confidence="0.992561">
Figure 1: Posteditor user interface
</figureCaption>
<table confidence="0.9966916">
Documents Sentences Words
Posteditor 1 44 950 20086
2 21 280 6031
3 25 476 10194
4 25 298 6164
5 20 301 5809
6 15 210 4433
7 10 140 2650
8 15 348 6743
All 175 3003 62110
</table>
<tableCaption confidence="0.99477">
Table 3: Number of documents within the
</tableCaption>
<bodyText confidence="0.778873666666667">
Russian-English test set processed by each
monolingual human posteditor. Number of
machine translated sentences processed by
each posteditor is also listed, along with the
total number of words in the corresponding
Russian source sentences.
</bodyText>
<table confidence="0.9995606">
# ✓ # X % ✓
Posteditor 1 684 266 72.0%
2 190 90 67.9%
3 308 168 64.7%
4 162 136 54.4%
5 194 107 64.5%
6 94 116 44.8%
7 88 52 62.9%
8 196 152 56.3%
All 1916 1087 63.8%
</table>
<tableCaption confidence="0.80441275">
Table 4: For each monolingual posteditor, the
number and percentage of sentences judged to
be correct (✓) versus incorrect (X) according
to a monolingual human judge.6
</tableCaption>
<table confidence="0.9754607">
12 The postedited translation is superior
to the reference translation
10 The meaning of the Russian source
sentence is fully conveyed in the post-
edited translation
8 Most of the meaning is conveyed
6 Misunderstands the sentence in a ma-
jor way; or has many small mistakes
4 Very little meaning is conveyed
2 The translation makes no sense at all
</table>
<tableCaption confidence="0.942176333333333">
Table 5: Evaluation guidelines for bilingual
human judges, adapted from Albrecht et al.
(2009).
</tableCaption>
<table confidence="0.995210333333333">
Evaluation Category
2 4 6 8 10 12
0.2% 2.2% 9.8% 24.7% 60.2% 2.8%
</table>
<tableCaption confidence="0.998388666666667">
Table 6: Percentage of evaluated sentences
judged to be in each category by a bilingual
judge. Category labels are defined in Table 5.
</tableCaption>
<table confidence="0.9988914">
Evaluation Category
2 4 6 8 10 12
# X 2 20 72 89 79 4
# ✓ 0 1 21 146 493 23
% ✓ 0% 5% 23% 62% 86% 85%
</table>
<tableCaption confidence="0.7971505">
Table 7: Number of sentences in each evalu-
ation category (see Table 5) that were judged
as correct (✓) or incorrect (X) according to a
monolingual human judge.
</tableCaption>
<page confidence="0.99662">
190
</page>
<bodyText confidence="0.999902421052632">
ful than the Gigaword LM. This intuition was
confirmed by an experimental variation of Sys-
tem 3 (not listed in Table 2) where we omitted
the BigLM; that variant performed substan-
tially worse, with scores of 25.3 BLEU and
24.2 BLEU-cased. We also tested a variant
of System 3 which did not perform rescoring;
that variant (also not listed in Table 2) per-
formed worse, with scores of 31.7 BLEU and
30.6 BLEU-cased.
The results of monolingual postediting (see
§3) of System 4 (a variant of System 2 tuned
using PRO) uncased output is System 5. Due
to time constraints, the monolingual post-
editing experiments in §3 were conducted (us-
ing the machine translation results from Sys-
tem 4) before the results of Systems 2 and 3
were available. The Moses recaser was applied
in all experiments except for System 5.
</bodyText>
<sectionHeader confidence="0.987876" genericHeader="method">
3 Monolingual Postediting
</sectionHeader>
<bodyText confidence="0.99980897260274">
Postediting is the process whereby a human
user corrects the output of a machine trans-
lation system. The use of basic postediting
tools by bilingual human translators has been
shown to yield substantial increases in terms
of productivity (Plitt and Masselot, 2010) as
well as improvements in translation quality
(Green et al., 2013) when compared to bilin-
gual human translators working without as-
sistance from machine translation and post-
editing tools. More sophisticated interactive
interfaces (Langlais et al., 2000; Barrachina
et al., 2009; Koehn, 2009b; Denkowski and
Lavie, 2012) may also provide benefit (Koehn,
2009a).
We hypothesize that for at least some lan-
guage pairs, monolingual posteditors with no
knowledge of the source language can success-
fully translate a substantial fraction of test
sentences. We expect this to be the case espe-
cially when the monolingual humans are do-
main experts with regard to the documents to
be translated. If this hypothesis is confirmed,
this could allow for multi-stage translation
workflows, where less highly skilled monolin-
gual posteditors triage the translation pro-
cess, postediting many of the sentences, while
forwarding on the most difficult sentences to
more highly skilled bilingual translators.
Small-scale studies have suggested that
monolingual human posteditors, working
without knowledge of the source language, can
also improve the quality of machine trans-
lation output (Callison-Burch, 2005; Koehn,
2010a; Mitchell et al., 2013), especially if well-
designed tools provide automated linguistic
analysis of source sentences (Albrecht et al.,
2009).
In this study, we designed a simple user in-
terface for postediting that presents the user
with the source sentence, machine transla-
tion, and word alignments for each sentence
in a test document (Figure 1). While it may
seem counter-intuitive to present monolingual
posteditors with the source sentence, we found
that the presence of alignment links between
source words and target words can in fact aid
a monolingual posteditor, especially with re-
gard to correcting word order. For example, in
our experiments posteditors encountered some
sentences where a word or phrase was enclosed
within bracketing punctuation marks (such as
quotation marks, commas, or parentheses) in
the source sentence, and the machine transla-
tion system incorrectly reordered the word or
phrase outside the enclosing punctuation; by
examining the alignment links the posteditors
were able to correct such reordering mistakes.
The Russian-English test set comprises 175
documents in the news domain, totaling 3003
sentences. We assigned each test document
to one of 8 monolingual5 posteditors (Table
3). The postediting tool did not record tim-
ing information. However, several posteditors
informally reported that they were able to pro-
cess on average approximately four documents
per hour; if accurate, this would indicate a
processing speed of around one sentence per
minute.
Following Koehn (2010a), we evaluated
postedited translation quality according to
a binary adequacy metric, as judged by a
monolingual English speaker6 against the En-
</bodyText>
<footnote confidence="0.752789545454545">
5All posteditors are native English speakers. Poste-
ditors 2 and 3 know Chinese and Arabic, respectively,
but not Russian. Posteditor 8 understands the Cyrillic
character set and has a minimal Russian vocabulary
from two undergraduate semesters of Russian taken
several years ago.
6All monolingual adequacy judgements were per-
formed by Posteditor 1. Additional analysis of Post-
editor 1’s 950 postedited translations were indepen-
dently judged by bilingual judges against the reference
and the source sentence (Table 7).
</footnote>
<page confidence="0.997573">
191
</page>
<bodyText confidence="0.999969196428571">
glish references. In this metric, incorrect
spellings of transliterated proper names were
not grounds to judge as incorrect an otherwise
adequate postedited translation. Binary ade-
quacy results are shown in Table 4; we observe
that correctness varied widely between poste-
ditors (44.8–72.0%), and between documents.
Interestingly, several posteditors self-
reported that they could tell which documents
were originally written in English and were
subsequently translated into Russian, and
which were originally written in Russian,
based on observations that sentences from
the latter were substantially more difficult to
postedit. Once per-document source language
data is released by WMT14 organizers, we
intend to examine translation quality on a
per-document basis and test whether postedi-
tors did indeed perform worse on documents
which originated in Russian.
Using bilingual judges, we further evaluate a
substantial subset of the postedited test set us-
ing a more fine-grained adequacy metric (Ta-
ble 5). Because of time constraints, only the
first 950 postedited sentences of the test set6
were evaluated in this manner. Each sentence
was evaluated by one of two bilingual human
judges. In addition to the 2-10 point scale of
Albrecht et al. (2009), judges were instructed
to indicate (with a score of 12) any sentences
where the postedited machine translation was
superior to the reference translation. Using
this metric, we show in Table 6 that monolin-
gual posteditors can successfully produce post-
edited translations that convey all or most of
the meaning of the original source sentence in
up to 87.8% of sentences; this includes 2.8%
which were superior to the reference.
Finally, as part of WMT14, the results of
our Systems 1 (hi-en), 3 (ru-en), and 5 (post-
edited ru-en) were ranked by monolingual hu-
man judges against the machine translation
output of other WMT14 participants. These
judgements are reported in WMT (2014).
Due to time constraints, the machine trans-
lations (from System 4) presented to postedi-
tors were not evaluated by human judges, nei-
ther using our 12-point evaluation scale nor
as part of the WMT human evaluation rank-
ings. However, to enable such evaluation by
future researchers, and to enable replication of
our experimental evaluation, the System 4 ma-
chine translations, the postedited translations,
and the monolingual and bilingual evaluation
results are released as supplementary data to
accompany this paper.
</bodyText>
<sectionHeader confidence="0.993" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999967428571429">
In this paper, we present data preparation and
language-specific processing techniques for our
Hindi-English and Russian-English submis-
sions to the 2014 Workshop on Machine Trans-
lation (WMT14) shared translation task. Our
submissions examine the effectiveness of han-
dling various monolingual target language cor-
pora as individual component language mod-
els (System 2) or alternatively, concatenated
together into a single big language model (Sys-
tem 3). We also examine the utility of n-
best list rescoring using class language model
and lexicalized translation model rescoring
features.
In addition, we present the results of mono-
lingual postediting of the entire 3003 sentences
of the WMT14 Russian-English test set. Post-
editing was performed by monolingual English
speakers, who corrected the output of ma-
chine translation without access to external
resources, such as bilingual dictionaries or on-
line search engines. This system scored high-
est according to BLEU of all Russian-English
submissions to WMT14.
Using a binary adequacy classification, we
evaluate the entire postedited test set for cor-
rectness against the reference translations. Us-
ing bilingual judges, we further evaluate a sub-
stantial subset of the postedited test set us-
ing a more fine-grained adequacy metric; using
this metric, we show that monolingual postedi-
tors can successfully produce postedited trans-
lations that convey all or most of the meaning
of the original source sentence in up to 87.8%
of sentences.
</bodyText>
<sectionHeader confidence="0.990095" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99723425">
We would like to thank the members of the
SCREAM group at Wright-Patterson AFB.
Opinions, interpretations, conclusions and recom-
mendations are those of the authors and are not nec-
essarily endorsed by the United States Government.
Cleared for public release on 1 Apr 2014. Origina-
tor reference number RH-14-112150. Case number
88ABW-2014-1328.
</bodyText>
<page confidence="0.9969">
192
</page>
<sectionHeader confidence="0.979237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999724991071429">
Joshua S. Albrecht, Rebecca Hwa, and G. Elisa-
beta Marai. 2009. Correcting automatic trans-
lations through collaborations between MT and
monolingual target-language users. In Proceed-
ings of the 12th Conference of the European
Chapter of the Association for Computational
Linguistics (EACL ’12), pages 60–68, Athens,
Greece, March–April.
Waleed Ammar, Victor Chahuneau, Michael
Denkowski, Greg Hanneman, Wang Ling,
Austin Matthews, Kenton Murray, Nicola
Segall, Yulia Tsvetkov, Alon Lavie, and Chris
Dyer. 2013. The CMU machine translation sys-
tems at WMT 2013: Syntax, synthetic trans-
lation options, and pseudo-references. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation (WMT ’13), pages 70–77,
Sofia, Bulgaria, August.
Sergio Barrachina, Oliver Bender, Francisco
Casacuberta, Jorge Civera, Elsa Cubel,
Shahram Khadivi, Antonio Lagarda, Hermann
Ney, Jesus Tomas, Enrique Vidal, and Juan-
Miguel Vilar. 2009. Statistical approaches to
computer-assisted translation. Computational
Linguistics, 35(1):3–28, March.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman.
2010. Data issues in English-to-Hindi machine
translation. In Proceedings of the Seventh In-
ternational Conference on Language Resources
and Evaluation (LREC ’10), pages 1771–1777,
Valletta, Malta, May.
Ondrej Bojar, Christian Buck, Chris Callison-
Burch, Christian Federmann, Barry Haddow,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2013. Findings of the
2013 Workshop on Statistical Machine Trans-
lation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation (WMT ’13),
pages 1–44, Sofia, Bulgaria, August.
Ondrej Bojar, Vojtech Diatka, Pavel Rychly, Pavel
Stranak, Ales Tamchyna, and Dan Zeman.
2014. Hindi-English and Hindi-only corpus for
machine translation. In Proceedings of the Ninth
International Language Resources and Evalua-
tion Conference (LREC ’14), Reykjavik, Ice-
land, May. ELRA, European Language Re-
sources Association.
Peter Brown, Vincent Della Pietra, Peter deSouza,
Jenifer Lai, and Robert Mercer. 1992. Class-
based n-gram models of natural language. Com-
putational Linguistics, 18(4):467–479, Decem-
ber.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The math-
ematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics,
19(2):263–311, June.
Chris Callison-Burch. 2005. Linear B system de-
scription for the 2005 NIST MT evaluation exer-
cise. In Proceedings of the NIST 2005 Machine
Translation Evaluation Workshop.
Mark Davis and Ken Whistler. 2013. Unicode nor-
malization forms. Technical Report UAX #15,
The Unicode Consortium, September. Rev. 39.
Michael Denkowski and Alon Lavie. 2012. Trans-
Center: Web-based translation research suite.
In Proceedings of the AMTA 2012 Workshop
on Post-Editing Technology and Practice Demo
Session, November.
Nadir Durrani, Helmut Schmid, and Alexander
Fraser. 2011. A joint sequence translation
model with integrated reordering. In Proceed-
ings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL ’11),
pages 1045–1054, Portland, Oregon, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase re-
ordering model. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP ’08), pages 848–856,
Honolulu, Hawai‘i, October.
Qin Gao and Stephan Vogel. 2008. Parallel im-
plementations of word alignment tool. In Soft-
ware Engineering, Testing and Quality Assur-
ance for Natural Language Processing, pages
49–57, Columbus, Ohio, June.
Spence Green, Jeffrey Heer, and Christopher D.
Manning. 2013. The efficacy of human post-
editing for language translation. In Proceedings
of the ACM SIGCHI Conference on Human Fac-
tors in Computing Systems (CHI ’13), pages
439–448, Paris, France, April–May.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable mod-
ified Kneser-Ney language model estimation. In
Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (ACL
’13), pages 690–696, Sofia, Bulgaria, August.
Mark Hopkins and Jonathan May. 2011. Tuning
as ranking. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP ’11), pages 1352–1362, Ed-
inburgh, Scotland, U.K.
Michaeel Kazi, Michael Coury, Elizabeth Salesky,
Jessica Ray, Wade Shen, Terry Gleason, Tim
Anderson, Grant Erdmann, Lane Schwartz,
Brian Ore, Raymond Slyh, Jeremy Gwinnup,
Katherine Young, and Michael Hutt. 2013.
The MIT-LL/AFRL IWSLT-2013 MT system.
In The 10th International Workshop on Spo-
ken Language Translation (IWSLT ’13), pages
136–143, Heidelberg, Germany, December.
</reference>
<page confidence="0.989663">
193
</page>
<reference confidence="0.999790647058823">
Philipp Koehn, Franz Joseph Och, and Daniel
Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of the 2003 Human
Language Technology Conference of the North
American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL ’13), pages
48–54, Edmonton, Canada, May–June.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL ’07) Demo and Poster
Sessions, pages 177–180, Prague, Czech Repub-
lic, June.
Philipp Koehn. 2009a. A process study of com-
puter aided translation. Machine Translation,
23(4):241–263, November.
Philipp Koehn. 2009b. A web-based interactive
computer aided translation tool. In Proceedings
of the ACL-IJCNLP 2009 Software Demonstra-
tions, pages 17–20, Suntec, Singapore, August.
Philipp Koehn. 2010a. Enabling monolingual
translators: Post-editing vs. options. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL ’10), pages 537–545, Los Ange-
les, California, June.
Philipp Koehn. 2010b. An experimental manage-
ment system. The Prague Bulletin of Mathemat-
ical Linguistics, 94:87–96, December.
Philippe Langlais, George Foster, and Guy La-
palme. 2000. TransType: A computer-aided
translation typing system. In Proceedings of
the ANLP/NAACL 2000 Workshop on Embed-
ded Machine Translation Systems, pages 46–51,
Seattle, Washington, May.
Leah S. Larkey, Margaret E. Connell, and Nasreen
Abduljaleel. 2003. Hindi CLIR in thirty days.
ACM Transactions on Asian Language Informa-
tion Processing (TALIP), 2(2):130–142, June.
Linda Mitchell, Johann Roturier, and Sharon
O’Brien. 2013. Community-based post-editing
of machine translation content: monolingual vs.
bilingual. In Proceedings of the 2nd Work-
shop on Post-editing Technology and Practice
(WPTP-2), pages 35–43, Nice, France, Septem-
ber. EAMT.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: A method for au-
tomatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL
’02), pages 311–318, Philadelphia, Pennsylva-
nia, July.
Mirko Plitt and Franc�ois Masselot. 2010. A pro-
ductivity test of statistical machine translation
post-editing in a typical localisation context.
The Prague Bulletin of Mathematical Linguis-
tics, 93:7–16, January.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Language Processing, Manchester, England,
September.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German.
In Proceedings of the EACL SIGDAT Workshop,
Dublin, Ireland, March.
Wade Shen, Brian Delaney, and Tim Anderson.
2006. The MIT-LL/AFRL IWSLT-2006 MT
system. In The 3rd International Workshop on
Spoken Language Translation (IWSLT ’06), Ky-
oto, Japan.
Jason R. Smith, Herve Saint-Amand, Magdalena
Plamada, Philipp Koehn, Chris Callison-Burch,
and Adam Lopez. 2013. Dirt cheap web-scale
parallel text from the common crawl. In Pro-
ceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (ACL
’13), pages 1374–1383, Sofia, Bulgaria, August.
Andreas Stolcke. 2002. SRILM — an extensible
language modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP ’02), pages 901–904,
Denver, Colorado, September.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics
(HLT-NAACL ’04), Companion Volume, pages
101–104, Boston, Massachusetts, May.
WMT. 2014. Findings of the 2014 Workshop on
Statistical Machine Translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation (WMT ’14), Baltimore, Maryland,
June.
</reference>
<page confidence="0.998784">
194
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806563">
<title confidence="0.9985045">Machine Translation and Monolingual The AFRL WMT-14 System</title>
<author confidence="0.999573">Lane O B Schwartz</author>
<affiliation confidence="0.993217">Air Force Research Laboratory</affiliation>
<email confidence="0.987052">lane.schwartz@us.af.mil</email>
<author confidence="0.999098">Jeremy Gwinnup</author>
<email confidence="0.99612">jeremy.gwinnup.ctr@us.af.mil</email>
<abstract confidence="0.988284647058824">This paper describes the AFRL statistical MT system and the improvements that were developed during the WMT14 evaluation campaign. As part of these efforts we experimented with a number of extensions to the standard phrase-based model that improve performance on Russian to English and Hindi to English translation tasks. In addition, we describe our efforts to make use of monolingual English speakers to correct the output of machine translation, and present the results of monolingual postediting of the entire 3003 sentences of the WMT14 Russian-English test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua S Albrecht</author>
<author>Rebecca Hwa</author>
<author>G Elisabeta Marai</author>
</authors>
<title>Correcting automatic translations through collaborations between MT and monolingual target-language users.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL ’12),</booktitle>
<pages>60--68</pages>
<location>Athens, Greece, March–April.</location>
<contexts>
<context position="16522" citStr="Albrecht et al. (2009)" startWordPosition="2641" endWordPosition="2644">16 1087 63.8% Table 4: For each monolingual posteditor, the number and percentage of sentences judged to be correct (✓) versus incorrect (X) according to a monolingual human judge.6 12 The postedited translation is superior to the reference translation 10 The meaning of the Russian source sentence is fully conveyed in the postedited translation 8 Most of the meaning is conveyed 6 Misunderstands the sentence in a major way; or has many small mistakes 4 Very little meaning is conveyed 2 The translation makes no sense at all Table 5: Evaluation guidelines for bilingual human judges, adapted from Albrecht et al. (2009). Evaluation Category 2 4 6 8 10 12 0.2% 2.2% 9.8% 24.7% 60.2% 2.8% Table 6: Percentage of evaluated sentences judged to be in each category by a bilingual judge. Category labels are defined in Table 5. Evaluation Category 2 4 6 8 10 12 # X 2 20 72 89 79 4 # ✓ 0 1 21 146 493 23 % ✓ 0% 5% 23% 62% 86% 85% Table 7: Number of sentences in each evaluation category (see Table 5) that were judged as correct (✓) or incorrect (X) according to a monolingual human judge. 190 ful than the Gigaword LM. This intuition was confirmed by an experimental variation of System 3 (not listed in Table 2) where we om</context>
<context position="19427" citStr="Albrecht et al., 2009" startWordPosition="3123" endWordPosition="3126">his could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of the sentences, while forwarding on the most difficult sentences to more highly skilled bilingual translators. Small-scale studies have suggested that monolingual human posteditors, working without knowledge of the source language, can also improve the quality of machine translation output (Callison-Burch, 2005; Koehn, 2010a; Mitchell et al., 2013), especially if welldesigned tools provide automated linguistic analysis of source sentences (Albrecht et al., 2009). In this study, we designed a simple user interface for postediting that presents the user with the source sentence, machine translation, and word alignments for each sentence in a test document (Figure 1). While it may seem counter-intuitive to present monolingual posteditors with the source sentence, we found that the presence of alignment links between source words and target words can in fact aid a monolingual posteditor, especially with regard to correcting word order. For example, in our experiments posteditors encountered some sentences where a word or phrase was enclosed within bracke</context>
<context position="22719" citStr="Albrecht et al. (2009)" startWordPosition="3621" endWordPosition="3624">Once per-document source language data is released by WMT14 organizers, we intend to examine translation quality on a per-document basis and test whether posteditors did indeed perform worse on documents which originated in Russian. Using bilingual judges, we further evaluate a substantial subset of the postedited test set using a more fine-grained adequacy metric (Table 5). Because of time constraints, only the first 950 postedited sentences of the test set6 were evaluated in this manner. Each sentence was evaluated by one of two bilingual human judges. In addition to the 2-10 point scale of Albrecht et al. (2009), judges were instructed to indicate (with a score of 12) any sentences where the postedited machine translation was superior to the reference translation. Using this metric, we show in Table 6 that monolingual posteditors can successfully produce postedited translations that convey all or most of the meaning of the original source sentence in up to 87.8% of sentences; this includes 2.8% which were superior to the reference. Finally, as part of WMT14, the results of our Systems 1 (hi-en), 3 (ru-en), and 5 (postedited ru-en) were ranked by monolingual human judges against the machine translatio</context>
</contexts>
<marker>Albrecht, Hwa, Marai, 2009</marker>
<rawString>Joshua S. Albrecht, Rebecca Hwa, and G. Elisabeta Marai. 2009. Correcting automatic translations through collaborations between MT and monolingual target-language users. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL ’12), pages 60–68, Athens, Greece, March–April.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Waleed Ammar</author>
<author>Victor Chahuneau</author>
<author>Michael Denkowski</author>
<author>Greg Hanneman</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Kenton Murray</author>
<author>Nicola Segall</author>
<author>Yulia Tsvetkov</author>
<author>Alon Lavie</author>
<author>Chris Dyer</author>
</authors>
<title>The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT ’13),</booktitle>
<pages>70--77</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7315" citStr="Ammar et al., 2013" startWordPosition="1125" endWordPosition="1128"> or IE (є Є) in either upper- or lowercase, with exceptions for U+0406 UKRAINIAN I (І) in Roman numerals and for U+0491 GHE WITH UPTURN (ґ) when it occurred as an encoding error artifact.3 Sentence pairs where the source was identified as non-Russian or the target was identified as non-English were removed from the parallel corpus. Overall, 12% of the parallel sentences were excluded based on a non-Russian source sentence (94k instances) or a non-English target sentence (11.8k instances). Our Russian-English parallel training data includes a parallel corpus extracted from Wikipedia headlines (Ammar et al., 2013), provided as part of the WMT14 shared translation task. Two files in this parallel corpus (wiki.ru-en and guessed-names.ru-en) contained some overlapping data. We removed 6415 duplicate lines within wiki.ru-en (about 1.4%), and removed 94 lines of guessed-names.ru-en that were already present in wiki.ru-en (about 0.17%). 3Specifically, we allowed lines containing ґ where it appears as an encoding error in place of an apostrophe within English words. For example: “Песня The Kelly Family Iґm So Happy представлена вам LyricsKeeper.” 187 2.2 Machine Translation Our baseline system is a variant of</context>
</contexts>
<marker>Ammar, Chahuneau, Denkowski, Hanneman, Ling, Matthews, Murray, Segall, Tsvetkov, Lavie, Dyer, 2013</marker>
<rawString>Waleed Ammar, Victor Chahuneau, Michael Denkowski, Greg Hanneman, Wang Ling, Austin Matthews, Kenton Murray, Nicola Segall, Yulia Tsvetkov, Alon Lavie, and Chris Dyer. 2013. The CMU machine translation systems at WMT 2013: Syntax, synthetic translation options, and pseudo-references. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT ’13), pages 70–77, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Barrachina</author>
<author>Oliver Bender</author>
<author>Francisco Casacuberta</author>
<author>Jorge Civera</author>
<author>Elsa Cubel</author>
<author>Shahram Khadivi</author>
<author>Antonio Lagarda</author>
<author>Hermann Ney</author>
<author>Jesus Tomas</author>
<author>Enrique Vidal</author>
<author>JuanMiguel Vilar</author>
</authors>
<title>Statistical approaches to computer-assisted translation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="18364" citStr="Barrachina et al., 2009" startWordPosition="2966" endWordPosition="2969">er was applied in all experiments except for System 5. 3 Monolingual Postediting Postediting is the process whereby a human user corrects the output of a machine translation system. The use of basic postediting tools by bilingual human translators has been shown to yield substantial increases in terms of productivity (Plitt and Masselot, 2010) as well as improvements in translation quality (Green et al., 2013) when compared to bilingual human translators working without assistance from machine translation and postediting tools. More sophisticated interactive interfaces (Langlais et al., 2000; Barrachina et al., 2009; Koehn, 2009b; Denkowski and Lavie, 2012) may also provide benefit (Koehn, 2009a). We hypothesize that for at least some language pairs, monolingual posteditors with no knowledge of the source language can successfully translate a substantial fraction of test sentences. We expect this to be the case especially when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of th</context>
</contexts>
<marker>Barrachina, Bender, Casacuberta, Civera, Cubel, Khadivi, Lagarda, Ney, Tomas, Vidal, Vilar, 2009</marker>
<rawString>Sergio Barrachina, Oliver Bender, Francisco Casacuberta, Jorge Civera, Elsa Cubel, Shahram Khadivi, Antonio Lagarda, Hermann Ney, Jesus Tomas, Enrique Vidal, and JuanMiguel Vilar. 2009. Statistical approaches to computer-assisted translation. Computational Linguistics, 35(1):3–28, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Pavel Stranak</author>
<author>Daniel Zeman</author>
</authors>
<title>Data issues in English-to-Hindi machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC ’10),</booktitle>
<pages>1771--1777</pages>
<location>Valletta, Malta,</location>
<contexts>
<context position="3217" citStr="Bojar et al. (2010)" startWordPosition="482" endWordPosition="485"> pass was performed on all data. Unicode characters in the unallocated and private use ranges were all removed, along with C0 and C1 control characters, zero-width and non-breaking spaces and joiners, directionality and paragraph markers. 2.1.1 Hindi Processing The HindEnCorp corpus (Bojar et al., 2014) is distributed in tokenized form; in order to ensure a uniform tokenization standard across all of our data, we began by detokenized this data using the Moses detokenization scripts. In addition to normalizing various extended Latin punctuation marks to their Basic Latin equivalents, following Bojar et al. (2010) we normalized DEVANAGARI DANDA (U+0964), DOUBLE DANDA (U+0965), and ABBREVIATION SIGN (U+0970) punctuation marks to Latin FULL STOP (U+002E), any DEVANA186 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 186–194, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics GARI DIGIT to the equivalent ASCII DIGIT, and decomposed all Hindi data into Unicode Normalization Form D (Davis and Whistler, 2013) using charlint.1 In addition, we performed Hindi diacritic and vowel normalization, following Larkey et al. (2003). Since no Hindi-En</context>
</contexts>
<marker>Bojar, Stranak, Zeman, 2010</marker>
<rawString>Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010. Data issues in English-to-Hindi machine translation. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC ’10), pages 1771–1777, Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris CallisonBurch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT ’13),</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8348" citStr="Bojar et al., 2013" startWordPosition="1287" endWordPosition="1290">e of an apostrophe within English words. For example: “Песня The Kelly Family Iґm So Happy представлена вам LyricsKeeper.” 187 2.2 Machine Translation Our baseline system is a variant of the MITLL/AFRL IWSLT 2013 system (Kazi et al., 2013) with some modifications to the training and decoding processes. 2.2.1 Phrase Table Training For our Russian-English system, we trained a phrase table using the Moses Experiment Management System (Koehn, 2010b), with mgiza (Gao and Vogel, 2008) as the word aligner; this phrase table was trained using the Russian-English Common Crawl, News Commentary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2.2 Language Model Training During the training process we built n-gram language models (LMs) for use in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased language models (Brown et al., 1992) were also trained, for later use in n-best list rescoring, using the SRILM language modelling toolkit (Stolcke, 2002).We trained a 6- gra</context>
</contexts>
<marker>Bojar, Buck, CallisonBurch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondrej Bojar, Christian Buck, Chris CallisonBurch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT ’13), pages 1–44, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Vojtech Diatka</author>
<author>Pavel Rychly</author>
<author>Pavel Stranak</author>
<author>Ales Tamchyna</author>
<author>Dan Zeman</author>
</authors>
<title>Hindi-English and Hindi-only corpus for machine translation.</title>
<date>2014</date>
<journal>ELRA, European Language Resources Association.</journal>
<booktitle>In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC ’14),</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="2902" citStr="Bojar et al., 2014" startWordPosition="434" endWordPosition="437">he Russian-toEnglish and Hindi-to-English MT shared tasks. In all submitted systems, we use the phrase-based moses decoder (Koehn et al., 2007). We used only the constrained data supplied by the evaluation for each language pair for training our systems. 2.1 Data Preparation Before training our systems, a cleaning pass was performed on all data. Unicode characters in the unallocated and private use ranges were all removed, along with C0 and C1 control characters, zero-width and non-breaking spaces and joiners, directionality and paragraph markers. 2.1.1 Hindi Processing The HindEnCorp corpus (Bojar et al., 2014) is distributed in tokenized form; in order to ensure a uniform tokenization standard across all of our data, we began by detokenized this data using the Moses detokenization scripts. In addition to normalizing various extended Latin punctuation marks to their Basic Latin equivalents, following Bojar et al. (2010) we normalized DEVANAGARI DANDA (U+0964), DOUBLE DANDA (U+0965), and ABBREVIATION SIGN (U+0970) punctuation marks to Latin FULL STOP (U+002E), any DEVANA186 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 186–194, Baltimore, Maryland USA, June 26–27, 2014. </context>
</contexts>
<marker>Bojar, Diatka, Rychly, Stranak, Tamchyna, Zeman, 2014</marker>
<rawString>Ondrej Bojar, Vojtech Diatka, Pavel Rychly, Pavel Stranak, Ales Tamchyna, and Dan Zeman. 2014. Hindi-English and Hindi-only corpus for machine translation. In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC ’14), Reykjavik, Iceland, May. ELRA, European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Vincent Della Pietra</author>
<author>Peter deSouza</author>
<author>Jenifer Lai</author>
<author>Robert Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8810" citStr="Brown et al., 1992" startWordPosition="1358" endWordPosition="1361">(Gao and Vogel, 2008) as the word aligner; this phrase table was trained using the Russian-English Common Crawl, News Commentary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2.2 Language Model Training During the training process we built n-gram language models (LMs) for use in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased language models (Brown et al., 1992) were also trained, for later use in n-best list rescoring, using the SRILM language modelling toolkit (Stolcke, 2002).We trained a 6- gram language model from the LDC English Gigaword Fifth Edition, for use in both the Hindi-English and Russian-English systems. All language models were binarized in order to reduce model disk usage and loading time. For the Russian-to-English task, we concatenated the English portion of the parallel training data for the WMT 2014 shared translation task (Common Crawl, News Commentary, Wiki Headlines and Yandex corpora) in addition to the shared task English mo</context>
<context position="11596" citStr="Brown et al., 1992" startWordPosition="1819" endWordPosition="1822">translation probabilities Pw(fj |ei) to obtain a sentence-level averaged lexical translation score (Eq. 2), which is added as an additional feature to each n-best list entry. ∏ Plex(F |E) = jE1...J (2) Shen et al. (2006) use the term “IBM model 1 score” to describe the value calculated in Eq. 2. While the lexical probability distribution 1 ∑ Pw(fj |ei) I + 1 iE1...I 188 from IBM Model 1 (Brown et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) defined by Koehn et al. (2003). We also add a 7-gram class language model score P,ass(E) (Brown et al., 1992) as an additional feature of each n-best list entry. After adding these features to each translation in an n-best list, Eq. 1 is applied, rescoring the entries to extract new 1-best translations. To optimize system performance we train scaling factors, A,., for both decoding and rescoring features so as to minimize an objective error criterion. In our systems we use DREM (Kazi et al., 2013) or PRO (Hopkins and May, 2011) to perform this optimization. For development data during optimization, we used newstest2013 for the Russian-toEnglish task and newsdev2014 for the Hindito-English task suppli</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter Brown, Vincent Della Pietra, Peter deSouza, Jenifer Lai, and Robert Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Vincent Della Pietra</author>
<author>Stephen Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11387" citStr="Brown et al., 1993" startWordPosition="1776" endWordPosition="1779">ing features, namely the Operation Sequence Model (Durrani et al., 2011) and Lexicalized Reordering Model (Tillman, 2004; Galley and Manning, 2008). Following Shen et al. (2006), we use the word-level lexical translation probabilities Pw(fj |ei) to obtain a sentence-level averaged lexical translation score (Eq. 2), which is added as an additional feature to each n-best list entry. ∏ Plex(F |E) = jE1...J (2) Shen et al. (2006) use the term “IBM model 1 score” to describe the value calculated in Eq. 2. While the lexical probability distribution 1 ∑ Pw(fj |ei) I + 1 iE1...I 188 from IBM Model 1 (Brown et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) defined by Koehn et al. (2003). We also add a 7-gram class language model score P,ass(E) (Brown et al., 1992) as an additional feature of each n-best list entry. After adding these features to each translation in an n-best list, Eq. 1 is applied, rescoring the entries to extract new 1-best translations. To optimize system performance we train scaling factors, A,., for both decoding and rescoring features so as to minimize an objective error criterion. In our systems we use DREM (Kazi et al., 201</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Vincent Della Pietra, Stephen Della Pietra, and Robert Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Linear B system description for the 2005 NIST MT evaluation exercise.</title>
<date>2005</date>
<booktitle>In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</booktitle>
<contexts>
<context position="19273" citStr="Callison-Burch, 2005" startWordPosition="3103" endWordPosition="3104"> be the case especially when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of the sentences, while forwarding on the most difficult sentences to more highly skilled bilingual translators. Small-scale studies have suggested that monolingual human posteditors, working without knowledge of the source language, can also improve the quality of machine translation output (Callison-Burch, 2005; Koehn, 2010a; Mitchell et al., 2013), especially if welldesigned tools provide automated linguistic analysis of source sentences (Albrecht et al., 2009). In this study, we designed a simple user interface for postediting that presents the user with the source sentence, machine translation, and word alignments for each sentence in a test document (Figure 1). While it may seem counter-intuitive to present monolingual posteditors with the source sentence, we found that the presence of alignment links between source words and target words can in fact aid a monolingual posteditor, especially with</context>
</contexts>
<marker>Callison-Burch, 2005</marker>
<rawString>Chris Callison-Burch. 2005. Linear B system description for the 2005 NIST MT evaluation exercise. In Proceedings of the NIST 2005 Machine Translation Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Davis</author>
<author>Ken Whistler</author>
</authors>
<title>Unicode normalization forms.</title>
<date>2013</date>
<journal>The Unicode Consortium, September. Rev.</journal>
<tech>Technical Report UAX #15,</tech>
<volume>39</volume>
<contexts>
<context position="3683" citStr="Davis and Whistler, 2013" startWordPosition="548" endWordPosition="551">ses detokenization scripts. In addition to normalizing various extended Latin punctuation marks to their Basic Latin equivalents, following Bojar et al. (2010) we normalized DEVANAGARI DANDA (U+0964), DOUBLE DANDA (U+0965), and ABBREVIATION SIGN (U+0970) punctuation marks to Latin FULL STOP (U+002E), any DEVANA186 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 186–194, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics GARI DIGIT to the equivalent ASCII DIGIT, and decomposed all Hindi data into Unicode Normalization Form D (Davis and Whistler, 2013) using charlint.1 In addition, we performed Hindi diacritic and vowel normalization, following Larkey et al. (2003). Since no Hindi-English development test set was provided in WMT14, we randomly sampled 1500 sentence pairs from the HindiEnglish parallel training data to serve this purpose. Upon discovering duplicate sentences in the corpus, 552 sentences that overlapped with the training portion were removed from the sample, leaving a development test set of 948 sentences. 2.1.2 Russian Processing The Russian sentences contained many examples of mixed-character spelling, in which both Latin a</context>
</contexts>
<marker>Davis, Whistler, 2013</marker>
<rawString>Mark Davis and Ken Whistler. 2013. Unicode normalization forms. Technical Report UAX #15, The Unicode Consortium, September. Rev. 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>TransCenter: Web-based translation research suite.</title>
<date>2012</date>
<booktitle>In Proceedings of the AMTA 2012 Workshop on Post-Editing Technology and Practice Demo Session,</booktitle>
<contexts>
<context position="18406" citStr="Denkowski and Lavie, 2012" startWordPosition="2972" endWordPosition="2975">t for System 5. 3 Monolingual Postediting Postediting is the process whereby a human user corrects the output of a machine translation system. The use of basic postediting tools by bilingual human translators has been shown to yield substantial increases in terms of productivity (Plitt and Masselot, 2010) as well as improvements in translation quality (Green et al., 2013) when compared to bilingual human translators working without assistance from machine translation and postediting tools. More sophisticated interactive interfaces (Langlais et al., 2000; Barrachina et al., 2009; Koehn, 2009b; Denkowski and Lavie, 2012) may also provide benefit (Koehn, 2009a). We hypothesize that for at least some language pairs, monolingual posteditors with no knowledge of the source language can successfully translate a substantial fraction of test sentences. We expect this to be the case especially when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of the sentences, while forwarding on the most </context>
</contexts>
<marker>Denkowski, Lavie, 2012</marker>
<rawString>Michael Denkowski and Alon Lavie. 2012. TransCenter: Web-based translation research suite. In Proceedings of the AMTA 2012 Workshop on Post-Editing Technology and Practice Demo Session, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11),</booktitle>
<pages>1045--1054</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="10840" citStr="Durrani et al., 2011" startWordPosition="1681" endWordPosition="1684">able 1: Models used in log-linear combination 2.2.3 Decoding, n-best List Rescoring, and Optimization We decode using the phrase-based moses decoder (Koehn et al., 2007), choosing the best translation for each source sentence according to a linear combination of decoding features: ∑ E� = arg max arhr(E, F) (1) E br We make use of a standard set of decoding features, listed in Table 1. In contrast to our IWSLT 2013 system, all experiments submitted to this year’s WMT evaluation made use of version 2.1 of moses, and incorporated additional decoding features, namely the Operation Sequence Model (Durrani et al., 2011) and Lexicalized Reordering Model (Tillman, 2004; Galley and Manning, 2008). Following Shen et al. (2006), we use the word-level lexical translation probabilities Pw(fj |ei) to obtain a sentence-level averaged lexical translation score (Eq. 2), which is added as an additional feature to each n-best list entry. ∏ Plex(F |E) = jE1...J (2) Shen et al. (2006) use the term “IBM model 1 score” to describe the value calculated in Eq. 2. While the lexical probability distribution 1 ∑ Pw(fj |ei) I + 1 iE1...I 188 from IBM Model 1 (Brown et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2,</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL ’11), pages 1045–1054, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP ’08),</booktitle>
<pages>848--856</pages>
<location>Honolulu, Hawai‘i,</location>
<contexts>
<context position="10915" citStr="Galley and Manning, 2008" startWordPosition="1691" endWordPosition="1694">st Rescoring, and Optimization We decode using the phrase-based moses decoder (Koehn et al., 2007), choosing the best translation for each source sentence according to a linear combination of decoding features: ∑ E� = arg max arhr(E, F) (1) E br We make use of a standard set of decoding features, listed in Table 1. In contrast to our IWSLT 2013 system, all experiments submitted to this year’s WMT evaluation made use of version 2.1 of moses, and incorporated additional decoding features, namely the Operation Sequence Model (Durrani et al., 2011) and Lexicalized Reordering Model (Tillman, 2004; Galley and Manning, 2008). Following Shen et al. (2006), we use the word-level lexical translation probabilities Pw(fj |ei) to obtain a sentence-level averaged lexical translation score (Eq. 2), which is added as an additional feature to each n-best list entry. ∏ Plex(F |E) = jE1...J (2) Shen et al. (2006) use the term “IBM model 1 score” to describe the value calculated in Eq. 2. While the lexical probability distribution 1 ∑ Pw(fj |ei) I + 1 iE1...I 188 from IBM Model 1 (Brown et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) defined by Koehn et al. (200</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP ’08), pages 848–856, Honolulu, Hawai‘i, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="8212" citStr="Gao and Vogel, 2008" startWordPosition="1265" endWordPosition="1268">e already present in wiki.ru-en (about 0.17%). 3Specifically, we allowed lines containing ґ where it appears as an encoding error in place of an apostrophe within English words. For example: “Песня The Kelly Family Iґm So Happy представлена вам LyricsKeeper.” 187 2.2 Machine Translation Our baseline system is a variant of the MITLL/AFRL IWSLT 2013 system (Kazi et al., 2013) with some modifications to the training and decoding processes. 2.2.1 Phrase Table Training For our Russian-English system, we trained a phrase table using the Moses Experiment Management System (Koehn, 2010b), with mgiza (Gao and Vogel, 2008) as the word aligner; this phrase table was trained using the Russian-English Common Crawl, News Commentary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2.2 Language Model Training During the training process we built n-gram language models (LMs) for use in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased language models (Brown et al., 1992) w</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing and Quality Assurance for Natural Language Processing, pages 49–57, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Jeffrey Heer</author>
<author>Christopher D Manning</author>
</authors>
<title>The efficacy of human postediting for language translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI ’13),</booktitle>
<pages>439--448</pages>
<location>Paris, France, April–May.</location>
<contexts>
<context position="18154" citStr="Green et al., 2013" startWordPosition="2936" endWordPosition="2939"> to time constraints, the monolingual postediting experiments in §3 were conducted (using the machine translation results from System 4) before the results of Systems 2 and 3 were available. The Moses recaser was applied in all experiments except for System 5. 3 Monolingual Postediting Postediting is the process whereby a human user corrects the output of a machine translation system. The use of basic postediting tools by bilingual human translators has been shown to yield substantial increases in terms of productivity (Plitt and Masselot, 2010) as well as improvements in translation quality (Green et al., 2013) when compared to bilingual human translators working without assistance from machine translation and postediting tools. More sophisticated interactive interfaces (Langlais et al., 2000; Barrachina et al., 2009; Koehn, 2009b; Denkowski and Lavie, 2012) may also provide benefit (Koehn, 2009a). We hypothesize that for at least some language pairs, monolingual posteditors with no knowledge of the source language can successfully translate a substantial fraction of test sentences. We expect this to be the case especially when the monolingual humans are domain experts with regard to the documents t</context>
</contexts>
<marker>Green, Heer, Manning, 2013</marker>
<rawString>Spence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human postediting for language translation. In Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI ’13), pages 439–448, Paris, France, April–May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL ’13),</booktitle>
<pages>690--696</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8761" citStr="Heafield et al., 2013" startWordPosition="1350" endWordPosition="1353">riment Management System (Koehn, 2010b), with mgiza (Gao and Vogel, 2008) as the word aligner; this phrase table was trained using the Russian-English Common Crawl, News Commentary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2.2 Language Model Training During the training process we built n-gram language models (LMs) for use in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased language models (Brown et al., 1992) were also trained, for later use in n-best list rescoring, using the SRILM language modelling toolkit (Stolcke, 2002).We trained a 6- gram language model from the LDC English Gigaword Fifth Edition, for use in both the Hindi-English and Russian-English systems. All language models were binarized in order to reduce model disk usage and loading time. For the Russian-to-English task, we concatenated the English portion of the parallel training data for the WMT 2014 shared translation task (Common Crawl, News Commentary, Wiki Headlines and Yandex c</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL ’13), pages 690–696, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP ’11),</booktitle>
<pages>1352--1362</pages>
<location>Edinburgh, Scotland, U.K.</location>
<contexts>
<context position="12020" citStr="Hopkins and May, 2011" startWordPosition="1893" endWordPosition="1896">act be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) defined by Koehn et al. (2003). We also add a 7-gram class language model score P,ass(E) (Brown et al., 1992) as an additional feature of each n-best list entry. After adding these features to each translation in an n-best list, Eq. 1 is applied, rescoring the entries to extract new 1-best translations. To optimize system performance we train scaling factors, A,., for both decoding and rescoring features so as to minimize an objective error criterion. In our systems we use DREM (Kazi et al., 2013) or PRO (Hopkins and May, 2011) to perform this optimization. For development data during optimization, we used newstest2013 for the Russian-toEnglish task and newsdev2014 for the Hindito-English task supplied by WMT14. 2.2.4 Unknown Words For the Hindi-to-English task, unknown words were marked during the decoding process and were transliterated by the icu4j Devanagarito-Latin transliterator.4 For the Russian-to-English task, we selectively stemmed and inflected input words not found in the phrase table. Each input sentence was examined to identify any source words which did not occur as a phrase of length 1 in the phrase </context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP ’11), pages 1352–1362, Edinburgh, Scotland, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaeel Kazi</author>
<author>Michael Coury</author>
<author>Elizabeth Salesky</author>
<author>Jessica Ray</author>
<author>Wade Shen</author>
<author>Terry Gleason</author>
<author>Tim Anderson</author>
<author>Grant Erdmann</author>
<author>Lane Schwartz</author>
<author>Brian Ore</author>
<author>Raymond Slyh</author>
<author>Jeremy Gwinnup</author>
<author>Katherine Young</author>
<author>Michael Hutt</author>
</authors>
<date>2013</date>
<booktitle>The MIT-LL/AFRL IWSLT-2013 MT system. In The 10th International Workshop on Spoken Language Translation (IWSLT ’13),</booktitle>
<pages>136--143</pages>
<location>Heidelberg, Germany,</location>
<contexts>
<context position="1132" citStr="Kazi et al., 2013" startWordPosition="162" endWordPosition="165">glish translation tasks. In addition, we describe our efforts to make use of monolingual English speakers to correct the output of machine translation, and present the results of monolingual postediting of the entire 3003 sentences of the WMT14 Russian-English test set. 1 Introduction As part of the 2014 Workshop on Machine Translation (WMT14) shared translation task, the human language technology team at the Air Force Research Laboratory participated in two language pairs: Russian-English and Hindi-English. Our machine translation system represents enhancements to our system from IWSLT 2013 (Kazi et al., 2013). In this paper, we focus on enhancements to our procedures with regard to data processing and the handling of unknown words. In addition, we describe our efforts to make use of monolingual English speakers to correct the output of machine translation, and present the results of monolingual postediting of the entire 3003 sentences of the WMT14 RussianEnglish test set. Using a binary adequacy classification, we evaluate the entire postedited †This work is sponsored by the Air Force Research Laboratory under Air Force contract FA-8650-09-D6939-029. Timothy Anderson Air Force Research Laboratory </context>
<context position="7968" citStr="Kazi et al., 2013" startWordPosition="1229" endWordPosition="1232">red translation task. Two files in this parallel corpus (wiki.ru-en and guessed-names.ru-en) contained some overlapping data. We removed 6415 duplicate lines within wiki.ru-en (about 1.4%), and removed 94 lines of guessed-names.ru-en that were already present in wiki.ru-en (about 0.17%). 3Specifically, we allowed lines containing ґ where it appears as an encoding error in place of an apostrophe within English words. For example: “Песня The Kelly Family Iґm So Happy представлена вам LyricsKeeper.” 187 2.2 Machine Translation Our baseline system is a variant of the MITLL/AFRL IWSLT 2013 system (Kazi et al., 2013) with some modifications to the training and decoding processes. 2.2.1 Phrase Table Training For our Russian-English system, we trained a phrase table using the Moses Experiment Management System (Koehn, 2010b), with mgiza (Gao and Vogel, 2008) as the word aligner; this phrase table was trained using the Russian-English Common Crawl, News Commentary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2</context>
<context position="11989" citStr="Kazi et al., 2013" startWordPosition="1887" endWordPosition="1890">wn et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) defined by Koehn et al. (2003). We also add a 7-gram class language model score P,ass(E) (Brown et al., 1992) as an additional feature of each n-best list entry. After adding these features to each translation in an n-best list, Eq. 1 is applied, rescoring the entries to extract new 1-best translations. To optimize system performance we train scaling factors, A,., for both decoding and rescoring features so as to minimize an objective error criterion. In our systems we use DREM (Kazi et al., 2013) or PRO (Hopkins and May, 2011) to perform this optimization. For development data during optimization, we used newstest2013 for the Russian-toEnglish task and newsdev2014 for the Hindito-English task supplied by WMT14. 2.2.4 Unknown Words For the Hindi-to-English task, unknown words were marked during the decoding process and were transliterated by the icu4j Devanagarito-Latin transliterator.4 For the Russian-to-English task, we selectively stemmed and inflected input words not found in the phrase table. Each input sentence was examined to identify any source words which did not occur as a ph</context>
</contexts>
<marker>Kazi, Coury, Salesky, Ray, Shen, Gleason, Anderson, Erdmann, Schwartz, Ore, Slyh, Gwinnup, Young, Hutt, 2013</marker>
<rawString>Michaeel Kazi, Michael Coury, Elizabeth Salesky, Jessica Ray, Wade Shen, Terry Gleason, Tim Anderson, Grant Erdmann, Lane Schwartz, Brian Ore, Raymond Slyh, Jeremy Gwinnup, Katherine Young, and Michael Hutt. 2013. The MIT-LL/AFRL IWSLT-2013 MT system. In The 10th International Workshop on Spoken Language Translation (IWSLT ’13), pages 136–143, Heidelberg, Germany, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’13),</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada, May–June.</location>
<contexts>
<context position="11517" citStr="Koehn et al. (2003)" startWordPosition="1805" endWordPosition="1808">nd Manning, 2008). Following Shen et al. (2006), we use the word-level lexical translation probabilities Pw(fj |ei) to obtain a sentence-level averaged lexical translation score (Eq. 2), which is added as an additional feature to each n-best list entry. ∏ Plex(F |E) = jE1...J (2) Shen et al. (2006) use the term “IBM model 1 score” to describe the value calculated in Eq. 2. While the lexical probability distribution 1 ∑ Pw(fj |ei) I + 1 iE1...I 188 from IBM Model 1 (Brown et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) defined by Koehn et al. (2003). We also add a 7-gram class language model score P,ass(E) (Brown et al., 1992) as an additional feature of each n-best list entry. After adding these features to each translation in an n-best list, Eq. 1 is applied, rescoring the entries to extract new 1-best translations. To optimize system performance we train scaling factors, A,., for both decoding and rescoring features so as to minimize an objective error criterion. In our systems we use DREM (Kazi et al., 2013) or PRO (Hopkins and May, 2011) to perform this optimization. For development data during optimization, we used newstest2013 for</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’13), pages 48–54, Edmonton, Canada, May–June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL ’07) Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2426" citStr="Koehn et al., 2007" startWordPosition="358" endWordPosition="361">erine.young.1.ctr@us.af.mil test set for correctness against the reference translations. Using bilingual judges, we further evaluate a substantial subset of the postedited test set using a more fine-grained adequacy metric; using this metric, we show that monolingual posteditors can successfully produce postedited translations that convey all or most of the meaning of the original source sentence in up to 87.8% of sentences. 2 System Description We submitted systems for the Russian-toEnglish and Hindi-to-English MT shared tasks. In all submitted systems, we use the phrase-based moses decoder (Koehn et al., 2007). We used only the constrained data supplied by the evaluation for each language pair for training our systems. 2.1 Data Preparation Before training our systems, a cleaning pass was performed on all data. Unicode characters in the unallocated and private use ranges were all removed, along with C0 and C1 control characters, zero-width and non-breaking spaces and joiners, directionality and paragraph markers. 2.1.1 Hindi Processing The HindEnCorp corpus (Bojar et al., 2014) is distributed in tokenized form; in order to ensure a uniform tokenization standard across all of our data, we began by de</context>
<context position="10388" citStr="Koehn et al., 2007" startWordPosition="1602" endWordPosition="1605">ns of the HindEnCorp and Wikipedia headlines parallel corpora, and from the monolingual English sections of the Europarl and News Crawl corpora. Decoding Features P(f |e) P(e |f) Pw(f |e) Pw(e |f) Phrase Penalty Lexical Backoff Word Penalty Distortion Model Unknown Word Penalty Lexicalized Reordering Model Operation Sequence Model Rescoring Features Pclass(E) – 7-gram class-based LM Plex(F |E) – sentence-level averaged lexical translation score Table 1: Models used in log-linear combination 2.2.3 Decoding, n-best List Rescoring, and Optimization We decode using the phrase-based moses decoder (Koehn et al., 2007), choosing the best translation for each source sentence according to a linear combination of decoding features: ∑ E� = arg max arhr(E, F) (1) E br We make use of a standard set of decoding features, listed in Table 1. In contrast to our IWSLT 2013 system, all experiments submitted to this year’s WMT evaluation made use of version 2.1 of moses, and incorporated additional decoding features, namely the Operation Sequence Model (Durrani et al., 2011) and Lexicalized Reordering Model (Tillman, 2004; Galley and Manning, 2008). Following Shen et al. (2006), we use the word-level lexical translation</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL ’07) Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A process study of computer aided translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="18377" citStr="Koehn, 2009" startWordPosition="2970" endWordPosition="2971">eriments except for System 5. 3 Monolingual Postediting Postediting is the process whereby a human user corrects the output of a machine translation system. The use of basic postediting tools by bilingual human translators has been shown to yield substantial increases in terms of productivity (Plitt and Masselot, 2010) as well as improvements in translation quality (Green et al., 2013) when compared to bilingual human translators working without assistance from machine translation and postediting tools. More sophisticated interactive interfaces (Langlais et al., 2000; Barrachina et al., 2009; Koehn, 2009b; Denkowski and Lavie, 2012) may also provide benefit (Koehn, 2009a). We hypothesize that for at least some language pairs, monolingual posteditors with no knowledge of the source language can successfully translate a substantial fraction of test sentences. We expect this to be the case especially when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of the sentences, </context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Philipp Koehn. 2009a. A process study of computer aided translation. Machine Translation, 23(4):241–263, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A web-based interactive computer aided translation tool.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Software Demonstrations,</booktitle>
<pages>17--20</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="18377" citStr="Koehn, 2009" startWordPosition="2970" endWordPosition="2971">eriments except for System 5. 3 Monolingual Postediting Postediting is the process whereby a human user corrects the output of a machine translation system. The use of basic postediting tools by bilingual human translators has been shown to yield substantial increases in terms of productivity (Plitt and Masselot, 2010) as well as improvements in translation quality (Green et al., 2013) when compared to bilingual human translators working without assistance from machine translation and postediting tools. More sophisticated interactive interfaces (Langlais et al., 2000; Barrachina et al., 2009; Koehn, 2009b; Denkowski and Lavie, 2012) may also provide benefit (Koehn, 2009a). We hypothesize that for at least some language pairs, monolingual posteditors with no knowledge of the source language can successfully translate a substantial fraction of test sentences. We expect this to be the case especially when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of the sentences, </context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Philipp Koehn. 2009b. A web-based interactive computer aided translation tool. In Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 17–20, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Enabling monolingual translators: Post-editing vs. options. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’10),</booktitle>
<pages>537--545</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="8176" citStr="Koehn, 2010" startWordPosition="1261" endWordPosition="1262">uessed-names.ru-en that were already present in wiki.ru-en (about 0.17%). 3Specifically, we allowed lines containing ґ where it appears as an encoding error in place of an apostrophe within English words. For example: “Песня The Kelly Family Iґm So Happy представлена вам LyricsKeeper.” 187 2.2 Machine Translation Our baseline system is a variant of the MITLL/AFRL IWSLT 2013 system (Kazi et al., 2013) with some modifications to the training and decoding processes. 2.2.1 Phrase Table Training For our Russian-English system, we trained a phrase table using the Moses Experiment Management System (Koehn, 2010b), with mgiza (Gao and Vogel, 2008) as the word aligner; this phrase table was trained using the Russian-English Common Crawl, News Commentary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2.2 Language Model Training During the training process we built n-gram language models (LMs) for use in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased la</context>
<context position="19286" citStr="Koehn, 2010" startWordPosition="3105" endWordPosition="3106">y when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of the sentences, while forwarding on the most difficult sentences to more highly skilled bilingual translators. Small-scale studies have suggested that monolingual human posteditors, working without knowledge of the source language, can also improve the quality of machine translation output (Callison-Burch, 2005; Koehn, 2010a; Mitchell et al., 2013), especially if welldesigned tools provide automated linguistic analysis of source sentences (Albrecht et al., 2009). In this study, we designed a simple user interface for postediting that presents the user with the source sentence, machine translation, and word alignments for each sentence in a test document (Figure 1). While it may seem counter-intuitive to present monolingual posteditors with the source sentence, we found that the presence of alignment links between source words and target words can in fact aid a monolingual posteditor, especially with regard to co</context>
<context position="20802" citStr="Koehn (2010" startWordPosition="3335" endWordPosition="3336">r phrase outside the enclosing punctuation; by examining the alignment links the posteditors were able to correct such reordering mistakes. The Russian-English test set comprises 175 documents in the news domain, totaling 3003 sentences. We assigned each test document to one of 8 monolingual5 posteditors (Table 3). The postediting tool did not record timing information. However, several posteditors informally reported that they were able to process on average approximately four documents per hour; if accurate, this would indicate a processing speed of around one sentence per minute. Following Koehn (2010a), we evaluated postedited translation quality according to a binary adequacy metric, as judged by a monolingual English speaker6 against the En5All posteditors are native English speakers. Posteditors 2 and 3 know Chinese and Arabic, respectively, but not Russian. Posteditor 8 understands the Cyrillic character set and has a minimal Russian vocabulary from two undergraduate semesters of Russian taken several years ago. 6All monolingual adequacy judgements were performed by Posteditor 1. Additional analysis of Posteditor 1’s 950 postedited translations were independently judged by bilingual j</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010a. Enabling monolingual translators: Post-editing vs. options. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’10), pages 537–545, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>An experimental management system.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>94--87</pages>
<contexts>
<context position="8176" citStr="Koehn, 2010" startWordPosition="1261" endWordPosition="1262">uessed-names.ru-en that were already present in wiki.ru-en (about 0.17%). 3Specifically, we allowed lines containing ґ where it appears as an encoding error in place of an apostrophe within English words. For example: “Песня The Kelly Family Iґm So Happy представлена вам LyricsKeeper.” 187 2.2 Machine Translation Our baseline system is a variant of the MITLL/AFRL IWSLT 2013 system (Kazi et al., 2013) with some modifications to the training and decoding processes. 2.2.1 Phrase Table Training For our Russian-English system, we trained a phrase table using the Moses Experiment Management System (Koehn, 2010b), with mgiza (Gao and Vogel, 2008) as the word aligner; this phrase table was trained using the Russian-English Common Crawl, News Commentary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2.2 Language Model Training During the training process we built n-gram language models (LMs) for use in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased la</context>
<context position="19286" citStr="Koehn, 2010" startWordPosition="3105" endWordPosition="3106">y when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation process, postediting many of the sentences, while forwarding on the most difficult sentences to more highly skilled bilingual translators. Small-scale studies have suggested that monolingual human posteditors, working without knowledge of the source language, can also improve the quality of machine translation output (Callison-Burch, 2005; Koehn, 2010a; Mitchell et al., 2013), especially if welldesigned tools provide automated linguistic analysis of source sentences (Albrecht et al., 2009). In this study, we designed a simple user interface for postediting that presents the user with the source sentence, machine translation, and word alignments for each sentence in a test document (Figure 1). While it may seem counter-intuitive to present monolingual posteditors with the source sentence, we found that the presence of alignment links between source words and target words can in fact aid a monolingual posteditor, especially with regard to co</context>
<context position="20802" citStr="Koehn (2010" startWordPosition="3335" endWordPosition="3336">r phrase outside the enclosing punctuation; by examining the alignment links the posteditors were able to correct such reordering mistakes. The Russian-English test set comprises 175 documents in the news domain, totaling 3003 sentences. We assigned each test document to one of 8 monolingual5 posteditors (Table 3). The postediting tool did not record timing information. However, several posteditors informally reported that they were able to process on average approximately four documents per hour; if accurate, this would indicate a processing speed of around one sentence per minute. Following Koehn (2010a), we evaluated postedited translation quality according to a binary adequacy metric, as judged by a monolingual English speaker6 against the En5All posteditors are native English speakers. Posteditors 2 and 3 know Chinese and Arabic, respectively, but not Russian. Posteditor 8 understands the Cyrillic character set and has a minimal Russian vocabulary from two undergraduate semesters of Russian taken several years ago. 6All monolingual adequacy judgements were performed by Posteditor 1. Additional analysis of Posteditor 1’s 950 postedited translations were independently judged by bilingual j</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010b. An experimental management system. The Prague Bulletin of Mathematical Linguistics, 94:87–96, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>George Foster</author>
<author>Guy Lapalme</author>
</authors>
<title>TransType: A computer-aided translation typing system.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL 2000 Workshop on Embedded Machine Translation Systems,</booktitle>
<pages>46--51</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="18339" citStr="Langlais et al., 2000" startWordPosition="2962" endWordPosition="2965">ilable. The Moses recaser was applied in all experiments except for System 5. 3 Monolingual Postediting Postediting is the process whereby a human user corrects the output of a machine translation system. The use of basic postediting tools by bilingual human translators has been shown to yield substantial increases in terms of productivity (Plitt and Masselot, 2010) as well as improvements in translation quality (Green et al., 2013) when compared to bilingual human translators working without assistance from machine translation and postediting tools. More sophisticated interactive interfaces (Langlais et al., 2000; Barrachina et al., 2009; Koehn, 2009b; Denkowski and Lavie, 2012) may also provide benefit (Koehn, 2009a). We hypothesize that for at least some language pairs, monolingual posteditors with no knowledge of the source language can successfully translate a substantial fraction of test sentences. We expect this to be the case especially when the monolingual humans are domain experts with regard to the documents to be translated. If this hypothesis is confirmed, this could allow for multi-stage translation workflows, where less highly skilled monolingual posteditors triage the translation proces</context>
</contexts>
<marker>Langlais, Foster, Lapalme, 2000</marker>
<rawString>Philippe Langlais, George Foster, and Guy Lapalme. 2000. TransType: A computer-aided translation typing system. In Proceedings of the ANLP/NAACL 2000 Workshop on Embedded Machine Translation Systems, pages 46–51, Seattle, Washington, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>Margaret E Connell</author>
<author>Nasreen Abduljaleel</author>
</authors>
<title>Hindi CLIR in thirty days.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="3798" citStr="Larkey et al. (2003)" startWordPosition="566" endWordPosition="569">ivalents, following Bojar et al. (2010) we normalized DEVANAGARI DANDA (U+0964), DOUBLE DANDA (U+0965), and ABBREVIATION SIGN (U+0970) punctuation marks to Latin FULL STOP (U+002E), any DEVANA186 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 186–194, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics GARI DIGIT to the equivalent ASCII DIGIT, and decomposed all Hindi data into Unicode Normalization Form D (Davis and Whistler, 2013) using charlint.1 In addition, we performed Hindi diacritic and vowel normalization, following Larkey et al. (2003). Since no Hindi-English development test set was provided in WMT14, we randomly sampled 1500 sentence pairs from the HindiEnglish parallel training data to serve this purpose. Upon discovering duplicate sentences in the corpus, 552 sentences that overlapped with the training portion were removed from the sample, leaving a development test set of 948 sentences. 2.1.2 Russian Processing The Russian sentences contained many examples of mixed-character spelling, in which both Latin and Cyrillic characters are used in a single word, relying on the visual similarity of the characters. For example, </context>
</contexts>
<marker>Larkey, Connell, Abduljaleel, 2003</marker>
<rawString>Leah S. Larkey, Margaret E. Connell, and Nasreen Abduljaleel. 2003. Hindi CLIR in thirty days. ACM Transactions on Asian Language Information Processing (TALIP), 2(2):130–142, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linda Mitchell</author>
<author>Johann Roturier</author>
<author>Sharon O’Brien</author>
</authors>
<title>Community-based post-editing of machine translation content: monolingual vs. bilingual.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Workshop on Post-editing Technology and Practice (WPTP-2),</booktitle>
<pages>35--43</pages>
<publisher>EAMT.</publisher>
<location>Nice, France,</location>
<marker>Mitchell, Roturier, O’Brien, 2013</marker>
<rawString>Linda Mitchell, Johann Roturier, and Sharon O’Brien. 2013. Community-based post-editing of machine translation content: monolingual vs. bilingual. In Proceedings of the 2nd Workshop on Post-editing Technology and Practice (WPTP-2), pages 35–43, Nice, France, September. EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL ’02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="13657" citStr="Papineni et al., 2002" startWordPosition="2160" endWordPosition="2163">ginal Russian file. If multiple candidates were found, we used the one with the highest frequency of occurrence in the training data. This process replaces words that we know we cannot translate with semantically similar words that we can translate, replacing unknown words like c)OTOHOM “photon” (instrumental case) with a known morphological variant c)OTOH “photon” (nominative case) that is found in the 4http://site.icu-project.org BLEU BLEU-cased System 1 hi-en 13.1 12.1 2 ru-en 32.0 30.8 3 ru-en 32.2 31.0 4 ru-en 31.5 30.3 5 ru-en 33.0 31.1 Table 2: Translation results, as measured by BLEU (Papineni et al., 2002). phrase table. Selective stemming of just the unknown words allows us to retain information that would be lost if we applied stemming to all the data. Any remaining unknown words were transliterated as a post-process, using a simple letter-mapping from Cyrillic characters to Latin characters representing their typical sounds. 2.3 MT Results Our best Hindi-English system for newstest2014 is listed in Table 2 as System 1. This system uses a combination of 6-gram language models built from HindEnCorp, News Commentary, Europarl, and News Crawl corpora. Transliteration of unknown words was perform</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL ’02), pages 311–318, Philadelphia, Pennsylvania, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirko Plitt</author>
<author>Franc�ois Masselot</author>
</authors>
<title>A productivity test of statistical machine translation post-editing in a typical localisation context. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<pages>93--7</pages>
<contexts>
<context position="18086" citStr="Plitt and Masselot, 2010" startWordPosition="2925" endWordPosition="2928"> 4 (a variant of System 2 tuned using PRO) uncased output is System 5. Due to time constraints, the monolingual postediting experiments in §3 were conducted (using the machine translation results from System 4) before the results of Systems 2 and 3 were available. The Moses recaser was applied in all experiments except for System 5. 3 Monolingual Postediting Postediting is the process whereby a human user corrects the output of a machine translation system. The use of basic postediting tools by bilingual human translators has been shown to yield substantial increases in terms of productivity (Plitt and Masselot, 2010) as well as improvements in translation quality (Green et al., 2013) when compared to bilingual human translators working without assistance from machine translation and postediting tools. More sophisticated interactive interfaces (Langlais et al., 2000; Barrachina et al., 2009; Koehn, 2009b; Denkowski and Lavie, 2012) may also provide benefit (Koehn, 2009a). We hypothesize that for at least some language pairs, monolingual posteditors with no knowledge of the source language can successfully translate a substantial fraction of test sentences. We expect this to be the case especially when the </context>
</contexts>
<marker>Plitt, Masselot, 2010</marker>
<rawString>Mirko Plitt and Franc�ois Masselot. 2010. A productivity test of statistical machine translation post-editing in a typical localisation context. The Prague Bulletin of Mathematical Linguistics, 93:7–16, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, England,</location>
<contexts>
<context position="12687" citStr="Schmid, 1994" startWordPosition="1998" endWordPosition="1999">ing optimization, we used newstest2013 for the Russian-toEnglish task and newsdev2014 for the Hindito-English task supplied by WMT14. 2.2.4 Unknown Words For the Hindi-to-English task, unknown words were marked during the decoding process and were transliterated by the icu4j Devanagarito-Latin transliterator.4 For the Russian-to-English task, we selectively stemmed and inflected input words not found in the phrase table. Each input sentence was examined to identify any source words which did not occur as a phrase of length 1 in the phrase table. For each such unknown word, we used treetagger (Schmid, 1994; Schmid, 1995) to identify the part of speech, and then we removed inflectional endings to derive a stem. We applied all possible Russian inflectional endings for the given part of speech; if an inflected form of the unknown word could be found as a stand-alone phrase in the phrase table, that form was used to replace the unknown word in the original Russian file. If multiple candidates were found, we used the one with the highest frequency of occurrence in the training data. This process replaces words that we know we cannot translate with semantically similar words that we can translate, re</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, Manchester, England, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in part-ofspeech tagging with an application to German.</title>
<date>1995</date>
<booktitle>In Proceedings of the EACL SIGDAT Workshop,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="12702" citStr="Schmid, 1995" startWordPosition="2000" endWordPosition="2001">on, we used newstest2013 for the Russian-toEnglish task and newsdev2014 for the Hindito-English task supplied by WMT14. 2.2.4 Unknown Words For the Hindi-to-English task, unknown words were marked during the decoding process and were transliterated by the icu4j Devanagarito-Latin transliterator.4 For the Russian-to-English task, we selectively stemmed and inflected input words not found in the phrase table. Each input sentence was examined to identify any source words which did not occur as a phrase of length 1 in the phrase table. For each such unknown word, we used treetagger (Schmid, 1994; Schmid, 1995) to identify the part of speech, and then we removed inflectional endings to derive a stem. We applied all possible Russian inflectional endings for the given part of speech; if an inflected form of the unknown word could be found as a stand-alone phrase in the phrase table, that form was used to replace the unknown word in the original Russian file. If multiple candidates were found, we used the one with the highest frequency of occurrence in the training data. This process replaces words that we know we cannot translate with semantically similar words that we can translate, replacing unknown</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>Helmut Schmid. 1995. Improvements in part-ofspeech tagging with an application to German. In Proceedings of the EACL SIGDAT Workshop, Dublin, Ireland, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wade Shen</author>
<author>Brian Delaney</author>
<author>Tim Anderson</author>
</authors>
<date>2006</date>
<booktitle>The MIT-LL/AFRL IWSLT-2006 MT system. In The 3rd International Workshop on Spoken Language Translation (IWSLT ’06),</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="10945" citStr="Shen et al. (2006)" startWordPosition="1696" endWordPosition="1699">ode using the phrase-based moses decoder (Koehn et al., 2007), choosing the best translation for each source sentence according to a linear combination of decoding features: ∑ E� = arg max arhr(E, F) (1) E br We make use of a standard set of decoding features, listed in Table 1. In contrast to our IWSLT 2013 system, all experiments submitted to this year’s WMT evaluation made use of version 2.1 of moses, and incorporated additional decoding features, namely the Operation Sequence Model (Durrani et al., 2011) and Lexicalized Reordering Model (Tillman, 2004; Galley and Manning, 2008). Following Shen et al. (2006), we use the word-level lexical translation probabilities Pw(fj |ei) to obtain a sentence-level averaged lexical translation score (Eq. 2), which is added as an additional feature to each n-best list entry. ∏ Plex(F |E) = jE1...J (2) Shen et al. (2006) use the term “IBM model 1 score” to describe the value calculated in Eq. 2. While the lexical probability distribution 1 ∑ Pw(fj |ei) I + 1 iE1...I 188 from IBM Model 1 (Brown et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) defined by Koehn et al. (2003). We also add a 7-gram class</context>
</contexts>
<marker>Shen, Delaney, Anderson, 2006</marker>
<rawString>Wade Shen, Brian Delaney, and Tim Anderson. 2006. The MIT-LL/AFRL IWSLT-2006 MT system. In The 3rd International Workshop on Spoken Language Translation (IWSLT ’06), Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason R Smith</author>
<author>Herve Saint-Amand</author>
<author>Magdalena Plamada</author>
<author>Philipp Koehn</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Dirt cheap web-scale parallel text from the common crawl.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL ’13),</booktitle>
<pages>1374--1383</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5055" citStr="Smith et al., 2013" startWordPosition="770" endWordPosition="773">ter in the word cейчас appear visually indistinguishable, we find that the former is U+0063 LATIN SMALL LETTER C and the latter is U+0441 CYRILLIC SMALL LETTER Es. We created a spelling normalization program to convert these words to all Cyrillic or all Latin characters, with a preference for all-Cyrillic conversion if possible. Normalization also removes U+0301 COMBINING ACUTE ACCENT (̲́) and converts U+00F2 LATIN SMALL LETTER O WITH GRAVE (ò) and U+00F3 LATIN SMALL LETTER O WITH ACUTE (ó) to the unaccented U+043E CYRILLIC SMALL LETTER O (о). The Russian-English Common Crawl parallel corpus (Smith et al., 2013) is relatively noisy. A number of Russian source sentences are incorrectly encoded using characters in the Latin-1 supplement block; we correct these sentences by shifting these characters ahead by 350hex code points into the correct Cyrillic character range.2 We examine the Common Crawl parallel sentences and mark for removal any nonRussian source sentences and non-English target sentences. Target sentences were marked as non-English if more than half of the charac1http://www.w3.org/International/charlint 2For example: “Rïðàâêà ïî ãîðîäàì Ðîññèè è ìèðà.” becomes “Справка по городам России и м</context>
</contexts>
<marker>Smith, Saint-Amand, Plamada, Koehn, Callison-Burch, Lopez, 2013</marker>
<rawString>Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez. 2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL ’13), pages 1374–1383, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM — an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP ’02),</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado,</location>
<contexts>
<context position="8928" citStr="Stolcke, 2002" startWordPosition="1379" endWordPosition="1380">ntary, Yandex (Bojar et al., 2013), and Wikipedia headlines parallel corpora. The phrase table for our Hindi-English system was trained using a similar in-house training pipeline, making use of the HindEnCorp and Wikipedia headlines parallel corpora. 2.2.2 Language Model Training During the training process we built n-gram language models (LMs) for use in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased language models (Brown et al., 1992) were also trained, for later use in n-best list rescoring, using the SRILM language modelling toolkit (Stolcke, 2002).We trained a 6- gram language model from the LDC English Gigaword Fifth Edition, for use in both the Hindi-English and Russian-English systems. All language models were binarized in order to reduce model disk usage and loading time. For the Russian-to-English task, we concatenated the English portion of the parallel training data for the WMT 2014 shared translation task (Common Crawl, News Commentary, Wiki Headlines and Yandex corpora) in addition to the shared task English monolingual training data (Europarl, News Commentary and News Crawl corpora) into a training set for a large 6-gram lang</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM — an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP ’02), pages 901–904, Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’04), Companion Volume,</booktitle>
<pages>101--104</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="10888" citStr="Tillman, 2004" startWordPosition="1689" endWordPosition="1690">ding, n-best List Rescoring, and Optimization We decode using the phrase-based moses decoder (Koehn et al., 2007), choosing the best translation for each source sentence according to a linear combination of decoding features: ∑ E� = arg max arhr(E, F) (1) E br We make use of a standard set of decoding features, listed in Table 1. In contrast to our IWSLT 2013 system, all experiments submitted to this year’s WMT evaluation made use of version 2.1 of moses, and incorporated additional decoding features, namely the Operation Sequence Model (Durrani et al., 2011) and Lexicalized Reordering Model (Tillman, 2004; Galley and Manning, 2008). Following Shen et al. (2006), we use the word-level lexical translation probabilities Pw(fj |ei) to obtain a sentence-level averaged lexical translation score (Eq. 2), which is added as an additional feature to each n-best list entry. ∏ Plex(F |E) = jE1...J (2) Shen et al. (2006) use the term “IBM model 1 score” to describe the value calculated in Eq. 2. While the lexical probability distribution 1 ∑ Pw(fj |ei) I + 1 iE1...I 188 from IBM Model 1 (Brown et al., 1993) could in fact be used as the P,,,(fj I ez) in Eq. 2, in practice we use a variant of P,,,(fj I ez) d</context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’04), Companion Volume, pages 101–104, Boston, Massachusetts, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>WMT</author>
</authors>
<title>Findings of the 2014 Workshop on Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation (WMT ’14),</booktitle>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="9277" citStr="WMT 2014" startWordPosition="1436" endWordPosition="1437">in decoding and rescoring using the KenLM language modelling toolkit (Heafield et al., 2013). Classbased language models (Brown et al., 1992) were also trained, for later use in n-best list rescoring, using the SRILM language modelling toolkit (Stolcke, 2002).We trained a 6- gram language model from the LDC English Gigaword Fifth Edition, for use in both the Hindi-English and Russian-English systems. All language models were binarized in order to reduce model disk usage and loading time. For the Russian-to-English task, we concatenated the English portion of the parallel training data for the WMT 2014 shared translation task (Common Crawl, News Commentary, Wiki Headlines and Yandex corpora) in addition to the shared task English monolingual training data (Europarl, News Commentary and News Crawl corpora) into a training set for a large 6-gram language model using KenLM. We denote this model as “BigLM”. Individual 6-gram models were also constructed from each respective corpus. For the Hindi-to-English task, individual 6- gram models were constructed from the respective English portions of the HindEnCorp and Wikipedia headlines parallel corpora, and from the monolingual English sections of </context>
<context position="23400" citStr="WMT (2014)" startWordPosition="3734" endWordPosition="3735"> where the postedited machine translation was superior to the reference translation. Using this metric, we show in Table 6 that monolingual posteditors can successfully produce postedited translations that convey all or most of the meaning of the original source sentence in up to 87.8% of sentences; this includes 2.8% which were superior to the reference. Finally, as part of WMT14, the results of our Systems 1 (hi-en), 3 (ru-en), and 5 (postedited ru-en) were ranked by monolingual human judges against the machine translation output of other WMT14 participants. These judgements are reported in WMT (2014). Due to time constraints, the machine translations (from System 4) presented to posteditors were not evaluated by human judges, neither using our 12-point evaluation scale nor as part of the WMT human evaluation rankings. However, to enable such evaluation by future researchers, and to enable replication of our experimental evaluation, the System 4 machine translations, the postedited translations, and the monolingual and bilingual evaluation results are released as supplementary data to accompany this paper. 4 Conclusion In this paper, we present data preparation and language-specific proces</context>
</contexts>
<marker>WMT, 2014</marker>
<rawString>WMT. 2014. Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation (WMT ’14), Baltimore, Maryland, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000210">
<title confidence="0.9935545">
Back to the Blocks World: Learning New Actions through Situated
Human-Robot Dialogue
</title>
<author confidence="0.99932">
Lanbo She1, Shaohua Yang1, Yu Cheng2, Yunyi Jia2, Joyce Y. Chai1, Ning Xi2
</author>
<affiliation confidence="0.9962095">
1Department of Computer Science and Engineering
Michigan State University
</affiliation>
<address confidence="0.956796">
East Lansing, MI 48824, USA
</address>
<email confidence="0.993823">
{shelanbo, jchai, yangshao}@cse.msu.edu
</email>
<affiliation confidence="0.9938985">
2Department of Electrical and Computer Engineering
Michigan State University
</affiliation>
<address confidence="0.964357">
East Lansing, MI 48824, USA
</address>
<email confidence="0.99674">
{chengyu9, jiayunyi, xin}@egr.msu.edu
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837421052632">
This paper describes an approach for a
robotic arm to learn new actions through
dialogue in a simplified blocks world. In
particular, we have developed a three-
tier action knowledge representation that
on one hand, supports the connection be-
tween symbolic representations of lan-
guage and continuous sensorimotor repre-
sentations of the robot; and on the other
hand, supports the application of existing
planning algorithms to address novel situ-
ations. Our empirical studies have shown
that, based on this representation the robot
was able to learn and execute basic actions
in the blocks world. When a human is
engaged in a dialogue to teach the robot
new actions, step-by-step instructions lead
to better learning performance compared
to one-shot instructions.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999511431372549">
When a new generation of robots start to work
side-by-side with their human partners in joint
tasks (Christensen et al., 2010), they will often
encounter new objects or are required to perform
new actions. It is important for the robots to au-
tomatically learn new knowledge about the en-
vironment and the tasks from their human part-
ners. To address this issue, this paper describes
our recent work on action learning through dia-
logue. As a first step, we limit our investigation to
a simple blocks world motivated by Terry Wino-
grad’s early work (Winograd, 1972). By using
an industrial robotic arm (SCHUNK) in this small
world, we are interested in addressing the follow-
ing questions. First, human language has a dis-
crete and symbolic representation, but the robot
arm has a continuous representation for its move-
ments. Where should the connections between the
symbolic representation and the continuous repre-
sentation take place so that human language can
be used to direct the robot’s movements? Second,
when the robot learns new tasks from its human
partner, how to represent the acquired knowledge
effectively so that it can be applied in novel situa-
tions? Third, during human-robot dialogue, when
the robot fails to perform the expected actions due
to the lack of knowledge, how should the human
teach the robot new actions? through step-by-step
instructions or one-shot instructions?
With these questions in mind, we have devel-
oped a three-tier action knowledge representation
for the robotic arm. The lower level connects to
the physical arm and defines the trajectories of
executing three atomic actions supported by the
arm (i.e., open gripper, close gripper, move). The
middle level defines primitive operators such as
Open Grip, Close Grip and MoveTo in the fash-
ion of the traditional AI planner (Fikes and Nils-
son, 1971) and directly links to the lower level.
The upper-level captures the high-level actions ac-
quired by learning from the human. These high-
level actions are represented as the desired goal
states of the environment as a result of these ac-
tions. This three-tier representation allows the
robot to automatically come up with a sequence of
lower-level actions by applying existing planning
algorithms.
Based on this representation, we implemented
a dialogue system for action learning and further
conducted an empirical study with human sub-
jects. In particular, we compared the dialogue
</bodyText>
<page confidence="0.997242">
89
</page>
<note confidence="0.917979">
Proceedings of the SIGDIAL 2014 Conference, pages 89–97,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.884360333333333">
Figure 1: An example setup and dialogue. Objects
are marked with labels only for the illustration pur-
pose.
</figureCaption>
<bodyText confidence="0.999466636363636">
based on the step-by-step instructions (i.e., one
step at a time and wait for the robot’s response
at each step before going to the next step) with
the one-shot instructions (i.e., give the instruction
with all steps at once). Our empirical results have
shown that the three-tier knowledge representation
can capture the learned new action and apply it
to novel situations. Although the step-by-step in-
structions resulted in a lengthier teaching process
compared to the one-shot instructions, they led to
better learning performance for the robot.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999980717391305">
Over forty years ago, Terry Winograd developed
SHRDLU (Winograd, 1972) to demonstrate nat-
ural language understanding using a simulated
block-moving arm. One aspect he did not address,
but mentioned in his thesis (Winograd, 1972) as
an important aspect, was learning new actions
through natural language. Motivated by Wino-
grad’s early work, we start our initial investigation
on action learning in a physical blocks world and
with a physical robotic arm. The blocks world is
the most famous domain used for planning in ar-
tificial intelligence. Thus it allows us to focus on
mechanisms that, on one hand, connect symbolic
representations of language with lower-level con-
tinuous sensorimotor representations of the robot;
and on the other hand, support the use of the plan-
ning algorithms to address novel situations.
Most previous work on following human in-
structions are based on supervised learning (Kol-
lar et al., 2010; Tellex et al., 2011; Chen et al.,
2010) or reinforcement learning (Branavan et al.,
2012; Branavan et al., 2010). These types of learn-
ing may not be adequate in time-critical situations
where only resources available to the robot is its
human partners. Thus it is desirable that humans
can engage in a natural language dialogue to teach
robots new skills. Using natural language dialogue
to learn new skills have been explored previously
by (Allen et al., 2007) where an artificial agent was
developed to acquire skills through natural lan-
guage instructions (i.e., find restaurant). But this
work only grounds language to symbolic interface
widgets on web pages.
In the robotics community, previous work has
applied learning by demonstration to teach robots
new skills (Cakmak et al., 2010). To potentially
allow natural language instructions, previous work
has also explored connecting language with lower-
level control systems (Kress-Gazit et al., 2008;
Siskind, 1999; Matuszek et al., 2012). Different
from these previous works, here we investigate the
use of natural language dialogue for learning ac-
tions. Previous work described in (Cantrell et al.,
2012; Mohan et al., 2013) is most similar to our
work. Here we focus on both grounded learning
and the use of planning for action learning.
</bodyText>
<sectionHeader confidence="0.968797" genericHeader="method">
3 Dialogue System
</sectionHeader>
<figureCaption confidence="0.995425">
Figure 2: System Architecture
</figureCaption>
<bodyText confidence="0.999951375">
We developed a dialogue system to support
learning new actions. An example setup is shown
in Figure 1, in which a SCHUNK arm is used to
manipulate blocks placed on a surface. In Hi,
the human starts to ask the robot to stack the blue
block (i.e., Bi) on top of the red block (i.e., Rl).
The robot does not understand the action “stack”,
so it asks the human for instructions. Then the hu-
</bodyText>
<page confidence="0.998127">
90
</page>
<figureCaption confidence="0.985472666666667">
Figure 3: Example semantic representation and
action frame for the human utterance “stack the
blue block on the red block on your right.”
</figureCaption>
<bodyText confidence="0.999940694444445">
man provides detailed steps to accomplish this ac-
tion (i.e., H2 to H8) and also observes the robot’s
response in each step. Note that during this pro-
cess, another unknown action (i.e., “grab” as in
H2) is encountered. The robot thus needs to learn
this action first. The robot is able to keep track
of the dialogue structure so that actions and sub-
actions can be learned accordingly. Once the robot
receives a confirmation from the human that the
corresponding action is successfully performed
(i.e., H6 and H9), it acquires the new action and
explicitly represents it in its knowledge base for
future use. Instead of representing the acquired
knowledge as specific steps as illustrated by the
human, the acquired action is represented by the
expected final state, which represents the changes
of environment as a result of the action. The new
action can be directly applied to novel situations
by applying planning algorithms. Figure 2 shows
the system structure. Next we explain main system
modules in detail.
Natural Languge Processing: Natural language
processing modules capture semantic information
from human language inputs. In particular, the
Intention Recognizer is used to recognize
human intent (e.g., Command and Confirmation).
The Semantic Processor, implemented as
Combinatory Categorial Grammar (CCG) 1, is
used to generate semantic representation. Current
semantic information includes the actions (e.g.,
stack) and their roles (e.g., Theme and Destina-
tion). The roles are further represented by objects’
properties (Color, Location and Spatial Relation).
An example semantic representation of “H1: Stack
the blue block on the red block on your right.” is
shown in Figure 3.
</bodyText>
<footnote confidence="0.9530495">
1We utilized OpenCCG, which could be found at:
http://openccg.sourceforge.net/
</footnote>
<bodyText confidence="0.99441842">
Perception Modules: Besides interpreting human
language, the robot also continuously perceives
the shared environment with its camera. Ob-
jects in video frames are recognized through vi-
sion system (Collet et al., 2011), and further repre-
sented as a Vision Graph (computed by Vision
Graph Builder), which captures objects and
their properties (in the numerical form). The robot
can also access to its own internal status, such as
the location of the gripper and whether it’s open
or closed. Combining the robot’s state and en-
vironment information, the Discrete State
Builder can represent the entire environment as
a conjunction of predicates, which will be later
used for action planning.
Referential Grounding: To make the semantic
representation meaningful, it must be grounded to
the robot’s representation of perception. We use
the graph-based approach for referential ground-
ing as described in (Liu et al., 2012)(Liu et al.,
2013). Once the references are grounded, the se-
mantic representation becomes a Grounded Action
Frame. For example, as shown in Figure 3, “the
blue block” refers to B1 and “the red block on your
right” refers to R1.
Dialogue Manager: The Dialogue Manager
is used to decide what dialog acts the system
should perform give a situation. It is composed by:
a representation of dialogue state, a space of sys-
tem activity and a dialogue policy. The dialogue
status is computed based on the human intention a
dialogue state captures (from semantic representa-
tion) and the Grounded Action Frame. The
current space of system activities includes asking
for instructions, confirming, executing actions and
updating its action knowledge base with new ac-
tions. The dialogue policy stores the (dialogue
state, system activities) pairs. During interaction,
the Dialogue Manager will first identify the
current dialogue state and then apply the dialogue
acts associated with that state as specified in the
dialogue policy.
Action Modules: The Action Modules are
used to realize a high-level action from the
Grounded Action Frame with the physi-
cal arm and to learn new actions. For re-
alizing high-level actions, if the action in the
Grounded Action Frame has a record in
the Action Knowledge, which keeps track
of all the knowledge about various actions, the
</bodyText>
<page confidence="0.993777">
91
</page>
<bodyText confidence="0.987445473684211">
Discrete Planner will do planning to find a
sequence of primitive actions to achieve the high-
level action. Then these primitive actions will se-
quentially go through Continuous Planner
and be translated to the trajectories of arm motors.
By following these trajectories, the arm can per-
form the high-level action. For learning new ac-
tions, these modules will calculate state changes
before and after applying the action on the focus
object. Such changes of the state are generalized
and stored as knowledge representation of the new
action.
Response Generator: Currently, the Response
Generator is responsible for language genera-
tion to realize the detail sentence. In our current
investigation, the speech feedback is simple, so we
just used a set of pre-defined templates to do lan-
guage generation. And the parameters in the tem-
plates will be realized during run time.
</bodyText>
<sectionHeader confidence="0.973408" genericHeader="method">
4 Action Learning through Dialogue
</sectionHeader>
<bodyText confidence="0.9999776">
To realize the action learning functionality we
have developed a set of action related processes
including an action knowledge base, action execu-
tion processes and action learning processes. Next
we give detailed explanations.
</bodyText>
<subsectionHeader confidence="0.986827">
4.1 Action Modules
</subsectionHeader>
<figureCaption confidence="0.6964095">
Figure 4: Execution example for “Pick up the blue
block”.
</figureCaption>
<bodyText confidence="0.99744625">
As shown in Figure 4, the action knowledge
base is a three-level structure, which consists of
High-level action Knowledge, Discrete Planner
and Continuous Planner.
</bodyText>
<subsectionHeader confidence="0.853221">
4.1.1 Continuous Planner
</subsectionHeader>
<bodyText confidence="0.999988">
This lowest level planner defines three primitive
actions: open (i.e., open gripper), close (i.e., close
gripper) and move (i.e., move to the destination).
Each primitive action is defined as a trajectory
computing function, implemented as inverse kine-
matics. The outputs of these functions are control
commands sendt to each arm motor to keep the
arm following the trajectory.
</bodyText>
<subsectionHeader confidence="0.786532">
4.1.2 Discrete Planner
</subsectionHeader>
<bodyText confidence="0.9996654">
The Discrete Planner is used to decompose a
high-level action into a sequence of primitive ac-
tions. In our system, it is implemented as a
STRIPS (Fikes and Nilsson, 1971) planner, which
is defined as a quadruple (P, O, I, G):
</bodyText>
<listItem confidence="0.998230625">
• P: Set of predicates describing a domain.
• O: Set of operators. Each is specified by a set
of preconditions and effects. An operator is
applicable only when its preconditions could
be entailed in a state.
• I: Initial state, the starting point of a problem.
• G: Goal state, which should be achieved if the
problem is solved.
</listItem>
<bodyText confidence="0.9528535">
In our system, O set includes Open Gripper,
Close Gripper and 8 different kinds of
MoveTo (She et al., 2014). And the P set
consists of two dimensions of the environment:
</bodyText>
<listItem confidence="0.984992777777778">
• Arm States: G Open/Close (i.e., whether the
gripper is open or closed), G Full/Empty
(i.e., whether the gripper has an object in it)
and G At(x) (i.e, location of the arm).
• Object States: Top Uclr/Clr(o) (i.e., whether
the block o has another block on its top),
In/Out G(o) (i.e., whether o is within the
gripper fingers or not) and On(o,x) (i.e., o is
supported by x).
</listItem>
<bodyText confidence="0.995962">
The I and G are captured real-time during the
dialogue interaction.
</bodyText>
<subsectionHeader confidence="0.780417">
4.1.3 High-level action Knowledge
</subsectionHeader>
<bodyText confidence="0.999842625">
The high-level actions represent actions specified
by the human partner. They are modeled as de-
sired goal states rather than the action sequence
taught by human. For example, the “Stack(x,y)”
could be represented as “On(x,y)∧G Open”. If the
human specifies a high-level action out of the ac-
tion knowledge base, the dialogue manager will
verbally request for instructions to learn the action.
</bodyText>
<page confidence="0.973725">
92
</page>
<figureCaption confidence="0.8387435">
Figure 5: Learning process illustration. After hearing the stack action, the robot cannot perform. So the
human gives step by step instruction. When the instruction is completed, new knowledge of Grab(x) and
Stack(x,y) are learned in the high-level action knowledge base as the combination of the goal state of the
robotic arm and the changes of the state for the involved objects.
</figureCaption>
<subsectionHeader confidence="0.98843">
4.2 Action Execution
</subsectionHeader>
<bodyText confidence="0.9999675">
Given a Grounded Action Frame, it is
firstly checked with the high-level action knowl-
edge base. If the knowledge base has its record
(e.g., the Pickup and ClearTop in Figure 4.), a goal
state describing the action effect will be retrieved.
This goal state, together with the initial state cap-
tured from the current environment, will be sent
to the Discrete Planner. And, through au-
tomated planning, a sequence of primitive actions
will be generated to complete the task, which can
be immediately executed by the arm.
Take the “Pick up” action frame in Figure 4
as an example. By checking the grounded ac-
tion frame with the high-level action knowledge,
a related goal state (i.e., “G Close∧Top Clr(B1)
∧In G(B1)∧On(B1,air)”) can be retrieved. At
the same time, the Discrete Evn Builder
translates the real world environment as a con-
junction of predicates, which serves as the ini-
tial state. Given the combination of initial state
and goal state, the STRIPS planner can search for
a path of primitive actions to solve the problem.
For example, the PickUp(B1) in Figure 4 can be
solved by Open Grip, MoveTo(B1), Close Grip
and MoveTo(air).
The primitive actions are executed by the con-
tinuous planner and control process in the lower
robotic system. For the “open” and “close”, they
are executed by controlling the position of the
gripper fingers. For the “move”, a task-space tra-
jectory is first planned based on the minimum-time
motion planning algorithm to move the robot end-
effector from the current position to the final posi-
tion. A kinematic controller with redundancy res-
olution (Zhang et al., 2012) is then used to gener-
ate the joint movements for the robot to track the
planned trajectory. Achieving the end of the tra-
jectory indicates the action completion.
</bodyText>
<subsectionHeader confidence="0.990915">
4.3 Action Learning
</subsectionHeader>
<figureCaption confidence="0.953623333333333">
Figure 5 illustrates the system internal process of
acquiring action knowledge from the dialogue in
Figure 1.
</figureCaption>
<bodyText confidence="0.998809043478261">
At the beginning of the dialogue, the grounded
action frame Stack(B1, R1) captured from the first
human utterance is not in the action knowledge,
so it will be pushed to the top of the unknown ac-
tion stack as a new action waiting to be learned.
The environment state at this point is calculated as
shown in the figure. Then the robot will verbally
request instructions. During the instruction, it’s
possible that another unknown action Grab(B1) is
referred. The same as the Stack action, it will be
pushed to the top of unknown action stack waiting
to be learned.
In the next instruction, the human says “Open
your gripper”. This sentence can be translated as
action frame Open and the goal state “G Open”
can be retrieved from the action knowledge base.
After executing the action sequence, the grip-
per state will be changed from “G Close” to
“G Open”, as shown in Figure 5. In the follow-
ing two instructions, the human says “Move to the
blue block” and “Close gripper”. Similarly, these
two instructions are translated as action frames
Move(B1) and Close, then are executed accord-
</bodyText>
<page confidence="0.997396">
93
</page>
<bodyText confidence="0.979291684210526">
ingly. After executing these two steps, the state of
B1 is changed from “Out G(B1)” to “In G(B1)”.
At this point, the previous unknown action
Grab(B1) is achieved, so the human says “Now
you achieve the grab action” as a signal of teach-
ing completion. After acknowledging the teach-
ing completion, the action learning module will
learn the new action representation by combining
the arm state with the state changes of the argu-
ment objects in the unknown action frame. For
example, the argument object of unknown action
Grab(B1) is B1. By comparing the original state
of B1, [(Out G B1)∧(Top Clr B1)∧(On B1 table)]
with the final state, [(In G B1)∧(Top Clr B1)∧(On
B1 table)], B1 is changed from (Out G B1) to
(In G B1). So, the learning module will gener-
alize such state changes and acquire the knowl-
edge representation of the new action Grab(x) as
G Close∧In G(x).
</bodyText>
<sectionHeader confidence="0.982143" genericHeader="method">
5 Empirical Studies
</sectionHeader>
<bodyText confidence="0.999946">
The objectives of our empirical studies are two
folds. First, we aim to exam whether the current
representation can support planning algorithms
and execute the learned actions in novel situations.
Second, we aim to evaluate how extra effort from
the human partner through step-by-step instruc-
tions may affect the robot’s learning performance.
</bodyText>
<subsectionHeader confidence="0.965898">
5.1 Instruction Effort
</subsectionHeader>
<bodyText confidence="0.999790666666667">
Previous work on mediating perceptual differ-
ences between humans and robots have shown that
a high collaborative effort from the robot leads to
better referential grounding (Chai et al., 2014).
Motivated by this previous work, we are inter-
ested in examining how different levels of effort
from human partners may affect the robot’s learn-
ing performance. More specifically, we model two
levels of variations:
</bodyText>
<listItem confidence="0.91632">
• Collaborative Interaction: In this setting, a
human partner provides step-by-step instruc-
</listItem>
<bodyText confidence="0.94411125">
tions. At each step, the human will observe
the the robot’s response (i.e., arm movement)
before moving to the next step. For exam-
ple, to teach “stack”, the human would is-
sue “pick up the blue block”, observe the
robot’s movement, then issue “put it on the
red block” and observe the robot movement.
By this fashion, the human makes extra effort
to make sure the robot follows every step cor-
rectly before moving on. The human partner
can detect potential problems and respond to
immediate feedback from the robot.
</bodyText>
<listItem confidence="0.800368">
• Non-Collaborative Interaction: In this set-
ting, the human only provides a one-shot in-
</listItem>
<bodyText confidence="0.709516285714286">
struction. For example, to teach “stack”,
the human first issues a complete instruction
“pick up the blue block and put it on top of
the red block” and then observes the robot’s
responses. Compared to the collaborative set-
ting, the non-collaborative setting is poten-
tially more efficient.
</bodyText>
<subsectionHeader confidence="0.995618">
5.2 Experimental Tasks
</subsectionHeader>
<bodyText confidence="0.9999899375">
Similar to the setup shown in Figure 1, in the
study, we have multiple blocks with different col-
ors and sizes placed on a flat surface, with a
SCHUNK arm positioned on one side of the sur-
face and the human subject seated on the opposite
side. The video stream of the environment is sent
to the vision system (Collet et al., 2011). With the
pre-trained object model of each block, the vision
system could capture blocks’ 3D positions from
each frame. Five human subjects participated in
our experiments 2. During the study, each sub-
ject was informed about the basic actions the robot
can perform (i.e., open gripper, close gripper, and
move to) and was instructed to teach the robot sev-
eral new actions through dialogue. Each subject
would go through the following two phases:
</bodyText>
<subsectionHeader confidence="0.746047">
5.2.1 Teaching/Learning Phase
</subsectionHeader>
<bodyText confidence="0.999637315789474">
Each subject was asked to teach the following five
new actions under the two strategies (i.e., step-
by-step instructions vs. one-shot instructions):
{Pickup, Grab, Drop, ClearTop, Stack} Each time,
the subject can choose any blocks they think are
useful to teach the action. After finishing teaching
one action (either under step-by-step instructions
or under one-shot instructions), we would survey
the subject whether he/she thinks the teaching is
completed and the corresponding action is suc-
cessfully performed by the robot. We record the
teaching duration and then re-arrange the table top
setting to move to the next action.
For the teaching/learning phase, we use two
metrics for evaluation: 1) Teaching Completion
Rate(Rt) which stands for the number of actions
successfully taught and performed by the robot;
2)Teaching Completion Duration (Dt which mea-
sures the amount of time taken to teach an action.
</bodyText>
<footnote confidence="0.9982085">
2More human subjects will be recruited to participate in
our studies.
</footnote>
<page confidence="0.998274">
94
</page>
<subsectionHeader confidence="0.655564">
5.2.2 Execution Phase
</subsectionHeader>
<bodyText confidence="0.991660785714286">
The goal of learning is to be able to apply the
learned knowledge in novel situations. To evalu-
ate such capability, for each action, we designed
10 additional setups of the environment which
are different from the environment where the ac-
tion was learned. For example, as illustrated in
Figure 6, the human teaches the pick Up action
by instructing the robot how to perform “pick up
the blue block(i.e., B1)” under the environment
in 6(a). Once the knowledge is acquired about the
action “pick up”, we will test the acquired knowl-
edge in a novel situation by instructing the robot to
execute “pick up the green block(i.e., G1)” in the
environment shown in 6(b).
</bodyText>
<figureCaption confidence="0.576239">
Figure 7: The teaching completion result of the
</figureCaption>
<bodyText confidence="0.856116875">
50 teaching dialogues. “1” stands for the dialogue
where the subject considers the teaching/learning
as complete since the robot performs the corre-
sponding action correctly; and “0” indicates a fail-
ure in learning. The total numbers of teaching
completion are listed in the bottom row.
(a) Learning: the human
teaches the robot how to
“pick up the blue block
(i.e., B1)” during the learn-
ing phase
(b) Execution: the human
asks the robot to “pick up
the green block (i.e., G1)”
after the robot acquires the
knowledge about “pick up”
</bodyText>
<figureCaption confidence="0.974912">
Figure 6: Examples of a learning and an execution
setup.
</figureCaption>
<bodyText confidence="0.999700777777778">
For the execution phase, we also used
two factors to evaluate: 1) Action Sequence
Generation(Rg) which measures how many high-
level actions among the 10 execution scenarios
where the corresponding lower-level action se-
quences are correctly generated; 2) Action Se-
quence Execution(Rge) which measures the num-
ber of high level actions that are correctly executed
based on the lower level action sequences.
</bodyText>
<subsectionHeader confidence="0.999415">
5.3 Empirical Results
</subsectionHeader>
<bodyText confidence="0.9648824375">
Our experiments resulted in a total of 50 action
teaching dialogues. Half of these are under the
step-by-step instructions (i.e., collaborative inter-
action) and half are under one-shot instructions
(i.e., non-collaborative). As shown in Figure 7,
5 out of the 50 teaching dialogues were consid-
ered as incomplete by the human subjects and all
of them are from the Non-Collaborative setting.
For each of the 45 successful dialogues, an action
would be learned and acquired. For each of these
acquired actions, we further tested its execution
under 10 different setups.
Figure 8: The teaching completion duration re-
sults. The durations under the non-collaborative
strategy are smaller than the collaborative strategy
in most cases.
</bodyText>
<subsectionHeader confidence="0.946889">
5.3.1 Teaching Performance
</subsectionHeader>
<bodyText confidence="0.9999811">
The result of teaching completion is shown in Fig-
ure 7. Each subject contributes two columns: the
“Non” stands for the Non-Collaborative strategy
and the “Col” column refers to the Collaborative
strategy. As the table shows, all the 5 uncom-
pleted teaching are from the Non-Collaborative
strategy. In most of these 5 cases, the subjects
thought the actual performed actions were differ-
ent from their expectations. For example, in one of
the “stack” failures, the human one-shot instruc-
tion was “move the blue block to the red block on
the left.”. She thought the arm would put the blue
block on the top of red block, open gripper and
then move away. However, based on the robot’s
knowledge, it just moved the blue block above
the red block and stopped there. So the subject
considered this teaching as incomplete. On the
other hand, in the Collaborative interactions, the
robot’s actual actions could also be different from
the subject’s expectation. But, as the instruction
</bodyText>
<page confidence="0.99708">
95
</page>
<figureCaption confidence="0.8610595">
Figure 9: Each bar represents the number of suc-
cessfully generated action sequences during test-
</figureCaption>
<bodyText confidence="0.990329">
ing. The solid portion of each bar represents the
number of successfully executed action sequences.
The number of successfully execution is always
smaller than or equal to the generation. This is be-
cause we are dealing with dynamic environment,
and the inaccurate real-time localization will make
some correct action sequence fail to be executed.
was given step-by-step, the instructors could no-
tice the difference from the immediate feedback
and adjust their follow-up steps, which contributed
to a higher completion rate.
The duration of each teaching task is shown in
Figure 8. Bar heights represent average teaching
duration, the ranges stand for standard error of
the mean (SEM). The 5 actions are represented
by different groups. As shown in the figure, the
teaching duration under the Collaborative strategy
tends to take more time. Because in the Collab-
orative case, the human needs to plan next step
after observing the robot’s response to a previous
step. If an exception happens, a sub-dialogue is
often arranged to do correction. But in the Non-
Collaborative case, the human comes up with an
entire instruction at the beginning, which appears
more efficient.
</bodyText>
<subsectionHeader confidence="0.688749">
5.3.2 Execution Performance
</subsectionHeader>
<bodyText confidence="0.995732578947369">
Figure 9 illustrates the action sequence generation
and execution results in the execution phase.
As shown in Figure 9, testing results of actions
learned under the Collaborative strategy are higher
than the ones using Non-Collaborative, this is be-
cause teaching under the Collaborative strategy is
more likely to be successful. One exception is the
Clear Top action, which has lower generation rate
under the Col setting. By examining the collected
data, we noticed that our system failed to learn the
knowledge of Clear Top in one of the 5 teaching
phases using Col setting, although the human sub-
ject labeled it as successful. Another phenomenon
shown in Figure 9 is that the generation results are
always larger than or equal with the correspond-
ing execution results. This is caused by inaccurate
localization and camera calibration, which intro-
duced exceptions during executing the action se-
quence.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99997565">
This paper describes an approach to robot action
learning in a simplified blocks world. The sim-
plifications of the environment and the tasks allow
us to explore connections between symbolic repre-
sentations of natural language and continuous sen-
sorimotor representations of the robot which can
support automated planning for novel situations.
This investigation is only our first step. Many is-
sues have not been addressed. For example, the
world is full of uncertainties. Our current ap-
proach can only either succeed or fail executing
an action based on the acquired knowledge. There
is no approximation or reasoning of the uncertain
states which may affect potential execution. Also,
when the robot fails to execute an action, there is
no explanation why it fails. If the robot can artic-
ulate its internal representations regarding where
the problem occurs, the human can provide better
help or targeted teaching. These are the directions
we will pursue in our future work.
</bodyText>
<sectionHeader confidence="0.997583" genericHeader="conclusions">
7 Acknowledgment
</sectionHeader>
<bodyText confidence="0.982667666666667">
This work was supported by IIS-1208390 from the
National Science Foundation and N00014-11-1-
0410 from the Office of Naval Research.
</bodyText>
<sectionHeader confidence="0.998598" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999636428571428">
James F. Allen, Nathanael Chambers, George Fergu-
son, Lucian Galescu, Hyuckchul Jung, Mary D.
Swift, and William Taysom. 2007. Plow: A collab-
orative task learning agent. In AAAI, pages 1514–
1519. AAAI Press.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: Learn-
ing to map high-level instructions to commands. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ’10,
pages 1268–1277, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
S.R.K. Branavan, Nate Kushman, Tao Lei, and Regina
Barzilay. 2012. Learning high-level planning from
</reference>
<page confidence="0.971918">
96
</page>
<reference confidence="0.999702284090909">
text. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 126–135, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Maya Cakmak, Crystal Chao, and Andrea Lockerd
Thomaz. 2010. Designing interactions for robot ac-
tive learners. IEEE T. Autonomous Mental Develop-
ment, 2(2):108–118.
R. Cantrell, K. Talamadupula, P. Schermerhorn, J. Ben-
ton, S. Kambhampati, and M. Scheutz. 2012. Tell
me when and why to do it! run-time planner model
updates via natural language instruction. In Human-
Robot Interaction (HRI), 2012 7th ACM/IEEE Inter-
national Conference on, pages 471–478, March.
Joyce Y. Chai, Lanbo She, Rui Fang, Spencer Ottarson,
Cody Littley, Changsong Liu, and Kenneth Han-
son. 2014. Collaborative effort towards common
ground in situated human robot dialogue. In Pro-
ceedings of 9th ACM/IEEE International Confer-
ence on Human-Robot Interaction, Bielefeld, Ger-
many.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. J. Artif.
Int. Res., 37(1):397–436, January.
H. I. Christensen, G. M. Kruijff, and J. Wyatt, editors.
2010. Cognitive Systems. Springer.
Alvaro Collet, Manuel Martinez, and Siddhartha S.
Srinivasa. 2011. The MOPED framework: Object
Recognition and Pose Estimation for Manipulation.
Richard E. Fikes and Nils J. Nilsson. 1971. Strips: A
new approach to the application of theorem proving
to problem solving. In Proceedings of the 2Nd Inter-
national Joint Conference on Artificial Intelligence,
IJCAI’71, pages 608–620, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Toward understanding natural language
directions. In Proceedings of the 5th ACM/IEEE
International Conference on Human-robot Interac-
tion, HRI ’10, pages 259–266, Piscataway, NJ, USA.
IEEE Press.
Hadas Kress-Gazit, Georgios E. Fainekos, and
George J. Pappas. 2008. Translating structured
english to robot controllers. Advanced Robotics,
22(12):1343–1359.
Changsong Liu, Rui Fang, and Joyce Chai. 2012. To-
wards mediating shared perceptual basis in situated
dialogue. In Proceedings of the 13th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 140–149, Seoul, South Korea.
Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai.
2013. Modeling collaborative referring for situated
referential grounding. In Proceedings of the SIG-
DIAL 2013 Conference, pages 78–86, Metz, France.
Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer,
and Dieter Fox. 2012. Learning to parse nat-
ural language commands to a robot control sys-
tem. In Jaydev P. Desai, Gregory Dudek, Ous-
sama Khatib, and Vijay Kumar, editors, ISER, vol-
ume 88 of Springer Tracts in Advanced Robotics,
pages 403–415. Springer.
Shiwali Mohan, James Kirk, and John Laird. 2013. A
computational model for situated task learning with
interactive instruction. In Proceedings of ICCM
2013 - 12th International Conference on Cognitive
Modeling.
Lanbo She, Yu Cheng, Joyce Chai, Yunyi Jia, Shaohua
Yang, and Ning Xi. 2014. Teaching robots new ac-
tions through natural language instructions. In RO-
MAN.
Jeffrey Mark Siskind. 1999. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. J. Artif. Int. Res., 15(1):31–
90, February.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth J.
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Wolfram Burgard and Dan
Roth, editors, AAAI. AAAI Press.
T. Winograd. 1972. Procedures as a representation for
data in a computer program for understanding natu-
ral language. Cognitive Psychology, 3(1):1–191.
Huatao Zhang, Yunyi Jia, and Ning Xi. 2012. Sensor-
based redundancy resolution for a nonholonomic
mobile manipulator. In IROS, pages 5327–5332.
</reference>
<page confidence="0.999692">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.535539">
<title confidence="0.9813455">Back to the Blocks World: Learning New Actions through Human-Robot Dialogue</title>
<author confidence="0.999843">Shaohua Yu Yunyi Joyce Y Ning</author>
<affiliation confidence="0.9747185">of Computer Science and Michigan State</affiliation>
<address confidence="0.999196">East Lansing, MI 48824, USA</address>
<email confidence="0.965725">jchai,</email>
<affiliation confidence="0.6756655">of Electrical and Computer Michigan State</affiliation>
<address confidence="0.997932">East Lansing, MI 48824, USA</address>
<email confidence="0.993654">jiayunyi,</email>
<abstract confidence="0.9989782">This paper describes an approach for a robotic arm to learn new actions through dialogue in a simplified blocks world. In particular, we have developed a threetier action knowledge representation that on one hand, supports the connection between symbolic representations of language and continuous sensorimotor representations of the robot; and on the other hand, supports the application of existing planning algorithms to address novel situations. Our empirical studies have shown that, based on this representation the robot was able to learn and execute basic actions in the blocks world. When a human is engaged in a dialogue to teach the robot new actions, step-by-step instructions lead to better learning performance compared to one-shot instructions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Nathanael Chambers</author>
<author>George Ferguson</author>
<author>Lucian Galescu</author>
<author>Hyuckchul Jung</author>
<author>Mary D Swift</author>
<author>William Taysom</author>
</authors>
<title>Plow: A collaborative task learning agent.</title>
<date>2007</date>
<booktitle>In AAAI,</booktitle>
<pages>1514--1519</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="5839" citStr="Allen et al., 2007" startWordPosition="916" endWordPosition="919">he planning algorithms to address novel situations. Most previous work on following human instructions are based on supervised learning (Kollar et al., 2010; Tellex et al., 2011; Chen et al., 2010) or reinforcement learning (Branavan et al., 2012; Branavan et al., 2010). These types of learning may not be adequate in time-critical situations where only resources available to the robot is its human partners. Thus it is desirable that humans can engage in a natural language dialogue to teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has applied learning by demonstration to teach robots new skills (Cakmak et al., 2010). To potentially allow natural language instructions, previous work has also explored connecting language with lowerlevel control systems (Kress-Gazit et al., 2008; Siskind, 1999; Matuszek et al., 2012). Different from these previous works, here we investigate the use of</context>
</contexts>
<marker>Allen, Chambers, Ferguson, Galescu, Jung, Swift, Taysom, 2007</marker>
<rawString>James F. Allen, Nathanael Chambers, George Ferguson, Lucian Galescu, Hyuckchul Jung, Mary D. Swift, and William Taysom. 2007. Plow: A collaborative task learning agent. In AAAI, pages 1514– 1519. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1268--1277</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5490" citStr="Branavan et al., 2010" startWordPosition="858" endWordPosition="861">ocks world and with a physical robotic arm. The blocks world is the most famous domain used for planning in artificial intelligence. Thus it allows us to focus on mechanisms that, on one hand, connect symbolic representations of language with lower-level continuous sensorimotor representations of the robot; and on the other hand, support the use of the planning algorithms to address novel situations. Most previous work on following human instructions are based on supervised learning (Kollar et al., 2010; Tellex et al., 2011; Chen et al., 2010) or reinforcement learning (Branavan et al., 2012; Branavan et al., 2010). These types of learning may not be adequate in time-critical situations where only resources available to the robot is its human partners. Thus it is desirable that humans can engage in a natural language dialogue to teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has appl</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S. R. K. Branavan, Luke S. Zettlemoyer, and Regina Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1268–1277, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Nate Kushman</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning high-level planning from text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>126--135</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="5466" citStr="Branavan et al., 2012" startWordPosition="854" endWordPosition="857">arning in a physical blocks world and with a physical robotic arm. The blocks world is the most famous domain used for planning in artificial intelligence. Thus it allows us to focus on mechanisms that, on one hand, connect symbolic representations of language with lower-level continuous sensorimotor representations of the robot; and on the other hand, support the use of the planning algorithms to address novel situations. Most previous work on following human instructions are based on supervised learning (Kollar et al., 2010; Tellex et al., 2011; Chen et al., 2010) or reinforcement learning (Branavan et al., 2012; Branavan et al., 2010). These types of learning may not be adequate in time-critical situations where only resources available to the robot is its human partners. Thus it is desirable that humans can engage in a natural language dialogue to teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community</context>
</contexts>
<marker>Branavan, Kushman, Lei, Barzilay, 2012</marker>
<rawString>S.R.K. Branavan, Nate Kushman, Tao Lei, and Regina Barzilay. 2012. Learning high-level planning from text. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 126–135, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maya Cakmak</author>
<author>Crystal Chao</author>
<author>Andrea Lockerd Thomaz</author>
</authors>
<title>Designing interactions for robot active learners.</title>
<date>2010</date>
<journal>IEEE T. Autonomous Mental Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="6168" citStr="Cakmak et al., 2010" startWordPosition="966" endWordPosition="969">ical situations where only resources available to the robot is its human partners. Thus it is desirable that humans can engage in a natural language dialogue to teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has applied learning by demonstration to teach robots new skills (Cakmak et al., 2010). To potentially allow natural language instructions, previous work has also explored connecting language with lowerlevel control systems (Kress-Gazit et al., 2008; Siskind, 1999; Matuszek et al., 2012). Different from these previous works, here we investigate the use of natural language dialogue for learning actions. Previous work described in (Cantrell et al., 2012; Mohan et al., 2013) is most similar to our work. Here we focus on both grounded learning and the use of planning for action learning. 3 Dialogue System Figure 2: System Architecture We developed a dialogue system to support learn</context>
</contexts>
<marker>Cakmak, Chao, Thomaz, 2010</marker>
<rawString>Maya Cakmak, Crystal Chao, and Andrea Lockerd Thomaz. 2010. Designing interactions for robot active learners. IEEE T. Autonomous Mental Development, 2(2):108–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cantrell</author>
<author>K Talamadupula</author>
<author>P Schermerhorn</author>
<author>J Benton</author>
<author>S Kambhampati</author>
<author>M Scheutz</author>
</authors>
<title>Tell me when and why to do it! run-time planner model updates via natural language instruction.</title>
<date>2012</date>
<booktitle>In HumanRobot Interaction (HRI), 2012 7th ACM/IEEE International Conference on,</booktitle>
<pages>471--478</pages>
<contexts>
<context position="6537" citStr="Cantrell et al., 2012" startWordPosition="1020" endWordPosition="1023">nguage instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has applied learning by demonstration to teach robots new skills (Cakmak et al., 2010). To potentially allow natural language instructions, previous work has also explored connecting language with lowerlevel control systems (Kress-Gazit et al., 2008; Siskind, 1999; Matuszek et al., 2012). Different from these previous works, here we investigate the use of natural language dialogue for learning actions. Previous work described in (Cantrell et al., 2012; Mohan et al., 2013) is most similar to our work. Here we focus on both grounded learning and the use of planning for action learning. 3 Dialogue System Figure 2: System Architecture We developed a dialogue system to support learning new actions. An example setup is shown in Figure 1, in which a SCHUNK arm is used to manipulate blocks placed on a surface. In Hi, the human starts to ask the robot to stack the blue block (i.e., Bi) on top of the red block (i.e., Rl). The robot does not understand the action “stack”, so it asks the human for instructions. Then the hu90 Figure 3: Example semantic</context>
</contexts>
<marker>Cantrell, Talamadupula, Schermerhorn, Benton, Kambhampati, Scheutz, 2012</marker>
<rawString>R. Cantrell, K. Talamadupula, P. Schermerhorn, J. Benton, S. Kambhampati, and M. Scheutz. 2012. Tell me when and why to do it! run-time planner model updates via natural language instruction. In HumanRobot Interaction (HRI), 2012 7th ACM/IEEE International Conference on, pages 471–478, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joyce Y Chai</author>
<author>Lanbo She</author>
<author>Rui Fang</author>
<author>Spencer Ottarson</author>
<author>Cody Littley</author>
<author>Changsong Liu</author>
<author>Kenneth Hanson</author>
</authors>
<title>Collaborative effort towards common ground in situated human robot dialogue.</title>
<date>2014</date>
<booktitle>In Proceedings of 9th ACM/IEEE International Conference on Human-Robot Interaction,</booktitle>
<location>Bielefeld, Germany.</location>
<contexts>
<context position="19516" citStr="Chai et al., 2014" startWordPosition="3154" endWordPosition="3157"> new action Grab(x) as G Close∧In G(x). 5 Empirical Studies The objectives of our empirical studies are two folds. First, we aim to exam whether the current representation can support planning algorithms and execute the learned actions in novel situations. Second, we aim to evaluate how extra effort from the human partner through step-by-step instructions may affect the robot’s learning performance. 5.1 Instruction Effort Previous work on mediating perceptual differences between humans and robots have shown that a high collaborative effort from the robot leads to better referential grounding (Chai et al., 2014). Motivated by this previous work, we are interested in examining how different levels of effort from human partners may affect the robot’s learning performance. More specifically, we model two levels of variations: • Collaborative Interaction: In this setting, a human partner provides step-by-step instructions. At each step, the human will observe the the robot’s response (i.e., arm movement) before moving to the next step. For example, to teach “stack”, the human would issue “pick up the blue block”, observe the robot’s movement, then issue “put it on the red block” and observe the robot mov</context>
</contexts>
<marker>Chai, She, Fang, Ottarson, Littley, Liu, Hanson, 2014</marker>
<rawString>Joyce Y. Chai, Lanbo She, Rui Fang, Spencer Ottarson, Cody Littley, Changsong Liu, and Kenneth Hanson. 2014. Collaborative effort towards common ground in situated human robot dialogue. In Proceedings of 9th ACM/IEEE International Conference on Human-Robot Interaction, Bielefeld, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Joohyun Kim</author>
<author>Raymond J Mooney</author>
</authors>
<title>Training a multilingual sportscaster: Using perceptual context to learn language.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="5417" citStr="Chen et al., 2010" startWordPosition="847" endWordPosition="850">e start our initial investigation on action learning in a physical blocks world and with a physical robotic arm. The blocks world is the most famous domain used for planning in artificial intelligence. Thus it allows us to focus on mechanisms that, on one hand, connect symbolic representations of language with lower-level continuous sensorimotor representations of the robot; and on the other hand, support the use of the planning algorithms to address novel situations. Most previous work on following human instructions are based on supervised learning (Kollar et al., 2010; Tellex et al., 2011; Chen et al., 2010) or reinforcement learning (Branavan et al., 2012; Branavan et al., 2010). These types of learning may not be adequate in time-critical situations where only resources available to the robot is its human partners. Thus it is desirable that humans can engage in a natural language dialogue to teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interfac</context>
</contexts>
<marker>Chen, Kim, Mooney, 2010</marker>
<rawString>David L. Chen, Joohyun Kim, and Raymond J. Mooney. 2010. Training a multilingual sportscaster: Using perceptual context to learn language. J. Artif. Int. Res., 37(1):397–436, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H I</author>
</authors>
<title>Cognitive Systems.</title>
<date>2010</date>
<editor>Christensen, G. M. Kruijff, and J. Wyatt, editors.</editor>
<publisher>Springer.</publisher>
<marker>I, 2010</marker>
<rawString>H. I. Christensen, G. M. Kruijff, and J. Wyatt, editors. 2010. Cognitive Systems. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro Collet</author>
<author>Manuel Martinez</author>
<author>Siddhartha S Srinivasa</author>
</authors>
<title>The MOPED framework: Object Recognition and Pose Estimation for Manipulation.</title>
<date>2011</date>
<contexts>
<context position="9237" citStr="Collet et al., 2011" startWordPosition="1455" endWordPosition="1458">rent semantic information includes the actions (e.g., stack) and their roles (e.g., Theme and Destination). The roles are further represented by objects’ properties (Color, Location and Spatial Relation). An example semantic representation of “H1: Stack the blue block on the red block on your right.” is shown in Figure 3. 1We utilized OpenCCG, which could be found at: http://openccg.sourceforge.net/ Perception Modules: Besides interpreting human language, the robot also continuously perceives the shared environment with its camera. Objects in video frames are recognized through vision system (Collet et al., 2011), and further represented as a Vision Graph (computed by Vision Graph Builder), which captures objects and their properties (in the numerical form). The robot can also access to its own internal status, such as the location of the gripper and whether it’s open or closed. Combining the robot’s state and environment information, the Discrete State Builder can represent the entire environment as a conjunction of predicates, which will be later used for action planning. Referential Grounding: To make the semantic representation meaningful, it must be grounded to the robot’s representation of perce</context>
<context position="21065" citStr="Collet et al., 2011" startWordPosition="3417" endWordPosition="3420">e, to teach “stack”, the human first issues a complete instruction “pick up the blue block and put it on top of the red block” and then observes the robot’s responses. Compared to the collaborative setting, the non-collaborative setting is potentially more efficient. 5.2 Experimental Tasks Similar to the setup shown in Figure 1, in the study, we have multiple blocks with different colors and sizes placed on a flat surface, with a SCHUNK arm positioned on one side of the surface and the human subject seated on the opposite side. The video stream of the environment is sent to the vision system (Collet et al., 2011). With the pre-trained object model of each block, the vision system could capture blocks’ 3D positions from each frame. Five human subjects participated in our experiments 2. During the study, each subject was informed about the basic actions the robot can perform (i.e., open gripper, close gripper, and move to) and was instructed to teach the robot several new actions through dialogue. Each subject would go through the following two phases: 5.2.1 Teaching/Learning Phase Each subject was asked to teach the following five new actions under the two strategies (i.e., stepby-step instructions vs.</context>
</contexts>
<marker>Collet, Martinez, Srinivasa, 2011</marker>
<rawString>Alvaro Collet, Manuel Martinez, and Siddhartha S. Srinivasa. 2011. The MOPED framework: Object Recognition and Pose Estimation for Manipulation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard E Fikes</author>
<author>Nils J Nilsson</author>
</authors>
<title>Strips: A new approach to the application of theorem proving to problem solving.</title>
<date>1971</date>
<booktitle>In Proceedings of the 2Nd International Joint Conference on Artificial Intelligence, IJCAI’71,</booktitle>
<pages>608--620</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3060" citStr="Fikes and Nilsson, 1971" startWordPosition="476" endWordPosition="480">bot fails to perform the expected actions due to the lack of knowledge, how should the human teach the robot new actions? through step-by-step instructions or one-shot instructions? With these questions in mind, we have developed a three-tier action knowledge representation for the robotic arm. The lower level connects to the physical arm and defines the trajectories of executing three atomic actions supported by the arm (i.e., open gripper, close gripper, move). The middle level defines primitive operators such as Open Grip, Close Grip and MoveTo in the fashion of the traditional AI planner (Fikes and Nilsson, 1971) and directly links to the lower level. The upper-level captures the high-level actions acquired by learning from the human. These highlevel actions are represented as the desired goal states of the environment as a result of these actions. This three-tier representation allows the robot to automatically come up with a sequence of lower-level actions by applying existing planning algorithms. Based on this representation, we implemented a dialogue system for action learning and further conducted an empirical study with human subjects. In particular, we compared the dialogue 89 Proceedings of th</context>
<context position="13262" citStr="Fikes and Nilsson, 1971" startWordPosition="2095" endWordPosition="2098">ner and Continuous Planner. 4.1.1 Continuous Planner This lowest level planner defines three primitive actions: open (i.e., open gripper), close (i.e., close gripper) and move (i.e., move to the destination). Each primitive action is defined as a trajectory computing function, implemented as inverse kinematics. The outputs of these functions are control commands sendt to each arm motor to keep the arm following the trajectory. 4.1.2 Discrete Planner The Discrete Planner is used to decompose a high-level action into a sequence of primitive actions. In our system, it is implemented as a STRIPS (Fikes and Nilsson, 1971) planner, which is defined as a quadruple (P, O, I, G): • P: Set of predicates describing a domain. • O: Set of operators. Each is specified by a set of preconditions and effects. An operator is applicable only when its preconditions could be entailed in a state. • I: Initial state, the starting point of a problem. • G: Goal state, which should be achieved if the problem is solved. In our system, O set includes Open Gripper, Close Gripper and 8 different kinds of MoveTo (She et al., 2014). And the P set consists of two dimensions of the environment: • Arm States: G Open/Close (i.e., whether th</context>
</contexts>
<marker>Fikes, Nilsson, 1971</marker>
<rawString>Richard E. Fikes and Nils J. Nilsson. 1971. Strips: A new approach to the application of theorem proving to problem solving. In Proceedings of the 2Nd International Joint Conference on Artificial Intelligence, IJCAI’71, pages 608–620, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Kollar</author>
<author>Stefanie Tellex</author>
<author>Deb Roy</author>
<author>Nicholas Roy</author>
</authors>
<title>Toward understanding natural language directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th ACM/IEEE International Conference on Human-robot Interaction, HRI ’10,</booktitle>
<pages>259--266</pages>
<publisher>IEEE Press.</publisher>
<location>Piscataway, NJ, USA.</location>
<contexts>
<context position="5376" citStr="Kollar et al., 2010" startWordPosition="838" endWordPosition="842">age. Motivated by Winograd’s early work, we start our initial investigation on action learning in a physical blocks world and with a physical robotic arm. The blocks world is the most famous domain used for planning in artificial intelligence. Thus it allows us to focus on mechanisms that, on one hand, connect symbolic representations of language with lower-level continuous sensorimotor representations of the robot; and on the other hand, support the use of the planning algorithms to address novel situations. Most previous work on following human instructions are based on supervised learning (Kollar et al., 2010; Tellex et al., 2011; Chen et al., 2010) or reinforcement learning (Branavan et al., 2012; Branavan et al., 2010). These types of learning may not be adequate in time-critical situations where only resources available to the robot is its human partners. Thus it is desirable that humans can engage in a natural language dialogue to teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work o</context>
</contexts>
<marker>Kollar, Tellex, Roy, Roy, 2010</marker>
<rawString>Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. 2010. Toward understanding natural language directions. In Proceedings of the 5th ACM/IEEE International Conference on Human-robot Interaction, HRI ’10, pages 259–266, Piscataway, NJ, USA. IEEE Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadas Kress-Gazit</author>
<author>Georgios E Fainekos</author>
<author>George J Pappas</author>
</authors>
<title>Translating structured english to robot controllers.</title>
<date>2008</date>
<journal>Advanced Robotics,</journal>
<volume>22</volume>
<issue>12</issue>
<contexts>
<context position="6331" citStr="Kress-Gazit et al., 2008" startWordPosition="988" endWordPosition="991">o teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has applied learning by demonstration to teach robots new skills (Cakmak et al., 2010). To potentially allow natural language instructions, previous work has also explored connecting language with lowerlevel control systems (Kress-Gazit et al., 2008; Siskind, 1999; Matuszek et al., 2012). Different from these previous works, here we investigate the use of natural language dialogue for learning actions. Previous work described in (Cantrell et al., 2012; Mohan et al., 2013) is most similar to our work. Here we focus on both grounded learning and the use of planning for action learning. 3 Dialogue System Figure 2: System Architecture We developed a dialogue system to support learning new actions. An example setup is shown in Figure 1, in which a SCHUNK arm is used to manipulate blocks placed on a surface. In Hi, the human starts to ask the </context>
</contexts>
<marker>Kress-Gazit, Fainekos, Pappas, 2008</marker>
<rawString>Hadas Kress-Gazit, Georgios E. Fainekos, and George J. Pappas. 2008. Translating structured english to robot controllers. Advanced Robotics, 22(12):1343–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changsong Liu</author>
<author>Rui Fang</author>
<author>Joyce Chai</author>
</authors>
<title>Towards mediating shared perceptual basis in situated dialogue.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>140--149</pages>
<location>Seoul, South</location>
<contexts>
<context position="9936" citStr="Liu et al., 2012" startWordPosition="1565" endWordPosition="1568">ich captures objects and their properties (in the numerical form). The robot can also access to its own internal status, such as the location of the gripper and whether it’s open or closed. Combining the robot’s state and environment information, the Discrete State Builder can represent the entire environment as a conjunction of predicates, which will be later used for action planning. Referential Grounding: To make the semantic representation meaningful, it must be grounded to the robot’s representation of perception. We use the graph-based approach for referential grounding as described in (Liu et al., 2012)(Liu et al., 2013). Once the references are grounded, the semantic representation becomes a Grounded Action Frame. For example, as shown in Figure 3, “the blue block” refers to B1 and “the red block on your right” refers to R1. Dialogue Manager: The Dialogue Manager is used to decide what dialog acts the system should perform give a situation. It is composed by: a representation of dialogue state, a space of system activity and a dialogue policy. The dialogue status is computed based on the human intention a dialogue state captures (from semantic representation) and the Grounded Action Frame. </context>
</contexts>
<marker>Liu, Fang, Chai, 2012</marker>
<rawString>Changsong Liu, Rui Fang, and Joyce Chai. 2012. Towards mediating shared perceptual basis in situated dialogue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 140–149, Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changsong Liu</author>
<author>Rui Fang</author>
<author>Lanbo She</author>
<author>Joyce Chai</author>
</authors>
<title>Modeling collaborative referring for situated referential grounding.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>78--86</pages>
<location>Metz, France.</location>
<contexts>
<context position="9954" citStr="Liu et al., 2013" startWordPosition="1568" endWordPosition="1571">ts and their properties (in the numerical form). The robot can also access to its own internal status, such as the location of the gripper and whether it’s open or closed. Combining the robot’s state and environment information, the Discrete State Builder can represent the entire environment as a conjunction of predicates, which will be later used for action planning. Referential Grounding: To make the semantic representation meaningful, it must be grounded to the robot’s representation of perception. We use the graph-based approach for referential grounding as described in (Liu et al., 2012)(Liu et al., 2013). Once the references are grounded, the semantic representation becomes a Grounded Action Frame. For example, as shown in Figure 3, “the blue block” refers to B1 and “the red block on your right” refers to R1. Dialogue Manager: The Dialogue Manager is used to decide what dialog acts the system should perform give a situation. It is composed by: a representation of dialogue state, a space of system activity and a dialogue policy. The dialogue status is computed based on the human intention a dialogue state captures (from semantic representation) and the Grounded Action Frame. The current space </context>
</contexts>
<marker>Liu, Fang, She, Chai, 2013</marker>
<rawString>Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai. 2013. Modeling collaborative referring for situated referential grounding. In Proceedings of the SIGDIAL 2013 Conference, pages 78–86, Metz, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Evan Herbst</author>
<author>Luke S Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system.</title>
<date>2012</date>
<booktitle>Tracts in Advanced Robotics,</booktitle>
<volume>88</volume>
<pages>403--415</pages>
<editor>In Jaydev P. Desai, Gregory Dudek, Oussama Khatib, and Vijay Kumar, editors, ISER,</editor>
<publisher>Springer</publisher>
<contexts>
<context position="6370" citStr="Matuszek et al., 2012" startWordPosition="994" endWordPosition="997">language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has applied learning by demonstration to teach robots new skills (Cakmak et al., 2010). To potentially allow natural language instructions, previous work has also explored connecting language with lowerlevel control systems (Kress-Gazit et al., 2008; Siskind, 1999; Matuszek et al., 2012). Different from these previous works, here we investigate the use of natural language dialogue for learning actions. Previous work described in (Cantrell et al., 2012; Mohan et al., 2013) is most similar to our work. Here we focus on both grounded learning and the use of planning for action learning. 3 Dialogue System Figure 2: System Architecture We developed a dialogue system to support learning new actions. An example setup is shown in Figure 1, in which a SCHUNK arm is used to manipulate blocks placed on a surface. In Hi, the human starts to ask the robot to stack the blue block (i.e., Bi</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer, and Dieter Fox. 2012. Learning to parse natural language commands to a robot control system. In Jaydev P. Desai, Gregory Dudek, Oussama Khatib, and Vijay Kumar, editors, ISER, volume 88 of Springer Tracts in Advanced Robotics, pages 403–415. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiwali Mohan</author>
<author>James Kirk</author>
<author>John Laird</author>
</authors>
<title>A computational model for situated task learning with interactive instruction.</title>
<date>2013</date>
<booktitle>In Proceedings of ICCM 2013 - 12th International Conference on Cognitive Modeling.</booktitle>
<contexts>
<context position="6558" citStr="Mohan et al., 2013" startWordPosition="1024" endWordPosition="1027">e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has applied learning by demonstration to teach robots new skills (Cakmak et al., 2010). To potentially allow natural language instructions, previous work has also explored connecting language with lowerlevel control systems (Kress-Gazit et al., 2008; Siskind, 1999; Matuszek et al., 2012). Different from these previous works, here we investigate the use of natural language dialogue for learning actions. Previous work described in (Cantrell et al., 2012; Mohan et al., 2013) is most similar to our work. Here we focus on both grounded learning and the use of planning for action learning. 3 Dialogue System Figure 2: System Architecture We developed a dialogue system to support learning new actions. An example setup is shown in Figure 1, in which a SCHUNK arm is used to manipulate blocks placed on a surface. In Hi, the human starts to ask the robot to stack the blue block (i.e., Bi) on top of the red block (i.e., Rl). The robot does not understand the action “stack”, so it asks the human for instructions. Then the hu90 Figure 3: Example semantic representation and a</context>
</contexts>
<marker>Mohan, Kirk, Laird, 2013</marker>
<rawString>Shiwali Mohan, James Kirk, and John Laird. 2013. A computational model for situated task learning with interactive instruction. In Proceedings of ICCM 2013 - 12th International Conference on Cognitive Modeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lanbo She</author>
<author>Yu Cheng</author>
<author>Joyce Chai</author>
<author>Yunyi Jia</author>
<author>Shaohua Yang</author>
<author>Ning Xi</author>
</authors>
<title>Teaching robots new actions through natural language instructions.</title>
<date>2014</date>
<booktitle>In ROMAN.</booktitle>
<contexts>
<context position="13755" citStr="She et al., 2014" startWordPosition="2186" endWordPosition="2189">gh-level action into a sequence of primitive actions. In our system, it is implemented as a STRIPS (Fikes and Nilsson, 1971) planner, which is defined as a quadruple (P, O, I, G): • P: Set of predicates describing a domain. • O: Set of operators. Each is specified by a set of preconditions and effects. An operator is applicable only when its preconditions could be entailed in a state. • I: Initial state, the starting point of a problem. • G: Goal state, which should be achieved if the problem is solved. In our system, O set includes Open Gripper, Close Gripper and 8 different kinds of MoveTo (She et al., 2014). And the P set consists of two dimensions of the environment: • Arm States: G Open/Close (i.e., whether the gripper is open or closed), G Full/Empty (i.e., whether the gripper has an object in it) and G At(x) (i.e, location of the arm). • Object States: Top Uclr/Clr(o) (i.e., whether the block o has another block on its top), In/Out G(o) (i.e., whether o is within the gripper fingers or not) and On(o,x) (i.e., o is supported by x). The I and G are captured real-time during the dialogue interaction. 4.1.3 High-level action Knowledge The high-level actions represent actions specified by the hum</context>
</contexts>
<marker>She, Cheng, Chai, Jia, Yang, Xi, 2014</marker>
<rawString>Lanbo She, Yu Cheng, Joyce Chai, Yunyi Jia, Shaohua Yang, and Ning Xi. 2014. Teaching robots new actions through natural language instructions. In ROMAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic.</title>
<date>1999</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>15</volume>
<issue>1</issue>
<pages>90</pages>
<contexts>
<context position="6346" citStr="Siskind, 1999" startWordPosition="992" endWordPosition="993"> Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language to symbolic interface widgets on web pages. In the robotics community, previous work has applied learning by demonstration to teach robots new skills (Cakmak et al., 2010). To potentially allow natural language instructions, previous work has also explored connecting language with lowerlevel control systems (Kress-Gazit et al., 2008; Siskind, 1999; Matuszek et al., 2012). Different from these previous works, here we investigate the use of natural language dialogue for learning actions. Previous work described in (Cantrell et al., 2012; Mohan et al., 2013) is most similar to our work. Here we focus on both grounded learning and the use of planning for action learning. 3 Dialogue System Figure 2: System Architecture We developed a dialogue system to support learning new actions. An example setup is shown in Figure 1, in which a SCHUNK arm is used to manipulate blocks placed on a surface. In Hi, the human starts to ask the robot to stack </context>
</contexts>
<marker>Siskind, 1999</marker>
<rawString>Jeffrey Mark Siskind. 1999. Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic. J. Artif. Int. Res., 15(1):31– 90, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Thomas Kollar</author>
<author>Steven Dickerson</author>
<author>Matthew R Walter</author>
<author>Ashis Gopal Banerjee</author>
<author>Seth J Teller</author>
<author>Nicholas Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<editor>In Wolfram Burgard and Dan Roth, editors, AAAI.</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="5397" citStr="Tellex et al., 2011" startWordPosition="843" endWordPosition="846">ograd’s early work, we start our initial investigation on action learning in a physical blocks world and with a physical robotic arm. The blocks world is the most famous domain used for planning in artificial intelligence. Thus it allows us to focus on mechanisms that, on one hand, connect symbolic representations of language with lower-level continuous sensorimotor representations of the robot; and on the other hand, support the use of the planning algorithms to address novel situations. Most previous work on following human instructions are based on supervised learning (Kollar et al., 2010; Tellex et al., 2011; Chen et al., 2010) or reinforcement learning (Branavan et al., 2012; Branavan et al., 2010). These types of learning may not be adequate in time-critical situations where only resources available to the robot is its human partners. Thus it is desirable that humans can engage in a natural language dialogue to teach robots new skills. Using natural language dialogue to learn new skills have been explored previously by (Allen et al., 2007) where an artificial agent was developed to acquire skills through natural language instructions (i.e., find restaurant). But this work only grounds language </context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth J. Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In Wolfram Burgard and Dan Roth, editors, AAAI. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Procedures as a representation for data in a computer program for understanding natural language.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="1789" citStr="Winograd, 1972" startWordPosition="275" endWordPosition="276">ance compared to one-shot instructions. 1 Introduction When a new generation of robots start to work side-by-side with their human partners in joint tasks (Christensen et al., 2010), they will often encounter new objects or are required to perform new actions. It is important for the robots to automatically learn new knowledge about the environment and the tasks from their human partners. To address this issue, this paper describes our recent work on action learning through dialogue. As a first step, we limit our investigation to a simple blocks world motivated by Terry Winograd’s early work (Winograd, 1972). By using an industrial robotic arm (SCHUNK) in this small world, we are interested in addressing the following questions. First, human language has a discrete and symbolic representation, but the robot arm has a continuous representation for its movements. Where should the connections between the symbolic representation and the continuous representation take place so that human language can be used to direct the robot’s movements? Second, when the robot learns new tasks from its human partner, how to represent the acquired knowledge effectively so that it can be applied in novel situations? </context>
<context position="4528" citStr="Winograd, 1972" startWordPosition="705" endWordPosition="706">by-step instructions (i.e., one step at a time and wait for the robot’s response at each step before going to the next step) with the one-shot instructions (i.e., give the instruction with all steps at once). Our empirical results have shown that the three-tier knowledge representation can capture the learned new action and apply it to novel situations. Although the step-by-step instructions resulted in a lengthier teaching process compared to the one-shot instructions, they led to better learning performance for the robot. 2 Related Work Over forty years ago, Terry Winograd developed SHRDLU (Winograd, 1972) to demonstrate natural language understanding using a simulated block-moving arm. One aspect he did not address, but mentioned in his thesis (Winograd, 1972) as an important aspect, was learning new actions through natural language. Motivated by Winograd’s early work, we start our initial investigation on action learning in a physical blocks world and with a physical robotic arm. The blocks world is the most famous domain used for planning in artificial intelligence. Thus it allows us to focus on mechanisms that, on one hand, connect symbolic representations of language with lower-level conti</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>T. Winograd. 1972. Procedures as a representation for data in a computer program for understanding natural language. Cognitive Psychology, 3(1):1–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huatao Zhang</author>
<author>Yunyi Jia</author>
<author>Ning Xi</author>
</authors>
<title>Sensorbased redundancy resolution for a nonholonomic mobile manipulator.</title>
<date>2012</date>
<booktitle>In IROS,</booktitle>
<pages>5327--5332</pages>
<contexts>
<context position="16695" citStr="Zhang et al., 2012" startWordPosition="2677" endWordPosition="2680">h of primitive actions to solve the problem. For example, the PickUp(B1) in Figure 4 can be solved by Open Grip, MoveTo(B1), Close Grip and MoveTo(air). The primitive actions are executed by the continuous planner and control process in the lower robotic system. For the “open” and “close”, they are executed by controlling the position of the gripper fingers. For the “move”, a task-space trajectory is first planned based on the minimum-time motion planning algorithm to move the robot endeffector from the current position to the final position. A kinematic controller with redundancy resolution (Zhang et al., 2012) is then used to generate the joint movements for the robot to track the planned trajectory. Achieving the end of the trajectory indicates the action completion. 4.3 Action Learning Figure 5 illustrates the system internal process of acquiring action knowledge from the dialogue in Figure 1. At the beginning of the dialogue, the grounded action frame Stack(B1, R1) captured from the first human utterance is not in the action knowledge, so it will be pushed to the top of the unknown action stack as a new action waiting to be learned. The environment state at this point is calculated as shown in t</context>
</contexts>
<marker>Zhang, Jia, Xi, 2012</marker>
<rawString>Huatao Zhang, Yunyi Jia, and Ning Xi. 2012. Sensorbased redundancy resolution for a nonholonomic mobile manipulator. In IROS, pages 5327–5332.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
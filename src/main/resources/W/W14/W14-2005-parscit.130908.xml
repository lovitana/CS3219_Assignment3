<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006659">
<title confidence="0.986665">
Learning Verb Classes in an Incremental Model
</title>
<author confidence="0.997952">
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
</author>
<affiliation confidence="0.843165">
Department of Computer Science
University of Toronto
Toronto, Canada
</affiliation>
<email confidence="0.999568">
{libbyb,afsaneh,suzanne}@cs.toronto.edu
</email>
<sectionHeader confidence="0.994819" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951368421053">
The ability of children to generalize over
the linguistic input they receive is key to
acquiring productive knowledge of verbs.
Such generalizations help children extend
their learned knowledge of constructions
to a novel verb, and use it appropriately in
syntactic patterns previously unobserved
for that verb—a key factor in language
productivity. Computational models can
help shed light on the gradual development
of more abstract knowledge during verb
acquisition. We present an incremental
Bayesian model that simultaneously and
incrementally learns argument structure
constructions and verb classes given nat-
uralistic language input. We show how the
distributional properties in the input lan-
guage influence the formation of general-
izations over the constructions and classes.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968607142858">
Usage-based accounts of language learning note
that young children rely on verb-specific knowl-
edge to produce their early utterances (e.g.,
Tomasello, 2003). However, evidence suggests
that even young children can generalize their
verb knowledge to novel verbs and syntactic
frames (e.g., Fisher, 2002), and that the abstract
knowledge gradually strengthens over time (e.g.,
Tomasello and Abbot-Smith, 2002). One area of
verb usage where more sophisticated abstraction
appears necessary for fully adult productivity in
language is the knowledge of verb alternations.
A verb alternation is a pairing of constructions
shared by a number of verbs, in which the two
constructions express related argument structures
(Levin, 1993): e.g., the dative alternation involves
the related forms of the prepositional dative (PD;
X gave Yto Z) and the double-object dative (DO; X
gave Z Y). Such alternations enable language users
to readily adapt new and low frequency verbs to
appropriate constructions of the language by gen-
eralizing the observed use of one such form to the
other.1
For example, Conwell and Demuth (2007) show
that 3-year-old children understand that a novel
verb observed only in the DO dative (John gor-
ped Heather the book) can also be used in the PD
form (John gorped the book to Heather), though
the children can only generalize such knowledge
under certain experimental conditions. Wonnacott
et al. (2008) demonstrate the proficiency of adults
in making such generalizations within an artificial
language learning scenario, which enables the re-
searchers to explore the distributional properties
of the linguistic input that facilitate the acquisition
of such generalizations. The results suggest that
the overall frequency of the syntactic patterns as
well as the distribution of verbs across the patterns
play a facilitatory role in the formation of abstract
verb knowledge (in the form of verb alternations)
in adult language learners.
In this work, we propose a computational
model that extends an existing Bayesian model of
verb argument structure acquisition (Alishahi and
Stevenson, 2008)[AS08] to support the learning of
verb classes over the acquired constructions. Our
model is novel in its approach to verb class forma-
tion, because it clusters tokens of a verb that reflect
the distribution of the verb over the learned con-
structions each time the verb is used in an input.
That is, the model forms verb classes by cluster-
ing verb tokens that reflect the evolving usages of
the verbs in various constructions.
We use this new model to analyze the role of
the classes and the distributional properties of the
input in learning abstract verb knowledge, given
</bodyText>
<footnote confidence="0.981082666666667">
1The generalization of an alternation refers to a speaker
using one variant of an alternation for a verb (e.g., PD) having
only observed the verb in the other variant (e.g., DO).
</footnote>
<page confidence="0.989987">
37
</page>
<note confidence="0.7879835">
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 37–45,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999850882352941">
naturalistic input that contains many verbs and
many constructions. The model can form higher-
level generalizations such as learning verb alterna-
tions, which is not possible with the AS08 model
(cf. the findings of Parisien and Stevenson, 2010).
Moreover, because our model gradually forms its
representations of constructions and classes over
time (in contrast to other Bayesian models, such
as Parisien and Stevenson, 2010; Perfors et al.,
2010), it is possible to analyze the monotonically-
growing representations and show their compati-
bility with the developmental patterns seen in chil-
dren (Conwell and Demuth, 2007). We also repli-
cate some of the observations of Wonnacott et al.
(2008) on the role of distributional properties of
the language in influencing the degree of general-
ization over an alternation.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999982240740741">
To explore the properties of learning mechanisms
that are capable of mimicking child and adult psy-
cholinguistic observations, a number of cognitive
modeling studies have focused on learning ab-
stract verb knowledge from individual verb usages
(e.g., Alishahi and Stevenson, 2008; Perfors et al.,
2010; Parisien and Stevenson, 2010). Here we fo-
cus on such computational models that enable the
sort of higher-level generalization that people do
across verb alternations, unlike the AS08 model.
The hierarchical Bayesian models of Perfors
et al. (2010) and Parisien and Stevenson (2010)
focus on learning this kind of higher-level general-
ization. The model of Perfors et al. (2010) learns
verb alternations, i.e., pairs of syntactic patterns
shared by certain groups of verbs. By incorpo-
rating this sort of abstract knowledge into their
model, Perfors et al. are able to simulate the abil-
ity of adults to generalize across verb alternations
(as in Wonnacott et al., 2008). That is, Perfors
et al. predict the ability of a novel verb to occur
in a syntactic structure after exposure to it in the
alternative pattern of that alternation. However,
this model is trained on data that contains only a
limited number of verbs and syntactic patterns un-
like naturalistic Child-directed Speech (CDS) and
moreover incorporates built-in information about
verb constructions.
The hierarchical Dirichlet model of Parisien
and Stevenson (2010) addresses these limitations
by working with natural child-directed speech
(CDS) data. Moreover, the model of Parisien and
Stevenson simultaneously learns constructions as
in AS08 and verb classes based on verb alterna-
tion behaviour, showing that the latter level of ab-
straction is necessary to support effective learn-
ing of verb alternations. Still, the models of both
Parisien and Stevenson and Perfors et al. can only
be utilized as a batch process and hence are lim-
ited in the analysis of developmental trajectories.
Although it is possible to simulate development by
training such models on increasing portions of in-
put, such an approach does not ensure that the rep-
resentations given n + i inputs can be developed
from the representation given n inputs.
In this paper, we propose a significant extension
to the model of AS08, by adding an extra level of
abstraction that incrementally learns verb classes
by drawing on the distribution of verbs over the
learned constructions. The new model combines
the advantages of having a monotonic clustering
model that enables the analysis of developing clus-
ters, with the simultaneous learning of construc-
tions and verb classes.
</bodyText>
<sectionHeader confidence="0.993537" genericHeader="method">
3 The Computational Model
</sectionHeader>
<bodyText confidence="0.999989235294118">
As mentioned above, our model is an extension
of the model of AS08 in which we add a level of
learned abstract knowledge about verbs. Specif-
ically, our model uses a Bayesian clustering pro-
cess to learn clusters of verb usages that occur in
similar argument structure constructions, as in the
original model of AS08. To this, we add another
level of abstraction that learns clusters of verbs
that exhibit similar distributional patterns of oc-
currence across the learned constructions—that is,
classes of verbs that occur in similar sets of con-
structions, and in similar proportions. To distin-
guish between the clusters of the two levels of ab-
straction in our new model, we refer to the clusters
of verb usages as constructions, and to the group-
ings of verbs given their distribution over those
constructions as verb classes.
</bodyText>
<subsectionHeader confidence="0.999728">
3.1 Overview of the Model
</subsectionHeader>
<bodyText confidence="0.999921625">
The model learns from a sequence of frames,
where each frame is a collection of features rep-
resenting what the learner might extract from an
utterance s/he has heard. Similarly to previous
computational studies (e.g., Parisien and Steven-
son, 2010), here we focus on syntactic features
since our goal is to understand the acquisition of
acceptable syntactic structures of verbs indepen-
</bodyText>
<page confidence="0.999365">
38
</page>
<figureCaption confidence="0.969561666666667">
Figure 1: A visual representation of the two levels of ab-
straction in the model, with sample verb usages input (and
extracted input frames), constructions, and classes.
</figureCaption>
<bodyText confidence="0.998045861111111">
dently of their meaning, as in some relevant psy-
cholinguistic (Wonnacott et al., 2008) and com-
putational studies (Parisien and Stevenson, 2010).
We focus particularly on properties such as syn-
tactic slots and argument count. (These features,
as in Parisien and Stevenson (2010), provide a
more flexible and generalizable representation of a
syntactic structure than the syntactic pattern string
used by AS08.) See the bottom rows of boxes in
Figure 1 for sample input verb usages with their
extracted frames.
The model incrementally clusters the extracted
input frames into constructions that reflect prob-
abilistic associations of the features across simi-
lar verb usages; see the middle level of Figure 1.
Each learned cluster is a probabilistic (and possi-
bly noisy) representation of an argument structure
construction: e.g., a cluster containing frames cor-
responding to usages such as I eat apples, She took
the ball, and He got a book, etc., represents a Tran-
sitive Action construction.2 Such constructions al-
low for some degree of generalization over the ob-
served input; e.g., when seeing a novel verb in a
Transitive utterance, the model predicts the simi-
larity of this verb to other Action verbs appearing
in that pattern (Alishahi and Stevenson, 2008).
Grouping of verb usages into constructions may
not be sufficient for making higher-level general-
izations across verb alternations. Knowledge of al-
ternations is only captured indirectly in construc-
tions (because usages of the same verb can oc-
cur in multiple clusters). Following Parisien and
Stevenson (2010), we hypothesize that true gen-
eralization behaviour requires explicit knowledge
that verbs have commonalities in their patterns of
occurrence across constructions; this is the basis
</bodyText>
<footnote confidence="0.673164">
2Because the associations are probabilistic, a linguistic
construction may be represented by more than one cluster.
</footnote>
<bodyText confidence="0.9998255">
for verb classes (Levin, 1993; Merlo and Steven-
son, 2000; Schulte im Walde and Brew, 2002).
To capture this, our model learns groupings of
verbs that have similar distributions across the
learned constructions. These groupings form verb
classes that provide a higher-level of abstraction
over the input; see the top level in Figure 1. Con-
sider the dative alternation: the classes capture the
fact that some verbs may occur only in preposi-
tional dative (PD) forms, such as sing, while oth-
ers occur only in double object (DO) forms (call),
while still others alternate – i.e., they occur in both
(bring).
Our model simultaneously learns both of these
types of knowledge: constructions are clusters of
verb usages, and classes are clusters of verb dis-
tributions over those constructions. Importantly, it
does so incrementally, which allows us to exam-
ine the developmental trajectory of acquiring al-
ternations such as the dative as the learned clus-
ters grow over time. Moreover, both types of clus-
tering are monotonic, i.e., we do not re-structure
the groupings that our model learns. However, the
model in both levels is clustering verb tokens – i.e.,
the features corresponding to the verb at that time
in the input, its usage or its current distribution –
so that the same verb type may be added to various
clusters at different stages in the training.
</bodyText>
<subsectionHeader confidence="0.99982">
3.2 Learning Constructions of Verb Usages
</subsectionHeader>
<bodyText confidence="0.9999897">
The model of AS08 groups input frames into clus-
ters on the basis of the overall similarity in the
values of their features. Importantly, the model
learns these clusters incrementally in response to
the input; the number and type of clusters is not
predetermined. The model considers the creation
of a new cluster for a given frame if the frame is
not sufficiently similar to any of the existing clus-
ters. Formally, the model finds the best cluster for
a given input frame F as in:
</bodyText>
<equation confidence="0.955478">
BestCluster(F) = argmax P(k|F) (1)
k∈Clusters
</equation>
<bodyText confidence="0.99992">
where k ranges over all existing clusters and a new
one. Using Bayes rule:
</bodyText>
<equation confidence="0.99808">
P(k|F) = P( k)P() |k) ∝ P(k)P(F|k) (2)
</equation>
<bodyText confidence="0.99948">
P(FThe prior probability of a cluster P(k) is estimated
as the proportion of frames that are in k out of
all observed input frames, thus assigning a higher
</bodyText>
<page confidence="0.996578">
39
</page>
<bodyText confidence="0.9999425">
prior more frequent constructions. The likelihood
P(F|k) is estimated based on the match of fea-
ture values in F and in the frames of k (assuming
independence of the features):
</bodyText>
<equation confidence="0.975967">
P(F|k) = � Pi(j|k) (3)
i∈Features
</equation>
<bodyText confidence="0.722042">
where j is the value of the ith feature of F, and
Pi(j|k) is calculated using a smoothed version of:
</bodyText>
<equation confidence="0.984017666666667">
counti(j, k) (4)
Pi(j|k) =
nk
</equation>
<bodyText confidence="0.999983375">
where counti(j, k) is the number of times feature i
has the value j in cluster k, and nk is the number of
frames in k. We compare the slot features as sets to
capture similarities in overlapping syntactic slots
rather than enforcing an exact match. The model
uses the Jaccard similarity score to measure the
degree of overlap between two feature sets, instead
of the direct count of occurrence in Eqn. (4):
</bodyText>
<equation confidence="0.996494">
sim score(S1, S2) = |S1 n S2 |(5)
|S1 U S2|
</equation>
<bodyText confidence="0.999905">
where S1 and S2 in our experiments here are the
sets of syntactic slot features.
</bodyText>
<subsectionHeader confidence="0.999956">
3.3 Learning Verb Classes
</subsectionHeader>
<bodyText confidence="0.999990342105263">
Our new model extends the construction-
formation model of AS08 by grouping verbs into
classes on the basis of their distribution across
the learned constructions. That is, verbs that have
statistically-similar patterns of occurrence across
the learned constructions will be considered as
forming a verb class. For example, in Figure 1 we
see that bring and read may be put into the same
class because they both occur in a similar relative
frequency across the DO and PD constructions
(the leftmost and rightmost constructions in the
figure).
We use the same incremental Bayesian cluster-
ing algorithm for learning the verb classes as for
learning constructions. At the class level, the fea-
ture used for determining similarity of items in
clustering is the distribution of each verb across
the learned constructions. As for constructions,
the model learns the verb classes incrementally;
the number and type is not predetermined. More-
over, just as constructions are gradually formed
from successively processing a particular verb us-
age at each input step, the model forms verb
classes from a sequence of snapshots of the input
verb’s distribution over the constructions at each
input step. This means that our model is forming
classes of verb tokens rather than types; if a verb’s
behaviour changes over the duration of the input,
subsequent tokens (the distributions over construc-
tions at later points in time) may be clustered into
a different class (or classes) than earlier tokens,
even though prior decisions cannot be undone.
Formally, after clustering the input frame at
time t into a construction, as explained above, the
model extracts the current distribution dvt of its
head verb v over the learned constructions; this is
estimated as a smoothed version of v’s relative fre-
quency in each construction:
</bodyText>
<equation confidence="0.9692755">
P(k|v) =
nv
</equation>
<bodyText confidence="0.999968333333333">
where count(v, k) is the number of times that in-
puts with verb v have been clustered into construc-
tion k, and nv is the number of times v has oc-
curred in the input thus far.
To cluster this snapshot of the verb’s distribu-
tion, dvt, it is compared to the distributions en-
coded by the model’s classes. The distribution dc
of an existing class c is the weighted average of
the distributions of its member verb tokens:
</bodyText>
<equation confidence="0.9248205">
dc = |S |1:count(v, c) x dv (7)
v∈c
</equation>
<bodyText confidence="0.9999148">
where |c |is the size of class c, count(v, c) is the
number of occurrences of v that have been as-
signed to c, and dv is the distribution of the verb v
given by the tokens of v (the “snapshots” of distri-
butions of v assigned to class c). That is, dv in c is
an average of the distributions of all dvt for verb v
that have been clustered into c.
The model finds the best class for a given verb
distribution dvt based on its similarity to the dis-
tributions of all existing classes and a new one:
</bodyText>
<equation confidence="0.977821">
BestClass(dvt) = argmax (1 − DJS(dclldvt))
c∈Classes
</equation>
<bodyText confidence="0.948116875">
(8)
where c ranges over all existing classes as well as
a new class that is represented as a uniform dis-
tribution over the existing constructions. Jensen–
Shannon divergence, DJS, is a popular method for
measuring the distance between two distributions:
It is based on the KL–divergence, but it is symmet-
ric and has a finite value between 0 and 1:
</bodyText>
<equation confidence="0.9974956">
DJS(pllq) =
2DKL(pll1
1 2(p+q))+ 12DKL(qll12(p+q)) (9)
count(v, k)
(6)
</equation>
<page confidence="0.978856">
40
</page>
<table confidence="0.998625">
non-ALT ALT
DO-only PD-only DO PD
Number of verbs 12 5 6
Relative frequency 14% 2% 2% 1%
</table>
<tableCaption confidence="0.933062666666667">
Table 1: Number of non-alternating (non-ALT) and alter-
nating (ALT) verbs in our lexicon, as well as the relative fre-
quency of each construction in our generated input corpora.
</tableCaption>
<sectionHeader confidence="0.999233" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.999824">
4.1 Generation of the Input Corpora
</subsectionHeader>
<bodyText confidence="0.999994064516129">
We follow the input generation method of AS08
to create naturalistic corpora that are based on the
distributional properties of verbs over various con-
structions, as observed in child-directed speech
(CDS). Our input-generation lexicon contains 71
verbs drawn from AS08 (11 action verbs) and
Barak et al. (2013) (31 verbs of varying syntac-
tic patterns), plus an additional 40 of the most fre-
quent verbs in CDS, in order to have a range of
verbs that occur with the PD and DO construc-
tions. Table 4.1 shows the number of verbs that
appear in the DO or PD construction only (non-
alternating), as well as those that alternate across
the two. (The table also gives the relative fre-
quency of each dative construction in our gener-
ated input corpora.) Each verb lexical entry in-
cludes its overall frequency, and its relative fre-
quency with each of a number of observed syn-
tactic constructions. The frequencies are extracted
from a manual annotation of a sample of 100
child-directed utterances per verb from a collec-
tion of eight corpora from CHILDES (MacWhin-
ney, 2000).3 An input corpus is generated by it-
eratively selecting a random verb and a syntactic
construction based on their frequencies according
to the lexicon, so that all input corpora used in our
simulations have the distributional properties ob-
served in CDS, but show some variation in precise
make-up and ordering of verb usages. The gener-
ated input consists offrames (a set of features) that
correspond to verb usages in CDS.
</bodyText>
<subsectionHeader confidence="0.988904">
4.2 Simulations
</subsectionHeader>
<bodyText confidence="0.9719415">
Because the generation of the input data is prob-
abilistic, we conduct 100 simulations for each
experiment (each using a different input cor-
pus) to avoid any dependency on specific id-
iosyncratic properties of a single generated cor-
pus. For each simulation, we train our model
3Brown (1973); Suppes (1974); Kuczaj (1977); Bloom
et al. (1974); Sachs (1983); Lieven et al. (2009).
on an automatically-generated corpus of 15, 000
frames, from which the model learns construc-
tions and verb classes. At specified points in
the input, we present the model with usages of
a novel verb in a DO and/or PD frame, and
then test the model’s generalization ability by
predicting DO and PD frames given that verb.
Since we are interested in the relative likeli-
hoods of the two frames, we report the differ-
ence between the log-likelihood of the DO frame
and the log-likelihood of the PD frame, i.e.,
log-likelihood(DO) − log-likelihood(PD).
Specifically, we form a partial frame Ftest (con-
taining all usage features except for the verb) that
reflects either the PD or the DO syntax, and assess
the probability P(Ftest|v) for each of these, as in:
</bodyText>
<equation confidence="0.969079">
�P(Ftest|v) = P(Ftest|k)P(k|v)
k∈Constructions
(10)
</equation>
<bodyText confidence="0.998122588235294">
where P(Ftest|k) is calculated as in Eqn. (3).
We can calculate P(k|v) in two different ways:
using only the knowledge in the constructions of
the model, and using the knowledge that takes into
account the verb classes over the constructions.
For model predictions based on the construction
level only, we calculate P(k|v) as in Eqn. (6),
which is the smoothed relative frequency of the
verb v over construction k.
Predictions using knowledge of the verb classes
will instead determine P(k|v) drawing on the fit
of verb v to the various classes (specifically, the
similarity of v’s distribution over constructions to
the distribution encoded in each class), and the
likelihood of each construction k for each class c
(specifically, the likelihood of k given the distribu-
tion over constructions encoded in c), as in:
</bodyText>
<equation confidence="0.9712385">
P(k|v) ≈ � P(k|c)P(c|v) (11)
c∈Classes
</equation>
<bodyText confidence="0.999938307692308">
where P(k|c) is the probability of construction
k given class c’s distribution over constructions
(dc); and P(c|v) is the probability of c given verb
v’s distribution dv over the constructions (using
Jensen-Shannon divergence as in Eqn. (9)).
Due to the different number of clusters in each
of the construction and class layers of the model,
the likelihoods computed for each will differ in
the range of values. For this reason, specific val-
ues cannot be directly compared across the layers
of the model, rather we must analyze the general
trends of the construction-only and class-based re-
sults.
</bodyText>
<page confidence="0.999567">
41
</page>
<sectionHeader confidence="0.997525" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999996880952381">
In this section we examine whether and how our
model generalizes across the two variants of the
dative alternation, the double-object dative (DO)
and the prepositional dative (PD). To do so, we
measure the tendency of the model to produce a
novel verb observed in one dative frame in that
same frame, or in the other dative frame (unob-
served for that verb). Our goal is to understand the
impact of the learned constructions and classes on
this generalization behaviour. Following Parisien
and Stevenson (2010), we examine three input
conditions in which the novel verb occurs: (i)
twice with the DO syntax (non-alternating); (ii)
twice with the PD syntax (non-alternating); or (iii)
once each with DO and PD syntax (alternating).4
We then ask the model to predict the likelihood of
producing each dative frame with that verb. Our
focus here is on comparing the generalization abil-
ities of the two levels of abstract knowledge in our
model: the constructions versus the verb classes.
As a reminder, we use the dative alternation as
one example for considering this kind of higher-
level generalization behaviour observed in adults
and to a lesser extent in children. Moreover, we
perform the analysis in the context of naturalistic
input that contains many verbs (those that appear
in the dative and those that do not), and a variety of
constructions , to provide a realistic setting for the
task. Our settings differ from the psycholinguis-
tic studies in the variability of constructions com-
pared with the artificial language used by Won-
nacott et al., and in focusing only on the syntac-
tic properties unlike Conwell and Demuth. How-
ever, we follow the settings of these studies in an-
alyzing the syntactic properties of a generated ut-
terance given minimal exposure to a novel verb.
Therefore, we aim to replicate their general ob-
servations by showing that (i) children are limited
in their ability to generalize across verb alterna-
tions compared with adults, and (ii) the frequency
of a construction has a positive correlation with the
generalization rate of the construction.
</bodyText>
<subsectionHeader confidence="0.99951">
5.1 Generalization of Learned Knowledge
</subsectionHeader>
<bodyText confidence="0.99963325">
We examine the generalization patterns of our
model when presented with a novel verb in DO/PD
forms after being trained on 15, 000 inputs, which
we compare to the performance of adults in such
</bodyText>
<footnote confidence="0.9692845">
4For the alternating condition, half the simulations have
DO first, and half have PD first.
</footnote>
<figureCaption confidence="0.8283384">
Figure 2: The difference between the log-likelihood values
of the DO and PD frames, given each of the three input con-
ditions: DO only, PD only, and Alternating. Values above
zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
</figureCaption>
<bodyText confidence="0.999890216216216">
language tasks. We first consider the case where
the model predictions are based solely on the
knowledge of constructions. Here we expect the
predictions to correspond to the syntactic proper-
ties of the two inputs observed for the novel verb,
with limited generalization. That is, we expect a
non-alternating verb to be much more likely in the
observed dative frame, and an alternating verb to
be equally likely in both frames. The left hand
side of Figure 2 presents the differences in log-
likelihoods of the predicted DO and PD frames for
the novel verb using the construction-based prob-
abilities. The results confirm our expectation that
the knowledge of constructions can support only
limited generalization across the variants of an al-
ternation. For the non-alternating conditions, the
observed frame is highly favoured, and for the
Alternating test scenario, the DO and PD frames
have nearly equal likelihoods.
We next turn to using the knowledge of verb
classes, which we expect to enable generaliza-
tions that correspond to verb alternation behaviour
— that is, we expect the model predictions here
to reflect the knowledge that verbs that occur in
one form of the alternation also often occur in
the other form of the alternation. This is possible
because the classes in the model encode the dis-
tributional patterns of verbs across constructions.
In the absence of other factors, we would expect
the Alternating condition to again show near equal
likelihoods for the two frames, and the two non-
alternating conditions to show a slight preference
for the observed frame (rather than the strong pref-
erence seen in the construction-based predictions),
because the unobserved frame is also likely due to
the knowledge here of the alternation.
The right hand side of Figure 2 presents the
</bodyText>
<page confidence="0.997929">
42
</page>
<bodyText confidence="0.999905962264151">
difference in the log-likelihoods of the DO and
PD frames when using the knowledge encoded
in the verb classes. The results are not directly
in line with the simple prediction above: The
non-alternating (DO-only and PD-only) condi-
tions show a weak preference (as expected) for one
frame over another, but both favour the DO frame,
as does the Alternating condition. That is, the PD-
only and Alternating conditions show a preference
for the DO frame that does not follow simply from
the knowledge of alternations.
The DO preference in the PD-only and Alter-
nating conditions arises due to distributional fac-
tors in the input, related to the frequencies of the
constructions reported in Table 1. First, the DO
frame is overall much more likely than the PD
frame, causing generalization in the PD-only and
Alternating conditions to lean more to that frame.
Second, fully 1/3 of the uses of the PD frame in
the corpus are with verbs that alternate (i.e., 1%
of the corpus are PD frames of alternating PD-
DO verbs, out of a total of 3% of the corpus be-
ing PD frames), while only 1/8 of the uses of the
DO frame are with alternating rather than non-
alternating verbs. Recall that our classes encode
the distribution (roughly relative frequency) of the
verbs in the class occurring across the different
constructions. This means that in our class-based
predictions, greater weight will be given to con-
structions with DO when observing a PD frame
than to constructions with PD when observing a
DO frame. These results underline the importance
of using naturalistic input and considering the im-
pact of various distributional factors on general-
ization of verb knowledge.
In contrast to the construction-based results, our
class-based results conform with the experimental
findings of Wonnacott et al. (2008), who show that
adult (artificial) language learners robustly gener-
alize a newly-learned verb observed in a single
syntactic form by producing it in the alternating
syntactic form under certain language conditions.
Moreover, we show similar distributional effects
to theirs – the overall frequency of the syntactic
patterns, as well as the distribution of verbs across
those patterns – in the level of preference for one
form over another, within the context of our nat-
uralistic data with multiple verbs, constructions,
and alternations. These results show that the verb
classes in the model are able to capture useful ab-
stract knowledge that is key to understanding the
human ability to make high-level generalizations
across verb alternations.
</bodyText>
<subsectionHeader confidence="0.998674">
5.2 Development of Generalizations
</subsectionHeader>
<bodyText confidence="0.999606042553192">
Next, we present the results of our model evalu-
ated throughout the course of training in order to
understand the developmental pattern of general-
ization. We perform the same construction-based
or class-based prediction tasks (the likelihoods of
a DO and PD frame), following the same input
conditions (a novel verb with two DO frames, two
PD frames, or one of each) at given points during
the 15, 000 inputs. As above, we present the dif-
ference in the log-likelihood values of the DO and
the PD frames in order to focus on the relative like-
lihoods of the two frames within each condition of
construction-based or class-based predictions.
Figure 3(a) presents the results for the DO-
only test scenario. As in Section 5.1, for
both construction-based and class-based predic-
tions there is a higher likelihood for the DO frame
throughout the course of training. In contrast, the
incremental results for the PD-only test scenario,
in Figure 3(b), display a developing level of gen-
eralization throughout the training stage for the
class-based predictions. While the construction-
based predictions reflect a much higher likelihood
for the PD frame, the results from the verb classes
are in favor of the PD frame only initially; after
training on 5000 input frames, the likelihood of
the DO frame becomes higher for this test sce-
nario. These results indicate that using construc-
tion knowledge alone does not enable generaliza-
tion from the PD frame to the DO frame; in con-
trast, the verb class knowledge enables the grad-
ual acquisition of generalization ability over the
course of training.
Finally, Figure 3(c) presents the results for the
Alternating test scenario for the two types of pre-
dictions. As in Section 5.1, both construction-
based and class-based predictions have a small
preference for the DO frame. In the construction-
based predictions, this preference lessens over
time to where the likelihoods for DO and PD are
almost equal, while the class-based predictions
stay relatively constant in their preference for the
DO frame. In some ways the construction-based
predictions are more expected in response to an
apparently alternating verb; however, the class-
based predictions show a higher degree of general-
ization, responding to the higher frequency of the
</bodyText>
<page confidence="0.999601">
43
</page>
<figure confidence="0.991028">
(a) DO only (b) PD only (c) Alternating
</figure>
<figureCaption confidence="0.839773333333333">
Figure 3: Difference of log-likelihood values of the DO and PD frames over the course of training for the constructions and
the verb classes for each of the 3 test scenarios. Values above zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
</figureCaption>
<bodyText confidence="0.99994247826087">
DO frame and the higher association of PD frames
with DO alternates. These results again empha-
size the importance of further exploring the role
of distributional factors on generalization of verb
knowledge in children.
The developmental results presented here are in
line with the suggestions of Tomasello (2003) that
the productions of younger children follow ob-
served patterns in the input, and only later reflect
robust generalizations of their knowledge across
verbs. Conwell and Demuth (2007) for example,
found evidence of generalization across verb al-
ternations in 3-year-old children, but their produc-
tion of unobserved forms for a novel verb was
very sensitive to the precise context of the ex-
periment and the distributional patterns across the
novel verbs. In accord with these observations, the
developmental trajectories in our model show that
our class-based predictions increase in their degree
of generalization over time, and are sensitive to
various distributional factors in the input, such as
the overall expectation for a frame and the expec-
tation that a verb will alternate.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999990770833334">
We present a novel computational model that
probabilistically learns two levels of abstractions
over individual verb usages: constructions that
are clusters of similar verb usages, and classes of
verbs with similar distributional behaviour across
the constructions. Specifically, we extend the
model of AS08 by incrementally learning token-
based verb classes that generalize over the con-
struction knowledge level. In contrast to the mod-
els of Parisien and Stevenson and Perfors et al.,
our model is incremental, and hence enables the
analysis of the monotonically developing classes
to show the relation to the development of gener-
alization ability in human learners.
We analyze how generalization is supported by
each level of learning in our model: constructions
and verb classes. Our results confirm (cf. Parisien
and Stevenson, 2010) that a higher-level knowl-
edge of the verb classes is required to replicate the
observed patterns of generalization, such as pro-
ducing a novel verb gorp in the in the prepositional
dative pattern after hearing it in the double object
dative pattern. In addition, our analysis of the in-
crementally developing verb classes shows that the
generalization knowledge gradually emerges over
time, similar to what is observed in children.
The flexibility of input representation of our
model enables us to further explore the properties
of the input in learning abstract knowledge, fol-
lowing psycholinguistic studies. Our results repli-
cate the findings of Wonnacott et al. on the role
of the distributional properties over the alternat-
ing syntactic forms, but in naturalistic settings of
many constructions. In future, we plan to extend
this analysis by manipulating the distributions of
our input data to replicate the exact settings of the
artificial language used by Wonnacott et al.. More-
over, in this study, we followed the settings of pre-
vious computational and psycholinguistic studies
that focused on the syntactic properties of the in-
put (Perfors et al., 2010; Parisien and Stevenson,
2010; Wonnacott et al., 2008; Conwell and De-
muth, 2007). However, we can further our anal-
ysis by incorporating semantic features in the in-
put to study syntactic bootstrapping effects (Scott
and Fisher, 2009) as well as the role of seman-
tic properties in constraining the generalizations
across the alternating forms.
</bodyText>
<sectionHeader confidence="0.998824" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997206333333333">
The authors would like to thank Afra Alishahi for
providing us with the code and data for her model,
and to Chris Parisien for sharing his data with us.
</bodyText>
<page confidence="0.998858">
44
</page>
<sectionHeader confidence="0.993635" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997292837837837">
Afra Alishahi and Suzanne Stevenson. 2008. A
computational model of early argument struc-
ture acquisition. Cognitive Science, 32(5):789–
834.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2013. Acquisition of desires before beliefs:
A computational investigation. In Proceedings
of CoNLL-2013.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development:
If, when, and why. Cognitive Psychology,
6(3):380–420.
Roger Brown. 1973. A first language: The early
stages. Harvard Univ. Press.
Erin Conwell and Katherine Demuth. 2007. Early
syntactic productivity: Evidence from dative
shift. Cognition, 103(2):163–179.
Cynthia Fisher. 2002. The role of abstract syntac-
tic knowledge in language acquisition: A reply
to Tomasello. Cognition, 82(3):259–278.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589–600.
B. Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation, volume 348.
University of Chicago press Chicago, IL.
Elena Lieven, Doroth´e Salomo, and Michael
Tomasello. 2009. Two-year-old children’s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481–507.
B. MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
P. Merlo and S. Stevenson. 2000. Automatic verb
classification based on statistical distributions
of argument structure. Computational Linguis-
tics, 27(3):373–408.
Christopher Parisien and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the
32nd annual meeting of the Cognitive Science
Society.
Amy Perfors, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, negative
evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
37(03):607–642.
Jacqueline Sachs. 1983. Talking about the There
and Then: The emergence of displaced refer-
ence in parent–child discourse. Children’s lan-
guage, 4.
Sabine Schulte im Walde and Chris Brew. 2002.
Inducing German semantic verb classes from
purely syntactic subcategorisation information.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 223–230, Philadelphia, PA.
Rose M Scott and Cynthia Fisher. 2009. Two-
year-olds use distributional cues to interpret
transitivity-alternating verbs. Language and
cognitive processes, 24(6):777–803.
Patrick Suppes. 1974. The semantics of children’s
language. American psychologist, 29(2):103.
Michael Tomasello. 2003. Constructing a lan-
guage: A usage-based theory of language ac-
quisition.
Michael Tomasello and Kirsten Abbot-Smith.
2002. A tale of two theories: Response to
Fisher. Cognition, 83(2):207–214.
Elizabeth Wonnacott, Elissa L Newport, and
Michael K Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cog-
nitive psychology, 56(3):165–209.
</reference>
<page confidence="0.99939">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.731094">
<title confidence="0.999136">Learning Verb Classes in an Incremental Model</title>
<author confidence="0.973144">Afsaneh Fazly Barak</author>
<affiliation confidence="0.999855">Department of Computer University of</affiliation>
<address confidence="0.785025">Toronto,</address>
<abstract confidence="0.99785445">The ability of children to generalize over the linguistic input they receive is key to acquiring productive knowledge of verbs. Such generalizations help children extend their learned knowledge of constructions to a novel verb, and use it appropriately in syntactic patterns previously unobserved for that verb—a key factor in language productivity. Computational models can help shed light on the gradual development of more abstract knowledge during verb acquisition. We present an incremental Bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input. We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Afra Alishahi</author>
<author>Suzanne Stevenson</author>
</authors>
<title>A computational model of early argument structure acquisition.</title>
<date>2008</date>
<journal>Cognitive Science,</journal>
<volume>32</volume>
<issue>5</issue>
<pages>834</pages>
<contexts>
<context position="3118" citStr="Alishahi and Stevenson, 2008" startWordPosition="456" endWordPosition="459">lizations within an artificial language learning scenario, which enables the researchers to explore the distributional properties of the linguistic input that facilitate the acquisition of such generalizations. The results suggest that the overall frequency of the syntactic patterns as well as the distribution of verbs across the patterns play a facilitatory role in the formation of abstract verb knowledge (in the form of verb alternations) in adult language learners. In this work, we propose a computational model that extends an existing Bayesian model of verb argument structure acquisition (Alishahi and Stevenson, 2008)[AS08] to support the learning of verb classes over the acquired constructions. Our model is novel in its approach to verb class formation, because it clusters tokens of a verb that reflect the distribution of the verb over the learned constructions each time the verb is used in an input. That is, the model forms verb classes by clustering verb tokens that reflect the evolving usages of the verbs in various constructions. We use this new model to analyze the role of the classes and the distributional properties of the input in learning abstract verb knowledge, given 1The generalization of an a</context>
<context position="5169" citStr="Alishahi and Stevenson, 2008" startWordPosition="782" endWordPosition="785"> monotonicallygrowing representations and show their compatibility with the developmental patterns seen in children (Conwell and Demuth, 2007). We also replicate some of the observations of Wonnacott et al. (2008) on the role of distributional properties of the language in influencing the degree of generalization over an alternation. 2 Related Work To explore the properties of learning mechanisms that are capable of mimicking child and adult psycholinguistic observations, a number of cognitive modeling studies have focused on learning abstract verb knowledge from individual verb usages (e.g., Alishahi and Stevenson, 2008; Perfors et al., 2010; Parisien and Stevenson, 2010). Here we focus on such computational models that enable the sort of higher-level generalization that people do across verb alternations, unlike the AS08 model. The hierarchical Bayesian models of Perfors et al. (2010) and Parisien and Stevenson (2010) focus on learning this kind of higher-level generalization. The model of Perfors et al. (2010) learns verb alternations, i.e., pairs of syntactic patterns shared by certain groups of verbs. By incorporating this sort of abstract knowledge into their model, Perfors et al. are able to simulate t</context>
<context position="10183" citStr="Alishahi and Stevenson, 2008" startWordPosition="1590" endWordPosition="1593">f the features across similar verb usages; see the middle level of Figure 1. Each learned cluster is a probabilistic (and possibly noisy) representation of an argument structure construction: e.g., a cluster containing frames corresponding to usages such as I eat apples, She took the ball, and He got a book, etc., represents a Transitive Action construction.2 Such constructions allow for some degree of generalization over the observed input; e.g., when seeing a novel verb in a Transitive utterance, the model predicts the similarity of this verb to other Action verbs appearing in that pattern (Alishahi and Stevenson, 2008). Grouping of verb usages into constructions may not be sufficient for making higher-level generalizations across verb alternations. Knowledge of alternations is only captured indirectly in constructions (because usages of the same verb can occur in multiple clusters). Following Parisien and Stevenson (2010), we hypothesize that true generalization behaviour requires explicit knowledge that verbs have commonalities in their patterns of occurrence across constructions; this is the basis 2Because the associations are probabilistic, a linguistic construction may be represented by more than one cl</context>
</contexts>
<marker>Alishahi, Stevenson, 2008</marker>
<rawString>Afra Alishahi and Suzanne Stevenson. 2008. A computational model of early argument structure acquisition. Cognitive Science, 32(5):789– 834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libby Barak</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Acquisition of desires before beliefs: A computational investigation.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL-2013.</booktitle>
<contexts>
<context position="17756" citStr="Barak et al. (2013)" startWordPosition="2883" endWordPosition="2886">O-only PD-only DO PD Number of verbs 12 5 6 Relative frequency 14% 2% 2% 1% Table 1: Number of non-alternating (non-ALT) and alternating (ALT) verbs in our lexicon, as well as the relative frequency of each construction in our generated input corpora. 4 Experimental Setup 4.1 Generation of the Input Corpora We follow the input generation method of AS08 to create naturalistic corpora that are based on the distributional properties of verbs over various constructions, as observed in child-directed speech (CDS). Our input-generation lexicon contains 71 verbs drawn from AS08 (11 action verbs) and Barak et al. (2013) (31 verbs of varying syntactic patterns), plus an additional 40 of the most frequent verbs in CDS, in order to have a range of verbs that occur with the PD and DO constructions. Table 4.1 shows the number of verbs that appear in the DO or PD construction only (nonalternating), as well as those that alternate across the two. (The table also gives the relative frequency of each dative construction in our generated input corpora.) Each verb lexical entry includes its overall frequency, and its relative frequency with each of a number of observed syntactic constructions. The frequencies are extra</context>
</contexts>
<marker>Barak, Fazly, Stevenson, 2013</marker>
<rawString>Libby Barak, Afsaneh Fazly, and Suzanne Stevenson. 2013. Acquisition of desires before beliefs: A computational investigation. In Proceedings of CoNLL-2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lois Bloom</author>
<author>Lois Hood</author>
<author>Patsy Lightbown</author>
</authors>
<title>Imitation in language development: If, when, and why.</title>
<date>1974</date>
<journal>Cognitive Psychology,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="19285" citStr="Bloom et al. (1974)" startWordPosition="3145" endWordPosition="3148"> all input corpora used in our simulations have the distributional properties observed in CDS, but show some variation in precise make-up and ordering of verb usages. The generated input consists offrames (a set of features) that correspond to verb usages in CDS. 4.2 Simulations Because the generation of the input data is probabilistic, we conduct 100 simulations for each experiment (each using a different input corpus) to avoid any dependency on specific idiosyncratic properties of a single generated corpus. For each simulation, we train our model 3Brown (1973); Suppes (1974); Kuczaj (1977); Bloom et al. (1974); Sachs (1983); Lieven et al. (2009). on an automatically-generated corpus of 15, 000 frames, from which the model learns constructions and verb classes. At specified points in the input, we present the model with usages of a novel verb in a DO and/or PD frame, and then test the model’s generalization ability by predicting DO and PD frames given that verb. Since we are interested in the relative likelihoods of the two frames, we report the difference between the log-likelihood of the DO frame and the log-likelihood of the PD frame, i.e., log-likelihood(DO) − log-likelihood(PD). Specifically, w</context>
</contexts>
<marker>Bloom, Hood, Lightbown, 1974</marker>
<rawString>Lois Bloom, Lois Hood, and Patsy Lightbown. 1974. Imitation in language development: If, when, and why. Cognitive Psychology, 6(3):380–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
</authors>
<title>A first language: The early stages.</title>
<date>1973</date>
<publisher>Harvard Univ. Press.</publisher>
<contexts>
<context position="19234" citStr="Brown (1973)" startWordPosition="3139" endWordPosition="3140">requencies according to the lexicon, so that all input corpora used in our simulations have the distributional properties observed in CDS, but show some variation in precise make-up and ordering of verb usages. The generated input consists offrames (a set of features) that correspond to verb usages in CDS. 4.2 Simulations Because the generation of the input data is probabilistic, we conduct 100 simulations for each experiment (each using a different input corpus) to avoid any dependency on specific idiosyncratic properties of a single generated corpus. For each simulation, we train our model 3Brown (1973); Suppes (1974); Kuczaj (1977); Bloom et al. (1974); Sachs (1983); Lieven et al. (2009). on an automatically-generated corpus of 15, 000 frames, from which the model learns constructions and verb classes. At specified points in the input, we present the model with usages of a novel verb in a DO and/or PD frame, and then test the model’s generalization ability by predicting DO and PD frames given that verb. Since we are interested in the relative likelihoods of the two frames, we report the difference between the log-likelihood of the DO frame and the log-likelihood of the PD frame, i.e., log-l</context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Roger Brown. 1973. A first language: The early stages. Harvard Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erin Conwell</author>
<author>Katherine Demuth</author>
</authors>
<title>Early syntactic productivity: Evidence from dative shift.</title>
<date>2007</date>
<journal>Cognition,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="2123" citStr="Conwell and Demuth (2007)" startWordPosition="304" endWordPosition="307">y for fully adult productivity in language is the knowledge of verb alternations. A verb alternation is a pairing of constructions shared by a number of verbs, in which the two constructions express related argument structures (Levin, 1993): e.g., the dative alternation involves the related forms of the prepositional dative (PD; X gave Yto Z) and the double-object dative (DO; X gave Z Y). Such alternations enable language users to readily adapt new and low frequency verbs to appropriate constructions of the language by generalizing the observed use of one such form to the other.1 For example, Conwell and Demuth (2007) show that 3-year-old children understand that a novel verb observed only in the DO dative (John gorped Heather the book) can also be used in the PD form (John gorped the book to Heather), though the children can only generalize such knowledge under certain experimental conditions. Wonnacott et al. (2008) demonstrate the proficiency of adults in making such generalizations within an artificial language learning scenario, which enables the researchers to explore the distributional properties of the linguistic input that facilitate the acquisition of such generalizations. The results suggest tha</context>
<context position="4683" citStr="Conwell and Demuth, 2007" startWordPosition="706" endWordPosition="709">stics naturalistic input that contains many verbs and many constructions. The model can form higherlevel generalizations such as learning verb alternations, which is not possible with the AS08 model (cf. the findings of Parisien and Stevenson, 2010). Moreover, because our model gradually forms its representations of constructions and classes over time (in contrast to other Bayesian models, such as Parisien and Stevenson, 2010; Perfors et al., 2010), it is possible to analyze the monotonicallygrowing representations and show their compatibility with the developmental patterns seen in children (Conwell and Demuth, 2007). We also replicate some of the observations of Wonnacott et al. (2008) on the role of distributional properties of the language in influencing the degree of generalization over an alternation. 2 Related Work To explore the properties of learning mechanisms that are capable of mimicking child and adult psycholinguistic observations, a number of cognitive modeling studies have focused on learning abstract verb knowledge from individual verb usages (e.g., Alishahi and Stevenson, 2008; Perfors et al., 2010; Parisien and Stevenson, 2010). Here we focus on such computational models that enable the </context>
<context position="31755" citStr="Conwell and Demuth (2007)" startWordPosition="5192" endWordPosition="5195">lues above zero denote a higher likelihood for the DO frame, and values below zero denote a higher likelihood for the PD frame. DO frame and the higher association of PD frames with DO alternates. These results again emphasize the importance of further exploring the role of distributional factors on generalization of verb knowledge in children. The developmental results presented here are in line with the suggestions of Tomasello (2003) that the productions of younger children follow observed patterns in the input, and only later reflect robust generalizations of their knowledge across verbs. Conwell and Demuth (2007) for example, found evidence of generalization across verb alternations in 3-year-old children, but their production of unobserved forms for a novel verb was very sensitive to the precise context of the experiment and the distributional patterns across the novel verbs. In accord with these observations, the developmental trajectories in our model show that our class-based predictions increase in their degree of generalization over time, and are sensitive to various distributional factors in the input, such as the overall expectation for a frame and the expectation that a verb will alternate. 6</context>
</contexts>
<marker>Conwell, Demuth, 2007</marker>
<rawString>Erin Conwell and Katherine Demuth. 2007. Early syntactic productivity: Evidence from dative shift. Cognition, 103(2):163–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Fisher</author>
</authors>
<title>The role of abstract syntactic knowledge in language acquisition: A reply to Tomasello.</title>
<date>2002</date>
<journal>Cognition,</journal>
<volume>82</volume>
<issue>3</issue>
<contexts>
<context position="1315" citStr="Fisher, 2002" startWordPosition="180" endWordPosition="181">emental Bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input. We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes. 1 Introduction Usage-based accounts of language learning note that young children rely on verb-specific knowledge to produce their early utterances (e.g., Tomasello, 2003). However, evidence suggests that even young children can generalize their verb knowledge to novel verbs and syntactic frames (e.g., Fisher, 2002), and that the abstract knowledge gradually strengthens over time (e.g., Tomasello and Abbot-Smith, 2002). One area of verb usage where more sophisticated abstraction appears necessary for fully adult productivity in language is the knowledge of verb alternations. A verb alternation is a pairing of constructions shared by a number of verbs, in which the two constructions express related argument structures (Levin, 1993): e.g., the dative alternation involves the related forms of the prepositional dative (PD; X gave Yto Z) and the double-object dative (DO; X gave Z Y). Such alternations enable </context>
</contexts>
<marker>Fisher, 2002</marker>
<rawString>Cynthia Fisher. 2002. The role of abstract syntactic knowledge in language acquisition: A reply to Tomasello. Cognition, 82(3):259–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kuczaj</author>
<author>Stan</author>
</authors>
<title>The acquisition of regular and irregular past tense forms.</title>
<date>1977</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>16</volume>
<issue>5</issue>
<marker>Kuczaj, Stan, 1977</marker>
<rawString>A. Kuczaj, Stan. 1977. The acquisition of regular and irregular past tense forms. Journal of Verbal Learning and Verbal Behavior, 16(5):589–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English verb classes and alternations: A preliminary investigation, volume 348.</title>
<date>1993</date>
<institution>University of Chicago press Chicago, IL.</institution>
<contexts>
<context position="1738" citStr="Levin, 1993" startWordPosition="242" endWordPosition="243">ir early utterances (e.g., Tomasello, 2003). However, evidence suggests that even young children can generalize their verb knowledge to novel verbs and syntactic frames (e.g., Fisher, 2002), and that the abstract knowledge gradually strengthens over time (e.g., Tomasello and Abbot-Smith, 2002). One area of verb usage where more sophisticated abstraction appears necessary for fully adult productivity in language is the knowledge of verb alternations. A verb alternation is a pairing of constructions shared by a number of verbs, in which the two constructions express related argument structures (Levin, 1993): e.g., the dative alternation involves the related forms of the prepositional dative (PD; X gave Yto Z) and the double-object dative (DO; X gave Z Y). Such alternations enable language users to readily adapt new and low frequency verbs to appropriate constructions of the language by generalizing the observed use of one such form to the other.1 For example, Conwell and Demuth (2007) show that 3-year-old children understand that a novel verb observed only in the DO dative (John gorped Heather the book) can also be used in the PD form (John gorped the book to Heather), though the children can on</context>
<context position="10819" citStr="Levin, 1993" startWordPosition="1684" endWordPosition="1685"> into constructions may not be sufficient for making higher-level generalizations across verb alternations. Knowledge of alternations is only captured indirectly in constructions (because usages of the same verb can occur in multiple clusters). Following Parisien and Stevenson (2010), we hypothesize that true generalization behaviour requires explicit knowledge that verbs have commonalities in their patterns of occurrence across constructions; this is the basis 2Because the associations are probabilistic, a linguistic construction may be represented by more than one cluster. for verb classes (Levin, 1993; Merlo and Stevenson, 2000; Schulte im Walde and Brew, 2002). To capture this, our model learns groupings of verbs that have similar distributions across the learned constructions. These groupings form verb classes that provide a higher-level of abstraction over the input; see the top level in Figure 1. Consider the dative alternation: the classes capture the fact that some verbs may occur only in prepositional dative (PD) forms, such as sing, while others occur only in double object (DO) forms (call), while still others alternate – i.e., they occur in both (bring). Our model simultaneously l</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English verb classes and alternations: A preliminary investigation, volume 348. University of Chicago press Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Lieven</author>
<author>Doroth´e Salomo</author>
<author>Michael Tomasello</author>
</authors>
<title>Two-year-old children’s production of multiword utterances: A usage-based analysis.</title>
<date>2009</date>
<journal>Cognitive Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="19321" citStr="Lieven et al. (2009)" startWordPosition="3151" endWordPosition="3154">lations have the distributional properties observed in CDS, but show some variation in precise make-up and ordering of verb usages. The generated input consists offrames (a set of features) that correspond to verb usages in CDS. 4.2 Simulations Because the generation of the input data is probabilistic, we conduct 100 simulations for each experiment (each using a different input corpus) to avoid any dependency on specific idiosyncratic properties of a single generated corpus. For each simulation, we train our model 3Brown (1973); Suppes (1974); Kuczaj (1977); Bloom et al. (1974); Sachs (1983); Lieven et al. (2009). on an automatically-generated corpus of 15, 000 frames, from which the model learns constructions and verb classes. At specified points in the input, we present the model with usages of a novel verb in a DO and/or PD frame, and then test the model’s generalization ability by predicting DO and PD frames given that verb. Since we are interested in the relative likelihoods of the two frames, we report the difference between the log-likelihood of the DO frame and the log-likelihood of the PD frame, i.e., log-likelihood(DO) − log-likelihood(PD). Specifically, we form a partial frame Ftest (contai</context>
</contexts>
<marker>Lieven, Salomo, Tomasello, 2009</marker>
<rawString>Elena Lieven, Doroth´e Salomo, and Michael Tomasello. 2009. Two-year-old children’s production of multiword utterances: A usage-based analysis. Cognitive Linguistics, 20(3):481–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk, volume 2.</title>
<date>2000</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="18506" citStr="MacWhinney, 2000" startWordPosition="3018" endWordPosition="3020"> that occur with the PD and DO constructions. Table 4.1 shows the number of verbs that appear in the DO or PD construction only (nonalternating), as well as those that alternate across the two. (The table also gives the relative frequency of each dative construction in our generated input corpora.) Each verb lexical entry includes its overall frequency, and its relative frequency with each of a number of observed syntactic constructions. The frequencies are extracted from a manual annotation of a sample of 100 child-directed utterances per verb from a collection of eight corpora from CHILDES (MacWhinney, 2000).3 An input corpus is generated by iteratively selecting a random verb and a syntactic construction based on their frequencies according to the lexicon, so that all input corpora used in our simulations have the distributional properties observed in CDS, but show some variation in precise make-up and ordering of verb usages. The generated input consists offrames (a set of features) that correspond to verb usages in CDS. 4.2 Simulations Because the generation of the input data is probabilistic, we conduct 100 simulations for each experiment (each using a different input corpus) to avoid any dep</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>B. MacWhinney. 2000. The CHILDES project: Tools for analyzing talk, volume 2. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>S Stevenson</author>
</authors>
<title>Automatic verb classification based on statistical distributions of argument structure.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="10846" citStr="Merlo and Stevenson, 2000" startWordPosition="1686" endWordPosition="1690">ctions may not be sufficient for making higher-level generalizations across verb alternations. Knowledge of alternations is only captured indirectly in constructions (because usages of the same verb can occur in multiple clusters). Following Parisien and Stevenson (2010), we hypothesize that true generalization behaviour requires explicit knowledge that verbs have commonalities in their patterns of occurrence across constructions; this is the basis 2Because the associations are probabilistic, a linguistic construction may be represented by more than one cluster. for verb classes (Levin, 1993; Merlo and Stevenson, 2000; Schulte im Walde and Brew, 2002). To capture this, our model learns groupings of verbs that have similar distributions across the learned constructions. These groupings form verb classes that provide a higher-level of abstraction over the input; see the top level in Figure 1. Consider the dative alternation: the classes capture the fact that some verbs may occur only in prepositional dative (PD) forms, such as sing, while others occur only in double object (DO) forms (call), while still others alternate – i.e., they occur in both (bring). Our model simultaneously learns both of these types o</context>
</contexts>
<marker>Merlo, Stevenson, 2000</marker>
<rawString>P. Merlo and S. Stevenson. 2000. Automatic verb classification based on statistical distributions of argument structure. Computational Linguistics, 27(3):373–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Parisien</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Learning verb alternations in a usagebased bayesian model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd annual meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="4307" citStr="Parisien and Stevenson, 2010" startWordPosition="650" endWordPosition="653">ledge, given 1The generalization of an alternation refers to a speaker using one variant of an alternation for a verb (e.g., PD) having only observed the verb in the other variant (e.g., DO). 37 Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 37–45, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics naturalistic input that contains many verbs and many constructions. The model can form higherlevel generalizations such as learning verb alternations, which is not possible with the AS08 model (cf. the findings of Parisien and Stevenson, 2010). Moreover, because our model gradually forms its representations of constructions and classes over time (in contrast to other Bayesian models, such as Parisien and Stevenson, 2010; Perfors et al., 2010), it is possible to analyze the monotonicallygrowing representations and show their compatibility with the developmental patterns seen in children (Conwell and Demuth, 2007). We also replicate some of the observations of Wonnacott et al. (2008) on the role of distributional properties of the language in influencing the degree of generalization over an alternation. 2 Related Work To explore the </context>
<context position="6317" citStr="Parisien and Stevenson (2010)" startWordPosition="963" endWordPosition="966">ort of abstract knowledge into their model, Perfors et al. are able to simulate the ability of adults to generalize across verb alternations (as in Wonnacott et al., 2008). That is, Perfors et al. predict the ability of a novel verb to occur in a syntactic structure after exposure to it in the alternative pattern of that alternation. However, this model is trained on data that contains only a limited number of verbs and syntactic patterns unlike naturalistic Child-directed Speech (CDS) and moreover incorporates built-in information about verb constructions. The hierarchical Dirichlet model of Parisien and Stevenson (2010) addresses these limitations by working with natural child-directed speech (CDS) data. Moreover, the model of Parisien and Stevenson simultaneously learns constructions as in AS08 and verb classes based on verb alternation behaviour, showing that the latter level of abstraction is necessary to support effective learning of verb alternations. Still, the models of both Parisien and Stevenson and Perfors et al. can only be utilized as a batch process and hence are limited in the analysis of developmental trajectories. Although it is possible to simulate development by training such models on incr</context>
<context position="8616" citStr="Parisien and Stevenson, 2010" startWordPosition="1339" endWordPosition="1343"> constructions—that is, classes of verbs that occur in similar sets of constructions, and in similar proportions. To distinguish between the clusters of the two levels of abstraction in our new model, we refer to the clusters of verb usages as constructions, and to the groupings of verbs given their distribution over those constructions as verb classes. 3.1 Overview of the Model The model learns from a sequence of frames, where each frame is a collection of features representing what the learner might extract from an utterance s/he has heard. Similarly to previous computational studies (e.g., Parisien and Stevenson, 2010), here we focus on syntactic features since our goal is to understand the acquisition of acceptable syntactic structures of verbs indepen38 Figure 1: A visual representation of the two levels of abstraction in the model, with sample verb usages input (and extracted input frames), constructions, and classes. dently of their meaning, as in some relevant psycholinguistic (Wonnacott et al., 2008) and computational studies (Parisien and Stevenson, 2010). We focus particularly on properties such as syntactic slots and argument count. (These features, as in Parisien and Stevenson (2010), provide a mo</context>
<context position="10492" citStr="Parisien and Stevenson (2010)" startWordPosition="1636" endWordPosition="1639"> etc., represents a Transitive Action construction.2 Such constructions allow for some degree of generalization over the observed input; e.g., when seeing a novel verb in a Transitive utterance, the model predicts the similarity of this verb to other Action verbs appearing in that pattern (Alishahi and Stevenson, 2008). Grouping of verb usages into constructions may not be sufficient for making higher-level generalizations across verb alternations. Knowledge of alternations is only captured indirectly in constructions (because usages of the same verb can occur in multiple clusters). Following Parisien and Stevenson (2010), we hypothesize that true generalization behaviour requires explicit knowledge that verbs have commonalities in their patterns of occurrence across constructions; this is the basis 2Because the associations are probabilistic, a linguistic construction may be represented by more than one cluster. for verb classes (Levin, 1993; Merlo and Stevenson, 2000; Schulte im Walde and Brew, 2002). To capture this, our model learns groupings of verbs that have similar distributions across the learned constructions. These groupings form verb classes that provide a higher-level of abstraction over the input</context>
<context position="22105" citStr="Parisien and Stevenson (2010)" startWordPosition="3608" endWordPosition="3611"> the model, rather we must analyze the general trends of the construction-only and class-based results. 41 5 Evaluation In this section we examine whether and how our model generalizes across the two variants of the dative alternation, the double-object dative (DO) and the prepositional dative (PD). To do so, we measure the tendency of the model to produce a novel verb observed in one dative frame in that same frame, or in the other dative frame (unobserved for that verb). Our goal is to understand the impact of the learned constructions and classes on this generalization behaviour. Following Parisien and Stevenson (2010), we examine three input conditions in which the novel verb occurs: (i) twice with the DO syntax (non-alternating); (ii) twice with the PD syntax (non-alternating); or (iii) once each with DO and PD syntax (alternating).4 We then ask the model to predict the likelihood of producing each dative frame with that verb. Our focus here is on comparing the generalization abilities of the two levels of abstract knowledge in our model: the constructions versus the verb classes. As a reminder, we use the dative alternation as one example for considering this kind of higherlevel generalization behaviour </context>
<context position="33203" citStr="Parisien and Stevenson, 2010" startWordPosition="5414" endWordPosition="5417">similar distributional behaviour across the constructions. Specifically, we extend the model of AS08 by incrementally learning tokenbased verb classes that generalize over the construction knowledge level. In contrast to the models of Parisien and Stevenson and Perfors et al., our model is incremental, and hence enables the analysis of the monotonically developing classes to show the relation to the development of generalization ability in human learners. We analyze how generalization is supported by each level of learning in our model: constructions and verb classes. Our results confirm (cf. Parisien and Stevenson, 2010) that a higher-level knowledge of the verb classes is required to replicate the observed patterns of generalization, such as producing a novel verb gorp in the in the prepositional dative pattern after hearing it in the double object dative pattern. In addition, our analysis of the incrementally developing verb classes shows that the generalization knowledge gradually emerges over time, similar to what is observed in children. The flexibility of input representation of our model enables us to further explore the properties of the input in learning abstract knowledge, following psycholinguistic</context>
</contexts>
<marker>Parisien, Stevenson, 2010</marker>
<rawString>Christopher Parisien and Suzanne Stevenson. 2010. Learning verb alternations in a usagebased bayesian model. In Proceedings of the 32nd annual meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Perfors</author>
<author>Joshua B Tenenbaum</author>
<author>Elizabeth Wonnacott</author>
</authors>
<title>Variability, negative evidence, and the acquisition of verb argument constructions.</title>
<date>2010</date>
<journal>Journal of Child Language,</journal>
<volume>37</volume>
<issue>03</issue>
<contexts>
<context position="4510" citStr="Perfors et al., 2010" startWordPosition="680" endWordPosition="683">he 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 37–45, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics naturalistic input that contains many verbs and many constructions. The model can form higherlevel generalizations such as learning verb alternations, which is not possible with the AS08 model (cf. the findings of Parisien and Stevenson, 2010). Moreover, because our model gradually forms its representations of constructions and classes over time (in contrast to other Bayesian models, such as Parisien and Stevenson, 2010; Perfors et al., 2010), it is possible to analyze the monotonicallygrowing representations and show their compatibility with the developmental patterns seen in children (Conwell and Demuth, 2007). We also replicate some of the observations of Wonnacott et al. (2008) on the role of distributional properties of the language in influencing the degree of generalization over an alternation. 2 Related Work To explore the properties of learning mechanisms that are capable of mimicking child and adult psycholinguistic observations, a number of cognitive modeling studies have focused on learning abstract verb knowledge from</context>
</contexts>
<marker>Perfors, Tenenbaum, Wonnacott, 2010</marker>
<rawString>Amy Perfors, Joshua B. Tenenbaum, and Elizabeth Wonnacott. 2010. Variability, negative evidence, and the acquisition of verb argument constructions. Journal of Child Language, 37(03):607–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacqueline Sachs</author>
</authors>
<title>Talking about the There and Then: The emergence of displaced reference in parent–child discourse. Children’s language,</title>
<date>1983</date>
<pages>4</pages>
<contexts>
<context position="19299" citStr="Sachs (1983)" startWordPosition="3149" endWordPosition="3150">ed in our simulations have the distributional properties observed in CDS, but show some variation in precise make-up and ordering of verb usages. The generated input consists offrames (a set of features) that correspond to verb usages in CDS. 4.2 Simulations Because the generation of the input data is probabilistic, we conduct 100 simulations for each experiment (each using a different input corpus) to avoid any dependency on specific idiosyncratic properties of a single generated corpus. For each simulation, we train our model 3Brown (1973); Suppes (1974); Kuczaj (1977); Bloom et al. (1974); Sachs (1983); Lieven et al. (2009). on an automatically-generated corpus of 15, 000 frames, from which the model learns constructions and verb classes. At specified points in the input, we present the model with usages of a novel verb in a DO and/or PD frame, and then test the model’s generalization ability by predicting DO and PD frames given that verb. Since we are interested in the relative likelihoods of the two frames, we report the difference between the log-likelihood of the DO frame and the log-likelihood of the PD frame, i.e., log-likelihood(DO) − log-likelihood(PD). Specifically, we form a parti</context>
</contexts>
<marker>Sachs, 1983</marker>
<rawString>Jacqueline Sachs. 1983. Talking about the There and Then: The emergence of displaced reference in parent–child discourse. Children’s language, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Chris Brew</author>
</authors>
<title>Inducing German semantic verb classes from purely syntactic subcategorisation information.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>223--230</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="10880" citStr="Walde and Brew, 2002" startWordPosition="1693" endWordPosition="1696">g higher-level generalizations across verb alternations. Knowledge of alternations is only captured indirectly in constructions (because usages of the same verb can occur in multiple clusters). Following Parisien and Stevenson (2010), we hypothesize that true generalization behaviour requires explicit knowledge that verbs have commonalities in their patterns of occurrence across constructions; this is the basis 2Because the associations are probabilistic, a linguistic construction may be represented by more than one cluster. for verb classes (Levin, 1993; Merlo and Stevenson, 2000; Schulte im Walde and Brew, 2002). To capture this, our model learns groupings of verbs that have similar distributions across the learned constructions. These groupings form verb classes that provide a higher-level of abstraction over the input; see the top level in Figure 1. Consider the dative alternation: the classes capture the fact that some verbs may occur only in prepositional dative (PD) forms, such as sing, while others occur only in double object (DO) forms (call), while still others alternate – i.e., they occur in both (bring). Our model simultaneously learns both of these types of knowledge: constructions are clu</context>
</contexts>
<marker>Walde, Brew, 2002</marker>
<rawString>Sabine Schulte im Walde and Chris Brew. 2002. Inducing German semantic verb classes from purely syntactic subcategorisation information. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 223–230, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rose M Scott</author>
<author>Cynthia Fisher</author>
</authors>
<title>Twoyear-olds use distributional cues to interpret transitivity-alternating verbs. Language and cognitive processes,</title>
<date>2009</date>
<pages>24--6</pages>
<marker>Scott, Fisher, 2009</marker>
<rawString>Rose M Scott and Cynthia Fisher. 2009. Twoyear-olds use distributional cues to interpret transitivity-alternating verbs. Language and cognitive processes, 24(6):777–803.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Suppes</author>
</authors>
<title>The semantics of children’s language.</title>
<date>1974</date>
<journal>American psychologist,</journal>
<pages>29--2</pages>
<contexts>
<context position="19249" citStr="Suppes (1974)" startWordPosition="3141" endWordPosition="3142">ording to the lexicon, so that all input corpora used in our simulations have the distributional properties observed in CDS, but show some variation in precise make-up and ordering of verb usages. The generated input consists offrames (a set of features) that correspond to verb usages in CDS. 4.2 Simulations Because the generation of the input data is probabilistic, we conduct 100 simulations for each experiment (each using a different input corpus) to avoid any dependency on specific idiosyncratic properties of a single generated corpus. For each simulation, we train our model 3Brown (1973); Suppes (1974); Kuczaj (1977); Bloom et al. (1974); Sachs (1983); Lieven et al. (2009). on an automatically-generated corpus of 15, 000 frames, from which the model learns constructions and verb classes. At specified points in the input, we present the model with usages of a novel verb in a DO and/or PD frame, and then test the model’s generalization ability by predicting DO and PD frames given that verb. Since we are interested in the relative likelihoods of the two frames, we report the difference between the log-likelihood of the DO frame and the log-likelihood of the PD frame, i.e., log-likelihood(DO) −</context>
</contexts>
<marker>Suppes, 1974</marker>
<rawString>Patrick Suppes. 1974. The semantics of children’s language. American psychologist, 29(2):103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>Constructing a language: A usage-based theory of language acquisition.</title>
<date>2003</date>
<contexts>
<context position="1169" citStr="Tomasello, 2003" startWordPosition="159" endWordPosition="160">ductivity. Computational models can help shed light on the gradual development of more abstract knowledge during verb acquisition. We present an incremental Bayesian model that simultaneously and incrementally learns argument structure constructions and verb classes given naturalistic language input. We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes. 1 Introduction Usage-based accounts of language learning note that young children rely on verb-specific knowledge to produce their early utterances (e.g., Tomasello, 2003). However, evidence suggests that even young children can generalize their verb knowledge to novel verbs and syntactic frames (e.g., Fisher, 2002), and that the abstract knowledge gradually strengthens over time (e.g., Tomasello and Abbot-Smith, 2002). One area of verb usage where more sophisticated abstraction appears necessary for fully adult productivity in language is the knowledge of verb alternations. A verb alternation is a pairing of constructions shared by a number of verbs, in which the two constructions express related argument structures (Levin, 1993): e.g., the dative alternation </context>
<context position="31570" citStr="Tomasello (2003)" startWordPosition="5166" endWordPosition="5167">igure 3: Difference of log-likelihood values of the DO and PD frames over the course of training for the constructions and the verb classes for each of the 3 test scenarios. Values above zero denote a higher likelihood for the DO frame, and values below zero denote a higher likelihood for the PD frame. DO frame and the higher association of PD frames with DO alternates. These results again emphasize the importance of further exploring the role of distributional factors on generalization of verb knowledge in children. The developmental results presented here are in line with the suggestions of Tomasello (2003) that the productions of younger children follow observed patterns in the input, and only later reflect robust generalizations of their knowledge across verbs. Conwell and Demuth (2007) for example, found evidence of generalization across verb alternations in 3-year-old children, but their production of unobserved forms for a novel verb was very sensitive to the precise context of the experiment and the distributional patterns across the novel verbs. In accord with these observations, the developmental trajectories in our model show that our class-based predictions increase in their degree of </context>
</contexts>
<marker>Tomasello, 2003</marker>
<rawString>Michael Tomasello. 2003. Constructing a language: A usage-based theory of language acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
<author>Kirsten Abbot-Smith</author>
</authors>
<title>A tale of two theories: Response to Fisher.</title>
<date>2002</date>
<journal>Cognition,</journal>
<volume>83</volume>
<issue>2</issue>
<contexts>
<context position="1420" citStr="Tomasello and Abbot-Smith, 2002" startWordPosition="192" endWordPosition="195"> constructions and verb classes given naturalistic language input. We show how the distributional properties in the input language influence the formation of generalizations over the constructions and classes. 1 Introduction Usage-based accounts of language learning note that young children rely on verb-specific knowledge to produce their early utterances (e.g., Tomasello, 2003). However, evidence suggests that even young children can generalize their verb knowledge to novel verbs and syntactic frames (e.g., Fisher, 2002), and that the abstract knowledge gradually strengthens over time (e.g., Tomasello and Abbot-Smith, 2002). One area of verb usage where more sophisticated abstraction appears necessary for fully adult productivity in language is the knowledge of verb alternations. A verb alternation is a pairing of constructions shared by a number of verbs, in which the two constructions express related argument structures (Levin, 1993): e.g., the dative alternation involves the related forms of the prepositional dative (PD; X gave Yto Z) and the double-object dative (DO; X gave Z Y). Such alternations enable language users to readily adapt new and low frequency verbs to appropriate constructions of the language </context>
</contexts>
<marker>Tomasello, Abbot-Smith, 2002</marker>
<rawString>Michael Tomasello and Kirsten Abbot-Smith. 2002. A tale of two theories: Response to Fisher. Cognition, 83(2):207–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Wonnacott</author>
<author>Elissa L Newport</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Acquiring and processing verb argument structure: Distributional learning in a miniature language. Cognitive psychology,</title>
<date>2008</date>
<pages>56--3</pages>
<contexts>
<context position="2429" citStr="Wonnacott et al. (2008)" startWordPosition="355" endWordPosition="358">positional dative (PD; X gave Yto Z) and the double-object dative (DO; X gave Z Y). Such alternations enable language users to readily adapt new and low frequency verbs to appropriate constructions of the language by generalizing the observed use of one such form to the other.1 For example, Conwell and Demuth (2007) show that 3-year-old children understand that a novel verb observed only in the DO dative (John gorped Heather the book) can also be used in the PD form (John gorped the book to Heather), though the children can only generalize such knowledge under certain experimental conditions. Wonnacott et al. (2008) demonstrate the proficiency of adults in making such generalizations within an artificial language learning scenario, which enables the researchers to explore the distributional properties of the linguistic input that facilitate the acquisition of such generalizations. The results suggest that the overall frequency of the syntactic patterns as well as the distribution of verbs across the patterns play a facilitatory role in the formation of abstract verb knowledge (in the form of verb alternations) in adult language learners. In this work, we propose a computational model that extends an exis</context>
<context position="4754" citStr="Wonnacott et al. (2008)" startWordPosition="719" endWordPosition="722"> The model can form higherlevel generalizations such as learning verb alternations, which is not possible with the AS08 model (cf. the findings of Parisien and Stevenson, 2010). Moreover, because our model gradually forms its representations of constructions and classes over time (in contrast to other Bayesian models, such as Parisien and Stevenson, 2010; Perfors et al., 2010), it is possible to analyze the monotonicallygrowing representations and show their compatibility with the developmental patterns seen in children (Conwell and Demuth, 2007). We also replicate some of the observations of Wonnacott et al. (2008) on the role of distributional properties of the language in influencing the degree of generalization over an alternation. 2 Related Work To explore the properties of learning mechanisms that are capable of mimicking child and adult psycholinguistic observations, a number of cognitive modeling studies have focused on learning abstract verb knowledge from individual verb usages (e.g., Alishahi and Stevenson, 2008; Perfors et al., 2010; Parisien and Stevenson, 2010). Here we focus on such computational models that enable the sort of higher-level generalization that people do across verb alternat</context>
<context position="9011" citStr="Wonnacott et al., 2008" startWordPosition="1403" endWordPosition="1406">om a sequence of frames, where each frame is a collection of features representing what the learner might extract from an utterance s/he has heard. Similarly to previous computational studies (e.g., Parisien and Stevenson, 2010), here we focus on syntactic features since our goal is to understand the acquisition of acceptable syntactic structures of verbs indepen38 Figure 1: A visual representation of the two levels of abstraction in the model, with sample verb usages input (and extracted input frames), constructions, and classes. dently of their meaning, as in some relevant psycholinguistic (Wonnacott et al., 2008) and computational studies (Parisien and Stevenson, 2010). We focus particularly on properties such as syntactic slots and argument count. (These features, as in Parisien and Stevenson (2010), provide a more flexible and generalizable representation of a syntactic structure than the syntactic pattern string used by AS08.) See the bottom rows of boxes in Figure 1 for sample input verb usages with their extracted frames. The model incrementally clusters the extracted input frames into constructions that reflect probabilistic associations of the features across similar verb usages; see the middle</context>
<context position="27870" citStr="Wonnacott et al. (2008)" startWordPosition="4566" endWordPosition="4569">our classes encode the distribution (roughly relative frequency) of the verbs in the class occurring across the different constructions. This means that in our class-based predictions, greater weight will be given to constructions with DO when observing a PD frame than to constructions with PD when observing a DO frame. These results underline the importance of using naturalistic input and considering the impact of various distributional factors on generalization of verb knowledge. In contrast to the construction-based results, our class-based results conform with the experimental findings of Wonnacott et al. (2008), who show that adult (artificial) language learners robustly generalize a newly-learned verb observed in a single syntactic form by producing it in the alternating syntactic form under certain language conditions. Moreover, we show similar distributional effects to theirs – the overall frequency of the syntactic patterns, as well as the distribution of verbs across those patterns – in the level of preference for one form over another, within the context of our naturalistic data with multiple verbs, constructions, and alternations. These results show that the verb classes in the model are able</context>
</contexts>
<marker>Wonnacott, Newport, Tanenhaus, 2008</marker>
<rawString>Elizabeth Wonnacott, Elissa L Newport, and Michael K Tanenhaus. 2008. Acquiring and processing verb argument structure: Distributional learning in a miniature language. Cognitive psychology, 56(3):165–209.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002563">
<title confidence="0.865113">
LIMSI Submission for WMT’14 QE Task
</title>
<author confidence="0.713554">
Guillaume Wisniewski and Nicolas P´echeux and Alexandre Allauzen and Franc¸ois Yvon
</author>
<affiliation confidence="0.38644">
Universit´e Paris Sud and LIMSI-CNRS
91403 ORSAY CEDEX, France
</affiliation>
<email confidence="0.928164">
{wisniews, pecheux, allauzen, yvon}@limsi.fr
</email>
<sectionHeader confidence="0.99274" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901285714286">
This paper describes LIMSI participation
to the WMT’14 Shared Task on Qual-
ity Estimation; we took part to the word-
level quality estimation task for English
to Spanish translations. Our system re-
lies on a random forest classifier, an en-
semble method that has been shown to
be very competitive for this kind of task,
when only a few dense and continuous fea-
tures are used. Notably, only 16 features
are used in our experiments. These fea-
tures describe, on the one hand, the qual-
ity of the association between the source
sentence and each target word and, on the
other hand, the fluency of the hypothe-
sis. Since the evaluation criterion is the
fl measure, a specific tuning strategy is
proposed to select the optimal values for
the hyper-parameters. Overall, our system
achieves a 0.67 fl score on a randomly ex-
tracted test set.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932666666667">
This paper describes LIMSI submission to the
WMT’14 Shared Task on Quality Estimation. We
participated in the word-level quality estimation
task (Task 2) for the English to Spanish direction.
This task consists in predicting, for each word in
a translation hypothesis, whether this word should
be post-edited or should rather be kept unchanged.
Predicting translation quality at the word level
raises several interesting challenges. First, this is
a (relatively) new task and the best way to for-
mulate and evaluate it has still to be established.
Second, as most works on quality estimation have
only considered prediction at the sentence level, it
is not clear yet which features are really effective
to predict quality at the word and a set of base-
line features has still to be found. Finally, sev-
eral characteristic of the task (the limited number
of training examples, the unbalanced classes, etc.)
makes the use of ‘traditional’ machine learning al-
gorithms difficult. This papers describes how we
addressed this different issues for our participation
to the WMT’14 Shared Task.
The rest of this paper is organized as follows.
Section 2 gives an overview of the shared task data
that will justify some of the design decisions we
made. Section 3 describes the different features
we have considered and Section 4, the learning
methods used to estimate the classifiers parame-
ters. Finally the results of our models are pre-
sented and analyzed in Section 5.
</bodyText>
<sectionHeader confidence="0.94719" genericHeader="method">
2 World-Level Quality Estimation
</sectionHeader>
<bodyText confidence="0.9999575">
WMT’14 shared task on quality estimation num-
ber 2 consists in predicting, for each word of a
translation hypothesis, whether this word should
be post-edited (denoted by the BAD label) or
should be kept unchanged (denoted by the OK la-
bel). The shared task organizers provide a bilin-
gual dataset from English to Spanish1 made of
translations produced by three different MT sys-
tems and by one human translator; these transla-
tions have then been annotated with word-level la-
bels by professional translators. No additional in-
formation about the systems used, the derivation
of the translation (such as the lattices or the align-
ment between the source and the best translation
hypothesis) or the tokenization applied to identify
words is provided.
The distributions of the two labels for the dif-
ferent systems is displayed in Table 1. As it
could be expected, the class are, overall, unbal-
anced and the systems are of very different qual-
ity: the proportion of BAD and OK labels highly
depends on the system used to produce the transla-
tion hypotheses. However, as our preliminary ex-
periments have shown, the number of examples is
</bodyText>
<footnote confidence="0.975378">
1We did not consider the other language pairs.
</footnote>
<page confidence="0.93777">
348
</page>
<bodyText confidence="0.928541947368421">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 348–354,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
too small to train a different confidence estimation
system for each system.
The distribution of the number of BAD labels
per sentence is very skewed: on average, one word
out of three (precisely 35.04%) in a sentence is la-
beled as BAD but the median of the distribution of
the ratio of word labeled BAD in a sentence is 20%
and its standard deviation is pretty high (34.75%).
Several sentences have all their words labeled as
either OK or BAD, which is quite surprising as the
sentences of the corpus for Task 2 have been se-
lected because there were ‘near miss translations’
that is to say translations that should have con-
tained no more that 2 or 3 errors.
Another interesting finding is that the propor-
tion of word to post-edit is the same across the
different parts-of-speech (see Table 2).2
</bodyText>
<tableCaption confidence="0.9902175">
Table 1: Number of examples and distribution of
labels for the different systems on the training set
</tableCaption>
<table confidence="0.933753166666667">
System #sent. #words % OK % BAD
1 791 19,456 75.48 24.52
2 621 14,620 59.11 40.89
3 454 11,012 59.76 40.24
4 90 2,296 36.85 63.15
Total 1,956 47,384 64.90 35.10
</table>
<tableCaption confidence="0.9861075">
Table 2: Distribution of labels according to the
POS on the training set
</tableCaption>
<table confidence="0.9981414">
POS % in train % BAD
NOUN 23.81 35.02
ADP 15.06 35.48
DET 14.90 32.88
VERB 14.64 41.26
PUNCT 10.92 27.26
ADJ 6.61 35.68
CONJ 5.04 30.77
PRON 4.58 43.15
ADV 4.39 36.56
</table>
<bodyText confidence="0.998413">
As the classes are unbalanced, prediction per-
formance will be evaluated in terms of precision,
recall and f1 score computed on the BAD label.
More precisely, if the number of true positive (i.e.
</bodyText>
<footnote confidence="0.9978344">
2We used FreeLing (http:nlp.lsi.upc.edu/
freeling/) to predict the POS tags of the translation
hypotheses and, for the sake of clarity, mapped the 71 tags
used by FreeLing to the 11 universal POS tags of Petrov et
al. (2012).
</footnote>
<bodyText confidence="0.973111166666667">
BAD word predicted as BAD), false positive (OK
word predicted as BAD) and false negative (BAD
word predicted as OK) are denoted tpBAD, fpBAD
and fnBAD, respectively, the quality of a confidence
estimation system is evaluated by the three follow-
ing metrics:
</bodyText>
<equation confidence="0.9991998">
tpBAD (1)
tpBAD (2)
2 · pBAD · rBAD
f1 = (3)
pBAD + rBAD
</equation>
<sectionHeader confidence="0.998054" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999818666666667">
In our experiments, we used 16 features to de-
scribe a given target word ti in a translation hy-
pothesis t = (tj)mj=1. To avoid sparsity issues we
decided not to include any lexicalized information
such as the word or the previous word identities.
As the translation hypotheses were generated by
different MT systems, no white-box features (such
as word alignment or model scores) are consid-
ered. Our features can be organized in two broad
categories:
Association Features These features measure
the quality of the ‘association’ between the source
sentence and a target word: they characterize the
probability for a target word to appear in a transla-
tion of the source sentence. Two kinds of associa-
tion features can be distinguished.
The first one is derived from the lexicalized
probabilities p(t|s) that estimate the probability
that a source word s is translated by the target
word tj. These probabilities are aggregated using
an arithmetic mean:
</bodyText>
<equation confidence="0.988731">
p(tj|si) (4)
</equation>
<bodyText confidence="0.999934333333333">
where s = (si)ni=1 is the source sentence (with an
extra NULL token). We assume that p(tj|si) = 0 if
the words tj and si have never been aligned in the
train set and also consider the geometric mean of
the lexicalized probabilities, their maximum value
(i.e. maxs∈s p(tj|s)) as well as a binary feature
that fires when the target word tj is not in the lex-
icalized probabilities table.
The second kind of association features relies
on pseudo-references, that is to say, translations
of the source sentence produced by an indepen-
dent MT system. Many works have considered
</bodyText>
<equation confidence="0.999744625">
pBAD =
rBAD =
tpBAD + fpBAD
tpBAD + fnBAD
1
p(tj|s) = n
n
i=1
</equation>
<page confidence="0.990248">
349
</page>
<bodyText confidence="0.999986311111111">
pseudo-references to design new MT metrics (Al-
brecht and Hwa, 2007; Albrecht and Hwa, 2008)
or for confidence estimation (Soricut and Echi-
habi, 2010; Soricut and Narsale, 2012) but, to the
best of our knowledge, this is the first time that
they are used to predict confidence at the word
level.
Pseudo-references are used to define 3 binary
features which fire if the target word is in the
pseudo-reference, in a 2-gram shared between the
pseudo-reference and the translation hypothesis or
in a common 3-gram, respectively. The lattices
representing the search space considered to gen-
erate these pseudo-references also allow us to es-
timate the posterior probability of a target word
that quantifies the probability that it is part of the
system output (Gispert et al., 2013). Posteriors ag-
gregate two pieces of information for each word in
the final hypothesis: first, all the paths in the lat-
tice (i.e. the number of translation hypotheses in
the search space) where the word appears in are
considered; second, the decoder scores of these
paths are accumulated in order to derive a confi-
dence measure at the word level. In our experi-
ments, we considered pseudo-references and lat-
tices produced by the n-gram based system de-
veloped by our team for last year WMT evalu-
ation campaign (Allauzen et al., 2013), that has
achieved very good performance.
Fluency Features These features measure the
‘fluency’ of the target sentence and are based on
different language models: a ‘traditional’ 4-gram
language model estimated on WMT monolingual
and bilingual data (the language model used by
our system to generate the pseudo-references); a
continuous-space 10-gram language model esti-
mated with SOUL (Le et al., 2011) (also used by
our MT system) and a 4-gram language model
based on Part-of-Speech sequences. The latter
model was estimated on the Spanish side of the
bilingual data provided in the translation shared
task in 2013. These data were POS-tagged with
FreeLing (Padr´o and Stanilovsky, 2012).
All these language models have been used to de-
fine two different features :
</bodyText>
<listItem confidence="0.5797776">
• the probability of the word of interest p(tj|h)
where h = tj−1, ..., tj−n+1 is the history
made of the n − 1 previous words or POS
• the ratio between the probability of
the sentence and the ‘best’ probabil-
</listItem>
<bodyText confidence="0.996021035714286">
ity that can be achieved if the target
word is replaced by any other word (i.e.
maxvEV p(t1, ..., tj−1, v, tj+1, ..., tm) where
the max runs over all the words of the
vocabulary).
There is also a feature that describes the back-off
behavior of the conventional language model: its
value is the size of the largest n-gram of the trans-
lation hypothesis that can be estimated by the lan-
guage model without relying on back-off probabil-
ities.
Finally, there is a feature describing, for each
word that appears more than once in the train set,
the probability that this word is labeled BAD. This
probability is simply estimated by the ratio be-
tween the number of times this word is labeled
BAD and the number of occurrences of this word.
It must be noted that most of the features we
consider rely on models that are part of a ‘clas-
sic’ MT system. However their use for predicting
translation quality at the word-level is not straight-
forward, as they need to be applied to sentences
with a given unknown tokenization. Matching the
tokenization used to estimate the model to the one
used for collecting the annotations is a tedious and
error-prone process and some of the prediction er-
rors most probably result from mismatches in tok-
enization.
</bodyText>
<sectionHeader confidence="0.999637" genericHeader="method">
4 Learning Methods
</sectionHeader>
<subsectionHeader confidence="0.918853">
4.1 Classifiers
</subsectionHeader>
<bodyText confidence="0.999165176470588">
Predicting whether a word in a translation hypoth-
esis should be post-edited or not can naturally be
framed as a binary classification task. Based on
our experiments in previous campaigns (Singh et
al., 2013; Zhuang et al., 2012), we considered ran-
dom forest in all our experiments.3
Random forest (Breiman, 2001) is an ensem-
ble method that learns many classification trees
and predicts an aggregation of their result (for in-
stance by majority voting). In contrast with stan-
dard decision trees, in which each node is split
using the best split among all features, in a ran-
dom forest the split is chosen randomly. In spite
of this simple and counter-intuitive learning strat-
egy, random forests have proven to be very good
‘out-of-the-box’ learners. Random forests have
achieved very good performance in many similar
</bodyText>
<footnote confidence="0.9698855">
3we have used the implementation provided by
scikit-learn (Pedregosa et al., 2011).
</footnote>
<page confidence="0.997438">
350
</page>
<bodyText confidence="0.999920722222222">
tasks (Chapelle and Chang, 2011), in which only
a few dense and continuous features are available,
possibly because of their ability to take into ac-
count complex interactions between features and
to automatically partition the continuous features
value into a discrete set of intervals that achieves
the best classification performance.
As a baseline, we consider logistic regres-
sion (Hastie et al., 2003), a simple linear model
where the parameters are estimated by maximiz-
ing the likelihood of the training set.
These two classifiers do not produce only a class
decision but yield an instance probability that rep-
resents the degree to which an instance is a mem-
ber of a class. As detailed in the next section,
thresholding this probability will allow us to di-
rectly optimize the f1 score used to evaluate pre-
diction performance.
</bodyText>
<subsectionHeader confidence="0.979014">
4.2 Optimizing the f1 Score
</subsectionHeader>
<bodyText confidence="0.999951652173913">
As explained in Section 2, quality prediction will
be evaluated in terms of f1 score. The learn-
ing methods we consider can not, as most learn-
ing method, directly optimize the f1 measure dur-
ing training, since this metric does not decompose
over the examples. It is however possible to take
advantage of the fact that they actually estimate a
probability to find the largest f1 score on the train-
ing set.
Indeed these probabilities are used with a
threshold (usually 0.5) to produce a discrete (bi-
nary) decision: if the probability is above the
threshold, the classifier produces a positive out-
put, and otherwise, a negative one. Each thresh-
old value produces a different trade-off between
true positives and false positives and consequently
between recall and precision: as the the threshold
becomes lower and lower, more and more exam-
ple are assigned to the positive class and recall in-
crease at the expense of precision.
Based on these observations, we propose the
following three-step method to optimize the f1
score on the training set:
</bodyText>
<listItem confidence="0.974122142857143">
1. the classifier is first trained using the ‘stan-
dard’ learning procedure that optimizes either
the 0/1 loss (for random forest) or the likeli-
hood (for the logistic regression);
2. all the possible trade-offs between recall
and precision are enumerated by varying
the threshold; exploiting the monotonicity of
</listItem>
<bodyText confidence="0.5742485">
thresholded classifications,4 this enumeration
can be efficiently done in O (n · log n) and
results in at most n threshold values, where n
is the size of the training set (Fawcett, 2003);
</bodyText>
<listItem confidence="0.9887265">
3. all the f1 scores achieved for the different
thresholds found in the previous step are eval-
uated; there are strong theoretical guaran-
tees that the optimal f1 score that can be
achieved on the training set is one of these
values (Boyd and Vandenberghe, 2004).
</listItem>
<figureCaption confidence="0.986135142857143">
Figure 1 shows how f1 score varies with the deci-
sion threshold and allows to assess the difference
between the optimal value of the threshold and its
default value (0.5).
Figure 1: Evolution of the f1 score with respect to
the threshold used to transform probabilities into
binary decisions
</figureCaption>
<sectionHeader confidence="0.998549" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999799">
The features and learning strategies described in
the two previous sections were evaluated on the
English to Spanish datasets. As no official devel-
opment set was provided by the shared task orga-
nizers, we randomly sampled 200 sentences from
the training set and use them as a test set through-
out the rest of this article. Preliminary experiments
show that the choice of this test has a very low im-
pact on the classification performance. The dif-
ferent hyper-parameters of the training algorithm
</bodyText>
<footnote confidence="0.970654333333333">
4Any instance that is classified positive with respect to a
given threshold will be classified positive for all lower thresh-
olds as well.
</footnote>
<page confidence="0.997712">
351
</page>
<tableCaption confidence="0.997646333333333">
Table 3: Prediction performance for the two learn-
ing strategies considered
Table 4: Prediction performance for each POS tag
</tableCaption>
<table confidence="0.968795333333333">
Classifier thres. rBAD pBAD fl
Random forest 0.43 0.64 0.69 0.67
Logistic regression 0.27 0.51 0.72 0.59
</table>
<bodyText confidence="0.999173720930232">
were chosen by maximizing classification perfor-
mance (as evaluated by the fi score) estimated on
150 sentences of the training set kept apart as a
validation set.
Results for the different learning algorithms
considered are presented in Table 3. Random for-
est clearly outperforms a simple logistic regres-
sion, which shows the importance of using non-
linear decision functions, a conclusion at pair with
our previous results (Zhuang et al., 2012; Singh et
al., 2013).
The overall performance, with a fl measure of
0.67, is pretty low and in our opinion, not good
enough to consider using such a quality estimation
system in a computer-assisted post-edition con-
text. However, as shown in Table 4, the prediction
performance highly depends on the POS category
of the words: it is quite good for ‘plain’ words
(like verb and nouns) but much worse for other
categories.
There are two possible explanations for this
observation: predicting the correctness of some
morpho-syntaxic categories may be intrinsically
harder (e.g. for punctuation the choice of which
can be highly controversial) or depend on infor-
mation that is not currently available to our sys-
tem. In particular, we do not consider any in-
formation about the structure of the sentence and
about the labels of the context, which may explain
why our system does not perform well in predict-
ing the labels of determiners and conjunctions. In
both cases, this result brings us to moderate our
previous conclusions: as a wrong punctuation sign
has not the same impact on translation quality as a
wrong verb, our system might, regardless of its fl
score, be able to provide useful information about
the quality of a translation. This also suggests that
we should look for a more ‘task-oriented’ metric.
Finally, Figure 2 displays the importance of the
different features used in our system. Random
forests deliver a quantification of the importance
of a feature with respect to the predictability of the
target variable. This quantification is derived from
</bodyText>
<table confidence="0.996815454545455">
System fi
VERB 0.73
PRON 0.72
ADJ 0.70
NOUN 0.69
ADV 0.69
overall 0.67
DET 0.62
ADP 0.61
CONJ 0.57
PUNCT 0.56
</table>
<bodyText confidence="0.999899214285714">
the position of a feature in a decision tree: fea-
tures used in the top nodes of the trees, which con-
tribute to the final prediction decision of a larger
fraction of the input samples, play a more impor-
tant role than features used near the leaves of the
tree. It appears that, as for our previous experi-
ments (Wisniewski et al., 2013), the most relevant
feature for predicting translation quality is the fea-
ture derived from the SOUL language model, even
if other fluency features seem to also play an im-
portant role. Surprisingly enough, features related
to the pseudo-reference do not seem to be useful.
Further experiments are needed to explain the rea-
sons of this observation.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999374444444444">
In this paper we described the system submitted
for Task 2 of WMT’14 Shared Task on Quality
Estimation. Our system relies on a binary clas-
sifier and consider only a few dense and contin-
uous features. While the overall performance is
pretty low, a fine-grained analysis of the errors of
our system shows that it can predict the quality of
plain words pretty accurately which indicates that
a more ‘task-oriented’ evaluation may be needed.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996375">
This work was partly supported by ANR project
Transread (ANR-12-CORD-0015). Warm thanks
to Quoc Khanh Do for his help for training a SOUL
model for Spanish.
</bodyText>
<sectionHeader confidence="0.989621" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.704448">
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level mt evaluation with pseudo refer-
</bodyText>
<page confidence="0.981653">
352
</page>
<figure confidence="0.999084">
Feature
diffMaxLM
geomIBM1
maxMatchingNGramSize
pseudoRefCommon2gram
pseudoRefCommon1gram
wordPosterior
bestAl
arithIBM1
posLM
tradiLM
maxLMScore
priorProba
soulLM
pseudoRefCommon3gram
wordNotInSearchSpace
notInIBM1Table
0 0.02 0.04 0.06 0.08 0.1 0.12 0.14
Feature Importance
</figure>
<figureCaption confidence="0.996404">
Figure 2: Features considered by our system sorted by their relevance for predicting translation errors
</figureCaption>
<reference confidence="0.998679545454546">
ences. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
296–303, Prague, Czech Republic, June. ACL.
Joshua Albrecht and Rebecca Hwa. 2008. The role
of pseudo references in MT evaluation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 187–190, Columbus, Ohio, June.
ACL.
Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur´elien
Max, Hai-Son Le, and Franc¸ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 62–
69, Sofia, Bulgaria, August. ACL.
Stephen Boyd and Lieven Vandenberghe. 2004. Con-
vex Optimization. Cambridge University Press, New
York, NY, USA.
Leo Breiman. 2001. Random forests. Mach. Learn.,
45(1):5–32, October.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learn-
ing to rank challenge overview. In Olivier Chapelle,
Yi Chang, and Tie-Yan Liu, editors, Yahoo! Learn-
ing to Rank Challenge, volume 14 of JMLR Pro-
ceedings, pages 1–24. JMLR.org.
Tom Fawcett. 2003. ROC Graphs: Notes and Practical
Considerations for Researchers. Technical Report
HPL-2003-4, HP Laboratories, Palo Alto.
Adri`a Gispert, Graeme Blackwood, Gonzalo Iglesias,
and William Byrne. 2013. N-gram posterior prob-
ability confidence measures for statistical machine
translation: an empirical study. Machine Transla-
tion, 27(2):85–114.
Trevor Hastie, Robert Tibshirani, and Jerome H. Fried-
man. 2003. The Elements of Statistical Learning.
Springer, July.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc¸ois Yvon. 2011. Structured
output layer neural network language model. In
Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, pages
5524–5527. IEEE.
Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Anil Kumar Singh, Guillaume Wisniewski, and
Franc¸ois Yvon. 2013. LIMSI submission for
the WMT’13 quality estimation task: an experi-
ment with n-gram posteriors. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 398–404, Sofia, Bulgaria, August. ACL.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
</reference>
<page confidence="0.990158">
353
</page>
<reference confidence="0.999522888888889">
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612–621, Uppsala, Sweden, July.
ACL.
Radu Soricut and Sushant Narsale. 2012. Combining
quality prediction and system selection for improved
automatic translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 163–170, Montr´eal, Canada, June. ACL.
Guillaume Wisniewski, Anil Kumar Singh, and
Franc¸ois Yvon. 2013. Quality estimation for ma-
chine translation: Some lessons learned. Machine
Translation, 27(3).
Yong Zhuang, Guillaume Wisniewski, and Franc¸ois
Yvon. 2012. Non-linear models for confidence es-
timation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 157–162,
Montr´eal, Canada, June. ACL.
</reference>
<page confidence="0.999149">
354
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728847">
<title confidence="0.99208">LIMSI Submission for WMT’14 QE Task</title>
<author confidence="0.977451">Wisniewski P´echeux Allauzen</author>
<affiliation confidence="0.962989">Universit´e Paris Sud and</affiliation>
<address confidence="0.934855">91403 ORSAY CEDEX,</address>
<email confidence="0.824401">pecheux,allauzen,</email>
<abstract confidence="0.996765136363636">This paper describes LIMSI participation to the WMT’14 Shared Task on Quality Estimation; we took part to the wordlevel quality estimation task for English to Spanish translations. Our system relies on a random forest classifier, an ensemble method that has been shown to be very competitive for this kind of task, when only a few dense and continuous feaare used. Notably, only are used in our experiments. These features describe, on the one hand, the quality of the association between the source sentence and each target word and, on the other hand, the fluency of the hypothesis. Since the evaluation criterion is the a specific tuning strategy is proposed to select the optimal values for the hyper-parameters. Overall, our system a 0.67 on a randomly extracted test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ences</author>
</authors>
<date></date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>296--303</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<marker>ences, </marker>
<rawString>ences. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 296–303, Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>The role of pseudo references in MT evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>187--190</pages>
<publisher>ACL.</publisher>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7653" citStr="Albrecht and Hwa, 2008" startWordPosition="1291" endWordPosition="1294">words tj and si have never been aligned in the train set and also consider the geometric mean of the lexicalized probabilities, their maximum value (i.e. maxs∈s p(tj|s)) as well as a binary feature that fires when the target word tj is not in the lexicalized probabilities table. The second kind of association features relies on pseudo-references, that is to say, translations of the source sentence produced by an independent MT system. Many works have considered pBAD = rBAD = tpBAD + fpBAD tpBAD + fnBAD 1 p(tj|s) = n n i=1 349 pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that qua</context>
</contexts>
<marker>Albrecht, Hwa, 2008</marker>
<rawString>Joshua Albrecht and Rebecca Hwa. 2008. The role of pseudo references in MT evaluation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 187–190, Columbus, Ohio, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Allauzen</author>
<author>Nicolas P´echeux</author>
<author>Quoc Khanh Do</author>
<author>Marco Dinarelli</author>
<author>Thomas Lavergne</author>
<author>Aur´elien Max</author>
<author>Hai-Son Le</author>
<author>Franc¸ois Yvon</author>
</authors>
<date>2013</date>
<booktitle>LIMSI @ WMT13. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>62--69</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria,</location>
<marker>Allauzen, P´echeux, Do, Dinarelli, Lavergne, Max, Le, Yvon, 2013</marker>
<rawString>Alexandre Allauzen, Nicolas P´echeux, Quoc Khanh Do, Marco Dinarelli, Thomas Lavergne, Aur´elien Max, Hai-Son Le, and Franc¸ois Yvon. 2013. LIMSI @ WMT13. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62– 69, Sofia, Bulgaria, August. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boyd</author>
<author>Lieven Vandenberghe</author>
</authors>
<title>Convex Optimization.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="14676" citStr="Boyd and Vandenberghe, 2004" startWordPosition="2470" endWordPosition="2473">r the likelihood (for the logistic regression); 2. all the possible trade-offs between recall and precision are enumerated by varying the threshold; exploiting the monotonicity of thresholded classifications,4 this enumeration can be efficiently done in O (n · log n) and results in at most n threshold values, where n is the size of the training set (Fawcett, 2003); 3. all the f1 scores achieved for the different thresholds found in the previous step are evaluated; there are strong theoretical guarantees that the optimal f1 score that can be achieved on the training set is one of these values (Boyd and Vandenberghe, 2004). Figure 1 shows how f1 score varies with the decision threshold and allows to assess the difference between the optimal value of the threshold and its default value (0.5). Figure 1: Evolution of the f1 score with respect to the threshold used to transform probabilities into binary decisions 5 Experiments The features and learning strategies described in the two previous sections were evaluated on the English to Spanish datasets. As no official development set was provided by the shared task organizers, we randomly sampled 200 sentences from the training set and use them as a test set througho</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<journal>Mach. Learn.,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="11427" citStr="Breiman, 2001" startWordPosition="1933" endWordPosition="1934">tences with a given unknown tokenization. Matching the tokenization used to estimate the model to the one used for collecting the annotations is a tedious and error-prone process and some of the prediction errors most probably result from mismatches in tokenization. 4 Learning Methods 4.1 Classifiers Predicting whether a word in a translation hypothesis should be post-edited or not can naturally be framed as a binary classification task. Based on our experiments in previous campaigns (Singh et al., 2013; Zhuang et al., 2012), we considered random forest in all our experiments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many similar 3we have used the implementation provided by scikit-learn (Pedregosa et al., 2011). 350 tasks (Cha</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Mach. Learn., 45(1):5–32, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Chapelle</author>
<author>Yi Chang</author>
</authors>
<title>Yahoo! learning to rank challenge overview.</title>
<date>2011</date>
<booktitle>Learning to Rank Challenge, volume 14 of JMLR Proceedings,</booktitle>
<pages>1--24</pages>
<editor>In Olivier Chapelle, Yi Chang, and Tie-Yan Liu, editors, Yahoo!</editor>
<contexts>
<context position="12049" citStr="Chapelle and Chang, 2011" startWordPosition="2032" endWordPosition="2035">01) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many similar 3we have used the implementation provided by scikit-learn (Pedregosa et al., 2011). 350 tasks (Chapelle and Chang, 2011), in which only a few dense and continuous features are available, possibly because of their ability to take into account complex interactions between features and to automatically partition the continuous features value into a discrete set of intervals that achieves the best classification performance. As a baseline, we consider logistic regression (Hastie et al., 2003), a simple linear model where the parameters are estimated by maximizing the likelihood of the training set. These two classifiers do not produce only a class decision but yield an instance probability that represents the degre</context>
</contexts>
<marker>Chapelle, Chang, 2011</marker>
<rawString>Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Olivier Chapelle, Yi Chang, and Tie-Yan Liu, editors, Yahoo! Learning to Rank Challenge, volume 14 of JMLR Proceedings, pages 1–24. JMLR.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Fawcett</author>
</authors>
<title>ROC Graphs: Notes and Practical Considerations for Researchers.</title>
<date>2003</date>
<tech>Technical Report HPL-2003-4, HP Laboratories,</tech>
<location>Palo Alto.</location>
<contexts>
<context position="14414" citStr="Fawcett, 2003" startWordPosition="2426" endWordPosition="2427">ased on these observations, we propose the following three-step method to optimize the f1 score on the training set: 1. the classifier is first trained using the ‘standard’ learning procedure that optimizes either the 0/1 loss (for random forest) or the likelihood (for the logistic regression); 2. all the possible trade-offs between recall and precision are enumerated by varying the threshold; exploiting the monotonicity of thresholded classifications,4 this enumeration can be efficiently done in O (n · log n) and results in at most n threshold values, where n is the size of the training set (Fawcett, 2003); 3. all the f1 scores achieved for the different thresholds found in the previous step are evaluated; there are strong theoretical guarantees that the optimal f1 score that can be achieved on the training set is one of these values (Boyd and Vandenberghe, 2004). Figure 1 shows how f1 score varies with the decision threshold and allows to assess the difference between the optimal value of the threshold and its default value (0.5). Figure 1: Evolution of the f1 score with respect to the threshold used to transform probabilities into binary decisions 5 Experiments The features and learning strat</context>
</contexts>
<marker>Fawcett, 2003</marker>
<rawString>Tom Fawcett. 2003. ROC Graphs: Notes and Practical Considerations for Researchers. Technical Report HPL-2003-4, HP Laboratories, Palo Alto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a Gispert</author>
<author>Graeme Blackwood</author>
<author>Gonzalo Iglesias</author>
<author>William Byrne</author>
</authors>
<title>N-gram posterior probability confidence measures for statistical machine translation: an empirical study.</title>
<date>2013</date>
<journal>Machine Translation,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="8336" citStr="Gispert et al., 2013" startWordPosition="1403" endWordPosition="1406">icut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that quantifies the probability that it is part of the system output (Gispert et al., 2013). Posteriors aggregate two pieces of information for each word in the final hypothesis: first, all the paths in the lattice (i.e. the number of translation hypotheses in the search space) where the word appears in are considered; second, the decoder scores of these paths are accumulated in order to derive a confidence measure at the word level. In our experiments, we considered pseudo-references and lattices produced by the n-gram based system developed by our team for last year WMT evaluation campaign (Allauzen et al., 2013), that has achieved very good performance. Fluency Features These fea</context>
</contexts>
<marker>Gispert, Blackwood, Iglesias, Byrne, 2013</marker>
<rawString>Adri`a Gispert, Graeme Blackwood, Gonzalo Iglesias, and William Byrne. 2013. N-gram posterior probability confidence measures for statistical machine translation: an empirical study. Machine Translation, 27(2):85–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome H Friedman</author>
</authors>
<title>The Elements of Statistical Learning.</title>
<date>2003</date>
<publisher>Springer,</publisher>
<contexts>
<context position="12422" citStr="Hastie et al., 2003" startWordPosition="2089" endWordPosition="2092">ests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many similar 3we have used the implementation provided by scikit-learn (Pedregosa et al., 2011). 350 tasks (Chapelle and Chang, 2011), in which only a few dense and continuous features are available, possibly because of their ability to take into account complex interactions between features and to automatically partition the continuous features value into a discrete set of intervals that achieves the best classification performance. As a baseline, we consider logistic regression (Hastie et al., 2003), a simple linear model where the parameters are estimated by maximizing the likelihood of the training set. These two classifiers do not produce only a class decision but yield an instance probability that represents the degree to which an instance is a member of a class. As detailed in the next section, thresholding this probability will allow us to directly optimize the f1 score used to evaluate prediction performance. 4.2 Optimizing the f1 Score As explained in Section 2, quality prediction will be evaluated in terms of f1 score. The learning methods we consider can not, as most learning m</context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2003</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. 2003. The Elements of Statistical Learning. Springer, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Ilya Oparin</author>
<author>Alexandre Allauzen</author>
<author>JeanLuc Gauvain</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5524--5527</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="9271" citStr="Le et al., 2011" startWordPosition="1555" endWordPosition="1558"> measure at the word level. In our experiments, we considered pseudo-references and lattices produced by the n-gram based system developed by our team for last year WMT evaluation campaign (Allauzen et al., 2013), that has achieved very good performance. Fluency Features These features measure the ‘fluency’ of the target sentence and are based on different language models: a ‘traditional’ 4-gram language model estimated on WMT monolingual and bilingual data (the language model used by our system to generate the pseudo-references); a continuous-space 10-gram language model estimated with SOUL (Le et al., 2011) (also used by our MT system) and a 4-gram language model based on Part-of-Speech sequences. The latter model was estimated on the Spanish side of the bilingual data provided in the translation shared task in 2013. These data were POS-tagged with FreeLing (Padr´o and Stanilovsky, 2012). All these language models have been used to define two different features : • the probability of the word of interest p(tj|h) where h = tj−1, ..., tj−n+1 is the history made of the n − 1 previous words or POS • the ratio between the probability of the sentence and the ‘best’ probability that can be achieved if </context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, Ilya Oparin, Alexandre Allauzen, JeanLuc Gauvain, and Franc¸ois Yvon. 2011. Structured output layer neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5524–5527. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs Padr´o</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>Freeling 3.0: Towards wider multilinguality.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC 2012),</booktitle>
<publisher>ELRA.</publisher>
<location>Istanbul, Turkey,</location>
<marker>Padr´o, Stanilovsky, 2012</marker>
<rawString>Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards wider multilinguality. In Proceedings of the Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="12011" citStr="Pedregosa et al., 2011" startWordPosition="2026" endWordPosition="2029">riments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many similar 3we have used the implementation provided by scikit-learn (Pedregosa et al., 2011). 350 tasks (Chapelle and Chang, 2011), in which only a few dense and continuous features are available, possibly because of their ability to take into account complex interactions between features and to automatically partition the continuous features value into a discrete set of intervals that achieves the best classification performance. As a baseline, we consider logistic regression (Hastie et al., 2003), a simple linear model where the parameters are estimated by maximizing the likelihood of the training set. These two classifiers do not produce only a class decision but yield an instance</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="5636" citStr="Petrov et al. (2012)" startWordPosition="944" endWordPosition="947">ng to the POS on the training set POS % in train % BAD NOUN 23.81 35.02 ADP 15.06 35.48 DET 14.90 32.88 VERB 14.64 41.26 PUNCT 10.92 27.26 ADJ 6.61 35.68 CONJ 5.04 30.77 PRON 4.58 43.15 ADV 4.39 36.56 As the classes are unbalanced, prediction performance will be evaluated in terms of precision, recall and f1 score computed on the BAD label. More precisely, if the number of true positive (i.e. 2We used FreeLing (http:nlp.lsi.upc.edu/ freeling/) to predict the POS tags of the translation hypotheses and, for the sake of clarity, mapped the 71 tags used by FreeLing to the 11 universal POS tags of Petrov et al. (2012). BAD word predicted as BAD), false positive (OK word predicted as BAD) and false negative (BAD word predicted as OK) are denoted tpBAD, fpBAD and fnBAD, respectively, the quality of a confidence estimation system is evaluated by the three following metrics: tpBAD (1) tpBAD (2) 2 · pBAD · rBAD f1 = (3) pBAD + rBAD 3 Features In our experiments, we used 16 features to describe a given target word ti in a translation hypothesis t = (tj)mj=1. To avoid sparsity issues we decided not to include any lexicalized information such as the word or the previous word identities. As the translation hypothes</context>
</contexts>
<marker>Petrov, Das, McDonald, 2012</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Kumar Singh</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>LIMSI submission for the WMT’13 quality estimation task: an experiment with n-gram posteriors.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>398--404</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11321" citStr="Singh et al., 2013" startWordPosition="1914" endWordPosition="1917">for predicting translation quality at the word-level is not straightforward, as they need to be applied to sentences with a given unknown tokenization. Matching the tokenization used to estimate the model to the one used for collecting the annotations is a tedious and error-prone process and some of the prediction errors most probably result from mismatches in tokenization. 4 Learning Methods 4.1 Classifiers Predicting whether a word in a translation hypothesis should be post-edited or not can naturally be framed as a binary classification task. Based on our experiments in previous campaigns (Singh et al., 2013; Zhuang et al., 2012), we considered random forest in all our experiments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many </context>
<context position="16312" citStr="Singh et al., 2013" startWordPosition="2739" endWordPosition="2742"> Table 4: Prediction performance for each POS tag Classifier thres. rBAD pBAD fl Random forest 0.43 0.64 0.69 0.67 Logistic regression 0.27 0.51 0.72 0.59 were chosen by maximizing classification performance (as evaluated by the fi score) estimated on 150 sentences of the training set kept apart as a validation set. Results for the different learning algorithms considered are presented in Table 3. Random forest clearly outperforms a simple logistic regression, which shows the importance of using nonlinear decision functions, a conclusion at pair with our previous results (Zhuang et al., 2012; Singh et al., 2013). The overall performance, with a fl measure of 0.67, is pretty low and in our opinion, not good enough to consider using such a quality estimation system in a computer-assisted post-edition context. However, as shown in Table 4, the prediction performance highly depends on the POS category of the words: it is quite good for ‘plain’ words (like verb and nouns) but much worse for other categories. There are two possible explanations for this observation: predicting the correctness of some morpho-syntaxic categories may be intrinsically harder (e.g. for punctuation the choice of which can be hig</context>
</contexts>
<marker>Singh, Wisniewski, Yvon, 2013</marker>
<rawString>Anil Kumar Singh, Guillaume Wisniewski, and Franc¸ois Yvon. 2013. LIMSI submission for the WMT’13 quality estimation task: an experiment with n-gram posteriors. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 398–404, Sofia, Bulgaria, August. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>612--621</pages>
<publisher>ACL.</publisher>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7710" citStr="Soricut and Echihabi, 2010" startWordPosition="1299" endWordPosition="1303">et and also consider the geometric mean of the lexicalized probabilities, their maximum value (i.e. maxs∈s p(tj|s)) as well as a binary feature that fires when the target word tj is not in the lexicalized probabilities table. The second kind of association features relies on pseudo-references, that is to say, translations of the source sentence produced by an independent MT system. Many works have considered pBAD = rBAD = tpBAD + fpBAD tpBAD + fnBAD 1 p(tj|s) = n n i=1 349 pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that quantifies the probability that it is part of the system out</context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>Radu Soricut and Abdessamad Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 612–621, Uppsala, Sweden, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Sushant Narsale</author>
</authors>
<title>Combining quality prediction and system selection for improved automatic translation output.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>163--170</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7738" citStr="Soricut and Narsale, 2012" startWordPosition="1304" endWordPosition="1307">metric mean of the lexicalized probabilities, their maximum value (i.e. maxs∈s p(tj|s)) as well as a binary feature that fires when the target word tj is not in the lexicalized probabilities table. The second kind of association features relies on pseudo-references, that is to say, translations of the source sentence produced by an independent MT system. Many works have considered pBAD = rBAD = tpBAD + fpBAD tpBAD + fnBAD 1 p(tj|s) = n n i=1 349 pseudo-references to design new MT metrics (Albrecht and Hwa, 2007; Albrecht and Hwa, 2008) or for confidence estimation (Soricut and Echihabi, 2010; Soricut and Narsale, 2012) but, to the best of our knowledge, this is the first time that they are used to predict confidence at the word level. Pseudo-references are used to define 3 binary features which fire if the target word is in the pseudo-reference, in a 2-gram shared between the pseudo-reference and the translation hypothesis or in a common 3-gram, respectively. The lattices representing the search space considered to generate these pseudo-references also allow us to estimate the posterior probability of a target word that quantifies the probability that it is part of the system output (Gispert et al., 2013). </context>
</contexts>
<marker>Soricut, Narsale, 2012</marker>
<rawString>Radu Soricut and Sushant Narsale. 2012. Combining quality prediction and system selection for improved automatic translation output. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 163–170, Montr´eal, Canada, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
<author>Anil Kumar Singh</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Quality estimation for machine translation: Some lessons learned.</title>
<date>2013</date>
<journal>Machine Translation,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="18297" citStr="Wisniewski et al., 2013" startWordPosition="3077" endWordPosition="3080">in our system. Random forests deliver a quantification of the importance of a feature with respect to the predictability of the target variable. This quantification is derived from System fi VERB 0.73 PRON 0.72 ADJ 0.70 NOUN 0.69 ADV 0.69 overall 0.67 DET 0.62 ADP 0.61 CONJ 0.57 PUNCT 0.56 the position of a feature in a decision tree: features used in the top nodes of the trees, which contribute to the final prediction decision of a larger fraction of the input samples, play a more important role than features used near the leaves of the tree. It appears that, as for our previous experiments (Wisniewski et al., 2013), the most relevant feature for predicting translation quality is the feature derived from the SOUL language model, even if other fluency features seem to also play an important role. Surprisingly enough, features related to the pseudo-reference do not seem to be useful. Further experiments are needed to explain the reasons of this observation. 6 Conclusion In this paper we described the system submitted for Task 2 of WMT’14 Shared Task on Quality Estimation. Our system relies on a binary classifier and consider only a few dense and continuous features. While the overall performance is pretty </context>
</contexts>
<marker>Wisniewski, Singh, Yvon, 2013</marker>
<rawString>Guillaume Wisniewski, Anil Kumar Singh, and Franc¸ois Yvon. 2013. Quality estimation for machine translation: Some lessons learned. Machine Translation, 27(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yong Zhuang</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Non-linear models for confidence estimation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>157--162</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="11343" citStr="Zhuang et al., 2012" startWordPosition="1918" endWordPosition="1921">lation quality at the word-level is not straightforward, as they need to be applied to sentences with a given unknown tokenization. Matching the tokenization used to estimate the model to the one used for collecting the annotations is a tedious and error-prone process and some of the prediction errors most probably result from mismatches in tokenization. 4 Learning Methods 4.1 Classifiers Predicting whether a word in a translation hypothesis should be post-edited or not can naturally be framed as a binary classification task. Based on our experiments in previous campaigns (Singh et al., 2013; Zhuang et al., 2012), we considered random forest in all our experiments.3 Random forest (Breiman, 2001) is an ensemble method that learns many classification trees and predicts an aggregation of their result (for instance by majority voting). In contrast with standard decision trees, in which each node is split using the best split among all features, in a random forest the split is chosen randomly. In spite of this simple and counter-intuitive learning strategy, random forests have proven to be very good ‘out-of-the-box’ learners. Random forests have achieved very good performance in many similar 3we have used </context>
<context position="16291" citStr="Zhuang et al., 2012" startWordPosition="2735" endWordPosition="2738">strategies considered Table 4: Prediction performance for each POS tag Classifier thres. rBAD pBAD fl Random forest 0.43 0.64 0.69 0.67 Logistic regression 0.27 0.51 0.72 0.59 were chosen by maximizing classification performance (as evaluated by the fi score) estimated on 150 sentences of the training set kept apart as a validation set. Results for the different learning algorithms considered are presented in Table 3. Random forest clearly outperforms a simple logistic regression, which shows the importance of using nonlinear decision functions, a conclusion at pair with our previous results (Zhuang et al., 2012; Singh et al., 2013). The overall performance, with a fl measure of 0.67, is pretty low and in our opinion, not good enough to consider using such a quality estimation system in a computer-assisted post-edition context. However, as shown in Table 4, the prediction performance highly depends on the POS category of the words: it is quite good for ‘plain’ words (like verb and nouns) but much worse for other categories. There are two possible explanations for this observation: predicting the correctness of some morpho-syntaxic categories may be intrinsically harder (e.g. for punctuation the choic</context>
</contexts>
<marker>Zhuang, Wisniewski, Yvon, 2012</marker>
<rawString>Yong Zhuang, Guillaume Wisniewski, and Franc¸ois Yvon. 2012. Non-linear models for confidence estimation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 157–162, Montr´eal, Canada, June. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000321">
<title confidence="0.9991115">
Evaluating Distant Supervision for Subjectivity and Sentiment Analysis
on Arabic Twitter Feeds
</title>
<author confidence="0.885136">
Eshrag Refaee and Verena Rieser
</author>
<affiliation confidence="0.7733645">
Interaction Lab, School of Mathematical and Computer Sciences,
Heriot-Watt University,
</affiliation>
<address confidence="0.681228">
EH14 4AS Edinburgh, United Kingdom.
</address>
<email confidence="0.994383">
eaar1@hw.ac.uk, v.t.rieser@hw.ac.uk
</email>
<sectionHeader confidence="0.997293" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999699777777778">
Supervised machine learning methods for
automatic subjectivity and sentiment anal-
ysis (SSA) are problematic when applied
to social media, such as Twitter, since they
do not generalise well to unseen topics. A
possible remedy of this problem is to ap-
ply distant supervision (DS) approaches,
which learn from large amounts of auto-
matically annotated data. This research
empirically evaluates the performance of
DS approaches for SSA on Arabic Twitter
feeds. Results for emoticon- and lexicon-
based DS show a significant performance
gain over a fully supervised baseline, es-
pecially for detecting subjectivity, where
we achieve 95.19% accuracy, which is a
48.47% absolute improvement over previ-
ous fully supervised results.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977638297873">
Subjectivity and sentiment analysis (SSA) aims to
determine the attitude of an author with respect
to some topic, e.g. objective or subjective, or
the overall contextual polarity of an utterance, e.g.
positive or negative. Previous work on automatic
SSA has used manually annotated gold standard
data sets to analyse which feature sets and mod-
els perform best for this task, e.g. (Wilson et al.,
2009; Wiebe et al., 1999). Most of this work is in
English, but there have been first attempts to apply
similar techniques to Arabic, e.g. (Abdul-Mageed
et al., 2011; Mourad and Darwish, 2013). While
these models work well when tested using cross-
validation on limited static data sets, our previ-
ous results reveal that these models do not gen-
eralise to new data sets, e.g. collected at a later
point in time, due to their limited coverage (Refaee
and Rieser, 2014). While there is a growing inter-
est within the NLP community in building Arabic
corpora by harvesting the web, e.g. (Al-Sabbagh
and Girju, 2012; Abdul-Mageed and Diab, 2012;
Zaidan and Callison-Burch, 2013), these resources
have not been publicly released yet and only small
amounts of these data-sets are (manually) anno-
tated. We therefore turn to an approach known
as distant supervision (DS), as first proposed by
(Read, 2005), which uses readily available fea-
tures, such as emoticons, as noisy labels in or-
der to efficiently annotate large amounts of data
for learning domain-independent models. This ap-
proach has been shown to be successful for En-
glish SSA, e.g. (Go et al., 2009), and SSA for
under-resourced languages, such as Chinese (Yuan
and Purver, 2012).
The contributions of this paper are as follows:
we first collect two large corpora using emoticons
and lexicon-based features as noisy labels, which
we plan to release as part of this submission. Sec-
ond, this work is the first to apply and empirically
evaluate DS approaches on Arabic Twitter feeds.
We find that DS significantly outperforms fully su-
pervised SSA on our held-out test set. However,
compared to a majority baseline, predicting nega-
tive sentiment proves to be difficult using DS ap-
proaches. Third, we conduct an error analysis to
critically evaluate the results and give recommen-
dations for future directions.
</bodyText>
<sectionHeader confidence="0.966476" genericHeader="method">
2 Arabic Twitter SSA Corpora
</sectionHeader>
<bodyText confidence="0.989108">
We start by collecting three corpora at different
times over one year to account for the cyclic ef-
fects of topic change in social media (Eisenstein,
2013). Table 1 shows the distributions of labels in
our data-sets:
</bodyText>
<listItem confidence="0.922695">
1. A gold standard data-set which we use for
training and evaluation (spring 2013);
2. A data-set for DS using emoticon-based
queries (autumn 2013);
3. Another data-set for DS using a lexicon-
based approach (winter 2014).
</listItem>
<page confidence="0.937951">
174
</page>
<note confidence="0.3683585">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 174–179,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.9992342">
Data set Neutral Polar Positive Negative Total
Gold standard training 1,157 937 470 467 3,031
Emoticon-based training 55,076 62,466 32,842 33,629 184,013
Lexicon-based training 55,076 55,538 18,442 5,013 134,069
Manually labelled test 422 579 278 301 1,580
</table>
<tableCaption confidence="0.8321875">
Table 1: Sentiment label distribution of the gold standard manually annotated and distant supervision
data sets.
</tableCaption>
<listItem confidence="0.8656605">
Gold Standard Data-set: We harvest two gold
standard data sets at different time steps, which
we label manually. We first harvest a data set of
3,031 multi-dialectal Arabic tweets randomly re-
</listItem>
<bodyText confidence="0.978381166666667">
trieved over the period from February to March
2013. We use this set as a training set for our
fully supervised approach. We also manually label
1,580 tweets collected in autumn 2013, which we
use as an independent held-out test set. Two na-
tive speakers were recruited to manually annotate
the collected data for subjectivity and sentiment,
where we define sentiment as a positive or nega-
tive emotion, opinion or attitude, following (Wil-
son et al., 2009). Our gold standard annotations
reached a weighted κ = 0.76, which indicates re-
liable annotations (Carletta, 1996). We also auto-
matically annotate the corpus with a rich set of lin-
guistically motivated features using freely avail-
able processing tools for Arabic, such as MADA
(Nizar Habash and Roth, 2009), see Table 2. For
more details on gold standard corpus annotation
please see (Refaee and Rieser, 2014). 1
Type Feature-sets
Morphological diacritic, aspect, gender, mood, per-
son, part of speech (POS), state, voice,
has morphological analysis.
Syntactic n-grams of words and POS, lem-
mas, including bag of words (BOW),
bag of lemmas.
Semantic has positive lexicon,
has negative lexicon,
has neutral lexicon, has negator,
has positive emoticon,
has negative emoticon.
</bodyText>
<tableCaption confidence="0.928986">
Table 2: Annotated Feature-sets
</tableCaption>
<bodyText confidence="0.955627444444444">
Emoticon-Based Queries: In order to investi-
gate DS approaches to SSA, we also collect a
much larger data set of Arabic tweets, where
we use emoticons as noisy labels, following e.g.
(Read, 2005; Go et al., 2009; Pak and Paroubek,
2010; Yuan and Purver, 2012; Suttles and Ide,
2013). We query Twitter API for tweets with vari-
ations of positive and negative emoticons to ob-
tain pairs of micro-blog texts (statuses) and using
</bodyText>
<table confidence="0.960801">
Emoticon Sentiment label
:) , :-) , :)), (: , (-: positive
((:
:( , :-( , :(( , :(( , ): negative
)): )-:
</table>
<tableCaption confidence="0.968679">
Table 3: Emoticons used to automatically label the
training data-set.
</tableCaption>
<bodyText confidence="0.9989995">
emoticons as author-provided emotion labels. In
following (Purver and Battersby, 2012; Zhang et
al., 2011; Suttles and Ide, 2013), we also utilise
some sentiment-bearing hash tags to query emo-
tional tweets, e.g. �_, ; happiness and v_,� &gt;
sadness. Note that emoticons and hash-tags are
merely used to collect and build the training set
and were replaced by the standard (positive/ neg-
ative) labels. In order to collect neutral instances,
we query a set of official news accounts, following
an approach by (Pak and Paroubek, 2010). Exam-
ples of the accounts queried are: BBC-Arabic, Al-
Jazeera Arabic, SkyNews Arabia, Reuters Arabic,
France24-Arabic, and DW Arabic. We then au-
tomatically extract the same set of linguistically
motivated features as for the gold standard corpus.
Lexicon-Based Annotation: We also inves-
tigate an alternative approach to DS, which
combines rule-driven lexicon-based SSA, e.g.
(Taboada et al., 2011), with machine learning ap-
proaches, following (Zhang et al., 2011). We
build a new training dataset by combining three
lexica. We first exploit two existing subjectiv-
ity lexica: a manually annotated Arabic subjectiv-
ity lexicon (Abdul-Mageed and Diab, 2012) and
a publicly available English subjectivity lexicon,
MPQA (Wilson et al., 2009), which we automati-
cally translate using Google Translate, following a
</bodyText>
<footnote confidence="0.901229333333333">
1This GS data-set has been shared via
a special LREC repository available at
http://www.resourcebook.eu/shareyourlr/index.php
</footnote>
<page confidence="0.997414">
175
</page>
<bodyText confidence="0.999996083333333">
similar technique to (Mourad and Darwish, 2013).
The translated lexicon is manually corrected by re-
moving translations with neutral or no clear senti-
ment indicator.2 This results in 2,627 translated in-
stances after correction. We then construct a third
dialectal lexicon of 484 words that we extracted
from an independent Twitter development set and
manually annotated for sentiment. All lexicons
were merged into a combined lexicon of 4,422 an-
notated sentiment words (duplicates removed). In
order to obtain automatic labels for positive and
negative instances, we follow a simplified version
of the rule-based aggregation approach of Taboada
et al. (2011). First, all lexicons and tweets are lem-
matised. For each tweet, matched sentiment words
are marked with either (+1) or (-1) to incorporate
the semantic orientation of individual constituents.
This achieves a coverage level of 76.62% (which
is computed as a percentage of tweets with at least
one lexicon word) using the combined lexicon.
The identified sentiment words are replaced by
place-holders to avoid bias. To account for nega-
tion, we reverse the polarity (switch negation) fol-
lowing (Taboada et al., 2011). The sentiment ori-
entation of the entire tweet is then computed by
summing up the sentiment scores of all sentiment
words in a given tweet into a single score that au-
tomatically determines the label as being: positive
or negative. Instances where the score equals zero
are excluded from the training set as they represent
mixed-sentiment instances with an even number of
sentiment words. We validate this lexicon-based
labelling approach against a separate development
set by comparing the automatically computed la-
bels against manually annotated ones, reaching an
accuracy of 69.06%.
</bodyText>
<sectionHeader confidence="0.868465" genericHeader="method">
3 Classification Experiments Using
Distant Supervision
</sectionHeader>
<bodyText confidence="0.9746061">
We experiment with a number of machine learn-
ing methods and we report the results of the best
performing scheme, namely Support Vector Ma-
chines (SVMs), where we use the implementation
provided by WEKA (Witten and Frank, 2005). We
report the results on two metrics: F-score and ac-
curacy. We use paired t-tests to establish signifi-
cant differences (p &lt; .05). We experiment with
different feature sets and report on the best results
(Bag-of-Words (BOW) + morphological + seman-
</bodyText>
<footnote confidence="0.912216333333333">
2For instance, the day ofjudgement is assigned with a neg-
ative label while its Arabic translation is neutral considering
the context-independent polarity.
</footnote>
<bodyText confidence="0.99997005882353">
tic). We compare our results against a majority
baseline and against a fully supervised approach.
It is important to mention the most prominent pre-
vious work on SSA of Arabic tweets like (Abdul-
Mageed et al., 2012) who trained SVM classifiers
on a nearly 3K manually labelled data-set to curry
out two-stage binary classification attaining accu-
racy up to 65.87% for the sentiment classification
task. In a later work, (Mourad and Darwish, 2013)
employ SVM and Naive Bayes classifiers trained
on a set of 2,300 manually labelled Arabic tweets.
With 10-fold cross-validation settings, the author
reported an accuracy score of 72.5% for the senti-
ment classification task (positive vs. negative).
We evaluate the approaches on a separate held-
out test-set that is collected at a later point in time,
as described in Section 2.
</bodyText>
<subsectionHeader confidence="0.991048">
3.1 Emoticon-Based Distant Supervision
</subsectionHeader>
<bodyText confidence="0.999953848484848">
We first evaluate the potential of exploiting train-
ing data that is automatically labelled using emoti-
cons. The results are summarised in Table 4.
Polar vs. neutral: The results show a signifi-
cant improvement over the majority baseline, as
well as over the classifier trained on the gold stan-
dard data set: We achieve 95.19% accuracy on
the held-out set, which is a 48.47% absolute im-
provement over our previous fully supervised re-
sults. We attribute this improvement to two fac-
tors. First, the emoticon-based data set is about 60
times bigger than the gold standard data set (see
Table 1) and thus the emoticon-based model better
generalises to unseen events. Note that this perfor-
mance is comparable with (Suttles and Ide, 2013)
who achieved up to 98% accuracy using emoticon-
based DS on English tweets using 5.9 million
tweets. Second, neutral instances were sampled
from news accounts, which are mainly written in
modern standard Arabic (MSA), whereas we as-
sume that tweets including emoticons (which we
use for acquiring polar instances) are mainly writ-
ten in dialectal Arabic (DA). In future work, we
plan to investigate this hypothesis further by au-
tomatically detecting MSA/DA for a given tweet,
e.g. (Zaidan and Callison-Burch, 2013). Abdul-
Mageed et al. (2012) show that having such a fea-
ture can result in no significant impact on the over-
all performance of both subjectivity and sentiment
analysis tasks.
Positive vs. negative: For sentiment classifica-
tion, the performance of the emoticon-based ap-
proach degrades notably to 51%, which is still
</bodyText>
<page confidence="0.986238">
176
</page>
<table confidence="0.9988926">
Data-set majority fully super- emoticon DS lexicon- lexicon-aggr.
baseline vised presence
F Acc. F Acc. F Acc. F Acc. F Acc.
polar vs. neutral 0.69 53.0 0.43 46.62 0.95 95.19 0.95 95.09 0.91 91.09
positive vs. negative 0.67 50.89 0.41 49.65 0.51 51.25 0.53 57.06 0.52 52.98
</table>
<tableCaption confidence="0.99945">
Table 4: 2-level and single-level SSA classification using distant supervision (DS).
</tableCaption>
<bodyText confidence="0.954303222222222">
significantly better that the fully supervised base-
line, but nevertheless worse than a simple majority
baseline. These results are much lower than previ-
ous results on emoticon-based sentiment analysis
on English tweets by (Go et al., 2009; Bifet and
Frank, 2010) which both achieved around 83% ac-
curacy. The confusion matrix shows that mostly
negative instances are misclassified as positive,
with a very low recall on negative instances, see
</bodyText>
<tableCaption confidence="0.915411">
Table 5. Next, we investigate possible reasons in a
detailed error analysis.
</tableCaption>
<table confidence="0.9998796">
Data set Precision Recall
emoticon DS
positive 0.479 0.81
negative 0.556 0.212
lexicon-presence DS
positive 0.521 0.866
negative 0.733 0.317
lexicon-aggregation DS
positive 0.496 0.650
negative 0.583 0.426
</table>
<tableCaption confidence="0.9639225">
Table 5: Recall and precision for Sentiment Anal-
ysis
</tableCaption>
<subsubsectionHeader confidence="0.566961">
3.1.1 Error Analysis for Emoticon-Based DS
</subsubsectionHeader>
<bodyText confidence="0.9957103">
In particular, we investigate the use of sarcasm and
the direction emoticons face in right-to-left alpha-
bets.
Use of sarcasm and irony: Using an emoticon
as a label is naturally noisy, since we cannot know
for sure the intended meaning the author wishes
to express. This is especially problematic when
emoticons are used in a sarcastic way, i.e. their
intended meaning is the opposite of the expressed
emotion. An example from our data set is:
</bodyText>
<listItem confidence="0.53401">
(1) ):great job Ahli : ( — refer-
</listItem>
<bodyText confidence="0.970117842105263">
&apos; - ring to a famous football
team.
Research in psychology shows that up to 31% of
the time, emoticons are used sarcastically (Wolf,
2000). In order to investigate this hypothesis
we manually labelled a random sample of 303
misclassified instances for neutral, positive, nega-
tive, as well as sarcastic, mixed and unclear sen-
timents, see Table 6. Interestingly, the sarcas-
tic instances represent only 4.29%, while tweets
with mixed (positive and negative) sentiments rep-
resent 5.94% of the manually annotated sub-set.
In 34.32% of the instances, the manual labels
have matched the automatic emoticon-based la-
bels. Surprisingly, automatic emoticon-based la-
bel contrasts the manual labels in 36.63% of the
instances. Instances labelled as neutral represent
4.95%. The rest of the instances were assigned
‘unclear sentiment orientation’.
</bodyText>
<table confidence="0.999644">
Emoticon Predicted Manual label # in-
Label label stances
Positive Negative Mixed 8
Negative Positive Mixed 10
Positive Negative Negative 59
Negative Positive Negative 42
Positive Negative Neutral 29
Negative Positive Neutral 7
Positive Negative Positive 62
Negative Positive Positive 52
Positive Negative Sarcastic 8
Negative Positive Sarcastic 5
Positive Negative Unclear senti- 19
ment indicator
Negative Positive Unclear senti- 2
ment indicator
</table>
<tableCaption confidence="0.922447666666667">
Table 6: Results of labelling sarcasm, mixed emo-
tions and unclear sentiment for misclassified in-
stances.
</tableCaption>
<bodyText confidence="0.97977575">
Facing of emoticons: We therefore investigate
another possible error source following (Mourad
and Darwish, 2013), who claim that the right-to-
left alphabetic writing of Arabic might result in
emoticons being mistakenly interchanged while
typing. On some Arabic keyboards, typing “ )”
will produce the opposite “ (” parentheses. The
following example (2) illustrates a case of a mis-
classified instance, where we assume that the fac-
ing of emoticons might have been interchanged or
mistyped.
�
</bodyText>
<listItem confidence="0.888554">
(2) (: ÉÓ@ ú ¯A~Ó •C~ gno hope anymore :)
</listItem>
<subsectionHeader confidence="0.999876">
3.2 Lexicon-Based Distant Supervision
</subsectionHeader>
<bodyText confidence="0.999791">
To avoid the issue of ambiguity in the direction
of facing, we experiment with a lexicon-based ap-
proach to DS: instead of using emoticons, we now
</bodyText>
<page confidence="0.994658">
177
</page>
<bodyText confidence="0.99997135">
utilise the adjectives in our sentiment lexicon as
noisy labels. We experiment with two different
settings for the lexicon-based DS approach. First,
we experiment with a lexicon-presence approach
that automatically labels a tweet as a positive in-
stance if it only includes positive lexicon(s) and
the same for the negative class. Data instances
having mixed positive and negative lexicons or no
sentiment lexicons are excluded from the training
set. The second approach is based on assigning
a numerical value to sentiment words and aggre-
gating the value into a single score, see Section 2.
The results are summarised in Table 4.
Polar vs. neutral: We can observe that the mod-
els trained with the lexicon-presence approach sig-
nificantly outperform the majority baseline, the
fully supervised learning, as well as the lexicon-
aggregation approach. The lexicon-presence and
the emoticon-based DS approaches reach almost
identical performance on our test set.
Positive vs. negative: Again, we observe that
it is difficult to discriminate negative instances
for both lexicon-based approaches. The lexicon-
presence approach significantly outperforms the
majority baseline, the fully supervised learn-
ing, and the lexicon-aggregation approach. But
this time it also significantly outperforms the
emoticon-based approach, which allows us to con-
clude that lexicon-based labelling introduces less
noise for sentiment analysis. However, our re-
sults are significantly worse than the lexicon-based
approach of Taboada et al. (2011), with up to
80% accuracy, and the learning-based approach
of Zhanh et al. (2011), with up to 85% accu-
racy on English tweets. The lexicon-presence ap-
proach achieves the highest precision for negative
tweets, see table 5, but still has a low recall. The
lexicon-aggregation approach has the highest re-
call for negative tweets, but its precision is almost
identical to the emoticon-based approach.
</bodyText>
<subsectionHeader confidence="0.642112">
3.2.1 Error Analysis for Lexicon-Based DS
</subsectionHeader>
<bodyText confidence="0.999984111111111">
We conduct an error analysis in order to fur-
ther investigate the difference in performance
between the lexicon-presence and the lexicon-
aggregation approach. We hypothesise that the
lexicon-aggregation might perform better on in-
stances with mixed emotions, i.e. tweets with
positive and negative indicators, but a clear over-
all sentiment. We therefore manually add 36 in-
stances to the test set which contain mixed emo-
tions (but a unique sentiment label). However, the
results on the new test set confirm the superiority
of the lexicon-presence approach. In general, both
lexicon-based approaches perform worse for sen-
timent classification. Taboada et al. (2011) high-
light the issue of “positive bias” associated with
lexicon-based approaches of sentiment analysis,
as people tend to prefer using positive expressions
and understate negative ones.
</bodyText>
<sectionHeader confidence="0.99024" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.970159705882353">
We address the task of subjectivity and sentiment
analysis (SSA) for Arabic Twitter feeds. We em-
pirically investigate the performance of distant su-
pervision (DS) approaches on a manually labelled
independent test set, in comparison to a fully su-
pervised baseline, trained on a manually labelled
gold standard data set. Our experiments reveal:
(1) DS approaches to SSA for Arabic Twitter
feeds show significantly higher performance in ac-
curacy and F-score than a fully supervised ap-
proach. Despite providing noisy labels, they allow
larger amounts of data to be rapidly annotated, and
thus, can account for the topic shifts observed in
social media.
(2) DS approaches which use a subjectivity lex-
icon for labelling outperform approaches using
emoticon-based labels for sentiment analysis, but
achieve a very similar performance for subjectiv-
ity detection. We hypothesise that this can be at-
tributed to unclear facings of the emoticons.
(3) We also find that both our DS approaches
achieve good results of up to 95% accuracy for
subjectivity analysis, which is comparable to pre-
vious work on English tweets. However, we detect
a decrease in performance for sentiment analysis,
where negative instances repeatedly get misclas-
sified as positive. We assume that this can be at-
tributed to the more indirect ways adopted by peo-
ple to express their emotions verbally via social
media (Taboada et al., 2011). Other possible rea-
sons for this, which we will explore in future work,
include culturally specific differences (Hong et al.,
2011), as well as pragmatic/ context-dependent as-
pects of opinion (Sayeed, 2013).
</bodyText>
<page confidence="0.997616">
178
</page>
<sectionHeader confidence="0.994774" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999151533898305">
Muhammad Abdul-Mageed and Mona Diab. 2012.
AWATIF: A multi-genre corpus for modern standard
Arabic subjectivity and sentiment analysis. In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12), Is-
tanbul, Turkey. European Language Resources As-
sociation (ELRA).
Muhammad Abdul-Mageed, Mona T. Diab, and Mo-
hammed Korayem. 2011. Subjectivity and senti-
ment analysis of modern standard Arabic. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, pages 587–591, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Muhammad Abdul-Mageed, Sandra Kuebler, and
Mona Diab. 2012. SAMAR: A system for subjec-
tivity and sentiment analysis of Arabic social me-
dia. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 19–28. Association for Computa-
tional Linguistics.
Rania Al-Sabbagh and Roxana Girju. 2012. YADAC:
Yet another dialectal Arabic corpus. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istan-
bul, Turkey. European Language Resources Associ-
ation (ELRA).
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Discov-
ery Science, pages 1–15. Springer.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2):249–254.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In Proceedings of NAACL-
HLT, pages 359–369.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1–12.
Lichan Hong, Gregorio Convertino, and Ed H Chi.
2011. Language matters in twitter: A large scale
study. In ICWSM.
Ahmed Mourad and Kareem Darwish. 2013. Sub-
jectivity and sentiment analysis of modern stan-
dard Arabic and Arabic microblogs. WASSA 2013,
page 55.
Owen Rambow Nizar Habash and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, pos
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools, Cairo, Egypt, April.
The MEDAR Consortium.
A. Pak and P. Paroubek. 2010. Twitter as a corpus for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. In Proceedings of the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL), pages 482–491, Avi-
gnon, France, April. Association for Computational
Linguistics.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop, pages 43–48. Association
for Computational Linguistics.
Eshrag Refaee and Verena Rieser. 2014. An Arabic
twitter corpus for subjectivity and sentiment anal-
ysis. In Proceedings of the Ninth International
Conference on Language Resources and Evalua-
tion (LREC’14), Reykjavik, Iceland, may. European
Language Resources Association (ELRA).
Asad Sayeed. 2013. An opinion about opinions about
opinions: subjectivity and the aggregate reader. In
Proceedings of NAACL-HLT, pages 691–696.
Jared Suttles and Nancy Ide. 2013. Distant supervi-
sion for emotion classification with discrete binary
values. In Computational Linguistics and Intelligent
Text Processing, pages 121–136. Springer.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267–307.
Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.
O’Hara. 1999. Development and use of a gold-
standard data set for subjectivity classifications. In
Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Com-
putational Linguistics, ACL ’99, pages 246–253,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399–433.
Ian H Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
Alecia Wolf. 2000. Emotional expression online: Gen-
der differences in emoticon use. CyberPsychology
&amp; Behavior, 3(5):827–833.
Zheng Yuan and Matthew Purver. 2012. Predicting
emotion labels for chinese microblog texts. In Pro-
ceedings of the 1st International Workshop on Senti-
ment Discovery from Affective Data (SDAD), pages
40–47, Bristol, UK, September.
Omar F. Zaidan and Chris Callison-Burch. 2013. Ara-
bic dialect identification. Computational Linguis-
tics.
Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-
ichun Hsu, and Bing Liu. 2011. Combining lex-
iconbased and learning-based methods for twitter
sentiment analysis. HP Laboratories, Technical Re-
port HPL-2011, 89.
</reference>
<page confidence="0.998808">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407265">
<title confidence="0.97514425">Evaluating Distant Supervision for Subjectivity and Sentiment on Arabic Twitter Feeds Eshrag Refaee and Verena Interaction Lab, School of Mathematical and Computer</title>
<note confidence="0.585251">Heriot-Watt EH14 4AS Edinburgh, United</note>
<abstract confidence="0.99884847368421">Supervised machine learning methods for automatic subjectivity and sentiment analysis (SSA) are problematic when applied to social media, such as Twitter, since they do not generalise well to unseen topics. A possible remedy of this problem is to apply distant supervision (DS) approaches, which learn from large amounts of automatically annotated data. This research empirically evaluates the performance of DS approaches for SSA on Arabic Twitter feeds. Results for emoticonand lexiconbased DS show a significant performance gain over a fully supervised baseline, especially for detecting subjectivity, where we achieve 95.19% accuracy, which is a 48.47% absolute improvement over previous fully supervised results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Muhammad Abdul-Mageed</author>
<author>Mona Diab</author>
</authors>
<title>AWATIF: A multi-genre corpus for modern standard Arabic subjectivity and sentiment analysis.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul,</location>
<contexts>
<context position="2062" citStr="Abdul-Mageed and Diab, 2012" startWordPosition="315" endWordPosition="318">be et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. We therefore turn to an approach known as distant supervision (DS), as first proposed by (Read, 2005), which uses readily available features, such as emoticons, as noisy labels in order to efficiently annotate large amounts of data for learning domain-independent models. This approach has been shown to be successful for English SSA, e.g. (Go et al., 2009), and SSA for under-resourced languages, such as Chinese (Yuan and Purver, 2012). The c</context>
<context position="7530" citStr="Abdul-Mageed and Diab, 2012" startWordPosition="1190" endWordPosition="1193">s queried are: BBC-Arabic, AlJazeera Arabic, SkyNews Arabia, Reuters Arabic, France24-Arabic, and DW Arabic. We then automatically extract the same set of linguistically motivated features as for the gold standard corpus. Lexicon-Based Annotation: We also investigate an alternative approach to DS, which combines rule-driven lexicon-based SSA, e.g. (Taboada et al., 2011), with machine learning approaches, following (Zhang et al., 2011). We build a new training dataset by combining three lexica. We first exploit two existing subjectivity lexica: a manually annotated Arabic subjectivity lexicon (Abdul-Mageed and Diab, 2012) and a publicly available English subjectivity lexicon, MPQA (Wilson et al., 2009), which we automatically translate using Google Translate, following a 1This GS data-set has been shared via a special LREC repository available at http://www.resourcebook.eu/shareyourlr/index.php 175 similar technique to (Mourad and Darwish, 2013). The translated lexicon is manually corrected by removing translations with neutral or no clear sentiment indicator.2 This results in 2,627 translated instances after correction. We then construct a third dialectal lexicon of 484 words that we extracted from an indepen</context>
</contexts>
<marker>Abdul-Mageed, Diab, 2012</marker>
<rawString>Muhammad Abdul-Mageed and Mona Diab. 2012. AWATIF: A multi-genre corpus for modern standard Arabic subjectivity and sentiment analysis. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhammad Abdul-Mageed</author>
<author>Mona T Diab</author>
<author>Mohammed Korayem</author>
</authors>
<title>Subjectivity and sentiment analysis of modern standard Arabic.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>587--591</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1591" citStr="Abdul-Mageed et al., 2011" startWordPosition="235" endWordPosition="238">rovement over previous fully supervised results. 1 Introduction Subjectivity and sentiment analysis (SSA) aims to determine the attitude of an author with respect to some topic, e.g. objective or subjective, or the overall contextual polarity of an utterance, e.g. positive or negative. Previous work on automatic SSA has used manually annotated gold standard data sets to analyse which feature sets and models perform best for this task, e.g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets</context>
</contexts>
<marker>Abdul-Mageed, Diab, Korayem, 2011</marker>
<rawString>Muhammad Abdul-Mageed, Mona T. Diab, and Mohammed Korayem. 2011. Subjectivity and sentiment analysis of modern standard Arabic. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 587–591, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhammad Abdul-Mageed</author>
<author>Sandra Kuebler</author>
<author>Mona Diab</author>
</authors>
<title>SAMAR: A system for subjectivity and sentiment analysis of Arabic social media.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<pages>pages</pages>
<marker>Abdul-Mageed, Kuebler, Diab, 2012</marker>
<rawString>Muhammad Abdul-Mageed, Sandra Kuebler, and Mona Diab. 2012. SAMAR: A system for subjectivity and sentiment analysis of Arabic social media. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, pages 19–28. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rania Al-Sabbagh</author>
<author>Roxana Girju</author>
</authors>
<title>YADAC: Yet another dialectal Arabic corpus.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul,</location>
<contexts>
<context position="2033" citStr="Al-Sabbagh and Girju, 2012" startWordPosition="311" endWordPosition="314">g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. We therefore turn to an approach known as distant supervision (DS), as first proposed by (Read, 2005), which uses readily available features, such as emoticons, as noisy labels in order to efficiently annotate large amounts of data for learning domain-independent models. This approach has been shown to be successful for English SSA, e.g. (Go et al., 2009), and SSA for under-resourced languages, such as Chinese (</context>
</contexts>
<marker>Al-Sabbagh, Girju, 2012</marker>
<rawString>Rania Al-Sabbagh and Roxana Girju. 2012. YADAC: Yet another dialectal Arabic corpus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data.</title>
<date>2010</date>
<booktitle>In Discovery Science,</booktitle>
<pages>1--15</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13293" citStr="Bifet and Frank, 2010" startWordPosition="2100" endWordPosition="2103"> Data-set majority fully super- emoticon DS lexicon- lexicon-aggr. baseline vised presence F Acc. F Acc. F Acc. F Acc. F Acc. polar vs. neutral 0.69 53.0 0.43 46.62 0.95 95.19 0.95 95.09 0.91 91.09 positive vs. negative 0.67 50.89 0.41 49.65 0.51 51.25 0.53 57.06 0.52 52.98 Table 4: 2-level and single-level SSA classification using distant supervision (DS). significantly better that the fully supervised baseline, but nevertheless worse than a simple majority baseline. These results are much lower than previous results on emoticon-based sentiment analysis on English tweets by (Go et al., 2009; Bifet and Frank, 2010) which both achieved around 83% accuracy. The confusion matrix shows that mostly negative instances are misclassified as positive, with a very low recall on negative instances, see Table 5. Next, we investigate possible reasons in a detailed error analysis. Data set Precision Recall emoticon DS positive 0.479 0.81 negative 0.556 0.212 lexicon-presence DS positive 0.521 0.866 negative 0.733 0.317 lexicon-aggregation DS positive 0.496 0.650 negative 0.583 0.426 Table 5: Recall and precision for Sentiment Analysis 3.1.1 Error Analysis for Emoticon-Based DS In particular, we investigate the use of</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Discovery Science, pages 1–15. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="5060" citStr="Carletta, 1996" startWordPosition="795" endWordPosition="796">3,031 multi-dialectal Arabic tweets randomly retrieved over the period from February to March 2013. We use this set as a training set for our fully supervised approach. We also manually label 1,580 tweets collected in autumn 2013, which we use as an independent held-out test set. Two native speakers were recruited to manually annotate the collected data for subjectivity and sentiment, where we define sentiment as a positive or negative emotion, opinion or attitude, following (Wilson et al., 2009). Our gold standard annotations reached a weighted κ = 0.76, which indicates reliable annotations (Carletta, 1996). We also automatically annotate the corpus with a rich set of linguistically motivated features using freely available processing tools for Arabic, such as MADA (Nizar Habash and Roth, 2009), see Table 2. For more details on gold standard corpus annotation please see (Refaee and Rieser, 2014). 1 Type Feature-sets Morphological diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, ha</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACLHLT,</booktitle>
<pages>359--369</pages>
<contexts>
<context position="3464" citStr="Eisenstein, 2013" startWordPosition="546" endWordPosition="547">sion. Second, this work is the first to apply and empirically evaluate DS approaches on Arabic Twitter feeds. We find that DS significantly outperforms fully supervised SSA on our held-out test set. However, compared to a majority baseline, predicting negative sentiment proves to be difficult using DS approaches. Third, we conduct an error analysis to critically evaluate the results and give recommendations for future directions. 2 Arabic Twitter SSA Corpora We start by collecting three corpora at different times over one year to account for the cyclic effects of topic change in social media (Eisenstein, 2013). Table 1 shows the distributions of labels in our data-sets: 1. A gold standard data-set which we use for training and evaluation (spring 2013); 2. A data-set for DS using emoticon-based queries (autumn 2013); 3. Another data-set for DS using a lexiconbased approach (winter 2014). 174 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 174–179, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Data set Neutral Polar Positive Negative Total Gold standard training 1,157 937 470 467 3,031 Emoticon-based training 55,076 62,466 3</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of NAACLHLT, pages 359–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="2575" citStr="Go et al., 2009" startWordPosition="399" endWordPosition="402">ing Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. We therefore turn to an approach known as distant supervision (DS), as first proposed by (Read, 2005), which uses readily available features, such as emoticons, as noisy labels in order to efficiently annotate large amounts of data for learning domain-independent models. This approach has been shown to be successful for English SSA, e.g. (Go et al., 2009), and SSA for under-resourced languages, such as Chinese (Yuan and Purver, 2012). The contributions of this paper are as follows: we first collect two large corpora using emoticons and lexicon-based features as noisy labels, which we plan to release as part of this submission. Second, this work is the first to apply and empirically evaluate DS approaches on Arabic Twitter feeds. We find that DS significantly outperforms fully supervised SSA on our held-out test set. However, compared to a majority baseline, predicting negative sentiment proves to be difficult using DS approaches. Third, we con</context>
<context position="5959" citStr="Go et al., 2009" startWordPosition="936" endWordPosition="939">ser, 2014). 1 Type Feature-sets Morphological diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using Emoticon Sentiment label :) , :-) , :)), (: , (-: positive ((: :( , :-( , :(( , :(( , ): negative )): )-: Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emotional tweets, e.g.</context>
<context position="13269" citStr="Go et al., 2009" startWordPosition="2096" endWordPosition="2099">hich is still 176 Data-set majority fully super- emoticon DS lexicon- lexicon-aggr. baseline vised presence F Acc. F Acc. F Acc. F Acc. F Acc. polar vs. neutral 0.69 53.0 0.43 46.62 0.95 95.19 0.95 95.09 0.91 91.09 positive vs. negative 0.67 50.89 0.41 49.65 0.51 51.25 0.53 57.06 0.52 52.98 Table 4: 2-level and single-level SSA classification using distant supervision (DS). significantly better that the fully supervised baseline, but nevertheless worse than a simple majority baseline. These results are much lower than previous results on emoticon-based sentiment analysis on English tweets by (Go et al., 2009; Bifet and Frank, 2010) which both achieved around 83% accuracy. The confusion matrix shows that mostly negative instances are misclassified as positive, with a very low recall on negative instances, see Table 5. Next, we investigate possible reasons in a detailed error analysis. Data set Precision Recall emoticon DS positive 0.479 0.81 negative 0.556 0.212 lexicon-presence DS positive 0.521 0.866 negative 0.733 0.317 lexicon-aggregation DS positive 0.496 0.650 negative 0.583 0.426 Table 5: Recall and precision for Sentiment Analysis 3.1.1 Error Analysis for Emoticon-Based DS In particular, w</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lichan Hong</author>
<author>Gregorio Convertino</author>
<author>Ed H Chi</author>
</authors>
<title>Language matters in twitter: A large scale study.</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<marker>Hong, Convertino, Chi, 2011</marker>
<rawString>Lichan Hong, Gregorio Convertino, and Ed H Chi. 2011. Language matters in twitter: A large scale study. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Mourad</author>
<author>Kareem Darwish</author>
</authors>
<title>Subjectivity and sentiment analysis of modern standard Arabic and Arabic microblogs. WASSA</title>
<date>2013</date>
<pages>55</pages>
<contexts>
<context position="1618" citStr="Mourad and Darwish, 2013" startWordPosition="239" endWordPosition="242">y supervised results. 1 Introduction Subjectivity and sentiment analysis (SSA) aims to determine the attitude of an author with respect to some topic, e.g. objective or subjective, or the overall contextual polarity of an utterance, e.g. positive or negative. Previous work on automatic SSA has used manually annotated gold standard data sets to analyse which feature sets and models perform best for this task, e.g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. </context>
<context position="7860" citStr="Mourad and Darwish, 2013" startWordPosition="1234" endWordPosition="1237">-based SSA, e.g. (Taboada et al., 2011), with machine learning approaches, following (Zhang et al., 2011). We build a new training dataset by combining three lexica. We first exploit two existing subjectivity lexica: a manually annotated Arabic subjectivity lexicon (Abdul-Mageed and Diab, 2012) and a publicly available English subjectivity lexicon, MPQA (Wilson et al., 2009), which we automatically translate using Google Translate, following a 1This GS data-set has been shared via a special LREC repository available at http://www.resourcebook.eu/shareyourlr/index.php 175 similar technique to (Mourad and Darwish, 2013). The translated lexicon is manually corrected by removing translations with neutral or no clear sentiment indicator.2 This results in 2,627 translated instances after correction. We then construct a third dialectal lexicon of 484 words that we extracted from an independent Twitter development set and manually annotated for sentiment. All lexicons were merged into a combined lexicon of 4,422 annotated sentiment words (duplicates removed). In order to obtain automatic labels for positive and negative instances, we follow a simplified version of the rule-based aggregation approach of Taboada et </context>
<context position="10696" citStr="Mourad and Darwish, 2013" startWordPosition="1680" endWordPosition="1683">OW) + morphological + seman2For instance, the day ofjudgement is assigned with a negative label while its Arabic translation is neutral considering the context-independent polarity. tic). We compare our results against a majority baseline and against a fully supervised approach. It is important to mention the most prominent previous work on SSA of Arabic tweets like (AbdulMageed et al., 2012) who trained SVM classifiers on a nearly 3K manually labelled data-set to curry out two-stage binary classification attaining accuracy up to 65.87% for the sentiment classification task. In a later work, (Mourad and Darwish, 2013) employ SVM and Naive Bayes classifiers trained on a set of 2,300 manually labelled Arabic tweets. With 10-fold cross-validation settings, the author reported an accuracy score of 72.5% for the sentiment classification task (positive vs. negative). We evaluate the approaches on a separate heldout test-set that is collected at a later point in time, as described in Section 2. 3.1 Emoticon-Based Distant Supervision We first evaluate the potential of exploiting training data that is automatically labelled using emoticons. The results are summarised in Table 4. Polar vs. neutral: The results show </context>
<context position="15827" citStr="Mourad and Darwish, 2013" startWordPosition="2487" endWordPosition="2490">el label stances Positive Negative Mixed 8 Negative Positive Mixed 10 Positive Negative Negative 59 Negative Positive Negative 42 Positive Negative Neutral 29 Negative Positive Neutral 7 Positive Negative Positive 62 Negative Positive Positive 52 Positive Negative Sarcastic 8 Negative Positive Sarcastic 5 Positive Negative Unclear senti- 19 ment indicator Negative Positive Unclear senti- 2 ment indicator Table 6: Results of labelling sarcasm, mixed emotions and unclear sentiment for misclassified instances. Facing of emoticons: We therefore investigate another possible error source following (Mourad and Darwish, 2013), who claim that the right-toleft alphabetic writing of Arabic might result in emoticons being mistakenly interchanged while typing. On some Arabic keyboards, typing “ )” will produce the opposite “ (” parentheses. The following example (2) illustrates a case of a misclassified instance, where we assume that the facing of emoticons might have been interchanged or mistyped. � (2) (: ÉÓ@ ú ¯A~Ó •C~ gno hope anymore :) 3.2 Lexicon-Based Distant Supervision To avoid the issue of ambiguity in the direction of facing, we experiment with a lexicon-based approach to DS: instead of using emoticons, we </context>
</contexts>
<marker>Mourad, Darwish, 2013</marker>
<rawString>Ahmed Mourad and Kareem Darwish. 2013. Subjectivity and sentiment analysis of modern standard Arabic and Arabic microblogs. WASSA 2013, page 55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow Nizar Habash</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, pos tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<publisher>The MEDAR Consortium.</publisher>
<location>Cairo, Egypt,</location>
<contexts>
<context position="5251" citStr="Habash and Roth, 2009" startWordPosition="825" endWordPosition="828">y label 1,580 tweets collected in autumn 2013, which we use as an independent held-out test set. Two native speakers were recruited to manually annotate the collected data for subjectivity and sentiment, where we define sentiment as a positive or negative emotion, opinion or attitude, following (Wilson et al., 2009). Our gold standard annotations reached a weighted κ = 0.76, which indicates reliable annotations (Carletta, 1996). We also automatically annotate the corpus with a rich set of linguistically motivated features using freely available processing tools for Arabic, such as MADA (Nizar Habash and Roth, 2009), see Table 2. For more details on gold standard corpus annotation please see (Refaee and Rieser, 2014). 1 Type Feature-sets Morphological diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger da</context>
</contexts>
<marker>Habash, Roth, 2009</marker>
<rawString>Owen Rambow Nizar Habash and Ryan Roth. 2009. MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, pos tagging, stemming and lemmatization. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo, Egypt, April. The MEDAR Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pak</author>
<author>P Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="5983" citStr="Pak and Paroubek, 2010" startWordPosition="940" endWordPosition="943">e Feature-sets Morphological diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using Emoticon Sentiment label :) , :-) , :)), (: , (-: positive ((: :( , :-( , :(( , :(( , ): negative )): )-: Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emotional tweets, e.g. �_, ; happiness and v_,</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>A. Pak and P. Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Stuart Battersby</author>
</authors>
<title>Experimenting with distant supervision for emotion classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>482--491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="6433" citStr="Purver and Battersby, 2012" startWordPosition="1018" endWordPosition="1021">hes to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using Emoticon Sentiment label :) , :-) , :)), (: , (-: positive ((: :( , :-( , :(( , :(( , ): negative )): )-: Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emotional tweets, e.g. �_, ; happiness and v_,� &gt; sadness. Note that emoticons and hash-tags are merely used to collect and build the training set and were replaced by the standard (positive/ negative) labels. In order to collect neutral instances, we query a set of official news accounts, following an approach by (Pak and Paroubek, 2010). Examples of the accounts queried are: BBC-Arabic, AlJazeera Arabic, SkyNews Arabia, Reuters Arabic, France24-Arabic, and DW Arabic. We then automatically </context>
</contexts>
<marker>Purver, Battersby, 2012</marker>
<rawString>Matthew Purver and Stuart Battersby. 2012. Experimenting with distant supervision for emotion classification. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 482–491, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
</authors>
<title>Using emoticons to reduce dependency in machine learning techniques for sentiment classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2319" citStr="Read, 2005" startWordPosition="357" endWordPosition="358">ts, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. We therefore turn to an approach known as distant supervision (DS), as first proposed by (Read, 2005), which uses readily available features, such as emoticons, as noisy labels in order to efficiently annotate large amounts of data for learning domain-independent models. This approach has been shown to be successful for English SSA, e.g. (Go et al., 2009), and SSA for under-resourced languages, such as Chinese (Yuan and Purver, 2012). The contributions of this paper are as follows: we first collect two large corpora using emoticons and lexicon-based features as noisy labels, which we plan to release as part of this submission. Second, this work is the first to apply and empirically evaluate D</context>
<context position="5942" citStr="Read, 2005" startWordPosition="934" endWordPosition="935">faee and Rieser, 2014). 1 Type Feature-sets Morphological diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using Emoticon Sentiment label :) , :-) , :)), (: , (-: positive ((: :( , :-( , :(( , :(( , ): negative )): )-: Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emoti</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Jonathon Read. 2005. Using emoticons to reduce dependency in machine learning techniques for sentiment classification. In Proceedings of the ACL Student Research Workshop, pages 43–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eshrag Refaee</author>
<author>Verena Rieser</author>
</authors>
<title>An Arabic twitter corpus for subjectivity and sentiment analysis.</title>
<date>2014</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="1890" citStr="Refaee and Rieser, 2014" startWordPosition="287" endWordPosition="290"> on automatic SSA has used manually annotated gold standard data sets to analyse which feature sets and models perform best for this task, e.g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. We therefore turn to an approach known as distant supervision (DS), as first proposed by (Read, 2005), which uses readily available features, such as emoticons, as noisy labels in order to efficiently annotate large amounts of data for learning domain-independent models. </context>
<context position="5354" citStr="Refaee and Rieser, 2014" startWordPosition="842" endWordPosition="845">native speakers were recruited to manually annotate the collected data for subjectivity and sentiment, where we define sentiment as a positive or negative emotion, opinion or attitude, following (Wilson et al., 2009). Our gold standard annotations reached a weighted κ = 0.76, which indicates reliable annotations (Carletta, 1996). We also automatically annotate the corpus with a rich set of linguistically motivated features using freely available processing tools for Arabic, such as MADA (Nizar Habash and Roth, 2009), see Table 2. For more details on gold standard corpus annotation please see (Refaee and Rieser, 2014). 1 Type Feature-sets Morphological diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al.,</context>
</contexts>
<marker>Refaee, Rieser, 2014</marker>
<rawString>Eshrag Refaee and Verena Rieser. 2014. An Arabic twitter corpus for subjectivity and sentiment analysis. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asad Sayeed</author>
</authors>
<title>An opinion about opinions about opinions: subjectivity and the aggregate reader.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>691--696</pages>
<marker>Sayeed, 2013</marker>
<rawString>Asad Sayeed. 2013. An opinion about opinions about opinions: subjectivity and the aggregate reader. In Proceedings of NAACL-HLT, pages 691–696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jared Suttles</author>
<author>Nancy Ide</author>
</authors>
<title>Distant supervision for emotion classification with discrete binary values.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>121--136</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6030" citStr="Suttles and Ide, 2013" startWordPosition="948" endWordPosition="951"> gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using Emoticon Sentiment label :) , :-) , :)), (: , (-: positive ((: :( , :-( , :(( , :(( , ): negative )): )-: Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emotional tweets, e.g. �_, ; happiness and v_,� &gt; sadness. Note that emoticons and hash-tags </context>
<context position="11843" citStr="Suttles and Ide, 2013" startWordPosition="1868" endWordPosition="1871">The results are summarised in Table 4. Polar vs. neutral: The results show a significant improvement over the majority baseline, as well as over the classifier trained on the gold standard data set: We achieve 95.19% accuracy on the held-out set, which is a 48.47% absolute improvement over our previous fully supervised results. We attribute this improvement to two factors. First, the emoticon-based data set is about 60 times bigger than the gold standard data set (see Table 1) and thus the emoticon-based model better generalises to unseen events. Note that this performance is comparable with (Suttles and Ide, 2013) who achieved up to 98% accuracy using emoticonbased DS on English tweets using 5.9 million tweets. Second, neutral instances were sampled from news accounts, which are mainly written in modern standard Arabic (MSA), whereas we assume that tweets including emoticons (which we use for acquiring polar instances) are mainly written in dialectal Arabic (DA). In future work, we plan to investigate this hypothesis further by automatically detecting MSA/DA for a given tweet, e.g. (Zaidan and Callison-Burch, 2013). AbdulMageed et al. (2012) show that having such a feature can result in no significant </context>
</contexts>
<marker>Suttles, Ide, 2013</marker>
<rawString>Jared Suttles and Nancy Ide. 2013. Distant supervision for emotion classification with discrete binary values. In Computational Linguistics and Intelligent Text Processing, pages 121–136. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Kimberly Voll</author>
<author>Manfred Stede</author>
</authors>
<title>Lexiconbased methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<pages>37--2</pages>
<contexts>
<context position="7274" citStr="Taboada et al., 2011" startWordPosition="1151" endWordPosition="1154">ect and build the training set and were replaced by the standard (positive/ negative) labels. In order to collect neutral instances, we query a set of official news accounts, following an approach by (Pak and Paroubek, 2010). Examples of the accounts queried are: BBC-Arabic, AlJazeera Arabic, SkyNews Arabia, Reuters Arabic, France24-Arabic, and DW Arabic. We then automatically extract the same set of linguistically motivated features as for the gold standard corpus. Lexicon-Based Annotation: We also investigate an alternative approach to DS, which combines rule-driven lexicon-based SSA, e.g. (Taboada et al., 2011), with machine learning approaches, following (Zhang et al., 2011). We build a new training dataset by combining three lexica. We first exploit two existing subjectivity lexica: a manually annotated Arabic subjectivity lexicon (Abdul-Mageed and Diab, 2012) and a publicly available English subjectivity lexicon, MPQA (Wilson et al., 2009), which we automatically translate using Google Translate, following a 1This GS data-set has been shared via a special LREC repository available at http://www.resourcebook.eu/shareyourlr/index.php 175 similar technique to (Mourad and Darwish, 2013). The translat</context>
<context position="8984" citStr="Taboada et al., 2011" startWordPosition="1409" endWordPosition="1412">e instances, we follow a simplified version of the rule-based aggregation approach of Taboada et al. (2011). First, all lexicons and tweets are lemmatised. For each tweet, matched sentiment words are marked with either (+1) or (-1) to incorporate the semantic orientation of individual constituents. This achieves a coverage level of 76.62% (which is computed as a percentage of tweets with at least one lexicon word) using the combined lexicon. The identified sentiment words are replaced by place-holders to avoid bias. To account for negation, we reverse the polarity (switch negation) following (Taboada et al., 2011). The sentiment orientation of the entire tweet is then computed by summing up the sentiment scores of all sentiment words in a given tweet into a single score that automatically determines the label as being: positive or negative. Instances where the score equals zero are excluded from the training set as they represent mixed-sentiment instances with an even number of sentiment words. We validate this lexicon-based labelling approach against a separate development set by comparing the automatically computed labels against manually annotated ones, reaching an accuracy of 69.06%. 3 Classificati</context>
<context position="17951" citStr="Taboada et al. (2011)" startWordPosition="2814" endWordPosition="2817">proaches reach almost identical performance on our test set. Positive vs. negative: Again, we observe that it is difficult to discriminate negative instances for both lexicon-based approaches. The lexiconpresence approach significantly outperforms the majority baseline, the fully supervised learning, and the lexicon-aggregation approach. But this time it also significantly outperforms the emoticon-based approach, which allows us to conclude that lexicon-based labelling introduces less noise for sentiment analysis. However, our results are significantly worse than the lexicon-based approach of Taboada et al. (2011), with up to 80% accuracy, and the learning-based approach of Zhanh et al. (2011), with up to 85% accuracy on English tweets. The lexicon-presence approach achieves the highest precision for negative tweets, see table 5, but still has a low recall. The lexicon-aggregation approach has the highest recall for negative tweets, but its precision is almost identical to the emoticon-based approach. 3.2.1 Error Analysis for Lexicon-Based DS We conduct an error analysis in order to further investigate the difference in performance between the lexicon-presence and the lexiconaggregation approach. We hy</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>Maite Taboada, Julian Brooke, Milan Tofiloski, Kimberly Voll, and Manfred Stede. 2011. Lexiconbased methods for sentiment analysis. Computational linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Rebecca F Bruce</author>
<author>Thomas P O’Hara</author>
</authors>
<title>Development and use of a goldstandard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>246--253</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P. O’Hara. 1999. Development and use of a goldstandard data set for subjectivity classifications. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 246–253, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="1430" citStr="Wilson et al., 2009" startWordPosition="207" endWordPosition="210"> performance gain over a fully supervised baseline, especially for detecting subjectivity, where we achieve 95.19% accuracy, which is a 48.47% absolute improvement over previous fully supervised results. 1 Introduction Subjectivity and sentiment analysis (SSA) aims to determine the attitude of an author with respect to some topic, e.g. objective or subjective, or the overall contextual polarity of an utterance, e.g. positive or negative. Previous work on automatic SSA has used manually annotated gold standard data sets to analyse which feature sets and models perform best for this task, e.g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2</context>
<context position="4946" citStr="Wilson et al., 2009" startWordPosition="775" endWordPosition="779">e harvest two gold standard data sets at different time steps, which we label manually. We first harvest a data set of 3,031 multi-dialectal Arabic tweets randomly retrieved over the period from February to March 2013. We use this set as a training set for our fully supervised approach. We also manually label 1,580 tweets collected in autumn 2013, which we use as an independent held-out test set. Two native speakers were recruited to manually annotate the collected data for subjectivity and sentiment, where we define sentiment as a positive or negative emotion, opinion or attitude, following (Wilson et al., 2009). Our gold standard annotations reached a weighted κ = 0.76, which indicates reliable annotations (Carletta, 1996). We also automatically annotate the corpus with a rich set of linguistically motivated features using freely available processing tools for Arabic, such as MADA (Nizar Habash and Roth, 2009), see Table 2. For more details on gold standard corpus annotation please see (Refaee and Rieser, 2014). 1 Type Feature-sets Morphological diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, includi</context>
<context position="7612" citStr="Wilson et al., 2009" startWordPosition="1202" endWordPosition="1205">ic, and DW Arabic. We then automatically extract the same set of linguistically motivated features as for the gold standard corpus. Lexicon-Based Annotation: We also investigate an alternative approach to DS, which combines rule-driven lexicon-based SSA, e.g. (Taboada et al., 2011), with machine learning approaches, following (Zhang et al., 2011). We build a new training dataset by combining three lexica. We first exploit two existing subjectivity lexica: a manually annotated Arabic subjectivity lexicon (Abdul-Mageed and Diab, 2012) and a publicly available English subjectivity lexicon, MPQA (Wilson et al., 2009), which we automatically translate using Google Translate, following a 1This GS data-set has been shared via a special LREC repository available at http://www.resourcebook.eu/shareyourlr/index.php 175 similar technique to (Mourad and Darwish, 2013). The translated lexicon is manually corrected by removing translations with neutral or no clear sentiment indicator.2 This results in 2,627 translated instances after correction. We then construct a third dialectal lexicon of 484 words that we extracted from an independent Twitter development set and manually annotated for sentiment. All lexicons we</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="9850" citStr="Witten and Frank, 2005" startWordPosition="1544" endWordPosition="1547">here the score equals zero are excluded from the training set as they represent mixed-sentiment instances with an even number of sentiment words. We validate this lexicon-based labelling approach against a separate development set by comparing the automatically computed labels against manually annotated ones, reaching an accuracy of 69.06%. 3 Classification Experiments Using Distant Supervision We experiment with a number of machine learning methods and we report the results of the best performing scheme, namely Support Vector Machines (SVMs), where we use the implementation provided by WEKA (Witten and Frank, 2005). We report the results on two metrics: F-score and accuracy. We use paired t-tests to establish significant differences (p &lt; .05). We experiment with different feature sets and report on the best results (Bag-of-Words (BOW) + morphological + seman2For instance, the day ofjudgement is assigned with a negative label while its Arabic translation is neutral considering the context-independent polarity. tic). We compare our results against a majority baseline and against a fully supervised approach. It is important to mention the most prominent previous work on SSA of Arabic tweets like (AbdulMage</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alecia Wolf</author>
</authors>
<title>Emotional expression online: Gender differences in emoticon use.</title>
<date>2000</date>
<journal>CyberPsychology &amp; Behavior,</journal>
<volume>3</volume>
<issue>5</issue>
<contexts>
<context position="14467" citStr="Wolf, 2000" startWordPosition="2293" endWordPosition="2294">particular, we investigate the use of sarcasm and the direction emoticons face in right-to-left alphabets. Use of sarcasm and irony: Using an emoticon as a label is naturally noisy, since we cannot know for sure the intended meaning the author wishes to express. This is especially problematic when emoticons are used in a sarcastic way, i.e. their intended meaning is the opposite of the expressed emotion. An example from our data set is: (1) ):great job Ahli : ( — refer&apos; - ring to a famous football team. Research in psychology shows that up to 31% of the time, emoticons are used sarcastically (Wolf, 2000). In order to investigate this hypothesis we manually labelled a random sample of 303 misclassified instances for neutral, positive, negative, as well as sarcastic, mixed and unclear sentiments, see Table 6. Interestingly, the sarcastic instances represent only 4.29%, while tweets with mixed (positive and negative) sentiments represent 5.94% of the manually annotated sub-set. In 34.32% of the instances, the manual labels have matched the automatic emoticon-based labels. Surprisingly, automatic emoticon-based label contrasts the manual labels in 36.63% of the instances. Instances labelled as ne</context>
</contexts>
<marker>Wolf, 2000</marker>
<rawString>Alecia Wolf. 2000. Emotional expression online: Gender differences in emoticon use. CyberPsychology &amp; Behavior, 3(5):827–833.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Yuan</author>
<author>Matthew Purver</author>
</authors>
<title>Predicting emotion labels for chinese microblog texts.</title>
<date>2012</date>
<booktitle>In Proceedings of the 1st International Workshop on Sentiment Discovery from Affective Data (SDAD),</booktitle>
<pages>40--47</pages>
<location>Bristol, UK,</location>
<contexts>
<context position="2655" citStr="Yuan and Purver, 2012" startWordPosition="411" endWordPosition="414">; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. We therefore turn to an approach known as distant supervision (DS), as first proposed by (Read, 2005), which uses readily available features, such as emoticons, as noisy labels in order to efficiently annotate large amounts of data for learning domain-independent models. This approach has been shown to be successful for English SSA, e.g. (Go et al., 2009), and SSA for under-resourced languages, such as Chinese (Yuan and Purver, 2012). The contributions of this paper are as follows: we first collect two large corpora using emoticons and lexicon-based features as noisy labels, which we plan to release as part of this submission. Second, this work is the first to apply and empirically evaluate DS approaches on Arabic Twitter feeds. We find that DS significantly outperforms fully supervised SSA on our held-out test set. However, compared to a majority baseline, predicting negative sentiment proves to be difficult using DS approaches. Third, we conduct an error analysis to critically evaluate the results and give recommendatio</context>
<context position="6006" citStr="Yuan and Purver, 2012" startWordPosition="944" endWordPosition="947">ical diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. Syntactic n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. Semantic has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using Emoticon Sentiment label :) , :-) , :)), (: , (-: positive ((: :( , :-( , :(( , :(( , ): negative )): )-: Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emotional tweets, e.g. �_, ; happiness and v_,� &gt; sadness. Note that </context>
</contexts>
<marker>Yuan, Purver, 2012</marker>
<rawString>Zheng Yuan and Matthew Purver. 2012. Predicting emotion labels for chinese microblog texts. In Proceedings of the 1st International Workshop on Sentiment Discovery from Affective Data (SDAD), pages 40–47, Bristol, UK, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Arabic dialect identification. Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="2096" citStr="Zaidan and Callison-Burch, 2013" startWordPosition="319" endWordPosition="322">s work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh and Girju, 2012; Abdul-Mageed and Diab, 2012; Zaidan and Callison-Burch, 2013), these resources have not been publicly released yet and only small amounts of these data-sets are (manually) annotated. We therefore turn to an approach known as distant supervision (DS), as first proposed by (Read, 2005), which uses readily available features, such as emoticons, as noisy labels in order to efficiently annotate large amounts of data for learning domain-independent models. This approach has been shown to be successful for English SSA, e.g. (Go et al., 2009), and SSA for under-resourced languages, such as Chinese (Yuan and Purver, 2012). The contributions of this paper are as </context>
<context position="12354" citStr="Zaidan and Callison-Burch, 2013" startWordPosition="1949" endWordPosition="1952">on-based model better generalises to unseen events. Note that this performance is comparable with (Suttles and Ide, 2013) who achieved up to 98% accuracy using emoticonbased DS on English tweets using 5.9 million tweets. Second, neutral instances were sampled from news accounts, which are mainly written in modern standard Arabic (MSA), whereas we assume that tweets including emoticons (which we use for acquiring polar instances) are mainly written in dialectal Arabic (DA). In future work, we plan to investigate this hypothesis further by automatically detecting MSA/DA for a given tweet, e.g. (Zaidan and Callison-Burch, 2013). AbdulMageed et al. (2012) show that having such a feature can result in no significant impact on the overall performance of both subjectivity and sentiment analysis tasks. Positive vs. negative: For sentiment classification, the performance of the emoticon-based approach degrades notably to 51%, which is still 176 Data-set majority fully super- emoticon DS lexicon- lexicon-aggr. baseline vised presence F Acc. F Acc. F Acc. F Acc. F Acc. polar vs. neutral 0.69 53.0 0.43 46.62 0.95 95.19 0.95 95.09 0.91 91.09 positive vs. negative 0.67 50.89 0.41 49.65 0.51 51.25 0.53 57.06 0.52 52.98 Table 4:</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2013</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ley Zhang</author>
<author>Riddhiman Ghosh</author>
<author>Mohamed Dekhil</author>
<author>Meichun Hsu</author>
<author>Bing Liu</author>
</authors>
<title>Combining lexiconbased and learning-based methods for twitter sentiment analysis. HP Laboratories,</title>
<date>2011</date>
<tech>Technical Report HPL-2011, 89.</tech>
<contexts>
<context position="6453" citStr="Zhang et al., 2011" startWordPosition="1022" endWordPosition="1025">a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using Emoticon Sentiment label :) , :-) , :)), (: , (-: positive ((: :( , :-( , :(( , :(( , ): negative )): )-: Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emotional tweets, e.g. �_, ; happiness and v_,� &gt; sadness. Note that emoticons and hash-tags are merely used to collect and build the training set and were replaced by the standard (positive/ negative) labels. In order to collect neutral instances, we query a set of official news accounts, following an approach by (Pak and Paroubek, 2010). Examples of the accounts queried are: BBC-Arabic, AlJazeera Arabic, SkyNews Arabia, Reuters Arabic, France24-Arabic, and DW Arabic. We then automatically extract the same set</context>
</contexts>
<marker>Zhang, Ghosh, Dekhil, Hsu, Liu, 2011</marker>
<rawString>Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Meichun Hsu, and Bing Liu. 2011. Combining lexiconbased and learning-based methods for twitter sentiment analysis. HP Laboratories, Technical Report HPL-2011, 89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
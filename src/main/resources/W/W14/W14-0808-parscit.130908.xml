<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002347">
<title confidence="0.995377">
German Compounds and Statistical Machine Translation.
Can they get along?
</title>
<author confidence="0.995365">
Carla Parra Escartín Stephan Peitz Hermann Ney
</author>
<affiliation confidence="0.995756">
University of Bergen RWTH Aachen University RWTH Aachen University
</affiliation>
<address confidence="0.574209">
Bergen, Norway Aachen, Germany Aachen, Germany
</address>
<email confidence="0.992439">
carla.parra@uib.no peitz@cs.rwth-aachen.de ney@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.99374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993288">
This paper reports different experiments
created to study the impact of using
linguistics to preprocess German com-
pounds prior to translation in Statistical
Machine Translation (SMT). Compounds
are a known challenge both in Machine
Translation (MT) and Translation in gen-
eral as well as in other Natural Language
Processing (NLP) applications. In the case
of SMT, German compounds are split into
their constituents to decrease the number
of unknown words and improve the re-
sults of evaluation measures like the Bleu
score. To assess to which extent it is neces-
sary to deal with German compounds as a
part of preprocessing in SMT systems, we
have tested different compound splitters
and strategies, such as adding lists of com-
pounds and their translations to the train-
ing set. This paper summarizes the re-
sults of our experiments and attempts to
yield better translations of German nom-
inal compounds into Spanish and shows
how our approach improves by up to 1.4
Bleu points with respect to the baseline.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911916666667">
The pair of languages German-4Spanish is not a
widely researched combination in Statistical Ma-
chine Translation (SMT) and yet it is a challenging
one as both languages belong to different language
families (Germanic and Romance) and their char-
acteristics and inner structure differ greatly. As it
may happen with other language pair combinations
involving a Germanic and a Romance language,
when it comes to the translation of German com-
pounds into Spanish, the challenge is greater than
when translating into other Germanic languages
such as English. The translation of the German
</bodyText>
<page confidence="0.996351">
48
</page>
<bodyText confidence="0.998593285714286">
compound does not correspond to the translation
of its parts, but rather constitutes a phraseological
structure which must conform the Spanish gram-
matical rules. Examples 1 and 2 show the split-
tings of the German compounds Warmwasserbere-
itung and Wärmerückgewinnungssysteme and their
translations into English and Spanish.
</bodyText>
<listItem confidence="0.7705055">
(1) Warm Wasser Bereitung
caliente agua preparaci6n
warm water production
[ES]: ‘Preparaci6n de agua caliente’
[EN]: ‘Warm water production’
(2) Wärme Rückgewinnung s Systeme
</listItem>
<bodyText confidence="0.950430458333333">
calor recuperaci6n Ø sistemas
heat recovery Ø Systems
[ES]: ‘sistemas de recuperaci6n de calor’
[EN]: ‘heat recovery systems’
As may be observed in Examples 1 and 2, in
Spanish not only there is word reordering, but also
there is usage of other word categories such as
prepositions. While the examples above are quite
simple, the work done by researchers such as An-
gele (1992), G6mez Pérez (2001) and Oster (2003)
for the pair of languages German-4Spanish shows
that the translational equivalences in Spanish not
only are very varied, but also unpredictable to a
certain extent. Thus, while a mere compound split-
ting strategy may work for English, in the case of
Spanish further processing is required to yield the
correct translation.
According to Atkins et al. (2001)1, complex
nominals (i.e. nominal compounds and some
nominal phrases) are to be considered a special
type of MWE because they do have some partic-
ular features and to some extent they behave as
a single unit because they refer to a single con-
cept. Despite focusing on another language pair
</bodyText>
<footnote confidence="0.903293">
1Appendix F of Deliverable D2.2-D3.2 of the ISLE
project.
</footnote>
<note confidence="0.982493">
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 48–56,
Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999755684210527">
(English-4Italian), in the case of our language pair
(German-4Spanish) a similar claim could be done.
Besides, the issue of compounds being translated
into phrases in different languages is essentially a
MWE problem.
In this paper, we report on the results of our
research facing this particular challenge. More
concretely, Section 2 briefly discusses the prob-
lem of compounds in general and Section 3 de-
scribes our case of study. Subsection 3.1 briefly
discusses the large presence of German nominal
compounds in specialized corpora and presents the
results of a preliminary study and Subsection 3.2
summarizes the state-of-the-art strategies to deal
with compounds in SMT. Section 4 focuses on the
experiments carried out and reported here and the
results thereof are presented and discussed in Sec-
tion 5. Finally, Section 6 summarizes the findings
of our research and discusses future work.
</bodyText>
<sectionHeader confidence="0.989116" genericHeader="method">
2 German Compounds
</sectionHeader>
<bodyText confidence="0.999470212121212">
German compounds may be lexicalized or not.
Lexicalized compounds are those which can be
found in general dictionaries, such as Straffen-
lampe (“street lamp/light” in German). Non lex-
icalized compounds are formed in a similar man-
ner to that of phrases and/or sentences and are
coined on-the-fly (i.e. Warmwasserbereitungsan-
lagen, see Example 3). Non lexicalized com-
pounds usually appear in technical and formal texts
and German shows a great tendency to produce
them. In SMT, the translational correspondences
are computed from a sentence aligned training cor-
pus and translation dictionaries are not present.
Rather, word alignment algorithms are used to pro-
duce the phrase tables that will in turn be used
to produce the translations. Thus, although non
lexicalized compounds pose a greater challenge
(they are unpredictable), lexicalized compounds
are not distinguished either. As this formal distinc-
tion cannot be done when dealing with SMT, here
we will refer to compounds irrespectively whether
they are lexicalized or not, unless otherwise spec-
ified.
Moreover, German compounds may be nouns,
adjectives, adverbs and verbs, although the largest
group is the one corresponding to nominal com-
pounds. Finally, it is also important to highlight
that sometimes more than one compound-forming
phenomenon may take place subsequently to form
a new, longer, compound. Previous Example 1 is
the result of such a process, and as illustrated in Ex-
ample 3 it can, in turn, be the base for a yet newer
compound.
</bodyText>
<listItem confidence="0.999689">
(3) warm (ADJ) + Wasser(N) = Warmwasser (N)
+ Bereitung(N) = Warmwasserbereitung
(N) + s + Anlagen(N) =
Warmwasserbereitungsanlagen (N) [EN:
warm water production systems]
</listItem>
<bodyText confidence="0.996289083333333">
As may also be observed in Example 3, the word
class of the compound is determined by the ele-
ment located in the rightmost position of the com-
pound (i.e. the combination of the adjective warm
and the noun Wasser yields a nominal compound).
Finally, it is also important to highlight that be-
sides words, compounds may also include particles
to join those words together, as the “s” between
Warmwasserbereitung and Anlagen in Example 3
or truncations (part of one of the component words
is deleted). Example 4 illustrates the case when
one of the component words has been truncated:
</bodyText>
<listItem confidence="0.993992">
(4) abstellen(V) - en + Anlagen(N) =
Abstellanlagen (N) [EN: parking facilities]
</listItem>
<bodyText confidence="0.9998862">
The morphology of German compounds has
been widely researched, both within linguistics
(Fleischer, 1975; Wellman, 1984; Eichinger, 2000,
among others), as in NLP (Langer, 1998; Girju et
al., 2005; Marek, 2006; Girju, 2008, among oth-
ers). Here, we will focus on the impact of prepro-
cessing nominal compounds in SMT.
Baroni et al. (2002) report that 47% of the vo-
cabulary (types) in the APA corpus2 were com-
pounds. As will be observed in Section 4, the com-
pound splitters we used also detected a high per-
centage of compounds in the corpora used in our
experiments. This fact confirms that it is crucial to
find a successful way of processing compounds in
NLP applications and in our case in SMT.
</bodyText>
<sectionHeader confidence="0.870427" genericHeader="method">
3 Case Study
</sectionHeader>
<bodyText confidence="0.999957">
The experiments carried out here have used the
texts corresponding to the domain B00: Construc-
tion of the TRIS corpus (Parra Escartín, 2012),
and an internally compiled version of the Europarl
Corpus (Koehn, 2005) for the pair of languages
German-Spanish3. The domain (B00: Construc-
tion) was selected because it is the biggest one of
</bodyText>
<footnote confidence="0.99528725">
2Corpus of the Austria Presse Agentur (APA). Recently it
has been released as the AMC corpus (Austrian Media Cor-
pus) (Ransmayr et al., 2013).
3See Table 2 for an overview of the corpus statistics.
</footnote>
<page confidence="0.999657">
49
</page>
<bodyText confidence="0.9999726875">
the three domains currently available in the TRIS
corpus4. Only one domain was used because we
aimed at testing in-domain translation. Besides,
the TRIS corpus was selected because it is a spe-
cialised German-Spanish parallel corpus. As op-
posed to the Europarl, the TRIS corpus is divided in
domains and the source and target languages have
been verified (i.e. the texts were originally written
in German and translated into Spanish). Moreover,
the texts included in the Europarl are transcrip-
tions of the sessions of the European Parliament,
and thus the style is rather oral and less technical.
As compounds tend to be more frequent in domain
specific texts, the TRIS corpus has been used for
testing, while the Europarl Corpus has been used
in the training set to avoid data scarcity problems
and increase the vocabulary coverage of the SMT
system.
In the case of Machine Translation (MT), both
rule-based MT systems (RBMT systems) and Sta-
tistical MT systems (SMT systems) encounter prob-
lems when dealing with compounds. For the pur-
poses of this paper, the treatment of compounds
in German has been tested within the SMT toolkit
Jane (Wuebker et al., 2012; Vilar et al., 2010).
We have carried out several experiments translat-
ing German specialized texts into Spanish to test
to which extent incorporating a linguistic analy-
sis of the corpora and compiling compound lists
improves the overall SMT results. At this stage, in-
cluding further linguistic information such as Part-
of-Speech tagging (POS tagging) or phrase chunk-
ing has been disregarded. Forcing the translation
of compounds in the phrase tables produced by
Jane has also been disregarded. The overall aim
was to test how the SMT system performs using dif-
ferent pre-processing strategies of the training data
but without altering its mechanism. Since it is a
challenge to factor out what is really the translation
of the compounds, the overall quality of the trans-
lations at document level has been measured as an
indirect way of assessing the quality of the com-
pound translations5. To evaluate the compound
translations into Spanish, these need to be man-
ually validated because we currently do not have
access to fully automatic methods. A qualitative
analysis of the compound translations will be done
in future work.
</bodyText>
<footnote confidence="0.90045725">
4The domain C00A: Agriculture, Fishing and Foodstuffs
has 137.354 words and the domain H00: Domestic Leisure
Equipment has 58328 words).
5The results of this evaluation are reported in Section 5.
</footnote>
<subsectionHeader confidence="0.997538">
3.1 Preliminary study
</subsectionHeader>
<bodyText confidence="0.999983235294118">
With the purpose of assessing the presence of com-
pounds in the TRIS corpus and evaluating the split-
tings at a later stage as well as the impact of such
splittings in SMT, we analysed manually two short
texts of the TRIS corpus. The two files correspond
to the subcorpus B30: Construction - Environment
and account for 261 sentences and 2870 words.
For this preliminary study, all German nominal
compounds and their corresponding Spanish trans-
lations were manually extracted. Adjectival and
verbal compounds were not included at this stage.
Abbreviated nominal compounds (i.e. “EKZ” in-
stead of “Energiekennzahl”, [energy index]) were
not included either. Table 1 offers an overview of
the number of running words in each file without
punctuation, the number of nominal compounds
found (with an indication as to which percentage
of the total number of words they account for),
the number of unique compounds (i.e. compound
types), and the number of lexicalized and non lexi-
calized compounds in total (with the percentage of
the text they account for), and unique. For the pur-
poses of this study, all compounds found in a Ger-
man monolingual dictionary were considered lex-
icalized, whereas those not appearing where con-
sidered non-lexicalized.
As can be seen in Table 1, compound nominals
constitute a relatively high percentage of the total
number of words in a text. This is specially the
case of domain specific texts such as the ones taken
into consideration here. We can thus assume that
finding a way to translate compounds appropri-
ately into other languages would improve the over-
all quality of the translations produced by SMT.
</bodyText>
<subsectionHeader confidence="0.984162">
3.2 Related work: compounds in SMT
</subsectionHeader>
<bodyText confidence="0.999938714285714">
RBMT systems require that compounds are in-
cluded in their dictionaries to be able to retrieve
the appropriate translation in each case. Alterna-
tively, they should include a special rule for han-
dling compounds which are beyond their lexical
coverage. On the other hand, SMT systems en-
counter problems when dealing with compounds
because they rely on the words observed during the
training phase. Thus, if the compound did not ap-
pear in the training set of the system its translation
will subsequently fail. The state-of-the-art strat-
egy to deal with compounds in SMT systems con-
sists on splitting the compounds to reduce the num-
ber of unseen words. Previous research (Koehn
</bodyText>
<page confidence="0.956383">
50
</page>
<table confidence="0.9990995">
Text A Text B
Number of words 2431 439
Number of comp. 265 (10.9%) 62 (14.12%)
Number of unique comp. 143 25
Lexicalized comp. 99 (4.07%) 18 (4.1%)
Unique lexicalized comp. 63 4
Not lexicalized comp. 166 (6.8%) 44 (10.06%)
Unique not lexicalized comp. 80 21
</table>
<tableCaption confidence="0.999949">
Table 1: Compound nominals found in the two texts taken for the preliminary study.
</tableCaption>
<bodyText confidence="0.999867266666667">
and Knight, 2003; Popović et al., 2006; Stymne,
2008; Fritzinger and Fraser, 2010; Stymne et al.,
2013) has shown that splitting the compounds in
German results in better Bleu scores (Papineni et
al., 2001) and vocabulary coverage (fewer “un-
known” words). However, the experiments car-
ried out so far have also claimed that significant
changes in error measures were not to be expected
because the percentage of running words affected
by compound splitting was rather low (Popović et
al., 2006; Stymne, 2008). As will be observed in
Section 4.1, in our case the percentage of running
words affected by compound splitting was higher.
This might be due to the kind of texts used in our
experiments.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999834033333333">
As mentioned in Section 3, for the experi-
ments reported here two corpora have been used:
the TRIS corpus and the Europarl corpus for
German--�Spanish. In order to focus on in-domain
translation, only the largest subcorpus of TRIS has
been used.
Table 2 summarizes the number of sentences
and words in our experiment setup.
To reduce possible mistakes and mismatches ob-
served in the corpora used in the experiments, the
spelling of the German vowels named umlaut (“¨”)
was simplified. Thus, “Ä, Ö, Ü, ä, ö, ü” were trans-
formed into “Ae, Oe, Ue, ae, oe, ue” correspond-
ingly. Also the German “ß” was substituted by a
double s: “ss”. By doing this, words appearing in
the corpus and written differently were unified and
thus their frequencies were higher.
Additionally, a list of 185 German nominal com-
pounds present in the training set was manually ex-
tracted together with their translations into Span-
ish. If different translations had been found for
the same compound, these were included in our
list too. This list was used in some of our exper-
iments to determine whether extracting such lists
has an impact in the overall translation quality of
SMT systems. As the texts belong to the same
domain, there was partial overlap with the com-
pounds found in the test set. However, not all com-
pounds in the test set were present in the training
corpus and viceversa.
</bodyText>
<subsectionHeader confidence="0.9962">
4.1 Training environments
</subsectionHeader>
<bodyText confidence="0.999956612903226">
Taking the normalised version of our corpus as
a baseline, different training environments have
been tested. We designed five possible training
environments in which German compounds were
preprocessed.
In our first experiment (hereinafter “compList”),
the list of manually extracted compounds was ap-
pended to the end of the training set and no further
preprocessing was carried out.
In our second experiment (hereinafter “RWTH”),
the state-of-the-art compound splitting approach
implemented by Popović et al. (2006) was used to
split all possible compounds. As also implemented
by Koehn and Knight (2003), this approach uses
the corpus itself to create a vocabulary that is then
subsequently used to calculate the possible split-
tings in the corpus. It has the advantage of being
a stand-alone approach which does not depend on
any external resources. A possible drawback of
this approach would be that it relies on a large cor-
pus to be able to compute the splittings. Thus, it
may not be as efficient with smaller corpora (i.e. if
we were to use only the TRIS corpus, for instance).
The third experiment (hereinafter
“RWTH+compList”) used the split corpus pre-
pared in our second experiment (“RWTH”) but
merged with the list of compounds that was also
used in the first experiment. In total, 128 of all
compounds detected by the splitter were also in
our compound list. In order to avoid noise, the
compounds present in the list were deleted from
</bodyText>
<page confidence="0.988293">
51
</page>
<table confidence="0.999601">
training dev test
Sentences 1.8M 2382 1192
Running words without punctuation (tokens) 40.8M 20K 11K
Vocabulary size (types) 338K 4050 2087
</table>
<tableCaption confidence="0.918702666666667">
Table 2: Corpus statistics. The training corpus is a concatenation of the complete Europarl Corpus
German-4Spanish and a greater part of the TRIS corpus, while in dev and test only texts from the
TRIS corpus were used.
</tableCaption>
<bodyText confidence="0.998052727272727">
the list of splittings to be carried out in the corpus.
Thus, after all possible splittings were calculated,
those splittings that were present in the manually
compiled compound list were deleted to ensure
that they were not split in the corpus and remained
the same.
In the fourth experiment (hereinafter “IMS”) we
used another compound splitter developed at the
Institut für Maschinelle Sprachverarbeitung of the
University of Stuttgart (Weller and Heid, 2012).
This splitter was also developed using a frequency-
based approach. However, in this case the train-
ing data consists of a list of lemmatized word-
forms together with their POS tags. A set of rules
to model transitional elements is also used. While
this splitter might be used by processing our corpus
with available tools such as TreeTagger (Schmid,
1994)6 and then computing frequencies, in our ex-
periments we used the CELEX7 database for Ger-
man (Baayen et al., 1993). This was done so be-
cause CELEX is an extensive high quality lexical
database which already included all the informa-
tion we needed to process and did not require any
further preprocessing and clean up of our corpus.
In the fifth experiment (hereinafter
“IMS+compList”), we repeated the same procedure
of our third experiment (“RWTH+compList”): we
added the compound list to our training corpus
already split, but this time using the compound
splitter developed in Stuttgart. In total, 125
of all compounds detected by the splitter were
also in our compound list. The splitting of such
compounds was avoided.
</bodyText>
<subsectionHeader confidence="0.996578">
4.2 Compounds detected
</subsectionHeader>
<bodyText confidence="0.99993275">
Table 3 summarizes the number of compounds de-
tected by the two compound splitters and the per-
centage they account for with respect to the vocab-
ulary and the number of running words.
</bodyText>
<footnote confidence="0.990352333333333">
6http://www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger/
7http://wwwlands2.let.kun.nl/members/
</footnote>
<bodyText confidence="0.989076025">
As can be observed in Table 3, the percentage
of compounds in the test set is considerably higher
than in the training set. This is due to the fact that
in the test set only a subcorpus of the TRIS corpus
was used, whereas in the training corpus Europarl
was also used and as stated earlier (cf. Subsec-
tion 3.1 and table 1), domain specific corpora tend
to have more compounds. It is also noticeable that
the compound splitter developed in Stuttgart de-
tects and splits fewer compounds. A possible ex-
planation would be that Weller and Heid (2012)
only split words into content words and use POS
tags to filter out other highly frequent words that do
not create compounds. The presence of lexicalized
compounds in the CELEX database does not seem
to have affected the accuracy of the splitter (i.e.
they were not skipped by the splitter). Finally, it is
also noticeable that the percentage of compounds
detected in the training set is similar to the one re-
ported by Baroni et al. (2002) and referenced to in
Section 2. This seems to indicate that both splitting
algorithms perform correctly. A thorough analy-
sis of their outputs has been carried out confirm-
ing this hypothesis as the accuracies of both split-
ters were considerably high: 97.19% (RWTH) and
97.49% IMS (Parra Escartín, forthcoming)8.
As SMT system, we employ the state-of-the-
art phrase-based translation approach (Zens and
Ney, 2008) implemented in Jane. The baseline is
trained on the concatination of the TRIS and Eu-
roparl corpus. Word alignments are trained with
fastAlign (Dyer et al., 2013). Further, we apply
a 4-gram language model trained with the SRILM
toolkit (Stolcke, 2002) on the target side of the
training corpus. The log-linear parameter weights
are tuned with MERT (Och, 2003) on the develop-
ment set (dev). As optimization criterion we use
Bleu. The parameter setting for all experiments
was the same to allow for comparisons.
software/celex_gug.pdf
</bodyText>
<footnote confidence="0.8455345">
8The analysis was done following the method proposed by
Koehn and Knight (2003).
</footnote>
<page confidence="0.991706">
52
</page>
<table confidence="0.989250142857143">
Popovic et al. (2006) Weller and Heid (2012)
Compounds in training 182334 141789
% Vocabulary 54% 42%
% Running words 0.4% 0.3%
Compounds in test 924 444
% Vocabulary 44.3% 21.3%
% Running words 8.5% 4%
</table>
<tableCaption confidence="0.99751">
Table 3: Number of compounds detected by each of the splitters used and the percentages they account
</tableCaption>
<bodyText confidence="0.6065265">
for with respect to the vocabulary (types) and the number of running words (tokens) in the corpora used
in the experiments.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999954302631579">
Table 4 reports the results of the five training en-
vironments described in Subsection 4.1 and the
baseline. We report results in Bleu [%] and Ter
[%] (Snover et al., 2006). All reported results are
averages over three independent MERT runs, and
we evaluate statistical significance with MultEval
(Clark et al., 2011).
As can be observed in Table 4, adding com-
pound lists to the training set significantly im-
proves the Bleu and Ter scores with respect to the
baseline. This is also the case when compounds
were preprocessed and split. Moreover, while the
Bleu scores for both splitters are the same when
processing the entire corpus, adding the compound
list to the training corpus yields better scores. In
fact, the combination of the compound list and
the compound splitter developed by Weller and
Heid (2012) improves by 3.8 points in Bleu, while
the approach by Popović et al. (2006) improves by
3.4 Bleu points against Baseline. When comparing
it with compList, the improvements are of 3% and
2.4% Bleu respectively. To ensure a fair compar-
ison, RWTH is defined as second baseline. Again,
we observe significant improvement over this sec-
ond baseline by adding the compound list to the
training corpus. In terms of Bleu we gain an im-
provement of up to 1.4 points.
These results seem promising as they show sig-
nificant improvements both in terms of Bleu and
Ter scores. As previously mentioned in Sec-
tion 3.2, one possible explanation to the higher
Bleu scores we obtained might be that the num-
ber of running words affected by compound split-
ting was higher than in other experiments like
the ones carried out by Popović et al. (2006)
and Stymne (2008). Fritzinger and Fraser (2010)
used a hybrid splitting algorithm which combined
the corpus-based approach and linguistic infor-
mation and also reported better Bleu scores for
German-4English translations than splitting algo-
rithms based only in corpus frequencies. They sug-
gested that fewer split compounds but better split
could yield better results. However, in our case the
two splitters score the same in terms of Bleu. Fur-
ther experiments with other language pairs should
be carried out to test whether this is only the case
with German-4Spanish translation tasks or not.
If this were to be confirmed, a language depen-
dent approach to dealing with compounds in SMT
might then be needed. The improvements in terms
of Bleu and Ter obtained when adding the man-
ually extracted compound list to our training cor-
pus (particularly in the IMS+compList experiment)
suggest that further preprocessing than just split-
ting the compounds in the corpora would result
in overall better quality translations. It is par-
ticularly noticeable that while the fewest number
of unknown words occurs when using a corpus-
based splitting algorithm (experiments RWTH and
RWTH+compList), this does not seem to directly
correlate with better Bleu and Ter scores. Exper-
iments IMS and IMS+compList had in fact a larger
number of unknown words and yet obtain better
scores.
Table 5 reports the number of compounds of the
compound list found in the test sets across the dif-
ferent experiments. As the compound list was not
preprocessed, the number of compounds found in
RWTH and IMS is smaller than those found in Base-
line and compList. In the case of RWTH+compList
and IMS+compList, however, the productivity of
German compounds mentioned earlier in Section 2
may have influenced the number of compounds
found. If a compound found in our compound list
was present in other compounds and those were
split in such a way that it resulted in one of the
</bodyText>
<page confidence="0.996151">
53
</page>
<table confidence="0.999046375">
Experiment Splitting Method Compound List Bleu1%] test OOVs
Ter1%]
Baseline - no 45.9 43.9 181
compList - yes 46.7 42.9 169
RWTH Popović et al. (2006) no 48.3 40.8 104
RWTH+compList yes 49.1 40.5 104
ImS Weller and Heid (2012) no 48.3 40.5 114
ImS+compList yes 49.7 39.2 114
</table>
<tableCaption confidence="0.983801">
Table 4: Results for the German-4Spanish TRIS data. Statistically significant improvements with at least
</tableCaption>
<bodyText confidence="0.9290072">
99% confidence over the respective baselines (Baseline and RWTH) are printed in boldface.
formants being that compound, its frequency got
higher. As can be observed, the highest number of
correct translations of compounds corresponds to
RWTH+compList and ImS+compList.
Table 6 shows the results of a sample sentence
in our test set including several compounds. As
can be observed, in the ImS+compList experiment
all compounds are correctly translated. This seems
to indicate that the manually compiled list of com-
pounds added to the training corpus helped to in-
crease the probabilities of alignment of 1:n corre-
spondences (German compound – Spanish MWE)
and thus the compound translations in the phrase
tables are better.
</bodyText>
<sectionHeader confidence="0.972399" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999982868421053">
In this paper, we have reported the results of our
experiments processing German compounds and
carrying out SMT tasks into Spanish. As has been
observed, adding manually handcrafted compound
lists to the training set significantly improves the
qualitative results of SMT and therefore a way of
automating their extraction would be desired. Fur-
thermore, a combination of splitting compounds
and adding them already aligned to their transla-
tions in the training corpus yields also significant
improvements with respect to the baseline. A qual-
itative analysis is currently being done to assess the
kind of improvements that come from the splitting
and/or the compound list added to training.
As a follow up of the experiments reported here,
the compound splitters used have being evaluated
to assess their precision and recall and determine
which splitting algorithms could be more promis-
ing for SMT tasks and whether or not their quality
has a correlation with better translations. From the
experiments carried out so far, it seems that it may
be the case, but this shall be further explored as our
results do not differ greatly between each other.
In future work we will research whether the ap-
proach suggested here also yields better results in
data used by the MT community. Obtaining bet-
ter overall results would confirm that our approach
is right, in which case we will research how we
can combine both strategies (compound splitting
and adding compound lists and their translations
to training corpora) in a successful and automatic
way. We also intend to explore how we can do
so minimizing the amount of external resources
needed.
Obtaining positive results in these further ex-
periments would suggest that a similar approach
may also yield positive results in dealing with other
types of MWEs within SMT.
</bodyText>
<sectionHeader confidence="0.990764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999824">
The research reported in this paper has received
funding from the EU under FP7, Marie Curie Ac-
tions, SP3 People ITN, grant agreement no 238405
(project CLARA9). The authors would also like to
thank the anonymous reviewers for their valuable
comments.
</bodyText>
<sectionHeader confidence="0.998129" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.968004">
Sybille Angele. 1992. Nominalkomposita des
Deutschen und ihre Entsprechungen im Spanischen.
Eine kontrastive Untersuchung anhand von Tex-
ten aus Wirtschaft und Literatur. iudicium verlag
GmbH, München.
S. Atkins, N. Bel, P. Bouillon, T. Charoenporn, D. Gib-
bon, R. Grishman, C.-R. Huan, A. Kawtrakul, N. Ide,
H.-Y. Lee, P. J. K. Li, J. McNaught, J. Odijk,
M. Palmer, V. Quochi, R. Reeves, D. M. Sharma,
V. Sornlertlamvanich, T. Tokunaga, G. Thurmair,
M. Villegas, A. Zampolli, and E. Zeiton. 2001. Stan-
dards and Best Practice for Multiligual Computa-
tional Lexicons. MILE (the Multilingual ISLE Lex-
</reference>
<footnote confidence="0.893251">
9http://clara.uib.no
</footnote>
<page confidence="0.994006">
54
</page>
<figure confidence="0.449761285714286">
Experiment Compounds (DE) Compound translations (ES)
Baseline 154 48
compList 154 54
RWTH 85 61
RWTH+compList 175 80
IMS 46 57
IMS+compList 173 76
</figure>
<tableCaption confidence="0.990635">
Table 5: Number of compounds present in our compound list found in the test set for each of the experi-
ments both in German and in Spanish. The experiments with the highest number of translations present
in our compound list are printed in boldface.
</tableCaption>
<table confidence="0.998724142857143">
Sentence type Example
Original (DE) Abstellanlagen fuer Kraftfahrzeuge in Tiefgaragen oder in Parkdecks
Reference (ES)
mit mindestens zwei Geschossen
instalaciones de estacionamiento de automóviles en garajes subterráneos
o en estacionamientos cubiertos que tengan como mínimo dos plantas
Baseline (DE) Abstellanlagen fuer Kraftfahrzeuge in Tiefgaragen oder in Parkdecks
Baseline (ES)
mit mindestens zwei Geschossen
plazas para vehículos en aparcamientos subterráneos o en plantas
con al menos dos pisos
IMS (DE) abstellen Anlagen fuer Kraft fahren Zeuge in tief Garagen oder in Park Decks
IMS (ES)
mit mindestens zwei Geschossen
plazas para vehículos en aparcamientos subterráneos o en plantas
con al menos dos pisos
IMS+compList (DE) Abstellanlagen fuer Kraftfahrzeuge in Tiefgaragen oder in Parkdecks
IMS+compList (ES)
mit mindestens zwei Geschossen
instalaciones de estacionamiento para automóviles estacionamientos cubiertos
en garajes subterráneos o en plantas con al menos dos pisos
</table>
<tableCaption confidence="0.8512195">
Table 6: Sample translations for German-4Spanish for the baseline and the experiments IMS and
IMS+compList. Each compound and its translation have the same format.
</tableCaption>
<reference confidence="0.9958759375">
ical Entry) Deliverable D2.2-D3.2. ISLE project:
ISLE Computational Lexicon Working Group.
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX Lexical Database (CD-ROM). Linguis-
tic Data Consortium, University of Pennsylvania,
Philadelphia, PA.
Marco Baroni, Johannes Matiasek, and Harald Trost.
2002. Wordform- and Class-based Prediction of the
Components of German Nominal Compounds in an
AAC System. In 19th International Conference on
Computational Linguistics, COLING 2002, Taipei,
Taiwan, August 24 - September 1, 2002.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176—181, Portland, Ore-
gon, June.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proc. of NAACL.
Ludwig M. Eichinger. 2000. Deutsche Wortbildung.
Eine Einführung. Gunter Narr Verlag Tübingen.
Wolfgang Fleischer. 1975. Wortbildung der deutschen
Gegenwartssprache. Max Niemeyer Verlag Tübin-
gen, 4 edition.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, WMT ’10, pages 224–234, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun
compounds. Computer Speech and Language,
(4):479–496.
Roxana Girju. 2008. The Syntax and Semantics
of Prepositions in the Task of Automatic Inter-
pretation of Nominal Phrases and Compounds: A
Cross-Linguistic Study. Computational Linguistics,
35(2):185–228.
Carmen Gómez Pérez. 2001. La composición nominal
alemana desde la perspectiva textual: El compuesto
nominal como dificultad de traducción del alemán al
español. Ph.D. thesis, Departamento de Traducción
</reference>
<page confidence="0.991965">
55
</page>
<reference confidence="0.998112345454546">
e Interpretación, Universidad de Salamanca, Sala-
manca.
Philipp Koehn and Kevin Knight. 2003. Empiri-
cal Methods for Compound splitting. In Proceed-
ings of the Tenth Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 187–193, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the Tenth Machine Translation Sum-
mit, pages 79–86, Phuket, Thailand.
Stefan Langer. 1998. Zur morphologie und seman-
tik von nominalkomposita. In Tagungsband der
4. Konferenz zur Verarbeitung natürlicher Sprache
(KOVENS).
Torsten Marek. 2006. Analysis of German Compounds
Using Weighted Finite State Transducers. Technical
report, Eberhard-Karls-Universität Tübingen.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. pages 160–167,
Sapporo, Japan, July.
Ulrike Oster. 2003. Los términos de la cerámica en
alemán y en español. Análisis semántico orientado
a la traducción de los compuestos nominales ale-
manes. Ph.D. thesis, Departament de Traducció i
Comunicació, Universitat Jaume I, Castellón.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. IBM Research
Report RC22176 (W0109-022), IBM Research Divi-
sion, Thomas J. Watson Research Center, P.O. Box
218, Yorktown Heights, NY 10598, September.
Carla Parra Escartín. 2012. Design and compila-
tion of a specialized Spanish-German parallel cor-
pus. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC’12), Istanbul, Turkey, May. European Lan-
guage Resources Association.
Carla Parra Escartín. forthcoming. Chasing the perfect
splitter: A comparison of different compound split-
ting tools. In Proceedings of the Ninth Conference
on International Language Resources and Evalua-
tion (LREC’14), Reykjavik, Island, May. European
Language Resources Association.
Maja Popović, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of german compound
words. In Proceedings of the 5th international con-
ference on Advances in Natural Language Process-
ing, FinTAL’06, pages 616–624, Berlin, Heidelberg.
Springer-Verlag.
Jutta Ransmayr, Karlheinz Moerth, and Matej Durco.
2013. Linguistic variation in the austrian media cor-
pus. dealing with the challenges of large amounts of
data. In Proceedings of International Conference on
Corpus Linguistics (CILC), Alicante. University Al-
icante, University Alicante.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, pages 44–49, Manchester, UK.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223–231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901–904, Denver, CO, September.
Sara Stymne, Nicola Cancedda, and Lars Ahrenberg.
2013. Generation of Compound Words in Statistical
Machine Translation into Compounding Languages.
Computational Linguistics, pages 1–42.
Sara Stymne. 2008. German Compounds in Fac-
tored Statistical Machine Translation. In GoTAL’08:
Proceedings of the 6th international conference on
Advances in Natural Language Processing, pages
464–475. Springer-Verlag.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ’10, pages 262–270, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Marion Weller and Ulrich Heid. 2012. Analyzing
and Aligning German compound nouns. In Pro-
ceedings of the Eight International Conference on
Language Resources and Evaluation (LREC’12), Is-
tanbul, Turkey, May. European Language Resources
Association.
Hans Wellman, 1984. DUDEN. Die Grammatik. Un-
entbehrlich für richtiges Deutsch, volume 4, chapter
Die Wortbildung. Duden Verlag.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483–491, Mum-
bai, India, December.
Richard Zens and Hermann Ney. 2008. Improvements
in Dynamic Programming Beam Search for Phrase-
based Statistical Machine Translation. In Interna-
tional Workshop on Spoken Language Translation,
pages 195–205, Honolulu, Hawaii, October.
</reference>
<page confidence="0.998411">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622202">
<title confidence="0.914163">German Compounds and Statistical Machine Can they get along?</title>
<author confidence="0.99991">Carla Parra Escartín Stephan Peitz Hermann Ney</author>
<affiliation confidence="0.999999">University of Bergen RWTH Aachen University RWTH Aachen University</affiliation>
<address confidence="0.934042">Bergen, Norway Aachen, Germany Aachen,</address>
<email confidence="0.907984">carla.parra@uib.nopeitz@cs.rwth-aachen.deney@cs.rwth-aachen.de</email>
<abstract confidence="0.994166423076923">This paper reports different experiments created to study the impact of using linguistics to preprocess German compounds prior to translation in Statistical Translation Compounds are a known challenge both in Machine and Translation in general as well as in other Natural Language applications. In the case German compounds are split into their constituents to decrease the number of unknown words and improve the results of evaluation measures like the Bleu score. To assess to which extent it is necessary to deal with German compounds as a of preprocessing in we have tested different compound splitters and strategies, such as adding lists of compounds and their translations to the training set. This paper summarizes the results of our experiments and attempts to yield better translations of German nominal compounds into Spanish and shows how our approach improves by up to 1.4 Bleu points with respect to the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sybille Angele</author>
</authors>
<date>1992</date>
<booktitle>Nominalkomposita des Deutschen und ihre Entsprechungen im Spanischen. Eine kontrastive Untersuchung anhand von Texten aus Wirtschaft und Literatur. iudicium verlag GmbH,</booktitle>
<location>München.</location>
<contexts>
<context position="2786" citStr="Angele (1992)" startWordPosition="428" endWordPosition="430">nungssysteme and their translations into English and Spanish. (1) Warm Wasser Bereitung caliente agua preparaci6n warm water production [ES]: ‘Preparaci6n de agua caliente’ [EN]: ‘Warm water production’ (2) Wärme Rückgewinnung s Systeme calor recuperaci6n Ø sistemas heat recovery Ø Systems [ES]: ‘sistemas de recuperaci6n de calor’ [EN]: ‘heat recovery systems’ As may be observed in Examples 1 and 2, in Spanish not only there is word reordering, but also there is usage of other word categories such as prepositions. While the examples above are quite simple, the work done by researchers such as Angele (1992), G6mez Pérez (2001) and Oster (2003) for the pair of languages German-4Spanish shows that the translational equivalences in Spanish not only are very varied, but also unpredictable to a certain extent. Thus, while a mere compound splitting strategy may work for English, in the case of Spanish further processing is required to yield the correct translation. According to Atkins et al. (2001)1, complex nominals (i.e. nominal compounds and some nominal phrases) are to be considered a special type of MWE because they do have some particular features and to some extent they behave as a single unit </context>
</contexts>
<marker>Angele, 1992</marker>
<rawString>Sybille Angele. 1992. Nominalkomposita des Deutschen und ihre Entsprechungen im Spanischen. Eine kontrastive Untersuchung anhand von Texten aus Wirtschaft und Literatur. iudicium verlag GmbH, München.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Atkins</author>
<author>N Bel</author>
<author>P Bouillon</author>
<author>T Charoenporn</author>
<author>D Gibbon</author>
<author>R Grishman</author>
<author>C-R Huan</author>
<author>A Kawtrakul</author>
<author>N Ide</author>
<author>H-Y Lee</author>
<author>P J K Li</author>
<author>J McNaught</author>
<author>J Odijk</author>
<author>M Palmer</author>
<author>V Quochi</author>
<author>R Reeves</author>
<author>D M Sharma</author>
<author>V Sornlertlamvanich</author>
<author>T Tokunaga</author>
<author>G Thurmair</author>
<author>M Villegas</author>
<author>A Zampolli</author>
<author>E Zeiton</author>
</authors>
<date>2001</date>
<booktitle>Standards and Best Practice for Multiligual Computational Lexicons. MILE (the Multilingual ISLE Lexical Entry) Deliverable D2.2-D3.2. ISLE project: ISLE Computational Lexicon Working Group.</booktitle>
<contexts>
<context position="3179" citStr="Atkins et al. (2001)" startWordPosition="490" endWordPosition="493">xamples 1 and 2, in Spanish not only there is word reordering, but also there is usage of other word categories such as prepositions. While the examples above are quite simple, the work done by researchers such as Angele (1992), G6mez Pérez (2001) and Oster (2003) for the pair of languages German-4Spanish shows that the translational equivalences in Spanish not only are very varied, but also unpredictable to a certain extent. Thus, while a mere compound splitting strategy may work for English, in the case of Spanish further processing is required to yield the correct translation. According to Atkins et al. (2001)1, complex nominals (i.e. nominal compounds and some nominal phrases) are to be considered a special type of MWE because they do have some particular features and to some extent they behave as a single unit because they refer to a single concept. Despite focusing on another language pair 1Appendix F of Deliverable D2.2-D3.2 of the ISLE project. Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 48–56, Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics (English-4Italian), in the case of our language pair (German-4Spanish) a similar cl</context>
</contexts>
<marker>Atkins, Bel, Bouillon, Charoenporn, Gibbon, Grishman, Huan, Kawtrakul, Ide, Lee, Li, McNaught, Odijk, Palmer, Quochi, Reeves, Sharma, Sornlertlamvanich, Tokunaga, Thurmair, Villegas, Zampolli, Zeiton, 2001</marker>
<rawString>S. Atkins, N. Bel, P. Bouillon, T. Charoenporn, D. Gibbon, R. Grishman, C.-R. Huan, A. Kawtrakul, N. Ide, H.-Y. Lee, P. J. K. Li, J. McNaught, J. Odijk, M. Palmer, V. Quochi, R. Reeves, D. M. Sharma, V. Sornlertlamvanich, T. Tokunaga, G. Thurmair, M. Villegas, A. Zampolli, and E. Zeiton. 2001. Standards and Best Practice for Multiligual Computational Lexicons. MILE (the Multilingual ISLE Lexical Entry) Deliverable D2.2-D3.2. ISLE project: ISLE Computational Lexicon Working Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX Lexical Database (CD-ROM). Linguistic Data</booktitle>
<institution>Consortium, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. The CELEX Lexical Database (CD-ROM). Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Johannes Matiasek</author>
<author>Harald Trost</author>
</authors>
<title>Wordform- and Class-based Prediction of the Components of German Nominal Compounds in an AAC System.</title>
<date>2002</date>
<booktitle>In 19th International Conference on Computational Linguistics, COLING 2002,</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="7280" citStr="Baroni et al. (2002)" startWordPosition="1142" endWordPosition="1145">“s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding to the domain B00: Construction of the TRIS corpus (Parra Escartín, 2012), and an internally compiled version of the Europarl Corpus (Koehn, 2005) for the pair o</context>
<context position="20100" citStr="Baroni et al. (2002)" startWordPosition="3274" endWordPosition="3277">re compounds. It is also noticeable that the compound splitter developed in Stuttgart detects and splits fewer compounds. A possible explanation would be that Weller and Heid (2012) only split words into content words and use POS tags to filter out other highly frequent words that do not create compounds. The presence of lexicalized compounds in the CELEX database does not seem to have affected the accuracy of the splitter (i.e. they were not skipped by the splitter). Finally, it is also noticeable that the percentage of compounds detected in the training set is similar to the one reported by Baroni et al. (2002) and referenced to in Section 2. This seems to indicate that both splitting algorithms perform correctly. A thorough analysis of their outputs has been carried out confirming this hypothesis as the accuracies of both splitters were considerably high: 97.19% (RWTH) and 97.49% IMS (Parra Escartín, forthcoming)8. As SMT system, we employ the state-of-theart phrase-based translation approach (Zens and Ney, 2008) implemented in Jane. The baseline is trained on the concatination of the TRIS and Europarl corpus. Word alignments are trained with fastAlign (Dyer et al., 2013). Further, we apply a 4-gra</context>
</contexts>
<marker>Baroni, Matiasek, Trost, 2002</marker>
<rawString>Marco Baroni, Johannes Matiasek, and Harald Trost. 2002. Wordform- and Class-based Prediction of the Components of German Nominal Compounds in an AAC System. In 19th International Conference on Computational Linguistics, COLING 2002, Taipei, Taiwan, August 24 - September 1, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers,</booktitle>
<pages>176--181</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="21883" citStr="Clark et al., 2011" startWordPosition="3565" endWordPosition="3568">ds in test 924 444 % Vocabulary 44.3% 21.3% % Running words 8.5% 4% Table 3: Number of compounds detected by each of the splitters used and the percentages they account for with respect to the vocabulary (types) and the number of running words (tokens) in the corpora used in the experiments. 5 Results Table 4 reports the results of the five training environments described in Subsection 4.1 and the baseline. We report results in Bleu [%] and Ter [%] (Snover et al., 2006). All reported results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). As can be observed in Table 4, adding compound lists to the training set significantly improves the Bleu and Ter scores with respect to the baseline. This is also the case when compounds were preprocessed and split. Moreover, while the Bleu scores for both splitters are the same when processing the entire corpus, adding the compound list to the training corpus yields better scores. In fact, the combination of the compound list and the compound splitter developed by Weller and Heid (2012) improves by 3.8 points in Bleu, while the approach by Popović et al. (2006) improves by 3.4 Bleu points a</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176—181, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of ibm model 2. In</title>
<date>2013</date>
<booktitle>Proc. of NAACL.</booktitle>
<contexts>
<context position="20673" citStr="Dyer et al., 2013" startWordPosition="3365" endWordPosition="3368">r to the one reported by Baroni et al. (2002) and referenced to in Section 2. This seems to indicate that both splitting algorithms perform correctly. A thorough analysis of their outputs has been carried out confirming this hypothesis as the accuracies of both splitters were considerably high: 97.19% (RWTH) and 97.49% IMS (Parra Escartín, forthcoming)8. As SMT system, we employ the state-of-theart phrase-based translation approach (Zens and Ney, 2008) implemented in Jane. The baseline is trained on the concatination of the TRIS and Europarl corpus. Word alignments are trained with fastAlign (Dyer et al., 2013). Further, we apply a 4-gram language model trained with the SRILM toolkit (Stolcke, 2002) on the target side of the training corpus. The log-linear parameter weights are tuned with MERT (Och, 2003) on the development set (dev). As optimization criterion we use Bleu. The parameter setting for all experiments was the same to allow for comparisons. software/celex_gug.pdf 8The analysis was done following the method proposed by Koehn and Knight (2003). 52 Popovic et al. (2006) Weller and Heid (2012) Compounds in training 182334 141789 % Vocabulary 54% 42% % Running words 0.4% 0.3% Compounds in tes</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludwig M Eichinger</author>
</authors>
<title>Deutsche Wortbildung. Eine Einführung. Gunter Narr Verlag Tübingen.</title>
<date>2000</date>
<contexts>
<context position="7080" citStr="Eichinger, 2000" startWordPosition="1108" endWordPosition="1109">tive warm and the noun Wasser yields a nominal compound). Finally, it is also important to highlight that besides words, compounds may also include particles to join those words together, as the “s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out </context>
</contexts>
<marker>Eichinger, 2000</marker>
<rawString>Ludwig M. Eichinger. 2000. Deutsche Wortbildung. Eine Einführung. Gunter Narr Verlag Tübingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Fleischer</author>
</authors>
<title>Wortbildung der deutschen Gegenwartssprache.</title>
<date>1975</date>
<journal>Max Niemeyer Verlag Tübingen,</journal>
<volume>4</volume>
<pages>edition.</pages>
<contexts>
<context position="7048" citStr="Fleischer, 1975" startWordPosition="1104" endWordPosition="1105">.e. the combination of the adjective warm and the noun Wasser yields a nominal compound). Finally, it is also important to highlight that besides words, compounds may also include particles to join those words together, as the “s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case St</context>
</contexts>
<marker>Fleischer, 1975</marker>
<rawString>Wolfgang Fleischer. 1975. Wortbildung der deutschen Gegenwartssprache. Max Niemeyer Verlag Tübingen, 4 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Fritzinger</author>
<author>Alexander Fraser</author>
</authors>
<title>How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>224--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13450" citStr="Fritzinger and Fraser, 2010" startWordPosition="2170" endWordPosition="2173">ranslation will subsequently fail. The state-of-the-art strategy to deal with compounds in SMT systems consists on splitting the compounds to reduce the number of unseen words. Previous research (Koehn 50 Text A Text B Number of words 2431 439 Number of comp. 265 (10.9%) 62 (14.12%) Number of unique comp. 143 25 Lexicalized comp. 99 (4.07%) 18 (4.1%) Unique lexicalized comp. 63 4 Not lexicalized comp. 166 (6.8%) 44 (10.06%) Unique not lexicalized comp. 80 21 Table 1: Compound nominals found in the two texts taken for the preliminary study. and Knight, 2003; Popović et al., 2006; Stymne, 2008; Fritzinger and Fraser, 2010; Stymne et al., 2013) has shown that splitting the compounds in German results in better Bleu scores (Papineni et al., 2001) and vocabulary coverage (fewer “unknown” words). However, the experiments carried out so far have also claimed that significant changes in error measures were not to be expected because the percentage of running words affected by compound splitting was rather low (Popović et al., 2006; Stymne, 2008). As will be observed in Section 4.1, in our case the percentage of running words affected by compound splitting was higher. This might be due to the kind of texts used in ou</context>
<context position="23250" citStr="Fritzinger and Fraser (2010)" startWordPosition="3801" endWordPosition="3804">is defined as second baseline. Again, we observe significant improvement over this second baseline by adding the compound list to the training corpus. In terms of Bleu we gain an improvement of up to 1.4 points. These results seem promising as they show significant improvements both in terms of Bleu and Ter scores. As previously mentioned in Section 3.2, one possible explanation to the higher Bleu scores we obtained might be that the number of running words affected by compound splitting was higher than in other experiments like the ones carried out by Popović et al. (2006) and Stymne (2008). Fritzinger and Fraser (2010) used a hybrid splitting algorithm which combined the corpus-based approach and linguistic information and also reported better Bleu scores for German-4English translations than splitting algorithms based only in corpus frequencies. They suggested that fewer split compounds but better split could yield better results. However, in our case the two splitters score the same in terms of Bleu. Further experiments with other language pairs should be carried out to test whether this is only the case with German-4Spanish translation tasks or not. If this were to be confirmed, a language dependent appr</context>
</contexts>
<marker>Fritzinger, Fraser, 2010</marker>
<rawString>Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 224–234, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="7140" citStr="Girju et al., 2005" startWordPosition="1117" endWordPosition="1120"> Finally, it is also important to highlight that besides words, compounds may also include particles to join those words together, as the “s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding to the domain B00: Co</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, (4):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>The Syntax and Semantics of Prepositions in the Task of Automatic Interpretation of Nominal Phrases and Compounds: A Cross-Linguistic Study.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>2</issue>
<contexts>
<context position="7166" citStr="Girju, 2008" startWordPosition="1123" endWordPosition="1124"> highlight that besides words, compounds may also include particles to join those words together, as the “s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding to the domain B00: Construction of the TRIS cor</context>
</contexts>
<marker>Girju, 2008</marker>
<rawString>Roxana Girju. 2008. The Syntax and Semantics of Prepositions in the Task of Automatic Interpretation of Nominal Phrases and Compounds: A Cross-Linguistic Study. Computational Linguistics, 35(2):185–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Gómez Pérez</author>
</authors>
<title>La composición nominal alemana desde la perspectiva textual: El compuesto nominal como dificultad de traducción del alemán al español.</title>
<date>2001</date>
<booktitle>Ph.D. thesis, Departamento de Traducción e Interpretación, Universidad de Salamanca,</booktitle>
<location>Salamanca.</location>
<contexts>
<context position="2806" citStr="Pérez (2001)" startWordPosition="432" endWordPosition="433">r translations into English and Spanish. (1) Warm Wasser Bereitung caliente agua preparaci6n warm water production [ES]: ‘Preparaci6n de agua caliente’ [EN]: ‘Warm water production’ (2) Wärme Rückgewinnung s Systeme calor recuperaci6n Ø sistemas heat recovery Ø Systems [ES]: ‘sistemas de recuperaci6n de calor’ [EN]: ‘heat recovery systems’ As may be observed in Examples 1 and 2, in Spanish not only there is word reordering, but also there is usage of other word categories such as prepositions. While the examples above are quite simple, the work done by researchers such as Angele (1992), G6mez Pérez (2001) and Oster (2003) for the pair of languages German-4Spanish shows that the translational equivalences in Spanish not only are very varied, but also unpredictable to a certain extent. Thus, while a mere compound splitting strategy may work for English, in the case of Spanish further processing is required to yield the correct translation. According to Atkins et al. (2001)1, complex nominals (i.e. nominal compounds and some nominal phrases) are to be considered a special type of MWE because they do have some particular features and to some extent they behave as a single unit because they refer t</context>
</contexts>
<marker>Pérez, 2001</marker>
<rawString>Carmen Gómez Pérez. 2001. La composición nominal alemana desde la perspectiva textual: El compuesto nominal como dificultad de traducción del alemán al español. Ph.D. thesis, Departamento de Traducción e Interpretación, Universidad de Salamanca, Salamanca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>187--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="16069" citStr="Koehn and Knight (2003)" startWordPosition="2603" endWordPosition="2606">ining environments Taking the normalised version of our corpus as a baseline, different training environments have been tested. We designed five possible training environments in which German compounds were preprocessed. In our first experiment (hereinafter “compList”), the list of manually extracted compounds was appended to the end of the training set and no further preprocessing was carried out. In our second experiment (hereinafter “RWTH”), the state-of-the-art compound splitting approach implemented by Popović et al. (2006) was used to split all possible compounds. As also implemented by Koehn and Knight (2003), this approach uses the corpus itself to create a vocabulary that is then subsequently used to calculate the possible splittings in the corpus. It has the advantage of being a stand-alone approach which does not depend on any external resources. A possible drawback of this approach would be that it relies on a large corpus to be able to compute the splittings. Thus, it may not be as efficient with smaller corpora (i.e. if we were to use only the TRIS corpus, for instance). The third experiment (hereinafter “RWTH+compList”) used the split corpus prepared in our second experiment (“RWTH”) but m</context>
<context position="21124" citStr="Koehn and Knight (2003)" startWordPosition="3436" endWordPosition="3439">Ney, 2008) implemented in Jane. The baseline is trained on the concatination of the TRIS and Europarl corpus. Word alignments are trained with fastAlign (Dyer et al., 2013). Further, we apply a 4-gram language model trained with the SRILM toolkit (Stolcke, 2002) on the target side of the training corpus. The log-linear parameter weights are tuned with MERT (Och, 2003) on the development set (dev). As optimization criterion we use Bleu. The parameter setting for all experiments was the same to allow for comparisons. software/celex_gug.pdf 8The analysis was done following the method proposed by Koehn and Knight (2003). 52 Popovic et al. (2006) Weller and Heid (2012) Compounds in training 182334 141789 % Vocabulary 54% 42% % Running words 0.4% 0.3% Compounds in test 924 444 % Vocabulary 44.3% 21.3% % Running words 8.5% 4% Table 3: Number of compounds detected by each of the splitters used and the percentages they account for with respect to the vocabulary (types) and the number of running words (tokens) in the corpora used in the experiments. 5 Results Table 4 reports the results of the five training environments described in Subsection 4.1 and the baseline. We report results in Bleu [%] and Ter [%] (Snover</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound splitting. In Proceedings of the Tenth Conference of the European Chapter of the Association for Computational Linguistics, pages 187–193, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the Tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="7865" citStr="Koehn, 2005" startWordPosition="1248" endWordPosition="1249">n SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding to the domain B00: Construction of the TRIS corpus (Parra Escartín, 2012), and an internally compiled version of the Europarl Corpus (Koehn, 2005) for the pair of languages German-Spanish3. The domain (B00: Construction) was selected because it is the biggest one of 2Corpus of the Austria Presse Agentur (APA). Recently it has been released as the AMC corpus (Austrian Media Corpus) (Ransmayr et al., 2013). 3See Table 2 for an overview of the corpus statistics. 49 the three domains currently available in the TRIS corpus4. Only one domain was used because we aimed at testing in-domain translation. Besides, the TRIS corpus was selected because it is a specialised German-Spanish parallel corpus. As opposed to the Europarl, the TRIS corpus is</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the Tenth Machine Translation Summit, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Langer</author>
</authors>
<title>Zur morphologie und semantik von nominalkomposita.</title>
<date>1998</date>
<booktitle>In Tagungsband der 4. Konferenz zur Verarbeitung natürlicher Sprache (KOVENS).</booktitle>
<contexts>
<context position="7120" citStr="Langer, 1998" startWordPosition="1115" endWordPosition="1116">nal compound). Finally, it is also important to highlight that besides words, compounds may also include particles to join those words together, as the “s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding t</context>
</contexts>
<marker>Langer, 1998</marker>
<rawString>Stefan Langer. 1998. Zur morphologie und semantik von nominalkomposita. In Tagungsband der 4. Konferenz zur Verarbeitung natürlicher Sprache (KOVENS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Marek</author>
</authors>
<title>Analysis of German Compounds Using Weighted Finite State Transducers.</title>
<date>2006</date>
<tech>Technical report, Eberhard-Karls-Universität Tübingen.</tech>
<contexts>
<context position="7153" citStr="Marek, 2006" startWordPosition="1121" endWordPosition="1122"> important to highlight that besides words, compounds may also include particles to join those words together, as the “s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding to the domain B00: Construction of</context>
</contexts>
<marker>Marek, 2006</marker>
<rawString>Torsten Marek. 2006. Analysis of German Compounds Using Weighted Finite State Transducers. Technical report, Eberhard-Karls-Universität Tübingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="20871" citStr="Och, 2003" startWordPosition="3399" endWordPosition="3400">ut confirming this hypothesis as the accuracies of both splitters were considerably high: 97.19% (RWTH) and 97.49% IMS (Parra Escartín, forthcoming)8. As SMT system, we employ the state-of-theart phrase-based translation approach (Zens and Ney, 2008) implemented in Jane. The baseline is trained on the concatination of the TRIS and Europarl corpus. Word alignments are trained with fastAlign (Dyer et al., 2013). Further, we apply a 4-gram language model trained with the SRILM toolkit (Stolcke, 2002) on the target side of the training corpus. The log-linear parameter weights are tuned with MERT (Och, 2003) on the development set (dev). As optimization criterion we use Bleu. The parameter setting for all experiments was the same to allow for comparisons. software/celex_gug.pdf 8The analysis was done following the method proposed by Koehn and Knight (2003). 52 Popovic et al. (2006) Weller and Heid (2012) Compounds in training 182334 141789 % Vocabulary 54% 42% % Running words 0.4% 0.3% Compounds in test 924 444 % Vocabulary 44.3% 21.3% % Running words 8.5% 4% Table 3: Number of compounds detected by each of the splitters used and the percentages they account for with respect to the vocabulary (ty</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Oster</author>
</authors>
<title>Los términos de la cerámica en alemán y en español. Análisis semántico orientado a la traducción de los compuestos nominales alemanes.</title>
<date>2003</date>
<booktitle>Ph.D. thesis, Departament de Traducció i Comunicació, Universitat Jaume I, Castellón.</booktitle>
<contexts>
<context position="2823" citStr="Oster (2003)" startWordPosition="435" endWordPosition="436">to English and Spanish. (1) Warm Wasser Bereitung caliente agua preparaci6n warm water production [ES]: ‘Preparaci6n de agua caliente’ [EN]: ‘Warm water production’ (2) Wärme Rückgewinnung s Systeme calor recuperaci6n Ø sistemas heat recovery Ø Systems [ES]: ‘sistemas de recuperaci6n de calor’ [EN]: ‘heat recovery systems’ As may be observed in Examples 1 and 2, in Spanish not only there is word reordering, but also there is usage of other word categories such as prepositions. While the examples above are quite simple, the work done by researchers such as Angele (1992), G6mez Pérez (2001) and Oster (2003) for the pair of languages German-4Spanish shows that the translational equivalences in Spanish not only are very varied, but also unpredictable to a certain extent. Thus, while a mere compound splitting strategy may work for English, in the case of Spanish further processing is required to yield the correct translation. According to Atkins et al. (2001)1, complex nominals (i.e. nominal compounds and some nominal phrases) are to be considered a special type of MWE because they do have some particular features and to some extent they behave as a single unit because they refer to a single concep</context>
</contexts>
<marker>Oster, 2003</marker>
<rawString>Ulrike Oster. 2003. Los términos de la cerámica en alemán y en español. Análisis semántico orientado a la traducción de los compuestos nominales alemanes. Ph.D. thesis, Departament de Traducció i Comunicació, Universitat Jaume I, Castellón.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, P.O. Box</journal>
<volume>218</volume>
<pages>10598</pages>
<location>Yorktown Heights, NY</location>
<contexts>
<context position="13575" citStr="Papineni et al., 2001" startWordPosition="2191" endWordPosition="2194">ompounds to reduce the number of unseen words. Previous research (Koehn 50 Text A Text B Number of words 2431 439 Number of comp. 265 (10.9%) 62 (14.12%) Number of unique comp. 143 25 Lexicalized comp. 99 (4.07%) 18 (4.1%) Unique lexicalized comp. 63 4 Not lexicalized comp. 166 (6.8%) 44 (10.06%) Unique not lexicalized comp. 80 21 Table 1: Compound nominals found in the two texts taken for the preliminary study. and Knight, 2003; Popović et al., 2006; Stymne, 2008; Fritzinger and Fraser, 2010; Stymne et al., 2013) has shown that splitting the compounds in German results in better Bleu scores (Papineni et al., 2001) and vocabulary coverage (fewer “unknown” words). However, the experiments carried out so far have also claimed that significant changes in error measures were not to be expected because the percentage of running words affected by compound splitting was rather low (Popović et al., 2006; Stymne, 2008). As will be observed in Section 4.1, in our case the percentage of running words affected by compound splitting was higher. This might be due to the kind of texts used in our experiments. 4 Experiments As mentioned in Section 3, for the experiments reported here two corpora have been used: the TRI</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carla Parra Escartín</author>
</authors>
<title>Design and compilation of a specialized Spanish-German parallel corpus.</title>
<date>2012</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="7792" citStr="Escartín, 2012" startWordPosition="1237" endWordPosition="1238">ers). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding to the domain B00: Construction of the TRIS corpus (Parra Escartín, 2012), and an internally compiled version of the Europarl Corpus (Koehn, 2005) for the pair of languages German-Spanish3. The domain (B00: Construction) was selected because it is the biggest one of 2Corpus of the Austria Presse Agentur (APA). Recently it has been released as the AMC corpus (Austrian Media Corpus) (Ransmayr et al., 2013). 3See Table 2 for an overview of the corpus statistics. 49 the three domains currently available in the TRIS corpus4. Only one domain was used because we aimed at testing in-domain translation. Besides, the TRIS corpus was selected because it is a specialised Germa</context>
</contexts>
<marker>Escartín, 2012</marker>
<rawString>Carla Parra Escartín. 2012. Design and compilation of a specialized Spanish-German parallel corpus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, May. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>forthcoming</author>
</authors>
<title>Chasing the perfect splitter: A comparison of different compound splitting tools.</title>
<date></date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proceedings of the Ninth Conference on International Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Island,</location>
<marker>forthcoming, </marker>
<rawString>Carla Parra Escartín. forthcoming. Chasing the perfect splitter: A comparison of different compound splitting tools. In Proceedings of the Ninth Conference on International Language Resources and Evaluation (LREC’14), Reykjavik, Island, May. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popović</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation of german compound words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international conference on Advances in Natural Language Processing, FinTAL’06,</booktitle>
<pages>616--624</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="13407" citStr="Popović et al., 2006" startWordPosition="2164" endWordPosition="2167">the training set of the system its translation will subsequently fail. The state-of-the-art strategy to deal with compounds in SMT systems consists on splitting the compounds to reduce the number of unseen words. Previous research (Koehn 50 Text A Text B Number of words 2431 439 Number of comp. 265 (10.9%) 62 (14.12%) Number of unique comp. 143 25 Lexicalized comp. 99 (4.07%) 18 (4.1%) Unique lexicalized comp. 63 4 Not lexicalized comp. 166 (6.8%) 44 (10.06%) Unique not lexicalized comp. 80 21 Table 1: Compound nominals found in the two texts taken for the preliminary study. and Knight, 2003; Popović et al., 2006; Stymne, 2008; Fritzinger and Fraser, 2010; Stymne et al., 2013) has shown that splitting the compounds in German results in better Bleu scores (Papineni et al., 2001) and vocabulary coverage (fewer “unknown” words). However, the experiments carried out so far have also claimed that significant changes in error measures were not to be expected because the percentage of running words affected by compound splitting was rather low (Popović et al., 2006; Stymne, 2008). As will be observed in Section 4.1, in our case the percentage of running words affected by compound splitting was higher. This m</context>
<context position="15980" citStr="Popović et al. (2006)" startWordPosition="2588" endWordPosition="2591">ll compounds in the test set were present in the training corpus and viceversa. 4.1 Training environments Taking the normalised version of our corpus as a baseline, different training environments have been tested. We designed five possible training environments in which German compounds were preprocessed. In our first experiment (hereinafter “compList”), the list of manually extracted compounds was appended to the end of the training set and no further preprocessing was carried out. In our second experiment (hereinafter “RWTH”), the state-of-the-art compound splitting approach implemented by Popović et al. (2006) was used to split all possible compounds. As also implemented by Koehn and Knight (2003), this approach uses the corpus itself to create a vocabulary that is then subsequently used to calculate the possible splittings in the corpus. It has the advantage of being a stand-alone approach which does not depend on any external resources. A possible drawback of this approach would be that it relies on a large corpus to be able to compute the splittings. Thus, it may not be as efficient with smaller corpora (i.e. if we were to use only the TRIS corpus, for instance). The third experiment (hereinafte</context>
<context position="22453" citStr="Popović et al. (2006)" startWordPosition="3663" endWordPosition="3666">cal significance with MultEval (Clark et al., 2011). As can be observed in Table 4, adding compound lists to the training set significantly improves the Bleu and Ter scores with respect to the baseline. This is also the case when compounds were preprocessed and split. Moreover, while the Bleu scores for both splitters are the same when processing the entire corpus, adding the compound list to the training corpus yields better scores. In fact, the combination of the compound list and the compound splitter developed by Weller and Heid (2012) improves by 3.8 points in Bleu, while the approach by Popović et al. (2006) improves by 3.4 Bleu points against Baseline. When comparing it with compList, the improvements are of 3% and 2.4% Bleu respectively. To ensure a fair comparison, RWTH is defined as second baseline. Again, we observe significant improvement over this second baseline by adding the compound list to the training corpus. In terms of Bleu we gain an improvement of up to 1.4 points. These results seem promising as they show significant improvements both in terms of Bleu and Ter scores. As previously mentioned in Section 3.2, one possible explanation to the higher Bleu scores we obtained might be th</context>
<context position="25291" citStr="Popović et al. (2006)" startWordPosition="4139" endWordPosition="4142">xperiments. As the compound list was not preprocessed, the number of compounds found in RWTH and IMS is smaller than those found in Baseline and compList. In the case of RWTH+compList and IMS+compList, however, the productivity of German compounds mentioned earlier in Section 2 may have influenced the number of compounds found. If a compound found in our compound list was present in other compounds and those were split in such a way that it resulted in one of the 53 Experiment Splitting Method Compound List Bleu1%] test OOVs Ter1%] Baseline - no 45.9 43.9 181 compList - yes 46.7 42.9 169 RWTH Popović et al. (2006) no 48.3 40.8 104 RWTH+compList yes 49.1 40.5 104 ImS Weller and Heid (2012) no 48.3 40.5 114 ImS+compList yes 49.7 39.2 114 Table 4: Results for the German-4Spanish TRIS data. Statistically significant improvements with at least 99% confidence over the respective baselines (Baseline and RWTH) are printed in boldface. formants being that compound, its frequency got higher. As can be observed, the highest number of correct translations of compounds corresponds to RWTH+compList and ImS+compList. Table 6 shows the results of a sample sentence in our test set including several compounds. As can be</context>
</contexts>
<marker>Popović, Stein, Ney, 2006</marker>
<rawString>Maja Popović, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of german compound words. In Proceedings of the 5th international conference on Advances in Natural Language Processing, FinTAL’06, pages 616–624, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jutta Ransmayr</author>
<author>Karlheinz Moerth</author>
<author>Matej Durco</author>
</authors>
<title>Linguistic variation in the austrian media corpus. dealing with the challenges of large amounts of data. In</title>
<date>2013</date>
<booktitle>Proceedings of International Conference on Corpus Linguistics (CILC),</booktitle>
<institution>Alicante. University Alicante, University Alicante.</institution>
<contexts>
<context position="8126" citStr="Ransmayr et al., 2013" startWordPosition="1290" endWordPosition="1293">ts. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experiments carried out here have used the texts corresponding to the domain B00: Construction of the TRIS corpus (Parra Escartín, 2012), and an internally compiled version of the Europarl Corpus (Koehn, 2005) for the pair of languages German-Spanish3. The domain (B00: Construction) was selected because it is the biggest one of 2Corpus of the Austria Presse Agentur (APA). Recently it has been released as the AMC corpus (Austrian Media Corpus) (Ransmayr et al., 2013). 3See Table 2 for an overview of the corpus statistics. 49 the three domains currently available in the TRIS corpus4. Only one domain was used because we aimed at testing in-domain translation. Besides, the TRIS corpus was selected because it is a specialised German-Spanish parallel corpus. As opposed to the Europarl, the TRIS corpus is divided in domains and the source and target languages have been verified (i.e. the texts were originally written in German and translated into Spanish). Moreover, the texts included in the Europarl are transcriptions of the sessions of the European Parliament</context>
</contexts>
<marker>Ransmayr, Moerth, Durco, 2013</marker>
<rawString>Jutta Ransmayr, Karlheinz Moerth, and Matej Durco. 2013. Linguistic variation in the austrian media corpus. dealing with the challenges of large amounts of data. In Proceedings of International Conference on Corpus Linguistics (CILC), Alicante. University Alicante, University Alicante.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="18088" citStr="Schmid, 1994" startWordPosition="2943" endWordPosition="2944">re that they were not split in the corpus and remained the same. In the fourth experiment (hereinafter “IMS”) we used another compound splitter developed at the Institut für Maschinelle Sprachverarbeitung of the University of Stuttgart (Weller and Heid, 2012). This splitter was also developed using a frequencybased approach. However, in this case the training data consists of a list of lemmatized wordforms together with their POS tags. A set of rules to model transitional elements is also used. While this splitter might be used by processing our corpus with available tools such as TreeTagger (Schmid, 1994)6 and then computing frequencies, in our experiments we used the CELEX7 database for German (Baayen et al., 1993). This was done so because CELEX is an extensive high quality lexical database which already included all the information we needed to process and did not require any further preprocessing and clean up of our corpus. In the fifth experiment (hereinafter “IMS+compList”), we repeated the same procedure of our third experiment (“RWTH+compList”): we added the compound list to our training corpus already split, but this time using the compound splitter developed in Stuttgart. In total, 1</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="21738" citStr="Snover et al., 2006" startWordPosition="3544" endWordPosition="3547">(2003). 52 Popovic et al. (2006) Weller and Heid (2012) Compounds in training 182334 141789 % Vocabulary 54% 42% % Running words 0.4% 0.3% Compounds in test 924 444 % Vocabulary 44.3% 21.3% % Running words 8.5% 4% Table 3: Number of compounds detected by each of the splitters used and the percentages they account for with respect to the vocabulary (types) and the number of running words (tokens) in the corpora used in the experiments. 5 Results Table 4 reports the results of the five training environments described in Subsection 4.1 and the baseline. We report results in Bleu [%] and Ter [%] (Snover et al., 2006). All reported results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). As can be observed in Table 4, adding compound lists to the training set significantly improves the Bleu and Ter scores with respect to the baseline. This is also the case when compounds were preprocessed and split. Moreover, while the Bleu scores for both splitters are the same when processing the entire corpus, adding the compound list to the training corpus yields better scores. In fact, the combination of the compound list and the compound split</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO,</location>
<contexts>
<context position="20763" citStr="Stolcke, 2002" startWordPosition="3381" endWordPosition="3382">dicate that both splitting algorithms perform correctly. A thorough analysis of their outputs has been carried out confirming this hypothesis as the accuracies of both splitters were considerably high: 97.19% (RWTH) and 97.49% IMS (Parra Escartín, forthcoming)8. As SMT system, we employ the state-of-theart phrase-based translation approach (Zens and Ney, 2008) implemented in Jane. The baseline is trained on the concatination of the TRIS and Europarl corpus. Word alignments are trained with fastAlign (Dyer et al., 2013). Further, we apply a 4-gram language model trained with the SRILM toolkit (Stolcke, 2002) on the target side of the training corpus. The log-linear parameter weights are tuned with MERT (Och, 2003) on the development set (dev). As optimization criterion we use Bleu. The parameter setting for all experiments was the same to allow for comparisons. software/celex_gug.pdf 8The analysis was done following the method proposed by Koehn and Knight (2003). 52 Popovic et al. (2006) Weller and Heid (2012) Compounds in training 182334 141789 % Vocabulary 54% 42% % Running words 0.4% 0.3% Compounds in test 924 444 % Vocabulary 44.3% 21.3% % Running words 8.5% 4% Table 3: Number of compounds de</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Nicola Cancedda</author>
<author>Lars Ahrenberg</author>
</authors>
<date>2013</date>
<booktitle>Generation of Compound Words in Statistical Machine Translation into Compounding Languages. Computational Linguistics,</booktitle>
<pages>1--42</pages>
<contexts>
<context position="13472" citStr="Stymne et al., 2013" startWordPosition="2174" endWordPosition="2177">fail. The state-of-the-art strategy to deal with compounds in SMT systems consists on splitting the compounds to reduce the number of unseen words. Previous research (Koehn 50 Text A Text B Number of words 2431 439 Number of comp. 265 (10.9%) 62 (14.12%) Number of unique comp. 143 25 Lexicalized comp. 99 (4.07%) 18 (4.1%) Unique lexicalized comp. 63 4 Not lexicalized comp. 166 (6.8%) 44 (10.06%) Unique not lexicalized comp. 80 21 Table 1: Compound nominals found in the two texts taken for the preliminary study. and Knight, 2003; Popović et al., 2006; Stymne, 2008; Fritzinger and Fraser, 2010; Stymne et al., 2013) has shown that splitting the compounds in German results in better Bleu scores (Papineni et al., 2001) and vocabulary coverage (fewer “unknown” words). However, the experiments carried out so far have also claimed that significant changes in error measures were not to be expected because the percentage of running words affected by compound splitting was rather low (Popović et al., 2006; Stymne, 2008). As will be observed in Section 4.1, in our case the percentage of running words affected by compound splitting was higher. This might be due to the kind of texts used in our experiments. 4 Exper</context>
</contexts>
<marker>Stymne, Cancedda, Ahrenberg, 2013</marker>
<rawString>Sara Stymne, Nicola Cancedda, and Lars Ahrenberg. 2013. Generation of Compound Words in Statistical Machine Translation into Compounding Languages. Computational Linguistics, pages 1–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>German Compounds in Factored Statistical Machine Translation. In</title>
<date>2008</date>
<booktitle>GoTAL’08: Proceedings of the 6th international conference on Advances in Natural Language Processing,</booktitle>
<pages>464--475</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="13421" citStr="Stymne, 2008" startWordPosition="2168" endWordPosition="2169">e system its translation will subsequently fail. The state-of-the-art strategy to deal with compounds in SMT systems consists on splitting the compounds to reduce the number of unseen words. Previous research (Koehn 50 Text A Text B Number of words 2431 439 Number of comp. 265 (10.9%) 62 (14.12%) Number of unique comp. 143 25 Lexicalized comp. 99 (4.07%) 18 (4.1%) Unique lexicalized comp. 63 4 Not lexicalized comp. 166 (6.8%) 44 (10.06%) Unique not lexicalized comp. 80 21 Table 1: Compound nominals found in the two texts taken for the preliminary study. and Knight, 2003; Popović et al., 2006; Stymne, 2008; Fritzinger and Fraser, 2010; Stymne et al., 2013) has shown that splitting the compounds in German results in better Bleu scores (Papineni et al., 2001) and vocabulary coverage (fewer “unknown” words). However, the experiments carried out so far have also claimed that significant changes in error measures were not to be expected because the percentage of running words affected by compound splitting was rather low (Popović et al., 2006; Stymne, 2008). As will be observed in Section 4.1, in our case the percentage of running words affected by compound splitting was higher. This might be due to</context>
<context position="23220" citStr="Stymne (2008)" startWordPosition="3799" endWordPosition="3800">mparison, RWTH is defined as second baseline. Again, we observe significant improvement over this second baseline by adding the compound list to the training corpus. In terms of Bleu we gain an improvement of up to 1.4 points. These results seem promising as they show significant improvements both in terms of Bleu and Ter scores. As previously mentioned in Section 3.2, one possible explanation to the higher Bleu scores we obtained might be that the number of running words affected by compound splitting was higher than in other experiments like the ones carried out by Popović et al. (2006) and Stymne (2008). Fritzinger and Fraser (2010) used a hybrid splitting algorithm which combined the corpus-based approach and linguistic information and also reported better Bleu scores for German-4English translations than splitting algorithms based only in corpus frequencies. They suggested that fewer split compounds but better split could yield better results. However, in our case the two splitters score the same in terms of Bleu. Further experiments with other language pairs should be carried out to test whether this is only the case with German-4Spanish translation tasks or not. If this were to be confir</context>
</contexts>
<marker>Stymne, 2008</marker>
<rawString>Sara Stymne. 2008. German Compounds in Factored Statistical Machine Translation. In GoTAL’08: Proceedings of the 6th international conference on Advances in Natural Language Processing, pages 464–475. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: open source hierarchical translation, extended with reordering and lexicon models.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>262--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9364" citStr="Vilar et al., 2010" startWordPosition="1498" endWordPosition="1501">e is rather oral and less technical. As compounds tend to be more frequent in domain specific texts, the TRIS corpus has been used for testing, while the Europarl Corpus has been used in the training set to avoid data scarcity problems and increase the vocabulary coverage of the SMT system. In the case of Machine Translation (MT), both rule-based MT systems (RBMT systems) and Statistical MT systems (SMT systems) encounter problems when dealing with compounds. For the purposes of this paper, the treatment of compounds in German has been tested within the SMT toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). We have carried out several experiments translating German specialized texts into Spanish to test to which extent incorporating a linguistic analysis of the corpora and compiling compound lists improves the overall SMT results. At this stage, including further linguistic information such as Partof-Speech tagging (POS tagging) or phrase chunking has been disregarded. Forcing the translation of compounds in the phrase tables produced by Jane has also been disregarded. The overall aim was to test how the SMT system performs using different pre-processing strategies of the training data but with</context>
</contexts>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: open source hierarchical translation, extended with reordering and lexicon models. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 262–270, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Weller</author>
<author>Ulrich Heid</author>
</authors>
<title>Analyzing and Aligning German compound nouns.</title>
<date>2012</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="17734" citStr="Weller and Heid, 2012" startWordPosition="2881" endWordPosition="2884">enation of the complete Europarl Corpus German-4Spanish and a greater part of the TRIS corpus, while in dev and test only texts from the TRIS corpus were used. the list of splittings to be carried out in the corpus. Thus, after all possible splittings were calculated, those splittings that were present in the manually compiled compound list were deleted to ensure that they were not split in the corpus and remained the same. In the fourth experiment (hereinafter “IMS”) we used another compound splitter developed at the Institut für Maschinelle Sprachverarbeitung of the University of Stuttgart (Weller and Heid, 2012). This splitter was also developed using a frequencybased approach. However, in this case the training data consists of a list of lemmatized wordforms together with their POS tags. A set of rules to model transitional elements is also used. While this splitter might be used by processing our corpus with available tools such as TreeTagger (Schmid, 1994)6 and then computing frequencies, in our experiments we used the CELEX7 database for German (Baayen et al., 1993). This was done so because CELEX is an extensive high quality lexical database which already included all the information we needed t</context>
<context position="19661" citStr="Weller and Heid (2012)" startWordPosition="3197" endWordPosition="3200">tgart.de/projekte/ corplex/TreeTagger/ 7http://wwwlands2.let.kun.nl/members/ As can be observed in Table 3, the percentage of compounds in the test set is considerably higher than in the training set. This is due to the fact that in the test set only a subcorpus of the TRIS corpus was used, whereas in the training corpus Europarl was also used and as stated earlier (cf. Subsection 3.1 and table 1), domain specific corpora tend to have more compounds. It is also noticeable that the compound splitter developed in Stuttgart detects and splits fewer compounds. A possible explanation would be that Weller and Heid (2012) only split words into content words and use POS tags to filter out other highly frequent words that do not create compounds. The presence of lexicalized compounds in the CELEX database does not seem to have affected the accuracy of the splitter (i.e. they were not skipped by the splitter). Finally, it is also noticeable that the percentage of compounds detected in the training set is similar to the one reported by Baroni et al. (2002) and referenced to in Section 2. This seems to indicate that both splitting algorithms perform correctly. A thorough analysis of their outputs has been carried o</context>
<context position="21173" citStr="Weller and Heid (2012)" startWordPosition="3445" endWordPosition="3448">ained on the concatination of the TRIS and Europarl corpus. Word alignments are trained with fastAlign (Dyer et al., 2013). Further, we apply a 4-gram language model trained with the SRILM toolkit (Stolcke, 2002) on the target side of the training corpus. The log-linear parameter weights are tuned with MERT (Och, 2003) on the development set (dev). As optimization criterion we use Bleu. The parameter setting for all experiments was the same to allow for comparisons. software/celex_gug.pdf 8The analysis was done following the method proposed by Koehn and Knight (2003). 52 Popovic et al. (2006) Weller and Heid (2012) Compounds in training 182334 141789 % Vocabulary 54% 42% % Running words 0.4% 0.3% Compounds in test 924 444 % Vocabulary 44.3% 21.3% % Running words 8.5% 4% Table 3: Number of compounds detected by each of the splitters used and the percentages they account for with respect to the vocabulary (types) and the number of running words (tokens) in the corpora used in the experiments. 5 Results Table 4 reports the results of the five training environments described in Subsection 4.1 and the baseline. We report results in Bleu [%] and Ter [%] (Snover et al., 2006). All reported results are averages</context>
<context position="25367" citStr="Weller and Heid (2012)" startWordPosition="4153" endWordPosition="4156">nds found in RWTH and IMS is smaller than those found in Baseline and compList. In the case of RWTH+compList and IMS+compList, however, the productivity of German compounds mentioned earlier in Section 2 may have influenced the number of compounds found. If a compound found in our compound list was present in other compounds and those were split in such a way that it resulted in one of the 53 Experiment Splitting Method Compound List Bleu1%] test OOVs Ter1%] Baseline - no 45.9 43.9 181 compList - yes 46.7 42.9 169 RWTH Popović et al. (2006) no 48.3 40.8 104 RWTH+compList yes 49.1 40.5 104 ImS Weller and Heid (2012) no 48.3 40.5 114 ImS+compList yes 49.7 39.2 114 Table 4: Results for the German-4Spanish TRIS data. Statistically significant improvements with at least 99% confidence over the respective baselines (Baseline and RWTH) are printed in boldface. formants being that compound, its frequency got higher. As can be observed, the highest number of correct translations of compounds corresponds to RWTH+compList and ImS+compList. Table 6 shows the results of a sample sentence in our test set including several compounds. As can be observed, in the ImS+compList experiment all compounds are correctly transl</context>
</contexts>
<marker>Weller, Heid, 2012</marker>
<rawString>Marion Weller and Ulrich Heid. 2012. Analyzing and Aligning German compound nouns. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, May. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Wellman</author>
</authors>
<title>DUDEN. Die Grammatik. Unentbehrlich für richtiges Deutsch, volume 4, chapter Die Wortbildung.</title>
<date>1984</date>
<publisher>Duden Verlag.</publisher>
<contexts>
<context position="7063" citStr="Wellman, 1984" startWordPosition="1106" endWordPosition="1107">on of the adjective warm and the noun Wasser yields a nominal compound). Finally, it is also important to highlight that besides words, compounds may also include particles to join those words together, as the “s” between Warmwasserbereitung and Anlagen in Example 3 or truncations (part of one of the component words is deleted). Example 4 illustrates the case when one of the component words has been truncated: (4) abstellen(V) - en + Anlagen(N) = Abstellanlagen (N) [EN: parking facilities] The morphology of German compounds has been widely researched, both within linguistics (Fleischer, 1975; Wellman, 1984; Eichinger, 2000, among others), as in NLP (Langer, 1998; Girju et al., 2005; Marek, 2006; Girju, 2008, among others). Here, we will focus on the impact of preprocessing nominal compounds in SMT. Baroni et al. (2002) report that 47% of the vocabulary (types) in the APA corpus2 were compounds. As will be observed in Section 4, the compound splitters we used also detected a high percentage of compounds in the corpora used in our experiments. This fact confirms that it is crucial to find a successful way of processing compounds in NLP applications and in our case in SMT. 3 Case Study The experim</context>
</contexts>
<marker>Wellman, 1984</marker>
<rawString>Hans Wellman, 1984. DUDEN. Die Grammatik. Unentbehrlich für richtiges Deutsch, volume 4, chapter Die Wortbildung. Duden Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Matthias Huck</author>
<author>Stephan Peitz</author>
<author>Malte Nuhn</author>
<author>Markus Freitag</author>
<author>Jan-Thorsten Peter</author>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>Jane 2: Open source phrase-based and hierarchical statistical machine translation.</title>
<date>2012</date>
<booktitle>In International Conference on Computational Linguistics,</booktitle>
<pages>483--491</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="9343" citStr="Wuebker et al., 2012" startWordPosition="1494" endWordPosition="1497">ent, and thus the style is rather oral and less technical. As compounds tend to be more frequent in domain specific texts, the TRIS corpus has been used for testing, while the Europarl Corpus has been used in the training set to avoid data scarcity problems and increase the vocabulary coverage of the SMT system. In the case of Machine Translation (MT), both rule-based MT systems (RBMT systems) and Statistical MT systems (SMT systems) encounter problems when dealing with compounds. For the purposes of this paper, the treatment of compounds in German has been tested within the SMT toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). We have carried out several experiments translating German specialized texts into Spanish to test to which extent incorporating a linguistic analysis of the corpora and compiling compound lists improves the overall SMT results. At this stage, including further linguistic information such as Partof-Speech tagging (POS tagging) or phrase chunking has been disregarded. Forcing the translation of compounds in the phrase tables produced by Jane has also been disregarded. The overall aim was to test how the SMT system performs using different pre-processing strategies of the t</context>
</contexts>
<marker>Wuebker, Huck, Peitz, Nuhn, Freitag, Peter, Mansour, Ney, 2012</marker>
<rawString>Joern Wuebker, Matthias Huck, Stephan Peitz, Malte Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Mansour, and Hermann Ney. 2012. Jane 2: Open source phrase-based and hierarchical statistical machine translation. In International Conference on Computational Linguistics, pages 483–491, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in Dynamic Programming Beam Search for Phrasebased Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<pages>195--205</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="20511" citStr="Zens and Ney, 2008" startWordPosition="3338" endWordPosition="3341">f the splitter (i.e. they were not skipped by the splitter). Finally, it is also noticeable that the percentage of compounds detected in the training set is similar to the one reported by Baroni et al. (2002) and referenced to in Section 2. This seems to indicate that both splitting algorithms perform correctly. A thorough analysis of their outputs has been carried out confirming this hypothesis as the accuracies of both splitters were considerably high: 97.19% (RWTH) and 97.49% IMS (Parra Escartín, forthcoming)8. As SMT system, we employ the state-of-theart phrase-based translation approach (Zens and Ney, 2008) implemented in Jane. The baseline is trained on the concatination of the TRIS and Europarl corpus. Word alignments are trained with fastAlign (Dyer et al., 2013). Further, we apply a 4-gram language model trained with the SRILM toolkit (Stolcke, 2002) on the target side of the training corpus. The log-linear parameter weights are tuned with MERT (Och, 2003) on the development set (dev). As optimization criterion we use Bleu. The parameter setting for all experiments was the same to allow for comparisons. software/celex_gug.pdf 8The analysis was done following the method proposed by Koehn and </context>
</contexts>
<marker>Zens, Ney, 2008</marker>
<rawString>Richard Zens and Hermann Ney. 2008. Improvements in Dynamic Programming Beam Search for Phrasebased Statistical Machine Translation. In International Workshop on Spoken Language Translation, pages 195–205, Honolulu, Hawaii, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
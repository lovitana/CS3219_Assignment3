<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048067">
<title confidence="0.998203">
A Principled Approach to Context-Aware Machine Translation
</title>
<author confidence="0.947358">
Rafael E. Banchs
</author>
<affiliation confidence="0.932488">
Institute for Infocomm Research
</affiliation>
<address confidence="0.472549">
1 Fusionopolis Way, #21-01, Singapore 138632
</address>
<email confidence="0.972892">
rembanchs@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.993329" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999413916666667">
This paper presents a new principled approach to
context-aware machine translation. The proposed
approach reformulates the posterior probability of a
translation hypothesis given the source input by
incorporating the source-context information as an
additional conditioning variable. As a result, a new
model component, which is referred to as the
context-awareness model, is added into the original
noisy channel framework. A specific computation-
al implementation for the new model component is
also described along with its main properties and
limitations.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981172413793">
It is well known that source-context information
plays a significant role in human-based language
translation (Padilla and Bajo, 1998). A similar
claim can be supported for the case of Machine
Translation on the grounds of the Distributional
Hypothesis (Firth, 1957). According to the Dis-
tributional Hypothesis, much of the meaning of a
given word is implied by its context rather than
by the word itself.
In this work, we first focus our attention on the
fact that the classical formulation of the statis-
tical machine translation framework, implicitly
disregards the role of source-context information
within the translation generation process. Based
on this, we propose a principled reformulation
that allows for introducing context-awareness
into the statistical machine translation frame-
work. Then, a specific computational implement-
ation for the newly proposed model is derived
and described, along with its main properties and
limitations.
The remainder of the paper is structured as
follows. First, in section 2, the theoretical back-
ground and motivation for this work are present-
ed. Then, in section 3, the proposed model
derivation is described. In section 4, a specific
computational implementation for the model is
provided. And, finally in section 5, main conclu-
sions and future research work are presented.
</bodyText>
<sectionHeader confidence="0.982999" genericHeader="method">
2 Theoretical Background
</sectionHeader>
<bodyText confidence="0.99987725">
According to the original formulation of the
translation problem within the statistical frame-
work, the decoding process is implemented by
means of a probability maximization mechanism:
</bodyText>
<equation confidence="0.999651">
T = argmaxT p(T|S) (1)
</equation>
<bodyText confidence="0.989161142857143">
which means that the most likely translation T�
for a source sentence S is provided by the
hypothesis T that maximizes the conditional
probability of T given S.
Furthermore, by considering the noisy channel
approach introduced in communications theory,
the formulation in (1) can be rewritten as:
</bodyText>
<equation confidence="0.999851">
T = argmaxT p(S|T) p(T) (2)
</equation>
<bodyText confidence="0.999740384615385">
where the likelihood p(S|T) is referred to as the
translation model and the prior p(T) is referred
to as the language model.
Notice from the resulting formulation in (2)
that, as the maximization runs over the trans-
lation hypothesis space {T}, the evidence p(S) is
not accounted for.
This particular consequence of the mathema-
tical representation in (2) is counterintuitive to
the notion of source-context information being
useful for selecting appropriate translations.
This problem becomes more relevant when the
probability models in (2) are decomposed into
sub-sentence level probabilities for operational
purposes. Indeed, the computational implement-
ation of (2) requires the decomposition of senten-
ce-level probabilities p(S|T) and p(T) into sub-
sentence level probabilities p (s |t) and p (t) ,
were s and t refer to sub-sentence units, such as
words or groups of words.
In the original problem formulation (Brown et
al., 1993), the sentence-level translation model
p(S|T) in (2) is approximated by means of word-
level probabilities, and the sentence-level langua-
ge model p(T) is approximated by means of
word n-gram probabilities.
</bodyText>
<page confidence="0.982239">
70
</page>
<note confidence="0.7916335">
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 70–74,
Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999783">
Within this framework, translation probabilities
at the sentence-level are estimated from word-
level probabilities as follows1 :
</bodyText>
<equation confidence="0.999474">
𝑝(𝑆|𝑇) = ∏𝑘∑𝑛𝑝(𝑠𝑘|𝑡𝑛) (3)
</equation>
<bodyText confidence="0.999975333333333">
where 𝑠𝑘 and 𝑡𝑛 refer to individual words
occurring in 𝑆 and 𝑇, respectively. The proba-
bilities 𝑝 (𝑠𝑘 |𝑡𝑛) are referred to as lexical models
and they represent the probability of an indi-
vidual source word 𝑠𝑘 to be the translation of a
given target word 𝑡𝑛. These lexical models are
estimated by using word alignment probabilities.
In statistical phrase-based translation (Koehn et
al., 2003), the translation model is approximated
by means of phrase-level probabilities (a phrase
is a bilingual pair of sub-sentence units that is
consistent with the word alignments).
Within this framework, translation probabilities
at the sentence-level are computed from phrase-
level probabilities as follows:
</bodyText>
<equation confidence="0.999632">
𝑝(𝑆|𝑇) = ∏𝑖𝑝(𝑠𝑖|𝑡𝑖) (4)
</equation>
<bodyText confidence="0.999960777777778">
where 𝑠𝑖 and 𝑡𝑖 refer to phrases (i.e. groups of
words) occurring in 𝑆 and 𝑇, respectively. The
probabilities 𝑝(𝑠𝑖|𝑡𝑖) are estimated by means of
relative frequencies and, accordingly, they are
referred to as relative frequency models.
Finally, in (Och and Ney, 2002), the maximum
entropy framework was introduced into machine
translation and the two-model formulation in the
noisy channel approach (2) was extended to the
log-linear combination of as many relevant
models as can be reasonably derived from the
training data. In addition, the maximum entropy
framework also allows for tuning the weights in
the log-linear combinations of models by means
of discriminative training.
Within this framework, translation probabilities
at the sentence-level are estimated from phrase-
level probabilities as follows:
</bodyText>
<equation confidence="0.984737">
𝑝(𝑇|𝑆) = 1 ζexp{∑𝑖 ∑𝑚 𝜆𝑚 ℎ𝑚 (𝑡𝑖, 𝑠𝑖)} (5)
</equation>
<bodyText confidence="0.9902126">
where ℎ𝑚(𝑠𝑖, 𝑡𝑖) are referred to as feature models
or functions, 𝜆𝑚 are the feature weights of the
log-linear combination, and ζ is a normalization
factor. Notice from (5) that in the maximum
entropy framework the posterior probability
𝑝(𝑇 |𝑆) is modeled rather than the likelihood.
1 For the sake of clarity additional model components
such as fertility, reordering and distortion are omitted
in both (3) and (4).
From (3) and (4), it is clear that source-context
information is not taken into account during
translation hypothesis generation. In such cases,
the individual sub-sentence unit probabilities
depend only on the restricted context provided
by the same sub-sentence unit level as observed
from the training data.
In the case of (5), on the other hand, some
room is left for incorporating source-context
information in the hypothesis generation process
by means of context-aware feature models. This
is basically done by using features that relate the
occurrences of sub-sentence units with relevant
source-context information of lager extension.
Several research works have already addressed
the problem of incorporating source context
information into the translation process within
the maximum entropy framework (Carpuat and
Wu, 2007; Carpuat and Wu 2008; Haque et al.
2009; España-Bonet et al. 2009; Costa-jussà and
Banchs 2010; Haque et al. 2010; Banchs and
Costa-jussà 2011).
In the following section, we will reformulate
the translation problem, as originally described
in (1), in order to provide a principled approach
to context-aware machine translation for both the
noisy channel and the phrase-based approaches.
As seen later, this will result in the incorporation
of a new model component, which can be also
used as a feature function within the context of
the maximum entropy framework.
</bodyText>
<sectionHeader confidence="0.994041" genericHeader="method">
3 Model Derivation
</sectionHeader>
<bodyText confidence="0.997272909090909">
In our proposed formulation for context-aware
machine translation, we assume that the most
likely translation 𝑇� for a source sentence 𝑆 does
not depends on 𝑆 only, but also on the context 𝐶
in which 𝑆 occurs. While this information might
be not too relevant when estimating probabilities
at the sentence level, it certainly becomes a very
useful evidence support at the sub-sentence level.
Based on this simple idea, we can reformulate
the mathematical representation of the translation
problem presented in (1) as follows:
</bodyText>
<equation confidence="0.997917">
𝑇� = argmax𝑇 𝑝(𝑇 |𝑆, 𝐶) (6)
</equation>
<bodyText confidence="0.999907428571429">
where 𝑝(𝑇 |𝑆, 𝐶) is the conditional probability of
a translation hypothesis 𝑇 given the source
sentence 𝑆 and the context 𝐶 in which 𝑆 occurs.
This means that the most likely translation 𝑇� for
a source sentence 𝑆 is provided by the hypothesis
𝑇 that maximizes the conditional probability of 𝑇
given 𝑆 and 𝐶.
</bodyText>
<page confidence="0.993758">
71
</page>
<bodyText confidence="0.99996655">
For now, let us just consider the context to be
any unit of source language information with
larger span than the one of the units used to
represent 𝑆. For instance, if 𝑆 is a sentence, 𝐶
can be either a paragraph or a full document; if 𝑆
is a sub-sentence unit, 𝐶 can be a sentence; and
so on.
From the theoretical point of view, the formula-
tion in (6) is supported by the assumptions of the
Distributional Hypothesis, which states that
meaning is mainly derived from context rather
than from individual language units. According
to this, the formulation in (6) allows for incor-
porating context information into the translation
generation process, in a similar way humans take
source-context information into account when
producing a translation.
After some mathematical manipulations, the
conditional probability in (6) can be rewritten as
follows:
</bodyText>
<equation confidence="0.952926">
𝑝(𝑇|𝑆, 𝐶) = 𝑝(𝐶|𝑆,𝑇) 𝑝(𝑆|𝑇) 𝑝(𝑇) 𝑝(𝐶|𝑆) 𝑝(𝑆) (7)
</equation>
<bodyText confidence="0.999930333333333">
where 𝑝 (𝑆 |𝑇) and 𝑝 (𝑇) are the same translation
and language model probabilities as in (2), and
𝑝 (𝐶 |𝑆, 𝑇) is the conditional probability of the
source-context 𝐶 given the translation pair 〈𝑆, 𝑇〉.
Notice that if the translation pair is independent
of the context, i.e. 〈𝑆, 𝑇〉 ⊥ 𝐶, then (7) reduces to:
</bodyText>
<equation confidence="0.955812">
𝑝(𝑇 |𝑆, 𝐶) = 𝑝(𝑆|𝑇) 𝑝(𝑇) (8)
𝑝(𝑆)
</equation>
<bodyText confidence="0.999189285714286">
and the context-aware formulation in (6) reduces
to the noisy channel formulation presented
earlier in (2).
If we assume, on the other hand, that the
translation pair is not independent of the context,
the formulation in (6) can be rewritten in terms
of (7) as follows:
</bodyText>
<equation confidence="0.997973">
𝑇� = argmax𝑇 𝑝(𝐶|𝑆,𝑇) 𝑝(𝑆|𝑇) 𝑝(𝑇) (9)
</equation>
<bodyText confidence="0.999960444444444">
As seen from (2) and (9), the proposed context-
aware machine translation formulation is similar
to the noisy channel approach formulation with
the difference that a new probability model has
been introduced: 𝑝(𝐶 |𝑆, 𝑇). This new model will
be referred to as the context-awareness model,
and it acts as a complementary model, which
favors those translation hypotheses 𝑇 for which
the current source context 𝐶 is highly probable
given the translation pair 〈𝑆, 𝑇〉.
In the same way translation probabilities
𝑝 (𝑆|𝑇) at the sentence-level can be estimated
from lower-level unit probabilities, such as word
or phrases, context-awareness probabilities at the
sentence-level can be also estimated from lower-
level unit probabilities. For instance, 𝑝(𝐶 |𝑆, 𝑇)
can be approximated by means of phrase-level
probabilities according to the following equation:
</bodyText>
<equation confidence="0.982075">
𝑝(𝐶|𝑆,𝑇) = ∏𝑖𝑝(𝐶|𝑠𝑖,𝑡𝑖) (10)
</equation>
<bodyText confidence="0.999963166666667">
where 𝑠𝑖 and 𝑡𝑖 refer to phrase pairs occurring in
𝑆 and 𝑇, respectively, and 𝐶 is the source-context
for the translation under consideration.
In the following section we develop a specific
computational implementation for estimating the
probabilities of the context-awareness model.
</bodyText>
<sectionHeader confidence="0.995938" genericHeader="method">
4 Model Implementation
</sectionHeader>
<bodyText confidence="0.999985076923077">
Before developing a specific implementation for
the context-awareness model in (10), we need to
define what type of units 𝑠𝑖 and 𝑡𝑖 will be used
and what kind of source-context information 𝐶
will be taken into account.
Here, we will consider the phrase-based
machine translation scenario, where phrase pairs
&lt;𝑠𝑖, 𝑡𝑖&gt; are used as the building blocks of the
translation generation process. Accordingly, and
in order to be relevant, the span of the context
information to be used must be larger than the
one implicitly accounted for by the phrases.
Typically, phrases span vary from one to
several words, but most of the time they remain
within the sub-sentence level. Then, a context
definition at the sentence-level should be appro-
priate for the purpose of estimating context-
awareness probabilities at the phrase-level. In
this way, we can consider the context evidence 𝐶
to be the same sentence being translated 𝑆.
With these definitions on place, we can now
propose a maximum likelihood approach for
estimating context-awareness probabilities at the
phrase-level. According to this, the probabilities
can be computed by using relative frequencies as
follows:
</bodyText>
<equation confidence="0.5561355">
= 𝑐𝑜𝑢𝑛𝑡 (𝑆,𝑠𝑖,𝑡𝑖)
𝑝(𝑆|𝑠𝑖,𝑡𝑖) 𝑐𝑜𝑢𝑛𝑡 (𝑠𝑖,𝑡𝑖) (11)
</equation>
<bodyText confidence="0.98900675">
where the numerator accounts for the number of
times the phrase pair &lt;𝑠𝑖, 𝑡𝑖&gt; has been seen along
with context 𝑆 in the training data, and the
denominator accounts for the number of times
the phrase pair &lt;𝑠𝑖, 𝑡𝑖&gt; has been seen along with
any context in the training data.
While the computation of the denominator in
(11) is trivial, i.e. it just needs to count the
</bodyText>
<page confidence="0.995962">
72
</page>
<bodyText confidence="0.992195921052631">
number of times &lt;si, ti&gt; occurs in the parallel
text, the computation of the numerator requires
certain consideration.
Indeed, if we consider the context to be the
source sentence being translated S, counting the
number of times a phrase pair &lt;si, ti&gt; has been
seen along with context S implies that S is
expected to appear several times in the training
data. In practice, this rarely occurs! According to
this, the counts for the numerator in (11) will be
zero most of the time (when the sentence being
translated is not contained in the training data)
or, eventually, one (when the sentence being
translated is contained in the training data).
Moreover, if the sentence being translated is
contained in the training data, then its translation
is already known! So, why do we need to
generate any translation at all?
To circumvent this apparent inconsistency of
the model, and to compute proper estimates for
the values of count (S, si, ti) , our proposed
model implementation uses fractional counts.
This means that, instead of considering integer
counts of exact occurrences of the context S
within the training data, we will consider frac-
tional counts to account for the occurrences of
contexts that are similar to S. In order to serve
this purpose, a similarity metric within the range
from zero (no similarity at all) to one (maximum
similarity) is required.
In this way, for each source sentence Si,k in the
training data that is associated to the phrase pair
&lt;si, ti&gt;, its corresponding fractional count would
be given by the similarity between Si,k and the
input sentence being translated S.
f count(Si,k) = sim(S, Si,k) (12)
According to this, the numerator in (11) can be
expressed in terms of (12) as:
</bodyText>
<equation confidence="0.939632">
count (S, si, ti) = Ek sim(S, Si,k) (13)
</equation>
<bodyText confidence="0.996213">
and the context-awareness probability estimates
can be computed as:
</bodyText>
<equation confidence="0.960224333333333">
p(I1�
S s ti) = EksiM(S,Si,k) (14)
EkSiM(Si,k ,Si,k)
</equation>
<bodyText confidence="0.999887870967742">
Notice that in (14) it is assumed that the
number of times the phrase pair &lt;si, ti&gt; occurs in
the parallel text, i.e. count (si, ti), is equal to the
number of sentence pairs containing &lt;si, ti&gt;. In
other words, multiple occurrences of the same
phrase pair within a bilingual sentence pair are
accounted for only once.
Finally, two important differences between the
context-awareness model presented here and
other conventional models used in statistical
machine translation must be highlighted.
First, notice that the context-awareness model
is a dynamic model, in the sense that it has to be
estimated at run-time. In fact, as the model
probabilities depend on the input sentence to be
translated, such probabilities cannot be computed
beforehand as in the case of other models.
Second, different from the lexical models and
relative frequencies that can be computed on
both directions (source-to-target and target-to-
source), a symmetric version of the context-
awareness model cannot be implemented for
decoding. This is basically because estimating
probabilities of the form p (T |si, ti) requires the
knowledge of the translation output T, which is
not known until decoding is completed.
However, the symmetric version of the context-
awareness model can be certainly used at a post-
processing stage, such as in n-best rescoring; or,
alternatively, an incremental implementation can
be devised for its use during decoding.
</bodyText>
<sectionHeader confidence="0.998046" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999996478260869">
We have presented a new principled approach to
context-aware machine translation. The proposed
approach reformulates the posterior probability
of a translation hypothesis given the source input
by incorporating the source-context information
as and additional conditioning variable. As a
result, a new probability model component, the
context-awareness model, has been introduced
into the noisy channel approach formulation.
We also presented a specific computational
implementation of the context-awareness model,
in which likelihoods are estimated for the context
evidence at the phrase-level based on the use of
fractional counts, which can be computed by
means of a similarity metric.
Future work in this area includes efficient run-
time implementations and comparative evalua-
tions of different similarity metrics to be used for
computing the fractional counts. Similarly, a
comparative evaluation between an incremental
implementation of the symmetric version of the
context-awareness model and its use in a post-
processing stage should be also conducted.
</bodyText>
<sectionHeader confidence="0.998319" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999099333333333">
The author wants to thank I2R for its support and
permission to publish this work, as well as the
reviewers for their insightful comments.
</bodyText>
<page confidence="0.998449">
73
</page>
<sectionHeader confidence="0.995898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999942759259259">
Banchs, R.E., Costa-jussà, M. R. 2011. A Semantic
Feature for Statistical Machine Translation. In
Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
ACL HLT 2011, pp. 126-134.
Brown, P., Della-Pietra, S., Della-Pietra, V., Mercer,
R. 1993. The Mathematics of Statistical Machine
Translation: Computational Linguistics 19(2), 263-
- 311
Carpuat, M., Wu, D. 2007. How Phrase Sense
Disambiguation Outperforms Word Sense Disam-
biguation for Statistical Machine Translation. In:
11th International Conference on Theoretical and
Methodological Issues in Machine Translation.
Skovde
Carpuat, M., Wu, D. 2008. Evaluation of Context-
Dependent Phrasal Translation Lexicons for
Statistical Machine Translation. In: 6th Interna-
tional Conference on Language Resources and
Evaluation (LREC). Marrakech
Costa-jussà, M. R., Banchs, R.E. 2010. A Vector-
Space Dynamic Feature for Phrase-Based
Statistical Machine Translation. Journal of
Intelligent Information Systems
España-Bonet, C., Gimenez, J., Marquez, L. 2009.
Discriminative Phrase-Based Models for Arabic
Machine Translation. ACM Transactions on Asian
Language Information Processing Journal (Special
Issue on Arabic Natural Language Processing)
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis, 51: 1-31
Haque, R., Naskar, S. K., Ma, Y., Way, A. 2009.
Using Supertags as Source Language Context in
SMT. In: 13th Annual Conference of the European
Association for Machine Translation, pp. 234--241.
Barcelona
Haque, R., Naskar, S. K., van den Bosh, A., Way, A.
2010. Supertags as Source Language Context in
Hierarchical Phrase-Based SMT. In: 9th Con-
ference of the Association for Machine Translation
in the Americas (AMTA)
Koehn, P., Och, F. J., Marcu, D. 2003. Statistical
Phrase-Based Translation. In: Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language Proces-
sing (HLTEMNLP), pp. 48--54. Edmonton
Och, F. J., Ney, H. (2002) Discriminative Training
and Maximum Entropy Models for Statistical
Machine Translation. In: 40th Annual Meeting of
the Association for Computational Linguistics, pp.
295--302
Padilla, P., Bajo, T. (1998) Hacia un Modelo de
Memoria y Atención en la Interpretación Simul-
tánea. Quaderns: Revista de Traducció 2, 107--117
</reference>
<page confidence="0.999134">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395483">
<title confidence="0.999956">A Principled Approach to Context-Aware Machine Translation</title>
<author confidence="0.999929">Rafael E Banchs</author>
<affiliation confidence="0.95896">Institute for Infocomm Research</affiliation>
<address confidence="0.435716">1 Fusionopolis Way, #21-01, Singapore 138632</address>
<email confidence="0.684972">rembanchs@i2r.a-star.edu.sg</email>
<abstract confidence="0.996480538461538">This paper presents a new principled approach to context-aware machine translation. The proposed approach reformulates the posterior probability of a translation hypothesis given the source input by incorporating the source-context information as an additional conditioning variable. As a result, a new model component, which is referred to as the context-awareness model, is added into the original noisy channel framework. A specific computational implementation for the new model component is also described along with its main properties and limitations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R E Banchs</author>
<author>M R Costa-jussà</author>
</authors>
<title>A Semantic Feature for Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, ACL HLT</booktitle>
<pages>126--134</pages>
<contexts>
<context position="7085" citStr="Banchs and Costa-jussà 2011" startWordPosition="1067" endWordPosition="1070"> some room is left for incorporating source-context information in the hypothesis generation process by means of context-aware feature models. This is basically done by using features that relate the occurrences of sub-sentence units with relevant source-context information of lager extension. Several research works have already addressed the problem of incorporating source context information into the translation process within the maximum entropy framework (Carpuat and Wu, 2007; Carpuat and Wu 2008; Haque et al. 2009; España-Bonet et al. 2009; Costa-jussà and Banchs 2010; Haque et al. 2010; Banchs and Costa-jussà 2011). In the following section, we will reformulate the translation problem, as originally described in (1), in order to provide a principled approach to context-aware machine translation for both the noisy channel and the phrase-based approaches. As seen later, this will result in the incorporation of a new model component, which can be also used as a feature function within the context of the maximum entropy framework. 3 Model Derivation In our proposed formulation for context-aware machine translation, we assume that the most likely translation 𝑇� for a source sentence 𝑆 does not depends on 𝑆 o</context>
</contexts>
<marker>Banchs, Costa-jussà, 2011</marker>
<rawString>Banchs, R.E., Costa-jussà, M. R. 2011. A Semantic Feature for Statistical Machine Translation. In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, ACL HLT 2011, pp. 126-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della-Pietra</author>
<author>V Della-Pietra</author>
<author>R Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="3575" citStr="Brown et al., 1993" startWordPosition="533" endWordPosition="536"> the mathematical representation in (2) is counterintuitive to the notion of source-context information being useful for selecting appropriate translations. This problem becomes more relevant when the probability models in (2) are decomposed into sub-sentence level probabilities for operational purposes. Indeed, the computational implementation of (2) requires the decomposition of sentence-level probabilities p(S|T) and p(T) into subsentence level probabilities p (s |t) and p (t) , were s and t refer to sub-sentence units, such as words or groups of words. In the original problem formulation (Brown et al., 1993), the sentence-level translation model p(S|T) in (2) is approximated by means of wordlevel probabilities, and the sentence-level language model p(T) is approximated by means of word n-gram probabilities. 70 Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 70–74, Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics Within this framework, translation probabilities at the sentence-level are estimated from wordlevel probabilities as follows1 : 𝑝(𝑆|𝑇) = ∏𝑘∑𝑛𝑝(𝑠𝑘|𝑡𝑛) (3) where 𝑠𝑘 and 𝑡𝑛 refer to individual words occurring </context>
</contexts>
<marker>Brown, Della-Pietra, Della-Pietra, Mercer, 1993</marker>
<rawString>Brown, P., Della-Pietra, S., Della-Pietra, V., Mercer, R. 1993. The Mathematics of Statistical Machine Translation: Computational Linguistics 19(2), 263-- 311</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>How Phrase Sense Disambiguation Outperforms Word Sense Disambiguation for Statistical Machine Translation. In:</title>
<date>2007</date>
<booktitle>11th International Conference on Theoretical and Methodological Issues in Machine Translation. Skovde</booktitle>
<contexts>
<context position="6941" citStr="Carpuat and Wu, 2007" startWordPosition="1043" endWordPosition="1046">stricted context provided by the same sub-sentence unit level as observed from the training data. In the case of (5), on the other hand, some room is left for incorporating source-context information in the hypothesis generation process by means of context-aware feature models. This is basically done by using features that relate the occurrences of sub-sentence units with relevant source-context information of lager extension. Several research works have already addressed the problem of incorporating source context information into the translation process within the maximum entropy framework (Carpuat and Wu, 2007; Carpuat and Wu 2008; Haque et al. 2009; España-Bonet et al. 2009; Costa-jussà and Banchs 2010; Haque et al. 2010; Banchs and Costa-jussà 2011). In the following section, we will reformulate the translation problem, as originally described in (1), in order to provide a principled approach to context-aware machine translation for both the noisy channel and the phrase-based approaches. As seen later, this will result in the incorporation of a new model component, which can be also used as a feature function within the context of the maximum entropy framework. 3 Model Derivation In our proposed </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Carpuat, M., Wu, D. 2007. How Phrase Sense Disambiguation Outperforms Word Sense Disambiguation for Statistical Machine Translation. In: 11th International Conference on Theoretical and Methodological Issues in Machine Translation. Skovde</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Evaluation of ContextDependent Phrasal Translation Lexicons for Statistical Machine Translation. In:</title>
<date>2008</date>
<booktitle>6th International Conference on Language Resources and Evaluation (LREC). Marrakech</booktitle>
<contexts>
<context position="6962" citStr="Carpuat and Wu 2008" startWordPosition="1047" endWordPosition="1050">ded by the same sub-sentence unit level as observed from the training data. In the case of (5), on the other hand, some room is left for incorporating source-context information in the hypothesis generation process by means of context-aware feature models. This is basically done by using features that relate the occurrences of sub-sentence units with relevant source-context information of lager extension. Several research works have already addressed the problem of incorporating source context information into the translation process within the maximum entropy framework (Carpuat and Wu, 2007; Carpuat and Wu 2008; Haque et al. 2009; España-Bonet et al. 2009; Costa-jussà and Banchs 2010; Haque et al. 2010; Banchs and Costa-jussà 2011). In the following section, we will reformulate the translation problem, as originally described in (1), in order to provide a principled approach to context-aware machine translation for both the noisy channel and the phrase-based approaches. As seen later, this will result in the incorporation of a new model component, which can be also used as a feature function within the context of the maximum entropy framework. 3 Model Derivation In our proposed formulation for conte</context>
</contexts>
<marker>Carpuat, Wu, 2008</marker>
<rawString>Carpuat, M., Wu, D. 2008. Evaluation of ContextDependent Phrasal Translation Lexicons for Statistical Machine Translation. In: 6th International Conference on Language Resources and Evaluation (LREC). Marrakech</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Costa-jussà</author>
<author>R E Banchs</author>
</authors>
<title>A VectorSpace Dynamic Feature for Phrase-Based Statistical Machine Translation.</title>
<date>2010</date>
<journal>Journal of Intelligent Information Systems</journal>
<contexts>
<context position="7036" citStr="Costa-jussà and Banchs 2010" startWordPosition="1059" endWordPosition="1062">ing data. In the case of (5), on the other hand, some room is left for incorporating source-context information in the hypothesis generation process by means of context-aware feature models. This is basically done by using features that relate the occurrences of sub-sentence units with relevant source-context information of lager extension. Several research works have already addressed the problem of incorporating source context information into the translation process within the maximum entropy framework (Carpuat and Wu, 2007; Carpuat and Wu 2008; Haque et al. 2009; España-Bonet et al. 2009; Costa-jussà and Banchs 2010; Haque et al. 2010; Banchs and Costa-jussà 2011). In the following section, we will reformulate the translation problem, as originally described in (1), in order to provide a principled approach to context-aware machine translation for both the noisy channel and the phrase-based approaches. As seen later, this will result in the incorporation of a new model component, which can be also used as a feature function within the context of the maximum entropy framework. 3 Model Derivation In our proposed formulation for context-aware machine translation, we assume that the most likely translation 𝑇</context>
</contexts>
<marker>Costa-jussà, Banchs, 2010</marker>
<rawString>Costa-jussà, M. R., Banchs, R.E. 2010. A VectorSpace Dynamic Feature for Phrase-Based Statistical Machine Translation. Journal of Intelligent Information Systems</rawString>
</citation>
<citation valid="true">
<authors>
<author>C España-Bonet</author>
<author>J Gimenez</author>
<author>L Marquez</author>
</authors>
<title>Discriminative Phrase-Based Models for Arabic Machine Translation.</title>
<date>2009</date>
<journal>ACM Transactions on Asian Language Information Processing Journal (Special Issue on Arabic Natural Language Processing)</journal>
<contexts>
<context position="7007" citStr="España-Bonet et al. 2009" startWordPosition="1055" endWordPosition="1058">as observed from the training data. In the case of (5), on the other hand, some room is left for incorporating source-context information in the hypothesis generation process by means of context-aware feature models. This is basically done by using features that relate the occurrences of sub-sentence units with relevant source-context information of lager extension. Several research works have already addressed the problem of incorporating source context information into the translation process within the maximum entropy framework (Carpuat and Wu, 2007; Carpuat and Wu 2008; Haque et al. 2009; España-Bonet et al. 2009; Costa-jussà and Banchs 2010; Haque et al. 2010; Banchs and Costa-jussà 2011). In the following section, we will reformulate the translation problem, as originally described in (1), in order to provide a principled approach to context-aware machine translation for both the noisy channel and the phrase-based approaches. As seen later, this will result in the incorporation of a new model component, which can be also used as a feature function within the context of the maximum entropy framework. 3 Model Derivation In our proposed formulation for context-aware machine translation, we assume that </context>
</contexts>
<marker>España-Bonet, Gimenez, Marquez, 2009</marker>
<rawString>España-Bonet, C., Gimenez, J., Marquez, L. 2009. Discriminative Phrase-Based Models for Arabic Machine Translation. ACM Transactions on Asian Language Information Processing Journal (Special Issue on Arabic Natural Language Processing)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis,</title>
<date>1957</date>
<volume>51</volume>
<pages>1--31</pages>
<contexts>
<context position="1030" citStr="Firth, 1957" startWordPosition="141" endWordPosition="142">ext information as an additional conditioning variable. As a result, a new model component, which is referred to as the context-awareness model, is added into the original noisy channel framework. A specific computational implementation for the new model component is also described along with its main properties and limitations. 1 Introduction It is well known that source-context information plays a significant role in human-based language translation (Padilla and Bajo, 1998). A similar claim can be supported for the case of Machine Translation on the grounds of the Distributional Hypothesis (Firth, 1957). According to the Distributional Hypothesis, much of the meaning of a given word is implied by its context rather than by the word itself. In this work, we first focus our attention on the fact that the classical formulation of the statistical machine translation framework, implicitly disregards the role of source-context information within the translation generation process. Based on this, we propose a principled reformulation that allows for introducing context-awareness into the statistical machine translation framework. Then, a specific computational implementation for the newly proposed </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J.R. 1957. A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis, 51: 1-31</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Haque</author>
<author>S K Naskar</author>
<author>Y Ma</author>
<author>A Way</author>
</authors>
<title>Using Supertags as Source Language Context in SMT. In:</title>
<date>2009</date>
<booktitle>13th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>234--241</pages>
<location>Barcelona</location>
<contexts>
<context position="6981" citStr="Haque et al. 2009" startWordPosition="1051" endWordPosition="1054">entence unit level as observed from the training data. In the case of (5), on the other hand, some room is left for incorporating source-context information in the hypothesis generation process by means of context-aware feature models. This is basically done by using features that relate the occurrences of sub-sentence units with relevant source-context information of lager extension. Several research works have already addressed the problem of incorporating source context information into the translation process within the maximum entropy framework (Carpuat and Wu, 2007; Carpuat and Wu 2008; Haque et al. 2009; España-Bonet et al. 2009; Costa-jussà and Banchs 2010; Haque et al. 2010; Banchs and Costa-jussà 2011). In the following section, we will reformulate the translation problem, as originally described in (1), in order to provide a principled approach to context-aware machine translation for both the noisy channel and the phrase-based approaches. As seen later, this will result in the incorporation of a new model component, which can be also used as a feature function within the context of the maximum entropy framework. 3 Model Derivation In our proposed formulation for context-aware machine tr</context>
</contexts>
<marker>Haque, Naskar, Ma, Way, 2009</marker>
<rawString>Haque, R., Naskar, S. K., Ma, Y., Way, A. 2009. Using Supertags as Source Language Context in SMT. In: 13th Annual Conference of the European Association for Machine Translation, pp. 234--241. Barcelona</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Haque</author>
<author>S K Naskar</author>
<author>A van den Bosh</author>
<author>A Way</author>
</authors>
<title>Supertags as Source Language Context</title>
<date>2010</date>
<booktitle>in Hierarchical Phrase-Based SMT. In: 9th Conference of the Association for Machine Translation in the Americas (AMTA)</booktitle>
<marker>Haque, Naskar, van den Bosh, Way, 2010</marker>
<rawString>Haque, R., Naskar, S. K., van den Bosh, A., Way, A. 2010. Supertags as Source Language Context in Hierarchical Phrase-Based SMT. In: 9th Conference of the Association for Machine Translation in the Americas (AMTA)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation. In:</title>
<date>2003</date>
<booktitle>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLTEMNLP),</booktitle>
<pages>48--54</pages>
<location>Edmonton</location>
<contexts>
<context position="4514" citStr="Koehn et al., 2003" startWordPosition="677" endWordPosition="680"> Sweden, April 27, 2014. c�2014 Association for Computational Linguistics Within this framework, translation probabilities at the sentence-level are estimated from wordlevel probabilities as follows1 : 𝑝(𝑆|𝑇) = ∏𝑘∑𝑛𝑝(𝑠𝑘|𝑡𝑛) (3) where 𝑠𝑘 and 𝑡𝑛 refer to individual words occurring in 𝑆 and 𝑇, respectively. The probabilities 𝑝 (𝑠𝑘 |𝑡𝑛) are referred to as lexical models and they represent the probability of an individual source word 𝑠𝑘 to be the translation of a given target word 𝑡𝑛. These lexical models are estimated by using word alignment probabilities. In statistical phrase-based translation (Koehn et al., 2003), the translation model is approximated by means of phrase-level probabilities (a phrase is a bilingual pair of sub-sentence units that is consistent with the word alignments). Within this framework, translation probabilities at the sentence-level are computed from phraselevel probabilities as follows: 𝑝(𝑆|𝑇) = ∏𝑖𝑝(𝑠𝑖|𝑡𝑖) (4) where 𝑠𝑖 and 𝑡𝑖 refer to phrases (i.e. groups of words) occurring in 𝑆 and 𝑇, respectively. The probabilities 𝑝(𝑠𝑖|𝑡𝑖) are estimated by means of relative frequencies and, accordingly, they are referred to as relative frequency models. Finally, in (Och and Ney, 2002), the </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F. J., Marcu, D. 2003. Statistical Phrase-Based Translation. In: Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLTEMNLP), pp. 48--54. Edmonton</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In:</title>
<date>2002</date>
<booktitle>40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="5108" citStr="Och and Ney, 2002" startWordPosition="766" endWordPosition="769">ion (Koehn et al., 2003), the translation model is approximated by means of phrase-level probabilities (a phrase is a bilingual pair of sub-sentence units that is consistent with the word alignments). Within this framework, translation probabilities at the sentence-level are computed from phraselevel probabilities as follows: 𝑝(𝑆|𝑇) = ∏𝑖𝑝(𝑠𝑖|𝑡𝑖) (4) where 𝑠𝑖 and 𝑡𝑖 refer to phrases (i.e. groups of words) occurring in 𝑆 and 𝑇, respectively. The probabilities 𝑝(𝑠𝑖|𝑡𝑖) are estimated by means of relative frequencies and, accordingly, they are referred to as relative frequency models. Finally, in (Och and Ney, 2002), the maximum entropy framework was introduced into machine translation and the two-model formulation in the noisy channel approach (2) was extended to the log-linear combination of as many relevant models as can be reasonably derived from the training data. In addition, the maximum entropy framework also allows for tuning the weights in the log-linear combinations of models by means of discriminative training. Within this framework, translation probabilities at the sentence-level are estimated from phraselevel probabilities as follows: 𝑝(𝑇|𝑆) = 1 ζexp{∑𝑖 ∑𝑚 𝜆𝑚 ℎ𝑚 (𝑡𝑖, 𝑠𝑖)} (5) where ℎ𝑚(𝑠𝑖, 𝑡𝑖</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och, F. J., Ney, H. (2002) Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In: 40th Annual Meeting of the Association for Computational Linguistics, pp. 295--302</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Padilla</author>
<author>T Bajo</author>
</authors>
<title>Hacia un Modelo de Memoria y Atención en la Interpretación Simultánea.</title>
<date>1998</date>
<journal>Quaderns: Revista de Traducció</journal>
<volume>2</volume>
<pages>107--117</pages>
<contexts>
<context position="898" citStr="Padilla and Bajo, 1998" startWordPosition="118" endWordPosition="121">he proposed approach reformulates the posterior probability of a translation hypothesis given the source input by incorporating the source-context information as an additional conditioning variable. As a result, a new model component, which is referred to as the context-awareness model, is added into the original noisy channel framework. A specific computational implementation for the new model component is also described along with its main properties and limitations. 1 Introduction It is well known that source-context information plays a significant role in human-based language translation (Padilla and Bajo, 1998). A similar claim can be supported for the case of Machine Translation on the grounds of the Distributional Hypothesis (Firth, 1957). According to the Distributional Hypothesis, much of the meaning of a given word is implied by its context rather than by the word itself. In this work, we first focus our attention on the fact that the classical formulation of the statistical machine translation framework, implicitly disregards the role of source-context information within the translation generation process. Based on this, we propose a principled reformulation that allows for introducing context</context>
</contexts>
<marker>Padilla, Bajo, 1998</marker>
<rawString>Padilla, P., Bajo, T. (1998) Hacia un Modelo de Memoria y Atención en la Interpretación Simultánea. Quaderns: Revista de Traducció 2, 107--117</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
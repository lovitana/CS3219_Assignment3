<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025855">
<title confidence="0.976451">
The Illinois-Columbia System in the CoNLL-2014 Shared Task
</title>
<author confidence="0.997868">
Alla Rozovskaya1 Kai-Wei Chang2 Mark Sammons2 Dan Roth2 Nizar Habash1
</author>
<affiliation confidence="0.981911">
1Center for Computational Learning Systems, Columbia University
</affiliation>
<email confidence="0.9726">
{alla,habash}@ccls.columbia.edu
</email>
<sectionHeader confidence="0.421891" genericHeader="abstract">
2 Cognitive Computation Group, University of Illinois at Urbana-Champaign
</sectionHeader>
<email confidence="0.996426">
{kchang10,mssammon,danr}@illinois.edu
</email>
<sectionHeader confidence="0.997358" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999931333333333">
The CoNLL-2014 shared task is an ex-
tension of last year’s shared task and fo-
cuses on correcting grammatical errors in
essays written by non-native learners of
English. In this paper, we describe the
Illinois-Columbia system that participated
in the shared task. Our system ranked sec-
ond on the original annotations and first on
the revised annotations.
The core of the system is based on the
University of Illinois model that placed
first in the CoNLL-2013 shared task. This
baseline model has been improved and ex-
panded for this year’s competition in sev-
eral respects. We describe our underly-
ing approach, which relates to our previ-
ous work, and describe the novel aspects
of the system in more detail.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948066666667">
The topic of text correction has seen a lot of inter-
est in the past several years, with a focus on cor-
recting grammatical errors made by English as a
Second Language (ESL) learners. ESL error cor-
rection is an important problem since most writers
of English are not native English speakers. The in-
creased interest in this topic can be seen not only
from the number of papers published on the topic
but also from the three competitions devoted to
grammatical error correction for non-native writ-
ers that have recently taken place: HOO-2011
(Dale and Kilgarriff, 2011), HOO-2012 (Dale et
al., 2012), and the CoNLL-2013 shared task (Ng
et al., 2013).
In all three shared tasks, the participating sys-
tems performed at a level that is considered ex-
tremely low compared to performance obtained in
other areas of NLP: even the best systems attained
F1 scores in the range of 20-30 points.
The key reason that text correction is a diffi-
cult task is that even for non-native English speak-
ers, writing accuracy is very high, as errors are
very sparse. Even for some of the most com-
mon types of errors, such as article and preposi-
tion usage, the majority of the words in these cate-
gories (over 90%) are used correctly. For instance,
in the CoNLL training data, only 2% of preposi-
tions are incorrectly used. Because errors are so
sparse, it is more difficult for a system to identify a
mistake accurately and without introducing many
false alarms.
The CoNLL-2014 shared task (Ng et al., 2014)
is an extension of the CoNLL-2013 shared task
(Ng et al., 2013). Both competitions make use
of essays written by ESL learners at the National
University of Singapore. However, while the first
one focused on five kinds of mistakes that are com-
monly made by ESL writers – article, preposition,
noun number, verb agreement, and verb form –
this year’s competition covers all errors occurring
in the data. Errors outside the target group were
present in the task corpora last year as well, but
were not evaluated.
Our system extends the one developed by the
University of Illinois (Rozovskaya et al., 2013)
that placed first in the CoNLL-2013 competition.
For this year’s shared task, the system has been
extended and improved in several respects: we ex-
tended the set of errors addressed by the system,
developed a general approach for improving the
error-specific models, and added a joint inference
component to address interaction among errors.
See Rozovskaya and Roth (2013) for more detail.
We briefly discuss the task (Section 2) and give
an overview of the baseline Illinois system (Sec-
tion 3). Section 4 presents the novel aspects of the
system. In Section 5, we evaluate the complete
system on the development data and show the re-
sults obtained on test. We offer error analysis and a
brief discussion in Section 6. Section 7 concludes.
</bodyText>
<page confidence="0.982642">
34
</page>
<note confidence="0.9819115">
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34–42,
Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.984067379310345">
Error type Rel. freq. Examples
Article (ArtOrDet) 14.98% *∅/The government should help encourage *the/∅
breakthroughs as well as *a/∅ complete medication
system .
Wrong collocation (Wci) 11.94% Some people started to *think/wonder if electronic
products can replace human beings for better perfor-
mances .
Local redundancy (Rloc-) 10.52% Some solutions *{as examples}/∅ would be to design
plants/fertilizers that give higher yield ...
Noun number (Nn) 8.49% There are many reports around the internet and on
newspaper stating that some users ’ *iPhone/iPhones
exploded.
Verb tense (Vt) 7.21% Through the thousands of years , most Chinese scholars
*are/{have been} greatly affected by Confucianism.
Orthography/punctuation (Mec) 6.88% Even British Prime Minister , Gordon Brown *∅/, has
urged that all cars in *britain/Britain to be green by
2020 .
Preposition (Prep) 5.43% I do not agree *on/with this argument that surveillance
technology should not be used to track people .
Word form (Wform) 4.87% On the other hand, the application of surveillance tech-
nology serves as a warning to the *murders/murderers
and they might not commit more murder.
Subject-verb agreement (SVA) 3.44% However, tracking people *are/is difficult and different
from tracking goods .
Verb form (Vform) 3.25% Travelers survive in desert thanks to GPS
*guide/guiding them.
Tone (Wtone) 1.29% Hence , as technology especially in the medical field
continues to get developed and updated , people {do
n’t}/{do not} risk their lives anymore.
</table>
<tableCaption confidence="0.995066">
Table 1: Example errors. In the parentheses, the error codes used in the shared task are shown. Note
</tableCaption>
<bodyText confidence="0.9221435">
that only the errors exemplifying the relevant phenomena are marked in the table; the sentences may
contain other mistakes. Errors marked as verb form include multiple grammatical phenomena that may
characterize verbs. Our system addresses all of the error types except “Wrong Collocation” and “Local
Redundancy”.
</bodyText>
<sectionHeader confidence="0.979031" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999880653846154">
Both the training and the test data of the CoNLL-
2014 shared task consist of essays written by stu-
dents at the National University of Singapore. The
training data contains 1.2 million words from the
NUCLE corpus (Dahlmeier et al., 2013) corrected
by English teachers, and an additional set of about
30,000 words that was released last year as a test
set for the CoNLL-2013 shared task. We use last
year’s test data as a development set; the results in
the subsequent sections are reported on this subset.
The CoNLL corpus error tagset includes 28 er-
ror categories. Table 1 illustrates the most com-
mon error categories in the training data; errors are
marked with an asterisk, and ∅ denotes a missing
word. Our system targets all of these, with the ex-
ception of collocation and local redundancy errors.
Among the less commonly occurring error types,
our system addresses tone (style) errors; these are
illustrated in the table.
It should be noted that the proportion of erro-
neous instances is several times higher in the de-
velopment data than in the training data for all of
the error categories. For example, while only 2.4%
of noun phrases in the training data have deter-
miner errors, in the development data 10% of noun
phrases have determiner errors.
</bodyText>
<page confidence="0.998756">
35
</page>
<table confidence="0.998876">
“Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear tech-
nology.”
Error type Confusion set
Noun number {factor, factors}
Verb Agreement {contribute, contributes}
Verb Form {included, including,
includes, include}
</table>
<tableCaption confidence="0.989495">
Table 2: Sample confusion sets for noun num-
ber, verb agreement, and verb form.
</tableCaption>
<sectionHeader confidence="0.903664" genericHeader="method">
3 The Baseline System
</sectionHeader>
<bodyText confidence="0.999706638554217">
In this section, we briefly describe the Univer-
sity of Illinois system (henceforth Illinois; in the
overview paper of the shared task the system is re-
ferred to as UI) that achieved the best result in the
CoNLL-2013 shared task and which we use as our
baseline model. For a complete description, we
refer the reader to Rozovskaya et al. (2013).
The Illinois system implements five
independently-trained machine-learning clas-
sifiers that follow the popular approach to ESL
error correction borrowed from the context-
sensitive spelling correction task (Golding and
Roth, 1999; Carlson et al., 2001). A confusion
set is defined as a list of confusable words.
Each occurrence of a confusable word in text is
represented as a vector of features derived from a
context window around the target. The problem
is cast as a multi-class classification task and a
classifier is trained on native or learner data. At
prediction time, the model selects the most likely
candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions (this
year, we extend the confusion set and also target
extraneous preposition usage). The article confu-
sion set is as follows: {a, the, 0}.1 The confu-
sion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants. Table 2 shows sample confusion
sets for noun, agreement, and form errors.
Each classifier takes as input the corpus doc-
uments preprocessed with a part-of-speech tag-
10 denotes noun-phrase-initial contexts where an article
is likely to have been omitted. The variants “a” and “an” are
conflated and are restored later.
ger2 and shallow parser3 (Punyakanok and Roth,
2001). The other system components use the pre-
processing tools only as part of candidate genera-
tion (e.g., to identify all nouns in the data for the
noun classifier).
The choice of learning algorithm for each clas-
sifier is motivated by earlier findings showing
that discriminative classifiers outperform other
machine-learning methods on error correction
tasks (Rozovskaya and Roth, 2011). Thus, the
classifiers trained on the learner data make use of
a discriminative model. Because the Google cor-
pus does not contain complete sentences but only
n-gram counts of length up to five, training a dis-
criminative model is not desirable, and we thus use
NB (details in Rozovskaya and Roth (2011)).
The article classifier is a discriminative model
that draws on the state-of-the-art approach de-
scribed in Rozovskaya et al. (2012). The model
makes use of the Averaged Perceptron (AP) algo-
rithm (Freund and Schapire, 1996) and is trained
on the training data of the shared task with rich
features. The article module uses the POS and
chunker output to generate some of its features and
candidates (likely contexts for missing articles).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging mistakes, resulting in low
recall. To avoid this problem, we adopt the ap-
proach proposed in Rozovskaya et al. (2012), the
error inflation method, and add artificial article er-
rors to the training data based on the error distribu-
tion on the training set. This method prevents the
source feature from dominating the context fea-
tures, and improves the recall of the system.
The other classifiers in the baseline system –
noun number, verb agreement, verb form, and
preposition – are trained on native English data,
the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Naive
Bayes (NB) algorithm. All models use word n-
gram features derived from the 4-word window
around the target word. In the preposition model,
priors for preposition preferences are learned from
the shared task training data (Rozovskaya and
Roth, 2011).
The modules targeting verb agreement and
</bodyText>
<footnote confidence="0.99970025">
2http://cogcomp.cs.illinois.edu/page/
software view/POS
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
</footnote>
<page confidence="0.998447">
36
</page>
<bodyText confidence="0.999616666666667">
verb form mistakes draw on the linguistically-
motivated approach to correcting verb errors pro-
posed in Rozovskaya et. al (2014).
</bodyText>
<sectionHeader confidence="0.937815" genericHeader="method">
4 The CoNLL-2014 System
</sectionHeader>
<bodyText confidence="0.999941428571429">
The system in the CoNLL-2014 shared task is im-
proved in three ways: 1) Additional error-specific
classifiers: word form, orthography/punctuation,
and style; 2) Model combination; and 3) Joint in-
ference to address interacting errors. Table 3 sum-
marizes the Illinois and the Illinois-Columbia sys-
tems.
</bodyText>
<subsectionHeader confidence="0.999716">
4.1 Targeting Additional Errors
</subsectionHeader>
<bodyText confidence="0.999895333333333">
The Illinois-Columbia system implements several
new classifiers to address word form, orthography
and punctuation, and style errors (Table 1).
</bodyText>
<subsectionHeader confidence="0.593358">
4.1.1 Word Form Errors
</subsectionHeader>
<bodyText confidence="0.999875764705882">
Word form (Wform) errors are grammatical er-
rors that involve confusing words that share a
base form but differ in derivational morphology,
e.g. “use” and “usage” (see also Table 1). Con-
fusion sets for word form errors thus should in-
clude words that differ derivationally but share the
same base form. In contrast to verb form errors
where confusion sets specify all possible inflec-
tional forms for a given verb, here, the associated
parts-of-speech may vary more widely. An ex-
ample of a confusion set is {technique, technical,
technology, technological}.
Because word form errors encompass a wide
range of misuse, one approach is to consider ev-
ery word as an error candidate. We follow a more
conservative method and only attempt to correct
those words that occurred in the training data and
were tagged as word form errors (we cleaned up
that list by removing noisy annotations).
A further challenge in addressing word form er-
rors is generating confusion sets. We found that
about 45% of corrections for word form errors in
the development data are covered by the confusion
sets from the training data for the same word. We
thus derive the confusion sets using the training
data. Specifically, for every source word that is
tagged as a word form error in the training data,
the confusion set includes all labels to which that
word is mapped in the training data. In addition,
plural and singular forms are added for all words
tagged as nouns, and inflectional forms are added
for words tagged as verbs. For more detail on
correcting verb errors, we refer the reader to Ro-
zovskaya et al. (2014).
</bodyText>
<subsubsectionHeader confidence="0.822502">
4.1.2 Orthography and Punctuation Errors
</subsubsectionHeader>
<bodyText confidence="0.999991043478261">
The Mec error category includes errors in
spelling, context-sensitive spelling, capitalization,
and punctuation. Our system addresses punctua-
tion errors and capitalization errors.
To correct capitalization errors, we collected
words that are always capitalized in the train-
ing and development data when not occurring
sentence-initially.
The punctuation classifier includes two mod-
ules: a learned component targets missing and
extraneous comma usage and is an AP classifier
trained on the learner data with error inflation.
A second, pattern-based component, complements
the AP model: it inserts missing commas by using
a set of patterns that overwhelmingly prefer the us-
age of a comma, e.g. when a sentence starts with
the word “hence”. The patterns are learned auto-
matically over the training data: specifically, us-
ing a sliding window of three words on each side,
we compiled a list of word n-gram contexts that
are strongly associated with the usage of a comma.
This list is then used to insert missing commas in
the test data.
</bodyText>
<subsectionHeader confidence="0.772049">
4.1.3 Style Errors
</subsectionHeader>
<bodyText confidence="0.999884">
The style (Wtone) errors marked in the corpus are
diverse, and the annotations are often not consis-
tent. We constructed a pattern-based system to
deal with two types of style errors that are com-
monly annotated. The first type of style edit avoids
using contractions of negated auxiliary verbs. For
example, it changes “do n’t” to “do not”. We use a
pattern-based classifier to identify such errors and
replace the contractions. The second type of style
edit encourages the use of a semi-colon to join
two independent clauses when a conjunctive ad-
verb is used. For example, it edits “[clause], how-
ever, [clause]” to “[clause]; however, [clause]”. To
identify such errors, we use a part-of-speech tag-
ger to recognize conjunctive adverbs signifying in-
dependent clauses: if two clauses are joined by the
pattern “, [conjunctive adverb],”, we will replace it
with “; [conjunctive adverb],”.
</bodyText>
<subsectionHeader confidence="0.997808">
4.2 Modules not Included in the Final System
</subsectionHeader>
<bodyText confidence="0.999941333333333">
In addition to the modules described above, we at-
tempted to address two other common error cate-
gories: spelling errors and collocation errors. We
</bodyText>
<page confidence="0.99695">
37
</page>
<table confidence="0.999872">
Illinois
Classifiers Training data Algorithm
Article Learner AP with inflation
Preposition Native NB-priors
Noun number Native NB
Verb agreement Native NB
Verb form Native NB
Illinois-Columbia
Classifiers Training data Algorithm
Article Learner and native AP with infl. (learner) and NB-priors (native)
Preposition Learner and native AP with infl. (learner) and NB-priors (native)
Noun number Learner and native AP with infl. (learner) and NB (native)
Verb agreement Native AP with infl. (learner) and NB (native)
Verb form Native NB-priors
Word form Native NB-priors
Orthography/punctuation Learner AP and pattern-based
Style Learner Pattern-based
Model combination Section 4.3
Global inference Section 4.4
</table>
<tableCaption confidence="0.965396">
Table 3: The baseline (Illinois) system vs. the Illinois-Columbia system. AP stands for Averaged
Perceptron, and NB stands for the Naive Bayes algorithm.
</tableCaption>
<bodyText confidence="0.999881230769231">
describe these below even though they were not
included in the final system.
Regular spelling errors are noticeable but not
very frequent, and a number are not marked in
the corpus (for example, the word “dictronary” in-
stead of “dictionary” is not tagged as an error). We
used an open source package – “Jazzy”4 – to at-
tempt to automatically correct these errors to im-
prove context signals for other modules. However,
there are often multiple similar words that can be
proposed as corrections, and Jazzy uses phonetic
guidelines that sometimes lead to unintuitive pro-
posals (such as “doctrinaire” for “dictronary”). It
would be possible to extend the system with a filter
on candidate answers that uses n-grams or some
other context model to choose better candidates,
but the relatively small number of such errors lim-
its the potential impact of such a system.
Collocation errors are the second most common
error category accounting for 11.94% of all errors
in the training data (Table 1). We tried using the
Illinois context-sensitive spelling system5 to de-
tect these errors, but this system requires prede-
fined confusion sets to detect possible errors and
to propose valid corrections. The coverage of the
pre-existing confusion sets was poor – the system
</bodyText>
<footnote confidence="0.999717">
4http://jazzy.sourceforge.net
5http://cogcomp.cs.illinois.edu/cssc/
</footnote>
<bodyText confidence="0.9986005">
could potentially correct only 2.5% of collocation
errors – and it is difficult to generate new con-
fusion sets that generalize well, which requires a
great deal of annotated training data. The sys-
tem performance was relatively poor because it
proposed many spurious corrections: we believe
this is due to the relatively limited context it uses,
which makes it particularly susceptible to making
mistakes when there are multiple errors in close
proximity.
</bodyText>
<subsectionHeader confidence="0.998769">
4.3 Model Combination
</subsectionHeader>
<bodyText confidence="0.99998675">
Model combination is another key extension of the
Illinois system.
In the Illinois-Columbia system, article, prepo-
sition, noun, and verb agreement errors are each
addressed via a model that combines error predic-
tions made by a classifier trained on the learner
data with the AP algorithm and those made by
the NB model trained on the Google corpus. The
AP classifiers all make use of richer sets of fea-
tures than the native-trained classifiers: the article,
noun number, and preposition classifiers employ
features that use POS information, while the verb
agreement classifier also makes use of dependency
features extracted using a parser (de Marneffe et
al., 2008). For more detail on the features used
in the agreement module, we refer the reader to
</bodyText>
<page confidence="0.998153">
38
</page>
<bodyText confidence="0.9998665">
Rozovskaya et al. (2014). Finally, all of the AP
models use the source word of the author as a fea-
ture and, similar to the article AP classifier (Sec-
tion 3), implement the error inflation method. The
combined model generates a union of corrections
produced by the components.
We found that for every error type, the com-
bined model is superior to each of the single classi-
fiers, as it combines the advantages of both of the
classifiers so that they complement one another.
In particular, while each of the learner and native
components have similar precision, since the pre-
dictions made differ, the recall of the combined
model improves.
</bodyText>
<subsectionHeader confidence="0.990806">
4.4 Joint Inference
</subsectionHeader>
<bodyText confidence="0.999981210526316">
One of the mistakes typical for Illinois system
were inconsistent predictions. Inconsistent predic-
tions occur when the classifiers address grammat-
ical phenomena that interact at the sentence level,
e.g. noun number and verb agreement. To ad-
dress this problem, the Illinois-Columbia system
makes use of global inference via an Integer Lin-
ear Programming formulation (Rozovskaya and
Roth, 2013). Note that Rozovskaya and Roth
(2013) also describe a joint learning model that
performs better than the joint inference approach.
However, the joint learning model is based on
training a joint model on the Google corpus, and
is not as strong as the individually-trained classi-
fiers of the Illinois-Columbia system that combine
predictions from two components – NB classifiers
trained on the native data from the Google corpus
and AP models trained on the learner data (Sec-
tion 4.3).
</bodyText>
<sectionHeader confidence="0.996605" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.976833928571429">
In Sections 3 and 4, we described the individual
system components that address different types of
errors. In this section, we show how the system
improves when each component is added into the
system. In this year’s competition, systems are
compared using F0.5 measure instead of F1. This
is because in error correction good precision is
more important than having a high recall, and the
F0.5 reflects that by weighing precision twice as
much as recall. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012).
Table 4 reports performance results of each in-
dividual classifier. In the final system, the arti-
cle, preposition, noun number, and verb agree-
</bodyText>
<table confidence="0.999976235294118">
Model P R F0.5
Articles (AP) 38.97 8.85 23.19
Articles (NB-priors) 47.34 6.01 19.93
Articles (Comb.) 38.73 10.93 25.67
Prep. (AP) 34.00 0.5 2.35
Prep. (NB-priors) 33.33 0.79 3.61
Prep. (Comb.) 30.06 1.17 5.13
Noun number (NB) 44.74 5.48 18.39
Noun number (AP) 82.35 0.41 2.01
Noun number (Comb.) 45.02 5.57 18.63
Verb agr. (AP) 38.56 1.23 5.46
Verb agr. (NB) 63.41 0.76 3.64
Verb agr. (Comb.) 41.09 1.55 6.75
Verb form (NB-priors) 59.26 1.41 6.42
Word form (NB-priors) 57.54 3.02 12.48
Mec (AP; patterns) 48.48 0.47 2.26
Style (patterns) 84.62 0.64 3.13
</table>
<tableCaption confidence="0.989753">
Table 4: Performance of classifiers targeting
specific errors.
</tableCaption>
<table confidence="0.999989466666667">
Model P R F0.5
The baseline (Illinois) system
Articles 38.97 8.85 23.19
+Prepositions 39.24 9.35 23.93
+Noun number 42.13 14.83 30.79
+Subject-verb agr. 42.25 16.06 31.86
+Verb form 43.19 17.20 33.17
Model Combination
+Model combination 42.72 20.19 34.92
Additional Classifiers
+Word form 43.39 21.54 36.07
+Mec 43.70 22.04 36.52
+Style 44.22 21.54 37.09
Joint Inference
+Joint Inference 44.28 22.57 37.13
</table>
<tableCaption confidence="0.826607">
Table 5: Results on the development data. The
top part of the table shows the performance of the
baseline (Illinois) system from last year.
</tableCaption>
<table confidence="0.9992394">
P R F0.5
Scores based on the original annotations
41.78 24.88 36.79
Scores based on the revised annotations
52.44 29.89 45.57
</table>
<tableCaption confidence="0.998838">
Table 6: Results on Test.
</tableCaption>
<page confidence="0.999284">
39
</page>
<bodyText confidence="0.999930447368421">
ment classifiers use combined models, each con-
sisting of a classifier trained on the learner data
and a classifier trained on native data. We report
performance of each such component separately
and when they are combined. The results show
that combining models boosts the performance of
each classifier: for example, the performance of
the article classifier improves by more than 2 F0.5
points. It should be noted that results are com-
puted with respect to all errors present in the data.
For this reason, recall is low.
Next, in Table 5, we show the contribution of
the novel components over the baseline system on
the development set. As described in Section 3,
the baseline Illinois system consists of five indi-
vidual components; their performance is shown in
the top part of the table. Note that although for the
development set we make use of last year’s test
set, these results are not comparable to the perfor-
mance results reported in last year’s competition
that used the F1 measure. Overall, the baseline
system achieves an F0.5 score of 33.17 on the de-
velopment set.
Then, by applying the model combination tech-
nique introduced in Section 4.3, the performance
is improved to 34.92. By adding modules to tar-
get three additional error types, the overall perfor-
mance becomes 37.09. Finally, the joint inference
technique (see Section 4.4) slightly improves the
performance further. The final system achieves an
F0.5 score of 37.13.
Table 6 shows the results on the test set provided
by the organizers. As was done previously, the
organizers also offered another set of annotations
based on the combination of revised official anno-
tations and accepted alternative annotations pro-
posed by participants. Performance results on this
set are also shown in Table 6.
</bodyText>
<sectionHeader confidence="0.992532" genericHeader="discussions">
6 Discussion and Error Analysis
</sectionHeader>
<bodyText confidence="0.9999966">
Here, we present some interesting errors that our
system makes on the development set and discuss
our observations on the competition. We analyze
both the false positive errors and those cases that
are missed by our system.
</bodyText>
<subsectionHeader confidence="0.901075">
6.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.978389230769231">
Stylistic preference Surveillance technology
such as RFID (radio-frequency identification) is
one type of examples that has currently been im-
plemented.
Here, our system proposes a change to plural
for the noun “technology”. The gold standard
solution instead proposes a large number of cor-
rections throughout that work with the choice of
the singular “technology”. However, using the
plural “technologies” as proposed by the Illinois-
Columbia system is quite acceptable, and a com-
parable number of corrections would make the rest
of the sentence compatible. Note also that the
gold standard proposes the use of commas around
the phrase “such as RFID (radio-frequency iden-
tification)”, which could also be omitted based on
stylistic considerations alone.
Word choice The high accuracy in utiliz-
ing surveillance technology eliminates the
*amount/number of disagreements among people.
The use of “amount” versus “number” depends
on the noun to which the term attaches. This could
conceivably be achieved by using a rule and word
list, but many such rules would be needed and each
would have relatively low coverage. Our system
does not detect this error.
Presence of multiple errors Not only the details
of location will be provided, but also may lead to
find out the root of this kind of children trading
agency and it helps to prevent more this kind of
tragedy to happen on any family.
The writer has made numerous errors in this
sentence. To determine the correct preposition in
the marked location requires at least the preced-
ing verb phrase to be corrected to “from happen-
ing”; the extraneous “more” after “prevent” in turn
makes the verb phrase correction more unlikely as
it perturbs the contextual clues that a system might
learn to make that correction. Our system pro-
poses a different preposition – “in” – that is better
than the original in the local context, but which is
not correct in the wider context.
Locally coherent, globally incorrect People’s
lives become from increasingly convenient to al-
most luxury, thanks to the implementation of in-
creasingly technology available for the Man’s life.
In this example, the system proposes to delete
the preposition “from”. This correctiom improves
the local coherency of the sentence. However, the
resulting construction is not consistent with “to al-
most luxury”, suggesting a more complex correc-
tion (changing the word “become” to “are going”).
</bodyText>
<page confidence="0.994454">
40
</page>
<bodyText confidence="0.993298545454546">
Cascading NLP errors In this, I mean that we
can input this device implant into an animal or
birds species, for us to track their movements and
actions relating to our human research that can
bring us to a new regime.
The word “implant” in the example sentence
has been identified as a verb by the system and
not a noun due to the unusual use as part of the
phrase “device implant”. As a result, the system
incorrectly proposes the verb form correction “im-
planted”.
</bodyText>
<subsectionHeader confidence="0.966501">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.9999815625">
The error analysis suggests that there are three sig-
nificant challenges to developing a better gram-
mar correction system for the CoNLL-2014 shared
task: identifying candidate errors; modeling the
context of possible errors widely enough to cap-
ture long-distance cues where necessary; and
modeling stylistic preferences involving word
choice, selection of plural or singular, standards
for punctuation, use of a definite or indefinite arti-
cle (or no article at all), and so on. For ESL writ-
ers, the tendency for multiple errors to be made in
close proximity means that global decisions must
be made about sets of possible mistakes, and a sys-
tem must therefore have a quite sophisticated ab-
stract model to generate the basis for consistent
sets of corrections to be proposed.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999988777777778">
We have described our system that participated in
the shared task on grammatical error correction.
The system builds on the elements of the Illinois
system that participated in last year’s shared task.
We extended and improved the Illinois system in
three key dimensions, which we presented and
evaluated in this paper. We have also presented
error analysis of the system output and discussed
possible directions for future work.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997611875">
This material is based on research sponsored by DARPA un-
der agreement number FA8750-13-2-0008. The U.S. Gov-
ernment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright nota-
tion thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as necessar-
ily representing the official policies or endorsements, either
expressed or implied, of DARPA or the U.S. Government.
This research is also supported by a grant from the U.S. De-
partment of Education and by the DARPA Machine Reading
Program under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-018. The first and last authors
were partially funded by grant NPRP-4-1058-1-168 from the
Qatar National Research Fund (a member of the Qatar Foun-
dation). The statements made herein are solely the responsi-
bility of the authors.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999890952380952">
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation
for grammatical error correction. In NAACL, pages
568–572, Montr´eal, Canada, June. Association for
Computational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLPfor Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proc. 13th
International Conference on Machine Learning.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning.
H.T. Ng, S.M. Wu, Y. Wu, C. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning: Shared Task, pages 1–12,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.987556">
41
</page>
<reference confidence="0.999073285714286">
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of the Eighteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
Baltimore, Maryland, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya and D. Roth. 2013. Joint learning
and inference for grammatical error correction. In
EMNLP, 10.
A. Rozovskaya, M. Sammons, and D. Roth. 2012.
The UI system in the HOO 2012 shared task on er-
ror correction. In Proc. of the Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL) Workshop
on Innovative Use of NLP for Building Educational
Applications.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya, D. Roth, and V. Srikumar. 2014. Cor-
recting grammatical verb errors. In EACL.
</reference>
<page confidence="0.999293">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573250">
<title confidence="0.99759">The Illinois-Columbia System in the CoNLL-2014 Shared Task</title>
<author confidence="0.997189">Kai-Wei Mark Dan Nizar</author>
<affiliation confidence="0.821491">for Computational Learning Systems, Columbia</affiliation>
<address confidence="0.593863">2Cognitive Computation Group, University of Illinois at</address>
<abstract confidence="0.999128052631579">The CoNLL-2014 shared task is an extension of last year’s shared task and focuses on correcting grammatical errors in essays written by non-native learners of English. In this paper, we describe the Illinois-Columbia system that participated in the shared task. Our system ranked second on the original annotations and first on the revised annotations. The core of the system is based on the University of Illinois model that placed first in the CoNLL-2013 shared task. This baseline model has been improved and expanded for this year’s competition in several respects. We describe our underlying approach, which relates to our previous work, and describe the novel aspects of the system in more detail.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A Franz</author>
</authors>
<title>Web 1T 5-gram Version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="11287" citStr="Brants and Franz, 2006" startWordPosition="1817" endWordPosition="1820">he model to abstain from flagging mistakes, resulting in low recall. To avoid this problem, we adopt the approach proposed in Rozovskaya et al. (2012), the error inflation method, and add artificial article errors to the training data based on the error distribution on the training set. This method prevents the source feature from dominating the context features, and improves the recall of the system. The other classifiers in the baseline system – noun number, verb agreement, verb form, and preposition – are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word ngram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). The modules targeting verb agreement and 2http://cogcomp.cs.illinois.edu/page/ software view/POS 3http://cogcomp.cs.illinois.edu/page/ software view/Chunker 36 verb form mistakes draw on the linguisticallymotivated approach to correcting verb errors proposed in Rozovskaya et. al (2014). 4 The CoNLL-2014 System The system in the CoNL</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>T. Brants and A. Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Carlson</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>Scaling up context sensitive text correction.</title>
<date>2001</date>
<booktitle>In IAAI.</booktitle>
<contexts>
<context position="8281" citStr="Carlson et al., 2001" startWordPosition="1324" endWordPosition="1327">orm. 3 The Baseline System In this section, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI) that achieved the best result in the CoNLL-2013 shared task and which we use as our baseline model. For a complete description, we refer the reader to Rozovskaya et al. (2013). The Illinois system implements five independently-trained machine-learning classifiers that follow the popular approach to ESL error correction borrowed from the contextsensitive spelling correction task (Golding and Roth, 1999; Carlson et al., 2001). A confusion set is defined as a list of confusable words. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task and a classifier is trained on native or learner data. At prediction time, the model selects the most likely candidate from the confusion set. The confusion set for prepositions includes the top 12 most frequent English prepositions (this year, we extend the confusion set and also target extraneous preposition usage). The article confusion set is a</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up context sensitive text correction. In IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In NAACL,</booktitle>
<pages>568--572</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="21629" citStr="Dahlmeier and Ng, 2012" startWordPosition="3458" endWordPosition="3461">om the Google corpus and AP models trained on the learner data (Section 4.3). 5 Experimental Results In Sections 3 and 4, we described the individual system components that address different types of errors. In this section, we show how the system improves when each component is added into the system. In this year’s competition, systems are compared using F0.5 measure instead of F1. This is because in error correction good precision is more important than having a high recall, and the F0.5 reflects that by weighing precision twice as much as recall. System output is scored with the M2 scorer (Dahlmeier and Ng, 2012). Table 4 reports performance results of each individual classifier. In the final system, the article, preposition, noun number, and verb agreeModel P R F0.5 Articles (AP) 38.97 8.85 23.19 Articles (NB-priors) 47.34 6.01 19.93 Articles (Comb.) 38.73 10.93 25.67 Prep. (AP) 34.00 0.5 2.35 Prep. (NB-priors) 33.33 0.79 3.61 Prep. (Comb.) 30.06 1.17 5.13 Noun number (NB) 44.74 5.48 18.39 Noun number (AP) 82.35 0.41 2.01 Noun number (Comb.) 45.02 5.57 18.63 Verb agr. (AP) 38.56 1.23 5.46 Verb agr. (NB) 63.41 0.76 3.64 Verb agr. (Comb.) 41.09 1.55 6.75 Verb form (NB-priors) 59.26 1.41 6.42 Word form </context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>D. Dahlmeier and H.T. Ng. 2012. Better evaluation for grammatical error correction. In NAACL, pages 568–572, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
<author>S M Wu</author>
</authors>
<title>Building a large annotated corpus of learner english: The nus corpus of learner english.</title>
<date>2013</date>
<booktitle>In Proc. of the NAACL HLT 2013 Eighth Workshop on Innovative Use of NLPfor Building Educational Applications,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="6269" citStr="Dahlmeier et al., 2013" startWordPosition="1002" endWordPosition="1005">he error codes used in the shared task are shown. Note that only the errors exemplifying the relevant phenomena are marked in the table; the sentences may contain other mistakes. Errors marked as verb form include multiple grammatical phenomena that may characterize verbs. Our system addresses all of the error types except “Wrong Collocation” and “Local Redundancy”. 2 Task Description Both the training and the test data of the CoNLL2014 shared task consist of essays written by students at the National University of Singapore. The training data contains 1.2 million words from the NUCLE corpus (Dahlmeier et al., 2013) corrected by English teachers, and an additional set of about 30,000 words that was released last year as a test set for the CoNLL-2013 shared task. We use last year’s test data as a development set; the results in the subsequent sections are reported on this subset. The CoNLL corpus error tagset includes 28 error categories. Table 1 illustrates the most common error categories in the training data; errors are marked with an asterisk, and ∅ denotes a missing word. Our system targets all of these, with the exception of collocation and local redundancy errors. Among the less commonly occurring </context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building a large annotated corpus of learner english: The nus corpus of learner english. In Proc. of the NAACL HLT 2013 Eighth Workshop on Innovative Use of NLPfor Building Educational Applications, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>A Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="1630" citStr="Dale and Kilgarriff, 2011" startWordPosition="252" endWordPosition="255">e novel aspects of the system in more detail. 1 Introduction The topic of text correction has seen a lot of interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners. ESL error correction is an important problem since most writers of English are not native English speakers. The increased interest in this topic can be seen not only from the number of papers published on the topic but also from the three competitions devoted to grammatical error correction for non-native writers that have recently taken place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). In all three shared tasks, the participating systems performed at a level that is considered extremely low compared to performance obtained in other areas of NLP: even the best systems attained F1 scores in the range of 20-30 points. The key reason that text correction is a difficult task is that even for non-native English speakers, writing accuracy is very high, as errors are very sparse. Even for some of the most common types of errors, such as article and preposition usage, the majority of the words in these </context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>R. Dale and A. Kilgarriff. 2011. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>I Anisimoff</author>
<author>G Narroway</author>
</authors>
<title>A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proc. of the NAACL HLT 2012 Seventh Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="1660" citStr="Dale et al., 2012" startWordPosition="257" endWordPosition="260">detail. 1 Introduction The topic of text correction has seen a lot of interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners. ESL error correction is an important problem since most writers of English are not native English speakers. The increased interest in this topic can be seen not only from the number of papers published on the topic but also from the three competitions devoted to grammatical error correction for non-native writers that have recently taken place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). In all three shared tasks, the participating systems performed at a level that is considered extremely low compared to performance obtained in other areas of NLP: even the best systems attained F1 scores in the range of 20-30 points. The key reason that text correction is a difficult task is that even for non-native English speakers, writing accuracy is very high, as errors are very sparse. Even for some of the most common types of errors, such as article and preposition usage, the majority of the words in these categories (over 90%) are used</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>R. Dale, I. Anisimoff, and G. Narroway. 2012. A report on the preposition and determiner error correction shared task. In Proc. of the NAACL HLT 2012 Seventh Workshop on Innovative Use of NLP for Building Educational Applications, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Finding contradictions in text.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<marker>de Marneffe, Rafferty, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning. 2008. Finding contradictions in text. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Proc. 13th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="10307" citStr="Freund and Schapire, 1996" startWordPosition="1652" endWordPosition="1655">iers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011). Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)). The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles). The original word choice (the source article) used by the writer is also used as a feature. Since the errors are sparse, this feature causes the model to abstain from flagging mistakes, resulting in low recall. To avoid this problem, we adopt the approach proposed in Rozovskaya et al. (2012), the error inflation method, and add artificial article errors to the training data based on</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting algorithm. In Proc. 13th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow based approach to context-sensitive spelling correction.</title>
<date>1999</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="8258" citStr="Golding and Roth, 1999" startWordPosition="1320" endWordPosition="1323">rb agreement, and verb form. 3 The Baseline System In this section, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI) that achieved the best result in the CoNLL-2013 shared task and which we use as our baseline model. For a complete description, we refer the reader to Rozovskaya et al. (2013). The Illinois system implements five independently-trained machine-learning classifiers that follow the popular approach to ESL error correction borrowed from the contextsensitive spelling correction task (Golding and Roth, 1999; Carlson et al., 2001). A confusion set is defined as a list of confusable words. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task and a classifier is trained on native or learner data. At prediction time, the model selects the most likely candidate from the confusion set. The confusion set for prepositions includes the top 12 most frequent English prepositions (this year, we extend the confusion set and also target extraneous preposition usage). The art</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow based approach to context-sensitive spelling correction. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>S M Wu</author>
<author>Y Wu</author>
<author>C Hadiwinoto</author>
<author>J Tetreault</author>
</authors>
<title>The conll-2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1710" citStr="Ng et al., 2013" startWordPosition="266" endWordPosition="269">has seen a lot of interest in the past several years, with a focus on correcting grammatical errors made by English as a Second Language (ESL) learners. ESL error correction is an important problem since most writers of English are not native English speakers. The increased interest in this topic can be seen not only from the number of papers published on the topic but also from the three competitions devoted to grammatical error correction for non-native writers that have recently taken place: HOO-2011 (Dale and Kilgarriff, 2011), HOO-2012 (Dale et al., 2012), and the CoNLL-2013 shared task (Ng et al., 2013). In all three shared tasks, the participating systems performed at a level that is considered extremely low compared to performance obtained in other areas of NLP: even the best systems attained F1 scores in the range of 20-30 points. The key reason that text correction is a difficult task is that even for non-native English speakers, writing accuracy is very high, as errors are very sparse. Even for some of the most common types of errors, such as article and preposition usage, the majority of the words in these categories (over 90%) are used correctly. For instance, in the CoNLL training da</context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>H.T. Ng, S.M. Wu, Y. Wu, C. Hadiwinoto, and J. Tetreault. 2013. The conll-2013 shared task on grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>S M Wu</author>
<author>T Briscoe</author>
<author>C Hadiwinoto</author>
<author>R H Susanto</author>
<author>C Bryant</author>
</authors>
<title>The CoNLL-2014 shared task on grammatical error correction.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="2544" citStr="Ng et al., 2014" startWordPosition="414" endWordPosition="417"> of 20-30 points. The key reason that text correction is a difficult task is that even for non-native English speakers, writing accuracy is very high, as errors are very sparse. Even for some of the most common types of errors, such as article and preposition usage, the majority of the words in these categories (over 90%) are used correctly. For instance, in the CoNLL training data, only 2% of prepositions are incorrectly used. Because errors are so sparse, it is more difficult for a system to identify a mistake accurately and without introducing many false alarms. The CoNLL-2014 shared task (Ng et al., 2014) is an extension of the CoNLL-2013 shared task (Ng et al., 2013). Both competitions make use of essays written by ESL learners at the National University of Singapore. However, while the first one focused on five kinds of mistakes that are commonly made by ESL writers – article, preposition, noun number, verb agreement, and verb form – this year’s competition covers all errors occurring in the data. Errors outside the target group were present in the task corpora last year as well, but were not evaluated. Our system extends the one developed by the University of Illinois (Rozovskaya et al., 20</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H. Susanto, and C. Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="9398" citStr="Punyakanok and Roth, 2001" startWordPosition="1507" endWordPosition="1510">ar, we extend the confusion set and also target extraneous preposition usage). The article confusion set is as follows: {a, the, 0}.1 The confusion sets for noun, agreement, and form modules depend on the target word and include its morphological variants. Table 2 shows sample confusion sets for noun, agreement, and form errors. Each classifier takes as input the corpus documents preprocessed with a part-of-speech tag10 denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. ger2 and shallow parser3 (Punyakanok and Roth, 2001). The other system components use the preprocessing tools only as part of candidate generation (e.g., to identify all nouns in the data for the noun classifier). The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011). Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminat</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Algorithm selection and model adaptation for esl correction tasks.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="9781" citStr="Rozovskaya and Roth, 2011" startWordPosition="1565" endWordPosition="1568">preprocessed with a part-of-speech tag10 denotes noun-phrase-initial contexts where an article is likely to have been omitted. The variants “a” and “an” are conflated and are restored later. ger2 and shallow parser3 (Punyakanok and Roth, 2001). The other system components use the preprocessing tools only as part of candidate generation (e.g., to identify all nouns in the data for the noun classifier). The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011). Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)). The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features</context>
<context position="11551" citStr="Rozovskaya and Roth, 2011" startWordPosition="1859" endWordPosition="1862">ution on the training set. This method prevents the source feature from dominating the context features, and improves the recall of the system. The other classifiers in the baseline system – noun number, verb agreement, verb form, and preposition – are trained on native English data, the Google Web 1T 5-gram corpus (henceforth, Google, (Brants and Franz, 2006)) with the Naive Bayes (NB) algorithm. All models use word ngram features derived from the 4-word window around the target word. In the preposition model, priors for preposition preferences are learned from the shared task training data (Rozovskaya and Roth, 2011). The modules targeting verb agreement and 2http://cogcomp.cs.illinois.edu/page/ software view/POS 3http://cogcomp.cs.illinois.edu/page/ software view/Chunker 36 verb form mistakes draw on the linguisticallymotivated approach to correcting verb errors proposed in Rozovskaya et. al (2014). 4 The CoNLL-2014 System The system in the CoNLL-2014 shared task is improved in three ways: 1) Additional error-specific classifiers: word form, orthography/punctuation, and style; 2) Model combination; and 3) Joint inference to address interacting errors. Table 3 summarizes the Illinois and the Illinois-Colu</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>A. Rozovskaya and D. Roth. 2011. Algorithm selection and model adaptation for esl correction tasks. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Joint learning and inference for grammatical error correction.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>10</pages>
<contexts>
<context position="3519" citStr="Rozovskaya and Roth (2013)" startWordPosition="574" endWordPosition="577"> competition covers all errors occurring in the data. Errors outside the target group were present in the task corpora last year as well, but were not evaluated. Our system extends the one developed by the University of Illinois (Rozovskaya et al., 2013) that placed first in the CoNLL-2013 competition. For this year’s shared task, the system has been extended and improved in several respects: we extended the set of errors addressed by the system, developed a general approach for improving the error-specific models, and added a joint inference component to address interaction among errors. See Rozovskaya and Roth (2013) for more detail. We briefly discuss the task (Section 2) and give an overview of the baseline Illinois system (Section 3). Section 4 presents the novel aspects of the system. In Section 5, we evaluate the complete system on the development data and show the results obtained on test. We offer error analysis and a brief discussion in Section 6. Section 7 concludes. 34 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34–42, Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics Error type Rel. freq. Examples A</context>
<context position="20599" citStr="Rozovskaya and Roth, 2013" startWordPosition="3288" endWordPosition="3291">classifiers so that they complement one another. In particular, while each of the learner and native components have similar precision, since the predictions made differ, the recall of the combined model improves. 4.4 Joint Inference One of the mistakes typical for Illinois system were inconsistent predictions. Inconsistent predictions occur when the classifiers address grammatical phenomena that interact at the sentence level, e.g. noun number and verb agreement. To address this problem, the Illinois-Columbia system makes use of global inference via an Integer Linear Programming formulation (Rozovskaya and Roth, 2013). Note that Rozovskaya and Roth (2013) also describe a joint learning model that performs better than the joint inference approach. However, the joint learning model is based on training a joint model on the Google corpus, and is not as strong as the individually-trained classifiers of the Illinois-Columbia system that combine predictions from two components – NB classifiers trained on the native data from the Google corpus and AP models trained on the learner data (Section 4.3). 5 Experimental Results In Sections 3 and 4, we described the individual system components that address different ty</context>
</contexts>
<marker>Rozovskaya, Roth, 2013</marker>
<rawString>A. Rozovskaya and D. Roth. 2013. Joint learning and inference for grammatical error correction. In EMNLP, 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<title>The UI system in the HOO 2012 shared task on error correction.</title>
<date>2012</date>
<booktitle>In Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="10216" citStr="Rozovskaya et al. (2012)" startWordPosition="1637" endWordPosition="1640"> for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011). Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011)). The article classifier is a discriminative model that draws on the state-of-the-art approach described in Rozovskaya et al. (2012). The model makes use of the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features. The article module uses the POS and chunker output to generate some of its features and candidates (likely contexts for missing articles). The original word choice (the source article) used by the writer is also used as a feature. Since the errors are sparse, this feature causes the model to abstain from flagging mistakes, resulting in low recall. To avoid this problem, we adopt the approach proposed in Rozovskaya et al. (2012), </context>
</contexts>
<marker>Rozovskaya, Sammons, Roth, 2012</marker>
<rawString>A. Rozovskaya, M. Sammons, and D. Roth. 2012. The UI system in the HOO 2012 shared task on error correction. In Proc. of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>K-W Chang</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<date>2013</date>
<booktitle>The University of Illinois system in the CoNLL-2013 shared task. In CoNLL Shared Task.</booktitle>
<contexts>
<context position="3147" citStr="Rozovskaya et al., 2013" startWordPosition="516" endWordPosition="519">ask (Ng et al., 2014) is an extension of the CoNLL-2013 shared task (Ng et al., 2013). Both competitions make use of essays written by ESL learners at the National University of Singapore. However, while the first one focused on five kinds of mistakes that are commonly made by ESL writers – article, preposition, noun number, verb agreement, and verb form – this year’s competition covers all errors occurring in the data. Errors outside the target group were present in the task corpora last year as well, but were not evaluated. Our system extends the one developed by the University of Illinois (Rozovskaya et al., 2013) that placed first in the CoNLL-2013 competition. For this year’s shared task, the system has been extended and improved in several respects: we extended the set of errors addressed by the system, developed a general approach for improving the error-specific models, and added a joint inference component to address interaction among errors. See Rozovskaya and Roth (2013) for more detail. We briefly discuss the task (Section 2) and give an overview of the baseline Illinois system (Section 3). Section 4 presents the novel aspects of the system. In Section 5, we evaluate the complete system on the</context>
<context position="8029" citStr="Rozovskaya et al. (2013)" startWordPosition="1290" endWordPosition="1293"> problems in nuclear technology.” Error type Confusion set Noun number {factor, factors} Verb Agreement {contribute, contributes} Verb Form {included, including, includes, include} Table 2: Sample confusion sets for noun number, verb agreement, and verb form. 3 The Baseline System In this section, we briefly describe the University of Illinois system (henceforth Illinois; in the overview paper of the shared task the system is referred to as UI) that achieved the best result in the CoNLL-2013 shared task and which we use as our baseline model. For a complete description, we refer the reader to Rozovskaya et al. (2013). The Illinois system implements five independently-trained machine-learning classifiers that follow the popular approach to ESL error correction borrowed from the contextsensitive spelling correction task (Golding and Roth, 1999; Carlson et al., 2001). A confusion set is defined as a list of confusable words. Each occurrence of a confusable word in text is represented as a vector of features derived from a context window around the target. The problem is cast as a multi-class classification task and a classifier is trained on native or learner data. At prediction time, the model selects the m</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>A. Rozovskaya, K.-W. Chang, M. Sammons, and D. Roth. 2013. The University of Illinois system in the CoNLL-2013 shared task. In CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
<author>V Srikumar</author>
</authors>
<title>Correcting grammatical verb errors.</title>
<date>2014</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="13957" citStr="Rozovskaya et al. (2014)" startWordPosition="2240" endWordPosition="2244"> that about 45% of corrections for word form errors in the development data are covered by the confusion sets from the training data for the same word. We thus derive the confusion sets using the training data. Specifically, for every source word that is tagged as a word form error in the training data, the confusion set includes all labels to which that word is mapped in the training data. In addition, plural and singular forms are added for all words tagged as nouns, and inflectional forms are added for words tagged as verbs. For more detail on correcting verb errors, we refer the reader to Rozovskaya et al. (2014). 4.1.2 Orthography and Punctuation Errors The Mec error category includes errors in spelling, context-sensitive spelling, capitalization, and punctuation. Our system addresses punctuation errors and capitalization errors. To correct capitalization errors, we collected words that are always capitalized in the training and development data when not occurring sentence-initially. The punctuation classifier includes two modules: a learned component targets missing and extraneous comma usage and is an AP classifier trained on the learner data with error inflation. A second, pattern-based component,</context>
<context position="19574" citStr="Rozovskaya et al. (2014)" startWordPosition="3121" endWordPosition="3124">e each addressed via a model that combines error predictions made by a classifier trained on the learner data with the AP algorithm and those made by the NB model trained on the Google corpus. The AP classifiers all make use of richer sets of features than the native-trained classifiers: the article, noun number, and preposition classifiers employ features that use POS information, while the verb agreement classifier also makes use of dependency features extracted using a parser (de Marneffe et al., 2008). For more detail on the features used in the agreement module, we refer the reader to 38 Rozovskaya et al. (2014). Finally, all of the AP models use the source word of the author as a feature and, similar to the article AP classifier (Section 3), implement the error inflation method. The combined model generates a union of corrections produced by the components. We found that for every error type, the combined model is superior to each of the single classifiers, as it combines the advantages of both of the classifiers so that they complement one another. In particular, while each of the learner and native components have similar precision, since the predictions made differ, the recall of the combined mod</context>
</contexts>
<marker>Rozovskaya, Roth, Srikumar, 2014</marker>
<rawString>A. Rozovskaya, D. Roth, and V. Srikumar. 2014. Correcting grammatical verb errors. In EACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
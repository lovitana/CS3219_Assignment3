<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011546">
<title confidence="0.992391">
Using Comparable Corpora to Adapt MT Models to New Domains
</title>
<author confidence="0.985923">
Ann Irvine Chris Callison-Burch
</author>
<affiliation confidence="0.9892805">
Center for Language and Speech Processing Computer and Information Science Dept.
Johns Hopkins University University of Pennsylvania
</affiliation>
<sectionHeader confidence="0.978721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999666652173913">
In previous work we showed that when us-
ing an SMT model trained on old-domain
data to translate text in a new-domain,
most errors are due to unseen source
words, unseen target translations, and in-
accurate translation model scores (Irvine
et al., 2013a). In this work, we target er-
rors due to inaccurate translation model
scores using new-domain comparable cor-
pora, which we mine from Wikipedia. We
assume that we have access to a large old-
domain parallel training corpus but only
enough new-domain parallel data to tune
model parameters and do evaluation. We
use the new-domain comparable corpora
to estimate additional feature scores over
the phrase pairs in our baseline models.
Augmenting models with the new features
improves the quality of machine transla-
tions in the medical and science domains
by up to 1.3 BLEU points over very strong
baselines trained on the 150 million word
Canadian Hansard dataset.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970170212766">
Domain adaptation for machine translation is
known to be a challenging research problem that
has substantial real-world application. In this set-
ting, we have access to training data in some old-
domain of text but very little or no training data
in the domain of the text that we wish to translate.
For example, we may have a large corpus of par-
allel newswire training data but no training data in
the medical domain, resulting in low quality trans-
lations at test time due to the mismatch.
In Irvine et al. (2013a), we introduced a tax-
onomy for classifying machine translation errors
related to lexical choice. Our ‘S4’ taxonomy in-
cludes seen, sense, score, and search errors. Seen
errors result when a source language word or
phrase in the test set was not observed at all during
training. Sense errors occur when the source lan-
guage word or phrase was observed during train-
ing but not with the correct target language trans-
lation. If the source language word or phrase was
observed with its correct translation during train-
ing, but an incorrect alternative outweighs the cor-
rect translation, then a score error has occurred.
Search errors are due to pruning in beam search
decoding. We measured the impact of each error
type in a domain adaptation setting and concluded
that seen and sense errors are the most frequent but
that there is also room for improving errors due to
inaccurate translation model scores (Irvine et al.,
2013a). In this work, we target score errors, using
comparable corpora to reduce their frequency in a
domain adaptation setting.
We assume the setting where we have an old-
domain parallel training corpus but no new domain
training corpus.1 We do, however, have access
to a mixed-domain comparable corpus. We iden-
tify new-domain text within our comparable cor-
pus and use that data to estimate new translation
features on the translation models extracted from
old-domain training data. Specifically, we focus
on the French-English language pair because care-
fully curated datasets exist in several domains for
tuning and evaluation. Following our prior work,
we use the Canadian Hansard parliamentary pro-
ceedings as our old-domain and adapt models to
both the medical and the science domains (Irvine
et al., 2013a). At over 8 million sentence pairs,
</bodyText>
<footnote confidence="0.98579">
1Some prior work has referred to old-domain and new-
domain corpora as out-of-domain and in-domain, respec-
tively.
</footnote>
<page confidence="0.936176">
437
</page>
<note confidence="0.719971">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 437–444,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9990594375">
the Canadian Hansard dataset is one of the largest
publicly available parallel corpora and provides a
very strong baseline. We give details about each
dataset in Section 4.1.
We use comparable corpora to estimate sev-
eral signals of translation equivalence. In partic-
ular, we estimate the contextual, topic, and or-
thographic similarity of each phrase pair in our
baseline old-domain translation model. In Sec-
tion 3, we describe each feature in detail. Us-
ing just 5 thousand comparable new-domain doc-
ument pairs, which we mine from Wikipedia, and
five new phrase table features, we observe perfor-
mance gains of up to 1.3 BLEU points on the sci-
ence and medical translation tasks over very strong
baselines.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961310344828">
Recent work on machine translation domain adap-
tation has focused on either the language model-
ing component or the translation modeling com-
ponent of an SMT model. Language modeling re-
search has explored methods for subselecting new-
domain data from a large monolingual target lan-
guage corpus for use as language model training
data (Lin et al., 1997; Klakow, 2000; Gao et al.,
2002; Moore and Lewis, 2010; Mansour et al.,
2011). Translation modeling research has typi-
cally assumed that either (1) two parallel datasets
are available, one in the old domain and one in the
new, or (2) a large, mixed-domain parallel training
corpus is available. In the first setting, the goal is
to effectively make use of both the old-domain and
the new-domain parallel training corpora (Civera
and Juan, 2007; Koehn and Schroeder, 2007; Fos-
ter and Kuhn, 2007; Foster et al., 2010; Haddow
and Koehn, 2012; Haddow, 2013). In the sec-
ond setting, it has been shown that, in some cases,
training a translation model on a subset of new-
domain parallel training data within a larger train-
ing corpus can be more effective than using the
complete dataset (Mansour et al., 2011; Axelrod
et al., 2011; Sennrich, 2012; Gasc´o et al., 2012).
For many language pairs and domains, no new-
domain parallel training data is available. Wu et
al. (2008) machine translate new-domain source
language monolingual corpora and use the syn-
thetic parallel corpus as additional training data.
Daum´e and Jagarlamudi (2011), Zhang and Zong
(2013), and Irvine et al. (2013b) use new-domain
comparable corpora to mine translations for un-
seen words. That work follows a long line of re-
search on bilingual lexicon induction (e.g. Rapp
(1995), Schafer and Yarowsky (2002), Koehn and
Knight (2002), Haghighi et al. (2008), Irvine and
Callison-Burch (2013), Razmara et al. (2013)).
These efforts improve S4 seen, and, in some in-
stances, sense error types. To our knowledge,
no prior work has focused on fixing errors due
to inaccurate translation model scores in the set-
ting where no new-domain parallel training data is
available.
In Klementiev et al. (2012), we used compara-
ble corpora to estimate several features for a given
phrase pair that indicate translation equivalence,
including contextual, temporal, and topical simi-
larity. The definitions of phrasal and lexical con-
textual and topic similarity that we use here are
taken from our prior work, where we replaced
bilingually estimated phrase table features with
the new features and cited applications to low re-
source SMT. In this work we also focus on scoring
a phrase table using comparable corpora. How-
ever, here we work in a domain adaptation setting
and seek to augment, not replace, an existing set
of bilingually estimated phrase table features.
</bodyText>
<sectionHeader confidence="0.974629" genericHeader="method">
3 Phrase Table Scoring
</sectionHeader>
<bodyText confidence="0.999597047619048">
We begin with a scored phrase table estimated us-
ing our old-domain parallel training corpus. The
phrase table contains about 201 million unique
source phrases up to length seven and about 479
million total phrase pairs. We use Wikipedia as a
source for comparable document pairs (details are
given in Section 4.1). We augment the bilingually
estimated features with the following: (1) lexical
and phrasal contextual similarity estimated over a
comparable corpus, (2) lexical and phrasal topi-
cal similarity estimated over a comparable corpus,
and (3) lexical orthographic similarity.
Contextual Similarity We estimate contextual
similarity2 by first computing a context vector for
each source and target word and phrase in our
phrase table using the source and target sides of
our comparable corpus, respectively. We begin by
collecting vectors of counts of words that appear
in the context of each source and target phrase, ps
and pt. We use a bag-of-words context consist-
ing of the two words to the left and two words to
</bodyText>
<footnote confidence="0.9932735">
2Similar to distributional similarity, which is typically de-
fined monolingually.
</footnote>
<page confidence="0.99748">
438
</page>
<bodyText confidence="0.999813714285714">
the right of each occurrence of each phrase. Vari-
ous means of computing the component values of
context vectors from raw context frequency counts
have been proposed (e.g. Rapp (1999), Fung and
Yee (1998)). Following Fung and Yee (1998), we
compute the value of the k-th component of ps’s
contextual vector, Cps, as follows:
</bodyText>
<equation confidence="0.562267">
Cpsk “ nps,k ˚ plogpn{nkq ` 1q
</equation>
<bodyText confidence="0.99992975">
where nps,k and nk are the number of times the
k-th source word, sk, appears in the context of ps
and in the entire corpus, and n is the maximum
number of occurrences of any word in the data.
Intuitively, the more frequently sk appears with ps
and the less common it is in the corpus in general,
the higher its component value. The context vector
for ps, Cps, is M-dimensional, where M is the
size of the source language vocabulary. Similarly,
we compute N-dimensional context vectors for all
target language words and phrases, where N is the
size of the target language vocabulary.
We identify the most probable translation t for
each of the M source language words, s, as the
target word with the highest ppt|sq under our word
aligned old-domain training corpus. Given this
dictionary of unigram translations, we then project
each M-dimensional source language context vec-
tor into the N-dimensional target language context
vector space. To compare a given pair of source
and target context vectors, Cps and Cpt, respec-
tively, we compute their cosine similarity, or their
dot product divided by the product of their magni-
tudes:
</bodyText>
<subsectionHeader confidence="0.648357">
Cps ¨ Cpt
</subsectionHeader>
<bodyText confidence="0.994533758620689">
simcontextualpps, ptq “ ||Cps |Cpt||
For a given phrase pair in our phrase table, we
estimate phrasal contextual similarity by directly
comparing the context vectors of the two phrases
themselves. Because context vectors for phrases,
which tend to be less frequent than words, can be
sparse, we also compute lexical contextual simi-
larity over phrase pairs. We define lexical con-
textual similarity as the average of the contextual
similarity between all word pairs within the phrase
pair.
Topic Similarity Phrases and their translations
are likely to appear in articles written about the
same topic in two languages. We estimate topic
similarity using the distribution of words and
phrases across Wikipedia pages, for which we
have interlingual French-English links. Specif-
ically, we compute topical vectors by counting
the number of occurrences of each word and
phrase across Wikipedia pages. That is, for each
source and target phrase, ps and pt, we collect M-
dimensional topic vectors, where M is the number
of Wikipedia page pairs used (in our experiments,
M is typically 5, 000). We use Wikipedia’s inter-
lingual links to align the French and English topic
vectors and normalize each topic vector by the to-
tal count. As with contextual similarity, we com-
pare a pair of source and target topic vectors, Tps
and Tpt, respectively, using cosine similarity:
</bodyText>
<subsectionHeader confidence="0.523347">
Tps ¨ Tpt
</subsectionHeader>
<bodyText confidence="0.964977459459459">
||Tps |Tpt||
We estimate both phrasal and lexical topic simi-
larity for each phrase pair. As before, lexical topic
similarity is estimated by taking an average topic
similarity across all word pairs in a given phrase
pair.
Orthographic Similarity We make use of one
additional signal of translation equivalence: ortho-
graphic similarity. In this case, we do not refer-
ence comparable corpora but simply compute the
edit distance between a given pair of phrases. This
signal is often useful for identifying translations
of technical terms, which appear frequently in our
medical and science domain corpora. However,
because of word order variation, we do not mea-
sure edit distance on phrase pairs directly. For ex-
ample, French embryon humain translates as En-
glish human embryo; embryon translates as em-
bryo and humain translates as human. Although
both word pairs are cognates, the words appear
in opposite orders in the two phrases. Therefore,
directly measuring string edit distance across the
phrase pair would not effectively capture the relat-
edness of the words. Hence, we only measure lex-
ical orthographic similarity, not phrasal. We com-
pute lexical orthographic similarity by first com-
puting the edit distance between each word pair,
ws and wt, within a given phrase pair, normalized
by the lengths of the two words:
edpws, wtq
simorthpws, wtq “ |ws||wt|
2
We then compute the average normalized edit dis-
tance across all word pairs.
The above similarity metrics all allow for scores
of zero, which can be problematic for our log-
simtopicpps, ptq “
</bodyText>
<page confidence="0.972116">
439
</page>
<table confidence="0.999844333333333">
Corpus Source Words Target Words
Training 144.5 m
Canadian Hansard 161.7 m
Tune-1 / Tune-2 /Test 46k / 38k / 30k
Medical 53k / 43k / 35k 75k / 101k / 101k
Science 92k / 120k / 120k
Language Modeling and Comparable Corpus Selection
Medical - 5.9 m
Science - 3.6 m
</table>
<tableCaption confidence="0.990269">
Table 1: Summary of the size of each corpus of text used
in this work in terms of the number of source and target word
tokens.
</tableCaption>
<bodyText confidence="0.969109">
linear translation models. We describe our ex-
periments with different minimum score cutoffs in
Section 4.2.
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.888035">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.9983645">
We assume that the following data is available in
our translation setting:
</bodyText>
<listItem confidence="0.983001222222222">
• Large old-domain parallel corpus for training
• Small new-domain parallel corpora for tun-
ing and testing
• Large new-domain English monolingual cor-
pus for language modeling and identifying
new-domain-like comparable corpora
• Large mixed-domain comparable corpus,
which includes some text from the new-
domain
</listItem>
<bodyText confidence="0.999915746268657">
These data conditions are typical for many real-
world uses of machine translation. A summary of
the size of each corpus is given in Table 1.
Our old-domain training data is taken from
the Canadian Hansard parliamentary proceedings
dataset, which consists of manual transcriptions
and translations of meetings of the Canadian par-
liament. The dataset is substantially larger than
the commonly used Europarl corpus, containing
over 8 million sentence pairs and about 150 mil-
lion word tokens of French and English.
For tuning and evaluation, we use new-domain
medical and science parallel datasets released by
Irvine et al. (2013a). The medical texts con-
sist of documents from the European Medical
Agency (EMEA), originally released by Tiede-
mann (2009). This data is primarily taken from
prescription drug label text. The science data is
made up of translated scientific abstracts from the
fields of physics, biology, and computer science.
For both the medical and science domains, we
use three held-out parallel datasets of about 40
and 100 thousand words,3 respectively, released
by Irvine et al. (2013a). We do tuning on dev],
additional parameter selection on test2, and blind
testing on test].
We use large new-domain monolingual English
corpora for language modeling and for selecting
new-domain-like comparable corpora from our
mixed domain comparable corpus. Specifically,
we use the English side of the medical and science
training datasets released by Irvine et al. (2013a).
We do not use the parallel French side of the train-
ing data at all; our data setting assumes that no
new-domain parallel data is available for training.
We use Wikipedia as a source of compara-
ble corpora. There are over half a million
pairs of inter-lingually linked French and English
Wikipedia documents.4 We assume that we have
enough monolingual new-domain data in one lan-
guage to rank Wikipedia pages according to how
new-domain-like they are. In particular, we use
our new-domain English language modeling data
to measure new-domain-likeness. We could have
targeted our learning even more by using our new-
domain French test sets to select comparable cor-
pora. Doing so may increase the similarity be-
tween our test data and comparable corpora. How-
ever, to avoid overfitting any particular test set, we
use our large English new-domain language mod-
eling corpus instead.
For each inter-lingually linked pair of French
and English Wikipedia documents, we compute
the percent of English phrases up to length four
that are observed in the English monolingual new-
domain corpus and rank document pairs by the ge-
ometric mean of the four overlap measures. More
sophisticated ways to identify new-domain-like
Wikipedia pages (e.g. (Moore and Lewis, 2010))
may yield additional performance gains, but, qual-
itatively, the ranked Wikipedia pages seem rea-
sonable for the purposes of generating a large set
of top-k new-domain document pairs. The top-10
ranked pages for each domain are listed in Table 2.
The top ranked science domain pages are primar-
ily related to concepts from the field of physics
but also include computer science and chemistry
</bodyText>
<footnote confidence="0.96826325">
3Or about 4 thousand lines each. The sentences in the
medical domain text are much shorter than those in the sci-
ence domain.
4As of January 2014.
</footnote>
<page confidence="0.989775">
440
</page>
<table confidence="0.998505">
Science Medical
Diagnosis (artificial intelligence) Pregabalin
Absorption spectroscopy Cetuximab
Spectral line Fluconazole
Chemical kinetics Calcitonin
Mahalanobis distance Pregnancy category
Dynamic light scattering Trazodone
Amorphous solid Rivaroxaban
Magnetic hyperthermia Spironolactone
Photoelasticity Anakinra
Galaxy rotation curve Cladribine
</table>
<tableCaption confidence="0.993498">
Table 2: Top 10 Wikipedia articles ranked by their similar-
ity to large new-domain English monolingual corpora.
</tableCaption>
<bodyText confidence="0.995943333333333">
topics. The top ranked medical domain pages are
nearly all prescription drugs, which makes sense
given the content of the EMEA medical corpus.
</bodyText>
<subsectionHeader confidence="0.987073">
4.2 Phrase-based Machine Translation
</subsectionHeader>
<bodyText confidence="0.999973942307693">
We word align our old-domain training corpus
using GIZA++ and use the Moses SMT toolkit
(Koehn et al., 2007) to extract a translation gram-
mar. In this work, we focus on phrase-based
SMT models, however our approach to using new-
domain comparable corpora to estimate translation
scores is theoretically applicable to any type of
translation grammar.
Our baseline models use a phrase limit of seven
and the standard phrase-based SMT feature set, in-
cluding forward and backward phrase and lexical
translation probabilities. Additionally, we use the
standard lexicalized reordering model. We exper-
iment with two 5-gram language models trained
using SRILM with Kneser-Ney smoothing on (1)
the English side of the Hansard training corpus,
and (2) the relevant new-domain monolingual En-
glish corpus. We experiment with using, first, only
the old-domain language model and, then, both the
old-domain and the new-domain language models.
Our first comparison system augments the stan-
dard feature set with the orthographic similarity
feature, which is not based on comparable cor-
pora. Our second comparison system uses both
the orthographic feature and the contextual and
topic similarity features estimated over a random
set of comparable document pairs. The third sys-
tem estimates contextual and topic similarity using
new-domain-like comparable corpora. We tune
our phrase table feature weights for each model
separately using batch MIRA (Cherry and Fos-
ter, 2012) and new-domain tuning data. Results
are averaged over three tuning runs, and we use
the implementation of approximate randomization
released by Clark et al. (2011) to measure the
statistical significance of each feature-augmented
model compared with the baseline model that uses
the same language model(s).
As noted in Section 3, the features that we
estimate from comparable corpora may be zero-
valued. We use our second tuning sets5 to tune
a minimum threshold parameter for our new fea-
tures. We measure performance in terms of BLEU
score on the second tuning set as we vary the new
feature threshold between 1e´07 and 0.5 for each
domain. A threshold of 0.01, for example, means
that we replace all feature with values less than
0.01 with 0.01. For both new-domains, perfor-
mance drops when we use thresholds lower than
0.01 and higher than 0.25. We use a minimum
threshold of 0.1 for all experiments presented be-
low for both domains.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.997621206896552">
Table 3 presents a summary of our results on the
test set in each domain. Using only the old-domain
language model, our baselines yield BLEU scores
of 22.70 and 21.29 on the medical and science test
sets, respectively. When we add the orthographic
similarity feature, BLEU scores increase signifi-
cantly, by about 0.4 on the medical data and 0.6 on
science. Adding the contextual and topic features
estimated over a random selection of comparable
document pairs improves BLEU scores slightly in
both domains. Finally, using the most new-domain
like document pairs to estimate the contextual and
topic features yields a 1.3 BLEU score improve-
ment over the baseline in both domains. For both
domains, this result is a statistically significant im-
provement6 over each of the first three systems.
In both domains, the new-domain language
models contribute substantially to translation qual-
ity. Baseline BLEU scores increase by about
6 and 5 BLEU score points in the medical and
science domains, respectively, when we add the
new-domain language models. In the medical do-
main, neither the orthographic feature nor the or-
thographic feature in combination with contextual
and topic features estimated over random docu-
ment pairs results in a significant BLEU score
improvement. However, using the orthographic
feature and the contextual and topic features es-
timated over new-domain document pairs yields a
</bodyText>
<footnote confidence="0.9994355">
5test2 datasets released by Irvine et al. (2013a)
6p-value ă 0.01
</footnote>
<page confidence="0.991198">
441
</page>
<table confidence="0.999631818181818">
Language Model(s) System Medical Science
Baseline 22.70 21.29
+ Orthographic Feature 23.09* (+0.4) 21.86* (+0.6)
Old 23.22* (+0.5) 21.88* (+0.6)
+ Orthographic &amp; Random CC Features 23.98* (+1.3) 22.55* (+1.3)
+ Orthographic &amp; New-domain CC Features
Baseline 28.82 26.18
+ Orthographic Feature 29.02 (+0.2) 26.40* (+0.2)
Old+New 28.86 (+0.0) 26.52* (+0.3)
+ Orthographic &amp; Random CC Features 29.16* (+0.3) 26.50* (+0.3)
+ Orthographic &amp; New-domain CC Features
</table>
<tableCaption confidence="0.850027857142857">
Table 3: Comparison between the performance of baseline old-domain translation models and domain-adapted models in
translating science and medical domain text. We experiment with two language models: old, trained on the English side of our
Hansard old-domain training corpus and new, trained on the English side of the parallel training data in each new domain. We
use comparable corpora of 5, 000 (1) random, and (2) the most new-domain-like document pairs to score phrase tables. All
results are averaged over three tuning runs, and we perform statistical significance testing comparing each system augmented
with additional features with the baseline system that uses the same language model(s). * indicates that the BLEU scores are
statistically significant with p ă 0.01.
</tableCaption>
<bodyText confidence="0.999963653061225">
small but significant improvement of 0.3 BLEU.
In the science domain, in contrast, all three aug-
mented models perform statistically significantly
better than the baseline. Contextual and topic fea-
tures yield only a slight improvement above the
model that uses only the orthographic feature, but
the difference is statistically significant. For the
science domain, when we use the new domain lan-
guage model, there is no difference between esti-
mating the contextual and topic features over ran-
dom comparable document pairs and those chosen
for their similarity with new-domain data.
Differences across domains may be due to the
fact that the medical domain corpora are much
more homogenous, containing the often boiler-
plate text of prescription drug labels, than the sci-
ence domain corpora. The science domain cor-
pora, in contrast, contain abstracts from several
different scientific fields; because that data is more
diverse, a randomly chosen mixed-domain set of
comparable corpora may still be relevant and use-
ful for adapting a translation model.
We experimented with varying the number of
comparable document pairs used for estimating
contextual and topic similarity but saw no sig-
nificant gains from using more than 5, 000 in ei-
ther domain. In fact, performance dropped in the
medical domain when we used more than a few
thousand document pairs. Our proposed approach
orders comparable document pairs by how new-
domain-like they are and augments models with
new features estimated over the top-k. As a result,
using more comparable document pairs means that
there is more data from which to estimate sig-
nals, but it also means that the data is less new-
domain like overall. Using a domain similarity
threshold to choose a subset of comparable doc-
ument pairs may prove useful in future work, as
the ideal amount of comparable data will depend
on the type and size of the initial mixed-domain
comparable corpus as well as the homogeneity of
the text domain of interest.
We also experimented with using a third lan-
guage model estimated over the English side of
our comparable corpora. However, we did not see
any significant improvements in translation qual-
ity when we used this language model in combina-
tion with the old and new domain language mod-
els.
</bodyText>
<sectionHeader confidence="0.996154" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999987333333334">
In this work, we targeted SMT errors due
to translation model scores using new-domain
comparable corpora. Our old-domain French-
English baseline model was trained on the Cana-
dian Hansard parliamentary proceedings dataset,
which, at 8 million sentence pairs, is one of the
largest publicly available parallel datasets. Our
task was to adapt this baseline to the medical and
scientific text domains using comparable corpora.
We used new-domain parallel data only to tune
model parameters and do evaluation. We mined
Wikipedia for new-domain-like comparable docu-
ment pairs, over which we estimated several addi-
tional features scores: contextual, temporal, and
orthographic similarity. Augmenting the strong
baseline with our new feature set improved the
quality of machine translations in the medical and
science domains by up to 1.3 BLEU points.
</bodyText>
<page confidence="0.997564">
442
</page>
<sectionHeader confidence="0.998099" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999890375">
This material is based on research sponsored by
DARPA under contract HR0011-09-1-0044 and
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence. The
views and conclusions contained in this publica-
tion are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999633040816327">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL).
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Workshop on Sta-
tistical Machine Translation (WMT).
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: controlling for
optimizer instability. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Hal Daum´e, III and Jagadeesh Jagarlamudi. 2011.
Domain adaptation for machine translation by min-
ing unseen words. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of
the Workshop on Statistical Machine Translation
(WMT).
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the Conference of the
Association for Computational Linguistics (ACL).
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to sta-
tistical language modeling for chinese. ACM Trans-
actions on Asian Language Information Processing
(TALIP).
Guillem Gasc´o, Martha-Alicia Rocha, Germ´an
Sanchis-Trilles, Jes´us Andr´es-Ferrer, and Francisco
Casacuberta. 2012. Does more data always yield
better translations? In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on SMT systems. In
Proceedings of the Workshop on Statistical Machine
Translation (WMT).
Barry Haddow. 2013. Applying pairwise ranked op-
timisation to improve the interpolation of transla-
tion models. In Proceedings of the Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Conference of the Association for Computational
Linguistics (ACL).
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Ann Irvine, John Morgan, Marine Carpuat, Hal Daum´e
III, and Dragos Munteanu. 2013a. Measuring ma-
chine translation errors in new domains. Transac-
tions of the Association for Computational Linguis-
tics, 1(October).
Ann Irvine, Chris Quirk, and Hal Daume III. 2013b.
Monolingual marginal matching for translation
model adaptation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).
Dietrich Klakow. 2000. Selecting articles from the
language model training corpus. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP).
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the Conference of the European Associ-
ation for Computational Linguistics (EACL).
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ACL Workshop on Unsupervised Lexical Acquisi-
tion.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Workshop on Statistical
Machine Translation (WMT).
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
</reference>
<page confidence="0.990565">
443
</page>
<reference confidence="0.999830677966102">
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, Ker-
Jiann Chen, and Lin-Shan Lee. 1997. Chinese lan-
guage model adaptation based on document classifi-
cation and multiple domain-specific language mod-
els. In Fifth European Conference on Speech Com-
munication and Technology.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining translation and language model
scoring for domain-specific data filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT).
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL).
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the Conference
of the Association for Computational Linguistics
(ACL).
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the Confer-
ence of the Association for Computational Linguis-
tics (ACL).
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proceedings of the Confer-
ence on Natural Language Learning (CoNLL).
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the Confer-
ence of the European Association for Computational
Linguistics (EACL).
J¨org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In N. Nicolov, K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances in
Natural Language Processing (RANLP).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the International Confer-
ence on Computational Linguistics (COLING).
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the Conference of the Association for
Computational Linguistics (ACL).
</reference>
<page confidence="0.998955">
444
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444822">
<title confidence="0.999969">Using Comparable Corpora to Adapt MT Models to New Domains</title>
<author confidence="0.992256">Ann Irvine Chris Callison-Burch</author>
<affiliation confidence="0.905325">Center for Language and Speech Processing Computer and Information Science Dept. Johns Hopkins University University of Pennsylvania</affiliation>
<abstract confidence="0.980815125">In previous work we showed that when using an SMT model trained on old-domain data to translate text in a new-domain, most errors are due to unseen source words, unseen target translations, and inaccurate translation model scores (Irvine et al., 2013a). In this work, we target errors due to inaccurate translation model scores using new-domain comparable corpora, which we mine from Wikipedia. We assume that we have access to a large olddomain parallel training corpus but only enough new-domain parallel data to tune model parameters and do evaluation. We use the new-domain comparable corpora to estimate additional feature scores over the phrase pairs in our baseline models. Augmenting models with the new features improves the quality of machine translations in the medical and science domains up to points over very strong baselines trained on the 150 million word Canadian Hansard dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5614" citStr="Axelrod et al., 2011" startWordPosition="919" endWordPosition="922">ain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), I</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="19023" citStr="Cherry and Foster, 2012" startWordPosition="3067" endWordPosition="3071">the old-domain language model and, then, both the old-domain and the new-domain language models. Our first comparison system augments the standard feature set with the orthographic similarity feature, which is not based on comparable corpora. Our second comparison system uses both the orthographic feature and the contextual and topic similarity features estimated over a random set of comparable document pairs. The third system estimates contextual and topic similarity using new-domain-like comparable corpora. We tune our phrase table feature weights for each model separately using batch MIRA (Cherry and Foster, 2012) and new-domain tuning data. Results are averaged over three tuning runs, and we use the implementation of approximate randomization released by Clark et al. (2011) to measure the statistical significance of each feature-augmented model compared with the baseline model that uses the same language model(s). As noted in Section 3, the features that we estimate from comparable corpora may be zerovalued. We use our second tuning sets5 to tune a minimum threshold parameter for our new features. We measure performance in terms of BLEU score on the second tuning set as we vary the new feature thresho</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Civera</author>
<author>Alfons Juan</author>
</authors>
<title>Domain adaptation in statistical machine translation with mixture modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT).</booktitle>
<contexts>
<context position="5234" citStr="Civera and Juan, 2007" startWordPosition="851" endWordPosition="854">e modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the</context>
</contexts>
<marker>Civera, Juan, 2007</marker>
<rawString>Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="19187" citStr="Clark et al. (2011)" startWordPosition="3093" endWordPosition="3096">rthographic similarity feature, which is not based on comparable corpora. Our second comparison system uses both the orthographic feature and the contextual and topic similarity features estimated over a random set of comparable document pairs. The third system estimates contextual and topic similarity using new-domain-like comparable corpora. We tune our phrase table feature weights for each model separately using batch MIRA (Cherry and Foster, 2012) and new-domain tuning data. Results are averaged over three tuning runs, and we use the implementation of approximate randomization released by Clark et al. (2011) to measure the statistical significance of each feature-augmented model compared with the baseline model that uses the same language model(s). As noted in Section 3, the features that we estimate from comparable corpora may be zerovalued. We use our second tuning sets5 to tune a minimum threshold parameter for our new features. We measure performance in terms of BLEU score on the second tuning set as we vary the new feature threshold between 1e´07 and 0.5 for each domain. A threshold of 0.01, for example, means that we replace all feature with values less than 0.01 with 0.01. For both new-dom</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: controlling for optimizer instability. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, Jagarlamudi, 2011</marker>
<rawString>Hal Daum´e, III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT).</booktitle>
<contexts>
<context position="5284" citStr="Foster and Kuhn, 2007" startWordPosition="859" endWordPosition="863">lecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training </context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>C Goutte</author>
<author>R Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in SMT.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5305" citStr="Foster et al., 2010" startWordPosition="864" endWordPosition="867">from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jaga</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>G. Foster, C. Goutte, and R. Kuhn. 2010. Discriminative instance weighting for domain adaptation in SMT. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="8535" citStr="Fung and Yee (1998)" startWordPosition="1387" endWordPosition="1390"> and target word and phrase in our phrase table using the source and target sides of our comparable corpus, respectively. We begin by collecting vectors of counts of words that appear in the context of each source and target phrase, ps and pt. We use a bag-of-words context consisting of the two words to the left and two words to 2Similar to distributional similarity, which is typically defined monolingually. 438 the right of each occurrence of each phrase. Various means of computing the component values of context vectors from raw context frequency counts have been proposed (e.g. Rapp (1999), Fung and Yee (1998)). Following Fung and Yee (1998), we compute the value of the k-th component of ps’s contextual vector, Cps, as follows: Cpsk “ nps,k ˚ plogpn{nkq ` 1q where nps,k and nk are the number of times the k-th source word, sk, appears in the context of ps and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with ps and the less common it is in the corpus in general, the higher its component value. The context vector for ps, Cps, is M-dimensional, where M is the size of the source language vocabulary. Similarly, we c</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Joshua Goodman</author>
<author>Mingjing Li</author>
<author>KaiFu Lee</author>
</authors>
<title>Toward a unified approach to statistical language modeling for chinese.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP).</journal>
<contexts>
<context position="4823" citStr="Gao et al., 2002" startWordPosition="783" endWordPosition="786">e new-domain document pairs, which we mine from Wikipedia, and five new phrase table features, we observe performance gains of up to 1.3 BLEU points on the science and medical translation tasks over very strong baselines. 2 Related Work Recent work on machine translation domain adaptation has focused on either the language modeling component or the translation modeling component of an SMT model. Language modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a tran</context>
</contexts>
<marker>Gao, Goodman, Li, Lee, 2002</marker>
<rawString>Jianfeng Gao, Joshua Goodman, Mingjing Li, and KaiFu Lee. 2002. Toward a unified approach to statistical language modeling for chinese. ACM Transactions on Asian Language Information Processing (TALIP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillem Gasc´o</author>
<author>Martha-Alicia Rocha</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Jes´us Andr´es-Ferrer</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Does more data always yield better translations?</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</booktitle>
<marker>Gasc´o, Rocha, Sanchis-Trilles, Andr´es-Ferrer, Casacuberta, 2012</marker>
<rawString>Guillem Gasc´o, Martha-Alicia Rocha, Germ´an Sanchis-Trilles, Jes´us Andr´es-Ferrer, and Francisco Casacuberta. 2012. Does more data always yield better translations? In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Analysing the effect of out-of-domain data on SMT systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT).</booktitle>
<contexts>
<context position="5329" citStr="Haddow and Koehn, 2012" startWordPosition="868" endWordPosition="871">ual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang an</context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on SMT systems. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
</authors>
<title>Applying pairwise ranked optimisation to improve the interpolation of translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="5344" citStr="Haddow, 2013" startWordPosition="872" endWordPosition="873">us for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), </context>
</contexts>
<marker>Haddow, 2013</marker>
<rawString>Barry Haddow. 2013. Applying pairwise ranked optimisation to improve the interpolation of translation models. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6211" citStr="Haghighi et al. (2008)" startWordPosition="1013" endWordPosition="1016">011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use her</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Supervised bilingual lexicon induction with multiple monolingual signals.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="6245" citStr="Irvine and Callison-Burch (2013)" startWordPosition="1017" endWordPosition="1020">1; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use here are taken from our prior work, w</context>
</contexts>
<marker>Irvine, Callison-Burch, 2013</marker>
<rawString>Ann Irvine and Chris Callison-Burch. 2013. Supervised bilingual lexicon induction with multiple monolingual signals. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>John Morgan</author>
<author>Marine Carpuat</author>
<author>Hal Daum´e</author>
<author>Dragos Munteanu</author>
</authors>
<title>Measuring machine translation errors in new domains. Transactions of the Association for Computational Linguistics,</title>
<date>2013</date>
<marker>Irvine, Morgan, Carpuat, Daum´e, Munteanu, 2013</marker>
<rawString>Ann Irvine, John Morgan, Marine Carpuat, Hal Daum´e III, and Dragos Munteanu. 2013a. Measuring machine translation errors in new domains. Transactions of the Association for Computational Linguistics, 1(October).</rawString>
</citation>
<citation valid="false">
<authors>
<author>2013b</author>
</authors>
<title>Monolingual marginal matching for translation model adaptation.</title>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="5942" citStr="(2013)" startWordPosition="972" endWordPosition="972">w, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. </context>
</contexts>
<marker>2013b, </marker>
<rawString>Ann Irvine, Chris Quirk, and Hal Daume III. 2013b. Monolingual marginal matching for translation model adaptation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietrich Klakow</author>
</authors>
<title>Selecting articles from the language model training corpus.</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="4805" citStr="Klakow, 2000" startWordPosition="781" endWordPosition="782">sand comparable new-domain document pairs, which we mine from Wikipedia, and five new phrase table features, we observe performance gains of up to 1.3 BLEU points on the science and medical translation tasks over very strong baselines. 2 Related Work Recent work on machine translation domain adaptation has focused on either the language modeling component or the translation modeling component of an SMT model. Language modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some case</context>
</contexts>
<marker>Klakow, 2000</marker>
<rawString>Dietrich Klakow. 2000. Selecting articles from the language model training corpus. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Klementiev</author>
<author>Ann Irvine</author>
<author>Chris Callison-Burch</author>
<author>David Yarowsky</author>
</authors>
<title>Toward statistical machine translation without parallel corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="6548" citStr="Klementiev et al. (2012)" startWordPosition="1068" endWordPosition="1071">ng and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use here are taken from our prior work, where we replaced bilingually estimated phrase table features with the new features and cited applications to low resource SMT. In this work we also focus on scoring a phrase table using comparable corpora. However, here we work in a domain adaptation setting and seek to augment, not replace, an existin</context>
</contexts>
<marker>Klementiev, Irvine, Callison-Burch, Yarowsky, 2012</marker>
<rawString>Alex Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky. 2012. Toward statistical machine translation without parallel corpora. In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Unsupervised Lexical Acquisition.</booktitle>
<contexts>
<context position="6187" citStr="Koehn and Knight (2002)" startWordPosition="1009" endWordPosition="1012">ataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic si</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In ACL Workshop on Unsupervised Lexical Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation (WMT).</booktitle>
<contexts>
<context position="5261" citStr="Koehn and Schroeder, 2007" startWordPosition="855" endWordPosition="858"> explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus </context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<location>Christine Moran,</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<marker>Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Chien Lin</author>
<author>Chi-Lung Tsai</author>
<author>Lee-Feng Chien</author>
<author>KerJiann Chen</author>
<author>Lin-Shan Lee</author>
</authors>
<title>Chinese language model adaptation based on document classification and multiple domain-specific language models. In</title>
<date>1997</date>
<booktitle>Fifth European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="4791" citStr="Lin et al., 1997" startWordPosition="777" endWordPosition="780"> Using just 5 thousand comparable new-domain document pairs, which we mine from Wikipedia, and five new phrase table features, we observe performance gains of up to 1.3 BLEU points on the science and medical translation tasks over very strong baselines. 2 Related Work Recent work on machine translation domain adaptation has focused on either the language modeling component or the translation modeling component of an SMT model. Language modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that</context>
</contexts>
<marker>Lin, Tsai, Chien, Chen, Lee, 1997</marker>
<rawString>Sung-Chien Lin, Chi-Lung Tsai, Lee-Feng Chien, KerJiann Chen, and Lin-Shan Lee. 1997. Chinese language model adaptation based on document classification and multiple domain-specific language models. In Fifth European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saab Mansour</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Combining translation and language model scoring for domain-specific data filtering.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="4869" citStr="Mansour et al., 2011" startWordPosition="791" endWordPosition="794">e from Wikipedia, and five new phrase table features, we observe performance gains of up to 1.3 BLEU points on the science and medical translation tasks over very strong baselines. 2 Related Work Recent work on machine translation domain adaptation has focused on either the language modeling component or the translation modeling component of an SMT model. Language modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain paralle</context>
</contexts>
<marker>Mansour, Wuebker, Ney, 2011</marker>
<rawString>Saab Mansour, Joern Wuebker, and Hermann Ney. 2011. Combining translation and language model scoring for domain-specific data filtering. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4846" citStr="Moore and Lewis, 2010" startWordPosition="787" endWordPosition="790">ent pairs, which we mine from Wikipedia, and five new phrase table features, we observe performance gains of up to 1.3 BLEU points on the science and medical translation tasks over very strong baselines. 2 Related Work Recent work on machine translation domain adaptation has focused on either the language modeling component or the translation modeling component of an SMT model. Language modeling research has explored methods for subselecting newdomain data from a large monolingual target language corpus for use as language model training data (Lin et al., 1997; Klakow, 2000; Gao et al., 2002; Moore and Lewis, 2010; Mansour et al., 2011). Translation modeling research has typically assumed that either (1) two parallel datasets are available, one in the old domain and one in the new, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subs</context>
<context position="16387" citStr="Moore and Lewis, 2010" startWordPosition="2674" endWordPosition="2677">est sets to select comparable corpora. Doing so may increase the similarity between our test data and comparable corpora. However, to avoid overfitting any particular test set, we use our large English new-domain language modeling corpus instead. For each inter-lingually linked pair of French and English Wikipedia documents, we compute the percent of English phrases up to length four that are observed in the English monolingual newdomain corpus and rank document pairs by the geometric mean of the four overlap measures. More sophisticated ways to identify new-domain-like Wikipedia pages (e.g. (Moore and Lewis, 2010)) may yield additional performance gains, but, qualitatively, the ranked Wikipedia pages seem reasonable for the purposes of generating a large set of top-k new-domain document pairs. The top-10 ranked pages for each domain are listed in Table 2. The top ranked science domain pages are primarily related to concepts from the field of physics but also include computer science and chemistry 3Or about 4 thousand lines each. The sentences in the medical domain text are much shorter than those in the science domain. 4As of January 2014. 440 Science Medical Diagnosis (artificial intelligence) Pregaba</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6133" citStr="Rapp (1995)" startWordPosition="1003" endWordPosition="1004">e more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The defi</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="8514" citStr="Rapp (1999)" startWordPosition="1385" endWordPosition="1386">r each source and target word and phrase in our phrase table using the source and target sides of our comparable corpus, respectively. We begin by collecting vectors of counts of words that appear in the context of each source and target phrase, ps and pt. We use a bag-of-words context consisting of the two words to the left and two words to 2Similar to distributional similarity, which is typically defined monolingually. 438 the right of each occurrence of each phrase. Various means of computing the component values of context vectors from raw context frequency counts have been proposed (e.g. Rapp (1999), Fung and Yee (1998)). Following Fung and Yee (1998), we compute the value of the k-th component of ps’s contextual vector, Cps, as follows: Cpsk “ nps,k ˚ plogpn{nkq ` 1q where nps,k and nk are the number of times the k-th source word, sk, appears in the context of ps and in the entire corpus, and n is the maximum number of occurrences of any word in the data. Intuitively, the more frequently sk appears with ps and the less common it is in the corpus in general, the higher its component value. The context vector for ps, Cps, is M-dimensional, where M is the size of the source language vocabu</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Maryam Siahbani</author>
<author>Reza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6268" citStr="Razmara et al. (2013)" startWordPosition="1021" endWordPosition="1024">2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexical contextual and topic similarity that we use here are taken from our prior work, where we replaced biling</context>
</contexts>
<marker>Razmara, Siahbani, Haffari, Sarkar, 2013</marker>
<rawString>Majid Razmara, Maryam Siahbani, Reza Haffari, and Anoop Sarkar. 2013. Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="6162" citStr="Schafer and Yarowsky (2002)" startWordPosition="1005" endWordPosition="1008">ive than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. (2012), we used comparable corpora to estimate several features for a given phrase pair that indicate translation equivalence, including contextual, temporal, and topical similarity. The definitions of phrasal and lexica</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="5630" citStr="Sennrich, 2012" startWordPosition="923" endWordPosition="924">, or (2) a large, mixed-domain parallel training corpus is available. In the first setting, the goal is to effectively make use of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callis</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity minimization for translation model domain adaptation in statistical machine translation. In Proceedings of the Conference of the European Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing (RANLP).</booktitle>
<editor>In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors,</editor>
<contexts>
<context position="14406" citStr="Tiedemann (2009)" startWordPosition="2360" endWordPosition="2362">Table 1. Our old-domain training data is taken from the Canadian Hansard parliamentary proceedings dataset, which consists of manual transcriptions and translations of meetings of the Canadian parliament. The dataset is substantially larger than the commonly used Europarl corpus, containing over 8 million sentence pairs and about 150 million word tokens of French and English. For tuning and evaluation, we use new-domain medical and science parallel datasets released by Irvine et al. (2013a). The medical texts consist of documents from the European Medical Agency (EMEA), originally released by Tiedemann (2009). This data is primarily taken from prescription drug label text. The science data is made up of translated scientific abstracts from the fields of physics, biology, and computer science. For both the medical and science domains, we use three held-out parallel datasets of about 40 and 100 thousand words,3 respectively, released by Irvine et al. (2013a). We do tuning on dev], additional parameter selection on test2, and blind testing on test]. We use large new-domain monolingual English corpora for language modeling and for selecting new-domain-like comparable corpora from our mixed domain comp</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="5757" citStr="Wu et al. (2008)" startWordPosition="943" endWordPosition="946">se of both the old-domain and the new-domain parallel training corpora (Civera and Juan, 2007; Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Foster et al., 2010; Haddow and Koehn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowl</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Chengqing Zong</author>
</authors>
<title>Learning a phrase-based translation model from monolingual data with application to domain adaptation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5942" citStr="Zhang and Zong (2013)" startWordPosition="969" endWordPosition="972">hn, 2012; Haddow, 2013). In the second setting, it has been shown that, in some cases, training a translation model on a subset of newdomain parallel training data within a larger training corpus can be more effective than using the complete dataset (Mansour et al., 2011; Axelrod et al., 2011; Sennrich, 2012; Gasc´o et al., 2012). For many language pairs and domains, no newdomain parallel training data is available. Wu et al. (2008) machine translate new-domain source language monolingual corpora and use the synthetic parallel corpus as additional training data. Daum´e and Jagarlamudi (2011), Zhang and Zong (2013), and Irvine et al. (2013b) use new-domain comparable corpora to mine translations for unseen words. That work follows a long line of research on bilingual lexicon induction (e.g. Rapp (1995), Schafer and Yarowsky (2002), Koehn and Knight (2002), Haghighi et al. (2008), Irvine and Callison-Burch (2013), Razmara et al. (2013)). These efforts improve S4 seen, and, in some instances, sense error types. To our knowledge, no prior work has focused on fixing errors due to inaccurate translation model scores in the setting where no new-domain parallel training data is available. In Klementiev et al. </context>
</contexts>
<marker>Zhang, Zong, 2013</marker>
<rawString>Jiajun Zhang and Chengqing Zong. 2013. Learning a phrase-based translation model from monolingual data with application to domain adaptation. In Proceedings of the Conference of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
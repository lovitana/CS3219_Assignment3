<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010718">
<title confidence="0.9942">
Identifying collocations using cross-lingual association measures
</title>
<author confidence="0.998233">
Lis Pereira1, Elga Strafella2, Kevin Duh1 and Yuji Matsumoto1
</author>
<affiliation confidence="0.996507">
1Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan
</affiliation>
<email confidence="0.796646">
{lis-k, kevinduh, matsu}@is.naist.jp
</email>
<affiliation confidence="0.668154">
2National Institute for Japanese Language and Linguistics, 10-2 Midoricho, Tachikawa, Tokyo 190-8561, Japan
</affiliation>
<email confidence="0.992247">
strafelga@gmail.com
</email>
<sectionHeader confidence="0.993762" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999904461538462">
We introduce a simple and effective cross-
lingual approach to identifying colloca-
tions. This approach is based on the obser-
vation that true collocations, which cannot
be translated word for word, will exhibit
very different association scores before
and after literal translation. Our exper-
iments in Japanese demonstrate that our
cross-lingual association measure can suc-
cessfully exploit the combination of bilin-
gual dictionary and large monolingual cor-
pora, outperforming monolingual associa-
tion measures.
</bodyText>
<sectionHeader confidence="0.998792" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999738733333333">
Collocations are part of the wide range of linguis-
tic phenomena such as idioms (kick the bucket),
compounds (single-mind) and fixed phrases (by
and large) defined as Multiword Expressions
(MWEs). MWEs, and collocations, in particu-
lar, are very pervasive not only in English, but
in other languages as well. Although handling
MWEs properly is crucial in many natural lan-
guage processing (NLP) tasks, manually annotat-
ing them is a very costly and time consuming task.
The main goal of this work-in-progress is,
therefore, to evaluate the effectiveness of a simple
cross-lingual approach that allows us to automat-
ically identify collocations in a corpus and subse-
quently distinguish them according to one of their
intrinsic properties: the meaning of the expression
cannot be predicted from the meaning of the parts,
i.e. they are characterized by limited composition-
ality (Manning and Sch¨utze, 1999). Given an ex-
pression, we predict whether the expression(s) re-
sulted from the word by word translation is also
commonly used in another language. If not, that
might be evidence that the original expression is
a collocation (or an idiom). This can be cap-
tured by the ratio of association scores, assigned
by association measures, in the target vs. source
language. The results indicate that our method
improves the precision comparing with standard
methods of MWE identification through monolin-
gual association measures.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999692882352941">
Most previous works on MWEs and, more specifi-
cally, collocation identification (Evert, 2008; Sere-
tan, 2011; Pecina, 2010; Ramisch, 2012) employ
a standard methodology consisting of two steps:
1) candidate extraction, where candidates are ex-
tracted based on n-grams or morphosyntactic pat-
terns and 2) candidate filtering, where association
measures are applied to rank the candidates based
on association scores and consequently remove
noise. One drawback of such method is that as-
sociation measures might not be able to perform a
clear-cut distinction between collocation and non-
collocations, since they only assign scores based
on statistical evidence, such as co-occurrence fre-
quency in the corpus. Our cross-lingual associa-
tion measure ameliorates this problem by exploit-
ing both corpora in two languages, one of which
may be large.
A few studies have attempted to identify non-
compositional MWE’s using parallel corpora and
dictionaries. Melamed (1997) investigates how
non-compositional compounds can be detected
from parallel corpus by identifying translation di-
vergences in the component words. Pichotta and
DeNero (2013) analyses the frequency statistics
of an expression and its component words, us-
ing many bilingual corpora to identifying phrasal
verbs in English. The disadvantage of such ap-
proach is that large-scale parallel corpora is avail-
able for only a few language pairs. On the other
hand, monolingual data is largely and freely avail-
able for many languages. Our approach requires
only a bilingual dictionary and non-parallel mono-
lingual corpora in both languages.
</bodyText>
<page confidence="0.996546">
109
</page>
<note confidence="0.4475095">
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113,
Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999674714285714">
Salehi and Cook (2013) predict the degree of
compositionality using the string distance between
the automatic translation into multiple languages
of an expression and the individual translation
of its components. They use an online database
called Panlex (Baldwin et al., 2010), that can
translate words and expressions from English into
many languages. Tsvetkov and Wintner (2013) is
probably the closest work to ours. They trained
a Bayesian Network for identfying MWE’s and
one of the features used is a binary feature that
assumes value is 1 if the literal translation of the
MWE candidate occurs more than 5 times in a
large English corpus.
</bodyText>
<sectionHeader confidence="0.99585" genericHeader="method">
3 Identifying Collocations
</sectionHeader>
<bodyText confidence="0.999971">
In this research, we predict whether the expres-
sion(s) resulted from the translation of the com-
ponents of a Japanese collocation candidate is/are
also commonly used in English. For instance, if
we translate the Japanese collocation�]R�
K mendou-wo-miru ”to care for someone” (care-
R-see)1 into English word by word, we obtain
”see care”, which sounds awkward and may not
appear in an English corpus very often. On the
other hand, the word to word translation of the free
combination H*UR-QK eiga-wo-miru ”to see a
movie” (movie-R-see) is more prone to appear
in an English corpus, since it corresponds to the
translation of the expression as well. In our work,
we focus on noun-verb expressions in Japanese.
Our proposed method consists of three steps:
</bodyText>
<listItem confidence="0.836186941176471">
1) Candidate Extraction: We focus on noun-
verb constructions in Japanese. We work with
three construction types: object-verb, subject-verb
and dative-verb constructions, represented respec-
tively as “noun wo verb (noun-R-verb)”, “noun ga
verb (noun-hl-verb)” and “noun ni verb (noun-+-
verb)”, respectively. The candidates are extracted
from a Japanese corpus using a dependency parser
(Kudo and Matsumoto, 2002) and ranked by fre-
quency.
2) Translation of the component words: for
each noun-verb candidate, we automatically ob-
tain all the possible English literal translations of
the noun and the verb using a Japanese/English
dictionary. Using that information, all the possi-
ble verb-noun combinations in English are then
generated. For instance, for the candidate *R
</listItem>
<footnote confidence="0.886382">
1In Japanese, R is a case marker that indicates the object-
verb dependency relation.
</footnote>
<bodyText confidence="0.99965225">
TY7 hon-wo-kau ”to buy a book” (buy-R-book),
we take the noun * hon and the verb TY7 kau
and check their translation given in the dictio-
nary. * has translations like ”book”, ”main” and
”head” and TY7 is translated as ”buy”. Based
on that, possible combinations are ”buy book” or
”buy main” (we filter out determiners, pronouns,
etc.).
</bodyText>
<listItem confidence="0.521920666666667">
3) Ranking of original and derived word to
word translated expression: we compare the
association score of the original expression in
</listItem>
<bodyText confidence="0.994564692307692">
Japanese (calculated using a Japanese corpus) and
its corresponding derived word to word translated
expressions. If the original expression has a much
higher score than its literal translations, it might be
a good evidence that we are dealing with a collo-
cation, instead of a free combination.
There is no defined criteria in choosing one par-
ticular association measure when applying it in
a specific task, since different measures highlight
different aspects of collocativity (Evert, 2008). A
state-of-the-art, language independent framework
that employs the standard methodology to identify
MWEs is mwetoolkit (Ramisch, 2012). It ranks
the extracted candidates using four different as-
sociation measures: log-likelihood-ratio, Dice co-
efficient, pointwise mutual information and Stu-
dent’s t-score. We previously conducted exper-
iments with these four measures for Japanese
(results are ommited), and Dice coefficient per-
formed best. Using Dice coefficient, we calcu-
late the ratio between the score of the original ex-
pression and the average score of its literal trans-
lations. Finally, the candidates are ranked by the
ratio value. Those that have a high value are ex-
pected to be collocations, while those with a low
value are expected to be free combinations.
</bodyText>
<sectionHeader confidence="0.98823" genericHeader="method">
4 Experiment Setup
</sectionHeader>
<subsectionHeader confidence="0.998624">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.999894090909091">
The following resources were used in our experi-
ments:
Japanese/English dictionary: we used Edict
(Breen, 1995), a freely available Japanese/English
Dictionary in machine-readable form, containing
110,424 entries. This dictionary was used to find
all the possible translations of each Japanese word
involved in the candidate (noun and verb). For our
test set, all the words were covered by the dictio-
nary. We obtained an average of 4.5 translations
per word. All the translations that contains more
</bodyText>
<page confidence="0.989526">
110
</page>
<bodyText confidence="0.998505581395349">
than three words are filtered out. For the transla-
tions of the Japanese noun, we only consider the
first noun appearing in each translation. For the
translations of the Japanese verb, we only consider
the first verb/phrasal verb appearing in each trans-
lation. For instance, in the Japanese collocation Z
+Z!K koi-ni-ochiru ”to fall in love” (love-+-
fall down)2, the translations in the dictionary and
the ones we consider (shown in bold type) of the
noun Z koi ”love” and the verb V!K ochiru ”to
fall down” are:
?7;: love, tender passion
%!K: to fall down, to fail, to crash, to
degenerate, to degrade
Bilingual resource: we used Hiragana Times
corpus, a Japanese-English bilingual corpus of
magazine articles of Hiragana Times 3, a bilingual
magazine written in Japanese and English to intro-
duce Japan to non-Japanese, covering a wide range
of topics (culture, society, history, politics, etc.).
The corpus contains articles from 2003-2102, with
a total of 117,492 sentence pairs. We used the
Japanese data to extract the noun-verb collocation
candidates using a dependency parser, Cabocha
(Kudo and Matsumoto, 2002). For our work, we
focus on the object-verb, subject-verb and dative-
verb dependency relations. The corpus was also
used to calculate the Dice score of each Japanese
candidate, using the Japanese data.
Monolingual resource: we used 75,377 En-
glish Wikipedia articles, crawled in July 2013. It
contains a total of 9.5 million sentences. The data
was used to calculate the Dice score of each can-
didate’s derived word to word translated expres-
sions. The corpus was annotated with Part-of-
Speech (POS) information, from where we de-
fined POS patterns to extract all the verb-noun
and noun-verb sequences, using the MWE toolkit
(Ramisch, 2012), which is an integrated frame-
work for MWE treatment, providing corpus pre-
processing facilities.
Table 1 shows simple statistics on the Hiragana
Times corpus and on the Wikipedia corpus.
</bodyText>
<subsectionHeader confidence="0.999481">
4.2 Test set
</subsectionHeader>
<bodyText confidence="0.999892">
In order to evaluate our system, the top 100 fre-
quent candidates extracted from Hiragana Times
corpus were manually annotated by 4 Japanese
native speakers. The judges were asked to make
</bodyText>
<footnote confidence="0.999758">
2+ is the dative case marker in Japanese.
3http://www.hiraganatimes.com
</footnote>
<table confidence="0.999730555555555">
Hiragana Wikipedia
Times
# jp sentences 117,492 -
# en sentences 117,492 9,500,000
# jp tokens 3,949,616 -
# en tokens 2,107,613 247,355,886
# jp noun-verb 31,013 -
# en noun-verb - 266,033
# en verb-noun - 734,250
</table>
<tableCaption confidence="0.999452">
Table 1: Statistics on the Hiragana Times corpus
</tableCaption>
<bodyText confidence="0.978417352941176">
and Wikipedia corpus, showing the number of sen-
tences, number of words and number of noun-
verb and verb-noun expressions in English and
Japanese.
a ternary judgment for each of the candidates on
whether the candidate is a collocation, idiom or
free combination. For each category, a judge was
shown the definition and some examples. We de-
fined collocations as all those expressions where
one of the component words preserves its lit-
eral meaning, while the other element assumes a
slightly different meaning and its use is blocked
(i.e. it cannot be substituted by a synonym). Id-
ioms were defined as the semantically and syntac-
tically fixed expressions where all the component
words loose their original meaning. Free combi-
nations were defined as all those expressions fre-
quently used where the components preserve their
literal meaning. The inter-annotator agreement
is computed using Fleiss’ Kappa statistic (Fleiss,
1971), since it involves more than 2 annotators.
Since our method does not differentiate colloca-
tions from idioms (although we plan to work on
that as future work), we group collocations and id-
ioms as one class. We obtained a Kappa coeffi-
cient of 0.4354, which is considered as showing
moderate agreement according to Fleiss (1971).
Only the candidates identically annotated by the
majority of judges (3 or more) were added to the
test set, resulting in a number of 87 candidates
(36 collocations and 51 free combinations). Af-
ter that, we obtained a new Kappa coefficient of
0.5427, which is also considered as showing mod-
erate agreement (Fleiss, 1971).
</bodyText>
<subsectionHeader confidence="0.993272">
4.3 Baseline
</subsectionHeader>
<bodyText confidence="0.989598805555555">
We compare our proposed method with two base-
lines: an association measure based system and
a Phrase-Based Statistical Machine Translation
(SMT) based system.
Monolingual Association Measure: The sys-
tem ranks the candidates in the test set according
to their Dice score calculated using the Hiragana
Times Japanese data.
Phrase-Based SMT system: a standard non-
factored phrase-based SMT system was built us-
ing the open source Moses toolkit (Koehn et al.,
2007) with parameters set similar to those of Neu-
big (2011), who provides a baseline system pre-
viously applied to a Japanese-English corpus built
from Wikipedia articles. For training, we used Hi-
ragana Times bilingual corpus. The Japanese sen-
tences were word-segmented and the English sen-
tences were tokenized and lowercased. All sen-
tences with size greater than 60 tokens were previ-
ously eliminated. The whole English corpus was
used as training data for a 5-gram language model
built with the SRILM toolkit (Stolcke, 2002).
Similar to what we did for our proposed
method, for each candidate in the test set, we find
all the possible literally translated expressions (as
described in Section 3). In the phrase-table gen-
erated after the training step, we look for all the
entries that contain the original candidate string
and check if at least one of the possible literal
translations appear as their corresponding transla-
tion. For the entries found, we compute the av-
erage of the sum of the candidate’s direct and in-
verse phrase translation probability scores. The di-
rect phrase translation probability and the inverse
phrase translation probability (Koehn et al., 2003)
are respectively defined as:
</bodyText>
<equation confidence="0.994371833333333">
count(f, e)
-D(e|f) = (1)
E f count(f, e)
count(f, e)
-D(f|e) = (2)
Ee count(f, e)
</equation>
<bodyText confidence="0.999806">
Where f and e indicate a foreign phrase and a
source phrase, independently.
The candidates are ranked according to the av-
erage score as described previously.
</bodyText>
<sectionHeader confidence="0.996632" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999736642857143">
In our evaluation, we average the precision con-
sidering all true collocations and idioms as thresh-
old points, obtaining the mean average precision
(MAP). Differently from the traditional approach
used to evaluate an association measure, using
MAP we do not need to set a hard threshold.
Table 2 presents the MAP values for our pro-
posed method and for the two baselines. Our
cross-lingual method performs best in terms of
MAP values against the two baselines. We found
out that it performs statistically better only com-
pared to the Monolingual Association Measure
baseline4. The Monolingual Association Measure
baseline performed worst, since free combinations
were assigned high scores as well, and the system
was not able to perform a clear separation into col-
locations and non-collocations. The Phrase-Based
SMT system obtained a higher MAP value than
Monolingual Association measure, but the score
may be optimistic since we are testing in-domain.
One concern is that there are only a very few bilin-
gual/parallel corpora for the Japanese/English lan-
guage pair, in case we want to test with a different
domain and larger test set. The fact that our pro-
posed method outperforms SMT implies that us-
ing such readily-available monolingual data (En-
glish Wikipedia) is a better way to exploit cross-
lingual information.
</bodyText>
<table confidence="0.99861375">
Method MAP value
Monolingual Association Measure 0.54
Phrase-Based SMT 0.67
Proposed Method 0.71
</table>
<tableCaption confidence="0.9806315">
Table 2: Mean average precision of proposed
method and baselines.
</tableCaption>
<bodyText confidence="0.999722947368421">
Some cases where the system could not per-
form well include those where a collocation can
also have a literal translation. For instance,
in Japanese, there is the collocation ibRIM�
kokoro-wo-hiraku ”to open your heart” (heart-R-
open), where the literal translation of the noun iL\
kokoro ”heart” and the verb IM&lt; hiraku ”open”
correspond to the translation of the expression as
well.
Another case is when the candidate expression
has both literal and non-literal meaning. For in-
stance, the collocation A R _, K hito-wo-miru
(person-R-see) can mean ”to see a person”, which
is the literal meaning, but when used together with
the noun H me ”eye”, for instance, it can also
can mean ”to judge human character”. When an-
notating the data, the judges classified as idioms
some of those expressions, for instance, because
the non-literal meaning is mostly used compared
</bodyText>
<footnote confidence="0.9844165">
4Statistical significance was calculated using a two-tailed
t-test for a confidence interval of 95%.
</footnote>
<page confidence="0.995977">
112
</page>
<bodyText confidence="0.99994425">
with the literal meaning. However, our system
found that the literal translated expressions are
also commonly used in English, which caused the
performance decrease.
</bodyText>
<sectionHeader confidence="0.995637" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999991782608696">
In this report of work in progress, we propose
a method to distinguish free combinations and
collocations (and idioms) by computing the ratio
of association measures in source and target lan-
guages. We demonstrated that our method, which
can exploit existing monolingual association mea-
sures on large monolingual corpora, performed
better than techniques previously applied in MWE
identification.
In the future work, we are interested in increas-
ing the size of the corpus and test set used (for
instance, include mid to low frequent MWE’s),
as well as applying our method to other collo-
cational patterns like Noun-Adjective, Adjective-
Noun, Adverb-Verb, in order to verify our ap-
proach. We also believe that our approach can
be used for other languages as well. We intend to
conduct a further investigation on how we can dif-
ferentiate collocations from idioms. Another step
of our research will be towards the integration of
the acquired data into a web interface for language
learning and learning materials for foreign learn-
ers as well.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99997625">
We would like to thank Masashi Shimbo, Xi-
aodong Liu, Mai Omura, Yu Sawai and Yoriko
Nishijima for their valuable help and anonymous
reviewers for the helpful comments and advice.
</bodyText>
<sectionHeader confidence="0.998939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998516876923077">
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all
words of all languages of the world. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Demonstrations, pages 37–
40. Association for Computational Linguistics.
Jim Breen. 1995. Building an electronic japanese-
english dictionary. In Japanese Studies Association
of Australia Conference. Citeseer.
Stefan Evert. 2008. Corpora and collocations. Corpus
Linguistics. An International Handbook, 2.
Joseph L Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological bul-
letin, 76(5):378.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
proceedings of the 6th conference on Natural lan-
guage learning-Volume 20, pages 1–7. Association
for Computational Linguistics.
Christopher D Manning and Hinrich Sch¨utze. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.
Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. EMNLP.
Graham Neubig. 2011. The kyoto free translation task.
Available on line at http://www. phontron. com/kftt.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language resources and
evaluation, 44(1-2):137–158.
Karl Pichotta and John DeNero. 2013. Identifying
phrasal verbs using many bilingual corpora. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 636–
646.
Carlos Ramisch. 2012. A generic framework for mul-
tiword expressions treatment: from acquisition to
applications. In Proceedings of ACL 2012 Student
Research Workshop, pages 61–66. Association for
Computational Linguistics.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages.
Violeta Seretan. 2011. Syntax-based collocation ex-
traction, volume 44. Springer.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov and Shuly Wintner. 2013. Identifica-
tion of multi-word expressions by combining multi-
ple linguistic information sources.
</reference>
<page confidence="0.999307">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.406203">
<title confidence="0.996561">Identifying collocations using cross-lingual association measures</title>
<author confidence="0.997299">Elga Kevin</author>
<author confidence="0.997299">Yuji</author>
<affiliation confidence="0.969814">Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192,</affiliation>
<email confidence="0.609622">kevinduh,</email>
<affiliation confidence="0.781603">Institute for Japanese Language and Linguistics, 10-2 Midoricho, Tachikawa, Tokyo 190-8561,</affiliation>
<email confidence="0.999213">strafelga@gmail.com</email>
<abstract confidence="0.997699071428571">We introduce a simple and effective crosslingual approach to identifying collocations. This approach is based on the observation that true collocations, which cannot be translated word for word, will exhibit very different association scores before and after literal translation. Our experiments in Japanese demonstrate that our cross-lingual association measure can successfully exploit the combination of bilingual dictionary and large monolingual corpora, outperforming monolingual association measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Jonathan Pool</author>
<author>Susan M Colowick</author>
</authors>
<title>Panlex and lextract: Translating all words of all languages of the world.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4381" citStr="Baldwin et al., 2010" startWordPosition="645" endWordPosition="648">gual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics Salehi and Cook (2013) predict the degree of compositionality using the string distance between the automatic translation into multiple languages of an expression and the individual translation of its components. They use an online database called Panlex (Baldwin et al., 2010), that can translate words and expressions from English into many languages. Tsvetkov and Wintner (2013) is probably the closest work to ours. They trained a Bayesian Network for identfying MWE’s and one of the features used is a binary feature that assumes value is 1 if the literal translation of the MWE candidate occurs more than 5 times in a large English corpus. 3 Identifying Collocations In this research, we predict whether the expression(s) resulted from the translation of the components of a Japanese collocation candidate is/are also commonly used in English. For instance, if we transla</context>
</contexts>
<marker>Baldwin, Pool, Colowick, 2010</marker>
<rawString>Timothy Baldwin, Jonathan Pool, and Susan M Colowick. 2010. Panlex and lextract: Translating all words of all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations, pages 37– 40. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Breen</author>
</authors>
<title>Building an electronic japaneseenglish dictionary.</title>
<date>1995</date>
<booktitle>In Japanese Studies Association of Australia Conference.</booktitle>
<publisher>Citeseer.</publisher>
<contexts>
<context position="8250" citStr="Breen, 1995" startWordPosition="1256" endWordPosition="1257">t-score. We previously conducted experiments with these four measures for Japanese (results are ommited), and Dice coefficient performed best. Using Dice coefficient, we calculate the ratio between the score of the original expression and the average score of its literal translations. Finally, the candidates are ranked by the ratio value. Those that have a high value are expected to be collocations, while those with a low value are expected to be free combinations. 4 Experiment Setup 4.1 Data Set The following resources were used in our experiments: Japanese/English dictionary: we used Edict (Breen, 1995), a freely available Japanese/English Dictionary in machine-readable form, containing 110,424 entries. This dictionary was used to find all the possible translations of each Japanese word involved in the candidate (noun and verb). For our test set, all the words were covered by the dictionary. We obtained an average of 4.5 translations per word. All the translations that contains more 110 than three words are filtered out. For the translations of the Japanese noun, we only consider the first noun appearing in each translation. For the translations of the Japanese verb, we only consider the fir</context>
</contexts>
<marker>Breen, 1995</marker>
<rawString>Jim Breen. 1995. Building an electronic japaneseenglish dictionary. In Japanese Studies Association of Australia Conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>Corpora and collocations. Corpus Linguistics.</title>
<date>2008</date>
<journal>An International Handbook,</journal>
<volume>2</volume>
<contexts>
<context position="2436" citStr="Evert, 2008" startWordPosition="357" endWordPosition="358">ession, we predict whether the expression(s) resulted from the word by word translation is also commonly used in another language. If not, that might be evidence that the original expression is a collocation (or an idiom). This can be captured by the ratio of association scores, assigned by association measures, in the target vs. source language. The results indicate that our method improves the precision comparing with standard methods of MWE identification through monolingual association measures. 2 Related Work Most previous works on MWEs and, more specifically, collocation identification (Evert, 2008; Seretan, 2011; Pecina, 2010; Ramisch, 2012) employ a standard methodology consisting of two steps: 1) candidate extraction, where candidates are extracted based on n-grams or morphosyntactic patterns and 2) candidate filtering, where association measures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in th</context>
<context position="7339" citStr="Evert, 2008" startWordPosition="1117" endWordPosition="1118">f original and derived word to word translated expression: we compare the association score of the original expression in Japanese (calculated using a Japanese corpus) and its corresponding derived word to word translated expressions. If the original expression has a much higher score than its literal translations, it might be a good evidence that we are dealing with a collocation, instead of a free combination. There is no defined criteria in choosing one particular association measure when applying it in a specific task, since different measures highlight different aspects of collocativity (Evert, 2008). A state-of-the-art, language independent framework that employs the standard methodology to identify MWEs is mwetoolkit (Ramisch, 2012). It ranks the extracted candidates using four different association measures: log-likelihood-ratio, Dice coefficient, pointwise mutual information and Student’s t-score. We previously conducted experiments with these four measures for Japanese (results are ommited), and Dice coefficient performed best. Using Dice coefficient, we calculate the ratio between the score of the original expression and the average score of its literal translations. Finally, the ca</context>
</contexts>
<marker>Evert, 2008</marker>
<rawString>Stefan Evert. 2008. Corpora and collocations. Corpus Linguistics. An International Handbook, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="12039" citStr="Fleiss, 1971" startWordPosition="1865" endWordPosition="1866">ion and some examples. We defined collocations as all those expressions where one of the component words preserves its literal meaning, while the other element assumes a slightly different meaning and its use is blocked (i.e. it cannot be substituted by a synonym). Idioms were defined as the semantically and syntactically fixed expressions where all the component words loose their original meaning. Free combinations were defined as all those expressions frequently used where the components preserve their literal meaning. The inter-annotator agreement is computed using Fleiss’ Kappa statistic (Fleiss, 1971), since it involves more than 2 annotators. Since our method does not differentiate collocations from idioms (although we plan to work on that as future work), we group collocations and idioms as one class. We obtained a Kappa coefficient of 0.4354, which is considered as showing moderate agreement according to Fleiss (1971). Only the candidates identically annotated by the majority of judges (3 or more) were added to the test set, resulting in a number of 87 candidates (36 collocations and 51 free combinations). After that, we obtained a new Kappa coefficient of 0.5427, which is also consider</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14326" citStr="Koehn et al., 2003" startWordPosition="2235" endWordPosition="2238">our proposed method, for each candidate in the test set, we find all the possible literally translated expressions (as described in Section 3). In the phrase-table generated after the training step, we look for all the entries that contain the original candidate string and check if at least one of the possible literal translations appear as their corresponding translation. For the entries found, we compute the average of the sum of the candidate’s direct and inverse phrase translation probability scores. The direct phrase translation probability and the inverse phrase translation probability (Koehn et al., 2003) are respectively defined as: count(f, e) -D(e|f) = (1) E f count(f, e) count(f, e) -D(f|e) = (2) Ee count(f, e) Where f and e indicate a foreign phrase and a source phrase, independently. The candidates are ranked according to the average score as described previously. 5 Evaluation In our evaluation, we average the precision considering all true collocations and idioms as threshold points, obtaining the mean average precision (MAP). Differently from the traditional approach used to evaluate an association measure, using MAP we do not need to set a hard threshold. Table 2 presents the MAP valu</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13159" citStr="Koehn et al., 2007" startWordPosition="2045" endWordPosition="2048">ee combinations). After that, we obtained a new Kappa coefficient of 0.5427, which is also considered as showing moderate agreement (Fleiss, 1971). 4.3 Baseline We compare our proposed method with two baselines: an association measure based system and a Phrase-Based Statistical Machine Translation (SMT) based system. Monolingual Association Measure: The system ranks the candidates in the test set according to their Dice score calculated using the Hiragana Times Japanese data. Phrase-Based SMT system: a standard nonfactored phrase-based SMT system was built using the open source Moses toolkit (Koehn et al., 2007) with parameters set similar to those of Neubig (2011), who provides a baseline system previously applied to a Japanese-English corpus built from Wikipedia articles. For training, we used Hiragana Times bilingual corpus. The Japanese sentences were word-segmented and the English sentences were tokenized and lowercased. All sentences with size greater than 60 tokens were previously eliminated. The whole English corpus was used as training data for a 5-gram language model built with the SRILM toolkit (Stolcke, 2002). Similar to what we did for our proposed method, for each candidate in the test </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In proceedings of the 6th conference on Natural language learning-Volume 20,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5939" citStr="Kudo and Matsumoto, 2002" startWordPosition="891" endWordPosition="894">more prone to appear in an English corpus, since it corresponds to the translation of the expression as well. In our work, we focus on noun-verb expressions in Japanese. Our proposed method consists of three steps: 1) Candidate Extraction: We focus on nounverb constructions in Japanese. We work with three construction types: object-verb, subject-verb and dative-verb constructions, represented respectively as “noun wo verb (noun-R-verb)”, “noun ga verb (noun-hl-verb)” and “noun ni verb (noun-+- verb)”, respectively. The candidates are extracted from a Japanese corpus using a dependency parser (Kudo and Matsumoto, 2002) and ranked by frequency. 2) Translation of the component words: for each noun-verb candidate, we automatically obtain all the possible English literal translations of the noun and the verb using a Japanese/English dictionary. Using that information, all the possible verb-noun combinations in English are then generated. For instance, for the candidate *R 1In Japanese, R is a case marker that indicates the objectverb dependency relation. TY7 hon-wo-kau ”to buy a book” (buy-R-book), we take the noun * hon and the verb TY7 kau and check their translation given in the dictionary. * has translation</context>
<context position="9760" citStr="Kudo and Matsumoto, 2002" startWordPosition="1495" endWordPosition="1498">”to fall down” are: ?7;: love, tender passion %!K: to fall down, to fail, to crash, to degenerate, to degrade Bilingual resource: we used Hiragana Times corpus, a Japanese-English bilingual corpus of magazine articles of Hiragana Times 3, a bilingual magazine written in Japanese and English to introduce Japan to non-Japanese, covering a wide range of topics (culture, society, history, politics, etc.). The corpus contains articles from 2003-2102, with a total of 117,492 sentence pairs. We used the Japanese data to extract the noun-verb collocation candidates using a dependency parser, Cabocha (Kudo and Matsumoto, 2002). For our work, we focus on the object-verb, subject-verb and dativeverb dependency relations. The corpus was also used to calculate the Dice score of each Japanese candidate, using the Japanese data. Monolingual resource: we used 75,377 English Wikipedia articles, crawled in July 2013. It contains a total of 9.5 million sentences. The data was used to calculate the Dice score of each candidate’s derived word to word translated expressions. The corpus was annotated with Part-ofSpeech (POS) information, from where we defined POS patterns to extract all the verb-noun and noun-verb sequences, usi</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In proceedings of the 6th conference on Natural language learning-Volume 20, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of statistical natural language processing, volume 999.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D Manning and Hinrich Sch¨utze. 1999. Foundations of statistical natural language processing, volume 999. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Automatic discovery of noncompositional compounds in parallel data.</title>
<date>1997</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="3300" citStr="Melamed (1997)" startWordPosition="488" endWordPosition="489">asures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measure ameliorates this problem by exploiting both corpora in two languages, one of which may be large. A few studies have attempted to identify noncompositional MWE’s using parallel corpora and dictionaries. Melamed (1997) investigates how non-compositional compounds can be detected from parallel corpus by identifying translation divergences in the component words. Pichotta and DeNero (2013) analyses the frequency statistics of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>Dan Melamed. 1997. Automatic discovery of noncompositional compounds in parallel data. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
</authors>
<title>The kyoto free translation task. Available on line at http://www.</title>
<date>2011</date>
<tech>phontron. com/kftt.</tech>
<contexts>
<context position="13213" citStr="Neubig (2011)" startWordPosition="2056" endWordPosition="2058">cient of 0.5427, which is also considered as showing moderate agreement (Fleiss, 1971). 4.3 Baseline We compare our proposed method with two baselines: an association measure based system and a Phrase-Based Statistical Machine Translation (SMT) based system. Monolingual Association Measure: The system ranks the candidates in the test set according to their Dice score calculated using the Hiragana Times Japanese data. Phrase-Based SMT system: a standard nonfactored phrase-based SMT system was built using the open source Moses toolkit (Koehn et al., 2007) with parameters set similar to those of Neubig (2011), who provides a baseline system previously applied to a Japanese-English corpus built from Wikipedia articles. For training, we used Hiragana Times bilingual corpus. The Japanese sentences were word-segmented and the English sentences were tokenized and lowercased. All sentences with size greater than 60 tokens were previously eliminated. The whole English corpus was used as training data for a 5-gram language model built with the SRILM toolkit (Stolcke, 2002). Similar to what we did for our proposed method, for each candidate in the test set, we find all the possible literally translated exp</context>
</contexts>
<marker>Neubig, 2011</marker>
<rawString>Graham Neubig. 2011. The kyoto free translation task. Available on line at http://www. phontron. com/kftt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
</authors>
<title>Lexical association measures and collocation extraction. Language resources and evaluation,</title>
<date>2010</date>
<pages>44--1</pages>
<contexts>
<context position="2465" citStr="Pecina, 2010" startWordPosition="362" endWordPosition="363">he expression(s) resulted from the word by word translation is also commonly used in another language. If not, that might be evidence that the original expression is a collocation (or an idiom). This can be captured by the ratio of association scores, assigned by association measures, in the target vs. source language. The results indicate that our method improves the precision comparing with standard methods of MWE identification through monolingual association measures. 2 Related Work Most previous works on MWEs and, more specifically, collocation identification (Evert, 2008; Seretan, 2011; Pecina, 2010; Ramisch, 2012) employ a standard methodology consisting of two steps: 1) candidate extraction, where candidates are extracted based on n-grams or morphosyntactic patterns and 2) candidate filtering, where association measures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual a</context>
</contexts>
<marker>Pecina, 2010</marker>
<rawString>Pavel Pecina. 2010. Lexical association measures and collocation extraction. Language resources and evaluation, 44(1-2):137–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pichotta</author>
<author>John DeNero</author>
</authors>
<title>Identifying phrasal verbs using many bilingual corpora.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>636--646</pages>
<contexts>
<context position="3472" citStr="Pichotta and DeNero (2013)" startWordPosition="509" endWordPosition="512">ght not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measure ameliorates this problem by exploiting both corpora in two languages, one of which may be large. A few studies have attempted to identify noncompositional MWE’s using parallel corpora and dictionaries. Melamed (1997) investigates how non-compositional compounds can be detected from parallel corpus by identifying translation divergences in the component words. Pichotta and DeNero (2013) analyses the frequency statistics of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, Gothenburg, Sweden, 26-27 April 2014. c�2014 Associatio</context>
</contexts>
<marker>Pichotta, DeNero, 2013</marker>
<rawString>Karl Pichotta and John DeNero. 2013. Identifying phrasal verbs using many bilingual corpora. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 636– 646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
</authors>
<title>A generic framework for multiword expressions treatment: from acquisition to applications.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012 Student Research Workshop,</booktitle>
<pages>61--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2481" citStr="Ramisch, 2012" startWordPosition="364" endWordPosition="365">s) resulted from the word by word translation is also commonly used in another language. If not, that might be evidence that the original expression is a collocation (or an idiom). This can be captured by the ratio of association scores, assigned by association measures, in the target vs. source language. The results indicate that our method improves the precision comparing with standard methods of MWE identification through monolingual association measures. 2 Related Work Most previous works on MWEs and, more specifically, collocation identification (Evert, 2008; Seretan, 2011; Pecina, 2010; Ramisch, 2012) employ a standard methodology consisting of two steps: 1) candidate extraction, where candidates are extracted based on n-grams or morphosyntactic patterns and 2) candidate filtering, where association measures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our cross-lingual association measu</context>
<context position="7476" citStr="Ramisch, 2012" startWordPosition="1134" endWordPosition="1135">culated using a Japanese corpus) and its corresponding derived word to word translated expressions. If the original expression has a much higher score than its literal translations, it might be a good evidence that we are dealing with a collocation, instead of a free combination. There is no defined criteria in choosing one particular association measure when applying it in a specific task, since different measures highlight different aspects of collocativity (Evert, 2008). A state-of-the-art, language independent framework that employs the standard methodology to identify MWEs is mwetoolkit (Ramisch, 2012). It ranks the extracted candidates using four different association measures: log-likelihood-ratio, Dice coefficient, pointwise mutual information and Student’s t-score. We previously conducted experiments with these four measures for Japanese (results are ommited), and Dice coefficient performed best. Using Dice coefficient, we calculate the ratio between the score of the original expression and the average score of its literal translations. Finally, the candidates are ranked by the ratio value. Those that have a high value are expected to be collocations, while those with a low value are ex</context>
<context position="10394" citStr="Ramisch, 2012" startWordPosition="1600" endWordPosition="1601">ocus on the object-verb, subject-verb and dativeverb dependency relations. The corpus was also used to calculate the Dice score of each Japanese candidate, using the Japanese data. Monolingual resource: we used 75,377 English Wikipedia articles, crawled in July 2013. It contains a total of 9.5 million sentences. The data was used to calculate the Dice score of each candidate’s derived word to word translated expressions. The corpus was annotated with Part-ofSpeech (POS) information, from where we defined POS patterns to extract all the verb-noun and noun-verb sequences, using the MWE toolkit (Ramisch, 2012), which is an integrated framework for MWE treatment, providing corpus preprocessing facilities. Table 1 shows simple statistics on the Hiragana Times corpus and on the Wikipedia corpus. 4.2 Test set In order to evaluate our system, the top 100 frequent candidates extracted from Hiragana Times corpus were manually annotated by 4 Japanese native speakers. The judges were asked to make 2+ is the dative case marker in Japanese. 3http://www.hiraganatimes.com Hiragana Wikipedia Times # jp sentences 117,492 - # en sentences 117,492 9,500,000 # jp tokens 3,949,616 - # en tokens 2,107,613 247,355,886 </context>
</contexts>
<marker>Ramisch, 2012</marker>
<rawString>Carlos Ramisch. 2012. A generic framework for multiword expressions treatment: from acquisition to applications. In Proceedings of ACL 2012 Student Research Workshop, pages 61–66. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bahar Salehi</author>
<author>Paul Cook</author>
</authors>
<title>Predicting the compositionality of multiword expressions using translations in multiple languages.</title>
<date>2013</date>
<contexts>
<context position="4126" citStr="Salehi and Cook (2013)" startWordPosition="608" endWordPosition="611">cs of an expression and its component words, using many bilingual corpora to identifying phrasal verbs in English. The disadvantage of such approach is that large-scale parallel corpora is available for only a few language pairs. On the other hand, monolingual data is largely and freely available for many languages. Our approach requires only a bilingual dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics Salehi and Cook (2013) predict the degree of compositionality using the string distance between the automatic translation into multiple languages of an expression and the individual translation of its components. They use an online database called Panlex (Baldwin et al., 2010), that can translate words and expressions from English into many languages. Tsvetkov and Wintner (2013) is probably the closest work to ours. They trained a Bayesian Network for identfying MWE’s and one of the features used is a binary feature that assumes value is 1 if the literal translation of the MWE candidate occurs more than 5 times in </context>
</contexts>
<marker>Salehi, Cook, 2013</marker>
<rawString>Bahar Salehi and Paul Cook. 2013. Predicting the compositionality of multiword expressions using translations in multiple languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Violeta Seretan</author>
</authors>
<title>Syntax-based collocation extraction,</title>
<date>2011</date>
<volume>44</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="2451" citStr="Seretan, 2011" startWordPosition="359" endWordPosition="361">edict whether the expression(s) resulted from the word by word translation is also commonly used in another language. If not, that might be evidence that the original expression is a collocation (or an idiom). This can be captured by the ratio of association scores, assigned by association measures, in the target vs. source language. The results indicate that our method improves the precision comparing with standard methods of MWE identification through monolingual association measures. 2 Related Work Most previous works on MWEs and, more specifically, collocation identification (Evert, 2008; Seretan, 2011; Pecina, 2010; Ramisch, 2012) employ a standard methodology consisting of two steps: 1) candidate extraction, where candidates are extracted based on n-grams or morphosyntactic patterns and 2) candidate filtering, where association measures are applied to rank the candidates based on association scores and consequently remove noise. One drawback of such method is that association measures might not be able to perform a clear-cut distinction between collocation and noncollocations, since they only assign scores based on statistical evidence, such as co-occurrence frequency in the corpus. Our c</context>
</contexts>
<marker>Seretan, 2011</marker>
<rawString>Violeta Seretan. 2011. Syntax-based collocation extraction, volume 44. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In INTERSPEECH.</booktitle>
<contexts>
<context position="13678" citStr="Stolcke, 2002" startWordPosition="2131" endWordPosition="2132">ored phrase-based SMT system was built using the open source Moses toolkit (Koehn et al., 2007) with parameters set similar to those of Neubig (2011), who provides a baseline system previously applied to a Japanese-English corpus built from Wikipedia articles. For training, we used Hiragana Times bilingual corpus. The Japanese sentences were word-segmented and the English sentences were tokenized and lowercased. All sentences with size greater than 60 tokens were previously eliminated. The whole English corpus was used as training data for a 5-gram language model built with the SRILM toolkit (Stolcke, 2002). Similar to what we did for our proposed method, for each candidate in the test set, we find all the possible literally translated expressions (as described in Section 3). In the phrase-table generated after the training step, we look for all the entries that contain the original candidate string and check if at least one of the possible literal translations appear as their corresponding translation. For the entries found, we compute the average of the sum of the candidate’s direct and inverse phrase translation probability scores. The direct phrase translation probability and the inverse phr</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Shuly Wintner</author>
</authors>
<title>Identification of multi-word expressions by combining multiple linguistic information sources.</title>
<date>2013</date>
<contexts>
<context position="4485" citStr="Tsvetkov and Wintner (2013)" startWordPosition="660" endWordPosition="663">al dictionary and non-parallel monolingual corpora in both languages. 109 Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 109–113, Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics Salehi and Cook (2013) predict the degree of compositionality using the string distance between the automatic translation into multiple languages of an expression and the individual translation of its components. They use an online database called Panlex (Baldwin et al., 2010), that can translate words and expressions from English into many languages. Tsvetkov and Wintner (2013) is probably the closest work to ours. They trained a Bayesian Network for identfying MWE’s and one of the features used is a binary feature that assumes value is 1 if the literal translation of the MWE candidate occurs more than 5 times in a large English corpus. 3 Identifying Collocations In this research, we predict whether the expression(s) resulted from the translation of the components of a Japanese collocation candidate is/are also commonly used in English. For instance, if we translate the Japanese collocation�]R� K mendou-wo-miru ”to care for someone” (careR-see)1 into English word by</context>
</contexts>
<marker>Tsvetkov, Wintner, 2013</marker>
<rawString>Yulia Tsvetkov and Shuly Wintner. 2013. Identification of multi-word expressions by combining multiple linguistic information sources.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000203">
<title confidence="0.997421">
Identifying Appropriate Support for Propositions in Online User
Comments
</title>
<author confidence="0.995579">
Joonsuk Park
</author>
<affiliation confidence="0.996897">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.69624">
Ithaca, NY, USA
</address>
<email confidence="0.999267">
jpark@cs.cornell.edu
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986230769231">
The ability to analyze the adequacy of sup-
porting information is necessary for deter-
mining the strength of an argument.1 This
is especially the case for online user com-
ments, which often consist of arguments
lacking proper substantiation and reason-
ing. Thus, we develop a framework for
automatically classifying each proposition
as UNVERIFIABLE, VERIFIABLE NON-
EXPERIENTIAL, or VERIFIABLE EXPE-
RIENTIAL2, where the appropriate type of
support is reason, evidence, and optional
evidence, respectively3. Once the exist-
ing support for propositions are identi-
fied, this classification can provide an es-
timate of how adequately the arguments
have been supported. We build a gold-
standard dataset of 9,476 sentences and
clauses from 1,047 comments submitted
to an eRulemaking platform and find that
Support Vector Machine (SVM) classifiers
trained with n-grams and additional fea-
tures capturing the verifiability and expe-
rientiality exhibit statistically significant
improvement over the unigram baseline,
achieving a macro-averaged F1 of 68.99%.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986225">
Argumentation mining is a relatively new field
focusing on identifying and extracting argumen-
tative structures in documents. An argument is
typically defined as a conclusion with supporting
</bodyText>
<footnote confidence="0.9953095">
1In this work, even unsupported propositions are consider
part of an argument. Not disregarding such implicit argu-
ments allows us to discuss the types of support that can fur-
ther be provided to strengthen the argument, as a form of as-
sessment.
2Verifiable Experiential propositions are verifiable propo-
sitions about personal state or experience. See Table 1 for
examples.
3We are assuming that there is no background knowledge
that eliminates the need of support.
</footnote>
<author confidence="0.891513">
Claire Cardie
</author>
<affiliation confidence="0.825536666666667">
Department of Computer Science
Cornell University
Ithaca, NY, USA
</affiliation>
<email confidence="0.993579">
cardie@cs.cornell.edu
</email>
<bodyText confidence="0.999816648648649">
premises, which can be conclusions of other argu-
ments themselves (Toulmin, 1958; Toulmin et al.,
1979; Pollock, 1987). To date, much of the argu-
mentation mining research has been conducted on
domains like news articles, parliamentary records
and legal documents, where the documents con-
tain well-formed explicit arguments, i.e. proposi-
tions with supporting reasons and evidence present
in the text (Moens et al., 2007; Palau and Moens,
2009; Wyner et al., 2010; Feng and Hirst, 2011;
Ashley and Walker, 2013).
Unlike documents written by professionals, on-
line user comments often contain arguments with
inappropriate or missing justification. One way
to deal with such implicit arguments is to sim-
ply disregard them and focus on extracting ar-
guments containing proper support (Villalba and
Saint-Dizier, 2012; Cabrio and Villata, 2012).
However, recognizing such propositions as part
of an argument,4 and determining the appropriate
types of support can be useful for assessing the ad-
equacy of the supporting information, and in turn,
the strength of the whole argument. Consider the
following examples:
How much does a small carton of
milk cost?1 More children should drink
milk2, because children who drink milk
everyday are taller than those who
don’t3. Children would want to drink
milk, anyway4.
Firstly, Sentence 1 does not need any support,
nor is it part of an argument. Next, Proposition 2
is an unverifiable proposition because it cannot be
proved with objective evidence, due to the value
judgement. Instead, it can be supported by a rea-
son explaining why it may be true. If the rea-
son, Proposition 3, were not true, the whole ar-
</bodyText>
<footnote confidence="0.995119">
4Not all sentences in user comments are part of an argu-
ment, e.g. questions and greetings. We address this in Sec-
tion 4.1
</footnote>
<page confidence="0.983377">
29
</page>
<note confidence="0.6808985">
Proceedings of the First Workshop on Argumentation Mining, pages 29–38,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999953105263158">
gument would fall apart, giving little weight to
Proposition 2. Thus, an objective evidence sup-
porting Proposition 3, which is a verifiable propo-
sition, could be provided to strengthen the argu-
ment. Lastly, as Proposition 4 is unverifiable, we
cannot expect an objective evidence that proves it,
but a reason as its support. Note that providing
a reason why Proposition 3 might be true is not
as effective as substantiating it with a proof, but
is still better than having no support. This shows
that not only the presence, but also the type of sup-
porting information affects the strength of the ar-
gument.
Examining each proposition in this way, i.e.
with respect to its verifiability, provides a means
to determine the desirable types of support, if
any, and enables the analysis of the arguments
in terms of the adequacy of their support. Thus,
we propose the task of classifying each proposi-
tion (the elementary unit of argumentation in this
work) in an argument as UNVERIFIABLE, VERI-
FIABLE PUBLIC, or VERIFIABLE PRIVATE, where
the appropriate type of support is reason, evidence,
and optional evidence, respectively. To perform
the experiments, we annotate 9,476 sentences and
clauses from 1,047 comments extracted from an
eRulemaking platform.
In the remainder of the paper, we describe the
annotation scheme and a newly created dataset
(Section 2), propose a supervised learning ap-
proach to the task (Section 3), evaluate the ap-
proach (Section 4), and survey related work (Sec-
tion 5). We find that Support Vector Machines
(SVM) classifiers trained with n-grams and other
features to capture the verifiability and experien-
tiality exhibit statistically significant improvement
over the unigram baseline, achieving a macro-
averaged F1 score of 68.99%.
</bodyText>
<sectionHeader confidence="0.988847" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999188">
We have collected and manually annotated sen-
tences and (independent) clauses from user com-
ments extracted from an eRulemaking website,
Regulation Room5. Rulemaking is the process by
which U.S. government agencies make new reg-
ulations and enact public policy; its digital coun-
terpart — eRulemaking — moves the process to
online platforms (see, e.g. (Park et al., 2012)).
By providing platforms in which the public can
discuss regulations that interest them, government
</bodyText>
<footnote confidence="0.946324">
5http://www.regulationroom.org
</footnote>
<bodyText confidence="0.999812586206896">
agencies hope to enlist the expertise and experi-
ence of participants to create better regulations.
In many rulemaking scenarios, agencies are, in
fact, required to obtain feedback from the pub-
lic on the proposed regulation as well as to ad-
dress all substantive questions, criticisms or sug-
gestions that are raised (Lubbers, 2006). In this
way, public comments can produce changes in the
final rule (Hochschild and Danielson, 1998) that,
in turn, can affect millions of lives. It is crucial,
therefore, for rule makers to be able to identify
credible comments from those submitted.
Regulation Room is an experimental web-
site operated by Cornell eRulemaking Initiative
(CeRI)6 to promote public participation in the
rulemaking process, help users write more infor-
mative comments and build collective knowledge
via active discussions guided by human moder-
ators. Regulation Room hosts actual regulations
from government agencies, such as the U.S. De-
partment of Transportation.
For our research, we collected and manually an-
notated 9,476 propositions from 1,047 user com-
ments from two recent rules: Airline Passenger
Rights (serving peanuts on the plane, tarmac de-
lay contingency plan, oversales of tickets, baggage
fees and other airline traveller rights) and Home
Mortgage Consumer Protection (loss mitigation,
accounting error resolution, etc.).
</bodyText>
<subsectionHeader confidence="0.998769">
2.1 Annotation Scheme
</subsectionHeader>
<bodyText confidence="0.972525631578947">
To start, we collected 1,147 comments and ran-
domly selected 100 of them to devise an annota-
tion scheme for identifying appropriate types of
support for propositions and to train annotators.
Initially, we allowed the annotators to define the
span for a propositions, leading to various compli-
cations and a low inter-annotator reliability. Thus,
we introduced an additional step in which com-
ments were manually sliced into propositions (or
non-propositional sentences) before being given to
the annotators. A proposition or sentence found
this way was split further if it consisted of two or
more independent clauses. The sliced comments
were then coded by two annotators into the fol-
lowing four disjoint classes (See Figure 1 for an
overview):
Verifiable Proposition [Experiential(VERIFEXP)
and Non-experiential(VERIFNON)]. A proposi-
tion is verifiable if it contains an objective asser-
</bodyText>
<footnote confidence="0.984132">
6http://www.lawschool.cornell.edu/ceri/
</footnote>
<page confidence="0.998348">
30
</page>
<figureCaption confidence="0.99915">
Figure 1: Flow chart for annotation (It refers to the sentence (or clause) being annotated)
</figureCaption>
<figure confidence="0.992200708333333">
# proposition
VERIFEXP 1 I’ve been a physician for 20 years.
2 My son has hypolycemia.
3 They flew me to NY in February.
4 The flight attendant yelled at the passengers.
VERIFNON 5 They can have inhalation reactions.
6 since they serve them to the whole plane.
7 Peanuts do not kill people.
8 Clearly, peanuts do not kill people.
9 I believe peanuts do not kill people.
10 The governor said that he enjoyed it.
11 food allergies are rare
12 food allergies are seen in less than 20% of the
population
UNVERIF 13 Again, keep it simple.
14 Banning peanuts will reduce deaths.
15 I enjoy having peanuts on the plane.
16 others are of uncertain significance
17 banning peanuts is a slippery slope
NONARG 18 Who is in charge of this?
19 I have two comments
20 http://www.someurl.com
21 Thanks for allowing me to comment.
22 - Mike
</figure>
<tableCaption confidence="0.729378">
Table 1: Example Sentences.
* Italics is used to illustrate core clause (Section 3.2).
</tableCaption>
<bodyText confidence="0.993396866666667">
tion, where objective means “expressing or deal-
ing with facts or conditions as perceived without
distortion by personal feelings, prejudices, or in-
terpretations.”7 Such assertions have truth values
that can be proved or disproved with objective ev-
idence8:
Consider the examples from Table 1. propo-
sitions 1 through 7 are clearly verifiable because
they only contain objective assertions. proposi-
tions 8 and 9 show that adding subjective expres-
sions such as “Clearly” (e.g. sentence 8) or “I be-
lieve that” (e.g. sentence 9) to an objectively veri-
fiable proposition (e.g. sentence 7) does not affect
the verifiability of the proposition. Sentence 10 is
considered verifiable because whether or not the
</bodyText>
<footnote confidence="0.989187333333333">
7http://www.merriam-webster.com/
8The correctness of the assertion or the availability of the
objective evidence does not matter.
</footnote>
<bodyText confidence="0.999985861111111">
governor said “he enjoyed the peanuts” can be ver-
ified with objective evidence, even though whether
he really does or not cannot be verified.
For the purpose of identifying an appropriate
type of support, we employ a rather lenient no-
tion of objectivity: an assertion is objectively veri-
fiable if the domain of comparison is free of inter-
pretation. For instance, sentence 11 is regarded as
objectively verifiable, because it is clear, i.e. it is
not open for interpretation, that percentage of the
population is the metric under comparison even
though the threshold is purely subjective9. The
rationale is that this type of proposition can be
sufficiently substantiated with objective evidence
(e.g. published statistics showing the percentage
of people suffering from food allergies). Another
way to think about it is that sentence 11 is a loose
way of saying a (more obviously) verifiable sen-
tence 12, where the commenter neglected to men-
tion the threshold. This is fundamentally different
from propositions 13 through 16 for which objec-
tive evidence cannot exist10.
A verifiable proposition can further be dis-
tinguished as experiential or not, depending on
whether the proposition is about the writer’s per-
sonal state or experience (VERIFEXP) or some-
thing non-experiential (VERIFNON). This dif-
ference determines whether objective evidence is
mandatory or optional with respect to the credibil-
ity of the comment. Evidence is optional when the
evidence contains private information or is prac-
tically impossible to be provided: While proposi-
tions 1 through 3 can be proved with pictures of
official documents, for instance, the commenters
may not want to provide them for privacy rea-
sons. Also, the website interface may not al-
</bodyText>
<footnote confidence="0.9729365">
9One may think anything less frequent than the average is
rare and another may have more stricter notion.
10Objective evidence may exist for propositions that pro-
vide reasons for propositions 13 through 16.
</footnote>
<page confidence="0.999546">
31
</page>
<table confidence="0.999342">
Regulation VERIFNON VERIFEXP UNVERIF Subtotal NONARG Total # of Comments
APR 1106 851 4413 6370 522 6892 820
HMCP 251 416 1733 2400 186 2586 227
Total 1357 1267 6146 8770 708 9476 1047
</table>
<tableCaption confidence="0.997604">
Table 2: Class Distribution Over Sentences and Clauses
</tableCaption>
<bodyText confidence="0.996111581395349">
low pictures to be uploaded in comment section,
which is the case with most websites. sentence 4
is practically impossible to prove unless the com-
menter happened to have recorded the conversa-
tion, and the website interface allows multimedia
files to be uploaded. This is different from propo-
sitions 5 through 12, which should be (if valid, that
is) based on non-experiential knowledge the com-
menter acquired through objective evidence avail-
able to the public.
In certain domains, VERIFEXP propositions—
sometimes referred to as anectotal evidence—
provide the novel knowledge that readers are seek-
ing. In eRulemaking, for instance, agencies ac-
cept a wide variety of comments from the pub-
lic, including accounts of personal experience with
the problems or conditions the new regulation pro-
poses to address. If these accounts are relevant and
plausible, the agencies may use them, even if they
include no independent substantiation. Taking it
to an extreme, even if the “experience” is fake, the
“experience” and opinions based on them are valu-
able to the agencies as long as the “experience” is
realistic.
Unverifiable Proposition (UNVERIF). A propo-
sition is unverifiable if it cannot be proved with ob-
jective evidence. UNVERIF propositions are typi-
cally opinions, suggestions, judgements, or asser-
tions about what will happen in the future. (See
propositions 13 through 17.) Assertions about the
future are typically unverifiable, because there is
no direct evidence that something will happen. A
very prominent exception is a prediction based on
a policy of organizations, i.e. “The store will be
open this Sunday.” where the policy serves as a di-
rect evidence.
Non-Argumentative (NONARG). A sentence or
clause is in this category if it is not a proposition,
i.e. it cannot be verified with objective evidence
and no supporting reason is required for the pur-
pose of improving the comment quality. Exam-
ples include question, greeting, citation, and URL.
(See sentences 18 through 21.)
</bodyText>
<subsectionHeader confidence="0.999763">
2.2 Annotation Results
</subsectionHeader>
<bodyText confidence="0.995505863636364">
The resulting distribution of classes is shown in
Table 2. Note that even though we employed
a rather lenient definition of objective proposi-
tions, the distribution is highly skewed towards
UNVERIF propositions. This is expected because
the comments are written by people who want to
express their opinions about a regulation. Also,
NONARG sentences comprise about 7% of the
data, suggesting that most comment propositions
need to be supported with a reason or evidence for
maximal credibility.
The inter-coder reliability checked on 30% of
the data is moderate, yielding an Unweighted Co-
hen’s κ of 0.73. Most of the disagreement oc-
curred in propositions like “Airlines have to pro-
vide compensation for both fees and lost bags” in
which it is not clear from the context whether it
is an opinion (UNVERIF) or a law (VERIFNON).
Also, opinions that may be verifiable (e.g. “The
problems with passenger experience are not de-
pendant on aircraft size!”) seem to cause disagree-
ment among annotators.
</bodyText>
<sectionHeader confidence="0.990934" genericHeader="method">
3 Proposition Type Classification
</sectionHeader>
<subsectionHeader confidence="0.999949">
3.1 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.998978">
To classify each proposition in an argument as
VERIFNON, VERIFEXP, or UNVERIF, we train
multiclass Support Vector Machines (SVM) as for-
mulated by Crammer and Singer (2002), and later
extended by Keerthi et al.(2008). We use the Lib-
Linear (Fan et al., 2008) implementation. We ex-
perimented with other multiclass SVM approaches
such as 1-vs-all and 1-vs-1 (all-vs-all), but the dif-
ferences were statistically insignificant, consistent
with Hsu and Lin’s (2002) empirical comparison
of these methods. Thus, we only report the per-
formance of the Crammer and Singer version of
Multiclass SVM.
</bodyText>
<subsectionHeader confidence="0.938784">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.9999838">
The features are binary-valued, and the feature
vector for each data point is normalized to have
the unit length: “Presence” features are binary
features indicating whether the given feature is
present in the proposition or not; “Count” features
</bodyText>
<page confidence="0.99645">
32
</page>
<bodyText confidence="0.999976071428571">
are numeric counts of the occurrence of each fea-
ture is converted to a set of three binary features
each denoting 0, 1 and 2 or more occurrences.
We first tried a binning method with each digit
as its own interval, resulting in binary features of
the form featCntn, but the three-interval approach
proved to be better empirically, and is consistent
with the approach by Riloff and Shoen (1995).
The features can be grouped into three cate-
gories by purpose: Verifiability-specific (VER),
Experientiality-specific (EXP) and Basic Features,
each designed to capture the given proposition’s
verifiability, experientiality, and both, respec-
tively. Now we discuss the features in more detail.
</bodyText>
<subsectionHeader confidence="0.462102">
3.2.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.998962509433962">
N-gram Presence A set of binary features de-
note whether a given unigram or bigram occurs
in the proposition. The intuition is that by ex-
amining the occurrence of words or phrases in
VERIFNON, VERIFEXP, and UNVERIF propo-
sitions, the classes that have close ties to certain
words and phrases can be identified. For instance,
when a proposition contains the word happy, the
proposition tends to be UNVERIF. From this ob-
servation, we can speculate that happy is highly
associated with UNVERIF, and went, VERIFEXP.
n-gram presence, rather than the raw or normal-
ized frequency is chosen for its superior perfor-
mance (O’Keefe and Koprinska, 2009).
Core Clause Tag (CCT) To correctly classify
propositions with main or subordinate clauses that
do not affect the verifiability of the proposition
(e.g. propositions 8 through 10 in Table 1, respec-
tively), it is necessary to distinguish features that
appear in the main clause from those that appear in
the subordinate clause. Thus, we employ an auxil-
iary feature that adds clausal information to other
features by tagging them as either core or acces-
sory clause.
Let’s consider propositions 7, 9 and 10 in Ta-
ble 1: In all three examples, the core clause is ital-
icized. In single clause cases like proposition 7,
the entire proposition is the core clause. However,
for proposition 9, the core clause is the subordi-
nate clause introduced by the main clause, i.e. “I
believe” should be ignored, since the verifiability
of “peanuts do not kill people” is not dependent on
it. It is the opposite for proposition 10: the main
clause “The governor said” is the core clause, and
the rest need not be considered. The reason is that
“said” is a speech event, and it is possible to objec-
tively verify whether or not the governor verbally
expressed his appreciation of peanuts.
To realize this intuition, we use syntactic parse
trees generated by the Stanford Parser (De Marn-
effe et al., 2006). In particular, Penn Treebank
2 Tags contain a clause-level tag SBAR denoting
a “clause introduced by a subordinating conjunc-
tion” (Marcus et al., 1993). The “that” clause in
proposition 10 spans a subtree rooted by SBAR,
whose left-most child has a lexical value “that.”
Similarly, the subordinate (non-italicized) clause
in proposition 9 falls in a subtree rooted by SBAR,
whose only child is S. Once the main clause of a
given proposition is identified, all features set off
by the clause are tagged as “core” and the rest are
tagged as ”accessory.” If a speech event is present,
the tags are flipped.
</bodyText>
<subsubsectionHeader confidence="0.679396">
3.2.2 Verifiability-specific Features (VER)
</subsubsectionHeader>
<bodyText confidence="0.999303333333333">
Parts-of-Speech (POS) Count Rayson et
al. (2001) have shown that the POS distribution
is distinct in imaginative vs. informative writing.
We expect this feature to distinguish UNVERIF
propositions from the rest.
Sentiment Clue Count Wilson et al. (2005) pro-
vides a subjectivity clue lexicon, which is a list of
words with sentiment strength tags, either strong
or weak, along with additional information, such
as the sentiment polarity, Part-of-Speech Count
(POS), etc. We suspect that propositions contain-
ing more sentiment words is more likely to be UN-
VERIF.
Speech Event Count We use the 50 most frequent
Objective-speech-event text anchors crawled from
the MPQA 2.0 corpus (Wilson and Wiebe, 2005)
as a speech event lexicon. The speech event text
anchors refer to words like “stated” and “wrote”
that introduce written or spoken propositions at-
tributed to a source. propositions containing
speech events such as proposition 10 in Table 1
are generally VERIFNON or VERIFEXP, since
whether the attributed source has indeed made the
proposition he allegedly made is objectively veri-
fiable regardless of the subjectivity of the proposi-
tion itself.
Imperative Expression Count Imperatives, i.e.
commands, are generally UNVERIF (e.g. “Do the
homework now!” that is, we expect there to be
no objective evidence proving that the homework
should be done right away.), unless the sentence
is a law or general procedure (e.g. “The library
should allow you to check out books.” where the
</bodyText>
<page confidence="0.996009">
33
</page>
<bodyText confidence="0.999686545454546">
context makes it clear that the writer is claiming
that the library lends out books.) This feature de-
notes whether the proposition begins with a verb
or contains the following: must, should, need to,
have to, ought to.
Emotion Expression Count These features tar-
get specific tokens “!”, and “...” as well as fully
capitalized word tokens to capture the emotion in
text. The rationale is that expression of emotion is
likely to be more prevalent in UNVERIF proposi-
tions.
</bodyText>
<subsectionHeader confidence="0.609165">
3.2.3 Experientiality-specific Features (EXP)
</subsectionHeader>
<bodyText confidence="0.99986652631579">
Tense Count propositions written in past tense
are rarely VERIFNON, because even in the case
that the statment is verifiable, they are likely to be
the commenter’s past experience, i.e. VERIFEXP.
Future tense are typically UNVERIF because
propositions about what will happen in the fu-
ture are often unverifiable with objective evidence,
with exception being propositions like predictions
based on policy of organizations, i.e. “Fedex will
deliver on Sunday.” To take advantage of these ob-
servations, three binary features capture each of
three tenses: past, present, and future.
Person Count First person narratives can suggest
that the proposition is UNVERIF or VERIFEXP,
except for rare cases like “We, the passengers,...”
in which the first person pronoun refers to a large
body of individuals. This intuition is captured by
having binary features for: 1st, 2nd and 3rd per-
son.
</bodyText>
<sectionHeader confidence="0.999894" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.992911">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.98074719047619">
A Note on Argument Detection A natural first
step in argumentation mining is to determine
which portions of the given document comprise
an argument. It can also be framed as a binary
classification task in which each proposition in the
document needs to be classified as either argumen-
tative or not. Some authors choose to skip this
step (Feng and Hirst, 2011), while others make
use of various classifiers to achieve high level of
accuracy, as Palau and Moens achieved over 70%
accuracy on Araucaria and ECHR corpus (Reed
and Moens, 2008; Palau and Moens, 2009).
As we have discussed in Section 1, our setup
is a bit unique in that we also consider implicit
arguments, where propositions are not supported
with explicit reason or evidence, as argumentative.
As a result, only about 7%(NONARG in Table 2) of
TOTAL
our entire dataset is marked as non-argumentative,
most of which consists of questions and greetings.
By simply searching for specific unigrams, such
as “?” and “thank”, we achieve over 99% F1 score
in determining which propositions are part of an
argument.
The remaining experiments were done without
non-argumentative propositions, i.e. NONARG in
Table 2.
Experimental Setup We first randomly selected
292 comments as held-out test set, resulting in the
distribution shown in Table 4. Then, VERIFNON
and VERIFEXP in the training set were oversam-
pled so that the classes are equally distributed.
During training, five fold cross-validation was
done on the training set to tune the C parameter
to 32. Because the micro-averaged F1 score can
be easily boosted on datasets with highly skewed
class distribution, we optimize for the macro-
averaged F1 score.
Preprocessing was kept at a minimal level: cap-
ital letters were lowercased after counting fully
capitalized words, and numbers were converted to
a NUM token.
</bodyText>
<table confidence="0.99701625">
VERIFNON VERIFEXP UNVERIF Total
Train 987 900 4459 6346
Test 370 367 1687 2424
Total 1357 1267 6146 8770
</table>
<tableCaption confidence="0.999544">
Table 4: # of propositions in Train and Test Set
</tableCaption>
<subsectionHeader confidence="0.780853">
4.2 Results &amp; Analysis
</subsectionHeader>
<bodyText confidence="0.9999062">
Table 3 shows a summary of the classification re-
sults. The best overall performance is achieved
by combining all features (UNI+BI+VER+EXP),
yielding 68.99% macro-averaged F1, where the
gain over the baseline is statistically significant
according to the bootstrap method with 10,000
samples (Efron and Tibshirani, 1994; Berg-
Kirkpatrick et al., 2012).
Core Clause Tag (CCT) We do not report the
performance of employing feature sets with Core
Clause Tag (CCT) in Table 3, because the effect
of CCT on each of the six sets of features is sta-
tistically insignificant. This is surprising at first,
given the strong motivation for distinguishing the
core clause from auxiliary clause, as addressed in
the previous section: Subordinate clauses like “I
believe” should not cause the entire proposition to
be classified as UNVERIF, and clauses like “He
said” should serve as a queue for VERIFNON or
VERIFEXP, even if an unverifiable clause follows
</bodyText>
<page confidence="0.998146">
34
</page>
<table confidence="0.999858333333333">
Feature Set UNVERIF vs All VERIFNON vs All VERIFEXP vs All Average F1
Pre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1 Macro Micro
UNI(base) 85.24 79.43 82.23 42.57 51.89 46.77 61.10 66.76 63.80 64.27 73.31
UNI+BI 82.14 89.69* 85.75* 51.67* 37.57 43.51 73.48* 62.67 67.65* 65.63 77.64*
VER 88.52* 52.10 65.60 28.41 61.35* 38.84 42.41 73.02* 53.65 52.70 56.68
EXP 82.42 4.45 8.44 20.92 76.49* 32.85 31.02 82.83* 45.14 28.81 27.31
VER+EXP 89.40* 49.50 63.72 29.25 71.62* 41.54 50.00 79.56* 61.41 55.55 57.43
UNI+BI+ 86.86* 83.05* 84.91* 49.88* 55.14 52.37* 66.67* 73.02* 69.70* 68.99* 77.27*
VER+EXP
</table>
<tableCaption confidence="0.9744098">
Table 3: Three class classification results in % (Crammer &amp; Singer’s Multiclass SVMs)
Precision, recall, and F1 scores are computed with respect to each one-vs-all classification problem for evaluation purposes,
though a single machine is built for the multi-class classification problem, instead of 3 one-vs-all classifiers. The star (*)
indicates that the given result is statistically significantly better than the unigram baseline.
Table 5: Most Informative Features for UNI and UNICCT
</tableCaption>
<bodyText confidence="0.964451142857143">
10 Unigrams with the largest weight (magnitude) with
respect to each class ( + : positive weight / - : negative
weight).
it. Our conjecture turned out to be wrong, mainly
because such distinction can be made for only a
small subset of the data: For instance, over 83%
of the unigrams are tagged as core in the UNI fea-
ture set. Thus, most of the important features for
feature sets with CCT end up being features with
core tag, and the important features for feature sets
with and without CCT are practically the same, as
shown in Table 5, resulting in statistically insignif-
icant performance differences.
Informative Features The most informative fea-
</bodyText>
<table confidence="0.7666918">
Feature Set UNI+BI+VER+EXP
+ should,StrSentClue&gt;2, VB&gt;2
UNVERIF - StrSentClue0, VBD&gt;2, air, since, no one, al-
lergic, not an
+ die, death, reaction, person, allergen, air-
</table>
<tableCaption confidence="0.6900408">
VERIFNON borne, no one, allergies
- PER13t, should
VERIFEXP + VBD&gt;2, PER13t, i have, his, he, him, time !
- VBZ&gt;2, PER2nd
Table 6: Most Informative Features for UNI+BI+VER+EXP
</tableCaption>
<footnote confidence="0.760683666666667">
10 Features with the largest weight (magnitude) with re-
spect to each class ( + : positive weight / - : negative
weight).
</footnote>
<bodyText confidence="0.999661307692308">
tures reported in Table 6 exhibit interesting differ-
ences among the three classes: Sentiment bearing
words, i.e. “should” and strong sentiment clues,
are good indicators of UNVERIF, whereas person
and tense information is crucial for VERIFEXP.
As expected, the strong indicators of UNVERIF
and VERIFEXP, namely “should” and PERist are
negatively associated with VERIFNON. It is in-
triguing to see that the heavily weighted features
of VERIFNON are non-verb content words, unlike
those of the other classes. One explanation for this
is that VERIFNON are rarely indicated by specific
cues; instead, a good sign of VERIFNON is the
absences of cues for the other classes, which are
often function words and verbs. What is remain-
ing, then, are non-verb content words. Also, cer-
tain content words seem to be more likely to bring
about factual discussions. For instance, technical
terms like“allergen” and “airborne,” appear in ver-
ifiable non-experiential propositions as “The FDA
requires labeling for the following 8 allergens.”
Non-n-gram Features Table 3 clearly shows that
the three non-n-gram features, VER, EXP, and
VER+EXP, do not perform as well as the n-gram
features. But still, the performance is impressive,
given the drastic difference in the dimensionality
of the features: Even the combined feature set,
VER+EXP, consists of only about 100 features,
when there are over 8,000 unigrams and close to
70,000 bigrams. In other words, the non-n-gram
features are effectively capturing characteristics
of each class. This is very promising, since this
shows that a better understanding of the types of
proposition can potentially lead to a more concise
set of features with equal, or even better, perfor-
mance.
Also notice that VER outperforms EXP for the
most part, even with respect to VERIFNON vs All
and VERIFEXP vs All, except for recall. This is in-
</bodyText>
<table confidence="0.988033171428572">
UNICCT
shouldC, shouldA,
understandC
exposedC, solveC,
NUMC, floridaC,
reactedC, poolC, owedC
impactedC, solveC,
carsC, NUMC, poolC,
writingC, deathC, linkC
shouldC, commentsC
owedC, consumedC,
expertC, reactedC,
happenedC, interestingC
impactedC, woC,
concernC, diedC
Fts
UNI
UNVERIF
+
- previous, solve, florida,
exposed, reacted, reply,
kinds
should, whatever, respon-
sibility
VERIFNON
+
- should, seems, comments
impacted, NUM, solve,
cars, pull, kinds, congress
VERIFEXP
+
- impacted, wo
owed, consumed, saw, ex-
pert, interesting, him, re-
acted, refinance
</table>
<page confidence="0.998425">
35
</page>
<bodyText confidence="0.9998488">
triguing, because VER are mostly from subjectiv-
ity detection domain, intended to capture the sub-
jectivity of words in the propositions leveraging
on pre-built lexia. Simply considering subjectivity
of words should provide no means of distinguish-
ing VERIFNON from VERIFEXP. One of the rea-
sons for VER’s superior performance over EXP is
that EXP by itself is inadequate for the classifi-
cation task: EXP consists of only 6 (or 12 with
CCT) features denoting the person and tense infor-
mation. Another reason is that VER, in a limited
fashion, does encode experientiality: For instance,
past tense propositions can be identified with the
existence of VBD(verb, past tense) and VBN(verb,
past participle).
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999929358490567">
Argumentation Mining The primary goal of ar-
gumentation mining has been to identify and ex-
tract argumentative structures present in docu-
ments, which are often written by profession-
als (Moens et al., 2007; Wyner et al., 2010; Feng
and Hirst, 2011; Ashley and Walker, 2013). In cer-
tain cases, the specific document structure allows
additional means of identify arguments (Mochales
and Moens, 2008). Even the work on online text
data, which are less rigid in structure and often
contain insufficiently supported propositions, fo-
cus on the extraction of arguments (Villalba and
Saint-Dizier, 2012; Cabrio and Villata, 2012). We,
however, are interested in the assessment of the
argumentative structure, potentially providing rec-
ommendations to readers and feedback to the writ-
ers. Thus it is crucial that we also process unsub-
stantiated propositions, which we consider as im-
plicit arguments. Our approach should be valu-
able for processing documents like online user
comment where arguments may not have adequate
support and an automatic means of analysis can be
useful.
Subjectivity Detection Work to distinguish sub-
jective from objective propositions (e.g.(Wiebe
and Riloff, 2005)), often a subtask for sentiment
analysis (Pang and Lee, 2008), is relevant to our
work since we are concerned with the objective
verifiability of propositions. In particular, previ-
ous work attempts to detect certain types of sub-
jective proposition: Conrad et al. (2012) iden-
tify arguing subjectivity propositions and tag them
with argument labels in order to cluster argument
paraphrases. Others incorporate this task as a com-
ponent for solving related problems, such as an-
swering opinion-based questions and determining
the writer’s political stance (Somasundaran et al.,
2007; Somasundaran and Wiebe, 2010). Similarly,
Rosenthal and McKeown (2012) identify opinion-
ated propositions expressing beliefs, leveraging
from previous work in sentiment analysis and be-
lief tagging. While the class of subjective propo-
sitions in subjectivity detection strictly contains
UNVERIF propositions, it also partially overlaps
with the VERIFEXP and VERIFNON classes of
our work: We want to identify verifiable assertions
within propositions, rather than determine the sub-
jectivity of the proposition as a whole (e.g. propo-
sition 8 in Table 1 is classified as a VERIFNON,
though “Clearly” is subjective.). We also distin-
guish two types of verifiable propositions, which
is necessary for the purpose of identifying appro-
priate types of support.
</bodyText>
<sectionHeader confidence="0.999076" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99997448">
We have proposed a novel task of automatically
classifying each proposition as UNVERIFIABLE,
VERIFIABLE NONEXPERIENTIAL, or VERIFI-
ABLE EXPERIENTIAL, where the appropriate type
of support is reason, evidence, and optional evi-
dence, respectively. This classification, once the
existing support relations among propositions are
identified, can provide an estimate of how well the
arguments are supported. We find that Support
Vector Machines (SVM) classifiers trained with
n-grams and other features to capture the verifi-
ability and experientiality exhibit statistically sig-
nificant improvement over the unigram baseline,
achieving a macro-averaged F1 score of 68.99%.
In the process, we have built a gold-standard
dataset of 9,476 propositions from 1,047 com-
ments submitted to an eRulemaking platform.
One immediate avenue for future work is to in-
corporate the identification of relations among the
propositions in an argument to the system to ana-
lyze the adequacy of the supporting information in
the argument. This, in turn, can be used to recom-
mend comments to readers and provide feedback
to writers so that they can construct better argu-
ments.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998096">
This work was supported in part by NSF grants
IIS-1111176 and IIS–1314778. We thank our
annotators, Pamela Ijeoma Amaechi and Simon
Boehme, as well as the Cornell NLP Group and
the reviewers for helpful comments.
</bodyText>
<page confidence="0.997508">
36
</page>
<sectionHeader confidence="0.988835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999397767857143">
Kevin D. Ashley and Vern R. Walker. 2013. From in-
formation retrieval (ir) to argument retrieval (ar) for
legal cases: Report on a baseline study. In JURIX,
pages 29–38.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statis-
tical significance in nlp. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, EMNLP-CoNLL ’12, pages
995–1005, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Elena Cabrio and Serena Villata. 2012. Combin-
ing textual entailment and argumentation theory for
supporting online debates interactions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 208–212, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca.
2012. Recognizing arguing subjectivity and argu-
ment tags. In Proceedings of the Workshop on
Extra-Propositional Aspects of Meaning in Com-
putational Linguistics, ExProM ’12, pages 80–88,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. J. Mach. Learn. Res., 2:265–292,
March.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In Proc. Intl Conf. on Language Resources and Eval-
uation (LREC, pages 449–454.
B. Efron and R.J. Tibshirani. 1994. An Introduction to
the Bootstrap. Chapman &amp; Hall/CRC Monographs
on Statistics &amp; Applied Probability. Taylor &amp; Fran-
cis.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871–1874, June.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ’11, pages 987–996, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jennifer L. Hochschild and Michael Danielson, 1998.
Can We Desegregate Public Schools and Subsi-
dized Housing? Lessons from the Sorry History of
Yonkers, New York, chapter 2, pages 23–44. Uni-
versity Press of Kansas, Lawrence KS, edited by
clarence stone edition.
Chih-Wei Hsu and Chih-Jen Lin. 2002. A comparison
of methods for multiclass support vector machines.
Trans. Neur. Netw., 13(2):415–425, March.
S. Sathiya Keerthi, S. Sundararajan, Kai-Wei Chang,
Cho-Jui Hsieh, and Chih-Jen Lin. 2008. A sequen-
tial dual method for large scale multi-class linear
svms. In Proceedings of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, KDD ’08, pages 408–416, New York,
NY, USA. ACM.
Jeffrey S. Lubbers. 2006. A Guide to Federal Agency
Rulemaking. American Bar Association Chicago,
4th ed. edition.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313–330.
Raquel Mochales and Marie-Francine Moens. 2008.
Study on the structure of argumentation in case law.
In Proceedings of the 2008 Conference on Legal
Knowledge and Information Systems: JURIX 2008:
The Twenty-First Annual Conference, pages 11–20,
Amsterdam, The Netherlands, The Netherlands. IOS
Press.
Marie-Francine Moens, Erik Boiy, Raquel Mochales
Palau, and Chris Reed. 2007. Automatic detection
of arguments in legal texts. In Proceedings of the
11th International Conference on Artificial Intelli-
gence and Law, ICAIL ’07, pages 225–230, New
York, NY, USA. ACM.
Tim O’Keefe and Irena Koprinska. 2009. Feature se-
lection and weighting methods in sentiment analy-
sis. In Proceedings of the 14th Australasian Docu-
ment Computing Symposium.
Raquel Mochales Palau and Marie-Francine Moens.
2009. Argumentation mining: The detection, classi-
fication and structure of arguments in text. In Pro-
ceedings of the 12th International Conference on Ar-
tificial Intelligence and Law, ICAIL ’09, pages 98–
107, New York, NY, USA. ACM.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135, January.
Joonsuk Park, Sally Klingel, Claire Cardie, Mary
Newhart, Cynthia Farina, and Joan-Josep Vallb´e.
2012. Facilitative moderation for online participa-
tion in erulemaking. In Proceedings of the 13th An-
nual International Conference on Digital Govern-
ment Research, dg.o ’12, pages 173–182, New York,
NY, USA. ACM.
John L. Pollock. 1987. Defeasible reasoning. Cogni-
tive Science, 11:481–518.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within the
british national corpus sampler. Language and Com-
puters.
</reference>
<page confidence="0.991102">
37
</page>
<reference confidence="0.999736282608696">
Raquel Mochales Palau Rowe Glenn Reed, Chris and
Marie-Francine Moens. 2008. Language resources
for studying argument. In Proceedings of the 6th
conference on language resources and evaluation -
LREC 2008, pages 91–100. ELRA.
Ellen Riloff and Jay Shoen. 1995. Automatically
acquiring conceptual patterns without an annotated
corpus. In In Proceedings of the Third Workshop on
Very Large Corpora, pages 148–161.
Sara Rosenthal and Kathleen McKeown. 2012. De-
tecting opinionated claims in online discussions. In
ICSC, pages 30–37.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ’10, pages 116–
124, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the SIGdial Workshop
on Discourse and Dialogue.
Stephen E. Toulmin, Richard Rieke, and Allan Janik.
1979. An Introduction to Reasoning. Macmillan
Publishing Company.
S.E. Toulmin. 1958. The Uses of Argument. Cam-
bridge University Press.
Maria Paz Garcia Villalba and Patrick Saint-Dizier.
2012. Some facets of argument mining for opinion
analysis. In COMMA, pages 23–34.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In In CICLing2005, pages 486–497.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
ACL Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky.
Theresa Wilson. 2005. Recognizing contextual po-
larity in phrase-level sentiment analysis. In In Pro-
ceedings of HLT-EMNLP, pages 347–354.
Adam Wyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Semantic pro-
cessing of legal texts. chapter Approaches to Text
Mining Arguments from Legal Cases, pages 60–79.
Springer-Verlag, Berlin, Heidelberg.
</reference>
<page confidence="0.999353">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445075">
<title confidence="0.9929395">Identifying Appropriate Support for Propositions in Online User Comments</title>
<author confidence="0.9569">Joonsuk</author>
<affiliation confidence="0.9658925">Department of Computer Cornell</affiliation>
<address confidence="0.762276">Ithaca, NY,</address>
<email confidence="0.999816">jpark@cs.cornell.edu</email>
<abstract confidence="0.986836384615384">The ability to analyze the adequacy of supporting information is necessary for deterthe strength of an This is especially the case for online user comments, which often consist of arguments lacking proper substantiation and reasoning. Thus, we develop a framework for automatically classifying each proposition or where the appropriate type of is and Once the existing support for propositions are identified, this classification can provide an estimate of how adequately the arguments have been supported. We build a goldstandard dataset of 9,476 sentences and clauses from 1,047 comments submitted to an eRulemaking platform and find that Support Vector Machine (SVM) classifiers trained with n-grams and additional features capturing the verifiability and experientiality exhibit statistically significant improvement over the unigram baseline, a macro-averaged 68.99%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kevin D Ashley</author>
<author>Vern R Walker</author>
</authors>
<title>From information retrieval (ir) to argument retrieval (ar) for legal cases: Report on a baseline study.</title>
<date>2013</date>
<booktitle>In JURIX,</booktitle>
<pages>29--38</pages>
<contexts>
<context position="2499" citStr="Ashley and Walker, 2013" startWordPosition="365" endWordPosition="368">d of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole argument. Consider the following examples: How </context>
<context position="31120" citStr="Ashley and Walker, 2013" startWordPosition="4946" endWordPosition="4949">equate for the classification task: EXP consists of only 6 (or 12 with CCT) features denoting the person and tense information. Another reason is that VER, in a limited fashion, does encode experientiality: For instance, past tense propositions can be identified with the existence of VBD(verb, past tense) and VBN(verb, past participle). 5 Related Work Argumentation Mining The primary goal of argumentation mining has been to identify and extract argumentative structures present in documents, which are often written by professionals (Moens et al., 2007; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). In certain cases, the specific document structure allows additional means of identify arguments (Mochales and Moens, 2008). Even the work on online text data, which are less rigid in structure and often contain insufficiently supported propositions, focus on the extraction of arguments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubstantiated propositions, which we consider as imp</context>
</contexts>
<marker>Ashley, Walker, 2013</marker>
<rawString>Kevin D. Ashley and Vern R. Walker. 2013. From information retrieval (ir) to argument retrieval (ar) for legal cases: Report on a baseline study. In JURIX, pages 29–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>An empirical investigation of statistical significance in nlp.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>995--1005</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Berg-Kirkpatrick, Burkett, Klein, 2012</marker>
<rawString>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical significance in nlp. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 995–1005, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Cabrio</author>
<author>Serena Villata</author>
</authors>
<title>Combining textual entailment and argumentation theory for supporting online debates interactions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>208--212</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2826" citStr="Cabrio and Villata, 2012" startWordPosition="413" endWordPosition="416">articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole argument. Consider the following examples: How much does a small carton of milk cost?1 More children should drink milk2, because children who drink milk everyday are taller than those who don’t3. Children would want to drink milk, anyway4. Firstly, Sentence 1 does not need any support, nor is it part of an argument. Next, Proposition 2 is an unverifiable proposition becau</context>
<context position="31468" citStr="Cabrio and Villata, 2012" startWordPosition="4998" endWordPosition="5001">d Work Argumentation Mining The primary goal of argumentation mining has been to identify and extract argumentative structures present in documents, which are often written by professionals (Moens et al., 2007; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). In certain cases, the specific document structure allows additional means of identify arguments (Mochales and Moens, 2008). Even the work on online text data, which are less rigid in structure and often contain insufficiently supported propositions, focus on the extraction of arguments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubstantiated propositions, which we consider as implicit arguments. Our approach should be valuable for processing documents like online user comment where arguments may not have adequate support and an automatic means of analysis can be useful. Subjectivity Detection Work to distinguish subjective from objective propositions (e.g.(Wiebe and Riloff, 2005)), often a subtask for sentiment analysis </context>
</contexts>
<marker>Cabrio, Villata, 2012</marker>
<rawString>Elena Cabrio and Serena Villata. 2012. Combining textual entailment and argumentation theory for supporting online debates interactions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 208–212, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Conrad</author>
<author>Janyce Wiebe</author>
<author>Hwa</author>
<author>Rebecca</author>
</authors>
<title>Recognizing arguing subjectivity and argument tags.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, ExProM ’12,</booktitle>
<pages>80--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32296" citStr="Conrad et al. (2012)" startWordPosition="5125" endWordPosition="5128">iated propositions, which we consider as implicit arguments. Our approach should be valuable for processing documents like online user comment where arguments may not have adequate support and an automatic means of analysis can be useful. Subjectivity Detection Work to distinguish subjective from objective propositions (e.g.(Wiebe and Riloff, 2005)), often a subtask for sentiment analysis (Pang and Lee, 2008), is relevant to our work since we are concerned with the objective verifiability of propositions. In particular, previous work attempts to detect certain types of subjective proposition: Conrad et al. (2012) identify arguing subjectivity propositions and tag them with argument labels in order to cluster argument paraphrases. Others incorporate this task as a component for solving related problems, such as answering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Similarly, Rosenthal and McKeown (2012) identify opinionated propositions expressing beliefs, leveraging from previous work in sentiment analysis and belief tagging. While the class of subjective propositions in subjectivity detection strictly contains UNVERI</context>
</contexts>
<marker>Conrad, Wiebe, Hwa, Rebecca, 2012</marker>
<rawString>Alexander Conrad, Janyce Wiebe, Hwa, and Rebecca. 2012. Recognizing arguing subjectivity and argument tags. In Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, ExProM ’12, pages 80–88, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernel-based vector machines.</title>
<date>2002</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>2--265</pages>
<contexts>
<context position="15710" citStr="Crammer and Singer (2002)" startWordPosition="2468" endWordPosition="2471">of 0.73. Most of the disagreement occurred in propositions like “Airlines have to provide compensation for both fees and lost bags” in which it is not clear from the context whether it is an opinion (UNVERIF) or a law (VERIFNON). Also, opinions that may be verifiable (e.g. “The problems with passenger experience are not dependant on aircraft size!”) seem to cause disagreement among annotators. 3 Proposition Type Classification 3.1 Learning Algorithm To classify each proposition in an argument as VERIFNON, VERIFEXP, or UNVERIF, we train multiclass Support Vector Machines (SVM) as formulated by Crammer and Singer (2002), and later extended by Keerthi et al.(2008). We use the LibLinear (Fan et al., 2008) implementation. We experimented with other multiclass SVM approaches such as 1-vs-all and 1-vs-1 (all-vs-all), but the differences were statistically insignificant, consistent with Hsu and Lin’s (2002) empirical comparison of these methods. Thus, we only report the performance of the Crammer and Singer version of Multiclass SVM. 3.2 Features The features are binary-valued, and the feature vector for each data point is normalized to have the unit length: “Presence” features are binary features indicating wheth</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector machines. J. Mach. Learn. Res., 2:265–292, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill Maccartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses. In</title>
<date>2006</date>
<booktitle>In Proc. Intl Conf. on Language Resources and Evaluation (LREC,</booktitle>
<pages>449--454</pages>
<marker>De Marneffe, Maccartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill Maccartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In In Proc. Intl Conf. on Language Resources and Evaluation (LREC, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman</title>
<date>1994</date>
<booktitle>Hall/CRC Monographs on Statistics &amp; Applied Probability.</booktitle>
<publisher>Taylor &amp; Francis.</publisher>
<contexts>
<context position="24900" citStr="Efron and Tibshirani, 1994" startWordPosition="3954" endWordPosition="3957"> minimal level: capital letters were lowercased after counting fully capitalized words, and numbers were converted to a NUM token. VERIFNON VERIFEXP UNVERIF Total Train 987 900 4459 6346 Test 370 367 1687 2424 Total 1357 1267 6146 8770 Table 4: # of propositions in Train and Test Set 4.2 Results &amp; Analysis Table 3 shows a summary of the classification results. The best overall performance is achieved by combining all features (UNI+BI+VER+EXP), yielding 68.99% macro-averaged F1, where the gain over the baseline is statistically significant according to the bootstrap method with 10,000 samples (Efron and Tibshirani, 1994; BergKirkpatrick et al., 2012). Core Clause Tag (CCT) We do not report the performance of employing feature sets with Core Clause Tag (CCT) in Table 3, because the effect of CCT on each of the six sets of features is statistically insignificant. This is surprising at first, given the strong motivation for distinguishing the core clause from auxiliary clause, as addressed in the previous section: Subordinate clauses like “I believe” should not cause the entire proposition to be classified as UNVERIF, and clauses like “He said” should serve as a queue for VERIFNON or VERIFEXP, even if an unveri</context>
</contexts>
<marker>Efron, Tibshirani, 1994</marker>
<rawString>B. Efron and R.J. Tibshirani. 1994. An Introduction to the Bootstrap. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability. Taylor &amp; Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>9--1871</pages>
<contexts>
<context position="15795" citStr="Fan et al., 2008" startWordPosition="2484" endWordPosition="2487">pensation for both fees and lost bags” in which it is not clear from the context whether it is an opinion (UNVERIF) or a law (VERIFNON). Also, opinions that may be verifiable (e.g. “The problems with passenger experience are not dependant on aircraft size!”) seem to cause disagreement among annotators. 3 Proposition Type Classification 3.1 Learning Algorithm To classify each proposition in an argument as VERIFNON, VERIFEXP, or UNVERIF, we train multiclass Support Vector Machines (SVM) as formulated by Crammer and Singer (2002), and later extended by Keerthi et al.(2008). We use the LibLinear (Fan et al., 2008) implementation. We experimented with other multiclass SVM approaches such as 1-vs-all and 1-vs-1 (all-vs-all), but the differences were statistically insignificant, consistent with Hsu and Lin’s (2002) empirical comparison of these methods. Thus, we only report the performance of the Crammer and Singer version of Multiclass SVM. 3.2 Features The features are binary-valued, and the feature vector for each data point is normalized to have the unit length: “Presence” features are binary features indicating whether the given feature is present in the proposition or not; “Count” features 32 are nu</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. J. Mach. Learn. Res., 9:1871–1874, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Classifying arguments by scheme.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>987--996</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2473" citStr="Feng and Hirst, 2011" startWordPosition="361" endWordPosition="364">hat eliminates the need of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole argument. Consider th</context>
<context position="22939" citStr="Feng and Hirst, 2011" startWordPosition="3639" endWordPosition="3642">s UNVERIF or VERIFEXP, except for rare cases like “We, the passengers,...” in which the first person pronoun refers to a large body of individuals. This intuition is captured by having binary features for: 1st, 2nd and 3rd person. 4 Experiments 4.1 Methodology A Note on Argument Detection A natural first step in argumentation mining is to determine which portions of the given document comprise an argument. It can also be framed as a binary classification task in which each proposition in the document needs to be classified as either argumentative or not. Some authors choose to skip this step (Feng and Hirst, 2011), while others make use of various classifiers to achieve high level of accuracy, as Palau and Moens achieved over 70% accuracy on Araucaria and ECHR corpus (Reed and Moens, 2008; Palau and Moens, 2009). As we have discussed in Section 1, our setup is a bit unique in that we also consider implicit arguments, where propositions are not supported with explicit reason or evidence, as argumentative. As a result, only about 7%(NONARG in Table 2) of TOTAL our entire dataset is marked as non-argumentative, most of which consists of questions and greetings. By simply searching for specific unigrams, s</context>
<context position="31094" citStr="Feng and Hirst, 2011" startWordPosition="4942" endWordPosition="4945"> EXP by itself is inadequate for the classification task: EXP consists of only 6 (or 12 with CCT) features denoting the person and tense information. Another reason is that VER, in a limited fashion, does encode experientiality: For instance, past tense propositions can be identified with the existence of VBD(verb, past tense) and VBN(verb, past participle). 5 Related Work Argumentation Mining The primary goal of argumentation mining has been to identify and extract argumentative structures present in documents, which are often written by professionals (Moens et al., 2007; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). In certain cases, the specific document structure allows additional means of identify arguments (Mochales and Moens, 2008). Even the work on online text data, which are less rigid in structure and often contain insufficiently supported propositions, focus on the extraction of arguments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubstantiated propositions</context>
</contexts>
<marker>Feng, Hirst, 2011</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2011. Classifying arguments by scheme. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 987–996, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer L Hochschild</author>
<author>Michael Danielson</author>
</authors>
<title>Can We Desegregate Public Schools and Subsidized Housing? Lessons from the Sorry History of Yonkers,</title>
<date>1998</date>
<volume>2</volume>
<pages>23--44</pages>
<publisher>University Press</publisher>
<location>New York,</location>
<contexts>
<context position="6607" citStr="Hochschild and Danielson, 1998" startWordPosition="1020" endWordPosition="1023"> moves the process to online platforms (see, e.g. (Park et al., 2012)). By providing platforms in which the public can discuss regulations that interest them, government 5http://www.regulationroom.org agencies hope to enlist the expertise and experience of participants to create better regulations. In many rulemaking scenarios, agencies are, in fact, required to obtain feedback from the public on the proposed regulation as well as to address all substantive questions, criticisms or suggestions that are raised (Lubbers, 2006). In this way, public comments can produce changes in the final rule (Hochschild and Danielson, 1998) that, in turn, can affect millions of lives. It is crucial, therefore, for rule makers to be able to identify credible comments from those submitted. Regulation Room is an experimental website operated by Cornell eRulemaking Initiative (CeRI)6 to promote public participation in the rulemaking process, help users write more informative comments and build collective knowledge via active discussions guided by human moderators. Regulation Room hosts actual regulations from government agencies, such as the U.S. Department of Transportation. For our research, we collected and manually annotated 9,4</context>
</contexts>
<marker>Hochschild, Danielson, 1998</marker>
<rawString>Jennifer L. Hochschild and Michael Danielson, 1998. Can We Desegregate Public Schools and Subsidized Housing? Lessons from the Sorry History of Yonkers, New York, chapter 2, pages 23–44. University Press of Kansas, Lawrence KS, edited by clarence stone edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Wei Hsu</author>
<author>Chih-Jen Lin</author>
</authors>
<title>A comparison of methods for multiclass support vector machines.</title>
<date>2002</date>
<journal>Trans. Neur. Netw.,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Hsu, Lin, 2002</marker>
<rawString>Chih-Wei Hsu and Chih-Jen Lin. 2002. A comparison of methods for multiclass support vector machines. Trans. Neur. Netw., 13(2):415–425, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sathiya Keerthi</author>
<author>S Sundararajan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Chih-Jen Lin</author>
</authors>
<title>A sequential dual method for large scale multi-class linear svms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08,</booktitle>
<pages>408--416</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Keerthi, Sundararajan, Chang, Hsieh, Lin, 2008</marker>
<rawString>S. Sathiya Keerthi, S. Sundararajan, Kai-Wei Chang, Cho-Jui Hsieh, and Chih-Jen Lin. 2008. A sequential dual method for large scale multi-class linear svms. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08, pages 408–416, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey S Lubbers</author>
</authors>
<title>A Guide to Federal Agency Rulemaking.</title>
<date>2006</date>
<publisher>American Bar Association</publisher>
<note>Chicago, 4th ed. edition.</note>
<contexts>
<context position="6506" citStr="Lubbers, 2006" startWordPosition="1006" endWordPosition="1007">ake new regulations and enact public policy; its digital counterpart — eRulemaking — moves the process to online platforms (see, e.g. (Park et al., 2012)). By providing platforms in which the public can discuss regulations that interest them, government 5http://www.regulationroom.org agencies hope to enlist the expertise and experience of participants to create better regulations. In many rulemaking scenarios, agencies are, in fact, required to obtain feedback from the public on the proposed regulation as well as to address all substantive questions, criticisms or suggestions that are raised (Lubbers, 2006). In this way, public comments can produce changes in the final rule (Hochschild and Danielson, 1998) that, in turn, can affect millions of lives. It is crucial, therefore, for rule makers to be able to identify credible comments from those submitted. Regulation Room is an experimental website operated by Cornell eRulemaking Initiative (CeRI)6 to promote public participation in the rulemaking process, help users write more informative comments and build collective knowledge via active discussions guided by human moderators. Regulation Room hosts actual regulations from government agencies, suc</context>
</contexts>
<marker>Lubbers, 2006</marker>
<rawString>Jeffrey S. Lubbers. 2006. A Guide to Federal Agency Rulemaking. American Bar Association Chicago, 4th ed. edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>COMPUTATIONAL LINGUISTICS,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="19172" citStr="Marcus et al., 1993" startWordPosition="3035" endWordPosition="3038">y of “peanuts do not kill people” is not dependent on it. It is the opposite for proposition 10: the main clause “The governor said” is the core clause, and the rest need not be considered. The reason is that “said” is a speech event, and it is possible to objectively verify whether or not the governor verbally expressed his appreciation of peanuts. To realize this intuition, we use syntactic parse trees generated by the Stanford Parser (De Marneffe et al., 2006). In particular, Penn Treebank 2 Tags contain a clause-level tag SBAR denoting a “clause introduced by a subordinating conjunction” (Marcus et al., 1993). The “that” clause in proposition 10 spans a subtree rooted by SBAR, whose left-most child has a lexical value “that.” Similarly, the subordinate (non-italicized) clause in proposition 9 falls in a subtree rooted by SBAR, whose only child is S. Once the main clause of a given proposition is identified, all features set off by the clause are tagged as “core” and the rest are tagged as ”accessory.” If a speech event is present, the tags are flipped. 3.2.2 Verifiability-specific Features (VER) Parts-of-Speech (POS) Count Rayson et al. (2001) have shown that the POS distribution is distinct in im</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. COMPUTATIONAL LINGUISTICS, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Study on the structure of argumentation in case law.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Legal Knowledge and Information Systems: JURIX 2008: The Twenty-First Annual Conference,</booktitle>
<pages>11--20</pages>
<publisher>IOS Press.</publisher>
<location>Amsterdam, The</location>
<contexts>
<context position="31244" citStr="Mochales and Moens, 2008" startWordPosition="4964" endWordPosition="4967">ion. Another reason is that VER, in a limited fashion, does encode experientiality: For instance, past tense propositions can be identified with the existence of VBD(verb, past tense) and VBN(verb, past participle). 5 Related Work Argumentation Mining The primary goal of argumentation mining has been to identify and extract argumentative structures present in documents, which are often written by professionals (Moens et al., 2007; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). In certain cases, the specific document structure allows additional means of identify arguments (Mochales and Moens, 2008). Even the work on online text data, which are less rigid in structure and often contain insufficiently supported propositions, focus on the extraction of arguments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubstantiated propositions, which we consider as implicit arguments. Our approach should be valuable for processing documents like online user comment where arguments may not h</context>
</contexts>
<marker>Mochales, Moens, 2008</marker>
<rawString>Raquel Mochales and Marie-Francine Moens. 2008. Study on the structure of argumentation in case law. In Proceedings of the 2008 Conference on Legal Knowledge and Information Systems: JURIX 2008: The Twenty-First Annual Conference, pages 11–20, Amsterdam, The Netherlands, The Netherlands. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
<author>Erik Boiy</author>
<author>Raquel Mochales Palau</author>
<author>Chris Reed</author>
</authors>
<title>Automatic detection of arguments in legal texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Artificial Intelligence and Law, ICAIL ’07,</booktitle>
<pages>225--230</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2408" citStr="Moens et al., 2007" startWordPosition="349" endWordPosition="352">mples. 3We are assuming that there is no background knowledge that eliminates the need of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting informat</context>
<context position="31052" citStr="Moens et al., 2007" startWordPosition="4934" endWordPosition="4937">’s superior performance over EXP is that EXP by itself is inadequate for the classification task: EXP consists of only 6 (or 12 with CCT) features denoting the person and tense information. Another reason is that VER, in a limited fashion, does encode experientiality: For instance, past tense propositions can be identified with the existence of VBD(verb, past tense) and VBN(verb, past participle). 5 Related Work Argumentation Mining The primary goal of argumentation mining has been to identify and extract argumentative structures present in documents, which are often written by professionals (Moens et al., 2007; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). In certain cases, the specific document structure allows additional means of identify arguments (Mochales and Moens, 2008). Even the work on online text data, which are less rigid in structure and often contain insufficiently supported propositions, focus on the extraction of arguments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we</context>
</contexts>
<marker>Moens, Boiy, Palau, Reed, 2007</marker>
<rawString>Marie-Francine Moens, Erik Boiy, Raquel Mochales Palau, and Chris Reed. 2007. Automatic detection of arguments in legal texts. In Proceedings of the 11th International Conference on Artificial Intelligence and Law, ICAIL ’07, pages 225–230, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim O’Keefe</author>
<author>Irena Koprinska</author>
</authors>
<title>Feature selection and weighting methods in sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 14th Australasian Document Computing Symposium.</booktitle>
<marker>O’Keefe, Koprinska, 2009</marker>
<rawString>Tim O’Keefe and Irena Koprinska. 2009. Feature selection and weighting methods in sentiment analysis. In Proceedings of the 14th Australasian Document Computing Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales Palau</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Argumentation mining: The detection, classification and structure of arguments in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th International Conference on Artificial Intelligence and Law, ICAIL ’09,</booktitle>
<pages>98--107</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2431" citStr="Palau and Moens, 2009" startWordPosition="353" endWordPosition="356">ing that there is no background knowledge that eliminates the need of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the s</context>
<context position="23141" citStr="Palau and Moens, 2009" startWordPosition="3673" endWordPosition="3676">for: 1st, 2nd and 3rd person. 4 Experiments 4.1 Methodology A Note on Argument Detection A natural first step in argumentation mining is to determine which portions of the given document comprise an argument. It can also be framed as a binary classification task in which each proposition in the document needs to be classified as either argumentative or not. Some authors choose to skip this step (Feng and Hirst, 2011), while others make use of various classifiers to achieve high level of accuracy, as Palau and Moens achieved over 70% accuracy on Araucaria and ECHR corpus (Reed and Moens, 2008; Palau and Moens, 2009). As we have discussed in Section 1, our setup is a bit unique in that we also consider implicit arguments, where propositions are not supported with explicit reason or evidence, as argumentative. As a result, only about 7%(NONARG in Table 2) of TOTAL our entire dataset is marked as non-argumentative, most of which consists of questions and greetings. By simply searching for specific unigrams, such as “?” and “thank”, we achieve over 99% F1 score in determining which propositions are part of an argument. The remaining experiments were done without non-argumentative propositions, i.e. NONARG in</context>
</contexts>
<marker>Palau, Moens, 2009</marker>
<rawString>Raquel Mochales Palau and Marie-Francine Moens. 2009. Argumentation mining: The detection, classification and structure of arguments in text. In Proceedings of the 12th International Conference on Artificial Intelligence and Law, ICAIL ’09, pages 98– 107, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="32088" citStr="Pang and Lee, 2008" startWordPosition="5092" endWordPosition="5095"> We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubstantiated propositions, which we consider as implicit arguments. Our approach should be valuable for processing documents like online user comment where arguments may not have adequate support and an automatic means of analysis can be useful. Subjectivity Detection Work to distinguish subjective from objective propositions (e.g.(Wiebe and Riloff, 2005)), often a subtask for sentiment analysis (Pang and Lee, 2008), is relevant to our work since we are concerned with the objective verifiability of propositions. In particular, previous work attempts to detect certain types of subjective proposition: Conrad et al. (2012) identify arguing subjectivity propositions and tag them with argument labels in order to cluster argument paraphrases. Others incorporate this task as a component for solving related problems, such as answering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Similarly, Rosenthal and McKeown (2012) identify op</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Sally Klingel</author>
<author>Claire Cardie</author>
<author>Mary Newhart</author>
<author>Cynthia Farina</author>
<author>Joan-Josep Vallb´e</author>
</authors>
<title>Facilitative moderation for online participation in erulemaking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual International Conference on Digital Government Research, dg.o ’12,</booktitle>
<pages>173--182</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Park, Klingel, Cardie, Newhart, Farina, Vallb´e, 2012</marker>
<rawString>Joonsuk Park, Sally Klingel, Claire Cardie, Mary Newhart, Cynthia Farina, and Joan-Josep Vallb´e. 2012. Facilitative moderation for online participation in erulemaking. In Proceedings of the 13th Annual International Conference on Digital Government Research, dg.o ’12, pages 173–182, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John L Pollock</author>
</authors>
<title>Defeasible reasoning.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<pages>11--481</pages>
<contexts>
<context position="2108" citStr="Pollock, 1987" startWordPosition="305" endWordPosition="306">argument. Not disregarding such implicit arguments allows us to discuss the types of support that can further be provided to strengthen the argument, as a form of assessment. 2Verifiable Experiential propositions are verifiable propositions about personal state or experience. See Table 1 for examples. 3We are assuming that there is no background knowledge that eliminates the need of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them a</context>
</contexts>
<marker>Pollock, 1987</marker>
<rawString>John L. Pollock. 1987. Defeasible reasoning. Cognitive Science, 11:481–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
<author>Andrew Wilson</author>
<author>Geoffrey Leech</author>
</authors>
<title>Grammatical word class variation within the british national corpus sampler. Language and Computers.</title>
<date>2001</date>
<contexts>
<context position="19717" citStr="Rayson et al. (2001)" startWordPosition="3124" endWordPosition="3127"> a “clause introduced by a subordinating conjunction” (Marcus et al., 1993). The “that” clause in proposition 10 spans a subtree rooted by SBAR, whose left-most child has a lexical value “that.” Similarly, the subordinate (non-italicized) clause in proposition 9 falls in a subtree rooted by SBAR, whose only child is S. Once the main clause of a given proposition is identified, all features set off by the clause are tagged as “core” and the rest are tagged as ”accessory.” If a speech event is present, the tags are flipped. 3.2.2 Verifiability-specific Features (VER) Parts-of-Speech (POS) Count Rayson et al. (2001) have shown that the POS distribution is distinct in imaginative vs. informative writing. We expect this feature to distinguish UNVERIF propositions from the rest. Sentiment Clue Count Wilson et al. (2005) provides a subjectivity clue lexicon, which is a list of words with sentiment strength tags, either strong or weak, along with additional information, such as the sentiment polarity, Part-of-Speech Count (POS), etc. We suspect that propositions containing more sentiment words is more likely to be UNVERIF. Speech Event Count We use the 50 most frequent Objective-speech-event text anchors craw</context>
</contexts>
<marker>Rayson, Wilson, Leech, 2001</marker>
<rawString>Paul Rayson, Andrew Wilson, and Geoffrey Leech. 2001. Grammatical word class variation within the british national corpus sampler. Language and Computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raquel Mochales Palau Rowe Glenn Reed</author>
<author>Chris</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Language resources for studying argument.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th conference on language resources and evaluation -LREC</booktitle>
<pages>91--100</pages>
<publisher>ELRA.</publisher>
<marker>Reed, Chris, Moens, 2008</marker>
<rawString>Raquel Mochales Palau Rowe Glenn Reed, Chris and Marie-Francine Moens. 2008. Language resources for studying argument. In Proceedings of the 6th conference on language resources and evaluation -LREC 2008, pages 91–100. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jay Shoen</author>
</authors>
<title>Automatically acquiring conceptual patterns without an annotated corpus. In</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>148--161</pages>
<contexts>
<context position="16782" citStr="Riloff and Shoen (1995)" startWordPosition="2643" endWordPosition="2646">nary-valued, and the feature vector for each data point is normalized to have the unit length: “Presence” features are binary features indicating whether the given feature is present in the proposition or not; “Count” features 32 are numeric counts of the occurrence of each feature is converted to a set of three binary features each denoting 0, 1 and 2 or more occurrences. We first tried a binning method with each digit as its own interval, resulting in binary features of the form featCntn, but the three-interval approach proved to be better empirically, and is consistent with the approach by Riloff and Shoen (1995). The features can be grouped into three categories by purpose: Verifiability-specific (VER), Experientiality-specific (EXP) and Basic Features, each designed to capture the given proposition’s verifiability, experientiality, and both, respectively. Now we discuss the features in more detail. 3.2.1 Basic Features N-gram Presence A set of binary features denote whether a given unigram or bigram occurs in the proposition. The intuition is that by examining the occurrence of words or phrases in VERIFNON, VERIFEXP, and UNVERIF propositions, the classes that have close ties to certain words and phr</context>
</contexts>
<marker>Riloff, Shoen, 1995</marker>
<rawString>Ellen Riloff and Jay Shoen. 1995. Automatically acquiring conceptual patterns without an annotated corpus. In In Proceedings of the Third Workshop on Very Large Corpora, pages 148–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Detecting opinionated claims in online discussions.</title>
<date>2012</date>
<booktitle>In ICSC,</booktitle>
<pages>30--37</pages>
<contexts>
<context position="32676" citStr="Rosenthal and McKeown (2012)" startWordPosition="5179" endWordPosition="5182"> sentiment analysis (Pang and Lee, 2008), is relevant to our work since we are concerned with the objective verifiability of propositions. In particular, previous work attempts to detect certain types of subjective proposition: Conrad et al. (2012) identify arguing subjectivity propositions and tag them with argument labels in order to cluster argument paraphrases. Others incorporate this task as a component for solving related problems, such as answering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Similarly, Rosenthal and McKeown (2012) identify opinionated propositions expressing beliefs, leveraging from previous work in sentiment analysis and belief tagging. While the class of subjective propositions in subjectivity detection strictly contains UNVERIF propositions, it also partially overlaps with the VERIFEXP and VERIFNON classes of our work: We want to identify verifiable assertions within propositions, rather than determine the subjectivity of the proposition as a whole (e.g. proposition 8 in Table 1 is classified as a VERIFNON, though “Clearly” is subjective.). We also distinguish two types of verifiable propositions, w</context>
</contexts>
<marker>Rosenthal, McKeown, 2012</marker>
<rawString>Sara Rosenthal and Kathleen McKeown. 2012. Detecting opinionated claims in online discussions. In ICSC, pages 30–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10,</booktitle>
<pages>116--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32635" citStr="Somasundaran and Wiebe, 2010" startWordPosition="5174" endWordPosition="5177">be and Riloff, 2005)), often a subtask for sentiment analysis (Pang and Lee, 2008), is relevant to our work since we are concerned with the objective verifiability of propositions. In particular, previous work attempts to detect certain types of subjective proposition: Conrad et al. (2012) identify arguing subjectivity propositions and tag them with argument labels in order to cluster argument paraphrases. Others incorporate this task as a component for solving related problems, such as answering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Similarly, Rosenthal and McKeown (2012) identify opinionated propositions expressing beliefs, leveraging from previous work in sentiment analysis and belief tagging. While the class of subjective propositions in subjectivity detection strictly contains UNVERIF propositions, it also partially overlaps with the VERIFEXP and VERIFNON classes of our work: We want to identify verifiable assertions within propositions, rather than determine the subjectivity of the proposition as a whole (e.g. proposition 8 in Table 1 is classified as a VERIFNON, though “Clearly” is subjective.). We also distinguis</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10, pages 116– 124, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Josef Ruppenhofer</author>
<author>Janyce Wiebe</author>
</authors>
<title>Detecting arguing and sentiment in meetings.</title>
<date>2007</date>
<booktitle>In Proceedings of the SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="32604" citStr="Somasundaran et al., 2007" startWordPosition="5170" endWordPosition="5173">tive propositions (e.g.(Wiebe and Riloff, 2005)), often a subtask for sentiment analysis (Pang and Lee, 2008), is relevant to our work since we are concerned with the objective verifiability of propositions. In particular, previous work attempts to detect certain types of subjective proposition: Conrad et al. (2012) identify arguing subjectivity propositions and tag them with argument labels in order to cluster argument paraphrases. Others incorporate this task as a component for solving related problems, such as answering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wiebe, 2010). Similarly, Rosenthal and McKeown (2012) identify opinionated propositions expressing beliefs, leveraging from previous work in sentiment analysis and belief tagging. While the class of subjective propositions in subjectivity detection strictly contains UNVERIF propositions, it also partially overlaps with the VERIFEXP and VERIFNON classes of our work: We want to identify verifiable assertions within propositions, rather than determine the subjectivity of the proposition as a whole (e.g. proposition 8 in Table 1 is classified as a VERIFNON, though “Clearly” is s</context>
</contexts>
<marker>Somasundaran, Ruppenhofer, Wiebe, 2007</marker>
<rawString>Swapna Somasundaran, Josef Ruppenhofer, and Janyce Wiebe. 2007. Detecting arguing and sentiment in meetings. In Proceedings of the SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Toulmin</author>
<author>Richard Rieke</author>
<author>Allan Janik</author>
</authors>
<title>An Introduction to Reasoning.</title>
<date>1979</date>
<publisher>Macmillan Publishing Company.</publisher>
<contexts>
<context position="2092" citStr="Toulmin et al., 1979" startWordPosition="301" endWordPosition="304">e consider part of an argument. Not disregarding such implicit arguments allows us to discuss the types of support that can further be provided to strengthen the argument, as a form of assessment. 2Verifiable Experiential propositions are verifiable propositions about personal state or experience. See Table 1 for examples. 3We are assuming that there is no background knowledge that eliminates the need of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply </context>
</contexts>
<marker>Toulmin, Rieke, Janik, 1979</marker>
<rawString>Stephen E. Toulmin, Richard Rieke, and Allan Janik. 1979. An Introduction to Reasoning. Macmillan Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Toulmin</author>
</authors>
<title>The Uses of Argument.</title>
<date>1958</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2070" citStr="Toulmin, 1958" startWordPosition="299" endWordPosition="300">propositions are consider part of an argument. Not disregarding such implicit arguments allows us to discuss the types of support that can further be provided to strengthen the argument, as a form of assessment. 2Verifiable Experiential propositions are verifiable propositions about personal state or experience. See Table 1 for examples. 3We are assuming that there is no background knowledge that eliminates the need of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit a</context>
</contexts>
<marker>Toulmin, 1958</marker>
<rawString>S.E. Toulmin. 1958. The Uses of Argument. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Paz Garcia Villalba</author>
<author>Patrick Saint-Dizier</author>
</authors>
<title>Some facets of argument mining for opinion analysis.</title>
<date>2012</date>
<booktitle>In COMMA,</booktitle>
<pages>23--34</pages>
<contexts>
<context position="2799" citStr="Villalba and Saint-Dizier, 2012" startWordPosition="409" endWordPosition="412">n conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole argument. Consider the following examples: How much does a small carton of milk cost?1 More children should drink milk2, because children who drink milk everyday are taller than those who don’t3. Children would want to drink milk, anyway4. Firstly, Sentence 1 does not need any support, nor is it part of an argument. Next, Proposition 2 is an unv</context>
<context position="31441" citStr="Villalba and Saint-Dizier, 2012" startWordPosition="4994" endWordPosition="4997">(verb, past participle). 5 Related Work Argumentation Mining The primary goal of argumentation mining has been to identify and extract argumentative structures present in documents, which are often written by professionals (Moens et al., 2007; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). In certain cases, the specific document structure allows additional means of identify arguments (Mochales and Moens, 2008). Even the work on online text data, which are less rigid in structure and often contain insufficiently supported propositions, focus on the extraction of arguments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubstantiated propositions, which we consider as implicit arguments. Our approach should be valuable for processing documents like online user comment where arguments may not have adequate support and an automatic means of analysis can be useful. Subjectivity Detection Work to distinguish subjective from objective propositions (e.g.(Wiebe and Riloff, 2005)), often a subt</context>
</contexts>
<marker>Villalba, Saint-Dizier, 2012</marker>
<rawString>Maria Paz Garcia Villalba and Patrick Saint-Dizier. 2012. Some facets of argument mining for opinion analysis. In COMMA, pages 23–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts. In</title>
<date>2005</date>
<booktitle>In CICLing2005,</booktitle>
<pages>486--497</pages>
<contexts>
<context position="32026" citStr="Wiebe and Riloff, 2005" startWordPosition="5082" endWordPosition="5085">ments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubstantiated propositions, which we consider as implicit arguments. Our approach should be valuable for processing documents like online user comment where arguments may not have adequate support and an automatic means of analysis can be useful. Subjectivity Detection Work to distinguish subjective from objective propositions (e.g.(Wiebe and Riloff, 2005)), often a subtask for sentiment analysis (Pang and Lee, 2008), is relevant to our work since we are concerned with the objective verifiability of propositions. In particular, previous work attempts to detect certain types of subjective proposition: Conrad et al. (2012) identify arguing subjectivity propositions and tag them with argument labels in order to cluster argument paraphrases. Others incorporate this task as a component for solving related problems, such as answering opinion-based questions and determining the writer’s political stance (Somasundaran et al., 2007; Somasundaran and Wie</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Janyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In In CICLing2005, pages 486–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
</authors>
<title>Annotating attributions and private states.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</booktitle>
<contexts>
<context position="20370" citStr="Wilson and Wiebe, 2005" startWordPosition="3226" endWordPosition="3229">ibution is distinct in imaginative vs. informative writing. We expect this feature to distinguish UNVERIF propositions from the rest. Sentiment Clue Count Wilson et al. (2005) provides a subjectivity clue lexicon, which is a list of words with sentiment strength tags, either strong or weak, along with additional information, such as the sentiment polarity, Part-of-Speech Count (POS), etc. We suspect that propositions containing more sentiment words is more likely to be UNVERIF. Speech Event Count We use the 50 most frequent Objective-speech-event text anchors crawled from the MPQA 2.0 corpus (Wilson and Wiebe, 2005) as a speech event lexicon. The speech event text anchors refer to words like “stated” and “wrote” that introduce written or spoken propositions attributed to a source. propositions containing speech events such as proposition 10 in Table 1 are generally VERIFNON or VERIFEXP, since whether the attributed source has indeed made the proposition he allegedly made is objectively verifiable regardless of the subjectivity of the proposition itself. Imperative Expression Count Imperatives, i.e. commands, are generally UNVERIF (e.g. “Do the homework now!” that is, we expect there to be no objective ev</context>
</contexts>
<marker>Wilson, Wiebe, 2005</marker>
<rawString>Theresa Wilson and Janyce Wiebe. 2005. Annotating attributions and private states. In Proceedings of ACL Workshop on Frontiers in Corpus Annotation II: Pie in the Sky.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In In Proceedings of HLT-EMNLP,</booktitle>
<pages>347--354</pages>
<marker>Wilson, 2005</marker>
<rawString>Theresa Wilson. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In In Proceedings of HLT-EMNLP, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Wyner</author>
<author>Raquel Mochales-Palau</author>
<author>Marie-Francine Moens</author>
<author>David Milward</author>
</authors>
<title>Semantic processing of legal texts. chapter Approaches to Text Mining Arguments from Legal Cases,</title>
<date>2010</date>
<pages>60--79</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2451" citStr="Wyner et al., 2010" startWordPosition="357" endWordPosition="360">ckground knowledge that eliminates the need of support. Claire Cardie Department of Computer Science Cornell University Ithaca, NY, USA cardie@cs.cornell.edu premises, which can be conclusions of other arguments themselves (Toulmin, 1958; Toulmin et al., 1979; Pollock, 1987). To date, much of the argumentation mining research has been conducted on domains like news articles, parliamentary records and legal documents, where the documents contain well-formed explicit arguments, i.e. propositions with supporting reasons and evidence present in the text (Moens et al., 2007; Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). Unlike documents written by professionals, online user comments often contain arguments with inappropriate or missing justification. One way to deal with such implicit arguments is to simply disregard them and focus on extracting arguments containing proper support (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). However, recognizing such propositions as part of an argument,4 and determining the appropriate types of support can be useful for assessing the adequacy of the supporting information, and in turn, the strength of the whole</context>
<context position="31072" citStr="Wyner et al., 2010" startWordPosition="4938" endWordPosition="4941">nce over EXP is that EXP by itself is inadequate for the classification task: EXP consists of only 6 (or 12 with CCT) features denoting the person and tense information. Another reason is that VER, in a limited fashion, does encode experientiality: For instance, past tense propositions can be identified with the existence of VBD(verb, past tense) and VBN(verb, past participle). 5 Related Work Argumentation Mining The primary goal of argumentation mining has been to identify and extract argumentative structures present in documents, which are often written by professionals (Moens et al., 2007; Wyner et al., 2010; Feng and Hirst, 2011; Ashley and Walker, 2013). In certain cases, the specific document structure allows additional means of identify arguments (Mochales and Moens, 2008). Even the work on online text data, which are less rigid in structure and often contain insufficiently supported propositions, focus on the extraction of arguments (Villalba and Saint-Dizier, 2012; Cabrio and Villata, 2012). We, however, are interested in the assessment of the argumentative structure, potentially providing recommendations to readers and feedback to the writers. Thus it is crucial that we also process unsubs</context>
</contexts>
<marker>Wyner, Mochales-Palau, Moens, Milward, 2010</marker>
<rawString>Adam Wyner, Raquel Mochales-Palau, Marie-Francine Moens, and David Milward. 2010. Semantic processing of legal texts. chapter Approaches to Text Mining Arguments from Legal Cases, pages 60–79. Springer-Verlag, Berlin, Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
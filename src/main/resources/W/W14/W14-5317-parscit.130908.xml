<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001321">
<title confidence="0.9699765">
Experiments in Sentence Language Identification with Groups of Similar
Languages
</title>
<author confidence="0.936189">
Ben King Dragomir Radev Steven Abney
</author>
<affiliation confidence="0.911945333333333">
Department of EECS Department of EECS Department of Linguistics
University of Michigan School of Information University of Michigan
Ann Arbor University of Michigan Ann Arbor
</affiliation>
<email confidence="0.7001615">
benking@umich.edu Ann Arbor abney@umich.edu
radev@umich.edu
</email>
<sectionHeader confidence="0.992982" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880888888889">
Language identification is a simple problem that becomes much more difficult when its usual
assumptions are broken. In this paper we consider the task of classifying short segments of text in
closely-related languages for the Discriminating Similar Languages shared task, which is broken
into six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czech
and Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish,
and (F) American and British English. We consider a number of different methods to boost
classification performance, such as feature selection and data filtering, but we ultimately find that
a simple naive Bayes classifier using character and word n-gram features is a strong baseline that
is difficult to improve on, achieving an average accuracy of 0.8746 across the six tasks.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999634222222222">
Language identification constitutes the first stage of many NLP pipelines. Before applying tools trained
on specific languages, one must determine the language of the text. It is also is often considered to be a
solved task because of the high accuracy of language identification methods in the canonical formulation
of the problem with long monolingual documents and a set of mostly dissimilar languages to choose
from. We consider a different setting with much shorter text in the form of single sentences drawn from
very similar languages or dialects.
This paper describes experiments related to and our submissions to the Discriminating Similar Lan-
guages (DSL) shared task. This shared task has six subtasks, each a classification task in which a sentence
must be labeled as belonging to a small set of related languages:
</bodyText>
<listItem confidence="0.999961333333333">
• Task A: Bosnian vs. Croatian vs. Serbian • Task D: Brazilian vs. European Portuguese
• Task B: Indonesian vs. Malay • Task E: Argentinian vs. Peninsular Spanish
• Task C: Czech vs. Slovak • Task F: American vs. British English
</listItem>
<bodyText confidence="0.94348375">
The first three tasks involve classes that could be rightly called separate languages or dialects. The
classes of each of the final three tasks have high mutual intelligibility and are so similar that some
linguists may not even classify them as separate dialects. We will use the term “language variant” to
refer to such classes.
In this paper we experiment with several types of methods aimed at improving the classification ac-
curacy of these tasks: machine learning methods, data pre-processing, feature selection, and additional
training data. We find that a simple naive Bayes classifier using character and word n-gram features is
a strong baseline that is difficult to improve on. Because this paper covers so many different types of
methods, its format eschews the standard “Results” section, instead providing comparisons of methods
as they are presented.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.966446">
146
</page>
<note confidence="0.989316">
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154,
Dublin, Ireland, August 23 2014.
</note>
<sectionHeader confidence="0.997193" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999832708333333">
Recent directions in language identification have included finer-grained language identification (King
and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs
(Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely
related languages.
Language identification for closely related languages has been considered by several researchers,
though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguish-
ing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most
frequent words along with a Markov model and a word blacklist, a list of words that are not allowed
to appear in a certain language. A similar approach was later used by Tiedemann and Ljubeˇsi´c (2012)
to distinguish Bosnian, Croatian, and Serbian. They further develop the idea of a blacklist classifier,
loosening the binary restriction of the earlier work’s blacklist and considering the frequencies of words
rather than their absolute counts. This blacklist classifier is able to outperform a naive Bayes classifier
with large amounts of training data. They also find training on parallel data to be important, as it al-
lows the machine learning methods to pick out features relating to the differences between the languages
themselves, rather than learning differences in domain.
Zampieri et al. consider classes that would be most often classified as language varieties rather than
separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al.,
2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and
Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity
between a document and a corpus according to the size of the intersection between the sets of types in
each.
A similar, but somewhat different problem of automatically identifying lexical variants between
closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they
are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch.
</bodyText>
<sectionHeader confidence="0.996638" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9995244">
This paper’s training data and evaluation data both come from the DSL corpus collection (DSLCC)
(Tan et al., 2014). We use the training section of this data for training and the development section for
evaluation. The training section consists of 18,000 labeled instances per class, while the development
section has 2,000 labeled instances per class.
In order to try to increase classifier accuracy (and to avoid the problems with the task F training
data), we decided to collect additional training data for each open-class task. For each task, we collected
newspaper text from the appropriate websites for each of the 2–3 languages. We used regular expressions
to split the text into sentences, and created a set of rules to filter out strings that were unlikely to be good
sentences. Because the pages on the newspaper websites tended to have some boilerplate text, we collated
all the sentences and only kept one copy of each sentence.
</bodyText>
<subsectionHeader confidence="0.947162">
Task Language/Dialect Newspaper Sentences Words
</subsectionHeader>
<table confidence="0.826695888888889">
Bosnian Nezavisne Novine 175,741 3,250,648
A Croatian Novi List 231,271 4,591,318
Serbian Ve˘cernje Novosti 239,390 5,213,507
Indonesian Kompas 114,785 1,896,138
B
Malay Berita Harian 36,144 695,597
Czech Den´ık 160,972 2,432,393
C
Slovak Denn´ık SME 62,908 970,913
Brazilian Portuguese O Estado de S. Paulo 558,169 11,199,168
D
European Portuguese Correio da Manh˜a 148,745 2,979,904
Argentinian Spanish La Naci´on 333,246 7,769,941
E
Peninsular Spanish El Pa´ıs 195,897 4,329,480
American English The New York Times 473,350 10,491,641
F
British English The Guardian 971,097 20,288,294
</table>
<tableCaption confidence="0.998988">
Table 1: Sources and amounts of training data collected for the open track for each task.
</tableCaption>
<page confidence="0.998613">
147
</page>
<bodyText confidence="0.9999446">
In order to create balanced training data, for each task we downsampled the number of sentences of
the larger collection(s) to match the number of sentences in the smaller collection. For example, we
downsampled the British English collection to 473,350 sentences and combined it with the American
English sentences to create the training data for English. Figure 1 shows results of training using this
external data.
</bodyText>
<subsectionHeader confidence="0.977205">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.9998352">
We use many types of features that have been found to be useful in previous language identification
work: word unigrams, word bigrams, and character n-grams (2 G n G 6). Character n-grams are simply
substrings of the sentence and may include in addition to letters, whitespace, punctuation, digits, and
anything else that might be in the sentence. Words, for the purpose of word unigrams and bigrams, are
simply maximal tokens not containing any punctuation, digit, or whitespace.
When instances are encoded into feature vectors, each feature has a value equal to the number of times
it occured in the corresponding sentence, so the majority of features have a value of 0 for any given
instance, but it is possible for a feature to occur multiple times in a sentence and have a value greater
than 1.0 in the feature vector. Table 2 below compares the performance of a naive Bayes classifier using
each of the different feature groups below.
</bodyText>
<table confidence="0.999115222222222">
Task All Word 2 3 Character 5 6
1 2 4
Bosnian/Croatian/Serbian 0.9348 0.9290 0.8183 0.7720 0.8808 0.9412 0.9338 0.9323
Indonesian/Malay 0.9918 0.9943 0.9885 0.8545 0.9518 0.9833 0.9908 0.9930
Czech/Slovak 0.9998 1.0000 0.9985 0.9980 0.9998 0.9998 1.0000 1.0000
Portuguese 0.9535 0.9468 0.9493 0.7935 0.8888 0.9318 0.9468 0.9570
Spanish 0.8623 0.8738 0.8625 0.7673 0.8273 0.8513 0.8610 0.8660
English 0.4970 0.4948 0.5005 0.4825 0.4988 0.5010 0.5048 0.4993
Average 0.8732 0.8731 0.8529 0.7780 0.8412 0.8681 0.8729 0.8746
</table>
<tableCaption confidence="0.942735">
Table 2: Accuracies compared for different sets of features compared. The classifier used here is naive
Bayes.
</tableCaption>
<sectionHeader confidence="0.9963" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.997353833333333">
Our baseline method against which we compare all other models is a naive Bayes classifier using word
unigram features trained on the DSL-provided training data. The methods we compare to it can be
broken into three classes: other machine learning methods, feature selection methods, and data filtering
methods.
The classification pipeline used here has the following stages: (1) data filtering, (2) feature extraction,
(3) feature selection, (4) training, and (5) classification.
</bodyText>
<subsectionHeader confidence="0.994002">
4.1 Machine Learning Methods
</subsectionHeader>
<bodyText confidence="0.996293222222222">
We will use the following notation throughout this section. An instance x, that is, a sentence to be
classified, with a corresponding class label y is encoded into a feature vector f(x), where each entry
is an integer denoting how many times the feature corresponding to that entry’s index occurred in the
sentence. The class label here is a language and it’s drawn from a small set y E Y.
In addition to the naive Bayes classifier, we also experiment with two versions of logistic regression
and a support vector machine classifier. The MALLET machine learning library implementations are
used for the first three classifiers (McCallum, 2002) and SVMLight is used for the fourth (Joachims, ).
Naive Bayes A naive Bayes classifier models the class label as an independent combination of input
features.
</bodyText>
<page confidence="0.894767">
148
</page>
<equation confidence="0.970600333333333">
n
P(y |f (x)) = P(f (x)) P(y)YP(f(x)i|y) (1)
i=1
ces (x, y) E T in the following way:
logP(yi|xi;θ)−λR(θ) (3)
The term
</equation>
<bodyText confidence="0.9900115">
above is a regularization term. It is common for such a classifier to overfit the pa-
rameters to the training data. To keep this from happening, a regularization term can be added which
keeps the parameters in
from growing too large. Two common choices for this function are L2 an
</bodyText>
<equation confidence="0.996648857142857">
R(θ)
θ
d L1
normalization:
1
RL2 = ||θ||22 = θ2i,RL1 = ||θ||1 = Xn |θi |(4)
i=1
</equation>
<bodyText confidence="0.857236">
ce across tasks, we use a naive Bayes classifier throughout the rest of the paper.
</bodyText>
<subsectionHeader confidence="0.982893">
4.2 Feature Selection Methods
</subsectionHeader>
<bodyText confidence="0.921345">
d automatic.
d f¯ the non-occurence
of a feature, we measure its information gain by the following formula:
</bodyText>
<equation confidence="0.99566">
⎡ ⎤ ⎡ ⎤
⎣X ⎣X
G(f) = P(f) P(y|f)logP(y|f) ⎦ + P( ¯f) logP(y|¯f)logP(y |¯f) ⎦(5)
yEY yEY
</equation>
<bodyText confidence="0.9970815">
As naive Bayes is a generative classifier, it has been shown to be able to outperform discriminative
classifiers when the number of training instances is small compared to the number of features (Ng and
Jordan, 2002). This classifier is additionally advantageous in that it has a simple closed-form solution
for maximizing its log likelihood.
</bodyText>
<figure confidence="0.909708777777778">
Logistic Regression A logistic regression classifier is a discriminative classifier whose parameters are
encoded in a vector θ. The conditional probability of a class label over an instance (x, y) is modeled as
follows:
exp
y)
(2)
yEY
The parameter vector
is commonly estimated by maximizing the log-likelihood of this function over
</figure>
<figureCaption confidence="0.295084">
the set of training instan
</figureCaption>
<equation confidence="0.9954286">
1 X
P(y|x; θ) = Z(x; θ)exp �f(x, y) · θ} ; Z(x, θ)
If(x,
·θ}
θ
= argmaxg
Xθ
(x,y)ET
Xn
i=1
</equation>
<bodyText confidence="0.992291705882353">
L2 regularization is well-grounded theoretically, as it is equivalent to a model with a Gaussian prior
on the parameters (Rennie, 2004). But L1 regularization has a reputation for enforcing sparsity on the
parameters. In fact, it has been shown to be quite effective when the number of irrelevant dimensions is
greater than the number of training examples, which we expect to be the case with many of the tasks in
this paper (Ng, 2004).
Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attempts
to find a boundary that linearly separates the training data with the maximum possible margin. SVMs
have been shown to be a very efficient and high accuracy method to classify data across a wide variety
of different types of tasks (Tsochantaridis et al., 2004).
Table 3 below compares these machine learning methods. Because of its consistently good perfor-
man
We expect that the majority of features are not relevant to the classification task, and so we experimented
with several methods of feature selection, both manual an
Information Gain As a fully automatic method of feature extraction, we used information gain to
score features according to their expected usefulness. Information gain (IG) is an information theoretic
concept that (colloquially) measures the amount of knowledge about the class label that is gained by
having access to a specific feature. If f is the occurence an individual feature an
</bodyText>
<page confidence="0.983795">
149
</page>
<table confidence="0.9999296">
Task Logistic Logistic Naive Bayes SVM
Regression Regression
(L2-norm) (L1-norm)
Bosnian/Croatian/Serbian 0.9138 0.9135 0.9290 0.9100
Indonesian/Malay 0.9878 0.9810 0.9943 0.9873
Czech/Slovak 0.9983 0.9958 1.0000 0.9985
Portuguese 0.9383 0.9368 0.9468 0.9325
Spanish 0.8843 0.8770 0.8738 0.8768
English 0.5000 0.4945 0.4948 0.4958
Average 0.8704 0.8648 0.8731 0.8668
</table>
<tableCaption confidence="0.913677">
Table 3: Comparison of different machine learning methods using word unigram features on the six
tasks.
</tableCaption>
<bodyText confidence="0.999006916666667">
To reduce the number of features being used in classification (and to hopefully remove irrelevant
features), we choose the 10,000 features with the highest IG scores. IG considers each feature indepen-
dently, so it is possible that redundant feature sets could be chosen. For example, it might happen that
both the quadrigram ther and the trigram the score highly according to IG and are both selected, even
though they are highly correlated with one another.
Parallel Text Feature Selection Because IG feature selection often seemed to choose features more
related to differences in domain than to differences in language (see Table 7), we wanted to try to isolate
features that are specific to language differences. It has been shown in previous work that training on
parallel text can help to isolate language differences since the domains of the languages are identical
(Tiedemann and Ljubeˇsi´c, 2012). For each of the tasks,1 we use translations of the complete Bible as a
parallel corpus, running IG feature selection exactly as above. Table 4 below gives more details about
the texts used.
</bodyText>
<figure confidence="0.9732333125">
Task Language/Dialect Bible
Indonesian Alkitab dalam Bahasa Indonesia Masa Kini
B
Malay 2001 Today’s Malay Version
Czech Cesk´y studijnipreklad
C
Slovak Slovensk´y Ekumenick´y Biblia
Brazilian Portuguese a B´IBLIA para todos
D
European Portuguese Almeida Revista e Corrigida (Portugal)
Argentinian Spanish La Palabra (versi´on hispanoamericana)
E
Peninsular Spanish La Palabra (versi´on espa˜nola)
American English New International Version
F
British English New International Version Anglicized
</figure>
<tableCaption confidence="0.993023">
Table 4: Bibles used as parallel corpora for feature selection.
</tableCaption>
<bodyText confidence="0.993974636363636">
Manual Feature Selection We also used manual feature selection, selecting features to use in the clas-
sifiers from lists published on Wikipedia comparing the two languages. Of course some of the features in
lists like these are features that are quite difficult to detect using NLP (especially before the language has
been identified) such as characteristic passive or genitive constructions. But there are many features that
we are able to detect and use in a list of manually selected features, such as character n-grams relating
to morphology and spelling and word n-grams relating to vocabulary differences.
Table 5 below compares these feature selection methods on each task. Since the manual feature selec-
tion suggested all types of features, including character n-gram and word unigram and bigram features,
the experiments in this section use all features described in Section 3.1. The results show that any type
of feature selection consistently hurts performance, though IG hurts the least, and it should be noted
that in certain cases with other machine learning methods, IG feature selection actually yielded better
</bodyText>
<footnote confidence="0.96071">
1excluding Task A, for which we were unable to find a Bible in Latin-script Serbian or any Bible in Bosnian
</footnote>
<page confidence="0.998235">
150
</page>
<bodyText confidence="0.995394333333333">
performance than all features. That the feature selection methods designed to isolate language-specific
features performed so poorly is one indicator that the labeled data has additional differences that are not
tied to the languages themselves. We discuss this idea further in Section 5.
</bodyText>
<table confidence="0.999756125">
Task No feature selection IG Parallel Manual
Bosnian/Croatian/Serbian 0.9348 0.9300 – 0.6328
Indonesian/Malay 0.9918 0.9768 0.8093 0.8485
Czech/Slovak 0.9998 0.9995 0.9940 0.8118
Portuguese 0.9535 0.9193 0.7215 0.6888
Spanish 0.8623 0.8310 0.5210 0.7023
English 0.4970 0.4978 0.5020 0.5053
Average 0.8732 0.8590 – 0.6982
</table>
<tableCaption confidence="0.9352245">
Table 5: Comparison of manual and automatic feature selection methods. IG and parallel feature selec-
tion both use the 10,000 features with the highest IG scores.
</tableCaption>
<subsectionHeader confidence="0.995749">
4.3 Data Filtering Methods
</subsectionHeader>
<bodyText confidence="0.999507523809524">
English Word Removal In looking through the training data for the non-English tasks, we observed
that it was not uncommon for sentences in these languages to contain English words and phrases. Be-
cause foreign words should be independent of the language/dialect used, English words included in the
sentences for other tasks should just be noise that, if removed will improve classification performance.
For each of the non-English tasks (A, B, C, D, and E), we create a new training set for identifying
English/non-English words by mixing together 1,000 random English words with 10,000 random task-
language words. The imbalance in the classes is a compromise, approximating the actual proportions in
the test without leading to a degenerate classifier. Because English and the other classes are so dissimilar,
the performance of the English word classifier is very insensitive to the actual ratio. From this data, we
train a naive Bayes classifier using character 3-grams, 4-grams, and 5-grams.
We manually labeled the words of 150 sentences from the five non-English tasks in order to evaluate
the English word classifier. Across the five tasks, the precision was 0.76 and the recall was 0.66, leading
to an F1-score of 0.70. Any words labeled as English by the classifier were removed from the sentence
and it was passed on to the feature extraction, classification, and training stages.
Named Entity Removal We also observed another common class of word that could potentially act
as a noise source: named entities. Across all the languages listed studied here, it is common for named
entities to begin with a capital letter. Lacking named entity recognizers for all the languages here, we
instead used the property of having an initial capital letter as a surrogate for recognizing a word as a
named entity. Because all the languaes studied here also have the convention of capitalizing the first
word of a sentence, we remove all words beginning with a capital letter except for the first and pass this
abridged sentence on to the feature extraction, classification, and training stages.
</bodyText>
<table confidence="0.999477111111111">
Task No data filtering English Word Named Entity
Removal Removal
Bosnian/Croatian/Serbian 0.9138 0.9105 0.9003
Indonesian/Malay 0.9878 0.9885 0.9778
Czech/Slovak 0.9983 0.9980 0.9973
Portuguese 0.9383 0.9365 0.9068
Spanish 0.8843 0.8835 0.8555
English 0.5000 0.5000 0.5050
Average 0.8704 0.8695 0.8571
</table>
<tableCaption confidence="0.999711">
Table 6: Comparison of data filtering methods using word unigram features on the six tasks.
</tableCaption>
<page confidence="0.974951">
151
</page>
<figure confidence="0.999621393442623">
(A) (B)
Accuracy
0.8
0.6
0.4
Accuracy
0.9
0.8
0.7
0.6
0.5
1
DSL
external
external (CV)
DSL
external
external (CV)
0 0.2 0.4 0.6 0.8 1 0.5 1 1.5 2 2.5
Training Instances per Class ·105 Training Instances per Class ·104
(C)
0 0.5 1 1.5
(D)
0 0.2 0.4 0.6 0.8 1
DSL
external
external (CV)
DSL
external
external (CV)
Accuracy 1
0.9
0.8
0.7
0.6
Accuracy 0.9
0.8
0.7
0.6
0.5
Training Instances per Class ·105 Training Instances per Class ·105
(E)
0 0.5 1 1.5
(F)
0.5 1 1.5 2
DSL
external
external (CV)
DSL
external
external (CV)
Accuracy 0.9
0.8
0.7
0.6
0.5
Accuracy 0.8
0.7
0.6
0.5
Training Instances per Class ·105 Training Instances per Class ·105
</figure>
<figureCaption confidence="0.9391824">
Figure 1: Learning curves for the six tasks as the number of training instances per language is varied.
The line marked “DSL” is the learning curve for the DSL-provided training data evaluated against the
developement data. The line marked “external” is our external newspaper training data evaluated against
the development data. The line marked “external (CV)” is our external training data evaluated using
10-fold cross-validation.
</figureCaption>
<page confidence="0.956046">
152
</page>
<bodyText confidence="0.600530727272727">
Bosnian/Croatian/Serbian Indonesian/Malay Czech/Slovak Portuguese Spanish English
da bisa sa Portugal the I
kako berkata se R Rosario you
sa kerana aj euros han The
kazao karena ako Brasil euros said
takode daripada ve cento Argentina Obama
rekao saat pre governo PP your
evra dari pro Lusa Fe If
tijekom beliau ktor´e PSD Rajoy that
posle selepas s´u Ele Espa˜na but
posto bahwa ktor´y Governo Madrid It
</bodyText>
<tableCaption confidence="0.818925">
Table 7: The ten word-unigram features given the highest weight by information gain feature selection
for each of the six tasks.
</tableCaption>
<sectionHeader confidence="0.999052" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999767391304348">
Across many of the tasks, there was evidence that performance was tied more strongly to domain-specific
features of the two classes rather than to language- (or language-variant-) specific features. For example,
Table 7 shows the best word-unigram features selected by information gain feature selection for each of
the tasks. The Portuguese, Spanish, and English tasks specifically have as many of their most important
features named entities and other non-language specific features.
It seems that for many of the tasks, it is easier to distinguish the subject matter written about than it is to
distinguish the languages/dialects themselves. With Portuguese, for example, Brazilian dialect speakers
were much more likely to discuss places in Brazil and mention Brazilian reais (currency, abbreviated
as R), while European speakers mentioned euros, places in Portugal, and discussed Portuguese politics.
While there are definite linguistic differences between Brazilian and European Portuguese, these seem
to be less pronounced than the superficial differences in subject matter.
Practically, this is not necessarily a bad thing for this shared task, as the domain information gives extra
clues that allow the task to be completed with higher accuracy than would otherwise be possible. This
would become problematic if one wanted to apply a classifier trained on this data to general domains,
where the classifier may not be able to rely on the speaker talking about a certain subject matter. To
address this, the classifier would either need to focus on features specific to the language pair itself or
would need to be trained on data that spanned many domains.
Further evidence of domain overfitting comes from the fact that the larger training sets drawn from
newspaper text were not able to improve performance on the development set over the provided training
data, which is presumably drawn from the same collection as the development data. Figure 1 shows
learning curves for each of the six tasks. Though all the external text is self-consistent (cross-validation
results in high accuracy), in none of the cases does training on a large amount of external data allow the
classifier to exceed the accuracy achieved by training on the DSL data.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999402083333333">
In this paper we experimented with several methods for classification of sentences in closely-related lan-
guages for the DSL shared task. Our analysis showed that, when dealing with closely related languages,
the task of classifying text according to its language was difficult to untie from the taks of classifying
other text characteristics, such as the domain. Across all our types of methods, we found that a naive
Bayes classifier using character n-gram, word unigram, and word bigram features was a strong baseline.
In future work, we would like to try to improve on these results by incorporating features that try to
capture syntactic relationships. Certainly some of the pairs of languages considered here are close enough
that they could be chunked, tagged, or parsed before knowing exactly which variety they belong to. This
would allow for the inclusion of features related to transitivity, agreement, complementation, etc. For
example, in British English, the verb “provide” is monotransitive, but ditransitive in American English. It
is unclear how much features like these would improve accuracy, but it is likely that they would ultimately
be necessary to improve classification of similar languages to human levels of performance.
</bodyText>
<page confidence="0.998878">
153
</page>
<sectionHeader confidence="0.995879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999693604651163">
Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identi-
fication for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language
in Social Media, pages 65–74. Association for Computational Linguistics.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias. 2013. Microblog language identification: Overcoming
the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195–215.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive approach towards text source classification based on
top-bag-of-word similarity. pages 404–410.
Thorsten Joachims. Svmlight: Support vector machine. http://svmlight. joachims. org/.
Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly
supervised methods. In Proceedings of NAACL-HLT, pages 1110–1119.
Nikola Ljube&amp;quot;si´c, Nives Mikeli´c, and Damir Boras. 2007. Language identication: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces, pages
541–546.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multi-
lingual documents. Transactions of the Association for Computational Linguistics, 2:27–40.
Andrew K. McCallum. 2002. Mallet: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Andrew Y Ng and Michael I Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic
regression and naive bayes. Advances in neural information processing systems, 2:841–848.
Andrew Y Ng. 2004. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings of the
twenty-first international conference on Machine learning, page 78. ACM.
Dong-Phuong Nguyen and A Seza Dogruoz. 2013. Word level language identification in online multilingual
communication. Association for Computational Linguistics.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16(4):469–491.
Jason Rennie. 2004. On l2-norm regularization and the gaussian prior.
http://people.csail.mit.edu/jrennie/writing.
Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;sic, and J¨org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The dsl corpus collection. In Proceedings of The 7th Workshop on
Building and Using Comparable Corpora (BUCC).
J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In
COLING, pages 2619–2634.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector ma-
chine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international
conference on Machine learning, page 104. ACM.
Marcos Zampieri and Binyam Gebrekidan. 2012. Automatic identification of language varieties: The case of
portuguese. In Proceedings of KONVENS, pages 233–237.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2012. Classifying pluricentric languages:
Extending the monolingual model. In Proceedings of the Fourth Swedish Language Technlogy Conference
(SLTC2012), pages 79–80.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and pos
distribution for the identification of spanish varieties. Proceedings of TALN2013, Sable dOlonne, France, pages
580–587.
</reference>
<page confidence="0.999771">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801590">
<title confidence="0.9956635">Experiments in Sentence Language Identification with Groups of Similar Languages</title>
<author confidence="0.999577">Ben Dragomir Steven</author>
<affiliation confidence="0.996071666666667">Department of Department of Department of University of School of University of Ann University of Ann</affiliation>
<author confidence="0.827224">benkingumich edu Ann abneyumich edu</author>
<email confidence="0.997906">radev@umich.edu</email>
<abstract confidence="0.9991367">Language identification is a simple problem that becomes much more difficult when its usual assumptions are broken. In this paper we consider the task of classifying short segments of text in closely-related languages for the Discriminating Similar Languages shared task, which is broken into six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czech and Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish, and (F) American and British English. We consider a number of different methods to boost classification performance, such as feature selection and data filtering, but we ultimately find that simple naive Bayes classifier using character and word features is a strong baseline that is difficult to improve on, achieving an average accuracy of 0.8746 across the six tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Paul McNamee</author>
<author>Mossaab Bagdouri</author>
<author>Clayton Fink</author>
<author>Theresa Wilson</author>
</authors>
<title>Language identification for creating language-specific twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media,</booktitle>
<pages>65--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3753" citStr="Bergsma et al., 2012" startWordPosition="564" endWordPosition="567">re presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that are not allowed to appear in a certain language. A similar approach was later used by Tiedemann and Ljubeˇs</context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identification for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language in Social Media, pages 65–74. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Carter</author>
<author>Wouter Weerkamp</author>
<author>Manos Tsagkias</author>
</authors>
<title>Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text.</title>
<date>2013</date>
<journal>Language Resources and Evaluation,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="3775" citStr="Carter et al., 2013" startWordPosition="568" endWordPosition="571">k is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that are not allowed to appear in a certain language. A similar approach was later used by Tiedemann and Ljubeˇsi´c (2012) to distingu</context>
</contexts>
<marker>Carter, Weerkamp, Tsagkias, 2013</marker>
<rawString>Simon Carter, Wouter Weerkamp, and Manos Tsagkias. 2013. Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Lung-Hao Lee</author>
</authors>
<title>Contrastive approach towards text source classification based on top-bag-of-word similarity.</title>
<date>2008</date>
<pages>404--410</pages>
<contexts>
<context position="5294" citStr="Huang and Lee (2008)" startWordPosition="804" endWordPosition="807">ssifier with large amounts of training data. They also find training on parallel data to be important, as it allows the machine learning methods to pick out features relating to the differences between the languages themselves, rather than learning differences in domain. Zampieri et al. consider classes that would be most often classified as language varieties rather than separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al., 2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity between a document and a corpus according to the size of the intersection between the sets of types in each. A similar, but somewhat different problem of automatically identifying lexical variants between closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch. 3 Data This paper’s training data and evaluation data both come from the DSL corpus collection (DSLCC) (Tan et al., 2014). We use the training section of this data</context>
</contexts>
<marker>Huang, Lee, 2008</marker>
<rawString>Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive approach towards text source classification based on top-bag-of-word similarity. pages 404–410.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Svmlight: Support vector machine.</title>
<note>http://svmlight. joachims. org/.</note>
<marker>Joachims, </marker>
<rawString>Thorsten Joachims. Svmlight: Support vector machine. http://svmlight. joachims. org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben King</author>
<author>Steven Abney</author>
</authors>
<title>Labeling the languages of words in mixed-language documents using weakly supervised methods.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1110--1119</pages>
<contexts>
<context position="3646" citStr="King and Abney, 2013" startWordPosition="548" endWordPosition="551">hods, its format eschews the standard “Results” section, instead providing comparisons of methods as they are presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that </context>
</contexts>
<marker>King, Abney, 2013</marker>
<rawString>Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly supervised methods. In Proceedings of NAACL-HLT, pages 1110–1119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikola Ljubesi´c</author>
<author>Nives Mikeli´c</author>
<author>Damir Boras</author>
</authors>
<title>Language identication: How to distinguish similar languages?</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th International Conference on Information Technology Interfaces,</booktitle>
<pages>541--546</pages>
<marker>Ljubesi´c, Mikeli´c, Boras, 2007</marker>
<rawString>Nikola Ljube&amp;quot;si´c, Nives Mikeli´c, and Damir Boras. 2007. Language identication: How to distinguish similar languages? In Proceedings of the 29th International Conference on Information Technology Interfaces, pages 541–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Jey Han Lau</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic detection and language identification of multilingual documents.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--27</pages>
<contexts>
<context position="3691" citStr="Lui et al., 2014" startWordPosition="556" endWordPosition="559">section, instead providing comparisons of methods as they are presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that are not allowed to appear in a certain langua</context>
</contexts>
<marker>Lui, Lau, Baldwin, 2014</marker>
<rawString>Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics, 2:27–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="10573" citStr="McCallum, 2002" startWordPosition="1633" endWordPosition="1634">tation throughout this section. An instance x, that is, a sentence to be classified, with a corresponding class label y is encoded into a feature vector f(x), where each entry is an integer denoting how many times the feature corresponding to that entry’s index occurred in the sentence. The class label here is a language and it’s drawn from a small set y E Y. In addition to the naive Bayes classifier, we also experiment with two versions of logistic regression and a support vector machine classifier. The MALLET machine learning library implementations are used for the first three classifiers (McCallum, 2002) and SVMLight is used for the fourth (Joachims, ). Naive Bayes A naive Bayes classifier models the class label as an independent combination of input features. 148 n P(y |f (x)) = P(f (x)) P(y)YP(f(x)i|y) (1) i=1 ces (x, y) E T in the following way: logP(yi|xi;θ)−λR(θ) (3) The term above is a regularization term. It is common for such a classifier to overfit the parameters to the training data. To keep this from happening, a regularization term can be added which keeps the parameters in from growing too large. Two common choices for this function are L2 an R(θ) θ d L1 normalization: 1 RL2 = ||</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems,</title>
<date>2002</date>
<pages>2--841</pages>
<contexts>
<context position="11741" citStr="Ng and Jordan, 2002" startWordPosition="1841" endWordPosition="1844">tion are L2 an R(θ) θ d L1 normalization: 1 RL2 = ||θ||22 = θ2i,RL1 = ||θ||1 = Xn |θi |(4) i=1 ce across tasks, we use a naive Bayes classifier throughout the rest of the paper. 4.2 Feature Selection Methods d automatic. d f¯ the non-occurence of a feature, we measure its information gain by the following formula: ⎡ ⎤ ⎡ ⎤ ⎣X ⎣X G(f) = P(f) P(y|f)logP(y|f) ⎦ + P( ¯f) logP(y|¯f)logP(y |¯f) ⎦(5) yEY yEY As naive Bayes is a generative classifier, it has been shown to be able to outperform discriminative classifiers when the number of training instances is small compared to the number of features (Ng and Jordan, 2002). This classifier is additionally advantageous in that it has a simple closed-form solution for maximizing its log likelihood. Logistic Regression A logistic regression classifier is a discriminative classifier whose parameters are encoded in a vector θ. The conditional probability of a class label over an instance (x, y) is modeled as follows: exp y) (2) yEY The parameter vector is commonly estimated by maximizing the log-likelihood of this function over the set of training instan 1 X P(y|x; θ) = Z(x; θ)exp �f(x, y) · θ} ; Z(x, θ) If(x, ·θ} θ = argmaxg Xθ (x,y)ET Xn i=1 L2 regularization is w</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Y Ng and Michael I Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. Advances in neural information processing systems, 2:841–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
</authors>
<title>Feature selection, l 1 vs. l 2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning,</booktitle>
<pages>78</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12754" citStr="Ng, 2004" startWordPosition="2015" endWordPosition="2016"> by maximizing the log-likelihood of this function over the set of training instan 1 X P(y|x; θ) = Z(x; θ)exp �f(x, y) · θ} ; Z(x, θ) If(x, ·θ} θ = argmaxg Xθ (x,y)ET Xn i=1 L2 regularization is well-grounded theoretically, as it is equivalent to a model with a Gaussian prior on the parameters (Rennie, 2004). But L1 regularization has a reputation for enforcing sparsity on the parameters. In fact, it has been shown to be quite effective when the number of irrelevant dimensions is greater than the number of training examples, which we expect to be the case with many of the tasks in this paper (Ng, 2004). Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attempts to find a boundary that linearly separates the training data with the maximum possible margin. SVMs have been shown to be a very efficient and high accuracy method to classify data across a wide variety of different types of tasks (Tsochantaridis et al., 2004). Table 3 below compares these machine learning methods. Because of its consistently good performan We expect that the majority of features are not relevant to the classification task, and so we experimented with several methods of featur</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Andrew Y Ng. 2004. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings of the twenty-first international conference on Machine learning, page 78. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong-Phuong Nguyen</author>
<author>A Seza Dogruoz</author>
</authors>
<title>Word level language identification in online multilingual communication. Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="3672" citStr="Nguyen and Dogruoz, 2013" startWordPosition="552" endWordPosition="555">ws the standard “Results” section, instead providing comparisons of methods as they are presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that are not allowed to appear </context>
</contexts>
<marker>Nguyen, Dogruoz, 2013</marker>
<rawString>Dong-Phuong Nguyen and A Seza Dogruoz. 2013. Word level language identification in online multilingual communication. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Dirk Geeraerts</author>
<author>Dirk Speelman</author>
</authors>
<title>The automatic identification of lexical variation between language varieties.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="5615" citStr="Peirsman et al., 2010" startWordPosition="853" endWordPosition="856">most often classified as language varieties rather than separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al., 2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity between a document and a corpus according to the size of the intersection between the sets of types in each. A similar, but somewhat different problem of automatically identifying lexical variants between closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch. 3 Data This paper’s training data and evaluation data both come from the DSL corpus collection (DSLCC) (Tan et al., 2014). We use the training section of this data for training and the development section for evaluation. The training section consists of 18,000 labeled instances per class, while the development section has 2,000 labeled instances per class. In order to try to increase classifier accuracy (and to avoid the problems with the task F training data), we decided to coll</context>
</contexts>
<marker>Peirsman, Geeraerts, Speelman, 2010</marker>
<rawString>Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation between language varieties. Natural Language Engineering, 16(4):469–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Rennie</author>
</authors>
<title>On l2-norm regularization and the gaussian prior.</title>
<date>2004</date>
<note>http://people.csail.mit.edu/jrennie/writing.</note>
<contexts>
<context position="12454" citStr="Rennie, 2004" startWordPosition="1962" endWordPosition="1963">mizing its log likelihood. Logistic Regression A logistic regression classifier is a discriminative classifier whose parameters are encoded in a vector θ. The conditional probability of a class label over an instance (x, y) is modeled as follows: exp y) (2) yEY The parameter vector is commonly estimated by maximizing the log-likelihood of this function over the set of training instan 1 X P(y|x; θ) = Z(x; θ)exp �f(x, y) · θ} ; Z(x, θ) If(x, ·θ} θ = argmaxg Xθ (x,y)ET Xn i=1 L2 regularization is well-grounded theoretically, as it is equivalent to a model with a Gaussian prior on the parameters (Rennie, 2004). But L1 regularization has a reputation for enforcing sparsity on the parameters. In fact, it has been shown to be quite effective when the number of irrelevant dimensions is greater than the number of training examples, which we expect to be the case with many of the tasks in this paper (Ng, 2004). Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attempts to find a boundary that linearly separates the training data with the maximum possible margin. SVMs have been shown to be a very efficient and high accuracy method to classify data across a wide var</context>
</contexts>
<marker>Rennie, 2004</marker>
<rawString>Jason Rennie. 2004. On l2-norm regularization and the gaussian prior. http://people.csail.mit.edu/jrennie/writing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Marcos Zampieri</author>
<author>Nikola Ljubesic</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Merging comparable data sources for the discrimination of similar languages: The dsl corpus collection.</title>
<date>2014</date>
<booktitle>In Proceedings of The 7th Workshop on Building and Using Comparable Corpora (BUCC).</booktitle>
<contexts>
<context position="5852" citStr="Tan et al., 2014" startWordPosition="890" endWordPosition="893">Singapore, and Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity between a document and a corpus according to the size of the intersection between the sets of types in each. A similar, but somewhat different problem of automatically identifying lexical variants between closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch. 3 Data This paper’s training data and evaluation data both come from the DSL corpus collection (DSLCC) (Tan et al., 2014). We use the training section of this data for training and the development section for evaluation. The training section consists of 18,000 labeled instances per class, while the development section has 2,000 labeled instances per class. In order to try to increase classifier accuracy (and to avoid the problems with the task F training data), we decided to collect additional training data for each open-class task. For each task, we collected newspaper text from the appropriate websites for each of the 2–3 languages. We used regular expressions to split the text into sentences, and created a se</context>
</contexts>
<marker>Tan, Zampieri, Ljubesic, Tiedemann, 2014</marker>
<rawString>Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;sic, and J¨org Tiedemann. 2014. Merging comparable data sources for the discrimination of similar languages: The dsl corpus collection. In Proceedings of The 7th Workshop on Building and Using Comparable Corpora (BUCC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
<author>Nikola Ljubesi´c</author>
</authors>
<title>Efficient discrimination between closely related languages. In</title>
<date>2012</date>
<booktitle>COLING,</booktitle>
<pages>2619--2634</pages>
<marker>Tiedemann, Ljubesi´c, 2012</marker>
<rawString>J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In COLING, pages 2619–2634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning,</booktitle>
<pages>104</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13116" citStr="Tsochantaridis et al., 2004" startWordPosition="2073" endWordPosition="2076">tation for enforcing sparsity on the parameters. In fact, it has been shown to be quite effective when the number of irrelevant dimensions is greater than the number of training examples, which we expect to be the case with many of the tasks in this paper (Ng, 2004). Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attempts to find a boundary that linearly separates the training data with the maximum possible margin. SVMs have been shown to be a very efficient and high accuracy method to classify data across a wide variety of different types of tasks (Tsochantaridis et al., 2004). Table 3 below compares these machine learning methods. Because of its consistently good performan We expect that the majority of features are not relevant to the classification task, and so we experimented with several methods of feature selection, both manual an Information Gain As a fully automatic method of feature extraction, we used information gain to score features according to their expected usefulness. Information gain (IG) is an information theoretic concept that (colloquially) measures the amount of knowledge about the class label that is gained by having access to a specific feat</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international conference on Machine learning, page 104. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan</author>
</authors>
<title>Automatic identification of language varieties: The case of portuguese.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="5133" citStr="Zampieri and Gebrekidan, 2012" startWordPosition="779" endWordPosition="782">the earlier work’s blacklist and considering the frequencies of words rather than their absolute counts. This blacklist classifier is able to outperform a naive Bayes classifier with large amounts of training data. They also find training on parallel data to be important, as it allows the machine learning methods to pick out features relating to the differences between the languages themselves, rather than learning differences in domain. Zampieri et al. consider classes that would be most often classified as language varieties rather than separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al., 2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity between a document and a corpus according to the size of the intersection between the sets of types in each. A similar, but somewhat different problem of automatically identifying lexical variants between closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch. 3 </context>
</contexts>
<marker>Zampieri, Gebrekidan, 2012</marker>
<rawString>Marcos Zampieri and Binyam Gebrekidan. 2012. Automatic identification of language varieties: The case of portuguese. In Proceedings of KONVENS, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
<author>Sascha Diwersy</author>
</authors>
<title>Classifying pluricentric languages: Extending the monolingual model.</title>
<date>2012</date>
<booktitle>In Proceedings of the Fourth Swedish Language Technlogy Conference (SLTC2012),</booktitle>
<pages>79--80</pages>
<contexts>
<context position="5102" citStr="Zampieri et al., 2012" startWordPosition="775" endWordPosition="778"> binary restriction of the earlier work’s blacklist and considering the frequencies of words rather than their absolute counts. This blacklist classifier is able to outperform a naive Bayes classifier with large amounts of training data. They also find training on parallel data to be important, as it allows the machine learning methods to pick out features relating to the differences between the languages themselves, rather than learning differences in domain. Zampieri et al. consider classes that would be most often classified as language varieties rather than separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al., 2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity between a document and a corpus according to the size of the intersection between the sets of types in each. A similar, but somewhat different problem of automatically identifying lexical variants between closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they are able to identify Netherlandic Dutch synonyms f</context>
</contexts>
<marker>Zampieri, Gebre, Diwersy, 2012</marker>
<rawString>Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2012. Classifying pluricentric languages: Extending the monolingual model. In Proceedings of the Fourth Swedish Language Technlogy Conference (SLTC2012), pages 79–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
<author>Sascha Diwersy</author>
</authors>
<title>N-gram language models and pos distribution for the identification of spanish varieties.</title>
<date>2013</date>
<booktitle>Proceedings of TALN2013, Sable dOlonne, France,</booktitle>
<pages>580--587</pages>
<contexts>
<context position="5157" citStr="Zampieri et al., 2013" startWordPosition="783" endWordPosition="786">d considering the frequencies of words rather than their absolute counts. This blacklist classifier is able to outperform a naive Bayes classifier with large amounts of training data. They also find training on parallel data to be important, as it allows the machine learning methods to pick out features relating to the differences between the languages themselves, rather than learning differences in domain. Zampieri et al. consider classes that would be most often classified as language varieties rather than separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al., 2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity between a document and a corpus according to the size of the intersection between the sets of types in each. A similar, but somewhat different problem of automatically identifying lexical variants between closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch. 3 Data This paper’s traini</context>
</contexts>
<marker>Zampieri, Gebre, Diwersy, 2013</marker>
<rawString>Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and pos distribution for the identification of spanish varieties. Proceedings of TALN2013, Sable dOlonne, France, pages 580–587.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
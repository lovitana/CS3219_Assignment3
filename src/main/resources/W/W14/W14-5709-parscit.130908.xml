<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.079940">
<title confidence="0.999234">
Distinguishing Degrees of Compositionality in Compound Splitting
for Statistical Machine Translation
</title>
<author confidence="0.966106">
Marion Weller1,2, Fabienne Cap2, Stefan M¨uller1
Sabine Schulte im Walde1, Alexander Fraser2
</author>
<affiliation confidence="0.947078">
1 IMS, University of Stuttgart
</affiliation>
<email confidence="0.86886">
{wellermn;muellesn;schulte}@ims.uni-stuttgart.de
</email>
<affiliation confidence="0.369349">
2 CIS, Ludwig-Maximilian University of Munich
</affiliation>
<email confidence="0.995812">
{cap;fraser}@cis.uni-muenchen.de
</email>
<sectionHeader confidence="0.997344" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999634">
The paper presents an approach to morphological compound splitting that takes the degree of
compositionality into account. We apply our approach to German noun compounds and particle
verbs within a German–English SMT system, and study the effect of only splitting compositional
compounds as opposed to an aggressive splitting. A qualitative study explores the translational
behaviour of non-compositional compounds.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999911">
In German, as in many other languages, two (or more) simplex words can be combined to form a com-
pound. This is a productive process, leading to a potentially infinite number of sound German com-
pounds. As a consequence, many NLP applications suffer from coverage issues for compounds which
do not appear or appear only infrequently in language resources. However, while many compounds are
not covered, their component words are often found in lexical resources or training data. Compound
processing allows access to these component words and thus can overcome these sparsity issues.
We use Statistical Machine Translation (SMT) as an example application for compound processing.
Our SMT system translates from German to English, where compounds are usually split in the German
source language prior to training and decoding. The benefits are obvious: vocabulary size is reduced
and the languages are adjusted in terms of granularity, as exemplified by the compound Holzzaun. This
</bodyText>
<figure confidence="0.955812333333333">
wooden
fence
wooden Holz
Holzzaun
fence Zaun
1:n alignment 1:1 alignment
</figure>
<bodyText confidence="0.968168318181818">
results in better alignment quality and model estimation.
Compound splitting also enables the translation of com-
pounds not occurring in the parallel data, if the parts have
been seen and can thus be translated individually. However, these assumptions only hold for compo-
sitional compounds like Holzzaun (’wooden fence’), whose meanings can be derived from the mean-
ings of their constituents, namely Holz (’wood’) and Zaun (’fence’). In contrast, the splitting of non-
compositional compounds may lead to translation errors: e.g. the meaning of J¨agerzaun (’lattice fence’)
cannot be represented by the meanings of its constituents J¨ager (’hunter’) and Zaun (’fence’). Here, an
erroneous splitting of the compound can lead to wrong generalizations or translation pairs, such as J¨ager
→ lattice, in the absence of other evidence about how to translate J¨ager. When splitting compounds
for SMT, two important factors should thus be considered: (1) whether a compound is compositional
and should be split, and if so (2) how the compound should be split. Most previous approaches mainly
focused on the second task, how to split a compound, e.g. using frequency statistics (Koehn and Knight,
2003) or a rule-based morphology (Fritzinger and Fraser, 2010), and all of them showed improved SMT
quality for compound splitting. The decision about whether the compound is compositional and should
be split at all has not received much attention in the past.
In this work, we examine the effect of only splitting compositional compounds, in contrast to splitting
all compounds. To this end, we combine (A) an approach relying on the distributional similarity be-
tween compounds and their constituents, to predict the degree of compositionality and thus to determine
whether to split the compound with (B) a combination of morphological and frequency-based features
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.985035">
81
</page>
<note confidence="0.9852525">
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 81–90,
Dublin, Ireland, August 24 2014.
</note>
<bodyText confidence="0.999900625">
to determine how to split a compound. We experiment with this novel semantically-informed compound
splitting on the source-side data of a German-English SMT system. As far as we know, we are the first
to study the impact of compositionality-aware compound splitting in SMT. We evaluate our systems on
a standard and on a specifically created test set, both for noun compounds and particle verbs. Our re-
sults show that phrase-based SMT is generally robust with regard to over-splitting non-compositional
compounds, with the exception of low-frequency words. This is in line with corresponding assumptions
from previous work. Furthermore, we present a small-scale study about the translational behaviour of
non-compositional compounds, which can surprisingly often be translated component-wise.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997889">
We combine morphology-based compound splitting with distributional semantics to improve phrase-
based SMT. Here, we discuss relevant work of compound splitting in SMT and distributional semantics.
</bodyText>
<subsectionHeader confidence="0.997466">
2.1 Compound Splitting in SMT
</subsectionHeader>
<bodyText confidence="0.999979826086957">
Compound splitting in SMT is a well-studied task. There is a wide range of previous work, including
purely string- and frequency-based approaches, but also linguistically-informed approaches. All lines
of research improved translation performance due to compound splitting. In Koehn and Knight (2003),
compounds are split through the identification of substrings from a corpus. The splitting is performed
without linguistic knowledge (except for the insertion of the filler letters “(e)s”), which necessarily leads
to many erroneous splittings. Multiple possible splitting options are disambiguated using the frequencies
of the substrings. Starting from Koehn and Knight (2003), Stymne (2008) covers more morphological
transformations and imposes POS constraints on the subwords. Nießen and Ney (2000) and Fritzinger
and Fraser (2010) perform compound splitting by relying on morphological analysers to identify suitable
split points. This has the advantage of returning only linguistically motivated splitting options, but the
analyses are often ambiguous and require disambiguation: Nießen and Ney (2000) use a parser for
context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best
split for each compound. Other approaches use a two-step word alignment process: first, word alignment
is performed on a split representation of the compounding language. Then, all former compound parts
for which there is no aligned counterpart in the non-compounding language are merged back to the
compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003)
for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This
blocks non-compositional compounds from being split if they are translated as one simplex English word
in the training data (e.g. Heckensch¨utze, lit. ’hedge|shooter’; ’sniper’) and aligned correctly. However,
cases like J¨agerzaun, ’lattice fence’ are not covered.
In the present work, we identify compounds with a morphological analyser, disambiguated with corpus
frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics.
We are not aware of any previous work that takes semantics into account for compound splitting in SMT.
</bodyText>
<subsectionHeader confidence="0.999881">
2.2 Distributional Semantics and Compounding
</subsectionHeader>
<bodyText confidence="0.999982583333333">
Distributional information has been a steadily increasing, integral part of lexical semantic research over
the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know
a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora
to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest.
Among many other tasks, distributional semantic information has been utilised to determine the degree
of compositionality (or: semantic transparency) of various types of compounds, most notably regarding
noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013),
Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson
(2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)).
Typically, these approaches rely on co-occurrence information from a corpus (either referring to bags-
of-words, or focusing on target-specific types of features), and compare the distributional features of
the compounds with those of the constituents, in order to predict the degree of compositionality of the
</bodyText>
<page confidence="0.988769">
82
</page>
<figure confidence="0.999799461538461">
Jägerzaun
Holzzaun
Input
Step 1: Identify
Component Words
Holz
Zaun
Jäger
Zaun
Preprocessing
Step 2: Predict
Similarity Scores
0.311
0.825
0.015
0.725
Step 3: Split Compo−
sitional Compounds
Holz
Zaun
Jägerzaun
SMT
Output
lattice fence
wooden
fence
</figure>
<figureCaption confidence="0.999989">
Figure 1: Semantically-informed compound processing in SMT.
</figureCaption>
<bodyText confidence="0.9984162">
compound. The underlying assumption is that a compound which is similar in meaning to a constituent
(as in Holzzaun–Zaun (‘wooden fence’–‘fence’) but not in L¨owenzahn–Zahn (‘lion|tooth (dandelion)’–
‘tooth’)) is also similar to the constituent with regard to co-occurrence information.
Most related to this work on noun compounds, Reddy et al. (2011) relied on window-based distribu-
tional models to predict the compositionality of English noun compounds, and Schulte im Walde et al.
(2013) compared window-based against syntax-based distributional models to predict the composition-
ality of German noun compounds. Zinsmeister and Heid (2004) used subcategorising verbs to predict
compound–head similarities of German noun compounds. Most recently, Salehi et al. (2014) extended
the previous approaches to take multi-lingual co-occurrence information into account, regarding English
and German noun compounds, and English particle verbs.
</bodyText>
<sectionHeader confidence="0.998328" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999981">
We integrate our semantically-informed compound splitting as a pre-processing step to the German
source language of an SMT system. See Figure 1 for an illustration of our compound processing pipeline.
</bodyText>
<subsectionHeader confidence="0.997802">
3.1 Target Compounds
</subsectionHeader>
<bodyText confidence="0.9999897">
German compounds are combinations of two (or more) simplex words. In some cases, a morphological
transformation is required: for example, when combining the two nouns Ausflug (‘excursion’) and Ziel
(‘destination’) → Ausflugsziel (‘excursion destination’), a filler letter (here: “s”) needs to be inserted.
Other such transformations include more filler letters or the deletion/substitution of letters.
Noun compounds are formed of a head noun and a modifier, which can consist of nouns, verbs, adjec-
tives or proper nouns.
Particle verbs are productive compositions of a base verb and a prefix particle, whose part-of-speech
varies between open-class nouns, adjectives, and verbs, and closed-class prepositions and adverbs. In
comparison to noun compounds, the constituents of German particle verbs exhibit a much higher degree
of ambiguity: Verbs in general are more ambiguous than nouns, and the largest sub-class of particles
(those with a preposition particle) is highly ambiguous by itself (e.g. Lechler and RoBdeutscher (2009)
and Springorum (2011)). For example, in anknabbern (‘to nibble partially’), the particle an expresses a
partitive meaning , whereas in ankleben (‘to glue onto sth.’) an has a topological meaning (to glue sth.
onto an implicit background). In addition, particle verb senses may be transparent or opaque with respect
to their base verbs. For example, abholen ‘fetch’ is rather transparent with respect to its base verb holen
‘fetch’, whereas anfangen ‘begin’ is more opaque with respect to fangen ‘catch’. In contrast, einsetzen
has both transparent (e.g. ‘insert’) and opaque (e.g. ‘begin’) verb senses with respect to setzen ‘put/sit
(down)’. The high degree of ambiguity makes particle verbs a challenge for NLP. Moreover, particle
and base verb can occur separately (er f¨angt an: ‘he begins’) or in one word (dass er anf¨angt: ‘that he
begins’), depending on the clausal type. This makes consistent treatment of particle verbs difficult.
</bodyText>
<subsectionHeader confidence="0.999723">
3.2 Identification of Component Parts
</subsectionHeader>
<bodyText confidence="0.999827666666667">
We use the rule-based morphological analyser SMOR (Schmid et al., 2004) to identify compounds and
their constituents in our parallel training data (cf. Section 4). It relies on a large lexicon of word lemmas
and feature rules for productive morphological processes in German, i.e., compounding, derivation and
</bodyText>
<page confidence="0.993293">
83
</page>
<bodyText confidence="0.999904">
inflection. In this paper, we will not consider splitting into derivational affixes (as needed for, e.g., Arabic
and Turkish), but instead identify simplex words that may also occur independently. Moreover, we only
keep noun compounds and particle verbs consisting of two constituents. The resulting set consists of
93,299 noun compound types and 3,689 particle verb types.
</bodyText>
<subsectionHeader confidence="0.999492">
3.3 Predicting Compositionality based on Distributional Similarity
</subsectionHeader>
<bodyText confidence="0.9999865">
Starting from this set of compounds as derived from our parallel training data, we collected distributional
co-occurrence information from two large German web corpora and the machine translation training data:
(i) the German COW corpus (Sch¨afer and Bildhauer (2012), —9 billion words), (ii) the SdeWaC (Faaß
and Eckart (2013), —880 million words), (iii) our MT parallel corpus (—40 million words) and (iv) MT
language model training data (—146 million words). We relied on earlier work and used the 20,000 most
frequent nouns from the SdeWaC as co-occurrence features, looking into a window of 20 words to the left
and to the right of our target compounds and their constituents. We thus obtained a co-occurrence matrix
of all compounds and their constituents with the 20,000 selected nouns. As co-occurrence strength (i.e.,
how strong is a co-occurrence between a target word and a co-occurring noun), we collected frequencies
and transformed them into local mutual information (LMI) values, cf. Evert (2005). Finally, we calcu-
lated the distributional similarity between the compounds and their constituents, relying on the standard
measure cosine. The cosine value is then used to predict the degree of compositionality between the
respective compound–constituent pairs. For example, the cosine value of the pair Baumschule–Baum1 is
0.38, while the cosine value of the pair Baumschule–Schule is only 0.01.
</bodyText>
<subsectionHeader confidence="0.992742">
3.4 Semantically-Informed Compound Splitting
</subsectionHeader>
<bodyText confidence="0.999988">
In the two preceding sections, we described how we identified component words and calculated distribu-
tional compositionality scores for all of the compounds found in our training data. Here, we give details
on how we include the semantic information into the compound splitting process. Recall that we only
want to split compositional compounds and keep non-compositional compounds together.
The splitting decision (to split/not split a compound) is based on the compositionality score of the
compound that takes into account either one or both of the compound–constituent cosine values: if the
predicted degree of compositionality is high, the compound is split. We consider and combine four dif-
ferent criteria: i) only the compound–modifier similarity (mod); (ii) only the compound–head similarity
(head); a combination of the compound–modifier and the compound–head similarities, relying on (iii) the
geometric mean (geom) or (iv) on the arithmetic mean (arith). We used different thresholds for each of
these criteria throughout our experiments, with a specific focus on distinguishing the contributions of the
modifiers vs. the heads in the splitting decision, following insights from recent work in psycholinguistic
studies (Gagn´e and Spalding, 2009; Gagn´e and Spalding, 2011) as well as in computational approaches
on noun compounding (Reddy et al., 2011; Schulte im Walde et al., 2013). Furthermore, we compare
the effects of splitting with regard to two types of compounds, noun compounds and particle verbs: Both
types are very productive and can generate a potentially infinite number of new forms.
</bodyText>
<sectionHeader confidence="0.998364" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<bodyText confidence="0.999873">
This section gives an overview on the technical details of the SMT system and our data sets. Compound
splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as
the input for parameter tuning and testing.2
Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al.,
2007). We use it with default settings to train a translation model and we do so separately for each of the
different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Fea-
ture weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence.
Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005))
and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we
</bodyText>
<footnote confidence="0.993505666666667">
1BaumIschule: ‘tree|school’ (tree nursery)
2Compounds not contained in the parallel data are always split, as they cannot be translated otherwise.
3Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09
</footnote>
<page confidence="0.998388">
84
</page>
<bodyText confidence="0.999920052631579">
use an English corpus of roughly 227 million words (including the English part of the parallel data) to
build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM
(Heafield, 2011). For parameter tuning, we use 1,025 sentences of news data.
Standard Test set 1,026 sentences of news data (test set from the 2009 WMT Shared Task): this set is
to measure the translation quality on a standard SMT test and make it comparable to other work.
Noun/Verb Test set As our main focus lies on sentences containing compounds, we created a second
test set which is rich in compounds. From the combined 2008-2013 Shared Task test sets, we extracted
all sentences containing at least one noun compound for which we have compound-constituent similarity
scores. Moreover, we excluded sentences containing nouns that are not in the parallel training data: such
compounds can only be translated when split which allows to translate their components. The final test
set consists of 2,574 sentences. Similarly, we also created a set rich in particle verbs (855 sentences).
Opaque Test set As the two first test sets mainly contain compositional compounds, we use a third test
set consisting of sentences with only non-compositional compounds. The underlying compounds were
chosen based on a list containing noun compounds and human ratings for compositionality (von der
Heide and Borgwaldt (2009)). As before, the compounds must have occurred in the parallel data. The
result is a list of 14 compounds, of which 11 have a low modifier-compound similarity and 3 have a low
head-compound similarity. We then extracted sentences containing these compounds (5 per compound =
70 in total) from German newspaper data4. In contrast to the other sets, we use this test set in a qualitative
study, to approximate the translation quality by counting the number of correctly translated compounds.
</bodyText>
<sectionHeader confidence="0.998051" genericHeader="method">
5 SMT Results
</sectionHeader>
<bodyText confidence="0.999807666666667">
In this section, we present and discuss the results of our machine translation experiments. We first report
results for two test sets in terms of a standard evaluation metric (BLEU) and then continue with a small-
scale qualitative study on the translational behaviour of non-compositional compounds.
</bodyText>
<subsectionHeader confidence="0.986294">
5.1 Compound Splitting within a Standard SMT Task
</subsectionHeader>
<bodyText confidence="0.99989775">
BLEU (Papineni et al., 2002) is a common metric to automatically measure the quality of SMT output
by comparing n-gram matches of the SMT output with a human reference translation. Table 1 lists the
results for our SMT-systems: we report on different compound-constituent scores and thresholds, for
noun compounds and particle verbs respectively. Note that BLEU scores are not comparable across dif-
</bodyText>
<table confidence="0.999336809523809">
nouns particle verbs
stand. noun stand. verb
baseline 21.00 21.08 21.00 20.29
DIST 22.00 22.02 21.02 20.11
FREQ 22.04 21.88 21.11 20.21
head 21.77 21.58 – –
50 mod. 22.01 21.74 – –
0. geom. 21.99 21.71 – –
arith. 21.95 21.95 – –
head 21.91 21.69 21.11 20.24
mod. 22.01 21.63 20.98 20.43
0. geom. 22.06 21.90 21.12 20.55
arith. 22.05 21.73 21.08 20.34
head 21.80 21.67 21.10 20.09
mod. 21.71 21.77 21.00 20.25
0. geom. 21.78 21.64 20.84 20.30
arith. 22.00 21.77 21.24 20.40
head 21.78 21.51 – –
2 mod. 21.78 21.45 – –
0. geom. 21.76 21.54 – –
arith. 22.02 21.79 – –
</table>
<tableCaption confidence="0.987757">
Table 1: BLEU scores for all compound-
constituent variations.
</tableCaption>
<bodyText confidence="0.999855315789474">
ferent test sets, but only illustrate system differences
within one test set. We compare our systems to the
scores of a baseline system (without compound process-
ing) and an aggressive split system in which all noun
compounds and particle verbs are split. The labels DIST
and FREQ indicate how several possible splittings were
disambiguated: DIST means we chose the splitting option
having the higher geometric mean of the two compound-
constituent scores, assuming that the variant expressing a
higher compositionality score leads to the more probable
splitting analysis. For FREQ, the decision is based on the
geometric mean of corpus frequencies of the respective
components of the compound, as is common practise for
the disambiguation of multiple splitting options in SMT
(Koehn and Knight, 2003; Fritzinger and Fraser, 2010).
In terms of BLEU, there is little difference for these two
variants. For further experiments, we thus decided to al-
ways use FREQ for disambiguation, assuming that com-
ponents chosen by frequency are potentially better repre-
</bodyText>
<footnote confidence="0.957999">
4www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.html
</footnote>
<page confidence="0.997818">
85
</page>
<table confidence="0.999324142857143">
rating compound gloss mod. head translation
HIGHLY Staats|bankrott nation|bankruptcy 0.4779 0.6527 national bankruptcy
COMP. Staats|gebilde nation|structure 0.6955 0.3431 national structure
MEDIUM Industrie|staat industry|nation 0.0258 0.1488 industrial nation
COMP. Staats|kasse nation|cash box 0.0718 0.2757 public purse, treasury
LOW Staats|spitze nation|top 0.0024 0.0040 top/head of state
COMP. Staats|monotheismus nation|monotheism 0.0071 0.0071 national monotheism
</table>
<tableCaption confidence="0.998086">
Table 2: Examples for different compound-constitutent score ranges: HIGH: highly compositional,
MEDIUM: cases of doubt, LOW: highly non-compositional, according to their scores.
</tableCaption>
<bodyText confidence="0.999283181818182">
sented in the training data. Thus, we first use frequencies to determine the best split option in the case
of several possibilities, and then we apply distributional semantics to determine whether to split at all.
The remainder of Table 1 reports on different variants of the semantically-informed splitting criteria we
used. The notation head/mod/geom/arith indicates which (combination of) compound-constituent scores
were applied as criterion, with the threshold indicated by the vertical number. We performed the first
set of experiments with different thresholds for noun compounds, and then applied the medium-range
thresholds to the particle verbs. Generally, there are no considerable differences between the systems
with semantically restricted splitting and the aggressive split systems, even though there seems to be a
slightly positive effect for particle verbs. Having a closer look, we find that for noun compounds on the
standard test set, the best results (threshold: 0.1) are at the same level as the aggressive split systems;
with some small losses in BLEU on some of the other settings.
</bodyText>
<subsectionHeader confidence="0.985076">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.99995108">
All settings clearly outperform the baseline system (without compound processing). This indicates that
phrase-based SMT is rather robust with regard to non-semantic splitting as it is can often recover from
over-splitting by translating the word sequence as a phrase. This is in line with previous observations
of Koehn and Knight (2003). The results for the noun test set, which is biased towards containing more
nominal compounds, even suggests that less splitting might harm the system, as the BLEU scores tend
to drop when increasing the threshold. For particle verbs,5 the picture is slightly different: first, splitting
only particle verbs does not lead to a considerable improvement over the baseline, as in the case of noun
compounds. For the verb test set, it even leads to a drop in BLEU. However, a more restricted splitting
leads to improved BLEU scores, even though not significantly better than the un-split baseline system.
Even though the handling of particle verbs needs to be refined in terms of dealing with their structural
behaviour (split vs. unsplit depending on the sentence structure) or ambiguities of the particle verb,
we consider this an encouraging result indicating that particle verbs can benefit from a semantically-
informed splitting process.
There are several possible reasons why a more restricted splitting might not lead to an improvement,
even though the idea of splitting only compositional compounds is intuitive and straightforward.
Inconsistent Splitting Compositionality is a continuum rather than a binary decision, with the scores of
many (compositional) compounds being in the medium range. Thus, it happens that some compounds
containing a certain constituent are split, whereas others are not: such inconsistent splittings do not
contribute to the generalization compound splitting aims for. Table 2 gives examples for compounds
with different degrees of compositionality, which illustrate this issue: for Industriestaat (’industrial
nation’) and Staatskasse (’public purse’) in the middle part of the table, a splitting decision based on
the head scores for thresholds of 0.15 or 0.2 leads to inconsistent splitting. Only compounds with high
scores, as the examples at the top of Table 2 are always split. The bottom part gives examples with
comparatively low compound-constituent scores that would benefit from splitting, but which will not be
split in any of our systems.
</bodyText>
<footnote confidence="0.905853">
5Note that there are considerably less particle verbs than noun compounds in the standard test set and the parallel data.
</footnote>
<page confidence="0.98658">
86
</page>
<table confidence="0.999533">
compound gloss translation unsplit f split f
Seehunde sea|dogs seals seals 5 seals 5
Flohmarkt flea|market flea market flea market 5 flea market 5
Kopfsalat head|salad lettuce lettuce 5 lettuce 5
Handtuch hand|cloth towel towel 5 towel 5
Kronleuchter crown|candelabra chandelier chandelier 5 crown leuchter 5
G¨urteltiere belt|animal armadillo armadillos 5 belt animals 5
Wasserhahn water|rooster tap tap 5 water tap 2
water supply 3
Meerschweinchen sea|piglet guinea pig guinea pig 5 guinea pig 4
sea pig 1
Taschenbuch pocket|book paperback paperback 5 paper back 3
pocket book 2
Kronkorken crown|cork crown cap *kronkorken 5 crown corks 5
Taschenlampe pocket|lamp flashlight *taschenlampe 5 pocket lamp 4
bag lamp 1
Fleischwolf meat|wolf meat grinder *fleischwolf 5 meat wolf 5
Marienk¨afer Mary|bug ladybug *marienk¨afer 5 *marie k¨afer 5
Blockfl¨oten block|flute recorder *blockfl¨oten 5 block might 4
bloc might 1
</table>
<tableCaption confidence="0.9567345">
Table 3: correct vs. wrong – Translation of non-compositional compounds (opaque test set) without
being split (unsplit) vs. being split prior to translation. ’*’ highlights untranslated compounds.
</tableCaption>
<bodyText confidence="0.998978294117647">
Coverage of Opaque Compounds Another relevant factor concerns the frequency ranges of compounds
that are most interesting for this approach. High/mid frequency compounds are usually well-covered by
the training data of an SMT system, and in most cases they are translated correctly even if they have
been split erroneously. This is due to the fact that split compounds can be learned and translated as a
phrase if there were enough instances for the system to learn a valid translation. In the case of low-
frequency compounds, the system is less likely to learn a correct translation from the parallel data.
However, low-frequency compounds are not well covered by the system and splitting should thus be
highly beneficial. Newly created, i.e. highly compositional compounds, tend to be of low frequency, as
is illustrated by the example of Staatsmonotheismus (freq=1 in the parallel data) in Table 2. However,
a wrong splitting decision for a non-compositional compound of low frequency is likely to lead to an
incorrect translation as the SMT system has better statistics for the individual parts than for the sequence
of the compounds constituents. We assume that for low-frequency compounds the distributional similar-
ity scores are generally less reliable, even though using LMI helps to minimize this. To a certain extent,
we expect non-compositional compounds –which are typically considered as lexicalized– to occur with
higher frequencies than novel compositional compounds.6 Furthermore, there are considerably more
compositional than non-compositional compounds in standard text. Thus, being in favor of splitting in
the case of low-frequency words should be reasonable in most contexts.
</bodyText>
<sectionHeader confidence="0.910451" genericHeader="method">
6 A Closer Look at Translating Opaque Compounds
</sectionHeader>
<bodyText confidence="0.999647909090909">
In this section, we compare the translations of non-compositional compounds when they are unsplit
and when they are split. We use a small test set containing 70 sentences, 5 for each of the 14 non-
compositional compounds (see Section 4). Then we conduct a small-scale qualitative analysis focusing
on the correct translation of opaque compounds.
Table 3 reports on correct translations for the non-compositional compounds for an experiment where
they have been split or not split (unsplit) prior to translation. Even though all compounds occurred in
the parallel data, five (which are marked with ’*’) cannot be translated by the unsplit system due to not
being aligned correctly. The other compounds are translated correctly (marked with “+” in Table 3).
In the course of our study, we found that many of the correct translations remain the same (seals, flea
market, lettuce, towel). In the case of guinea pig, paperback and tap there are mixed results of correct and
incorrect translations. Only in the cases of chandelier (“crown leuchter”) and armadillo (“belt animal”),
</bodyText>
<footnote confidence="0.996547">
6It has to be noted, though, that the model is influenced by the somewhat different domain of the parallel data (European
Parliament proceedings, a standard data set for SMT).
</footnote>
<page confidence="0.994877">
87
</page>
<table confidence="0.999206">
compound gloss translation
B¨arlauch bear|leek bear leek
Baumschule tree|school tree nursery
L¨owenanteil lion|share lion’s share
Fliegenpilz fly|mushroom fly agaric
Flohmarkt flea|market flea market
compound gloss translation
Handtasche hand|bag handbag
Hirschk¨afer stag|beetle stag beetle
H¨uttenk¨ase cottage|cheese cottage cheese
Kronkorken crown|cork crown cap
Teelicht tea|light tea candle
</table>
<tableCaption confidence="0.996447">
Table 4: Examples for (near) literal translation of non-compositional compounds.
</tableCaption>
<bodyText confidence="0.999530137931034">
which were translated correctly with the unsplit system, all translations obtained with the split system are
wrong. Somewhat surprisingly, in some cases there even is a benefit from splitting the non-compositional
compounds: Kronkorken, previously not translated at all, is correctly generated as crown cork. For other
previously untranslated words, Fleischwolf and Taschenlampe, literal translations of the constituents
are given: while meat wolf (instead of meat grinder) is probably not understandable, the translation of
Taschenlampe as pocket lamp is certainly preferable to the untranslated compound.
Due to the observed unexpected translational behaviour of 2 of the 14 non-compositional compounds
(Flohmarkt and Kronkorken), which can be translated literally and thus –in theory– benefit from splitting,
we present a small study illustrating that this phenomenon is not as rare as one would intuitively expect.
This study is not meant to be comprehensive, but rather to point out that the translational behaviour of
non-compositional compounds can correspond to that of compositional compounds; Table 4 lists a few
such examples. We assume that this behaviour is due to the fact that English and German are similar
languages with a similar background. Thus, the “images” used in non-compositional words often tend to
be similar. For some of the compounds (e.g. Flohmarkt) this is even true for some Romance languages,
too (IT: mercato delle pulci, FR: march´e aux puces) .
Generally, the SMT system should even be able to handle cases where the translation of one part is
not strictly literal (e.g. cap–cork or agaric–mushroom). In comparison to a dictionary, which only lists
few translations, the translation model offers a large choice of translation options that are not always
strictly synonymous, but can cover a large range of related meanings. In combination with the target-
side language model, this could allow to “guess” good translations of such compounds. However, the
component-wise translation of non-compositional compounds only works if the source- and target lan-
guage compounds contain the same number of constituents. For example, consider translating the word
Faultier (lazylanimal: “sloth”): even if the SMT system offers the translation faul–sloth, it would also
need to produce a translation for the constituent tier, probably resulting in something like sloth animal.
In conclusion, while phrase-based SMT is often able to recover from over-splitting by translating
a word sequence as a phrase, this is not always necessary for opaque compounds as they can have a
literal or near-literal translation. Thus, for explicitly handling non-compositional compounds in SMT, a
monolingual estimation of compositionality is not the only relevant factor. The translational behaviour
of compounds should also be taken into account.
</bodyText>
<sectionHeader confidence="0.996359" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999971153846154">
We studied the impact of compositionality in German-English SMT by restricting compound splitting to
compositional compounds. The decision about compositionality is based on the distributional similarity
between a compound and its constituents. We experimented with different threshold/score combinations
on a standard and a specifically created test set. Our results indicate that phrase-based SMT is very robust
with regard to over-splitting non-compositional noun compounds, with the exception of low-frequency
compounds. Furthermore, we studied the translational behaviour of non-compositional compounds with
a special focus on the fact that non-compositional compounds can in some cases be translated component-
wise, leading to the conclusion that a monolingual estimation of compositionality is not sufficient for an
optimal explicit handling of compounds in SMT applications.
The relatively low impact of distinguishing the degree of compositionality might also be due to the fact
that the task of translating noun compounds can be considered “easy”, as the split components always
occur adjacently. In contrast, handling other types of non-compositional structures (e.g. noun-verb or
preposition-noun-verb combinations which are non-compositional) is a challenging task for future work.
</bodyText>
<page confidence="0.998455">
88
</page>
<sectionHeader confidence="0.998507" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9993035">
This work was funded by the DFG Research Projects ”Distributional Approaches to Semantic Related-
ness” (Marion Weller, Stefan M¨uller) and “Models of Morphosyntax for Statistical Machine Transla-
tion – Phase 2” (Fabienne Cap, Alexander Fraser, Marion Weller) and the DFG Heisenberg Fellowship
SCHU-2580/1-1 (Sabine Schulte im Walde).
</bodyText>
<sectionHeader confidence="0.998849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9984345">
Ming-Hong Bai, Keh-Jiann Chen, and Jason S Chang. 2008. Improving word alignment by adjusting chinese word
segmentation. In IJCNLP’08: Proceedings of the 3rd International Joint Conference on Natural Language
Processing, pages 249–256.
Collin Bannard. 2005. Learning about the Meaning of Verb–Particle Constructions from Corpora. Computer
Speech and Language, 19:467–478.
Stefan Bott and Sabine Schulte im Walde. 2014. Optimizing a Distributional Semantic Model for the Prediction
of German Particle Verb Compositionality. In Proceedings of the 9th Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In HLT-
NAACL’12: Proceedings of the Human Language Technology Conference of the North American Chapter of the
Association for Computational Linguistics, volume 12, pages 34–35.
Paul Cook and Suzanne Stevenson. 2006. Classifying Particle Semantics in English Verb-Particle Constructions.
In Proceedings of the ACL/COLING Workshop on Multiword Expressions: Identifying and Exploiting Underly-
ing Properties, pages 45–53, Sydney, Australia.
Steve DeNeefe, Ulf Hermjakob, and Kevin Knight. 2008. Overcoming vocabulary sparsity in mt using lattices.
In AMTA’08: Proceedings of the 8th Biennial Conference of the Association for Machine Translation in the
Americas.
Stefan Evert. 2005. The Statistics of Word Co-Occurrences: Word Pairs and Collocations. Ph.D. thesis, Institut
f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.
Gertrud Faaß and Kerstin Eckart. 2013. SdeWaC – a Corpus of Parsable Sentences from the Web. In Proceedings
of the International Conference of the German Society for Computational Linguistics and Language Technology,
pages 61–68, Darmstadt, Germany.
John R. Firth. 1957. Papers in Linguistics 1934-51. Longmans, London, UK.
Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis
and Corpus Statistics for German Compound Processing. In Proceedings of the Fifth Workshop on Statistical
Machine Translation, pages 224–234. Association for Computational Linguistics.
Christina L. Gagn´e and Thomas L. Spalding. 2009. Constituent Integration during the Processing of Compound
Words: Does it involve the Use of Relational Structures? Journal of Memory and Language, 60:20–35.
Christina L. Gagn´e and Thomas L. Spalding. 2011. Inferential Processing and Meta-Knowledge as the Bases for
Property Inclusion in Combined Concepts. Journal of Memory and Language, 65:176–192.
Zellig Harris. 1968. Distributional Structure. In Jerold J. Katz, editor, The Philosophy of Linguistics, Oxford
Readings in Philosophy, pages 26–47. Oxford University Press.
Kenneth Heafield. 2011. Kenlm: faster and smaller language model queries. In EMNLP’11: Proceedings of the
6th workshop on statistical machine translation within the 8th Conference on Empirical Methods in Natural
Language Processing, pages 187–197.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In EACL ’03: Proceedings of
the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 187–193,
Morristown, NJ, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL’07: Proceedings of the
45th Annual Meeting of the Association for Computational Linguistics, Demonstration Session, pages 177–180.
</reference>
<page confidence="0.994587">
89
</page>
<reference confidence="0.997521857142857">
Philipp Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit’05: Proceed-
ings of the 10th machine translation summit, pages 79–86.
Natalie K¨uhner and Sabine Schulte im Walde. 2010. Determining the Degree of Compositionality of German Par-
ticle Verbs by Clustering Approaches. In Proceedings of the 10th Conference on Natural Language Processing,
pages 47–56, Saarbr¨ucken, Germany.
Andrea Lechler and Antje RoBdeutscher. 2009. German Particle Verbs with auf. Reconstructing their Composition
in a DRT-based Framework. Linguistische Berichte, 220.
Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a Continuum of Compositionality in Phrasal
Verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and
Treatment, pages 73–80, Sapporo, Japan.
Sonja NieBen and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In COLING’00:
Proceedings of the 18th International Conference on Computational Linguistics, pages 1081–1085. Morgan
Kaufmann.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51,.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation
of machine translation. In ACL’02: Proceedings of the 40th Annual Meeting of the Association for Computa-
tional Linguistics, pages 311–318.
Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An Empirical Study on Compositionality in Com-
pound Nouns. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages
210–218, Chiang Mai, Thailand.
Bahar Salehi, Paul Cook, and Timothy Baldwin. 2014. Using Distributional Similarity of Multi-Way Translations
to Predict Multiword Expression Compositionality. In Proceedings of EACL 2014.
Roland Sch¨afer and Felix Bildhauer. 2012. Building Large Corpora from the Web Using a New Efficient Tool
Chain. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
486–493, Istanbul, Turkey.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. Smor: A German computational morphology covering
derivation, composition and inflection. In LREC ’04: Proceedings of the 4th Conference on Language Resources
and Evaluation, pages 1263–1266.
Sabine Schulte im Walde, Stefan M¨uller, and Stephen Roller. 2013. Exploring Vector Space Models to Predict the
Compositionality of German Noun-Noun Compounds. In Proceedings of the 2nd Joint Conference on Lexical
and Computational Semantics, pages 255–265, Atlanta, GA.
Sylvia Springorum. 2011. DRT-based Analysis of the German Verb Particle ”an”. Leuvense Bijdragen, 97:80–
105.
Andreas Stolcke. 2002. SRILM – an extensible language modelling toolkit. In ICSLN’02: Proceedings of the
international conference on spoken language processing, pages 901–904.
Sara Stymne. 2008. German compounds in factored statistical machine translation. In GoTAL ’08: Proceedings
of the 6th International Conference on Natural Language Processing, pages 464–475. Springer Verlag.
Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter, Basis und Oberbegriffen. In
Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51–74.
Heike Zinsmeister and Ulrich Heid. 2004. Collocations of Complex Nouns: Evidence for Lexicalisation. In
Proceedings of Konvens, Vienna, Austria.
</reference>
<page confidence="0.998644">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.317335">
<title confidence="0.9995865">Distinguishing Degrees of Compositionality in Compound for Statistical Machine Translation</title>
<author confidence="0.8604925">Fabienne Stefan Schulte im Alexander</author>
<affiliation confidence="0.894594">1IMS, University of</affiliation>
<address confidence="0.426588">2CIS, Ludwig-Maximilian University of</address>
<abstract confidence="0.997296166666667">The paper presents an approach to morphological compound splitting that takes the degree of compositionality into account. We apply our approach to German noun compounds and particle within a German–English SMT system, and study the effect of splitting compositional opposed to an aggressive splitting. A qualitative study explores the translational behaviour of non-compositional compounds.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ming-Hong Bai</author>
<author>Keh-Jiann Chen</author>
<author>Jason S Chang</author>
</authors>
<title>Improving word alignment by adjusting chinese word segmentation.</title>
<date>2008</date>
<booktitle>In IJCNLP’08: Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="6789" citStr="Bai et al. (2008)" startWordPosition="985" endWordPosition="988">ey (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word alignment process: first, word alignment is performed on a split representation of the compounding language. Then, all former compound parts for which there is no aligned counterpart in the non-compounding language are merged back to the compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003) for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This blocks non-compositional compounds from being split if they are translated as one simplex English word in the training data (e.g. Heckensch¨utze, lit. ’hedge|shooter’; ’sniper’) and aligned correctly. However, cases like J¨agerzaun, ’lattice fence’ are not covered. In the present work, we identify compounds with a morphological analyser, disambiguated with corpus frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics. We are not aware of any previous work that takes semantics into account for compound splitting in SMT. 2.2 Dist</context>
</contexts>
<marker>Bai, Chen, Chang, 2008</marker>
<rawString>Ming-Hong Bai, Keh-Jiann Chen, and Jason S Chang. 2008. Improving word alignment by adjusting chinese word segmentation. In IJCNLP’08: Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Bannard</author>
</authors>
<title>Learning about the Meaning of Verb–Particle Constructions from Corpora. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="8236" citStr="Bannard (2005)" startWordPosition="1194" endWordPosition="1195">t ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 Jägerzaun Holzzaun Input Step 1: Identify Component Words Holz Zaun Jäger Zaun Preprocessing Step 2: Predict Similarity Scores 0.311 0.825 0.015 0.725 Step 3: Split </context>
</contexts>
<marker>Bannard, 2005</marker>
<rawString>Collin Bannard. 2005. Learning about the Meaning of Verb–Particle Constructions from Corpora. Computer Speech and Language, 19:467–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bott</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Optimizing a Distributional Semantic Model for the Prediction of German Particle Verb Compositionality.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th Conference on Language Resources and Evaluation,</booktitle>
<location>Reykjavik, Iceland.</location>
<marker>Bott, Walde, 2014</marker>
<rawString>Stefan Bott and Sabine Schulte im Walde. 2014. Optimizing a Distributional Semantic Model for the Prediction of German Particle Verb Compositionality. In Proceedings of the 9th Conference on Language Resources and Evaluation, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In HLTNAACL’12: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>12</volume>
<pages>34--35</pages>
<contexts>
<context position="16631" citStr="Cherry and Foster, 2012" startWordPosition="2448" endWordPosition="2451"> This section gives an overview on the technical details of the SMT system and our data sets. Compound splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007). We use it with default settings to train a translation model and we do so separately for each of the different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Feature weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence. Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1BaumIschule: ‘tree|school’ (tree nursery) 2Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 84 use an English corpus of roughly 227 million words (including the English part of the parallel da</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In HLTNAACL’12: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, volume 12, pages 34–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Classifying Particle Semantics in English Verb-Particle Constructions.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<pages>45--53</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="8263" citStr="Cook and Stevenson (2006)" startWordPosition="1196" endWordPosition="1199">w a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 Jägerzaun Holzzaun Input Step 1: Identify Component Words Holz Zaun Jäger Zaun Preprocessing Step 2: Predict Similarity Scores 0.311 0.825 0.015 0.725 Step 3: Split Compo− sitional Compounds H</context>
</contexts>
<marker>Cook, Stevenson, 2006</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2006. Classifying Particle Semantics in English Verb-Particle Constructions. In Proceedings of the ACL/COLING Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
</authors>
<title>Overcoming vocabulary sparsity in mt using lattices.</title>
<date>2008</date>
<booktitle>In AMTA’08: Proceedings of the 8th Biennial Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="6756" citStr="DeNeefe et al. (2008)" startWordPosition="978" endWordPosition="981"> require disambiguation: Nießen and Ney (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word alignment process: first, word alignment is performed on a split representation of the compounding language. Then, all former compound parts for which there is no aligned counterpart in the non-compounding language are merged back to the compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003) for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This blocks non-compositional compounds from being split if they are translated as one simplex English word in the training data (e.g. Heckensch¨utze, lit. ’hedge|shooter’; ’sniper’) and aligned correctly. However, cases like J¨agerzaun, ’lattice fence’ are not covered. In the present work, we identify compounds with a morphological analyser, disambiguated with corpus frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics. We are not aware of any previous work that takes semantics into account for co</context>
</contexts>
<marker>DeNeefe, Hermjakob, Knight, 2008</marker>
<rawString>Steve DeNeefe, Ulf Hermjakob, and Kevin Knight. 2008. Overcoming vocabulary sparsity in mt using lattices. In AMTA’08: Proceedings of the 8th Biennial Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Co-Occurrences: Word Pairs and Collocations.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</institution>
<contexts>
<context position="13929" citStr="Evert (2005)" startWordPosition="2040" endWordPosition="2041">and (iv) MT language model training data (—146 million words). We relied on earlier work and used the 20,000 most frequent nouns from the SdeWaC as co-occurrence features, looking into a window of 20 words to the left and to the right of our target compounds and their constituents. We thus obtained a co-occurrence matrix of all compounds and their constituents with the 20,000 selected nouns. As co-occurrence strength (i.e., how strong is a co-occurrence between a target word and a co-occurring noun), we collected frequencies and transformed them into local mutual information (LMI) values, cf. Evert (2005). Finally, we calculated the distributional similarity between the compounds and their constituents, relying on the standard measure cosine. The cosine value is then used to predict the degree of compositionality between the respective compound–constituent pairs. For example, the cosine value of the pair Baumschule–Baum1 is 0.38, while the cosine value of the pair Baumschule–Schule is only 0.01. 3.4 Semantically-Informed Compound Splitting In the two preceding sections, we described how we identified component words and calculated distributional compositionality scores for all of the compounds</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The Statistics of Word Co-Occurrences: Word Pairs and Collocations. Ph.D. thesis, Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertrud Faaß</author>
<author>Kerstin Eckart</author>
</authors>
<title>SdeWaC – a Corpus of Parsable Sentences from the Web.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology,</booktitle>
<pages>61--68</pages>
<location>Darmstadt, Germany.</location>
<contexts>
<context position="13245" citStr="Faaß and Eckart (2013)" startWordPosition="1929" endWordPosition="1932">y simplex words that may also occur independently. Moreover, we only keep noun compounds and particle verbs consisting of two constituents. The resulting set consists of 93,299 noun compound types and 3,689 particle verb types. 3.3 Predicting Compositionality based on Distributional Similarity Starting from this set of compounds as derived from our parallel training data, we collected distributional co-occurrence information from two large German web corpora and the machine translation training data: (i) the German COW corpus (Sch¨afer and Bildhauer (2012), —9 billion words), (ii) the SdeWaC (Faaß and Eckart (2013), —880 million words), (iii) our MT parallel corpus (—40 million words) and (iv) MT language model training data (—146 million words). We relied on earlier work and used the 20,000 most frequent nouns from the SdeWaC as co-occurrence features, looking into a window of 20 words to the left and to the right of our target compounds and their constituents. We thus obtained a co-occurrence matrix of all compounds and their constituents with the 20,000 selected nouns. As co-occurrence strength (i.e., how strong is a co-occurrence between a target word and a co-occurring noun), we collected frequenci</context>
</contexts>
<marker>Faaß, Eckart, 2013</marker>
<rawString>Gertrud Faaß and Kerstin Eckart. 2013. SdeWaC – a Corpus of Parsable Sentences from the Web. In Proceedings of the International Conference of the German Society for Computational Linguistics and Language Technology, pages 61–68, Darmstadt, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<date>1957</date>
<booktitle>Papers in Linguistics 1934-51.</booktitle>
<location>Longmans, London, UK.</location>
<contexts>
<context position="7603" citStr="Firth, 1957" startWordPosition="1099" endWordPosition="1100">igned correctly. However, cases like J¨agerzaun, ’lattice fence’ are not covered. In the present work, we identify compounds with a morphological analyser, disambiguated with corpus frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics. We are not aware of any previous work that takes semantics into account for compound splitting in SMT. 2.2 Distributional Semantics and Compounding Distributional information has been a steadily increasing, integral part of lexical semantic research over the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCar</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. Papers in Linguistics 1934-51. Longmans, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Fritzinger</author>
<author>Alexander Fraser</author>
</authors>
<title>How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth Workshop on Statistical Machine Translation,</booktitle>
<pages>224--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3093" citStr="Fritzinger and Fraser, 2010" startWordPosition="450" endWordPosition="453">tituents J¨ager (’hunter’) and Zaun (’fence’). Here, an erroneous splitting of the compound can lead to wrong generalizations or translation pairs, such as J¨ager → lattice, in the absence of other evidence about how to translate J¨ager. When splitting compounds for SMT, two important factors should thus be considered: (1) whether a compound is compositional and should be split, and if so (2) how the compound should be split. Most previous approaches mainly focused on the second task, how to split a compound, e.g. using frequency statistics (Koehn and Knight, 2003) or a rule-based morphology (Fritzinger and Fraser, 2010), and all of them showed improved SMT quality for compound splitting. The decision about whether the compound is compositional and should be split at all has not received much attention in the past. In this work, we examine the effect of only splitting compositional compounds, in contrast to splitting all compounds. To this end, we combine (A) an approach relying on the distributional similarity between compounds and their constituents, to predict the degree of compositionality and thus to determine whether to split the compound with (B) a combination of morphological and frequency-based featu</context>
<context position="5909" citStr="Fritzinger and Fraser (2010)" startWordPosition="854" endWordPosition="857">esearch improved translation performance due to compound splitting. In Koehn and Knight (2003), compounds are split through the identification of substrings from a corpus. The splitting is performed without linguistic knowledge (except for the insertion of the filler letters “(e)s”), which necessarily leads to many erroneous splittings. Multiple possible splitting options are disambiguated using the frequencies of the substrings. Starting from Koehn and Knight (2003), Stymne (2008) covers more morphological transformations and imposes POS constraints on the subwords. Nießen and Ney (2000) and Fritzinger and Fraser (2010) perform compound splitting by relying on morphological analysers to identify suitable split points. This has the advantage of returning only linguistically motivated splitting options, but the analyses are often ambiguous and require disambiguation: Nießen and Ney (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word alignment process: first, word alignment is performed on a split representation of the compounding language. Then, all former compound parts </context>
<context position="21249" citStr="Fritzinger and Fraser, 2010" startWordPosition="3189" endWordPosition="3192"> which all noun compounds and particle verbs are split. The labels DIST and FREQ indicate how several possible splittings were disambiguated: DIST means we chose the splitting option having the higher geometric mean of the two compoundconstituent scores, assuming that the variant expressing a higher compositionality score leads to the more probable splitting analysis. For FREQ, the decision is based on the geometric mean of corpus frequencies of the respective components of the compound, as is common practise for the disambiguation of multiple splitting options in SMT (Koehn and Knight, 2003; Fritzinger and Fraser, 2010). In terms of BLEU, there is little difference for these two variants. For further experiments, we thus decided to always use FREQ for disambiguation, assuming that components chosen by frequency are potentially better repre4www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.html 85 rating compound gloss mod. head translation HIGHLY Staats|bankrott nation|bankruptcy 0.4779 0.6527 national bankruptcy COMP. Staats|gebilde nation|structure 0.6955 0.3431 national structure MEDIUM Industrie|staat industry|nation 0.0258 0.1488 industrial nation COMP. Staats|kasse nation|cash box 0.0718 0.2757</context>
</contexts>
<marker>Fritzinger, Fraser, 2010</marker>
<rawString>Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing. In Proceedings of the Fifth Workshop on Statistical Machine Translation, pages 224–234. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina L Gagn´e</author>
<author>Thomas L Spalding</author>
</authors>
<title>Constituent Integration during the Processing of Compound Words: Does it involve the Use of Relational Structures?</title>
<date>2009</date>
<journal>Journal of Memory and Language,</journal>
<pages>60--20</pages>
<marker>Gagn´e, Spalding, 2009</marker>
<rawString>Christina L. Gagn´e and Thomas L. Spalding. 2009. Constituent Integration during the Processing of Compound Words: Does it involve the Use of Relational Structures? Journal of Memory and Language, 60:20–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina L Gagn´e</author>
<author>Thomas L Spalding</author>
</authors>
<title>Inferential Processing and Meta-Knowledge as the Bases for Property Inclusion in Combined Concepts.</title>
<date>2011</date>
<journal>Journal of Memory and Language,</journal>
<pages>65--176</pages>
<marker>Gagn´e, Spalding, 2011</marker>
<rawString>Christina L. Gagn´e and Thomas L. Spalding. 2011. Inferential Processing and Meta-Knowledge as the Bases for Property Inclusion in Combined Concepts. Journal of Memory and Language, 65:176–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional Structure. In</title>
<date>1968</date>
<booktitle>The Philosophy of Linguistics, Oxford Readings in Philosophy,</booktitle>
<pages>26--47</pages>
<editor>Jerold J. Katz, editor,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7618" citStr="Harris, 1968" startWordPosition="1101" endWordPosition="1102">ly. However, cases like J¨agerzaun, ’lattice fence’ are not covered. In the present work, we identify compounds with a morphological analyser, disambiguated with corpus frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics. We are not aware of any previous work that takes semantics into account for compound splitting in SMT. 2.2 Distributional Semantics and Compounding Distributional information has been a steadily increasing, integral part of lexical semantic research over the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (200</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Zellig Harris. 1968. Distributional Structure. In Jerold J. Katz, editor, The Philosophy of Linguistics, Oxford Readings in Philosophy, pages 26–47. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>Kenlm: faster and smaller language model queries. In</title>
<date>2011</date>
<booktitle>EMNLP’11: Proceedings of the 6th workshop on statistical machine translation within the 8th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>187--197</pages>
<contexts>
<context position="17349" citStr="Heafield, 2011" startWordPosition="2555" endWordPosition="2556">rpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1BaumIschule: ‘tree|school’ (tree nursery) 2Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 84 use an English corpus of roughly 227 million words (including the English part of the parallel data) to build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM (Heafield, 2011). For parameter tuning, we use 1,025 sentences of news data. Standard Test set 1,026 sentences of news data (test set from the 2009 WMT Shared Task): this set is to measure the translation quality on a standard SMT test and make it comparable to other work. Noun/Verb Test set As our main focus lies on sentences containing compounds, we created a second test set which is rich in compounds. From the combined 2008-2013 Shared Task test sets, we extracted all sentences containing at least one noun compound for which we have compound-constituent similarity scores. Moreover, we excluded sentences co</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. Kenlm: faster and smaller language model queries. In EMNLP’11: Proceedings of the 6th workshop on statistical machine translation within the 8th Conference on Empirical Methods in Natural Language Processing, pages 187–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In EACL ’03: Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>187--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3036" citStr="Koehn and Knight, 2003" startWordPosition="442" endWordPosition="445">’) cannot be represented by the meanings of its constituents J¨ager (’hunter’) and Zaun (’fence’). Here, an erroneous splitting of the compound can lead to wrong generalizations or translation pairs, such as J¨ager → lattice, in the absence of other evidence about how to translate J¨ager. When splitting compounds for SMT, two important factors should thus be considered: (1) whether a compound is compositional and should be split, and if so (2) how the compound should be split. Most previous approaches mainly focused on the second task, how to split a compound, e.g. using frequency statistics (Koehn and Knight, 2003) or a rule-based morphology (Fritzinger and Fraser, 2010), and all of them showed improved SMT quality for compound splitting. The decision about whether the compound is compositional and should be split at all has not received much attention in the past. In this work, we examine the effect of only splitting compositional compounds, in contrast to splitting all compounds. To this end, we combine (A) an approach relying on the distributional similarity between compounds and their constituents, to predict the degree of compositionality and thus to determine whether to split the compound with (B)</context>
<context position="5375" citStr="Koehn and Knight (2003)" startWordPosition="780" endWordPosition="783">ompositional compounds, which can surprisingly often be translated component-wise. 2 Related Work We combine morphology-based compound splitting with distributional semantics to improve phrasebased SMT. Here, we discuss relevant work of compound splitting in SMT and distributional semantics. 2.1 Compound Splitting in SMT Compound splitting in SMT is a well-studied task. There is a wide range of previous work, including purely string- and frequency-based approaches, but also linguistically-informed approaches. All lines of research improved translation performance due to compound splitting. In Koehn and Knight (2003), compounds are split through the identification of substrings from a corpus. The splitting is performed without linguistic knowledge (except for the insertion of the filler letters “(e)s”), which necessarily leads to many erroneous splittings. Multiple possible splitting options are disambiguated using the frequencies of the substrings. Starting from Koehn and Knight (2003), Stymne (2008) covers more morphological transformations and imposes POS constraints on the subwords. Nießen and Ney (2000) and Fritzinger and Fraser (2010) perform compound splitting by relying on morphological analysers </context>
<context position="6707" citStr="Koehn and Knight (2003)" startWordPosition="970" endWordPosition="973">g options, but the analyses are often ambiguous and require disambiguation: Nießen and Ney (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word alignment process: first, word alignment is performed on a split representation of the compounding language. Then, all former compound parts for which there is no aligned counterpart in the non-compounding language are merged back to the compound again. Finally, word alignment is re-run on this representation. See Koehn and Knight (2003) for experiments on German, DeNeefe et al. (2008) for Arabic and Bai et al. (2008) for Chinese. This blocks non-compositional compounds from being split if they are translated as one simplex English word in the training data (e.g. Heckensch¨utze, lit. ’hedge|shooter’; ’sniper’) and aligned correctly. However, cases like J¨agerzaun, ’lattice fence’ are not covered. In the present work, we identify compounds with a morphological analyser, disambiguated with corpus frequencies. Moreover, we restrict splitting to compositional compounds using distributional semantics. We are not aware of any previ</context>
<context position="21219" citStr="Koehn and Knight, 2003" startWordPosition="3185" endWordPosition="3188">gressive split system in which all noun compounds and particle verbs are split. The labels DIST and FREQ indicate how several possible splittings were disambiguated: DIST means we chose the splitting option having the higher geometric mean of the two compoundconstituent scores, assuming that the variant expressing a higher compositionality score leads to the more probable splitting analysis. For FREQ, the decision is based on the geometric mean of corpus frequencies of the respective components of the compound, as is common practise for the disambiguation of multiple splitting options in SMT (Koehn and Knight, 2003; Fritzinger and Fraser, 2010). In terms of BLEU, there is little difference for these two variants. For further experiments, we thus decided to always use FREQ for disambiguation, assuming that components chosen by frequency are potentially better repre4www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.html 85 rating compound gloss mod. head translation HIGHLY Staats|bankrott nation|bankruptcy 0.4779 0.6527 national bankruptcy COMP. Staats|gebilde nation|structure 0.6955 0.3431 national structure MEDIUM Industrie|staat industry|nation 0.0258 0.1488 industrial nation COMP. Staats|kasse</context>
<context position="23648" citStr="Koehn and Knight (2003)" startWordPosition="3521" endWordPosition="3524">tly positive effect for particle verbs. Having a closer look, we find that for noun compounds on the standard test set, the best results (threshold: 0.1) are at the same level as the aggressive split systems; with some small losses in BLEU on some of the other settings. 5.2 Discussion All settings clearly outperform the baseline system (without compound processing). This indicates that phrase-based SMT is rather robust with regard to non-semantic splitting as it is can often recover from over-splitting by translating the word sequence as a phrase. This is in line with previous observations of Koehn and Knight (2003). The results for the noun test set, which is biased towards containing more nominal compounds, even suggests that less splitting might harm the system, as the BLEU scores tend to drop when increasing the threshold. For particle verbs,5 the picture is slightly different: first, splitting only particle verbs does not lead to a considerable improvement over the baseline, as in the case of noun compounds. For the verb test set, it even leads to a drop in BLEU. However, a more restricted splitting leads to improved BLEU scores, even though not significantly better than the un-split baseline system</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In EACL ’03: Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 187–193, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In ACL’07: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Session,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="16367" citStr="Koehn et al., 2007" startWordPosition="2404" endWordPosition="2407"> Walde et al., 2013). Furthermore, we compare the effects of splitting with regard to two types of compounds, noun compounds and particle verbs: Both types are very productive and can generate a potentially infinite number of new forms. 4 Experimental Setting This section gives an overview on the technical details of the SMT system and our data sets. Compound splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007). We use it with default settings to train a translation model and we do so separately for each of the different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Feature weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence. Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1BaumIschule: ‘tree|school’ (tree nursery) 2Compounds not contained in the parallel data are </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL’07: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Session, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: a parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit’05: Proceedings of the 10th machine translation summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="16767" citStr="Koehn (2005)" startWordPosition="2469" endWordPosition="2470">ata, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007). We use it with default settings to train a translation model and we do so separately for each of the different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Feature weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence. Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1BaumIschule: ‘tree|school’ (tree nursery) 2Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 84 use an English corpus of roughly 227 million words (including the English part of the parallel data) to build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM (Heafield, 2011). For parameter tu</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit’05: Proceedings of the 10th machine translation summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie K¨uhner</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Determining the Degree of Compositionality of German Particle Verbs by Clustering Approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th Conference on Natural Language Processing,</booktitle>
<pages>47--56</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>K¨uhner, Walde, 2010</marker>
<rawString>Natalie K¨uhner and Sabine Schulte im Walde. 2010. Determining the Degree of Compositionality of German Particle Verbs by Clustering Approaches. In Proceedings of the 10th Conference on Natural Language Processing, pages 47–56, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Lechler</author>
<author>Antje RoBdeutscher</author>
</authors>
<title>German Particle Verbs with auf. Reconstructing their Composition in a DRT-based Framework. Linguistische Berichte,</title>
<date>2009</date>
<pages>220</pages>
<contexts>
<context position="11184" citStr="Lechler and RoBdeutscher (2009)" startWordPosition="1617" endWordPosition="1620">. Noun compounds are formed of a head noun and a modifier, which can consist of nouns, verbs, adjectives or proper nouns. Particle verbs are productive compositions of a base verb and a prefix particle, whose part-of-speech varies between open-class nouns, adjectives, and verbs, and closed-class prepositions and adverbs. In comparison to noun compounds, the constituents of German particle verbs exhibit a much higher degree of ambiguity: Verbs in general are more ambiguous than nouns, and the largest sub-class of particles (those with a preposition particle) is highly ambiguous by itself (e.g. Lechler and RoBdeutscher (2009) and Springorum (2011)). For example, in anknabbern (‘to nibble partially’), the particle an expresses a partitive meaning , whereas in ankleben (‘to glue onto sth.’) an has a topological meaning (to glue sth. onto an implicit background). In addition, particle verb senses may be transparent or opaque with respect to their base verbs. For example, abholen ‘fetch’ is rather transparent with respect to its base verb holen ‘fetch’, whereas anfangen ‘begin’ is more opaque with respect to fangen ‘catch’. In contrast, einsetzen has both transparent (e.g. ‘insert’) and opaque (e.g. ‘begin’) verb sens</context>
</contexts>
<marker>Lechler, RoBdeutscher, 2009</marker>
<rawString>Andrea Lechler and Antje RoBdeutscher. 2009. German Particle Verbs with auf. Reconstructing their Composition in a DRT-based Framework. Linguistische Berichte, 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a Continuum of Compositionality in Phrasal Verbs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>73--80</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="8220" citStr="McCarthy et al. (2003)" startWordPosition="1190" endWordPosition="1193"> 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 Jägerzaun Holzzaun Input Step 1: Identify Component Words Holz Zaun Jäger Zaun Preprocessing Step 2: Predict Similarity Scores 0.311 0.825 0.015 0.72</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a Continuum of Compositionality in Phrasal Verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, pages 73–80, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja NieBen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In COLING’00: Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>1081--1085</pages>
<publisher>Morgan Kaufmann.</publisher>
<marker>NieBen, Ney, 2000</marker>
<rawString>Sonja NieBen and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In COLING’00: Proceedings of the 18th International Conference on Computational Linguistics, pages 1081–1085. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="16561" citStr="Och and Ney, 2003" startWordPosition="2437" endWordPosition="2440">potentially infinite number of new forms. 4 Experimental Setting This section gives an overview on the technical details of the SMT system and our data sets. Compound splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based SMT systems (Koehn et al., 2007). We use it with default settings to train a translation model and we do so separately for each of the different compound splittings. Word alignment is performed using GIZA++ (Och and Ney, 2003). Feature weights are tuned using Batch-Mira (Cherry and Foster, 2012) with ’-safe-hope’ until convergence. Training Data Our parallel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1BaumIschule: ‘tree|school’ (tree nursery) 2Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 84 use an English corpus of ro</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL’02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="19426" citStr="Papineni et al., 2002" startWordPosition="2886" endWordPosition="2889"> per compound = 70 in total) from German newspaper data4. In contrast to the other sets, we use this test set in a qualitative study, to approximate the translation quality by counting the number of correctly translated compounds. 5 SMT Results In this section, we present and discuss the results of our machine translation experiments. We first report results for two test sets in terms of a standard evaluation metric (BLEU) and then continue with a smallscale qualitative study on the translational behaviour of non-compositional compounds. 5.1 Compound Splitting within a Standard SMT Task BLEU (Papineni et al., 2002) is a common metric to automatically measure the quality of SMT output by comparing n-gram matches of the SMT output with a human reference translation. Table 1 lists the results for our SMT-systems: we report on different compound-constituent scores and thresholds, for noun compounds and particle verbs respectively. Note that BLEU scores are not comparable across difnouns particle verbs stand. noun stand. verb baseline 21.00 21.08 21.00 20.29 DIST 22.00 22.02 21.02 20.11 FREQ 22.04 21.88 21.11 20.21 head 21.77 21.58 – – 50 mod. 22.01 21.74 – – 0. geom. 21.99 21.71 – – arith. 21.95 21.95 – – h</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In ACL’02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>An Empirical Study on Compositionality in Compound Nouns.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>210--218</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="8116" citStr="Reddy et al. (2011)" startWordPosition="1172" endWordPosition="1175">t of lexical semantic research over the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 Jägerzaun Holzzaun Input Step 1: Identify Com</context>
<context position="15736" citStr="Reddy et al., 2011" startWordPosition="2302" endWordPosition="2305">milarity (mod); (ii) only the compound–head similarity (head); a combination of the compound–modifier and the compound–head similarities, relying on (iii) the geometric mean (geom) or (iv) on the arithmetic mean (arith). We used different thresholds for each of these criteria throughout our experiments, with a specific focus on distinguishing the contributions of the modifiers vs. the heads in the splitting decision, following insights from recent work in psycholinguistic studies (Gagn´e and Spalding, 2009; Gagn´e and Spalding, 2011) as well as in computational approaches on noun compounding (Reddy et al., 2011; Schulte im Walde et al., 2013). Furthermore, we compare the effects of splitting with regard to two types of compounds, noun compounds and particle verbs: Both types are very productive and can generate a potentially infinite number of new forms. 4 Experimental Setting This section gives an overview on the technical details of the SMT system and our data sets. Compound splitting is applied to all source-language data, i.e. the parallel data used to train the model, as well as the input for parameter tuning and testing.2 Translation Model Moses is a state-of-the-art toolkit for phrase-based S</context>
</contexts>
<marker>Reddy, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An Empirical Study on Compositionality in Compound Nouns. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bahar Salehi</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Using Distributional Similarity of Multi-Way Translations to Predict Multiword Expression Compositionality.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="8170" citStr="Salehi et al. (2014)" startWordPosition="1182" endWordPosition="1185">. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 Jägerzaun Holzzaun Input Step 1: Identify Component Words Holz Zaun Jäger Zaun Preprocessing Step 2</context>
<context position="9748" citStr="Salehi et al. (2014)" startWordPosition="1410" endWordPosition="1413">t not in L¨owenzahn–Zahn (‘lion|tooth (dandelion)’– ‘tooth’)) is also similar to the constituent with regard to co-occurrence information. Most related to this work on noun compounds, Reddy et al. (2011) relied on window-based distributional models to predict the compositionality of English noun compounds, and Schulte im Walde et al. (2013) compared window-based against syntax-based distributional models to predict the compositionality of German noun compounds. Zinsmeister and Heid (2004) used subcategorising verbs to predict compound–head similarities of German noun compounds. Most recently, Salehi et al. (2014) extended the previous approaches to take multi-lingual co-occurrence information into account, regarding English and German noun compounds, and English particle verbs. 3 Methodology We integrate our semantically-informed compound splitting as a pre-processing step to the German source language of an SMT system. See Figure 1 for an illustration of our compound processing pipeline. 3.1 Target Compounds German compounds are combinations of two (or more) simplex words. In some cases, a morphological transformation is required: for example, when combining the two nouns Ausflug (‘excursion’) and Zi</context>
</contexts>
<marker>Salehi, Cook, Baldwin, 2014</marker>
<rawString>Bahar Salehi, Paul Cook, and Timothy Baldwin. 2014. Using Distributional Similarity of Multi-Way Translations to Predict Multiword Expression Compositionality. In Proceedings of EACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building Large Corpora from the Web Using a New Efficient Tool Chain.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation,</booktitle>
<pages>486--493</pages>
<location>Istanbul, Turkey.</location>
<marker>Sch¨afer, Bildhauer, 2012</marker>
<rawString>Roland Sch¨afer and Felix Bildhauer. 2012. Building Large Corpora from the Web Using a New Efficient Tool Chain. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 486–493, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Arne Fitschen</author>
<author>Ulrich Heid</author>
</authors>
<title>Smor: A German computational morphology covering derivation, composition and inflection.</title>
<date>2004</date>
<booktitle>In LREC ’04: Proceedings of the 4th Conference on Language Resources and Evaluation,</booktitle>
<pages>1263--1266</pages>
<contexts>
<context position="12235" citStr="Schmid et al., 2004" startWordPosition="1780" endWordPosition="1783">, whereas anfangen ‘begin’ is more opaque with respect to fangen ‘catch’. In contrast, einsetzen has both transparent (e.g. ‘insert’) and opaque (e.g. ‘begin’) verb senses with respect to setzen ‘put/sit (down)’. The high degree of ambiguity makes particle verbs a challenge for NLP. Moreover, particle and base verb can occur separately (er f¨angt an: ‘he begins’) or in one word (dass er anf¨angt: ‘that he begins’), depending on the clausal type. This makes consistent treatment of particle verbs difficult. 3.2 Identification of Component Parts We use the rule-based morphological analyser SMOR (Schmid et al., 2004) to identify compounds and their constituents in our parallel training data (cf. Section 4). It relies on a large lexicon of word lemmas and feature rules for productive morphological processes in German, i.e., compounding, derivation and 83 inflection. In this paper, we will not consider splitting into derivational affixes (as needed for, e.g., Arabic and Turkish), but instead identify simplex words that may also occur independently. Moreover, we only keep noun compounds and particle verbs consisting of two constituents. The resulting set consists of 93,299 noun compound types and 3,689 parti</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. Smor: A German computational morphology covering derivation, composition and inflection. In LREC ’04: Proceedings of the 4th Conference on Language Resources and Evaluation, pages 1263–1266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Stefan M¨uller</author>
<author>Stephen Roller</author>
</authors>
<title>Exploring Vector Space Models to Predict the Compositionality of German Noun-Noun Compounds.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>255--265</pages>
<location>Atlanta, GA.</location>
<marker>Walde, M¨uller, Roller, 2013</marker>
<rawString>Sabine Schulte im Walde, Stefan M¨uller, and Stephen Roller. 2013. Exploring Vector Space Models to Predict the Compositionality of German Noun-Noun Compounds. In Proceedings of the 2nd Joint Conference on Lexical and Computational Semantics, pages 255–265, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvia Springorum</author>
</authors>
<title>DRT-based Analysis of the German Verb Particle ”an”. Leuvense Bijdragen,</title>
<date>2011</date>
<pages>97--80</pages>
<contexts>
<context position="11206" citStr="Springorum (2011)" startWordPosition="1622" endWordPosition="1623">d noun and a modifier, which can consist of nouns, verbs, adjectives or proper nouns. Particle verbs are productive compositions of a base verb and a prefix particle, whose part-of-speech varies between open-class nouns, adjectives, and verbs, and closed-class prepositions and adverbs. In comparison to noun compounds, the constituents of German particle verbs exhibit a much higher degree of ambiguity: Verbs in general are more ambiguous than nouns, and the largest sub-class of particles (those with a preposition particle) is highly ambiguous by itself (e.g. Lechler and RoBdeutscher (2009) and Springorum (2011)). For example, in anknabbern (‘to nibble partially’), the particle an expresses a partitive meaning , whereas in ankleben (‘to glue onto sth.’) an has a topological meaning (to glue sth. onto an implicit background). In addition, particle verb senses may be transparent or opaque with respect to their base verbs. For example, abholen ‘fetch’ is rather transparent with respect to its base verb holen ‘fetch’, whereas anfangen ‘begin’ is more opaque with respect to fangen ‘catch’. In contrast, einsetzen has both transparent (e.g. ‘insert’) and opaque (e.g. ‘begin’) verb senses with respect to set</context>
</contexts>
<marker>Springorum, 2011</marker>
<rawString>Sylvia Springorum. 2011. DRT-based Analysis of the German Verb Particle ”an”. Leuvense Bijdragen, 97:80– 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modelling toolkit.</title>
<date>2002</date>
<booktitle>In ICSLN’02: Proceedings of the international conference on spoken language processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="17306" citStr="Stolcke, 2002" startWordPosition="2549" endWordPosition="2550">lel training data contains the Europarl corpus (version 4, cf. Koehn (2005)) and also newspaper texts, overall ca. 1.5 million sentences3 (roughly 44 million words). In addition, we 1BaumIschule: ‘tree|school’ (tree nursery) 2Compounds not contained in the parallel data are always split, as they cannot be translated otherwise. 3Data from the shared task of the EACL 2009 workshop on statistical machine translation: www.statmt.org/wmt09 84 use an English corpus of roughly 227 million words (including the English part of the parallel data) to build a target-side 5-gram language model with SRILM (Stolcke, 2002) in combination with KENLM (Heafield, 2011). For parameter tuning, we use 1,025 sentences of news data. Standard Test set 1,026 sentences of news data (test set from the 2009 WMT Shared Task): this set is to measure the translation quality on a standard SMT test and make it comparable to other work. Noun/Verb Test set As our main focus lies on sentences containing compounds, we created a second test set which is rich in compounds. From the combined 2008-2013 Shared Task test sets, we extracted all sentences containing at least one noun compound for which we have compound-constituent similarity</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modelling toolkit. In ICSLN’02: Proceedings of the international conference on spoken language processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>German compounds in factored statistical machine translation.</title>
<date>2008</date>
<booktitle>In GoTAL ’08: Proceedings of the 6th International Conference on Natural Language Processing,</booktitle>
<pages>464--475</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="5767" citStr="Stymne (2008)" startWordPosition="836" endWordPosition="837">ious work, including purely string- and frequency-based approaches, but also linguistically-informed approaches. All lines of research improved translation performance due to compound splitting. In Koehn and Knight (2003), compounds are split through the identification of substrings from a corpus. The splitting is performed without linguistic knowledge (except for the insertion of the filler letters “(e)s”), which necessarily leads to many erroneous splittings. Multiple possible splitting options are disambiguated using the frequencies of the substrings. Starting from Koehn and Knight (2003), Stymne (2008) covers more morphological transformations and imposes POS constraints on the subwords. Nießen and Ney (2000) and Fritzinger and Fraser (2010) perform compound splitting by relying on morphological analysers to identify suitable split points. This has the advantage of returning only linguistically motivated splitting options, but the analyses are often ambiguous and require disambiguation: Nießen and Ney (2000) use a parser for context-sensitive disambiguation, and Fritzinger and Fraser (2010) use corpus frequencies to find the best split for each compound. Other approaches use a two-step word</context>
</contexts>
<marker>Stymne, 2008</marker>
<rawString>Sara Stymne. 2008. German compounds in factored statistical machine translation. In GoTAL ’08: Proceedings of the 6th International Conference on Natural Language Processing, pages 464–475. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia von der Heide</author>
<author>Susanne Borgwaldt</author>
</authors>
<title>Assoziationen zu Unter, Basis und Oberbegriffen.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium,</booktitle>
<pages>51--74</pages>
<marker>von der Heide, Borgwaldt, 2009</marker>
<rawString>Claudia von der Heide and Susanne Borgwaldt. 2009. Assoziationen zu Unter, Basis und Oberbegriffen. In Proceedings of the 9th Norddeutsches Linguistisches Kolloquium, pages 51–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Zinsmeister</author>
<author>Ulrich Heid</author>
</authors>
<title>Collocations of Complex Nouns: Evidence for Lexicalisation.</title>
<date>2004</date>
<booktitle>In Proceedings of Konvens,</booktitle>
<location>Vienna, Austria.</location>
<contexts>
<context position="8095" citStr="Zinsmeister and Heid (2004)" startWordPosition="1168" endWordPosition="1171">dily increasing, integral part of lexical semantic research over the past 20 years. Based on the distributional hypothesis (Firth, 1957; Harris, 1968) that ”you shall know a word by the company it keeps”, distributional semantics exploits the co-occurrence of words in corpora to explore the meanings and the similarities of the words, phrases, sentences, etc. of interest. Among many other tasks, distributional semantic information has been utilised to determine the degree of compositionality (or: semantic transparency) of various types of compounds, most notably regarding noun compounds (e.g., Zinsmeister and Heid (2004), Reddy et al. (2011), Schulte im Walde et al. (2013), Salehi et al. (2014)) and particle verbs (e.g., McCarthy et al. (2003), Bannard (2005), Cook and Stevenson (2006), K¨uhner and Schulte im Walde (2010), Bott and Schulte im Walde (2014), Salehi et al. (2014)). Typically, these approaches rely on co-occurrence information from a corpus (either referring to bagsof-words, or focusing on target-specific types of features), and compare the distributional features of the compounds with those of the constituents, in order to predict the degree of compositionality of the 82 Jägerzaun Holzzaun Input</context>
<context position="9621" citStr="Zinsmeister and Heid (2004)" startWordPosition="1393" endWordPosition="1396">underlying assumption is that a compound which is similar in meaning to a constituent (as in Holzzaun–Zaun (‘wooden fence’–‘fence’) but not in L¨owenzahn–Zahn (‘lion|tooth (dandelion)’– ‘tooth’)) is also similar to the constituent with regard to co-occurrence information. Most related to this work on noun compounds, Reddy et al. (2011) relied on window-based distributional models to predict the compositionality of English noun compounds, and Schulte im Walde et al. (2013) compared window-based against syntax-based distributional models to predict the compositionality of German noun compounds. Zinsmeister and Heid (2004) used subcategorising verbs to predict compound–head similarities of German noun compounds. Most recently, Salehi et al. (2014) extended the previous approaches to take multi-lingual co-occurrence information into account, regarding English and German noun compounds, and English particle verbs. 3 Methodology We integrate our semantically-informed compound splitting as a pre-processing step to the German source language of an SMT system. See Figure 1 for an illustration of our compound processing pipeline. 3.1 Target Compounds German compounds are combinations of two (or more) simplex words. In</context>
</contexts>
<marker>Zinsmeister, Heid, 2004</marker>
<rawString>Heike Zinsmeister and Ulrich Heid. 2004. Collocations of Complex Nouns: Evidence for Lexicalisation. In Proceedings of Konvens, Vienna, Austria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
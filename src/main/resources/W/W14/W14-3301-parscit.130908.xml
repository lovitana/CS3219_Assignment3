<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.6876075">
Efficient Elicitation of Annotations for Human Evaluation of Machine
Translation
</title>
<author confidence="0.707807">
Keisuke Sakaguchi*, Matt Post†, Benjamin Van Durme†
</author>
<affiliation confidence="0.606502666666667">
* Center for Language and Speech Processing
† Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland
</affiliation>
<email confidence="0.999127">
{keisuke,post,vandurme}@cs.jhu.edu
</email>
<sectionHeader confidence="0.994804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917192307692">
A main output of the annual Workshop
on Statistical Machine Translation (WMT)
is a ranking of the systems that partici-
pated in its shared translation tasks, pro-
duced by aggregating pairwise sentence-
level comparisons collected from human
judges. Over the past few years, there
have been a number of tweaks to the ag-
gregation formula in attempts to address
issues arising from the inherent ambigu-
ity and subjectivity of the task, as well as
weaknesses in the proposed models and
the manner of model selection.
We continue this line of work by adapt-
ing the TrueSkillTM algorithm — an online
approach for modeling the relative skills
of players in ongoing competitions, such
as Microsoft’s Xbox Live — to the hu-
man evaluation of machine translation out-
put. Our experimental results show that
TrueSkill outperforms other recently pro-
posed models on accuracy, and also can
significantly reduce the number of pair-
wise annotations that need to be collected
by sampling non-uniformly from the space
of system competitions.
</bodyText>
<sectionHeader confidence="0.998876" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999481727272727">
The Workshop on Statistical Machine Translation
(WMT) has long been a central event in the ma-
chine translation (MT) community for the evalua-
tion of MT output. It hosts an annual set of shared
translation tasks focused mostly on the translation
of western European languages. One of its main
functions is to publish a ranking of the systems
for each task, which are produced by aggregating
a large number of human judgments of sentence-
level pairwise rankings of system outputs. While
the performance on many automatic metrics is also
</bodyText>
<table confidence="0.999840214285714">
# score range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.571 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
</table>
<tableCaption confidence="0.99413">
Table 1: System rankings presented as clusters
</tableCaption>
<bodyText confidence="0.99179925">
(WMT13 French-English competition). The score
column is the percentage of time each system was
judged better across its comparisons (§2.1).
reported (e.g., BLEU (Papineni et al., 2002)), the
human evaluation is considered primary, and is in
fact used as the gold standard for its metrics task,
where evaluation metrics are evaluated.
In machine translation, the longstanding dis-
agreements about evaluation measures do not go
away when moving from automatic metrics to hu-
man judges. This is due in no small part to the in-
herent ambiguity and subjectivity of the task, but
also arises from the particular way that the WMT
organizers produce the rankings. The system-
level rankings are produced by collecting pairwise
sentence-level comparisons between system out-
puts. These are then aggregated to produce a com-
plete ordering of all systems, or, more recently, a
partial ordering (Koehn, 2012), with systems clus-
tered where they cannot be distinguished in a sta-
tistically significant way (Table 1, taken from Bo-
jar et al. (2013)).
A number of problems have been noted with
this approach. The first has to do with the na-
ture of ranking itself. Over the past few years, the
WMT organizers have introduced a number of mi-
nor tweaks to the ranking algorithm (§2) in reac-
tion to largely intuitive arguments that have been
</bodyText>
<page confidence="0.827707">
1
</page>
<note confidence="0.718707">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1–11,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999926893939394">
raised about how the evaluation is conducted (Bo-
jar et al., 2011; Lopez, 2012). While these tweaks
have been sensible (and later corroborated), Hop-
kins and May (2013) point out that this is essen-
tially a model selection task, and should prop-
erly be driven by empirical performance on held-
out data according to some metric. Instead of in-
tuition, they suggest perplexity, and show that a
novel graphical model outperforms existing ap-
proaches on that metric, with less amount of data.
A second problem is the deficiency of the mod-
els used to produce the ranking, which work by
computing simple ratios of wins (and, option-
ally, ties) to losses. Such approaches do not con-
sider the relative difficulty of system matchups,
and thus leave open the possibility that a system
is ranked highly from the luck of comparisons
against poorer opponents.
Third, a large number of judgments need to be
collected in order to separate the systems into clus-
ters to produce a partial ranking. The sheer size of
the space of possible comparisons (all pairs of sys-
tems times the number of segments in the test set)
requires sampling from this space and distributing
the annotations across a number of judges. Even
still, the number of judgments needed to produce
statistically significant rankings like those in Ta-
ble 1 grows quadratically in the number of par-
ticipating systems (Koehn, 2012), often forcing
the use of paid, lower-quality annotators hired on
Amazon’s Mechanical Turk. Part of the prob-
lem is that the sampling strategy collects data uni-
formly across system pairings. Intuitively, we
should need many fewer annotations between sys-
tems with divergent base performance levels, in-
stead focusing the collection effort on system pairs
whose performance is more matched, in order to
tease out the gaps between similarly-performing
systems. Why spend precious human time on re-
dundantly affirming predictable outcomes?
To address these issues, we developed a varia-
tion of the TrueSkill model (Herbrich et al., 2006),
an adaptative model of competitions originally de-
veloped for the Xbox Live online gaming commu-
nity. It assumes that each player’s skill level fol-
lows a Gaussian distribution N(µ, Q2), in which
µ represents a player’s mean performance, and Q2
the system’s uncertainty about its current estimate
of this mean. These values are updated after each
“game” (in our case, the value of a ternary judg-
ment) in proportion to how surprising the outcome
is. TrueSkill has been adapted to a number of
areas, including chess, advertising, and academic
conference management.
The rest of this paper provides an empirical
comparison of a number of models of human eval-
uation (§2). We evaluate on perplexity and also
on accuracy, showing that the two are not always
correlated, and arguing for the primacy of the lat-
ter (§3). We find that TrueSkill outperforms other
models (§4). Moreover, TrueSkill also allows us to
drastically reduce the amount of data that needs to
be collected by sampling non-uniformly from the
space of all competitions (§5), which also allows
for greater separation of the systems into ranked
clusters (§6).
</bodyText>
<sectionHeader confidence="0.986342" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999973777777778">
Before introducing our adaptation of the TrueSkill
model for ranking translation systems with human
judgments (§2.3), we describe two comparisons:
the “Expected Wins” model used in recent evalu-
ations, and the Bayesian model proposed by Hop-
kins and May (§2.2).
As we described briefly in the introduction,
WMT produces system rankings by aggregating
sentence-level ternary judgments of the form:
</bodyText>
<equation confidence="0.955522">
(i, S1, S2, 7r)
</equation>
<bodyText confidence="0.9999697">
where i is the source segment (id), S1 and S2
are the system pair drawn from a set of systems
{S}, and 7r E {&lt;, &gt;, =} denotes whether the
first system was judged to be better than, worse
than, or equivalent to the second. These ternary
judgments are obtained by presenting judges with
a randomly-selected input sentence and the refer-
ence, followed by five randomly-selected transla-
tions of that sentence. Annotators are asked to
rank these systems from best (rank 1) to worst
(rank 5), ties permitted, and with no meaning as-
cribed to the absolute values or differences be-
tween ranks. This is done to accelerate data collec-
tion, since it yields ten pairwise comparisons per
ranking. Tens of thousands of judgments of this
form constitute the raw data used to compute the
system-level rankings. All the work described in
this section is computed over these pairwise com-
parisons, which are treated as if they were col-
lected independently.
</bodyText>
<subsectionHeader confidence="0.949326">
2.1 Expected Wins
</subsectionHeader>
<bodyText confidence="0.999198">
The “Expected Wins” model computes the per-
centage of times that each system wins in its
</bodyText>
<page confidence="0.988279">
2
</page>
<bodyText confidence="0.996133727272727">
pairwise comparisons. Let A be the complete
set of annotations or judgments of the form
{i, S1, S2, πR}. We assume these judgments have
been converted into a normal form where S1 is ei-
ther the winner or is tied with S2, and therefore
πR ∈ {&lt;, =}. Let δ(x, y) be the Kronecker delta
function.1 We then define the function:
which counts the number of annotations for which
system Si was ranked better than system Sj. We
define a single-variable version that marginalizes
over all annotations:
</bodyText>
<equation confidence="0.9902575">
wins(Si) = X wins(Si, Sj)
Sj7�Si
</equation>
<bodyText confidence="0.99965875">
We also define analogous functions for loses and
ties. Until the WMT11 evaluation (Callison-Burch
et al., 2011), the score for each system Si was
computed as follows:
</bodyText>
<equation confidence="0.999954">
wins(Si) + ties(Si)
score(Si) = wins(Si) + ties(Si) + loses(Si)
</equation>
<bodyText confidence="0.998028166666667">
Bojar et al. (2011) suggested that the inclusion of
ties biased the results, due to their large numbers,
the underlying similarity of many of the models,
and the fact that they are counted for both systems
in the tie, and proposed the following modified
scoring function:
</bodyText>
<equation confidence="0.9714845">
1 wins (Si, Sj)
score(Si) = |{S} |SYi wins(Si, Sj) + wins(Sj, Si)
</equation>
<bodyText confidence="0.999338769230769">
This metric computes an average relative fre-
quency of wins, excluding ties, and was used
in WMT12 and WMT13 (Callison-Burch et al.,
2012; Bojar et al., 2013).
The decision to exclude ties isn’t without
its problems; for example, an evaluation where
two systems are nearly always judged equivalent
should be relevant in producing the final ranking
of systems. Furthermore, as Hopkins and May
(2013) point out, throwing out data to avoid bi-
asing a model suggests a problem with the model.
We now turn to a description of their model, which
addresses these problems.
</bodyText>
<equation confidence="0.992122">
1δ(x, r 1 if x = y
y) = 1 0 o.w.
</equation>
<subsectionHeader confidence="0.998615">
2.2 The Hopkins and May (2013) model
</subsectionHeader>
<bodyText confidence="0.9997685">
Recent papers (Koehn, 2012; Hopkins and May,
2013) have proposed models focused on the rel-
ative ability of the competition systems. These
approaches assume that each system has a mean
quality represented by a Gaussian distribution with
a fixed variance shared across all systems. In the
graphical model formulation of Hopkins and May
(2013), the pairwise judgments (i, S1, S2, π) are
imagined to have been generated according to the
following process:
</bodyText>
<listItem confidence="0.999369">
• Select a source sentence i
• Select two systems S1 and S2. A system
Sj is associated with a Gaussian distribution
N(µSj,σ2a), samples from which represent
the quality of translations
• Draw two “translations”, adding random
Gaussian noise with variance σ2obs to simulate
the subjectivity of the task and the differences
among annotators:
</listItem>
<equation confidence="0.95959375">
22
q1 ∼ N(µS1, σa)
+ N(0, σobs)
q2 ∼ N (µS2, σ2a) + N(0, σ2obs)
</equation>
<listItem confidence="0.790355166666667">
• Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating π
according to:2
&lt; q1 − q2 &gt; d
&gt; q2 − q1 &gt; d
= otherwise
</listItem>
<bodyText confidence="0.999939266666667">
The task is to then infer the posterior parameters,
given the data: the system means µSj and, by ne-
cessity, the latent values {qi} for each of the pair-
wise comparison training instances. Hopkins and
May do not publish code or describe details of this
algorithm beyond mentioning Gibbs sampling, so
we used our own implementation,3 and describe it
here for completeness.
After initialization, we have training instances
of the form (i, S1, S2, πR, q1, q2), where all but the
qi are observed. At a high level, the sampler iter-
ates over the training data, inferring values of q1
and q2 for each annotation together in a single step
of the sampler from the current values of the sys-
tems means, {µj}.4 At the end of each iteration,
</bodyText>
<footnote confidence="0.704127">
2Note that better systems have higher relative abilities
{µSj }. Better translations subsequently have on-average
higher values {qi}, which translate into a lower ranking ir.
3github.com/keisks/wmt-trueskill
4This worked better than a version of the sampler that
changed one at a time.
</footnote>
<equation confidence="0.985982333333333">
wins(Si, Sj) =
X JAI
n=1
δ(Si, S(n)
1 )δ(Sj, S2n))δ(πRn), &lt;)
⎧
⎨
⎩
π =
</equation>
<page confidence="0.949567">
3
</page>
<bodyText confidence="0.994653333333333">
these means are then recomputed by re-averaging
all values of {qi} associated with that system. Af-
ter the burn-in period, the µs are stored as samples,
which are averaged when the sampling concludes.
During each iteration, q1 and q2 are resampled
from their corresponding system means:
</bodyText>
<equation confidence="0.9992515">
q1 _ N(µS1, σ2a)
q2 _ N(µS2, σ2a)
</equation>
<bodyText confidence="0.997798">
We then update these values to respect the annota-
tion π as follows. Let t = q1−q2 (S1 is the winner
by human judgments), and ensure that the values
are outside the decision radius, d:
In the case of a tie:
</bodyText>
<equation confidence="0.9991672">
1
{ q1 + 2(d − t) t &gt; d
q1 t &lt; d
1
q1 + 2(−d − t) t &lt; −d
</equation>
<bodyText confidence="0.9999085">
These values are stored for the current iteration
and averaged at its end to produce new estimates
of the system means. The quantity d − t can be in-
terpreted as a loss function, returning a high value
when the observed outcome is unexpected and a
low value otherwise (Figure 1).
</bodyText>
<subsectionHeader confidence="0.993653">
2.3 TrueSkill
</subsectionHeader>
<bodyText confidence="0.999981795454545">
Prior to 2012, the WMT organizers included refer-
ence translations among the system comparisons.
These were used as a control against which the
evaluators could be measured for consistency, on
the assumption that the reference was almost al-
ways best. They were also included as data points
in computing the system ranking. Another of
Bojar et al. (2011)’s suggestions was to exclude
this data, because systems compared more of-
ten against the references suffered unfairly. This
can be further generalized to the observation that
not all competitions are equal, and a good model
should incorporate some notion of “match diffi-
culty” when evaluating system’s abilities. The
inference procedure above incorporates this no-
tion implicitly in the inference procedure, but the
model itself does not include a notion of match
difficulty or outcome surprisal.
A model that does is TrueSkill5 (Herbrich et al.,
2006). TrueSkill is an adaptive, online system that
also assumes that each system’s skill level follows
a Gaussian distribution, maintaining a mean µSj
for each system Sj representing its current esti-
mate of that system’s native ability. However, it
also maintains a per-system variance, σ2Sj, which
represents TrueSkill’s uncertainty about its esti-
mate of each mean. After an outcome is observed
(a game in which the result is a win, loss, or draw),
the size of the updates is proportional to how sur-
prising the outcome was, which is computed from
the current system means and variances. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates.
Before defining the update equations, we need
to be more concrete about how this notion of sur-
prisal is incorporated. Let t = µS1 − µS2, the dif-
ference in system relative abilities, and let c be a
fixed hyper-parameter corresponding to the earlier
decision radius. We then define two loss functions
of this difference for wins and for ties:
</bodyText>
<equation confidence="0.999025666666667">
N(−� + t)
vwin(t, �) =
Φ(−c + t)
N(−� − t) − N(~ − t)
vtie(t, �) =
Φ(E − t) − Φ(−E − t)
</equation>
<bodyText confidence="0.993261615384616">
where Φ(x) is the cumulative distribution function
and the Ns are Gaussians. Figures 1 and 2 display
plots of these two functions compared to the Hop-
kins and May model. Note how vwin (Figure 1) in-
creases exponentially as µS2 becomes greater than
the (purportedly) better system, µS1.
As noted above, TrueSkill maintains not only
estimates {µSj} of system abilities, but also
system-specific confidences about those estimates
5The goal of this section is to provide an intuitive descrip-
tion of TrueSkill as adapted for WMT manual evaluations,
with enough detail to carry the main ideas. For more details,
please see Herbrich et al. (2006).
</bodyText>
<equation confidence="0.993309461538461">
� q2 t &gt; d
q2 =1
q2 − 2(d − t) otherwise
� q1 t &gt; d
q1 + 1(d − t) otherw
q�1 ise
2
q1 =
q2 =
q2 − 12(d−t) t &gt; d
q2 t &lt; d
q2 − 12(−d − t) t &lt; −d
{
</equation>
<page confidence="0.888867">
4
</page>
<figure confidence="0.973204">
−1.0 −0.5 0.0 0.5 1.0
t = µS, − µS
v(t, E)
0.5
0.0
1.5
1.0
T ueSHl
HM
</figure>
<figureCaption confidence="0.8336604">
Figure 1: TrueSkill’s vwin and the corresponding
loss function in the Hopkins and May model as
a function of the difference t of system means
(E = 0.5, c = 0.8 for TrueSkill, and d = 0.5 for
Hopkins and May model).
</figureCaption>
<figure confidence="0.99335075">
1.0
T ueSHl
HM
0.5
v(t, E) 0.0
−0.5
−1.0−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5
t = µS, − µS
</figure>
<figureCaption confidence="0.90983875">
Figure 2: TrueSkills vtie and the corresponding
loss function in the Hopkins and May model as
a function of the difference t of system means
(E = 0.5,c = 0.3, and d = 0.5).
</figureCaption>
<bodyText confidence="0.9084304">
{σSj}. These confidences also factor into the up-
dates: while surprising outcomes result in larger
updates to system means, higher confidences (rep-
resented by smaller variances) result in smaller
updates. TrueSkill defines the following value:
</bodyText>
<equation confidence="0.99154">
c2 = 2β2 + σ2S1 + σ2 S2
</equation>
<bodyText confidence="0.998254">
which accumulates the variances along β, another
free parameter. We can now define the update
equations for the system means:
</bodyText>
<equation confidence="0.94351925">
µS1 = µS1 +
µS2 = µS2
S1 v (t, E)
c c c
S2 v (t, E)
c c c
2
σ
</equation>
<page confidence="0.7760755">
σ
2
</page>
<bodyText confidence="0.9997305">
The second term in these equations captures the
idea about balancing surprisal with confidence,
described above.
In order to update the system-level confidences,
TrueSkill defines another set of functions, w, for
the cases of wins and ties. These functions are
multiplicative factors that affect the amount of
change in σ2:
</bodyText>
<equation confidence="0.995711333333333">
wwin(t, E) = vwin · (vwin + t − E)
wtie(t, E) = vtie + (E − t) · N(E − t) + (E + t) · N(E + t)
b(E − t) − b(−E − t)
</equation>
<bodyText confidence="0.9999435">
The underlying idea is that these functions cap-
ture the outcome surprisal via v. This update al-
ways decreases the size of the variances σ2, which
means uncertainty of µ decreases as comparisons
go on. With these defined, we can conclude by
defining the updates for σ2S1 and σ2S2:
</bodyText>
<equation confidence="0.998990625">
&amp;quot; ~#
2
2 2 σS1
σS1 = σS1 1 − · c2 ·w c, c �
&amp;quot; �#
2
2 2 σS2 t E
σS2 = σS2 1 − c2 w c, c
</equation>
<bodyText confidence="0.999918333333333">
One final complication not presented here but rel-
evant to adapting TrueSkill to the WMT setting:
the parameter β and another parameter (not dis-
cussed) τ are incorporated into the update equa-
tions to give more weight to recent matches. This
“latest-oriented” property is useful in the gaming
setting for which TrueSkill was built, where play-
ers improve over time, but is not applicable in the
WMT competition setting. To cancel this property
in TrueSkill, we set τ = 0 and β = 0.025 · |A |· σ2
in order to lessen the impact of the order in which
annotations are presented to the system.
</bodyText>
<subsectionHeader confidence="0.969085">
2.4 Data selection with TrueSkill
</subsectionHeader>
<bodyText confidence="0.999984769230769">
A drawback of the standard WMT data collection
method is that it samples uniformly from the space
of pairwise system combinations. This is undesir-
able: systems with vastly divergent relative abil-
ity need not be compared as often as systems that
are more evenly matched. Unfortunately, one can-
not sample non-uniformly without knowing ahead
of time which systems are better. TrueSkill pro-
vides a solution to this dilemma with its match-
selection ability: systems with similar means and
low variances can be confidently considered to be
close matches. This presents a strong possibility
of reducing the amount of data that needs to be
</bodyText>
<page confidence="0.965747">
5
</page>
<bodyText confidence="0.993583888888889">
collected in the WMT competitions. In fact, the
TrueSkill formulation provides a way to compute
the probability of a draw between two systems,
which can be used to compute for a system Si a
conditional distribution over matches with other
systems {Sj6=i}.
Formally, in the TrueSkill model, the match-
selection (chance to draw) between two players
(systems in WMT) is computed as follows:
</bodyText>
<equation confidence="0.8680145">
2β2
pdraw =rc2 · exp(
</equation>
<bodyText confidence="0.999907842105263">
However, our setting for canceling the “latest-
oriented” property affects this matching quality
equation, where most systems are almost equally
competitive (≈ 1). Therefore, we modify the equa-
tion in the following manner which simply de-
pends on the difference of µ.
each produced in two ways: selecting from all re-
searchers, or split between researchers and Turk-
ers. An important note is that the training data
differs according to the model. For the Expected
Wins and Hopkins and May model, we sim-
ply sample uniformly at random. The TrueSkill
model, however, selects its own training data (with
replacement) according to the description in Sec-
tion 2.4.7
For tuning hyperparameters and reporting test
results, we used development and test sets of 2,000
comparisons drawn entirely from the researcher
judgments, and fixed across all experiments.
</bodyText>
<subsectionHeader confidence="0.994033">
3.2 Perplexity
</subsectionHeader>
<bodyText confidence="0.999761333333333">
We first compare the Hopkins and May model and
TrueSkill using perplexity on the test data T, com-
puted as follows:
</bodyText>
<equation confidence="0.9068094">
(µa −µb)2
2c2
)
ˆpdraw = 1 ppl(p|T) = 2− y-(z,S1,S2,-)∈&amp;quot;log2 p(π|S1,S2)
exp(|µa − µb|)
</equation>
<bodyText confidence="0.999915666666667">
TrueSkill selects the matches it would like to
create, according to this selection criteria. We do
this according to the following process:
</bodyText>
<listItem confidence="0.999577714285714">
1. Select a system S1 (e.g., the one with the
highest variance)
2. Compute a normalized distribution over
matches with other systems pairs ˆpdraw
3. Draw a system S2 from this distribution
4. Draw a source sentence, and present to the
judge for annotation
</listItem>
<sectionHeader confidence="0.994568" genericHeader="method">
3 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.973577">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.968278388888889">
We used the evaluation data released by WMT13.6
The data contains (1) five-way system rankings
made by either researchers or Turkers and (2)
translation data consisting of source sentences, hu-
man reference translations, and submitted transla-
tions. Data exists for 10 language pairs. More de-
tails about the dataset can be found in the WMT
2013 overview paper (Bojar et al., 2013).
Each five-way system ranking was converted
into ten pairwise judgments (§2). We trained the
models using randomly selected sets of 400, 800,
1,600, 3,200, and 6,400 pairwise comparisons,
6statmt.org/wmt13/results.html
where p is the model under consideration. The
probability of each observed outcome π between
two systems S1 and S2 is computed by taking a
difference of the Gaussian distributions associated
with those systems:
</bodyText>
<equation confidence="0.962777">
N(µδ, σ2δ) = N(µS1, σ2S1) − N(µS2, σ2S2)
2 2
= N(µS1 − µS2, σS1 + σS2 )
</equation>
<bodyText confidence="0.9993996">
This Gaussian can then be carved into three pieces:
the area where S1 loses, the middle area represent-
ing ties (defined by a decision radius, r, whose
value is fit using development data), and a third
area representing where S1 wins. By integrating
over each of these regions, we have a probability
distribution over these outcomes:
We do not compute perplexity for the Expected
Wins model, which does not put any probability
mass on ties.
</bodyText>
<footnote confidence="0.935855">
7We use a Python implementation of TrueSkill
(github.com/sublee/trueskill).
</footnote>
<table confidence="0.9846945">
p(π  |S1, S2) = ⎧ ° N U2 ifπis &gt;
⎨⎪⎪⎪⎪⎪⎪⎪ f (µa, δ )
⎪⎪⎪⎪⎪⎪⎪⎩ R0r N(µδ, σ2δ) if π is∞ =
Rr N(µδ, σ2δ) if π is &lt;
</table>
<page confidence="0.997343">
6
</page>
<subsectionHeader confidence="0.995152">
3.3 Accuracy
</subsectionHeader>
<bodyText confidence="0.9998769375">
Perplexity is often viewed as a neutral metric, but
without access to unbounded training data or the
true model parameters, it can only be approxi-
mated. Furthermore, it does not always corre-
late perfectly with evaluation metrics. As such,
we also present accuracy results, measuring each
model’s ability to predict the values of the ternary
pairwise judgments made by the annotators. These
are computed using the above equation, picking
the highest value of p(7r) for all annotations be-
tween each system pair (Si, Sj). As with perplex-
ity, we emphasize that these predictions are func-
tions of the system pair only, and not the individual
sentences under consideration, so the same out-
come is always predicted for all sentences between
a system pair.
</bodyText>
<subsectionHeader confidence="0.976229">
3.4 Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999991733333333">
We follow the settings described in Hopkins and
May (2013) for their model: Qa = 0.5, Qobs = 1.0,
and d = 0.5. In TrueSkill, in accordance with the
Hopkins and May model, we set the initial µ and
Q values for each system to 0 and 0.5 respectively,
and c to 0.25.
For test data, we tuned the “decision ra-
dius” parameter r by doing grid search over
{0.001, 0.01, 0.1, 0.3, 0.5}, searching for the
value which minimized perplexity and maximized
accuracy on the development set. We do this for
each model and language pair. When tuned by
perplexity, r is typically either 0.3 or 0.5 for both
models and language pairs, whereas, for accuracy,
the best r is either 0.001, 0.01, or 0.1.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999846">
4.1 Model Comparison
</subsectionHeader>
<bodyText confidence="0.999481357142857">
Figure 3 shows the perplexity of the two mod-
els with regard to the number of training compar-
isons. The perplexities in the figure are averaged
over all ten language pairs in the WMT13 dataset.
Overall, perplexities decrease according to the in-
crease of training size. The Hopkins and May
and TrueSkill models trained on both researcher
and Turker judgments are comparable, whereas
the Hopkins and May model trained on researcher
judgments alone shows lower perplexity than the
corresponding TrueSkill model.
In terms of accuracy, we see that the TrueSkill
model has the highest accuracies, saturating at just
over 3,000 training instances (Figure 4). TrueSkill
</bodyText>
<figureCaption confidence="0.6598424">
Figure 3: Model Perplexities for WMT13 dataset.
‘all’ indicates that models are trained on both re-
searcher and Turker judgements, and ‘res’ means
that models are trained on only researcher judge-
ments.
</figureCaption>
<bodyText confidence="0.9981496">
outperforms Expected Win and the Hopkins and
May, especially when the training size is small
(Table 2). We also note that training on researcher
judgments alone (dashed lines) results in better
performance than training on both researchers and
Turker judgments. This likely reflects both a bet-
ter match between training and test data (recall the
test data consists of researcher judgments only),
as well as the higher consistency of this data, as
evidenced by the annotator agreement scores pub-
lished in the WMT overview paper (Bojar et al.,
2013). Recall that the models only have access
to the system pair (and not the sentences them-
selves), and thus make the same prediction for 7r
for a particular system pair, regardless of which
source sentence was selected. As an upper bound
for performance on this metric, Table 2 contains
an oracle score, which is computed by selecting,
for each pair of systems, the highest-probability
ranking.8
Comparing the plots, we see there is not a per-
fect relationship between perplexity and accuracy
among the models; the low perplexity does not
mean the high accuracy, and in fact the order of
the systems is different.
</bodyText>
<subsectionHeader confidence="0.956753">
4.2 Free-for-all matches
</subsectionHeader>
<bodyText confidence="0.9763515">
TrueSkill need not deal with judgments in pairs
only, but was in fact designed to be used in a vari-
ety of settings, including N-way free-for-all games
8Note that this might not represent a consistent ranking
among systems, but is itself an upper bound on the highest-
scoring consistent ranking.
</bodyText>
<figure confidence="0.990978909090909">
HM-all
HM-res
TS-all
TS-res
1000 2000 3000 4000 5000 6000
Training Data Size
Perplexity 2.95
2.90
2.85
3.00
2.80
</figure>
<page confidence="0.938618">
7
</page>
<figureCaption confidence="0.9623075">
Figure 4: Model accuracies with different training
domain for WMT13 dataset.
</figureCaption>
<table confidence="0.9871035">
Train Size Exp-Win HM TrueSkill
0.465 0.471 0.479
# N=2 N=3 N=4 N=5
400 0.479 0.482 0.491 0.492
800 0.483 0.493 0.495 0.495
1600 0.493 0.492 0.497 0.495
3200 0.493 0.494 0.498 0.497
6400 0.495 0.498 0.498 0.498
</table>
<tableCaption confidence="0.9959185">
Table 3: Accuracies when training with N-way
free-for-all models, fixing the number of matches.
</tableCaption>
<table confidence="0.999965333333333">
# N=2 N=3 N=4 N=5
400 0.479 0.475 0.470 0.459
800 0.483 0.488 0.476 0.466
1600 0.493 0.488 0.481 0.481
3200 0.493 0.492 0.487 0.489
6400 0.495 0.496 0.494 0.495
</table>
<tableCaption confidence="0.928502666666667">
Table 4: Accuracies when training with N-way
free-for-all models, fixing the number of pairwise
comparisons.
</tableCaption>
<figure confidence="0.990556555555555">
1000 2000 3000 4000 5000 6000
Training Data Size
Accuracy 0.500
0.495
0.490
0.485
0.480
0.475
0.470
0.465
0.460
i xpWin-all
i xpWin-res
HM-all
HM-res
TS-all
TS-res
400
800
all 1600
3200
6400
400
800
res 1600
3200
6400
</figure>
<bodyText confidence="0.997472588235294">
0.471 0.475 0.483
0.479 0.477 0.493
0.486 0.489 0.493
0.487 0.490 0.495
0.460 0.463 0.484
0.475 0.473 0.488
0.481 0.482 0.493
0.492 0.494 0.497
0.495 0.496 0.497
tute these comparisons. The results bear this out,
but also suggest that the standard WMT setting
— which extracts ten pairwise comparisons from
each 5-way match and treats them independently
— works well. We will not speculate further here,
but provide this experiment purely to motivate po-
tential future work. Here we will focus our con-
clusions to the pair-wise ranking scenario.
</bodyText>
<table confidence="0.33671">
Upper Bound
0.525
</table>
<tableCaption confidence="0.732227">
Table 2: Model accuracies: models are tuned by
</tableCaption>
<bodyText confidence="0.981107619047619">
accuracy instead of perplexity. Upper bound is
computed by selecting the most frequent choice
(&lt;, &gt;,for each system pair.
with many players all competing for first place.
This adapts nicely to WMT’s actual collection set-
ting. Recall that annotators are presented with five
translations which are then ranked; we can treat
this setting as a 5-way free-for-all match. While
the details of these updates are beyond the scope of
this paper, they are presented in the original model
and are implemented in the toolkit we used. We
thus also conducted experiments varying the value
of N from 2 to 5.
The results are shown in Tables 3 and 4, which
hold constant the number of matches and pairwise
judgments, respectively. When fixing the num-
ber of matches, the 5-way setting is at an advan-
tage, since there is much more information in each
match; in contrast, when fixing the number of pair-
wise comparisons, the 5-way setting is at a dis-
advantage, since many fewer competitions consti-
</bodyText>
<sectionHeader confidence="0.950302" genericHeader="method">
5 Reduced Data Collection with
</sectionHeader>
<subsectionHeader confidence="0.863546">
Non-uniform Match Selection
</subsectionHeader>
<bodyText confidence="0.999958">
As mentioned earlier, a drawback of the selection
of training data for annotation is that it is sampled
uniformly from the space of system pair compe-
titions, and an advantage of TrueSkill is its abil-
ity to instead compute a distribution over pairings
and thereby focus annotation efforts on competi-
tive matches. In this section, we report results in
the form of heat maps indicating the percentage of
pairwise judgments requested by TrueSkill across
the full cross-product of system pairs, using the
WMT13 French-English translation task.
Figure 5 depicts a system-versus-system heat
map for all judgments in the dataset. Across this
figure and the next two, systems are sorted along
each axis by the final values of µ inferred by
TrueSkill during training, and the heat of each
square is proportional to the percentage of judg-
ments obtained between those two systems. The
diagonal reflects the fact that systems do not com-
pete against themselves, and the stripe at row and
column 5 reflects a system that was entered late
</bodyText>
<page confidence="0.989017">
8
</page>
<figure confidence="0.996836466666667">
1 2 3 4 5 6 7 8 9 10 11 12 13
1
2
3
4
5
6
7
8
9
10
11
12
13
0.0
</figure>
<figureCaption confidence="0.980725333333333">
Figure 5: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems in
the WMT13 French-English translation task.
</figureCaption>
<figure confidence="0.919527">
1 2 3 4 5 6 7 8 9 10 11 12 13
</figure>
<figureCaption confidence="0.934061">
Figure 6: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems
used in the first 20% of TrueSkill model.
</figureCaption>
<figure confidence="0.977385466666666">
1 2 3 4 5 6 7 8 9 10 11 12 13
1
2
3
4
5
6
7
8
9
10
11
12
13
0.0
</figure>
<figureCaption confidence="0.996295">
Figure 7: Heat map for the ratio of pairwise judg-
</figureCaption>
<bodyText confidence="0.984203826086956">
ments across the full cross-product of systems
used in the last 20% of TrueSkill model.
into the WMT13 competition and thus had many
fewer judgments. It is clear that these values are
roughly uniformly distributed. This figure serves
as a sort of baseline, demonstrating the lack of pat-
terns in the data-selection process.
The next two figures focus on the data that
TrueSkill itself selected for its use from among all
of the available data. Figure 6 is a second heat
map presenting the set of system pairs selected by
TrueSkill for the first 20% of its matches chosen
during training, while Figure 7 presents a heat map
of the last 20%. The contrast is striking: whereas
the judgments are roughly uniformly distributed at
the beginning, the bulk of the judgments obtained
for the last set are clustered along the diagonal,
where the most competitive matches lie.
Together with the higher accuracy of TrueSkill,
this suggests that it could be used to decrease the
amount of data that needs to be collected in future
WMT human evaluations by focusing the annota-
tion effort on more closely-matched systems.
</bodyText>
<sectionHeader confidence="0.996188" genericHeader="method">
6 Clustering
</sectionHeader>
<bodyText confidence="0.999981461538462">
As pointed out by Koehn (2012), a ranking pre-
sented as a total ordering among systems con-
ceals the closeness of comparable systems. In the
WMT13 competition, systems are grouped into
clusters, which is equivalent to presenting only
a partial ordering among the systems. Clusters
are constructed using bootstrap resampling to in-
fer many system rankings. From these rankings,
rank ranges are then collected, which can be used
to construct 95% confidence intervals, and, in turn,
to cluster systems whose ranges overlap. We use
a similar approach for clustering in the TrueSkill
model. We obtain rank ranges for each system by
running the TrueSkill model 100 times,9 throw-
ing out the top and bottom 2 rankings for each
system, and clustering where rank ranges overlap.
For comparison, we also do this for the other two
models, altering the amount of training data from
1k to 25k in increments of 1,000, and plotting the
number of clusters that can be obtained from each
technique on each amount of training data.
Figure 8 show the number of clusters according
to the increase of training data for three models.
TrueSkill efficiently split the systems into clusters
compared to other two methods. Figure 9 and 10
present the result of clustering two different size of
</bodyText>
<footnote confidence="0.909262">
9We also tried the sampling 1,000 times and the clustering
granularities were the same.
</footnote>
<figure confidence="0.998112522727273">
2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
1
2
3
4
5
6
7
8
9
10
11
12
13
2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
</figure>
<page confidence="0.577212">
9
</page>
<figureCaption confidence="0.974811666666667">
Figure 8: The number of clusters according to
the increase of training data for WMT13 French-
English (13 systems in total).
</figureCaption>
<bodyText confidence="0.999571894736842">
training data (1K and 25K pairwise comparisons)
on the TrueSkill model, which indicates that the
rank ranges become narrow and generate clusters
reasonably as the number of training samples in-
creases. The ranking and clusters are slightly dif-
ferent from the official result (Table 1) mainly be-
cause the official result is based on Expected Wins.
One noteworthy observation is that the ranking
of systems between Figure 9 and Figure 10 is the
same, further corroborating the stability and ac-
curacy of the TrueSkill model even with a small
amount of data. Furthermore, while the need
to cluster systems forces the collection of sig-
nificantly more data than if we wanted only to
report a total ordering, TrueSkill here produces
nicely-sized clusters with only 25K pairwise com-
parisons, which is nearly one-third large of that
used in the WMT13 campaign (80K for French-
English, yielding 8 clusters).
</bodyText>
<sectionHeader confidence="0.998192" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.986142571428571">
Models of “relative ability” (Koehn, 2012; Hop-
kins and May, 2013) are a welcome addition to
methods for inferring system rankings from hu-
man judgments. The TrueSkill variant presented
in this paper is a promising further development,
both in its ability to achieve higher accuracy levels
than alternatives, and in its ability to sample non-
uniformly from the space of system pair match-
ings. It’s possible that future WMT evaluations
could significantly reduce the amount of data they
need to collect, also potentially allowing them to
draw from expert annotators alone (the developers
Figure 9: The result of clustering by TrueSkill
model with 1K training data from WMT13
French-English. The boxes range from the lower
to upper quartile values, with means in the middle.
The whiskers show the full range of each system’s
rank after the bootstrap resampling.
Figure 10: The result of clustering by TrueSkill
model with 25K training data. Dashed lines sep-
arate systems with non-overlapping rank ranges,
splitting the data into clusters.
of the participating systems), without the need to
hire non-experts on Mechanical Turk.
One piece missing from the methods explored
and proposed in this paper is models of the actual
translations being compared by judges. Clearly,
it is properties of the sentences themselves that
judges use to make their judgments, a fact which
is captured only indirectly by modeling transla-
tion qualities sampled from system abilities. This
observation has been used in the development
of automatic evaluation metrics (Song and Cohn,
2011), and is something we hope to explore in fu-
ture work for system ranking.
</bodyText>
<figure confidence="0.99607235">
ExpWin
FM
TS
5000 10000 15000 20000 25000
Pairwise Comparisons
2
1
0
7
6
5
4
3
Num. of Clusters
10
11
12
13
2
3
4
5
6
8
9
1
7
1
2
3
4
5
6
7
8
9
10
11
12
13
</figure>
<page confidence="0.963466">
10
</page>
<sectionHeader confidence="0.989584" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99994840625">
Ond&amp;quot;rej Bojar, Milo&amp;quot;s Ercegov&amp;quot;cevi´c, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
1–11, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, pages 22–64, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Ralf Herbrich, Tom Minka, and Thore Graepel. 2006.
TrueSkillTM: A Bayesian Skill Rating System. In
Proceedings of the Twentieth Annual Conference on
Neural Information Processing Systems, pages 569–
576, Vancouver, British Columbia, Canada, Decem-
ber. MIT Press.
Mark Hopkins and Jonathan May. 2013. Models of
translation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1416–1424, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Philipp Koehn. 2012. Simulating Human Judgment
in Machine Translation Evaluation Campaigns. In
Proceedings of the 9th International Workshop on
Spoken Language Translation (IWSLT), pages 179–
184, Hong Kong, China, December. International
Speech Communication Association.
Adam Lopez. 2012. Putting Human Assessments of
Machine Translation Systems in Order. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 1–9, Montr´eal, Canada, June. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, July. Association for Computational
Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
Ranking based Optimisation for Sentence Level MT
Evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 123–129,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.999485">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296685">
<title confidence="0.9995505">Efficient Elicitation of Annotations for Human Evaluation of Machine Translation</title>
<author confidence="0.999903">Matt Benjamin Van</author>
<affiliation confidence="0.628451666666667">for Language and Speech Language Technology Center of Johns Hopkins University, Baltimore,</affiliation>
<abstract confidence="0.999802407407407">A main output of the annual Workshop on Statistical Machine Translation (WMT) is a ranking of the systems that participated in its shared translation tasks, produced by aggregating pairwise sentencelevel comparisons collected from human judges. Over the past few years, there have been a number of tweaks to the aggregation formula in attempts to address issues arising from the inherent ambiguity and subjectivity of the task, as well as weaknesses in the proposed models and the manner of model selection. We continue this line of work by adaptthe algorithm — an online approach for modeling the relative skills of players in ongoing competitions, such as Microsoft’s Xbox Live — to the human evaluation of machine translation output. Our experimental results show that TrueSkill outperforms other recently proposed models on accuracy, and also can significantly reduce the number of pairwise annotations that need to be collected by sampling non-uniformly from the space of system competitions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Milos Ercegovcevi´c</author>
<author>Martin Popel</author>
<author>Omar Zaidan</author>
</authors>
<title>A Grain of Salt for the WMT Manual Evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bojar, Ercegovcevi´c, Popel, Zaidan, 2011</marker>
<rawString>Ond&amp;quot;rej Bojar, Milo&amp;quot;s Ercegov&amp;quot;cevi´c, Martin Popel, and Omar Zaidan. 2011. A Grain of Salt for the WMT Manual Evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 1–11, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3224" citStr="Bojar et al. (2013)" startWordPosition="513" endWordPosition="517"> measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggregated to produce a complete ordering of all systems, or, more recently, a partial ordering (Koehn, 2012), with systems clustered where they cannot be distinguished in a statistically significant way (Table 1, taken from Bojar et al. (2013)). A number of problems have been noted with this approach. The first has to do with the nature of ranking itself. Over the past few years, the WMT organizers have introduced a number of minor tweaks to the ranking algorithm (§2) in reaction to largely intuitive arguments that have been 1 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1–11, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corrobo</context>
<context position="9519" citStr="Bojar et al., 2013" startWordPosition="1566" endWordPosition="1569">011), the score for each system Si was computed as follows: wins(Si) + ties(Si) score(Si) = wins(Si) + ties(Si) + loses(Si) Bojar et al. (2011) suggested that the inclusion of ties biased the results, due to their large numbers, the underlying similarity of many of the models, and the fact that they are counted for both systems in the tie, and proposed the following modified scoring function: 1 wins (Si, Sj) score(Si) = |{S} |SYi wins(Si, Sj) + wins(Sj, Si) This metric computes an average relative frequency of wins, excluding ties, and was used in WMT12 and WMT13 (Callison-Burch et al., 2012; Bojar et al., 2013). The decision to exclude ties isn’t without its problems; for example, an evaluation where two systems are nearly always judged equivalent should be relevant in producing the final ranking of systems. Furthermore, as Hopkins and May (2013) point out, throwing out data to avoid biasing a model suggests a problem with the model. We now turn to a description of their model, which addresses these problems. 1δ(x, r 1 if x = y y) = 1 0 o.w. 2.2 The Hopkins and May (2013) model Recent papers (Koehn, 2012; Hopkins and May, 2013) have proposed models focused on the relative ability of the competition </context>
<context position="21304" citStr="Bojar et al., 2013" startWordPosition="3684" endWordPosition="3687">hest variance) 2. Compute a normalized distribution over matches with other systems pairs ˆpdraw 3. Draw a system S2 from this distribution 4. Draw a source sentence, and present to the judge for annotation 3 Experimental setup 3.1 Datasets We used the evaluation data released by WMT13.6 The data contains (1) five-way system rankings made by either researchers or Turkers and (2) translation data consisting of source sentences, human reference translations, and submitted translations. Data exists for 10 language pairs. More details about the dataset can be found in the WMT 2013 overview paper (Bojar et al., 2013). Each five-way system ranking was converted into ten pairwise judgments (§2). We trained the models using randomly selected sets of 400, 800, 1,600, 3,200, and 6,400 pairwise comparisons, 6statmt.org/wmt13/results.html where p is the model under consideration. The probability of each observed outcome π between two systems S1 and S2 is computed by taking a difference of the Gaussian distributions associated with those systems: N(µδ, σ2δ) = N(µS1, σ2S1) − N(µS2, σ2S2) 2 2 = N(µS1 − µS2, σS1 + σS2 ) This Gaussian can then be carved into three pieces: the area where S1 loses, the middle area repr</context>
<context position="25339" citStr="Bojar et al., 2013" startWordPosition="4364" endWordPosition="4367"> and ‘res’ means that models are trained on only researcher judgements. outperforms Expected Win and the Hopkins and May, especially when the training size is small (Table 2). We also note that training on researcher judgments alone (dashed lines) results in better performance than training on both researchers and Turker judgments. This likely reflects both a better match between training and test data (recall the test data consists of researcher judgments only), as well as the higher consistency of this data, as evidenced by the annotator agreement scores published in the WMT overview paper (Bojar et al., 2013). Recall that the models only have access to the system pair (and not the sentences themselves), and thus make the same prediction for 7r for a particular system pair, regardless of which source sentence was selected. As an upper bound for performance on this metric, Table 2 contains an oracle score, which is computed by selecting, for each pair of systems, the highest-probability ranking.8 Comparing the plots, we see there is not a perfect relationship between perplexity and accuracy among the models; the low perplexity does not mean the high accuracy, and in fact the order of the systems is </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ond&amp;quot;rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<date>2011</date>
<booktitle>Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="8904" citStr="Callison-Burch et al., 2011" startWordPosition="1460" endWordPosition="1463">e comparisons. Let A be the complete set of annotations or judgments of the form {i, S1, S2, πR}. We assume these judgments have been converted into a normal form where S1 is either the winner or is tied with S2, and therefore πR ∈ {&lt;, =}. Let δ(x, y) be the Kronecker delta function.1 We then define the function: which counts the number of annotations for which system Si was ranked better than system Sj. We define a single-variable version that marginalizes over all annotations: wins(Si) = X wins(Si, Sj) Sj7�Si We also define analogous functions for loses and ties. Until the WMT11 evaluation (Callison-Burch et al., 2011), the score for each system Si was computed as follows: wins(Si) + ties(Si) score(Si) = wins(Si) + ties(Si) + loses(Si) Bojar et al. (2011) suggested that the inclusion of ties biased the results, due to their large numbers, the underlying similarity of many of the models, and the fact that they are counted for both systems in the tie, and proposed the following modified scoring function: 1 wins (Si, Sj) score(Si) = |{S} |SYi wins(Si, Sj) + wins(Sj, Si) This metric computes an average relative frequency of wins, excluding ties, and was used in WMT12 and WMT13 (Callison-Burch et al., 2012; Boja</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 Workshop on Statistical Machine Translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="9498" citStr="Callison-Burch et al., 2012" startWordPosition="1562" endWordPosition="1565">ion (Callison-Burch et al., 2011), the score for each system Si was computed as follows: wins(Si) + ties(Si) score(Si) = wins(Si) + ties(Si) + loses(Si) Bojar et al. (2011) suggested that the inclusion of ties biased the results, due to their large numbers, the underlying similarity of many of the models, and the fact that they are counted for both systems in the tie, and proposed the following modified scoring function: 1 wins (Si, Sj) score(Si) = |{S} |SYi wins(Si, Sj) + wins(Sj, Si) This metric computes an average relative frequency of wins, excluding ties, and was used in WMT12 and WMT13 (Callison-Burch et al., 2012; Bojar et al., 2013). The decision to exclude ties isn’t without its problems; for example, an evaluation where two systems are nearly always judged equivalent should be relevant in producing the final ranking of systems. Furthermore, as Hopkins and May (2013) point out, throwing out data to avoid biasing a model suggests a problem with the model. We now turn to a description of their model, which addresses these problems. 1δ(x, r 1 if x = y y) = 1 0 o.w. 2.2 The Hopkins and May (2013) model Recent papers (Koehn, 2012; Hopkins and May, 2013) have proposed models focused on the relative abilit</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Herbrich</author>
<author>Tom Minka</author>
<author>Thore Graepel</author>
</authors>
<title>TrueSkillTM: A Bayesian Skill Rating System.</title>
<date>2006</date>
<booktitle>In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>569--576</pages>
<publisher>MIT Press.</publisher>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5687" citStr="Herbrich et al., 2006" startWordPosition="920" endWordPosition="923">forcing the use of paid, lower-quality annotators hired on Amazon’s Mechanical Turk. Part of the problem is that the sampling strategy collects data uniformly across system pairings. Intuitively, we should need many fewer annotations between systems with divergent base performance levels, instead focusing the collection effort on system pairs whose performance is more matched, in order to tease out the gaps between similarly-performing systems. Why spend precious human time on redundantly affirming predictable outcomes? To address these issues, we developed a variation of the TrueSkill model (Herbrich et al., 2006), an adaptative model of competitions originally developed for the Xbox Live online gaming community. It assumes that each player’s skill level follows a Gaussian distribution N(µ, Q2), in which µ represents a player’s mean performance, and Q2 the system’s uncertainty about its current estimate of this mean. These values are updated after each “game” (in our case, the value of a ternary judgment) in proportion to how surprising the outcome is. TrueSkill has been adapted to a number of areas, including chess, advertising, and academic conference management. The rest of this paper provides an em</context>
<context position="13862" citStr="Herbrich et al., 2006" startWordPosition="2330" endWordPosition="2333"> points in computing the system ranking. Another of Bojar et al. (2011)’s suggestions was to exclude this data, because systems compared more often against the references suffered unfairly. This can be further generalized to the observation that not all competitions are equal, and a good model should incorporate some notion of “match difficulty” when evaluating system’s abilities. The inference procedure above incorporates this notion implicitly in the inference procedure, but the model itself does not include a notion of match difficulty or outcome surprisal. A model that does is TrueSkill5 (Herbrich et al., 2006). TrueSkill is an adaptive, online system that also assumes that each system’s skill level follows a Gaussian distribution, maintaining a mean µSj for each system Sj representing its current estimate of that system’s native ability. However, it also maintains a per-system variance, σ2Sj, which represents TrueSkill’s uncertainty about its estimate of each mean. After an outcome is observed (a game in which the result is a win, loss, or draw), the size of the updates is proportional to how surprising the outcome was, which is computed from the current system means and variances. If a translation</context>
<context position="15815" citStr="Herbrich et al. (2006)" startWordPosition="2672" endWordPosition="2675">he cumulative distribution function and the Ns are Gaussians. Figures 1 and 2 display plots of these two functions compared to the Hopkins and May model. Note how vwin (Figure 1) increases exponentially as µS2 becomes greater than the (purportedly) better system, µS1. As noted above, TrueSkill maintains not only estimates {µSj} of system abilities, but also system-specific confidences about those estimates 5The goal of this section is to provide an intuitive description of TrueSkill as adapted for WMT manual evaluations, with enough detail to carry the main ideas. For more details, please see Herbrich et al. (2006). � q2 t &gt; d q2 =1 q2 − 2(d − t) otherwise � q1 t &gt; d q1 + 1(d − t) otherw q�1 ise 2 q1 = q2 = q2 − 12(d−t) t &gt; d q2 t &lt; d q2 − 12(−d − t) t &lt; −d { 4 −1.0 −0.5 0.0 0.5 1.0 t = µS, − µS v(t, E) 0.5 0.0 1.5 1.0 T ueSHl HM Figure 1: TrueSkill’s vwin and the corresponding loss function in the Hopkins and May model as a function of the difference t of system means (E = 0.5, c = 0.8 for TrueSkill, and d = 0.5 for Hopkins and May model). 1.0 T ueSHl HM 0.5 v(t, E) 0.0 −0.5 −1.0−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 t = µS, − µS Figure 2: TrueSkills vtie and the corresponding loss function in the Hopkins and </context>
</contexts>
<marker>Herbrich, Minka, Graepel, 2006</marker>
<rawString>Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkillTM: A Bayesian Skill Rating System. In Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, pages 569– 576, Vancouver, British Columbia, Canada, December. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Models of translation competitions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1416--1424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3854" citStr="Hopkins and May (2013)" startWordPosition="617" endWordPosition="621">ber of problems have been noted with this approach. The first has to do with the nature of ranking itself. Over the past few years, the WMT organizers have introduced a number of minor tweaks to the ranking algorithm (§2) in reaction to largely intuitive arguments that have been 1 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1–11, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corroborated), Hopkins and May (2013) point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric. Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data. A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses. Such approaches do not consider the relative difficulty of system matchups, and thus leave open the possibility that a system</context>
<context position="9759" citStr="Hopkins and May (2013)" startWordPosition="1603" endWordPosition="1606">rlying similarity of many of the models, and the fact that they are counted for both systems in the tie, and proposed the following modified scoring function: 1 wins (Si, Sj) score(Si) = |{S} |SYi wins(Si, Sj) + wins(Sj, Si) This metric computes an average relative frequency of wins, excluding ties, and was used in WMT12 and WMT13 (Callison-Burch et al., 2012; Bojar et al., 2013). The decision to exclude ties isn’t without its problems; for example, an evaluation where two systems are nearly always judged equivalent should be relevant in producing the final ranking of systems. Furthermore, as Hopkins and May (2013) point out, throwing out data to avoid biasing a model suggests a problem with the model. We now turn to a description of their model, which addresses these problems. 1δ(x, r 1 if x = y y) = 1 0 o.w. 2.2 The Hopkins and May (2013) model Recent papers (Koehn, 2012; Hopkins and May, 2013) have proposed models focused on the relative ability of the competition systems. These approaches assume that each system has a mean quality represented by a Gaussian distribution with a fixed variance shared across all systems. In the graphical model formulation of Hopkins and May (2013), the pairwise judgment</context>
<context position="23277" citStr="Hopkins and May (2013)" startWordPosition="4017" endWordPosition="4020">ith evaluation metrics. As such, we also present accuracy results, measuring each model’s ability to predict the values of the ternary pairwise judgments made by the annotators. These are computed using the above equation, picking the highest value of p(7r) for all annotations between each system pair (Si, Sj). As with perplexity, we emphasize that these predictions are functions of the system pair only, and not the individual sentences under consideration, so the same outcome is always predicted for all sentences between a system pair. 3.4 Parameter Tuning We follow the settings described in Hopkins and May (2013) for their model: Qa = 0.5, Qobs = 1.0, and d = 0.5. In TrueSkill, in accordance with the Hopkins and May model, we set the initial µ and Q values for each system to 0 and 0.5 respectively, and c to 0.25. For test data, we tuned the “decision radius” parameter r by doing grid search over {0.001, 0.01, 0.1, 0.3, 0.5}, searching for the value which minimized perplexity and maximized accuracy on the development set. We do this for each model and language pair. When tuned by perplexity, r is typically either 0.3 or 0.5 for both models and language pairs, whereas, for accuracy, the best r is either</context>
<context position="34136" citStr="Hopkins and May, 2013" startWordPosition="5897" endWordPosition="5901">teworthy observation is that the ranking of systems between Figure 9 and Figure 10 is the same, further corroborating the stability and accuracy of the TrueSkill model even with a small amount of data. Furthermore, while the need to cluster systems forces the collection of significantly more data than if we wanted only to report a total ordering, TrueSkill here produces nicely-sized clusters with only 25K pairwise comparisons, which is nearly one-third large of that used in the WMT13 campaign (80K for FrenchEnglish, yielding 8 clusters). 7 Conclusion Models of “relative ability” (Koehn, 2012; Hopkins and May, 2013) are a welcome addition to methods for inferring system rankings from human judgments. The TrueSkill variant presented in this paper is a promising further development, both in its ability to achieve higher accuracy levels than alternatives, and in its ability to sample nonuniformly from the space of system pair matchings. It’s possible that future WMT evaluations could significantly reduce the amount of data they need to collect, also potentially allowing them to draw from expert annotators alone (the developers Figure 9: The result of clustering by TrueSkill model with 1K training data from </context>
</contexts>
<marker>Hopkins, May, 2013</marker>
<rawString>Mark Hopkins and Jonathan May. 2013. Models of translation competitions. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1416–1424, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Simulating Human Judgment in Machine Translation Evaluation Campaigns.</title>
<date>2012</date>
<journal>International Speech Communication Association.</journal>
<booktitle>In Proceedings of the 9th International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>179--184</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="3089" citStr="Koehn, 2012" startWordPosition="492" endWordPosition="493">ts metrics task, where evaluation metrics are evaluated. In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggregated to produce a complete ordering of all systems, or, more recently, a partial ordering (Koehn, 2012), with systems clustered where they cannot be distinguished in a statistically significant way (Table 1, taken from Bojar et al. (2013)). A number of problems have been noted with this approach. The first has to do with the nature of ranking itself. Over the past few years, the WMT organizers have introduced a number of minor tweaks to the ranking algorithm (§2) in reaction to largely intuitive arguments that have been 1 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1–11, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics r</context>
<context position="5057" citStr="Koehn, 2012" startWordPosition="825" endWordPosition="826"> a system is ranked highly from the luck of comparisons against poorer opponents. Third, a large number of judgments need to be collected in order to separate the systems into clusters to produce a partial ranking. The sheer size of the space of possible comparisons (all pairs of systems times the number of segments in the test set) requires sampling from this space and distributing the annotations across a number of judges. Even still, the number of judgments needed to produce statistically significant rankings like those in Table 1 grows quadratically in the number of participating systems (Koehn, 2012), often forcing the use of paid, lower-quality annotators hired on Amazon’s Mechanical Turk. Part of the problem is that the sampling strategy collects data uniformly across system pairings. Intuitively, we should need many fewer annotations between systems with divergent base performance levels, instead focusing the collection effort on system pairs whose performance is more matched, in order to tease out the gaps between similarly-performing systems. Why spend precious human time on redundantly affirming predictable outcomes? To address these issues, we developed a variation of the TrueSkill</context>
<context position="10022" citStr="Koehn, 2012" startWordPosition="1658" endWordPosition="1659">y of wins, excluding ties, and was used in WMT12 and WMT13 (Callison-Burch et al., 2012; Bojar et al., 2013). The decision to exclude ties isn’t without its problems; for example, an evaluation where two systems are nearly always judged equivalent should be relevant in producing the final ranking of systems. Furthermore, as Hopkins and May (2013) point out, throwing out data to avoid biasing a model suggests a problem with the model. We now turn to a description of their model, which addresses these problems. 1δ(x, r 1 if x = y y) = 1 0 o.w. 2.2 The Hopkins and May (2013) model Recent papers (Koehn, 2012; Hopkins and May, 2013) have proposed models focused on the relative ability of the competition systems. These approaches assume that each system has a mean quality represented by a Gaussian distribution with a fixed variance shared across all systems. In the graphical model formulation of Hopkins and May (2013), the pairwise judgments (i, S1, S2, π) are imagined to have been generated according to the following process: • Select a source sentence i • Select two systems S1 and S2. A system Sj is associated with a Gaussian distribution N(µSj,σ2a), samples from which represent the quality of tr</context>
<context position="31561" citStr="Koehn (2012)" startWordPosition="5455" endWordPosition="5456"> first 20% of its matches chosen during training, while Figure 7 presents a heat map of the last 20%. The contrast is striking: whereas the judgments are roughly uniformly distributed at the beginning, the bulk of the judgments obtained for the last set are clustered along the diagonal, where the most competitive matches lie. Together with the higher accuracy of TrueSkill, this suggests that it could be used to decrease the amount of data that needs to be collected in future WMT human evaluations by focusing the annotation effort on more closely-matched systems. 6 Clustering As pointed out by Koehn (2012), a ranking presented as a total ordering among systems conceals the closeness of comparable systems. In the WMT13 competition, systems are grouped into clusters, which is equivalent to presenting only a partial ordering among the systems. Clusters are constructed using bootstrap resampling to infer many system rankings. From these rankings, rank ranges are then collected, which can be used to construct 95% confidence intervals, and, in turn, to cluster systems whose ranges overlap. We use a similar approach for clustering in the TrueSkill model. We obtain rank ranges for each system by runnin</context>
<context position="34112" citStr="Koehn, 2012" startWordPosition="5895" endWordPosition="5896"> Wins. One noteworthy observation is that the ranking of systems between Figure 9 and Figure 10 is the same, further corroborating the stability and accuracy of the TrueSkill model even with a small amount of data. Furthermore, while the need to cluster systems forces the collection of significantly more data than if we wanted only to report a total ordering, TrueSkill here produces nicely-sized clusters with only 25K pairwise comparisons, which is nearly one-third large of that used in the WMT13 campaign (80K for FrenchEnglish, yielding 8 clusters). 7 Conclusion Models of “relative ability” (Koehn, 2012; Hopkins and May, 2013) are a welcome addition to methods for inferring system rankings from human judgments. The TrueSkill variant presented in this paper is a promising further development, both in its ability to achieve higher accuracy levels than alternatives, and in its ability to sample nonuniformly from the space of system pair matchings. It’s possible that future WMT evaluations could significantly reduce the amount of data they need to collect, also potentially allowing them to draw from expert annotators alone (the developers Figure 9: The result of clustering by TrueSkill model wit</context>
</contexts>
<marker>Koehn, 2012</marker>
<rawString>Philipp Koehn. 2012. Simulating Human Judgment in Machine Translation Evaluation Campaigns. In Proceedings of the 9th International Workshop on Spoken Language Translation (IWSLT), pages 179– 184, Hong Kong, China, December. International Speech Communication Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Putting Human Assessments of Machine Translation Systems in Order.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="3766" citStr="Lopez, 2012" startWordPosition="606" endWordPosition="607">statistically significant way (Table 1, taken from Bojar et al. (2013)). A number of problems have been noted with this approach. The first has to do with the nature of ranking itself. Over the past few years, the WMT organizers have introduced a number of minor tweaks to the ranking algorithm (§2) in reaction to largely intuitive arguments that have been 1 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1–11, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics raised about how the evaluation is conducted (Bojar et al., 2011; Lopez, 2012). While these tweaks have been sensible (and later corroborated), Hopkins and May (2013) point out that this is essentially a model selection task, and should properly be driven by empirical performance on heldout data according to some metric. Instead of intuition, they suggest perplexity, and show that a novel graphical model outperforms existing approaches on that metric, with less amount of data. A second problem is the deficiency of the models used to produce the ranking, which work by computing simple ratios of wins (and, optionally, ties) to losses. Such approaches do not consider the r</context>
</contexts>
<marker>Lopez, 2012</marker>
<rawString>Adam Lopez. 2012. Putting Human Assessments of Machine Translation Systems in Order. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 1–9, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia,</location>
<contexts>
<context position="2384" citStr="Papineni et al., 2002" startWordPosition="376" endWordPosition="379">ments of sentencelevel pairwise rankings of system outputs. While the performance on many automatic metrics is also # score range system 1 0.638 1 UEDIN-HEAFIELD 2 0.604 2-3 UEDIN 0.591 2-3 ONLINE-B 4 0.571 4-5 LIMSI-SOUL 0.562 4-5 KIT 0.541 5-6 ONLINE-A 7 0.512 7 MES-SIMPLIFIED 8 0.486 8 DCU 9 0.439 9-10 RWTH 0.429 9-11 CMU-T2T 0.420 10-11 CU-ZEMAN 12 0.389 12 JHU 13 0.322 13 SHEF-WPROA Table 1: System rankings presented as clusters (WMT13 French-English competition). The score column is the percentage of time each system was judged better across its comparisons (§2.1). reported (e.g., BLEU (Papineni et al., 2002)), the human evaluation is considered primary, and is in fact used as the gold standard for its metrics task, where evaluation metrics are evaluated. In machine translation, the longstanding disagreements about evaluation measures do not go away when moving from automatic metrics to human judges. This is due in no small part to the inherent ambiguity and subjectivity of the task, but also arises from the particular way that the WMT organizers produce the rankings. The systemlevel rankings are produced by collecting pairwise sentence-level comparisons between system outputs. These are then aggr</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia,</rawString>
</citation>
<citation valid="false">
<authors>
<author>July Pennsylvania</author>
</authors>
<title>Association for Computational Linguistics.</title>
<marker>Pennsylvania, </marker>
<rawString>Pennsylvania, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingyi Song</author>
<author>Trevor Cohn</author>
</authors>
<title>Regression and Ranking based Optimisation for Sentence Level MT Evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>123--129</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Song, Cohn, 2011</marker>
<rawString>Xingyi Song and Trevor Cohn. 2011. Regression and Ranking based Optimisation for Sentence Level MT Evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 123–129, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
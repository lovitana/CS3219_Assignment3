<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001984">
<title confidence="0.982934">
Ontology-Based Argument Mining and Automatic Essay Scoring
</title>
<author confidence="0.99955">
Nathan Ong, Diane Litman, and Alexandra Brusilovsky
</author>
<affiliation confidence="0.856491">
Department of Computer Science, University of Pittsburgh
Pittsburgh, PA 15260 USA
</affiliation>
<email confidence="0.990257">
nro5,dlitman,apb27@pitt.edu
</email>
<sectionHeader confidence="0.993676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997728">
Essays are frequently used as a medium
for teaching and evaluating argumentation
skills. Recently, there has been interest in
diagrammatic outlining as a replacement
to the written outline that often precedes
essay writing. This paper presents a pre-
liminary approach for automatically iden-
tifying diagram ontology elements in es-
says, and demonstrates its positive corre-
lation with expert scores of essay quality.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979138888888">
Educators tend to favor students providing a
minimal-writing structure, or an outline, before
writing a paper. This allows teachers to give
early feedback to students to reduce the amount
of structural editing that might be needed later
on. However, there is evidence to suggest that
standard text-based outlines do not necessarily im-
prove writing quality (Torrance et al., 2000). Re-
cently, there has been growing interest in graph-
ical outline representations, especially for argu-
mentative essays in various domains (Scheuer et
al., 2009; Scheuer et al., 2010; Peldszus and Stede,
2013; Reed and Rowe, 2004; Reed et al., 2007).
Not only do they provide a different outlining for-
mat, but they also allow students to concretely vi-
sualize their argumentation structure. Our work
is part of the ArgumentPeer project (Falakmassir
et al., 2013), which combines computer-supported
argument diagramming and peer-review with the
goal of improving students’ writing skills.
In this paper, we follow the lead of others in dis-
course parsing for essay scoring (Burstein et al.,
2001), and we preliminarily attempt to answer two
questions: Q1) Can an argument mining system
be developed to automatically recognize the ar-
gument ontology used during diagramming, when
processing a student’s later written essay? Q2) If
so, is the number of ontological elements that can
be recognized in a student’s essay correlated with
the essay’s argumentation quality? Potentially, an-
swering these questions in the affirmative would
allow us to assist students with their writing by al-
lowing computer tutors to label sentences with the
ontology, determine which elements are missing,
and suggest adding these missing elements to im-
prove essay quality.
</bodyText>
<sectionHeader confidence="0.98821" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999965208333333">
Our corpus for argument mining consists of 52 es-
says written in two University of Pittsburgh un-
dergraduate psychology courses. In both courses,
students were asked to write an argumentative es-
say supporting two separate hypotheses that they
created based on data they were given. The aver-
age essay contains 5.2 paragraphs, 28.6 sentences,
and 592.1 words.
Before writing the essay, students were first re-
quired to generate an argument diagram justify-
ing their hypotheses using the LASAD argumen-
tation system1. LASAD argument diagrams con-
sist of nodes and arcs from an instructor-defined
ontology, as shown in Figure 1. Next, students
were required to turn their diagrams into writ-
ten argumentative essays. Automatically tagging
these essays according to the 4 node types (Cur-
rent Study, Hypothesis, Claim, Citation) and 2
arc types (Supports, Opposes) common to both
courses is the argument mining goal of this pa-
per. The tagged essay corresponding to Figure 1 is
shown in Table 1.2 While the diagram is required
to be completed by students, this work does not
utilize the student diagrams.
</bodyText>
<footnote confidence="0.998408833333333">
1http://lasad.dfki.de
2Both diagrams and papers were distributed to other stu-
dents in the class for peer review. While the diagrams were
not required to be revised, students needed to revise their es-
says to address peer feedback. To maximize diagram and es-
say similarity, here we work with only the first drafts.
</footnote>
<page confidence="0.987401">
24
</page>
<note confidence="0.3519375">
Proceedings of the First Workshop on Argumentation Mining, pages 24–28,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998999">
Figure 1: An argument diagram from a research methods course.
</figureCaption>
<bodyText confidence="0.999958875">
After the courses, expert graders were asked to
score all essays on a 5-point Likert scale (with 1
being the lowest and 5 being the highest) without
the diagrams, using a rubric with multiple crite-
ria. For the essay as a whole, graders not only
checked for correct grammar usage, but also for
flow and organization. In addition, essays were
graded based on the logic behind their argumen-
tation of their hypotheses, as well as addressing
claims that both supported and opposed their hy-
potheses. While not an explicit category, many of
the criteria required students to present multiple
citations backing their hypotheses. The average
expert score for the 52 essays is 3.03, and the me-
dian is 3, with the scores distributed as shown in
column four of Table 2.
</bodyText>
<sectionHeader confidence="0.99781" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99051925">
Essay Discourse Processing. Firstly, raw essays
are parsed for discourse connectives. Explicit dis-
course connectives are then tagged with their sense
(i.e. Expansion, Contingency, Comparison, or
Temporal) using the Discourse Connectives Tag-
ger3, as shown in Table 1.
Mining the Argument Ontology. We devel-
oped a rule-based algorithm to label each sentence
</bodyText>
<footnote confidence="0.750921">
3http://www.cis.upenn.edu/˜epitler/
discourse.html
</footnote>
<bodyText confidence="0.976123">
in an essay with at most one label from our tar-
get argument ontology. Our rules were developed
using our intuition and informal examination of 9
essays from the corpus of 52. The algorithm con-
sists of the following ordered4 rules:
Rule 1: If the sentence begins with a Compar-
ison discourse connective, or if the sentence con-
tains any string prefixes from {conflict, oppose}
and a four-digit number (intended as a year for a
citation), then tag with Opposes.
Rule 2: If the sentence begins with a Contin-
gency connective and does not contain a four-digit
number, then tag with Supports.
Rule 3: If the sentence contains a four-digit
number, then tag with Citation.
Rule 4: If the sentence contains string prefixes
from {suggest, evidence, shows, Essentially, indi-
cate} (case-sensitive), then tag with Claim.
Rule 5: If the sentence is in the first, second, or
last paragraph, and contains string prefixes from
{hypothes, predict}, or if the sentence contains the
word “should” and contains no Contingency con-
nectives, and does not contain a four-digit number
and does not contain string prefixes from {conflict,
oppose}, then tag with Hypothesis.
Rule 6: If the previous sentence was tagged
with Hypothesis, and this sentence begins with an
Expansion connective and does not contain a four-
</bodyText>
<footnote confidence="0.956215">
4When multiple rules apply, the tag of the earliest is used.
</footnote>
<page confidence="0.986266">
25
</page>
<table confidence="0.999905033333333">
# Essay Sentence Label Rule
1 The ultimate goal of this study is to investigate the relationship between Current 7
stop-sign violations and traffic activity. Study
2 To do this we analyzed two different variables on traffic activity: time of day None 8
and location.
... ... ... ...
6 Stop-signs indicate that the driver must come to a complete stop before the Claim 4
sign and check for oncoming and opposing traffic before[-Temporal] pro-
ceeding on.
7 For a stop to be considered complete the car must completely stop moving. None 8
... ... ... ...
16 The first hypothesis was: If[-Contingency] it is a high activity time of day at Hypothesis 5
an intersection then[-Contingency], there will be a higher ratio of complete
stops made than during a low activity time at the intersection.
17 The second hypothesis was: If[-Contingency] there is a busy intersection Hypothesis 5
then[-Contingency], there will be a higher ratio of complete stops made than
at an intersection that is less busy.
18 So[-Contingency] essentially, it was expected that when[-Temporal] there Supports 2
was a higher traffic activity level, either due to location or time of day, there
were to be less stop-sign violations.
19 There have been many studies which indicate that people do drive differently Claim 4
at different times of day and[-Expansion] that it does have an impact on
driving risk.
20 Reimer et al (2007) found that time of day did influence driving speed, reac- Citation 3
tion time, and speed variability measures.
... ... ... ...
24 However[-Comparison], McGarva &amp; Steiner (2000) oppose the second hy- Opposes 1
pothesis because[-Contingency] they found that provoked driver aggression
through honking horns, increased the rate of acceleration at a stop sign.
... ... ... ...
</table>
<tableCaption confidence="0.999442">
Table 1: Essay sentences, their mined ontological labels, and rules used to determine the labels, for the
</tableCaption>
<bodyText confidence="0.943800363636364">
essay associated with Figure 1. Inferred discourse connective senses are italicized in square brackets.
digit number, then tag with Hypothesis.
Rule 7: If the sentence is in the first or last para-
graph and contains at least one word from {study,
research} and does not contain the words {past,
previous, prior} (first letter case-insensitive) and
does not contain string prefixes from {hypothes,
predict} and does not contain a four-digit number,
then tag with Current Study.
Rule 8: Do not assign a tag to the sentence.
Some sample output can be found on Table 1.
Note that sentence 24 could have been tagged as
Citation using Rule 3, but because it fits the crite-
ria for Rule 1, it is tagged as Opposes.
Ontology-Based Essay Scoring. We also devel-
oped a rule-based algorithm to score each essay in
the corpus. These rules were developed using our
intuition in conjunction with the examination of
the expert grading rubric. These rules take a la-
beled essay from the argument mining algorithm
and outputs a score in the continuous range [0,5]
using the following procedure:5
</bodyText>
<listItem confidence="0.8325188">
1: Assign one point to essays that have at least
one sentence tagged with Current Study (CS).
2: Assign one point to essays that have at least
one sentence tagged with Hypothesis (H).
3: Assign one point to essays that have at least
one sentence tagged with Opposes (O).
4: Assign points based on the sum of the num-
ber of sentences tagged with Claim (Cl) and the
number of sentences tagged with Supports (S), all
divided by the number of paragraphs (#I). If this
</listItem>
<footnote confidence="0.972235">
5Score 0 occurs when no labels are assigned to the essay.
</footnote>
<page confidence="0.997694">
26
</page>
<bodyText confidence="0.828372">
value exceeds 1, assign only one point.
</bodyText>
<listItem confidence="0.956685">
5: Assign points based on the number of sen-
tences tagged with Citation (Ci) divided by the
number of paragraphs (#¶). If this value exceeds
1, assign only one point.
6: Sum all of the previously computed points.
</listItem>
<bodyText confidence="0.998674333333333">
For the three paragraph essay excerpted in Ta-
ble 1 (assigned expert score 3), there were three
sentences tagged with Current Study, three with
Hypothesis, one with Opposes, one with Sup-
ports, two with Claim and three with Citation.
The score is computed as follows:
</bodyText>
<equation confidence="0.8955255">
2Cl + 1S +
3#1f
</equation>
<sectionHeader confidence="0.999654" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999972461538461">
Since our essays do not have gold-standard on-
tology labels yet, we cannot intrinsically evaluate
the argument mining algorithm. We instead per-
formed an extrinsic evaluation via our use of the
mined argument labels for essay scoring.
The average automatic score for the corpus is
3.42 and the median is 3.5, while the correspond-
ing expert values are 3.03 and 3, respectively. A
paired t-test of the means has a significance of p &lt;
0.01, suggesting that our algorithm over-scores the
essays. We also ran a one-sample t-test on each ex-
pert score value to see if the automatic scores were
similar to the expert scores. We hypothesized that
within each expert score category predicted accu-
rately, we should not see a significant difference (p
≥ 0.05). Table 2 shows that while the automatic
score is not significantly different for expert score
4, the scores are significantly different for scores 2
and 3.
We also examined the Spearman’s rank corre-
lation between the computed and expert scores.6
We see that the Spearman’s rank correlation shows
significance of p &lt; 0.0001 with a rho value of
0.997. Together these metrics suggest that our au-
tomated scores are currently useful for ranking but
not for rating.
</bodyText>
<sectionHeader confidence="0.979143" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9994806">
We have presented simple rule-based algorithms
for argumentation mining in student essays and
essay scoring using argument mining. Based on
preliminary extrinsic evaluation, our pattern-based
recognition of a basic argumentation ontology
</bodyText>
<footnote confidence="0.605876">
6A Pearson correlation did not give significant results.
</footnote>
<table confidence="0.998005">
expert avg. auto t n p
score score
1 4.33 – 1 –
2 3.23 3.21 8 0.013
3 3.30 2.10 31 0.044
4 3.80 -1.00 12 0.337
</table>
<tableCaption confidence="0.999402">
Table 2: One-sample t-test results for scores.
</tableCaption>
<bodyText confidence="0.999695305555556">
seems to provide some insight into essay scores
across two courses. While the automatic scores
did not necessarily reflect the expert scores, the
ranking correlation demonstrated that more argu-
mentative elements were related to higher scores.
Even with the limitations of this study (e.g. no in-
trinsic evaluation, a small essay corpus, a limited
argument ontology, a scoring algorithm using only
ontology features, application of discourse con-
nector for a different genre), our results suggest
the promise of using argument mining to trigger
feedback in a writing tutoring system.
To develop a more linguistically sophisticated
and accurate argument mining algorithm, our fu-
ture plans include exploiting discourse informa-
tion beyond connectives, e.g., by parsing our es-
says in terms of PDTB (Lin et al., 2011) or RST
relations (Feng and Hirst, 2012). We also plan to
look at the helpfulness of argumentation schemes
(Feng and Hirst, 2011), and other linguistic and
essay features for automatic evaluation (Crossley
and McNamara, 2010). In addition, our essays
are being annotated with diagram ontology labels,
which will enable us to use machine learning to
conduct intrinsic argument mining evaluations and
to learn the weights for each rule or determine new
rules. Finally, we plan to explore using the dia-
grams to bootstrap the essay annotation process.
While some sentences in an essay can easily be
mapped to the corresponding diagram (e.g. sen-
tence 1 in Table 1 to node 1 in Figure 1), the com-
plication is that essays tend to be more fleshed-out
than diagrams, and at least in our corpus, also con-
tain argument changes motivated by diagram peer-
review. While sentence 6 in Table 1 is correctly
tagged as a Claim, this content is not in Figure 1.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996830333333333">
This work is supported by NSF Award 1122504.
We thank Huy Nguyen, Wenting Xiong, and
Michael Lipschultz.
</bodyText>
<equation confidence="0.846185666666667">
1CS + 1H + 1O +
3Ci = 5
3#1f
</equation>
<page confidence="0.994537">
27
</page>
<note confidence="0.9762788">
2009 Conference on Artificial Intelligence in Educa-
tion: Building Learning Systems That Care: From
Knowledge Representation to Affective Modelling,
pages 629–631, Amsterdam, The Netherlands, The
Netherlands. IOS Press.
References
[Burstein et al.2001] Jill Burstein, Karen Kukich, Su-
sanne Wolff, Ji Lu, and Martin Chodorow. 2001.
Enriching automated essay scoring using discourse
marking. ERIC Clearinghouse.
</note>
<reference confidence="0.995940633333333">
[Crossley and McNamara2010] Scott A Crossley and
Danielle S McNamara. 2010. Cohesion, coher-
ence, and expert evaluations of writing proficiency.
In Proceedings of the 32nd annual conference of the
Cognitive Science Society, pages 984–989. Austin,
TX: Cognitive ScienceSociety.
[Falakmassir et al.2013] Mohammad Falakmassir,
Kevin Ashley, and Christian Schunn. 2013. Using
argument diagramming to improve peer grading
of writing assignments. In Proceedings of the 1st
Workshop on Massive Open Online Courses at the
16th Annual Conference on Artificial Intelligence in
Education, Memphis, TN.
[Feng and Hirst2011] Vanessa Wei Feng and Graeme
Hirst. 2011. Classifying arguments by scheme. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 987–996. As-
sociation for Computational Linguistics.
[Feng and Hirst2012] Vanessa Wei Feng and Graeme
Hirst. 2012. Text-level discourse parsing with rich
linguistic features. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 60–68.
Association for Computational Linguistics.
[Lin et al.2011] Ziheng Lin, Hwee Tou Ng, and Min-
Yen Kan. 2011. Automatically evaluating text
coherence using discourse relations. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1, pages 997–1006. Associa-
tion for Computational Linguistics.
[Peldszus and Stede2013] Andreas Peldszus and Man-
fred Stede. 2013. From argument diagrams to argu-
mentation mining in texts: A survey. International
Journal of Cognitive Informatics and Natural Intel-
ligence (IJCINI), 7(1):1–31.
[Reed and Rowe2004] Chris Reed and Glenn Rowe.
2004. Araucaria: Software for argument analy-
sis, diagramming and representation. International
Journal on Artificial Intelligence Tools, 13(04):961–
979.
[Reed et al.2007] Chris Reed, Douglas Walton, and
Fabrizio Macagno. 2007. Argument diagramming
in logic, law and artificial intelligence. The Knowl-
edge Engineering Review, 22(01):87–109.
[Scheuer et al.2009] Oliver Scheuer, Bruce M.
McLaren, Frank Loll, and Niels Pinkwart. 2009.
An analysis and feedback infrastructure for argu-
mentation learning systems. In Proceedings of the
[Scheuer et al.2010] Oliver Scheuer, Frank Loll, Niels
Pinkwart, and Bruce M. McLaren. 2010. Computer-
supported argumentation: A review of the state
of the art. International Journal of Computer-
Supported Collaborative Learning, 5(1):43–102.
[Torrance et al.2000] Mark Torrance, Glyn V. Thomas,
and Elizabeth J. Robinson. 2000. Individual differ-
ences in undergraduate essay-writing strategies: A
longitudinal study. Higher Education, 39(2):181–
200.
</reference>
<page confidence="0.999067">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.817609">
<title confidence="0.999891">Ontology-Based Argument Mining and Automatic Essay Scoring</title>
<author confidence="0.997922">Nathan Ong</author>
<author confidence="0.997922">Diane Litman</author>
<author confidence="0.997922">Alexandra</author>
<affiliation confidence="0.999103">Department of Computer Science, University of</affiliation>
<address confidence="0.967162">Pittsburgh, PA 15260</address>
<note confidence="0.906897">nro5,dlitman,apb27@pitt.edu</note>
<abstract confidence="0.994038818181818">Essays are frequently used as a medium for teaching and evaluating argumentation skills. Recently, there has been interest in diagrammatic outlining as a replacement to the written outline that often precedes essay writing. This paper presents a preliminary approach for automatically identifying diagram ontology elements in essays, and demonstrates its positive correlation with expert scores of essay quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Scott A Crossley</author>
<author>Danielle S McNamara</author>
</authors>
<title>Cohesion, coherence, and expert evaluations of writing proficiency.</title>
<date>2010</date>
<booktitle>In Proceedings of the 32nd annual conference of the Cognitive Science Society,</booktitle>
<pages>984--989</pages>
<institution>Cognitive ScienceSociety.</institution>
<location>Austin, TX:</location>
<marker>[Crossley and McNamara2010]</marker>
<rawString>Scott A Crossley and Danielle S McNamara. 2010. Cohesion, coherence, and expert evaluations of writing proficiency. In Proceedings of the 32nd annual conference of the Cognitive Science Society, pages 984–989. Austin, TX: Cognitive ScienceSociety.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Falakmassir</author>
<author>Kevin Ashley</author>
<author>Christian Schunn</author>
</authors>
<title>Using argument diagramming to improve peer grading of writing assignments.</title>
<date>2013</date>
<booktitle>In Proceedings of the 1st Workshop on Massive Open Online Courses at the 16th Annual Conference on Artificial Intelligence in Education,</booktitle>
<location>Memphis, TN.</location>
<marker>[Falakmassir et al.2013]</marker>
<rawString>Mohammad Falakmassir, Kevin Ashley, and Christian Schunn. 2013. Using argument diagramming to improve peer grading of writing assignments. In Proceedings of the 1st Workshop on Massive Open Online Courses at the 16th Annual Conference on Artificial Intelligence in Education, Memphis, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Classifying arguments by scheme.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>987--996</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Feng and Hirst2011]</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2011. Classifying arguments by scheme. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 987–996. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Text-level discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Feng and Hirst2012]</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 60–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>MinYen Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>997--1006</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>[Lin et al.2011]</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and MinYen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 997–1006. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Peldszus</author>
<author>Manfred Stede</author>
</authors>
<title>From argument diagrams to argumentation mining in texts: A survey.</title>
<date>2013</date>
<journal>International Journal of Cognitive Informatics and Natural Intelligence (IJCINI),</journal>
<volume>7</volume>
<issue>1</issue>
<marker>[Peldszus and Stede2013]</marker>
<rawString>Andreas Peldszus and Manfred Stede. 2013. From argument diagrams to argumentation mining in texts: A survey. International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Reed</author>
<author>Glenn Rowe</author>
</authors>
<title>Araucaria: Software for argument analysis, diagramming and representation.</title>
<date>2004</date>
<journal>International Journal on Artificial Intelligence Tools,</journal>
<volume>13</volume>
<issue>04</issue>
<pages>979</pages>
<marker>[Reed and Rowe2004]</marker>
<rawString>Chris Reed and Glenn Rowe. 2004. Araucaria: Software for argument analysis, diagramming and representation. International Journal on Artificial Intelligence Tools, 13(04):961– 979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Reed</author>
<author>Douglas Walton</author>
<author>Fabrizio Macagno</author>
</authors>
<title>Argument diagramming in logic, law and artificial intelligence.</title>
<date>2007</date>
<journal>The Knowledge Engineering Review,</journal>
<volume>22</volume>
<issue>01</issue>
<marker>[Reed et al.2007]</marker>
<rawString>Chris Reed, Douglas Walton, and Fabrizio Macagno. 2007. Argument diagramming in logic, law and artificial intelligence. The Knowledge Engineering Review, 22(01):87–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Scheuer</author>
<author>Bruce M McLaren</author>
<author>Frank Loll</author>
<author>Niels Pinkwart</author>
</authors>
<title>An analysis and feedback infrastructure for argumentation learning systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the</booktitle>
<marker>[Scheuer et al.2009]</marker>
<rawString>Oliver Scheuer, Bruce M. McLaren, Frank Loll, and Niels Pinkwart. 2009. An analysis and feedback infrastructure for argumentation learning systems. In Proceedings of the</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Scheuer</author>
<author>Frank Loll</author>
<author>Niels Pinkwart</author>
<author>Bruce M McLaren</author>
</authors>
<title>Computersupported argumentation: A review of the state of the art.</title>
<date>2010</date>
<journal>International Journal of ComputerSupported Collaborative Learning,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>[Scheuer et al.2010]</marker>
<rawString>Oliver Scheuer, Frank Loll, Niels Pinkwart, and Bruce M. McLaren. 2010. Computersupported argumentation: A review of the state of the art. International Journal of ComputerSupported Collaborative Learning, 5(1):43–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Torrance</author>
<author>Glyn V Thomas</author>
<author>Elizabeth J Robinson</author>
</authors>
<title>Individual differences in undergraduate essay-writing strategies: A longitudinal study.</title>
<date>2000</date>
<journal>Higher Education,</journal>
<volume>39</volume>
<issue>2</issue>
<pages>200</pages>
<marker>[Torrance et al.2000]</marker>
<rawString>Mark Torrance, Glyn V. Thomas, and Elizabeth J. Robinson. 2000. Individual differences in undergraduate essay-writing strategies: A longitudinal study. Higher Education, 39(2):181– 200.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
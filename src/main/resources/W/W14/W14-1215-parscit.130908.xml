<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032544">
<title confidence="0.990558">
An evaluation of syntactic simplification rules for people with autism
</title>
<author confidence="0.992801">
Richard Evans, Constantin Or˘asan and Iustin Dornescu
</author>
<affiliation confidence="0.985482">
Research Institute in Information and Language Processing
University of Wolverhampton
</affiliation>
<address confidence="0.62762">
United Kingdom
</address>
<email confidence="0.996366">
{R.J.Evans, C.Orasan, I.Dornescu2}@wlv.ac.uk
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999618928571429">
Syntactically complex sentences consti-
tute an obstacle for some people with
Autistic Spectrum Disorders. This pa-
per evaluates a set of simplification rules
specifically designed for tackling complex
and compound sentences. In total, 127 dif-
ferent rules were developed for the rewrit-
ing of complex sentences and 56 for the
rewriting of compound sentences. The
evaluation assessed the accuracy of these
rules individually and revealed that fully
automatic conversion of these sentences
into a more accessible form is not very re-
liable.
</bodyText>
<sectionHeader confidence="0.998763" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999413363636364">
People with Autistic Spectrum Disorders (ASD)
show a diverse range of reading abilities: on
the one hand, 5%-10% of users have the capac-
ity to read words from an early age without the
need for formal learning (hyperlexia), on the other
hand many users demonstrate weak comprehen-
sion of what has been read (Volkmar and Wiesner,
2009). They may have difficulty inferring contex-
tual information or may have trouble understand-
ing mental verbs or emotional language, as well
as long sentences with complex syntactic structure
(Tager-Flusberg, 1981; Kover et al., 2012). To ad-
dress these difficulties, the FIRST project1 is de-
veloping a tool which makes texts more accessible
for people with ASD. In order to get a better un-
derstanding of the needs of these readers, a thor-
ough analysis was carried out to derive a list of
high priority obstacles to reading comprehension.
Some of these obstacles are related to syntactic
complexity and constitute the focus of this paper.
Even though the research in the FIRST project fo-
cuses on people with ASD, many of the obstacles
</bodyText>
<footnote confidence="0.901891">
1http://first-asd.eu
</footnote>
<bodyText confidence="0.999938617647059">
identified in the project can pose difficulties for a
wide range of readers such as language learners
and people with other language disorders.
This paper presents and evaluates a set of rules
used for simplifying English complex and com-
pound sentences. These rules were developed as
part of a syntactic simplification system which was
initially developed for users with ASD, but which
can also be used for other tasks that require syn-
tactic simplification of sentences. In our research,
we consider that syntactic complexity is usually
indicated by the occurrence of certain markers or
signs of syntactic complexity, referred to hereafter
as signs, such as punctuation ([,] and [;]), con-
junctions ([and], [but], and [or]), complementis-
ers ([that]) or wh-words ([what], [when], [where],
[which], [while], [who]). These signs may have
a range of syntactic linking and bounding func-
tions which need to be automatically identified,
and which we analysed in more detail in (Evans
and Orasan, 2013).
Our syntactic simplification process operates in
two steps. In the first, signs of syntactic complex-
ity are automatically classified and in the second,
manually crafted rules are applied to simplify the
relevant sentences. Section 3 presents more details
about the method. Evaluation of automatic simpli-
fication is a difficult issue. Given that the purpose
of this paper is to gain a better understanding of
the performance of the rules used for simplifying
compound sentences and complex sentences, Sec-
tion 4 presents the methodology developed for this
evaluation and discusses the results obtained. The
paper finishes with conclusions.
</bodyText>
<sectionHeader confidence="0.934736" genericHeader="introduction">
2 Background information
</sectionHeader>
<bodyText confidence="0.9946254">
Despite some findings to the contrary (Arya et al.,
2011), automatic syntactic simplification has been
motivated by numerous neurolinguistic and psy-
cholinguistic studies. Brain imaging studies indi-
cate that processing syntactically complex struc-
</bodyText>
<page confidence="0.977098">
131
</page>
<note confidence="0.9941255">
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131–140,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999934274509804">
tures requires more neurological activity than pro-
cessing simple structures (Just et al., 1996). A
study undertaken by Levy et al. (2012) showed
that people with aphasia are better able to un-
derstand syntactically simple reversible sentences
than syntactically complex ones.
Further motivation is brought by research in
NLP, which demonstrates that performance levels
in information extraction (Agarwal and Boggess,
1992; Rindflesch et al., 2000; Evans, 2011),
syntactic parsing (Tomita, 1985; McDonald and
Nivre, 2011), and, to some extent, machine trans-
lation (Gerber and Hovy, 1998) are somewhat de-
termined by the length and syntactic complexity of
the sentences being processed.
Numerous rule-based methods for syntactic
simplification have been developed (Siddharthan,
2006) and used to facilitate NLP tasks such as
biomedical information extraction (Agarwal and
Boggess, 1992; Rindflesch et al., 2000; Evans,
2011). In these approaches, rules are triggered
by pattern-matching applied to the output of text
analysis tool such as partial parsers and POS tag-
gers. Chandrasekar and Srinivas (1997) presented
an automatic method to learn syntactic simplifi-
cation rules for use in such systems. Unfortu-
nately, that approach is only capable of learning
a restricted range of rules and requires access to
expensive annotated resources.
With regard to applications improving text ac-
cessibility for human readers, Max (2000) de-
scribed the use of syntactic simplification for
aphasic readers. In work on the PSET project,
Canning (2002) implemented a system which ex-
ploits a syntactic parser in order to rewrite com-
pound sentences as sequences of simple sentences
and to convert passive sentences into active ones
for readers with aphasia. The success of these sys-
tems is tied to the performance levels of the syn-
tactic parsers that they employ.
More recently, the availability of resources such
as Simple Wikipedia has enabled text simplifi-
cation to be included in the paradigm of statis-
tical machine translation (Yatskar et al., 2010;
Coster and Kauchak, 2011). In this context,
translation models are learned by aligning sen-
tences in Wikipedia with their corresponding ver-
sions in Simple Wikipedia. Manifesting Basic En-
glish (Ogden, 1932), the extent to which Simple
Wikipedia is accessible to people with autism has
not yet been fully assessed.
The field of text summarisation includes numer-
ous approaches that can be regarded as examples
of syntactic simplification. For example, Cohn and
Lapata (2009) present a tree-to-tree transduction
method that is used to filter non-essential infor-
mation from syntactically parsed sentences. This
compression process often reduces the syntactic
complexity of those sentences. An advantage of
this approach is that it can identify elements for
deletion even when such elements are not indi-
cated by explicit signs of syntactic complexity.
The difficulty is that they rely on high levels of ac-
curacy and granularity of automatic syntactic anal-
ysis. As noted earlier, it has been observed that the
accuracy of parsers is inversely proportional to the
length and complexity of the sentences being anal-
ysed (Tomita, 1985; McDonald and Nivre, 2011).
The approach to syntactic simplification de-
scribed in the current paper is a two step pro-
cess involving detection and tagging of the bound-
ing and linking functions of various signs of syn-
tactic complexity followed by a rule-based sen-
tence rewriting step. Relevant to the first step, Van
Delden and Gomez (2002) developed a machine
learning method to determine the syntactic roles
of commas. Meier et al. (2012) describe German
language resources in which the linking functions
of commas and semicolons are annotated. The an-
notated resources exploited by the machine learn-
ing method presented in Section 3.2.1 of the cur-
rent paper are presented in (Evans and Orasan,
2013). From a linguistic perspective, Nunberg et
al. (2002) provide a grammatical analysis of punc-
tuation in English.
The work described in this paper was under-
taken in a project aiming to improve the accessibil-
ity of text for people with autism. It was motivated
at least in part by the work of O’Connor and Klein
(2004), which describes strategies to facilitate the
reading comprehension of people with ASD.
The proposed method is intended to reduce
complexity caused by both complex and com-
pound sentences and differs from those described
earlier in this section. Sentence compression
methods are not suitable for the types of rewrit-
ing required in simplifying compound sentences.
Parsers are more likely to have lower accuracy
when processing these sentences, and therefore the
proposed method does not use information about
the syntactic structure of sentences in the process.
Our method is presented in the next section.
</bodyText>
<page confidence="0.998548">
132
</page>
<sectionHeader confidence="0.979306" genericHeader="method">
3 The syntactic simplifier
</sectionHeader>
<bodyText confidence="0.999859733333333">
In our research, we regard coordination and sub-
ordination as key elements of syntactic complex-
ity. A thorough study of the potential obstacles to
the reading comprehension of people with autism
highlighted particular types of syntactic complex-
ity, many of which are linked to coordination
and subordination. Section 3.1 briefly presents
the main obstacles linked to syntactic complexity
identified by the study. It should be mentioned that
most of the obstacles are problematic not only for
autistic people and other types of reader can also
benefit from their removal. The obstacles identi-
fied constituted the basis for developing the sim-
plification approach briefly described in Section
3.2.
</bodyText>
<subsectionHeader confidence="0.998801">
3.1 User requirements
</subsectionHeader>
<bodyText confidence="0.99982504">
Consultations with 94 subjects meeting the strict
DSM-IV criteria for ASD and with IQ &gt; 70 led to
the derivation of user preferences and high priority
user requirements related to structural processing.
A comprehensive explanation of the findings can
be found in (Martos et al., 2013). This section dis-
cusses briefly the two types of information of rel-
evance to the processing of sentence complexity
obtained in our study.
First, in terms of the demand for access to texts
of particular genres/domains, it was found that
young people (aged 12-16) seek access to doc-
uments in informative (arts/leisure) domains and
they have less interest in periodicals and newspa-
pers or imaginative texts. Adults (aged 16+) seek
access to informative and scientific texts (includ-
ing newspapers), imaginative text, and the lan-
guage of social networking and communication.
In an attempt to accommodate the interests of both
young people and adults, we developed a cor-
pus which contains newspaper articles, texts about
health, and literary texts.
Second, the specific morpho-syntactic phenom-
ena that pose obstacles to reading comprehension
that are relevant to this paper are:
</bodyText>
<listItem confidence="0.995727833333333">
1. Compound sentences, which should be split
into sentences containing a single clause.
2. Complex sentences: in which relative clauses
should either be:
(a) converted into adjectival pre-modifiers
or
</listItem>
<bodyText confidence="0.967115388888889">
(b) deleted from complex sentences and
used to generate copular constructions
linking the NP in the matrix clause with
the predication of the relative clause
In addition, the analysis revealed other types
of obstacles such as explicative clauses, which
should be deleted, and uncommon conjunctions
(including conjuncts) which should be replaced
by more common ones. Conditional clauses that
follow the main clause and non-initial adverbial
clauses should be pre-posed, and passive sen-
tences should be converted in the active form. Var-
ious formatting issues such as page breaks that oc-
cur within paragraphs and end-of-line hyphenation
are also problematic and should be avoided.
Section 3.2 describes the method developed to
address the obstacles caused by compound and
complex sentences.
</bodyText>
<subsectionHeader confidence="0.999749">
3.2 The approach
</subsectionHeader>
<bodyText confidence="0.9993212">
Processing of obstacles to reading comprehension
in this research has focused on detection and re-
duction of syntactic complexity caused by the oc-
currence in text of compound sentences (1) and
complex sentences (2).
</bodyText>
<listItem confidence="0.900201166666667">
(1) Elaine Trego never bonded with 16-month-old
Jacob [and] he was often seen with bruises, a
murder trial was told.
(2) The two other patients, who are far more
fragile than me, would have been killed by
the move.
</listItem>
<bodyText confidence="0.999931933333333">
In (1), the underlined phrases are the conjoins
of a coordinate constituent. In (2), the underlined
phrase is a subordinate constituent of the larger,
superordinate phrase the two other patients, who
are far more fragile than me.
The overall syntactic simplification pipeline
consists of the following steps:
Step 1. Tagging of signs of syntactic complexity
with information about their syntactic linking
or bounding functions
Step 2. The complexity of sentences tagged in
step 1 is assessed and used to trigger the ap-
plication of two iterative simplification pro-
cesses, which are applied exhaustively and
sequentially to each input sentence:
</bodyText>
<page confidence="0.995173">
133
</page>
<bodyText confidence="0.996381142857143">
a. Decomposition of compound sentences
(the simplification function converts one
input string into two output strings)
b. Decomposition of complex sentences
(the simplification function converts one
input string into two output strings)
Step 3. Personalised transformation of sentences
according to user preference profiles which
list obstacles to be tackled and the threshold
complexity levels that specify whether sim-
plification is necessary.
Steps 1 and 2 are applied iteratively ensuring
that an input sentence can be exhaustively simpli-
fied by decomposition of the input string into pairs
of progressively simpler sentences. No further
simplification is applied to a sentence when the
system is unable to detect any signs of syntactic
complexity within it. This paper reports on steps 1
and 2. The personalisation step, which takes into
consideration the needs of individual users, is not
discussed.
</bodyText>
<subsectionHeader confidence="0.994054">
3.2.1 Identification of signs of complexity
</subsectionHeader>
<bodyText confidence="0.99997268">
Signs of syntactic complexity typically indicate
constituent boundaries, e.g. punctuation marks,
conjunctions, and complementisers. To facilitate
information extraction, a rule-based approach to
simplify coordinated conjoins was proposed by
Evans (2011), which relies on classifying signs
based on their linking functions.
In more recent work, an extended annotation
scheme was proposed in (Evans and Orasan, 2013)
which enables the encoding of links and bound-
aries between a wider range of syntactic con-
stituents and covers more syntactic phenomena.
A corpus covering three text categories (news ar-
ticles, literature, and patient healthcare informa-
tion leaflets), was annotated using this extended
scheme.2
Most sign labels contain three types of infor-
mation: boundary type, syntactic projection level,
and grammatical category of the constituent(s).
Some labels cover signs which bound interjec-
tions, tag questions, and reported speech and a
class denoting false signs of syntactic complex-
ity, such as use of the word that as a specifier or
anaphor. The class labels are a combination of the
following acronyms:
</bodyText>
<footnote confidence="0.963712">
2http://clg.wlv.ac.uk/resources/
SignsOfSyntacticComplexity/
</footnote>
<listItem confidence="0.980072866666667">
1. {C|SS|ES}, the generic function as a coor-
dinator (C), the left boundary of a subordi-
nate constituent (SS), or the right boundary
of a subordinate constituent (ES).
2. {P|L|I|M|E}, the syntactic projection level
of the constituent(s): prefix (P), lexical (L),
intermediate (I), maximal (M), or extended/-
clausal (E).
3. {A|Adv|N|P|Q|V }, the grammatical cate-
gory of the constituent(s): adjectival (A), ad-
verbial (Adv), nominal (N), prepositional (P),
quantificational (Q), and verbal (V).
4. {1|2}, used to further differentiate sub-
classes on the basis of some other label-
specific criterion.
</listItem>
<bodyText confidence="0.999856">
The scheme uses a total of 42 labels to distin-
guish between different syntactic functions of the
bounded constituents. Although signs are marked
by a small set of tokens (words and punctuation),
the high number of labels and their skewed dis-
tribution make signs highly ambiguous. In addi-
tion, each sign is only assigned exactly one label,
i.e. that of the dominant constituent in the case of
nesting, further increasing ambiguity. These char-
acteristics make automatic classification of signs
challenging.
The automatic classification of signs of syntac-
tic complexity is achieved using a machine learn-
ing approach described in more detail in Dornescu
et al. (2013). After experimenting with several
methods of representing the training data and with
several classifiers, the best results were obtained
by using the BIO model to train a CRF tagger. The
features used were the signs’ surrounding con-
text (a window of 10 tokens and their POS tags)
together with information about the distance to
other signs signs in the same sentence and their
types. The method achieved an overall accuracy
of 82.50% (using 10 fold cross-validation) on the
manually annotated corpus.
</bodyText>
<subsectionHeader confidence="0.891513">
3.2.2 Rule-based approach to simplification
of compound sentences and complex
sentences
</subsectionHeader>
<bodyText confidence="0.9999168">
The simplification method exploits two iterative
processes that are applied in sequence to input
text that has been tokenised with respect to sen-
tences, words, punctuation, and signs of syntac-
tic complexity. The word tokens in the input text
</bodyText>
<page confidence="0.992876">
134
</page>
<table confidence="0.998679222222222">
Rule ID CEV-12
Sentence type Compound (coordination)
Match pattern A that [B] signCEV [C] .
Transform pattern A that [B]. A that [C].
Ex: input [Investigations showed]a that [the glass came from a car’s side window]s andCEV
[thousands of batches had been tampered with on five separate weekends]C.
Ex: output [Investigations showed]a that [the glass came from a car’s side window]s.
[Investigations showed]a that [thousands of batches had been tampered with on five
separate weekends]C.
Rule ID CEV-26
Sentence type Compound (coordination)
Match pattern A vCC B: “[C] signCEV [D]”.
Transform pattern A v B: “[C]”. A v B: “[D]”.
Ex: input [He]a added[]s: “[If I were with Devon and Cornwall police I’d be very interested in
the result of this case]C andCEV [I certainly expect them to renew their interest]D.”
Ex: output [He]a added[]s: “[If I were with Devon and Cornwall police I’d be very interested in
the result of this case]C.”
[He]a added[]s: “[I certainly expect them to renew their interest]D.”
</table>
<tableCaption confidence="0.99997">
Table 1: Patterns used to identify conjoined clauses.
</tableCaption>
<bodyText confidence="0.993855925373134">
have also been labelled with their parts of speech
and the signs have been labelled with their gram-
matical linking and bounding functions. The pat-
terns rely mainly on nine sign labels which delimit
clauses (*EV)3, noun phrases (*MN) and adjecti-
val phrases (*MA). These sign labels can signal
either coordinated conjoins (C*) or the start (SS*)
or end (ES*) of a constituent.
The first iterative process exploits patterns in-
tended to identify the conjoins of compound sen-
tences. The elements common to these patterns
are signs tagged as linking clauses in coordination
(label CEV). The second process exploits patterns
intended to identify relative clauses in complex
sentences. The elements common to these patterns
are signs tagged as being left boundaries of subor-
dinate clauses (label SSEV).
The identification of conjoint clauses depends
on accurate tagging of words with information
about their parts of speech and signs with informa-
tion about their general roles in indicating the left
or right boundaries of subordinate constituents.
The identification of subordinate clauses requires
more detailed information. In addition to the in-
formation required to identify clause conjoins, in-
formation about the specific functions of signs is
required. The simplification process is thus highly
dependent on the performance of the automatic
sign tagger.
Table 1 displays two patterns for identifying
conjoined clauses and Table 2 displays two pat-
terns for identifying subordinate clauses. In the
3In these example the * character is used to indicate any
sequence of characters, representing the bounding or linking
function of the sign.
tables, upper case letters denote contiguous se-
quences of text,4 the underbar denotes signs of
class CEV (in row Compound) and SSEV (in row
Complex). Verbs with clause complements are
denoted by vCC, while words of part of speech
X are denoted by wX. The symbol s is used
to denote additional signs of syntactic complex-
ity while v denotes words with verbal POS tags.
Words explicitly appearing in the input text are
italicised. Elements of the patterns representing
clause conjoins and subordinate clauses appear in
square brackets.
Each pattern is associated with a sentence
rewriting rule. A rule is applied on each itera-
tion of the algorithm. Sentences containing signs
which correspond to conjoint clauses are con-
verted into two strings which are identical to the
original save that, in one, the conjoint clause is
replaced by a single conjoin identified in the con-
joint while in the other, the identified conjoin is
omitted. Sentences containing signs which indi-
cate subordinate clauses are converted into two
new strings. One is identical to the original save
that the relative clause is deleted. The second is
automatically generated, and consists of the NP in
the matrix clause modified by the relative clause, a
conjugated copula, and the predication of the rela-
tive clause. Tables 1 and 2 give examples of trans-
formation rules for the given patterns. In total,
127 different rules were developed for the rewrit-
ing of complex sentences and 56 for the rewriting
of compound sentences.
</bodyText>
<footnote confidence="0.990221">
4Note that these sequences of text may contain additional
signs tagged CEV or SSEV.
</footnote>
<page confidence="0.994733">
135
</page>
<table confidence="0.975824153846154">
Rule ID SSEV-61
Sentence type Complex (subordination)
Match pattern A s B [signSSEV C v D].
Transform pattern A s B. That C v D.
Ex: input [During the two-week trial, the jury heard how Thomas became a frequent visitor to
Roberts’s shop in the summer of 1997]A, [after meeting him through a friend]B [who
[lived near the shop,]C [described as a ”child magnet” by one officer]D.
Ex: output [During the two-week trial, the jury heard how Thomas became a frequent visitor to
Roberts’s shop in the summer of 1997]A, [after meeting him through a friend]B.
That friend [lived near the shop,]C [described as a ”child magnet” by one officer]D.
Rule ID SSEV-72
Sentence type Complex (subordination)
Match pattern [A wIN wDT* n {n1of1* signSSEV ] wV BD B {.1?1!1
</table>
<tableCaption confidence="0.768826">
Transform pattern N/A
Pattern SSEV-72 is used to prevent rewriting of complex sentences when the subordinate
clause is the argument of a clause complement verb. The result of this rule is to strip the
tag from the triggering sign of syntactic complexity
Ex: input [Eamon Reidy, 32,]A fled [across fields in Windsor Great Park after the crash[, the court
heard.]
Table 2: Patterns used to identify subordinate clauses.
</tableCaption>
<sectionHeader confidence="0.997205" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999872">
The detection and classification of signs of syntac-
tic complexity can be evaluated via standard meth-
ods in LT based on comparing classifications made
by the system with classifications made by linguis-
tic experts. This evaluation is reported in (Dor-
nescu et al., 2013). Unfortunately, the evaluation
of the actual simplification process is difficult, as
there are no well established methods for measur-
ing its accuracy. Potential methodologies for eval-
uation include comparison of system output with
human simplification of a given text, analysis of
the post-editing effort required to convert an au-
tomatically simplified text into a suitable form for
end users, comparisons using experimental meth-
ods such as eye tracking and extrinsic evaluation
via NLP applications such as information extrac-
tion, all of which have weaknesses in terms of ad-
equacy and expense.
Due to the challenges posed by these previously
established methods, we decided that before we
employ them and evaluate the output of the sys-
tem as a whole, we focus first on the evaluation
of the accuracy of the two rule sets employed by
the syntactic processor. The evaluation method is
based on comparing sets of simplified sentences
derived from an original sentence by linguistic ex-
perts with sets derived by the method described in
Section 3.
</bodyText>
<subsectionHeader confidence="0.979064">
4.1 The gold standard
</subsectionHeader>
<bodyText confidence="0.999973580645161">
Two gold standards were developed to support
evaluation of the two rule sets. Texts from the gen-
res of health, literature, and news were processed
by different versions of the syntactic simplifier. In
one case, the only rules activated in the syntac-
tic simplifier were those concerned with rewriting
compound sentences. In the second case, the only
rules activated were those concerned with rewrit-
ing complex sentences. The output of the two ver-
sions was corrected by a linguistic expert to ensure
that each generated sentence was grammatically
well-formed and consistent in meaning with the
original sentence. Sentences for which even man-
ual rewriting led to the generation of grammati-
cally well-formed sentences that were not consis-
tent in meaning with the originals were removed
from the test data. After filtering, the test data
contained nearly 1,500 sentences for use in eval-
uating rules to simplify of compound sentences,
and nearly 1,100 sentences in the set used in eval-
uating rules to simplify complex sentences. The
break down per genre/domain is given in Tables
3a and 3b.
The subset of sentences included in the gold
standard contained manually annotated informa-
tion about the signs of syntactic complexity. This
was done to enable reporting of the evaluation re-
sults in two modes: one in which the system con-
sults an oracle for classification of signs of syntac-
tic complexity and one in which the system con-
sults the output of the automatic sign tagger.
</bodyText>
<subsectionHeader confidence="0.987964">
4.2 Evaluation results
</subsectionHeader>
<bodyText confidence="0.99981375">
Evaluation results are reported in terms of accu-
racy of the simplification process and the change
in readability of the generated sentences. Com-
putation of accuracy is based on the mean Leven-
</bodyText>
<page confidence="0.995557">
136
</page>
<table confidence="0.9981846">
News Text category News Text category
Health Literature Health Literature
#Compound sentences 698 325 418 #Complex sentences 369 335 379
Accuracy Oracle 0.758 0.612 0.246 Accuracy Oracle 0.452 0.292 0.475
Classifier 0.314 0.443 0.115 Classifier 0.433 0.227 0.259
DFlesch Oracle 11.1 8.2 15.3 DFlesch Oracle 2.5 0.8 2.3
Classifier 9.9 10.2 13.6 Classifier 2.3 0.9 2.3
DAvg. Oracle -12.58 -9.86 -16.69 DAvg. Oracle -2.96 -0.90 -2.80
Sent. Len. Classifier -13.08 -12.30 -16.79 Sent. Len. Classifier -2.80 -0.99 -2.11
(a) Evaluation of simplification of compound sentences (b) Evaluation of simplification of complex sentences
</table>
<tableCaption confidence="0.999905">
Table 3: Evaluation results for the two syntactic phenomena on three text genres
</tableCaption>
<bodyText confidence="0.995398766233766">
shtein similarity5 between the sentences generated
by the system and the most similar simplified sen-
tences verified by the linguistic expert. Once the
most similar sentence in the key has been found,
that element is no longer considered for the rest of
the simplified sentences in the system’s response
to the original. In this evaluation, sentences are
considered to be converted correctly if their LS &gt;
0.95. The reason for setting such a high threshold
for the Levenshtein ratio is because the evaluation
method should only reward system responses that
match the gold standard almost perfectly save for a
few characters which could be caused by typos or
variations in the use of punctuation and spaces. A
sentence is considered successfully simplified, and
implicitly all the rules used in the process are con-
sidered correctly applied, when all the sentences
produced by the system are converted correctly ac-
cording to the gold standard. This evaluation ap-
proach may be considered too inflexible as it does
not take into consideration the fact that a sentence
can be simplified in several ways. However, the
purpose here is to evaluate the way in which sen-
tences are simplified using specific rules.
In order to calculate the readability of the gen-
erated sentences we initially used the Flesch score
(Flesch, 1949). However, our system changes the
text only by rewriting sentences into sequences of
simpler sentences and does not make any changes
at the lexical level. For this reason, any changes
observed in the Flesch score are due to changes
in the average sentence length. Therefore, for our
experiments we report both ΔFlesch score and
Δaverage sentence length.
The evaluation results are reported separately
for the three domains. In addition, the results are
calculated when the classes of the signs are de-
5Defined as 1 minus the ratio of Levenshtein distance be-
tween the two sentences to the length in characters of the
longest of the two sentences being compared.
rived from the manually annotated data (Oracle)
and from use of the automatic classifier (Classi-
fier).
Table 3a presents the accuracy of the rules im-
plemented to convert compound sentences into a
more accessible form. The row #Compound sen-
tences displays the number of sentences in the test
data that contain signs of conjoint clauses (signs
of class CEV). The results obtained are not unex-
pected. In all cases the accuracy of the simplifi-
cation rules is higher when the labels of signs are
assigned by the oracle. With the exception of the
health domain, the same pattern is observed when
ΔFlesch is considered. The highest accuracy is
obtained on the news texts, then the health do-
main, and finally the literature domain. However,
despite significantly lower accuracy on the litera-
ture domain, the readability of the sentences from
the literature domain benefits most from the auto-
matic simplification. This can be noticed both in
the improved Flesch scores and reduced sentence
length.
Table 3b presents the accuracy of the rules
which simplify complex sentences. In this table,
#Complex sentences denotes the number of sen-
tences in the test data that contain relative clauses.
The rest of the measures are calculated in the same
way as in Table 3a. Inspection of the table shows
that, for the news and health domains, the accu-
racy of these simplification rules is significantly
lower than the simplification rules used for com-
pound sentences. Surprisingly, the rules work bet-
ter for the literature domain than for the others.
The improvement in the readability of texts from
the health domain is negligible, which can be ex-
plained by the poor performance of the simplifica-
tion rules on this domain.
</bodyText>
<page confidence="0.993829">
137
</page>
<subsectionHeader confidence="0.997907">
4.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.999980321428572">
In order to have a better understanding of the per-
formance of the system, the performance of the
individual rules was also recorded. Tables 4 and 5
contain the most error prone trigger patterns for
conjoined and subordinate clauses respectively.
The statistics were derived from rules applied to
texts of all three categories of texts and the signs
of syntactic complexity were classified using an
oracle, in order to isolate the influence of the rules
in the system output. In this context, the accu-
racy with which the syntactic processor converts
sentences containing conjoint clauses into a more
accessible form is 0.577. The accuracy of this task
with regard to subordinate clauses is 0.411.
The most error-prone trigger patterns for con-
joined clauses are listed in Table 4, together with
information on the conjoin that they are intended
to detect (left or right), their error rate, and the
number of number of errors made. The same in-
formation is presented for the rules converting sen-
tences containing subordinate clauses in Table 5,
but in this case the patterns capture the subordina-
tion relations. In the patterns, words with partic-
ular parts of speech are denoted by the symbol w
with the relevant Penn Treebank tag appended as a
subscript. Verbs with clause complements are de-
noted vcc. Signs of syntactic complexity are de-
noted by the symbol s with the abbreviation of the
functional class appended as a subscript. Specific
words are printed in italics. In the patterns, the
clause coordinator is denoted ‘ ’ and upper case
letters are used to denote stretches of contiguous
text.
Rules CEV-25a and SSEV-78a are applied when
the input sentence triggers none of the other imple-
mented patterns. Errors of this type quantify the
number of sentences containing conjoint or subor-
dinate clauses that cannot be converted into a more
accessible form by rules included in the structural
complexity processor. Both rules have quite high
error rates, but these errors can only be addressed
via the addition of new rules or the adjustment of
already implemented rules.
SSEV-36a is a pattern used to prevent process-
ing of sentences that contain verbs with clause
complements. This pattern was introduced be-
cause using the sentence rewriting algorithm pro-
posed here to process sentences containing these
subordinate clauses would generate ungrammati-
cal output.
Table 5 contains only 4 items because for the
rest of the patterns the number of errors was less
than 3. A large number of these rules had an error
rate of 1 which motivated their deactivation. Un-
fortunately this did not lead to improved accuracy
of the overall conversion process.
</bodyText>
<sectionHeader confidence="0.99418" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999984321428571">
Error analysis revealed that fully automatic con-
version compound and complex sentences into a
more accessible form is quite unreliable, partic-
ularly for texts of the literature category. It was
noted that conversion of complex sentences into a
more accessible form is more difficult than con-
version of compound sentences. However, sub-
ordinate clauses are significantly more prevalent
than conjoint clauses in the training and testing
data collected so far.
The evaluation of the rule sets used in the con-
version of compound and complex sentences into
a more accessible form motivates further specific
development of the rule sets. This process in-
cludes deletion of rules that do not meet particu-
lar thresholds for accuracy and the development of
new rules to address cases where input sentences
fail to trigger any conversion rules (signalled by
activation of redundant rules CEV-25a and SSEV-
78a).
The results are disappointing given that the
syntactic simplification module presented in this
paper is expected to be integrated in a system
that makes texts more accessible for people with
autism. However, this simplification module will
be included in a post-editing environment for peo-
ple with ASD. In this setting, it may still prove
useful, despite its low accuracy.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999857">
The research described in this paper was par-
tially funded by the European Commission un-
der the Seventh (FP7-2007-2013) Framework Pro-
gramme for Research and Technological Devel-
opment (FP7- ICT-2011.5.5 FIRST 287607). We
gratefully acknowledge the contributions of all the
members of the FIRST consortium for their feed-
back and comments during the development of the
methods, and to Laura Hasler for her help with the
evaluation.
</bodyText>
<page confidence="0.994622">
138
</page>
<table confidence="0.998002142857143">
ID Conjoin Trigger pattern Error rate #Errors
CEV-24b B A B 0.131 59
CEV-24a A A B 0.119 54
CEV-12b A that C A that B C 0.595 25
CEV-25a NA NA 0.956 22
CEV-26a A vCCV B : “C” A vCC B : “C D” 0.213 16
CEV-26b A vCCV B : “D” A vCC B : “C D” 0.203 14
</table>
<tableCaption confidence="0.978874">
Table 4: Error rates for rules converting sentences with conjoint clauses
</tableCaption>
<table confidence="0.999743">
ID Matrix clause / subordinate clause Trigger pattern Error rate #Errors
SSEV-78a NA NA 0.517 45
SSEV-72a A , C w{verb} D A s B C w{verb} D 0.333 4
SSEV-36a NA A told w{noun|PRP|DT|IN} * B 0.117 4
SSEV-13b wVBN wIN (w{DT|PRP$|noun|CD} A wVBN wIN {w{DT|PRP$|noun|CD} 1 3
|-|,)* w{noun} B |-|,}* w{noun} B
</table>
<tableCaption confidence="0.998828">
Table 5: Error rates for rules converting sentences with subordinate clauses
</tableCaption>
<sectionHeader confidence="0.99554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998846216216216">
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th annual meeting for Computa-
tional Linguistics, pages 15–21, Newark, Delaware.
Association for Computational Linguistics.
D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson.
2011. The effects of syntactic and lexical com-
plexity on the comprehension of elementary science
texts. International Electronic Journal of Elemen-
tary Education, 4 (1):107–125.
Y. Canning. 2002. Syntactic Simplification of Text.
Ph.d. thesis, University of Sunderland.
R Chandrasekar and B Srinivas. 1997. Automatic in-
duction of rules for text simplification. Knowledge-
Based Systems, 10:183–190.
T. Cohn and M. Lapata. 2009. Sentence Compression
as Tree Transduction. Journal of Artificial Intelli-
gence Research, 20(34):637–74.
W. Coster and D. Kauchak. 2011. Simple english
wikipedia: A new text simplification task. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2011),
pages 665–669, Portland, Oregon, June. Association
of Computational Linguistics.
Iustin Dornescu, Richard Evans, and Constantin
Or˘asan. 2013. A Tagging Approach to Identify
Complex Constituents for Text Simplification. In
Proceedings of Recent Advances in Natural Lan-
guage Processing, pages 221 – 229, Hissar, Bul-
garia.
Richard Evans and Constantin Orasan. 2013. Annotat-
ing signs of syntactic complexity to support sentence
simplification. In I. Habernal and V. Matousek, edi-
tors, Text, Speech and Dialogue. Proceedings of the
16th International Conference TSD 2013, pages 92–
104. Springer, Plzen, Czech Republic.
R. Evans. 2011. Comparing methods for the syn-
tactic simplification of sentences in information ex-
traction. Literary and Linguistic Computing, 26
(4):371–388.
R. Flesch. 1949. The art of readable writing. Harper,
New York.
Laurie Gerber and Eduard H. Hovy. 1998. Improving
translation quality by manipulating sentence length.
In David Farwell, Laurie Gerber, and Eduard H.
Hovy, editors, AMTA, volume 1529 of Lecture Notes
in Computer Science, pages 448–460. Springer.
M. A. Just, P. A. Carpenter, and K. R. Thulborn. 1996.
Brain activation modulated by sentence comprehen-
sion. Science, 274:114–116.
S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J.
Hagerman, and L. Abbeduto. 2012. Syntactic com-
prehension in boys with autism spectrum disorders:
Evidence from specific constructions. In Proceed-
ings of the 2012 International Meeting for Autism
Research, Athens, Greece. International Society for
Autism Research.
J. Levy, E. Hoover, G. Waters, S. Kiran, D. Caplan,
A. Berardino, and C. Sandberg. 2012. Effects of
syntactic complexity, semantic reversibility, and ex-
plicitness on discourse comprehension in persons
with aphasia and in healthy controls. American
Journal of Speech–Language Pathology, 21(2):154
– 165.
Wolfgang Maier, Sandra K¨ubler, Erhard Hinrichs, and
Julia Kriwanek. 2012. Annotating coordination in
the penn treebank. In Proceedings of the Sixth Lin-
guistic Annotation Workshop, pages 166–174, Jeju,
Republic of Korea, July. Association for Computa-
tional Linguistics.
Juan Martos, Sandra Freire, Ana Gonzlez, David Gil,
Richard Evans, Vesna Jordanova, Arlinda Cerga,
Antoneta Shishkova, and Constantin Orasan. 2013.
User preferences: Updated report. Technical report,
</reference>
<page confidence="0.987063">
139
</page>
<reference confidence="0.999467907407408">
The FIRST Consortium, Available at http://first-
asd.eu/D2.2.
A. Max. 2000. Syntactic simplification - an applica-
tion to text for aphasic readers. Mphil in computer
speech and language processing, University of Cam-
bridge, Wolfson College.
Ryan T. McDonald and Joakim Nivre. 2011. Analyz-
ing and integrating dependency parsers. Computa-
tional Linguistics, 37(1):197–230.
Geoffrey Nunberg, Ted Briscoe, and Rodney Huddle-
ston. 2002. Punctuation. chapter 20 In Huddleston,
Rodney and Geoffrey K. Pullum (eds) The Cam-
bridge Grammar of the English Language, pages
1724–1764. Cambridge University Press.
I. M. O’Connor and P. D. Klein. 2004. Exploration
of strategies for facilitating the reading comprehen-
sion of high-functioning students with autism spec-
trum disorders. Journal of Autism and Developmen-
tal Disorders, 34:2:115–127.
C. K. Ogden. 1932. Basic English: a general intro-
duction with rules and grammar. K. Paul, Trench,
Trubner &amp; Co., Ltd., London.
Thomas C. Rindflesch, Jayant V. Rajan, and Lawrence
Hunter. 2000. Extracting molecular binding rela-
tionships from biomedical text. In Proceedings of
the sixth conference on Applied natural language
processing, pages 188–195, Seattle, Washington.
Association of Computational Linguistics.
A. Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language and Compu-
tation, 4:1:77–109.
Helen Tager-Flusberg. 1981. Sentence comprehen-
sion in autistic children. Applied Psycholinguistics,
2:1:5–24.
Masaru Tomita. 1985. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Kluwer Academic Publishers, Norwell, MA, USA.
Sebastian van Delden and Fernando Gomez. 2002.
Combining finite state automata and a greedy learn-
ing algorithm to determine the syntactic roles of
commas. In Proceedings of the 14th IEEE Inter-
national Conference on Tools with Artificial Intel-
ligence, ICTAI ’02, pages 293–, Washington, DC,
USA. IEEE Computer Society.
F.R. Volkmar and L. Wiesner. 2009. A Practical Guide
to Autism. Wiley, Hoboken, NJ.
M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and
L. Lee. 2010. For the sake of simplicity: Unsu-
pervised extraction of lexical simplifications from
wikipedia. In Proceedings of Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 365–
368, Los Angeles, California, June. Association of
Computational Linguistics.
</reference>
<page confidence="0.997537">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.309799">
<title confidence="0.998705">An evaluation of syntactic simplification rules for people with autism</title>
<author confidence="0.99742">Richard Evans</author>
<author confidence="0.99742">Constantin Or˘asan</author>
<author confidence="0.99742">Iustin</author>
<affiliation confidence="0.838064">Research Institute in Information and Language University of United</affiliation>
<email confidence="0.498518">C.Orasan,</email>
<abstract confidence="0.990397933333333">Syntactically complex sentences constitute an obstacle for some people with Autistic Spectrum Disorders. This paper evaluates a set of simplification rules specifically designed for tackling complex and compound sentences. In total, 127 different rules were developed for the rewriting of complex sentences and 56 for the rewriting of compound sentences. The evaluation assessed the accuracy of these rules individually and revealed that fully automatic conversion of these sentences into a more accessible form is not very reliable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rajeev Agarwal</author>
<author>Lois Boggess</author>
</authors>
<title>A simple but useful approach to conjunct identification.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th annual meeting for Computational Linguistics,</booktitle>
<pages>15--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Newark, Delaware.</location>
<contexts>
<context position="4469" citStr="Agarwal and Boggess, 1992" startWordPosition="676" endWordPosition="679">orkshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysi</context>
</contexts>
<marker>Agarwal, Boggess, 1992</marker>
<rawString>Rajeev Agarwal and Lois Boggess. 1992. A simple but useful approach to conjunct identification. In Proceedings of the 30th annual meeting for Computational Linguistics, pages 15–21, Newark, Delaware. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Arya</author>
<author>Elfrieda H Hiebert</author>
<author>P D Pearson</author>
</authors>
<title>The effects of syntactic and lexical complexity on the comprehension of elementary science texts.</title>
<date>2011</date>
<journal>International Electronic Journal of Elementary Education,</journal>
<volume>4</volume>
<pages>1--107</pages>
<contexts>
<context position="3628" citStr="Arya et al., 2011" startWordPosition="560" endWordPosition="563">y are automatically classified and in the second, manually crafted rules are applied to simplify the relevant sentences. Section 3 presents more details about the method. Evaluation of automatic simplification is a difficult issue. Given that the purpose of this paper is to gain a better understanding of the performance of the rules used for simplifying compound sentences and complex sentences, Section 4 presents the methodology developed for this evaluation and discusses the results obtained. The paper finishes with conclusions. 2 Background information Despite some findings to the contrary (Arya et al., 2011), automatic syntactic simplification has been motivated by numerous neurolinguistic and psycholinguistic studies. Brain imaging studies indicate that processing syntactically complex struc131 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are bette</context>
</contexts>
<marker>Arya, Hiebert, Pearson, 2011</marker>
<rawString>D. J. Arya, Elfrieda H. Hiebert, and P. D. Pearson. 2011. The effects of syntactic and lexical complexity on the comprehension of elementary science texts. International Electronic Journal of Elementary Education, 4 (1):107–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Canning</author>
</authors>
<title>Syntactic Simplification of Text.</title>
<date>2002</date>
<tech>Ph.d. thesis,</tech>
<institution>University of Sunderland.</institution>
<contexts>
<context position="5581" citStr="Canning (2002)" startWordPosition="845" endWordPosition="846">1). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS taggers. Chandrasekar and Srinivas (1997) presented an automatic method to learn syntactic simplification rules for use in such systems. Unfortunately, that approach is only capable of learning a restricted range of rules and requires access to expensive annotated resources. With regard to applications improving text accessibility for human readers, Max (2000) described the use of syntactic simplification for aphasic readers. In work on the PSET project, Canning (2002) implemented a system which exploits a syntactic parser in order to rewrite compound sentences as sequences of simple sentences and to convert passive sentences into active ones for readers with aphasia. The success of these systems is tied to the performance levels of the syntactic parsers that they employ. More recently, the availability of resources such as Simple Wikipedia has enabled text simplification to be included in the paradigm of statistical machine translation (Yatskar et al., 2010; Coster and Kauchak, 2011). In this context, translation models are learned by aligning sentences in</context>
</contexts>
<marker>Canning, 2002</marker>
<rawString>Y. Canning. 2002. Syntactic Simplification of Text. Ph.d. thesis, University of Sunderland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Chandrasekar</author>
<author>B Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification. KnowledgeBased Systems,</title>
<date>1997</date>
<pages>10--183</pages>
<contexts>
<context position="5149" citStr="Chandrasekar and Srinivas (1997)" startWordPosition="776" endWordPosition="779">ic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS taggers. Chandrasekar and Srinivas (1997) presented an automatic method to learn syntactic simplification rules for use in such systems. Unfortunately, that approach is only capable of learning a restricted range of rules and requires access to expensive annotated resources. With regard to applications improving text accessibility for human readers, Max (2000) described the use of syntactic simplification for aphasic readers. In work on the PSET project, Canning (2002) implemented a system which exploits a syntactic parser in order to rewrite compound sentences as sequences of simple sentences and to convert passive sentences into ac</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>R Chandrasekar and B Srinivas. 1997. Automatic induction of rules for text simplification. KnowledgeBased Systems, 10:183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence Compression as Tree Transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>20</volume>
<issue>34</issue>
<contexts>
<context position="6552" citStr="Cohn and Lapata (2009)" startWordPosition="999" endWordPosition="1002">urces such as Simple Wikipedia has enabled text simplification to be included in the paradigm of statistical machine translation (Yatskar et al., 2010; Coster and Kauchak, 2011). In this context, translation models are learned by aligning sentences in Wikipedia with their corresponding versions in Simple Wikipedia. Manifesting Basic English (Ogden, 1932), the extent to which Simple Wikipedia is accessible to people with autism has not yet been fully assessed. The field of text summarisation includes numerous approaches that can be regarded as examples of syntactic simplification. For example, Cohn and Lapata (2009) present a tree-to-tree transduction method that is used to filter non-essential information from syntactically parsed sentences. This compression process often reduces the syntactic complexity of those sentences. An advantage of this approach is that it can identify elements for deletion even when such elements are not indicated by explicit signs of syntactic complexity. The difficulty is that they rely on high levels of accuracy and granularity of automatic syntactic analysis. As noted earlier, it has been observed that the accuracy of parsers is inversely proportional to the length and comp</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>T. Cohn and M. Lapata. 2009. Sentence Compression as Tree Transduction. Journal of Artificial Intelligence Research, 20(34):637–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Coster</author>
<author>D Kauchak</author>
</authors>
<title>Simple english wikipedia: A new text simplification task.</title>
<date>2011</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011),</booktitle>
<pages>665--669</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="6107" citStr="Coster and Kauchak, 2011" startWordPosition="930" endWordPosition="933">he use of syntactic simplification for aphasic readers. In work on the PSET project, Canning (2002) implemented a system which exploits a syntactic parser in order to rewrite compound sentences as sequences of simple sentences and to convert passive sentences into active ones for readers with aphasia. The success of these systems is tied to the performance levels of the syntactic parsers that they employ. More recently, the availability of resources such as Simple Wikipedia has enabled text simplification to be included in the paradigm of statistical machine translation (Yatskar et al., 2010; Coster and Kauchak, 2011). In this context, translation models are learned by aligning sentences in Wikipedia with their corresponding versions in Simple Wikipedia. Manifesting Basic English (Ogden, 1932), the extent to which Simple Wikipedia is accessible to people with autism has not yet been fully assessed. The field of text summarisation includes numerous approaches that can be regarded as examples of syntactic simplification. For example, Cohn and Lapata (2009) present a tree-to-tree transduction method that is used to filter non-essential information from syntactically parsed sentences. This compression process </context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>W. Coster and D. Kauchak. 2011. Simple english wikipedia: A new text simplification task. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-2011), pages 665–669, Portland, Oregon, June. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iustin Dornescu</author>
<author>Richard Evans</author>
<author>Constantin Or˘asan</author>
</authors>
<title>A Tagging Approach to Identify Complex Constituents for Text Simplification.</title>
<date>2013</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing,</booktitle>
<pages>221--229</pages>
<location>Hissar, Bulgaria.</location>
<marker>Dornescu, Evans, Or˘asan, 2013</marker>
<rawString>Iustin Dornescu, Richard Evans, and Constantin Or˘asan. 2013. A Tagging Approach to Identify Complex Constituents for Text Simplification. In Proceedings of Recent Advances in Natural Language Processing, pages 221 – 229, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Evans</author>
<author>Constantin Orasan</author>
</authors>
<title>Annotating signs of syntactic complexity to support sentence simplification.</title>
<date>2013</date>
<booktitle>Speech and Dialogue. Proceedings of the 16th International Conference TSD 2013,</booktitle>
<pages>92--104</pages>
<editor>In I. Habernal and V. Matousek, editors, Text,</editor>
<publisher>Springer,</publisher>
<location>Plzen, Czech Republic.</location>
<contexts>
<context position="2906" citStr="Evans and Orasan, 2013" startWordPosition="450" endWordPosition="453"> ASD, but which can also be used for other tasks that require syntactic simplification of sentences. In our research, we consider that syntactic complexity is usually indicated by the occurrence of certain markers or signs of syntactic complexity, referred to hereafter as signs, such as punctuation ([,] and [;]), conjunctions ([and], [but], and [or]), complementisers ([that]) or wh-words ([what], [when], [where], [which], [while], [who]). These signs may have a range of syntactic linking and bounding functions which need to be automatically identified, and which we analysed in more detail in (Evans and Orasan, 2013). Our syntactic simplification process operates in two steps. In the first, signs of syntactic complexity are automatically classified and in the second, manually crafted rules are applied to simplify the relevant sentences. Section 3 presents more details about the method. Evaluation of automatic simplification is a difficult issue. Given that the purpose of this paper is to gain a better understanding of the performance of the rules used for simplifying compound sentences and complex sentences, Section 4 presents the methodology developed for this evaluation and discusses the results obtaine</context>
<context position="7904" citStr="Evans and Orasan, 2013" startWordPosition="1216" endWordPosition="1219">ed in the current paper is a two step process involving detection and tagging of the bounding and linking functions of various signs of syntactic complexity followed by a rule-based sentence rewriting step. Relevant to the first step, Van Delden and Gomez (2002) developed a machine learning method to determine the syntactic roles of commas. Meier et al. (2012) describe German language resources in which the linking functions of commas and semicolons are annotated. The annotated resources exploited by the machine learning method presented in Section 3.2.1 of the current paper are presented in (Evans and Orasan, 2013). From a linguistic perspective, Nunberg et al. (2002) provide a grammatical analysis of punctuation in English. The work described in this paper was undertaken in a project aiming to improve the accessibility of text for people with autism. It was motivated at least in part by the work of O’Connor and Klein (2004), which describes strategies to facilitate the reading comprehension of people with ASD. The proposed method is intended to reduce complexity caused by both complex and compound sentences and differs from those described earlier in this section. Sentence compression methods are not s</context>
<context position="14177" citStr="Evans and Orasan, 2013" startWordPosition="2182" endWordPosition="2185">complexity within it. This paper reports on steps 1 and 2. The personalisation step, which takes into consideration the needs of individual users, is not discussed. 3.2.1 Identification of signs of complexity Signs of syntactic complexity typically indicate constituent boundaries, e.g. punctuation marks, conjunctions, and complementisers. To facilitate information extraction, a rule-based approach to simplify coordinated conjoins was proposed by Evans (2011), which relies on classifying signs based on their linking functions. In more recent work, an extended annotation scheme was proposed in (Evans and Orasan, 2013) which enables the encoding of links and boundaries between a wider range of syntactic constituents and covers more syntactic phenomena. A corpus covering three text categories (news articles, literature, and patient healthcare information leaflets), was annotated using this extended scheme.2 Most sign labels contain three types of information: boundary type, syntactic projection level, and grammatical category of the constituent(s). Some labels cover signs which bound interjections, tag questions, and reported speech and a class denoting false signs of syntactic complexity, such as use of the</context>
</contexts>
<marker>Evans, Orasan, 2013</marker>
<rawString>Richard Evans and Constantin Orasan. 2013. Annotating signs of syntactic complexity to support sentence simplification. In I. Habernal and V. Matousek, editors, Text, Speech and Dialogue. Proceedings of the 16th International Conference TSD 2013, pages 92– 104. Springer, Plzen, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Evans</author>
</authors>
<title>Comparing methods for the syntactic simplification of sentences in information extraction.</title>
<date>2011</date>
<journal>Literary and Linguistic Computing,</journal>
<volume>26</volume>
<pages>4--371</pages>
<contexts>
<context position="4508" citStr="Evans, 2011" startWordPosition="684" endWordPosition="685"> for Target Reader Populations (PITR) @ EACL 2014, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS </context>
<context position="14016" citStr="Evans (2011)" startWordPosition="2159" endWordPosition="2160">irs of progressively simpler sentences. No further simplification is applied to a sentence when the system is unable to detect any signs of syntactic complexity within it. This paper reports on steps 1 and 2. The personalisation step, which takes into consideration the needs of individual users, is not discussed. 3.2.1 Identification of signs of complexity Signs of syntactic complexity typically indicate constituent boundaries, e.g. punctuation marks, conjunctions, and complementisers. To facilitate information extraction, a rule-based approach to simplify coordinated conjoins was proposed by Evans (2011), which relies on classifying signs based on their linking functions. In more recent work, an extended annotation scheme was proposed in (Evans and Orasan, 2013) which enables the encoding of links and boundaries between a wider range of syntactic constituents and covers more syntactic phenomena. A corpus covering three text categories (news articles, literature, and patient healthcare information leaflets), was annotated using this extended scheme.2 Most sign labels contain three types of information: boundary type, syntactic projection level, and grammatical category of the constituent(s). S</context>
</contexts>
<marker>Evans, 2011</marker>
<rawString>R. Evans. 2011. Comparing methods for the syntactic simplification of sentences in information extraction. Literary and Linguistic Computing, 26 (4):371–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Flesch</author>
</authors>
<title>The art of readable writing.</title>
<date>1949</date>
<publisher>Harper,</publisher>
<location>New York.</location>
<contexts>
<context position="27530" citStr="Flesch, 1949" startWordPosition="4328" endWordPosition="4329">. A sentence is considered successfully simplified, and implicitly all the rules used in the process are considered correctly applied, when all the sentences produced by the system are converted correctly according to the gold standard. This evaluation approach may be considered too inflexible as it does not take into consideration the fact that a sentence can be simplified in several ways. However, the purpose here is to evaluate the way in which sentences are simplified using specific rules. In order to calculate the readability of the generated sentences we initially used the Flesch score (Flesch, 1949). However, our system changes the text only by rewriting sentences into sequences of simpler sentences and does not make any changes at the lexical level. For this reason, any changes observed in the Flesch score are due to changes in the average sentence length. Therefore, for our experiments we report both ΔFlesch score and Δaverage sentence length. The evaluation results are reported separately for the three domains. In addition, the results are calculated when the classes of the signs are de5Defined as 1 minus the ratio of Levenshtein distance between the two sentences to the length in cha</context>
</contexts>
<marker>Flesch, 1949</marker>
<rawString>R. Flesch. 1949. The art of readable writing. Harper, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurie Gerber</author>
<author>Eduard H Hovy</author>
</authors>
<title>Improving translation quality by manipulating sentence length. In</title>
<date>1998</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>1529</volume>
<pages>448--460</pages>
<editor>David Farwell, Laurie Gerber, and Eduard H. Hovy, editors, AMTA,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="4634" citStr="Gerber and Hovy, 1998" startWordPosition="701" endWordPosition="704">iation for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS taggers. Chandrasekar and Srinivas (1997) presented an automatic method to learn syntactic simplification rules for use in suc</context>
</contexts>
<marker>Gerber, Hovy, 1998</marker>
<rawString>Laurie Gerber and Eduard H. Hovy. 1998. Improving translation quality by manipulating sentence length. In David Farwell, Laurie Gerber, and Eduard H. Hovy, editors, AMTA, volume 1529 of Lecture Notes in Computer Science, pages 448–460. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Just</author>
<author>P A Carpenter</author>
<author>K R Thulborn</author>
</authors>
<title>Brain activation modulated by sentence comprehension.</title>
<date>1996</date>
<journal>Science,</journal>
<pages>274--114</pages>
<contexts>
<context position="4144" citStr="Just et al., 1996" startWordPosition="629" endWordPosition="632">s with conclusions. 2 Background information Despite some findings to the contrary (Arya et al., 2011), automatic syntactic simplification has been motivated by numerous neurolinguistic and psycholinguistic studies. Brain imaging studies indicate that processing syntactically complex struc131 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rul</context>
</contexts>
<marker>Just, Carpenter, Thulborn, 1996</marker>
<rawString>M. A. Just, P. A. Carpenter, and K. R. Thulborn. 1996. Brain activation modulated by sentence comprehension. Science, 274:114–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Kover</author>
<author>E Haebig</author>
<author>A Oakes</author>
<author>A McDuffie</author>
<author>R J Hagerman</author>
<author>L Abbeduto</author>
</authors>
<title>Syntactic comprehension in boys with autism spectrum disorders: Evidence from specific constructions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Meeting for Autism Research,</booktitle>
<institution>Greece. International Society for Autism Research.</institution>
<location>Athens,</location>
<contexts>
<context position="1392" citStr="Kover et al., 2012" startWordPosition="204" endWordPosition="207">re accessible form is not very reliable. 1 Introduction People with Autistic Spectrum Disorders (ASD) show a diverse range of reading abilities: on the one hand, 5%-10% of users have the capacity to read words from an early age without the need for formal learning (hyperlexia), on the other hand many users demonstrate weak comprehension of what has been read (Volkmar and Wiesner, 2009). They may have difficulty inferring contextual information or may have trouble understanding mental verbs or emotional language, as well as long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, the FIRST project1 is developing a tool which makes texts more accessible for people with ASD. In order to get a better understanding of the needs of these readers, a thorough analysis was carried out to derive a list of high priority obstacles to reading comprehension. Some of these obstacles are related to syntactic complexity and constitute the focus of this paper. Even though the research in the FIRST project focuses on people with ASD, many of the obstacles 1http://first-asd.eu identified in the project can pose difficulties for a wide range of readers suc</context>
</contexts>
<marker>Kover, Haebig, Oakes, McDuffie, Hagerman, Abbeduto, 2012</marker>
<rawString>S. T. Kover, E. Haebig, A. Oakes, A. McDuffie, R. J. Hagerman, and L. Abbeduto. 2012. Syntactic comprehension in boys with autism spectrum disorders: Evidence from specific constructions. In Proceedings of the 2012 International Meeting for Autism Research, Athens, Greece. International Society for Autism Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Levy</author>
<author>E Hoover</author>
<author>G Waters</author>
<author>S Kiran</author>
<author>D Caplan</author>
<author>A Berardino</author>
<author>C Sandberg</author>
</authors>
<title>Effects of syntactic complexity, semantic reversibility, and explicitness on discourse comprehension in persons with aphasia and in healthy controls.</title>
<date>2012</date>
<journal>American Journal of Speech–Language Pathology,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>165</pages>
<contexts>
<context position="4186" citStr="Levy et al. (2012)" startWordPosition="637" endWordPosition="640">on Despite some findings to the contrary (Arya et al., 2011), automatic syntactic simplification has been motivated by numerous neurolinguistic and psycholinguistic studies. Brain imaging studies indicate that processing syntactically complex struc131 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplificati</context>
</contexts>
<marker>Levy, Hoover, Waters, Kiran, Caplan, Berardino, Sandberg, 2012</marker>
<rawString>J. Levy, E. Hoover, G. Waters, S. Kiran, D. Caplan, A. Berardino, and C. Sandberg. 2012. Effects of syntactic complexity, semantic reversibility, and explicitness on discourse comprehension in persons with aphasia and in healthy controls. American Journal of Speech–Language Pathology, 21(2):154 – 165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Sandra K¨ubler</author>
<author>Erhard Hinrichs</author>
<author>Julia Kriwanek</author>
</authors>
<title>Annotating coordination in the penn treebank.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth Linguistic Annotation Workshop,</booktitle>
<pages>166--174</pages>
<institution>Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic of</location>
<marker>Maier, K¨ubler, Hinrichs, Kriwanek, 2012</marker>
<rawString>Wolfgang Maier, Sandra K¨ubler, Erhard Hinrichs, and Julia Kriwanek. 2012. Annotating coordination in the penn treebank. In Proceedings of the Sixth Linguistic Annotation Workshop, pages 166–174, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Martos</author>
<author>Sandra Freire</author>
<author>Ana Gonzlez</author>
<author>David Gil</author>
<author>Richard Evans</author>
<author>Vesna Jordanova</author>
<author>Arlinda Cerga</author>
<author>Antoneta Shishkova</author>
<author>Constantin Orasan</author>
</authors>
<title>User preferences: Updated report.</title>
<date>2013</date>
<tech>Technical report,</tech>
<contexts>
<context position="9857" citStr="Martos et al., 2013" startWordPosition="1525" endWordPosition="1528">lexity identified by the study. It should be mentioned that most of the obstacles are problematic not only for autistic people and other types of reader can also benefit from their removal. The obstacles identified constituted the basis for developing the simplification approach briefly described in Section 3.2. 3.1 User requirements Consultations with 94 subjects meeting the strict DSM-IV criteria for ASD and with IQ &gt; 70 led to the derivation of user preferences and high priority user requirements related to structural processing. A comprehensive explanation of the findings can be found in (Martos et al., 2013). This section discusses briefly the two types of information of relevance to the processing of sentence complexity obtained in our study. First, in terms of the demand for access to texts of particular genres/domains, it was found that young people (aged 12-16) seek access to documents in informative (arts/leisure) domains and they have less interest in periodicals and newspapers or imaginative texts. Adults (aged 16+) seek access to informative and scientific texts (including newspapers), imaginative text, and the language of social networking and communication. In an attempt to accommodate </context>
</contexts>
<marker>Martos, Freire, Gonzlez, Gil, Evans, Jordanova, Cerga, Shishkova, Orasan, 2013</marker>
<rawString>Juan Martos, Sandra Freire, Ana Gonzlez, David Gil, Richard Evans, Vesna Jordanova, Arlinda Cerga, Antoneta Shishkova, and Constantin Orasan. 2013. User preferences: Updated report. Technical report,</rawString>
</citation>
<citation valid="false">
<authors>
<author>The FIRST Consortium</author>
</authors>
<note>Available at http://firstasd.eu/D2.2.</note>
<marker>Consortium, </marker>
<rawString>The FIRST Consortium, Available at http://firstasd.eu/D2.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Max</author>
</authors>
<title>Syntactic simplification - an application to text for aphasic readers. Mphil in computer speech and language processing,</title>
<date>2000</date>
<institution>University of Cambridge, Wolfson College.</institution>
<contexts>
<context position="5470" citStr="Max (2000)" startWordPosition="827" endWordPosition="828">s such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS taggers. Chandrasekar and Srinivas (1997) presented an automatic method to learn syntactic simplification rules for use in such systems. Unfortunately, that approach is only capable of learning a restricted range of rules and requires access to expensive annotated resources. With regard to applications improving text accessibility for human readers, Max (2000) described the use of syntactic simplification for aphasic readers. In work on the PSET project, Canning (2002) implemented a system which exploits a syntactic parser in order to rewrite compound sentences as sequences of simple sentences and to convert passive sentences into active ones for readers with aphasia. The success of these systems is tied to the performance levels of the syntactic parsers that they employ. More recently, the availability of resources such as Simple Wikipedia has enabled text simplification to be included in the paradigm of statistical machine translation (Yatskar et</context>
</contexts>
<marker>Max, 2000</marker>
<rawString>A. Max. 2000. Syntactic simplification - an application to text for aphasic readers. Mphil in computer speech and language processing, University of Cambridge, Wolfson College.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Analyzing and integrating dependency parsers.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="4568" citStr="McDonald and Nivre, 2011" startWordPosition="690" endWordPosition="693">14, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS taggers. Chandrasekar and Srinivas (1997) presented an autom</context>
<context position="7231" citStr="McDonald and Nivre, 2011" startWordPosition="1105" endWordPosition="1108">ed to filter non-essential information from syntactically parsed sentences. This compression process often reduces the syntactic complexity of those sentences. An advantage of this approach is that it can identify elements for deletion even when such elements are not indicated by explicit signs of syntactic complexity. The difficulty is that they rely on high levels of accuracy and granularity of automatic syntactic analysis. As noted earlier, it has been observed that the accuracy of parsers is inversely proportional to the length and complexity of the sentences being analysed (Tomita, 1985; McDonald and Nivre, 2011). The approach to syntactic simplification described in the current paper is a two step process involving detection and tagging of the bounding and linking functions of various signs of syntactic complexity followed by a rule-based sentence rewriting step. Relevant to the first step, Van Delden and Gomez (2002) developed a machine learning method to determine the syntactic roles of commas. Meier et al. (2012) describe German language resources in which the linking functions of commas and semicolons are annotated. The annotated resources exploited by the machine learning method presented in Sec</context>
</contexts>
<marker>McDonald, Nivre, 2011</marker>
<rawString>Ryan T. McDonald and Joakim Nivre. 2011. Analyzing and integrating dependency parsers. Computational Linguistics, 37(1):197–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
<author>Ted Briscoe</author>
<author>Rodney Huddleston</author>
</authors>
<title>Pullum (eds) The Cambridge Grammar of the English Language,</title>
<date>2002</date>
<pages>1724--1764</pages>
<publisher>Cambridge University Press.</publisher>
<note>Punctuation. chapter 20 In</note>
<contexts>
<context position="7958" citStr="Nunberg et al. (2002)" startWordPosition="1224" endWordPosition="1227">detection and tagging of the bounding and linking functions of various signs of syntactic complexity followed by a rule-based sentence rewriting step. Relevant to the first step, Van Delden and Gomez (2002) developed a machine learning method to determine the syntactic roles of commas. Meier et al. (2012) describe German language resources in which the linking functions of commas and semicolons are annotated. The annotated resources exploited by the machine learning method presented in Section 3.2.1 of the current paper are presented in (Evans and Orasan, 2013). From a linguistic perspective, Nunberg et al. (2002) provide a grammatical analysis of punctuation in English. The work described in this paper was undertaken in a project aiming to improve the accessibility of text for people with autism. It was motivated at least in part by the work of O’Connor and Klein (2004), which describes strategies to facilitate the reading comprehension of people with ASD. The proposed method is intended to reduce complexity caused by both complex and compound sentences and differs from those described earlier in this section. Sentence compression methods are not suitable for the types of rewriting required in simplif</context>
</contexts>
<marker>Nunberg, Briscoe, Huddleston, 2002</marker>
<rawString>Geoffrey Nunberg, Ted Briscoe, and Rodney Huddleston. 2002. Punctuation. chapter 20 In Huddleston, Rodney and Geoffrey K. Pullum (eds) The Cambridge Grammar of the English Language, pages 1724–1764. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I M O’Connor</author>
<author>P D Klein</author>
</authors>
<title>Exploration of strategies for facilitating the reading comprehension of high-functioning students with autism spectrum disorders.</title>
<date>2004</date>
<journal>Journal of Autism and Developmental Disorders,</journal>
<pages>34--2</pages>
<marker>O’Connor, Klein, 2004</marker>
<rawString>I. M. O’Connor and P. D. Klein. 2004. Exploration of strategies for facilitating the reading comprehension of high-functioning students with autism spectrum disorders. Journal of Autism and Developmental Disorders, 34:2:115–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Ogden</author>
</authors>
<title>Basic English: a general introduction with rules and</title>
<date>1932</date>
<publisher>Co., Ltd.,</publisher>
<location>Trench, Trubner</location>
<contexts>
<context position="6286" citStr="Ogden, 1932" startWordPosition="959" endWordPosition="960">s as sequences of simple sentences and to convert passive sentences into active ones for readers with aphasia. The success of these systems is tied to the performance levels of the syntactic parsers that they employ. More recently, the availability of resources such as Simple Wikipedia has enabled text simplification to be included in the paradigm of statistical machine translation (Yatskar et al., 2010; Coster and Kauchak, 2011). In this context, translation models are learned by aligning sentences in Wikipedia with their corresponding versions in Simple Wikipedia. Manifesting Basic English (Ogden, 1932), the extent to which Simple Wikipedia is accessible to people with autism has not yet been fully assessed. The field of text summarisation includes numerous approaches that can be regarded as examples of syntactic simplification. For example, Cohn and Lapata (2009) present a tree-to-tree transduction method that is used to filter non-essential information from syntactically parsed sentences. This compression process often reduces the syntactic complexity of those sentences. An advantage of this approach is that it can identify elements for deletion even when such elements are not indicated by</context>
</contexts>
<marker>Ogden, 1932</marker>
<rawString>C. K. Ogden. 1932. Basic English: a general introduction with rules and grammar. K. Paul, Trench, Trubner &amp; Co., Ltd., London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas C Rindflesch</author>
<author>Jayant V Rajan</author>
<author>Lawrence Hunter</author>
</authors>
<title>Extracting molecular binding relationships from biomedical text.</title>
<date>2000</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the sixth conference on Applied natural language processing,</booktitle>
<pages>188--195</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="4494" citStr="Rindflesch et al., 2000" startWordPosition="680" endWordPosition="683">mproving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial pa</context>
</contexts>
<marker>Rindflesch, Rajan, Hunter, 2000</marker>
<rawString>Thomas C. Rindflesch, Jayant V. Rajan, and Lawrence Hunter. 2000. Extracting molecular binding relationships from biomedical text. In Proceedings of the sixth conference on Applied natural language processing, pages 188–195, Seattle, Washington. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
</authors>
<title>Syntactic simplification and text cohesion.</title>
<date>2006</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>4--1</pages>
<contexts>
<context position="4828" citStr="Siddharthan, 2006" startWordPosition="729" endWordPosition="730"> aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS taggers. Chandrasekar and Srinivas (1997) presented an automatic method to learn syntactic simplification rules for use in such systems. Unfortunately, that approach is only capable of learning a restricted range of rules and requires access to expensive annotated resources. With regard to applications improving text a</context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>A. Siddharthan. 2006. Syntactic simplification and text cohesion. Research on Language and Computation, 4:1:77–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Tager-Flusberg</author>
</authors>
<title>Sentence comprehension in autistic children.</title>
<date>1981</date>
<journal>Applied Psycholinguistics,</journal>
<pages>2--1</pages>
<contexts>
<context position="1371" citStr="Tager-Flusberg, 1981" startWordPosition="202" endWordPosition="203">se sentences into a more accessible form is not very reliable. 1 Introduction People with Autistic Spectrum Disorders (ASD) show a diverse range of reading abilities: on the one hand, 5%-10% of users have the capacity to read words from an early age without the need for formal learning (hyperlexia), on the other hand many users demonstrate weak comprehension of what has been read (Volkmar and Wiesner, 2009). They may have difficulty inferring contextual information or may have trouble understanding mental verbs or emotional language, as well as long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, the FIRST project1 is developing a tool which makes texts more accessible for people with ASD. In order to get a better understanding of the needs of these readers, a thorough analysis was carried out to derive a list of high priority obstacles to reading comprehension. Some of these obstacles are related to syntactic complexity and constitute the focus of this paper. Even though the research in the FIRST project focuses on people with ASD, many of the obstacles 1http://first-asd.eu identified in the project can pose difficulties for a wide</context>
</contexts>
<marker>Tager-Flusberg, 1981</marker>
<rawString>Helen Tager-Flusberg. 1981. Sentence comprehension in autistic children. Applied Psycholinguistics, 2:1:5–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems.</title>
<date>1985</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="4541" citStr="Tomita, 1985" startWordPosition="688" endWordPosition="689">ITR) @ EACL 2014, pages 131–140, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics tures requires more neurological activity than processing simple structures (Just et al., 1996). A study undertaken by Levy et al. (2012) showed that people with aphasia are better able to understand syntactically simple reversible sentences than syntactically complex ones. Further motivation is brought by research in NLP, which demonstrates that performance levels in information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011), syntactic parsing (Tomita, 1985; McDonald and Nivre, 2011), and, to some extent, machine translation (Gerber and Hovy, 1998) are somewhat determined by the length and syntactic complexity of the sentences being processed. Numerous rule-based methods for syntactic simplification have been developed (Siddharthan, 2006) and used to facilitate NLP tasks such as biomedical information extraction (Agarwal and Boggess, 1992; Rindflesch et al., 2000; Evans, 2011). In these approaches, rules are triggered by pattern-matching applied to the output of text analysis tool such as partial parsers and POS taggers. Chandrasekar and Sriniva</context>
<context position="7204" citStr="Tomita, 1985" startWordPosition="1103" endWordPosition="1104">hod that is used to filter non-essential information from syntactically parsed sentences. This compression process often reduces the syntactic complexity of those sentences. An advantage of this approach is that it can identify elements for deletion even when such elements are not indicated by explicit signs of syntactic complexity. The difficulty is that they rely on high levels of accuracy and granularity of automatic syntactic analysis. As noted earlier, it has been observed that the accuracy of parsers is inversely proportional to the length and complexity of the sentences being analysed (Tomita, 1985; McDonald and Nivre, 2011). The approach to syntactic simplification described in the current paper is a two step process involving detection and tagging of the bounding and linking functions of various signs of syntactic complexity followed by a rule-based sentence rewriting step. Relevant to the first step, Van Delden and Gomez (2002) developed a machine learning method to determine the syntactic roles of commas. Meier et al. (2012) describe German language resources in which the linking functions of commas and semicolons are annotated. The annotated resources exploited by the machine learn</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Masaru Tomita. 1985. Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian van Delden</author>
<author>Fernando Gomez</author>
</authors>
<title>Combining finite state automata and a greedy learning algorithm to determine the syntactic roles of commas.</title>
<date>2002</date>
<booktitle>In Proceedings of the 14th IEEE International Conference on Tools with Artificial Intelligence, ICTAI ’02,</booktitle>
<pages>293</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<marker>van Delden, Gomez, 2002</marker>
<rawString>Sebastian van Delden and Fernando Gomez. 2002. Combining finite state automata and a greedy learning algorithm to determine the syntactic roles of commas. In Proceedings of the 14th IEEE International Conference on Tools with Artificial Intelligence, ICTAI ’02, pages 293–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Volkmar</author>
<author>L Wiesner</author>
</authors>
<title>A Practical Guide to Autism.</title>
<date>2009</date>
<publisher>Wiley,</publisher>
<location>Hoboken, NJ.</location>
<contexts>
<context position="1161" citStr="Volkmar and Wiesner, 2009" startWordPosition="170" endWordPosition="173">re developed for the rewriting of complex sentences and 56 for the rewriting of compound sentences. The evaluation assessed the accuracy of these rules individually and revealed that fully automatic conversion of these sentences into a more accessible form is not very reliable. 1 Introduction People with Autistic Spectrum Disorders (ASD) show a diverse range of reading abilities: on the one hand, 5%-10% of users have the capacity to read words from an early age without the need for formal learning (hyperlexia), on the other hand many users demonstrate weak comprehension of what has been read (Volkmar and Wiesner, 2009). They may have difficulty inferring contextual information or may have trouble understanding mental verbs or emotional language, as well as long sentences with complex syntactic structure (Tager-Flusberg, 1981; Kover et al., 2012). To address these difficulties, the FIRST project1 is developing a tool which makes texts more accessible for people with ASD. In order to get a better understanding of the needs of these readers, a thorough analysis was carried out to derive a list of high priority obstacles to reading comprehension. Some of these obstacles are related to syntactic complexity and c</context>
</contexts>
<marker>Volkmar, Wiesner, 2009</marker>
<rawString>F.R. Volkmar and L. Wiesner. 2009. A Practical Guide to Autism. Wiley, Hoboken, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yatskar</author>
<author>B Pang</author>
<author>C Danescu-Niculescu-Mizil</author>
<author>L Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia.</title>
<date>2010</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>365--368</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="6080" citStr="Yatskar et al., 2010" startWordPosition="926" endWordPosition="929">Max (2000) described the use of syntactic simplification for aphasic readers. In work on the PSET project, Canning (2002) implemented a system which exploits a syntactic parser in order to rewrite compound sentences as sequences of simple sentences and to convert passive sentences into active ones for readers with aphasia. The success of these systems is tied to the performance levels of the syntactic parsers that they employ. More recently, the availability of resources such as Simple Wikipedia has enabled text simplification to be included in the paradigm of statistical machine translation (Yatskar et al., 2010; Coster and Kauchak, 2011). In this context, translation models are learned by aligning sentences in Wikipedia with their corresponding versions in Simple Wikipedia. Manifesting Basic English (Ogden, 1932), the extent to which Simple Wikipedia is accessible to people with autism has not yet been fully assessed. The field of text summarisation includes numerous approaches that can be regarded as examples of syntactic simplification. For example, Cohn and Lapata (2009) present a tree-to-tree transduction method that is used to filter non-essential information from syntactically parsed sentences</context>
</contexts>
<marker>Yatskar, Pang, Danescu-Niculescu-Mizil, Lee, 2010</marker>
<rawString>M. Yatskar, B. Pang, C. Danescu-Niculescu-Mizil, and L. Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from wikipedia. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365– 368, Los Angeles, California, June. Association of Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
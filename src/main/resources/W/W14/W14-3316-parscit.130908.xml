<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000539">
<title confidence="0.990472">
Stanford University’s Submissions to the WMT 2014 Translation Task
</title>
<author confidence="0.993533">
Julia Neidert∗, Sebastian Schuster∗, Spence Green,
Kenneth Heafield, and Christopher D. Manning
</author>
<affiliation confidence="0.996957">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.993923">
{jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.99734" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9984176">
We describe Stanford’s participation in
the French-English and English-German
tracks of the 2014 Workshop on Statisti-
cal Machine Translation (WMT). Our sys-
tems used large feature sets, word classes,
and an optional unconstrained language
model. Among constrained systems, ours
performed the best according to uncased
BLEU: 36.0% for French-English and
20.9% for English-German.
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999841875">
Phrasal (Green et al., 2014b) is a phrase-based ma-
chine translation system (Och and Ney, 2004) with
an online, adaptive tuning algorithm (Green et al.,
2013c) which allows efficient tuning of feature-
rich translation models. We improved upon the
basic Phrasal system with sparse features over word
classes, class-based language models, and a web-
scale language model.
We submitted one constrained French-English
(Fr-En) system, one unconstrained English-German
(En-De) system with a huge language model, and
one constrained English-German system without it.
Each system was built using over 100,000 features
and was tuned on over 10,000 sentences. This paper
describes our submitted systems and discusses how
the improvements affect translation quality.
</bodyText>
<sectionHeader confidence="0.982167" genericHeader="method">
2 Data Preparation &amp; Post-Processing
</sectionHeader>
<bodyText confidence="0.99990625">
We used all relevant data allowed by the con-
strained condition, with the exception of HindEn-
Corp and Wiki Headlines, which we deemed too
noisy. Specifically, our parallel data consists of the
Europarl version 7 (Koehn, 2005), parallel Com-
monCrawl (Smith et al., 2013), French-English UN,
Giga-FrEn, and News Commentary corpora pro-
vided by the evaluation. For monolingual data, we
</bodyText>
<note confidence="0.79613">
∗These authors contributed equally.
</note>
<table confidence="0.767521333333333">
Sentences Tokens
En-De 4.5M 222M
Fr-En 36.3M 2.1B
</table>
<tableCaption confidence="0.9046825">
Table 1: Gross parallel corpus statistics after pre-
processing.
</tableCaption>
<table confidence="0.462987">
Constrained LM Unconstrained LM
German 1.7B 38.9 B
English 7.2B -
</table>
<tableCaption confidence="0.614327">
Table 2: Number of tokens in pre-processed mono-
</tableCaption>
<bodyText confidence="0.990382473684211">
lingual corpora used to estimate the language mod-
els. We split the constrained English data into two
models: 3.7 billion tokens from Gigaword and 3.5
billion tokens from all other sources.
used the provided news crawl data from all years,
English Gigaword version 5 (Parker et al., 2011),
and target sides of the parallel data. This includes
English from the Yandex, CzEng, and parallel Com-
monCrawl corpora. For parallel CommonCrawl,
we concatenated the English halves for various lan-
guage pairs and then deduplicated at the sentence
level.
In addition, our unconstrained English-German
system used German text extracted from the en-
tire 2012, 2013, and winter 2013 CommonCrawl1
corpora by Buck et al. (2014).
Tables 1 and 2 show the sizes of the pre-
processed corpora of parallel text and monolingual
text from which our systems were built.
</bodyText>
<subsectionHeader confidence="0.997702">
2.1 Pre-Processing
</subsectionHeader>
<bodyText confidence="0.989416">
We used Stanford CoreNLP to tokenize the English
and German data according to the Penn Treebank
standard (Marcus et al., 1993). The French source
data was tokenized similarly to the French Treebank
</bodyText>
<footnote confidence="0.998319">
1http://commoncrawl.org
</footnote>
<page confidence="0.951369">
150
</page>
<bodyText confidence="0.878947333333333">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
(Abeillé et al., 2003) using the Stanford French
tokenizer (Green et al., 2013b).
We also lowercased the data and removed any
control characters. Further, we filtered out all lines
that consisted mainly of punctuation marks, re-
moved characters that are frequently used as bullet
points and standardized white spaces and newlines.
We additionally filtered out sentences longer than
100 tokens from the parallel corpora in order to
speed up model learning.
</bodyText>
<subsectionHeader confidence="0.99931">
2.2 Alignment
</subsectionHeader>
<bodyText confidence="0.995972">
For both systems, we used the Berkeley Aligner
(Liang et al., 2006) with default settings to align
the parallel data. We symmetrized the alignments
using the grow-diag heuristic.
</bodyText>
<subsectionHeader confidence="0.99788">
2.3 Language Models
</subsectionHeader>
<bodyText confidence="0.975545">
Our systems used up to three language models.
</bodyText>
<subsectionHeader confidence="0.890022">
2.3.1 Constrained Language Models
</subsectionHeader>
<bodyText confidence="0.999401">
For En-De, we used lmplz (Heafield et al., 2013)
to estimate a 5-gram language model on all WMT
German monolingual data and the German side of
the parallel Common Crawl corpus. To query the
model, we used KenLM (Heafield, 2011).
For the Fr-En system, we also estimated a 5-gram
language model from all the monolingual English
data and the English side of the parallel Common
Crawl, UN, Giga-FrEn, CzEng and Yandex corpora
using the same procedure as above. Additionally,
we estimated a second language model from the
English Gigaword corpus.
All of these language models used interpolated
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998).
</bodyText>
<subsectionHeader confidence="0.96154">
2.3.2 Unconstrained Language Model
</subsectionHeader>
<bodyText confidence="0.999712461538462">
Our unconstrained En-De submission used an ad-
ditional language model trained on German web
text gathered by the Common Crawl Foundation
and processed by Buck et al. (2014). This cor-
pus was formed from the 2012, 2013, and winter
2013 Common Crawl releases, which consist of web
pages converted to UTF-8 encoding with HTML
stripped. Applying the Compact Language Detec-
tor 2,2 2.89% of the data was identified as German,
amounting to 1 TB of uncompressed text. After
splitting sentences with the Europarl sentence split-
ter (Koehn, 2005), the text was deduplicated at the
sentence level to reduce the impact of boilerplate
</bodyText>
<footnote confidence="0.969786">
2https://code.google.com/p/cld2/
</footnote>
<table confidence="0.9943785">
Order 1 2 3 4 5
Count 226 1,916 6,883 13,292 17,576
</table>
<tableCaption confidence="0.992286">
Table 3: Number of unique n-grams, in millions,
</tableCaption>
<bodyText confidence="0.989961222222222">
appearing in the Common Crawl German language
model.
and pages that appeared in multiple crawls, discard-
ing 78% of the data. We treated the resulting data
as normal text, pre-processing it as described in
Section 2.1 to yield 38.9 billion tokens. We built
an unpruned interpolated modified Kneser-Ney lan-
guage model with this corpus (Table 3) and added
it as an additional feature alongside the constrained
language models. At 38.9 billion tokens after dedu-
plication, this monolingual data is almost 23 times
as large as the rest of the German monolingual cor-
pus. Since the test data was also collected from the
web, we cannot be sure that the test sentences were
not in the language model. However, substantial
portions of the test set are translations from other
languages, which were not posted online until after
2013.
</bodyText>
<subsectionHeader confidence="0.949774">
2.3.3 Word-Class Language Model
</subsectionHeader>
<bodyText confidence="0.999965166666667">
We also built a word-class language model for the
En-De system. We trained 512 word classes on
the constrained German data using the predictive
one-sided class model of Whittaker and Woodland
(2001) with the parallelized clustering algorithm of
Uszkoreit and Brants (2008) by Green et al. (2014a).
All tokens were mapped to their word class; infre-
quent tokens appearing fewer than 5 times were
mapped to a special cluster for unknown tokens.
Finally, we estimated a 7-gram language model on
the mapped corpus with SRILM (Stolcke, 2002)
using Witten-Bell smoothing (Bell et al., 1990).
</bodyText>
<subsectionHeader confidence="0.999388">
2.4 Tuning and Test Data
</subsectionHeader>
<bodyText confidence="0.9999915">
For development, we tuned our systems on all
13,573 sentences contained in the newstest2008-
2012 data sets and tested on the 3,000 sentences of
the newstest2013 data set. The final system weights
were chosen among all tuning iterations using per-
formance on the newstest2013 data set.
</bodyText>
<subsectionHeader confidence="0.999646">
2.5 Post-Processing
</subsectionHeader>
<bodyText confidence="0.999921">
Our post-processor recases and detokenizes sys-
tem output. For the English-German system, we
combined both tasks by using a Conditional Ran-
dom Field (CRF) model (Lafferty et al., 2001) to
</bodyText>
<page confidence="0.992185">
151
</page>
<bodyText confidence="0.999932461538461">
learn transformations between the raw output char-
acters and the post-processed versions. For each
test dataset, we trained a separate model on 500,000
sentences selected using the Feature Decay Algo-
rithm for bitext selection (Biçici and Yuret, 2011).
Features used include the character type of the cur-
rent and surrounding characters, the token type of
the current and surrounding tokens, and the position
of the character within its token.
The English output was recased using a language
model based recaser (Lita et al., 2003). The lan-
guage model was trained on the English side of the
Fr-En parallel data using lmplz.
</bodyText>
<sectionHeader confidence="0.958551" genericHeader="method">
3 Translation System
</sectionHeader>
<bodyText confidence="0.993945">
We built our translation systems using Phrasal.
</bodyText>
<subsectionHeader confidence="0.878188">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999642983050848">
Our translation model has 19 dense features that
were computed for all translation hypotheses: the
nine Moses (Koehn et al., 2007) baseline features,
the eight hierarchical lexicalized reordering model
features by Galley and Manning (2008), the log
count of each rule, and an indicator for unique rules.
On top of that, the model uses the following addi-
tional features of Green et al. (2014a).
Rule indicator features: An indicator feature for
each translation rule. To combat overfitting, this
feature fires only for rules that occur more than
50 times in the parallel data. Additional indicator
features were constructed by mapping the words in
each rule to their corresponding word classes.
Target unigram class: An indicator feature for
the class of each target word.
Alignments: An indicator feature for each align-
ment in a translation rule, including multi-word
alignments. Again, class-based translation rules
were used to extract additional indicator features.
Source class deletion: An indicator feature for
the class of each unaligned source word in a trans-
lation rule.
Punctuation count ratio: The ratio of target
punctuation tokens to source punctuation tokens
for each derivation.
Function word ratio: The ratio of target function
words to source function words. The function words
for each language are the 35 most frequent words
on each side of the parallel data. Numbers and
punctuation marks are not included in this list.
Target-class bigram boundary: An indicator
feature for the concatenation of the word class of
the rightmost word in the left rule and the word
class of the leftmost word in the right rule in each
adjacent rule pair in a derivation.
Length features: Indicator features for the length
of the source side and for the length of the target
side of the translation rule and an indicator feature
for the concatenation of the two lengths.
Rule orientation features: An indicator feature
for each translation rule combined with its orienta-
tion class (monotone, swap, or discontinuous). This
feature also fires only for rules that occur more than
50 times in the parallel data. Again, class-based
translation rules were used to extract additional fea-
tures.
Signed linear distortion: The signed linear dis-
tortion S for two rules a and b is S = r(a)−l(b)+1,
where r(x) is the rightmost source index of rule x
and l(x) is the leftmost source index of rule x. Each
adjacent rule pair in a derivation has an indicator
feature for the signed linear distortion of this pair.
Many of these features consider word classes
instead of the actual tokens. For the target side, we
used the same word classes as we used to train the
class-based language model. For the source side,
we trained word classes on all available data using
the same method.
</bodyText>
<subsectionHeader confidence="0.997979">
3.2 Tuning
</subsectionHeader>
<bodyText confidence="0.999967083333333">
We used an online, adaptive tuning algorithm
(Green et al., 2013c) to learn the feature weights.
The loss function is an online variant of expected
BLEU (Green et al., 2014a). As a sentence-level
metric, we used the extended BLEU+1 metric that
smooths the unigram precision as well as the refer-
ence length (Nakov et al., 2012). For feature selec-
tion, we used Li regularization. Each tuning epoch
produces a different set of weights; we tried all of
them on newstest2013, which was held out from the
tuning set, then picked the weights that produced
the best uncased BLEU score.
</bodyText>
<subsectionHeader confidence="0.995954">
3.3 System Parameters
</subsectionHeader>
<bodyText confidence="0.999676333333333">
We started off with the parameters of our systems
for the WMT 2013 Translation Task (Green et
al., 2013a) and optimized the Li-regularization
strength. Both systems used the following tuning
parameters: a 200-best list, a learning rate of 0.02
and a mini-batch size of 20. The En-De system
</bodyText>
<page confidence="0.958231">
152
</page>
<table confidence="0.9842598">
Track Stanford Best Rank Track Stanford Best Rank
En-De constrained 19.9 20.1 3 En-De constrained 20.7 20.7 1
En-De unconstrained 20.0 20.6 5 En-De unconstrained 20.9 21.0 3
Fr-En constrained 34.5 35.0 3 Fr-En constrained 36.0 36.0 1
(a) cased BLEU (%) (b) uncased BLEU (%)
</table>
<tableCaption confidence="0.956762">
Table 4: Official results in terms of cased and uncased BLEU of our submitted systems compared to the
</tableCaption>
<bodyText confidence="0.807719363636364">
best systems for each track. The ranks for the unconstrained system are calculated relative to all primary
submissions for the language pair, whereas the ranks for the constrained systems are relative to only the
constrained systems submitted.
used a phrase length limit of 8, a distortion limit of
6 and a Li-regularization strength of 0.0002. The
Fr-En system used a phrase length limit of 9, a dis-
tortion limit of 5 and a Li-regularization strength
of 0.0001.
During tuning, we set the stack size for cube prun-
ing to Phrasal&apos;s default value of 1200. To decode
the test set, we increased the stack size to 3000.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.99984215625">
Table 4 shows the official results of our systems
compared to other submissions to the WMT shared
task. Both our En-De and Fr-En systems achieved
the highest uncased BLEU scores among all con-
strained submissions. However, our recaser evi-
dently performed quite poorly compared to other
systems, so our constrained systems ranked third by
cased BLEU score. Our unconstrained En-De sub-
mission ranked third among all systems by uncased
BLEU and fifth by cased BLEU.
To demonstrate the effectiveness of the individ-
ual improvements, we show results for four differ-
ent En-De systems: (1) A baseline that contains
only the 19 dense features, (2) a feature-rich trans-
lation system with the additional rich features, (3)
a feature-rich translation system with an additional
word class LM, and (4) a feature-rich translation
system with an additional wordclass LM and a huge
language model. For Fr-En we only built systems
(1)-(3). Results for all systems can be seen in Table
5 and Table 6. From these results, we can see that
both language pairs benefitted from adding rich fea-
tures (+0.4 BLEU for En-De and +0.5 BLEU for
Fr-En). However, we only see improvements from
the class-based language model in the case of the
En-De system (+0.4 BLEU). For this reason our Fr-
En submission did not use a class-based language
model. Using additional data in the form of a huge
language model further improved our En-De sys-
tem by almost 1% BLEU on the newstest2013 data
set. However, we only saw 0.2 BLEU improvement
on the newstest2014 data set.
</bodyText>
<subsectionHeader confidence="0.994776">
4.1 Analysis
</subsectionHeader>
<bodyText confidence="0.987675166666667">
Gains from rich features are in line with the gains
we saw in the WMT 2013 translation task (Green
et al., 2013a). We suspect that rich features would
improve the translation quality a lot more if we had
several reference translations to tune on.
The word class language model seemed to im-
prove only translations in our En-De system while
it had no effect on BLEU in our Fr-En system. One
of the main reasons seems to be that the 7-gram
word class language model helped particularly with
long range reordering, which happens far more fre-
quently in the En-De language pair compared to the
Fr-En pair. For example, in the following transla-
tion, we can see that the system with the class-based
language model successfully translated the verb in
the second clause (set in italic) while the system
without the class-based language model did not
translate the verb.
Source: It became clear to me that this is my path.
Feature-rich: Es wurde mir klar, dass das mein
Weg.
Word class LM: Es wurde mir klar, dass das mein
Weg ist.
We can also see that the long range of the word
class language model improved grammaticality as
shown in the following example:
Source: Meanwhile, more than 40 percent of the
population are HIV positive.
Feature-rich: Inzwischen sind mehr als 40
Prozent der Bevölkerung sind HIV positiv.
</bodyText>
<page confidence="0.997862">
153
</page>
<table confidence="0.9997238">
#iterations tune 2013 2013 cased 2014 2014 cased
Dense 8 16.9 19.6 18.7 20.0 19.2
Feature-rich 10 20.1 20.0 19.0 20.0 19.2
+ Word class LM 15 21.1 20.4 19.5 20.7 19.9
+ Huge LM 9 21.0 21.3 20.3 20.9 20.1
</table>
<tableCaption confidence="0.997797">
Table 5: En-De BLEU results. The tuning set is newstest2008–2012. Scores on newstest2014 were
computed after the system submission deadline using the released references.
</tableCaption>
<table confidence="0.99982825">
#iterations tune 2013 2013 cased 2014 2014 cased
Dense 1 29.1 32.0 30.4 35.6 34.0
Feature-rich 12 37.2 32.5 30.9 36.0 34.5
+ Word class LM 14 35.7 32.3 30.7 – –
</table>
<tableCaption confidence="0.9673215">
Table 6: Fr-En BLEU results. The tuning set is newstest2008–2012. Scores on newstest2014 were
computed after the system submission deadline using the released references.
</tableCaption>
<bodyText confidence="0.9414277">
Word class LM: Unterdessen mehr als 40 Prozent
der Bevölkerung sind HIV positiv.
In this example, the system without the class-
based language model translated the verb twice. In
the second translation, the class-based language
model prevented this long range disagreement. An
analysis of the differences in the translation output
of our Fr-En systems showed that the word class
language model mainly led to different word choices
but does not seem to help grammatically.
</bodyText>
<subsectionHeader confidence="0.996268">
4.2 Casing
</subsectionHeader>
<bodyText confidence="0.999924777777778">
Our system performed comparatively poorly at cas-
ing, as shown in Table 4. In analysis after the eval-
uation, we found many of these errors related to
words with internal capitals, such as “McCaskill”,
because the limited recaser we used, which is based
on a language model, considered only all lowercase,
an initial capital, or all uppercase words. We ad-
dressed this issue by allowing any casing seen in the
monolingual data. Some words were not seen at all
in the monolingual data but, since the target side of
the parallel data was included in monolingual data,
these words must have come from the source sen-
tence. In such situations, we preserved the word’s
original case. Table 7 shows the results with the re-
vised casing model. We gained about 0.24% BLEU
for German recasing and 0.15% BLEU for English
recasing over our submitted systems. In future work,
we plan to compare with a truecased system.
</bodyText>
<table confidence="0.8621524">
En-De Fr-En
Uncased Oracle 20.71 36.05
Conditional Random Field 19.85 –
Limited Recaser 19.82 34.51
Revised Recaser 20.09 34.66
</table>
<tableCaption confidence="0.962487">
Table 7: Casing results on newstest2014 performed
</tableCaption>
<bodyText confidence="0.765370333333333">
after the evaluation. The oracle scores are uncased
BLEU (%) while all other scores are cased. Sub-
mitted systems are shown in italic.
</bodyText>
<sectionHeader confidence="0.991095" genericHeader="method">
5 Negative Results
</sectionHeader>
<bodyText confidence="0.999565">
We experimented with several additions that did not
make it into the final submissions.
</bodyText>
<subsectionHeader confidence="0.99754">
5.1 Preordering
</subsectionHeader>
<bodyText confidence="0.999975866666667">
One of the key challenges when translating from
English to German is the long-range reordering of
verbs. For this reason, we implemented a depen-
dency tree based reordering system (Lerner and
Petrov, 2013). We parsed all source side sentences
using the Stanford Dependency Parser (De Marn-
effe et al., 2006) and trained the preordering system
on the entire bitext. Then we preordered the source
side of the bitext and the tuning and development
data sets using our preordering system, realigned
the bitext and tuned a machine translation system
using the preordered data. While preordering im-
proved verb reordering in many cases, many other
parts of the sentences were often also reordered
which led to an overall decrease in translation qual-
</bodyText>
<page confidence="0.99949">
154
</page>
<bodyText confidence="0.996148333333333">
ity. Therefore, we concluded that this system will re-
quire further development before it is useful within
our translation system.
</bodyText>
<figure confidence="0.4842015">
References
Anne Abeillé, Lionel Clément, and Alexandra Kinyon,
2003. Building a treebank for French, chapter 10.
Kluwer.
</figure>
<subsectionHeader confidence="0.988798">
5.2 Minimum Bayes Risk Decoding
</subsectionHeader>
<bodyText confidence="0.999978285714286">
We further attempted to improve our output by re-
ordering the best 1000 translations for each sentence
using Minimum Bayes Risk decoding (Kumar and
Byrne, 2004) with BLEU as the distance measure.
This in effect increases the score of candidates that
are “closer” to the other likely translations, where
“closeness” is measured by the BLEU score for the
candidate when the other translations are used as the
reference. Choosing the best translation following
this reordering improved overall performance when
tuned on the first half of the newstest2013 test set by
only 0.03 BLEU points for the English-German sys-
tem and 0.005 BLEU points for the French-English
system, so we abandoned this approach.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999505">
We submitted three systems: one constrained Fr-En
system, one constrained En-De system, and one un-
constrained En-De system. Among all constrained
systems, ours performed the best according to un-
cased BLEU. The key differentiating components
of our systems are class-based features, word class
language models, and a huge web-scale language
model. In ongoing work, we are investigating pre-
ordering for En-De translation as well as improved
recasing.
</bodyText>
<sectionHeader confidence="0.997363" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996584375">
We thank Michael Kayser and Thang Luong for
help with experiments. This work was supported
by the Defense Advanced Research Projects Agency
(DARPA) Broad Operational Language Translation
(BOLT) program through IBM. This work used the
Extreme Science and Engineering Discovery Envi-
ronment (XSEDE), which is supported by National
Science Foundation grant number OCI-1053575.
The authors acknowledge the Texas Advanced Com-
puting Center (TACC) at The University of Texas
at Austin for providing HPC resources that have
contributed to the research results reported within
this paper. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA or the US government.
</bodyText>
<reference confidence="0.9991686">
Timothy C. Bell, John G. Cleary, and Ian H. Witten.
1990. Text compression. Prentice-Hall.
Ergun Biçici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In WMT.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram counts and language models from the
common crawl. In LREC.
Stanley Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard Univer-
sity, August.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In LREC.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt,
John Bauer, Sida Wang, et al. 2013a. Feature-rich
phrase-based translation: Stanford University’s sub-
mission to the WMT 2013 translation task. In WMT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2013b. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195–227.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D. Manning. 2013c. Fast and adaptive online
training of feature-rich translation models. In ACL.
Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014a. An empirical comparison of features
and tuning for phrase-based machine translation. In
WMT.
Spence Green, Daniel Cer, and Christopher D. Man-
ning. 2014b. Phrasal: A toolkit for new directions
in statistical machine translation. In WMT.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modified
Kneser-Ney language model estimation. In ACL.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In WMT.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
ICASSP.
</reference>
<page confidence="0.98686">
155
</page>
<reference confidence="0.999615458333333">
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
et al. 2007. Moses: Open source toolkit for statisti-
cal machine translation. In ACL, Demonstration Ses-
sion.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings ofMT
Summit.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML.
Uri Lerner and Slav Petrov. 2013. Source-side classi-
fier preordering for machine translation. In EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and
Nanda Kambhatla. 2003. tRuEcasIng. In ACL.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19:313–330.
Preslav Nakov, Francisco Guzman, and Stephan Vogel.
2012. Optimizing for sentence-level BLEU+1 yields
short translations. In COLING.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Robert Parker, David Graff, Junbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword
fifth edition, june. Linguistic Data Consortium,
LDC2011T07.
Jason Smith, Hervé Saint-Amand, Magdalena Plamada,
Philipp Koehn, Chris Callison-Burch, and Adam
Lopez. 2013. Dirt cheap web-scale parallel text
from the common crawl. In ACL. Association for
Computational Linguistics, August.
Andreas Stolcke. 2002. SRILM—an extensible lan-
guage modeling toolkit. In ICLSP.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In ACL.
Ed W. D. Whittaker and Philip C. Woodland. 2001. Ef-
ficient class-based language modelling for very large
vocabularies. In ICASSP.
</reference>
<page confidence="0.998789">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.500402">
<title confidence="0.991834">Stanford University’s Submissions to the WMT 2014 Translation Task</title>
<author confidence="0.9839865">Sebastian Spence Green</author>
<author confidence="0.9839865">D Manning Heafield</author>
<affiliation confidence="0.999914">Computer Science Department, Stanford University</affiliation>
<email confidence="0.999063">jneid@cs.stanford.edu</email>
<email confidence="0.999063">sebschu@cs.stanford.edu</email>
<email confidence="0.999063">spenceg@cs.stanford.edu</email>
<email confidence="0.999063">heafield@cs.stanford.edu</email>
<email confidence="0.999063">manning@cs.stanford.edu</email>
<abstract confidence="0.946728727272727">We describe Stanford’s participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT). Our systems used large feature sets, word classes, and an optional unconstrained language model. Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy C Bell</author>
<author>John G Cleary</author>
<author>Ian H Witten</author>
</authors>
<title>Text compression.</title>
<date>1990</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="7004" citStr="Bell et al., 1990" startWordPosition="1075" endWordPosition="1078">3. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest2008- 2012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. 2.5 Post-Processing Our post-processor recases and detokenizes system output. For the English-German system, we combined both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to 151 learn transformations between the raw output characters and the post-processe</context>
</contexts>
<marker>Bell, Cleary, Witten, 1990</marker>
<rawString>Timothy C. Bell, John G. Cleary, and Ian H. Witten. 1990. Text compression. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Biçici</author>
<author>Deniz Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="7776" citStr="Biçici and Yuret, 2011" startWordPosition="1197" endWordPosition="1200">the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. 2.5 Post-Processing Our post-processor recases and detokenizes system output. For the English-German system, we combined both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to 151 learn transformations between the raw output characters and the post-processed versions. For each test dataset, we trained a separate model on 500,000 sentences selected using the Feature Decay Algorithm for bitext selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline featur</context>
</contexts>
<marker>Biçici, Yuret, 2011</marker>
<rawString>Ergun Biçici and Deniz Yuret. 2011. Instance selection for machine translation using feature decay algorithms. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Buck</author>
<author>Kenneth Heafield</author>
<author>Bas van Ooyen</author>
</authors>
<title>N-gram counts and language models from the common crawl.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<marker>Buck, Heafield, van Ooyen, 2014</marker>
<rawString>Christian Buck, Kenneth Heafield, and Bas van Ooyen. 2014. N-gram counts and language models from the common crawl. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University,</institution>
<contexts>
<context position="4777" citStr="Chen and Goodman, 1998" startWordPosition="713" endWordPosition="716"> 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Giga-FrEn, CzEng and Yandex corpora using the same procedure as above. Additionally, we estimated a second language model from the English Gigaword corpus. All of these language models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 2.3.2 Unconstrained Language Model Our unconstrained En-De submission used an additional language model trained on German web text gathered by the Common Crawl Foundation and processed by Buck et al. (2014). This corpus was formed from the 2012, 2013, and winter 2013 Common Crawl releases, which consist of web pages converted to UTF-8 encoding with HTML stripped. Applying the Compact Language Detector 2,2 2.89% of the data was identified as German, amounting to 1 TB of uncompressed text. After splitting sentences with the Europarl sentence splitter (Koehn, 2005), the text was deduplicated at</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8469" citStr="Galley and Manning (2008)" startWordPosition="1307" endWordPosition="1310">nding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline features, the eight hierarchical lexicalized reordering model features by Galley and Manning (2008), the log count of each rule, and an indicator for unique rules. On top of that, the model uses the following additional features of Green et al. (2014a). Rule indicator features: An indicator feature for each translation rule. To combat overfitting, this feature fires only for rules that occur more than 50 times in the parallel data. Additional indicator features were constructed by mapping the words in each rule to their corresponding word classes. Target unigram class: An indicator feature for the class of each target word. Alignments: An indicator feature for each alignment in a translatio</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Daniel Cer</author>
<author>Kevin Reschke</author>
<author>Rob Voigt</author>
<author>John Bauer</author>
<author>Sida Wang</author>
</authors>
<title>Feature-rich phrase-based translation: Stanford University’s submission to the WMT 2013 translation task.</title>
<date>2013</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="827" citStr="Green et al., 2013" startWordPosition="105" endWordPosition="108">ty {jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu Abstract We describe Stanford’s participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT). Our systems used large feature sets, word classes, and an optional unconstrained language model. Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German. 1 Introduction Phrasal (Green et al., 2014b) is a phrase-based machine translation system (Och and Ney, 2004) with an online, adaptive tuning algorithm (Green et al., 2013c) which allows efficient tuning of featurerich translation models. We improved upon the basic Phrasal system with sparse features over word classes, class-based language models, and a webscale language model. We submitted one constrained French-English (Fr-En) system, one unconstrained English-German (En-De) system with a huge language model, and one constrained English-German system without it. Each system was built using over 100,000 features and was tuned on over 10,000 sentences. This paper describes our submitted systems and discusses how the improvements affect translation quality. 2 Da</context>
<context position="3442" citStr="Green et al., 2013" startWordPosition="502" endWordPosition="505">2 show the sizes of the preprocessed corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to tokenize the English and German data according to the Penn Treebank standard (Marcus et al., 1993). The French source data was tokenized similarly to the French Treebank 1http://commoncrawl.org 150 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our system</context>
<context position="11071" citStr="Green et al., 2013" startWordPosition="1739" endWordPosition="1742"> linear distortion S for two rules a and b is S = r(a)−l(b)+1, where r(x) is the rightmost source index of rule x and l(x) is the leftmost source index of rule x. Each adjacent rule pair in a derivation has an indicator feature for the signed linear distortion of this pair. Many of these features consider word classes instead of the actual tokens. For the target side, we used the same word classes as we used to train the class-based language model. For the source side, we trained word classes on all available data using the same method. 3.2 Tuning We used an online, adaptive tuning algorithm (Green et al., 2013c) to learn the feature weights. The loss function is an online variant of expected BLEU (Green et al., 2014a). As a sentence-level metric, we used the extended BLEU+1 metric that smooths the unigram precision as well as the reference length (Nakov et al., 2012). For feature selection, we used Li regularization. Each tuning epoch produces a different set of weights; we tried all of them on newstest2013, which was held out from the tuning set, then picked the weights that produced the best uncased BLEU score. 3.3 System Parameters We started off with the parameters of our systems for the WMT 20</context>
<context position="14552" citStr="Green et al., 2013" startWordPosition="2332" endWordPosition="2335">irs benefitted from adding rich features (+0.4 BLEU for En-De and +0.5 BLEU for Fr-En). However, we only see improvements from the class-based language model in the case of the En-De system (+0.4 BLEU). For this reason our FrEn submission did not use a class-based language model. Using additional data in the form of a huge language model further improved our En-De system by almost 1% BLEU on the newstest2013 data set. However, we only saw 0.2 BLEU improvement on the newstest2014 data set. 4.1 Analysis Gains from rich features are in line with the gains we saw in the WMT 2013 translation task (Green et al., 2013a). We suspect that rich features would improve the translation quality a lot more if we had several reference translations to tune on. The word class language model seemed to improve only translations in our En-De system while it had no effect on BLEU in our Fr-En system. One of the main reasons seems to be that the 7-gram word class language model helped particularly with long range reordering, which happens far more frequently in the En-De language pair compared to the Fr-En pair. For example, in the following translation, we can see that the system with the class-based language model succe</context>
</contexts>
<marker>Green, Cer, Reschke, Voigt, Bauer, Wang, 2013</marker>
<rawString>Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt, John Bauer, Sida Wang, et al. 2013a. Feature-rich phrase-based translation: Stanford University’s submission to the WMT 2013 translation task. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing models for identifying multiword expressions.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Green, de Marneffe, Manning, 2013</marker>
<rawString>Spence Green, Marie-Catherine de Marneffe, and Christopher D. Manning. 2013b. Parsing models for identifying multiword expressions. Computational Linguistics, 39(1):195–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Sida Wang</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast and adaptive online training of feature-rich translation models.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="827" citStr="Green et al., 2013" startWordPosition="105" endWordPosition="108">ty {jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu Abstract We describe Stanford’s participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT). Our systems used large feature sets, word classes, and an optional unconstrained language model. Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German. 1 Introduction Phrasal (Green et al., 2014b) is a phrase-based machine translation system (Och and Ney, 2004) with an online, adaptive tuning algorithm (Green et al., 2013c) which allows efficient tuning of featurerich translation models. We improved upon the basic Phrasal system with sparse features over word classes, class-based language models, and a webscale language model. We submitted one constrained French-English (Fr-En) system, one unconstrained English-German (En-De) system with a huge language model, and one constrained English-German system without it. Each system was built using over 100,000 features and was tuned on over 10,000 sentences. This paper describes our submitted systems and discusses how the improvements affect translation quality. 2 Da</context>
<context position="3442" citStr="Green et al., 2013" startWordPosition="502" endWordPosition="505">2 show the sizes of the preprocessed corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to tokenize the English and German data according to the Penn Treebank standard (Marcus et al., 1993). The French source data was tokenized similarly to the French Treebank 1http://commoncrawl.org 150 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our system</context>
<context position="11071" citStr="Green et al., 2013" startWordPosition="1739" endWordPosition="1742"> linear distortion S for two rules a and b is S = r(a)−l(b)+1, where r(x) is the rightmost source index of rule x and l(x) is the leftmost source index of rule x. Each adjacent rule pair in a derivation has an indicator feature for the signed linear distortion of this pair. Many of these features consider word classes instead of the actual tokens. For the target side, we used the same word classes as we used to train the class-based language model. For the source side, we trained word classes on all available data using the same method. 3.2 Tuning We used an online, adaptive tuning algorithm (Green et al., 2013c) to learn the feature weights. The loss function is an online variant of expected BLEU (Green et al., 2014a). As a sentence-level metric, we used the extended BLEU+1 metric that smooths the unigram precision as well as the reference length (Nakov et al., 2012). For feature selection, we used Li regularization. Each tuning epoch produces a different set of weights; we tried all of them on newstest2013, which was held out from the tuning set, then picked the weights that produced the best uncased BLEU score. 3.3 System Parameters We started off with the parameters of our systems for the WMT 20</context>
<context position="14552" citStr="Green et al., 2013" startWordPosition="2332" endWordPosition="2335">irs benefitted from adding rich features (+0.4 BLEU for En-De and +0.5 BLEU for Fr-En). However, we only see improvements from the class-based language model in the case of the En-De system (+0.4 BLEU). For this reason our FrEn submission did not use a class-based language model. Using additional data in the form of a huge language model further improved our En-De system by almost 1% BLEU on the newstest2013 data set. However, we only saw 0.2 BLEU improvement on the newstest2014 data set. 4.1 Analysis Gains from rich features are in line with the gains we saw in the WMT 2013 translation task (Green et al., 2013a). We suspect that rich features would improve the translation quality a lot more if we had several reference translations to tune on. The word class language model seemed to improve only translations in our En-De system while it had no effect on BLEU in our Fr-En system. One of the main reasons seems to be that the 7-gram word class language model helped particularly with long range reordering, which happens far more frequently in the En-De language pair compared to the Fr-En pair. For example, in the following translation, we can see that the system with the class-based language model succe</context>
</contexts>
<marker>Green, Wang, Cer, Manning, 2013</marker>
<rawString>Spence Green, Sida Wang, Daniel Cer, and Christopher D. Manning. 2013c. Fast and adaptive online training of feature-rich translation models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>An empirical comparison of features and tuning for phrase-based machine translation.</title>
<date>2014</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="698" citStr="Green et al., 2014" startWordPosition="84" endWordPosition="87">∗, Sebastian Schuster∗, Spence Green, Kenneth Heafield, and Christopher D. Manning Computer Science Department, Stanford University {jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu Abstract We describe Stanford’s participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT). Our systems used large feature sets, word classes, and an optional unconstrained language model. Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German. 1 Introduction Phrasal (Green et al., 2014b) is a phrase-based machine translation system (Och and Ney, 2004) with an online, adaptive tuning algorithm (Green et al., 2013c) which allows efficient tuning of featurerich translation models. We improved upon the basic Phrasal system with sparse features over word classes, class-based language models, and a webscale language model. We submitted one constrained French-English (Fr-En) system, one unconstrained English-German (En-De) system with a huge language model, and one constrained English-German system without it. Each system was built using over 100,000 features and was tuned on over</context>
<context position="6715" citStr="Green et al. (2014" startWordPosition="1028" endWordPosition="1031">f the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest2008- 2012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set.</context>
<context position="8620" citStr="Green et al. (2014" startWordPosition="1336" endWordPosition="1339">ng a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline features, the eight hierarchical lexicalized reordering model features by Galley and Manning (2008), the log count of each rule, and an indicator for unique rules. On top of that, the model uses the following additional features of Green et al. (2014a). Rule indicator features: An indicator feature for each translation rule. To combat overfitting, this feature fires only for rules that occur more than 50 times in the parallel data. Additional indicator features were constructed by mapping the words in each rule to their corresponding word classes. Target unigram class: An indicator feature for the class of each target word. Alignments: An indicator feature for each alignment in a translation rule, including multi-word alignments. Again, class-based translation rules were used to extract additional indicator features. Source class deletion</context>
<context position="11179" citStr="Green et al., 2014" startWordPosition="1758" endWordPosition="1761">ule x and l(x) is the leftmost source index of rule x. Each adjacent rule pair in a derivation has an indicator feature for the signed linear distortion of this pair. Many of these features consider word classes instead of the actual tokens. For the target side, we used the same word classes as we used to train the class-based language model. For the source side, we trained word classes on all available data using the same method. 3.2 Tuning We used an online, adaptive tuning algorithm (Green et al., 2013c) to learn the feature weights. The loss function is an online variant of expected BLEU (Green et al., 2014a). As a sentence-level metric, we used the extended BLEU+1 metric that smooths the unigram precision as well as the reference length (Nakov et al., 2012). For feature selection, we used Li regularization. Each tuning epoch produces a different set of weights; we tried all of them on newstest2013, which was held out from the tuning set, then picked the weights that produced the best uncased BLEU score. 3.3 System Parameters We started off with the parameters of our systems for the WMT 2013 Translation Task (Green et al., 2013a) and optimized the Li-regularization strength. Both systems used th</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>Spence Green, Daniel Cer, and Christopher D. Manning. 2014a. An empirical comparison of features and tuning for phrase-based machine translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Phrasal: A toolkit for new directions in statistical machine translation.</title>
<date>2014</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="698" citStr="Green et al., 2014" startWordPosition="84" endWordPosition="87">∗, Sebastian Schuster∗, Spence Green, Kenneth Heafield, and Christopher D. Manning Computer Science Department, Stanford University {jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu Abstract We describe Stanford’s participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT). Our systems used large feature sets, word classes, and an optional unconstrained language model. Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German. 1 Introduction Phrasal (Green et al., 2014b) is a phrase-based machine translation system (Och and Ney, 2004) with an online, adaptive tuning algorithm (Green et al., 2013c) which allows efficient tuning of featurerich translation models. We improved upon the basic Phrasal system with sparse features over word classes, class-based language models, and a webscale language model. We submitted one constrained French-English (Fr-En) system, one unconstrained English-German (En-De) system with a huge language model, and one constrained English-German system without it. Each system was built using over 100,000 features and was tuned on over</context>
<context position="6715" citStr="Green et al. (2014" startWordPosition="1028" endWordPosition="1031">f the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest2008- 2012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set.</context>
<context position="8620" citStr="Green et al. (2014" startWordPosition="1336" endWordPosition="1339">ng a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline features, the eight hierarchical lexicalized reordering model features by Galley and Manning (2008), the log count of each rule, and an indicator for unique rules. On top of that, the model uses the following additional features of Green et al. (2014a). Rule indicator features: An indicator feature for each translation rule. To combat overfitting, this feature fires only for rules that occur more than 50 times in the parallel data. Additional indicator features were constructed by mapping the words in each rule to their corresponding word classes. Target unigram class: An indicator feature for the class of each target word. Alignments: An indicator feature for each alignment in a translation rule, including multi-word alignments. Again, class-based translation rules were used to extract additional indicator features. Source class deletion</context>
<context position="11179" citStr="Green et al., 2014" startWordPosition="1758" endWordPosition="1761">ule x and l(x) is the leftmost source index of rule x. Each adjacent rule pair in a derivation has an indicator feature for the signed linear distortion of this pair. Many of these features consider word classes instead of the actual tokens. For the target side, we used the same word classes as we used to train the class-based language model. For the source side, we trained word classes on all available data using the same method. 3.2 Tuning We used an online, adaptive tuning algorithm (Green et al., 2013c) to learn the feature weights. The loss function is an online variant of expected BLEU (Green et al., 2014a). As a sentence-level metric, we used the extended BLEU+1 metric that smooths the unigram precision as well as the reference length (Nakov et al., 2012). For feature selection, we used Li regularization. Each tuning epoch produces a different set of weights; we tried all of them on newstest2013, which was held out from the tuning set, then picked the weights that produced the best uncased BLEU score. 3.3 System Parameters We started off with the parameters of our systems for the WMT 2013 Translation Task (Green et al., 2013a) and optimized the Li-regularization strength. Both systems used th</context>
</contexts>
<marker>Green, Cer, Manning, 2014</marker>
<rawString>Spence Green, Daniel Cer, and Christopher D. Manning. 2014b. Phrasal: A toolkit for new directions in statistical machine translation. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4160" citStr="Heafield et al., 2013" startWordPosition="614" endWordPosition="617"> lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our systems used up to three language models. 2.3.1 Constrained Language Models For En-De, we used lmplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Giga-FrEn, CzEng and Yandex corpora using the same procedure as above. Additionally, we estimated a second language model from the English Gigaword corpus. All of these language models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen a</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In WMT.</booktitle>
<contexts>
<context position="4339" citStr="Heafield, 2011" startWordPosition="647" endWordPosition="648">entences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our systems used up to three language models. 2.3.1 Constrained Language Models For En-De, we used lmplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Giga-FrEn, CzEng and Yandex corpora using the same procedure as above. Additionally, we estimated a second language model from the English Gigaword corpus. All of these language models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 2.3.2 Unconstrained Language Model Our unconstrained En-De submission used an additional language model trained on German web text gathered by the Common Crawl </context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="4752" citStr="Kneser and Ney, 1995" startWordPosition="709" endWordPosition="712">mplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Giga-FrEn, CzEng and Yandex corpora using the same procedure as above. Additionally, we estimated a second language model from the English Gigaword corpus. All of these language models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 2.3.2 Unconstrained Language Model Our unconstrained En-De submission used an additional language model trained on German web text gathered by the Common Crawl Foundation and processed by Buck et al. (2014). This corpus was formed from the 2012, 2013, and winter 2013 Common Crawl releases, which consist of web pages converted to UTF-8 encoding with HTML stripped. Applying the Compact Language Detector 2,2 2.89% of the data was identified as German, amounting to 1 TB of uncompressed text. After splitting sentences with the Europarl sentence splitter (Koehn, 2005), the</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, Demonstration Session.</booktitle>
<contexts>
<context position="8360" citStr="Koehn et al., 2007" startWordPosition="1293" endWordPosition="1296"> selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline features, the eight hierarchical lexicalized reordering model features by Galley and Manning (2008), the log count of each rule, and an indicator for unique rules. On top of that, the model uses the following additional features of Green et al. (2014a). Rule indicator features: An indicator feature for each translation rule. To combat overfitting, this feature fires only for rules that occur more than 50 times in the parallel data. Additional indicator features were constructed by mapping the words in each rule to their corresponding word classes. Target unigram class: An indicator f</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofMT Summit.</booktitle>
<contexts>
<context position="1684" citStr="Koehn, 2005" startWordPosition="233" endWordPosition="234">-English (Fr-En) system, one unconstrained English-German (En-De) system with a huge language model, and one constrained English-German system without it. Each system was built using over 100,000 features and was tuned on over 10,000 sentences. This paper describes our submitted systems and discusses how the improvements affect translation quality. 2 Data Preparation &amp; Post-Processing We used all relevant data allowed by the constrained condition, with the exception of HindEnCorp and Wiki Headlines, which we deemed too noisy. Specifically, our parallel data consists of the Europarl version 7 (Koehn, 2005), parallel CommonCrawl (Smith et al., 2013), French-English UN, Giga-FrEn, and News Commentary corpora provided by the evaluation. For monolingual data, we ∗These authors contributed equally. Sentences Tokens En-De 4.5M 222M Fr-En 36.3M 2.1B Table 1: Gross parallel corpus statistics after preprocessing. Constrained LM Unconstrained LM German 1.7B 38.9 B English 7.2B - Table 2: Number of tokens in pre-processed monolingual corpora used to estimate the language models. We split the constrained English data into two models: 3.7 billion tokens from Gigaword and 3.5 billion tokens from all other so</context>
<context position="5347" citStr="Koehn, 2005" startWordPosition="807" endWordPosition="808">ser and Ney, 1995; Chen and Goodman, 1998). 2.3.2 Unconstrained Language Model Our unconstrained En-De submission used an additional language model trained on German web text gathered by the Common Crawl Foundation and processed by Buck et al. (2014). This corpus was formed from the 2012, 2013, and winter 2013 Common Crawl releases, which consist of web pages converted to UTF-8 encoding with HTML stripped. Applying the Compact Language Detector 2,2 2.89% of the data was identified as German, amounting to 1 TB of uncompressed text. After splitting sentences with the Europarl sentence splitter (Koehn, 2005), the text was deduplicated at the sentence level to reduce the impact of boilerplate 2https://code.google.com/p/cld2/ Order 1 2 3 4 5 Count 226 1,916 6,883 13,292 17,576 Table 3: Number of unique n-grams, in millions, appearing in the Common Crawl German language model. and pages that appeared in multiple crawls, discarding 78% of the data. We treated the resulting data as normal text, pre-processing it as described in Section 2.1 to yield 38.9 billion tokens. We built an unpruned interpolated modified Kneser-Ney language model with this corpus (Table 3) and added it as an additional feature </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings ofMT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="19466" citStr="Kumar and Byrne, 2004" startWordPosition="3148" endWordPosition="3151">data. While preordering improved verb reordering in many cases, many other parts of the sentences were often also reordered which led to an overall decrease in translation qual154 ity. Therefore, we concluded that this system will require further development before it is useful within our translation system. References Anne Abeillé, Lionel Clément, and Alexandra Kinyon, 2003. Building a treebank for French, chapter 10. Kluwer. 5.2 Minimum Bayes Risk Decoding We further attempted to improve our output by reordering the best 1000 translations for each sentence using Minimum Bayes Risk decoding (Kumar and Byrne, 2004) with BLEU as the distance measure. This in effect increases the score of candidates that are “closer” to the other likely translations, where “closeness” is measured by the BLEU score for the candidate when the other translations are used as the reference. Choosing the best translation following this reordering improved overall performance when tuned on the first half of the newstest2013 test set by only 0.03 BLEU points for the English-German system and 0.005 BLEU points for the French-English system, so we abandoned this approach. 6 Conclusion We submitted three systems: one constrained Fr-</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7519" citStr="Lafferty et al., 2001" startWordPosition="1157" endWordPosition="1160">guage model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest2008- 2012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. 2.5 Post-Processing Our post-processor recases and detokenizes system output. For the English-German system, we combined both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to 151 learn transformations between the raw output characters and the post-processed versions. For each test dataset, we trained a separate model on 500,000 sentences selected using the Feature Decay Algorithm for bitext selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-E</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-side classifier preordering for machine translation.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="18484" citStr="Lerner and Petrov, 2013" startWordPosition="2992" endWordPosition="2995">ased Oracle 20.71 36.05 Conditional Random Field 19.85 – Limited Recaser 19.82 34.51 Revised Recaser 20.09 34.66 Table 7: Casing results on newstest2014 performed after the evaluation. The oracle scores are uncased BLEU (%) while all other scores are cased. Submitted systems are shown in italic. 5 Negative Results We experimented with several additions that did not make it into the final submissions. 5.1 Preordering One of the key challenges when translating from English to German is the long-range reordering of verbs. For this reason, we implemented a dependency tree based reordering system (Lerner and Petrov, 2013). We parsed all source side sentences using the Stanford Dependency Parser (De Marneffe et al., 2006) and trained the preordering system on the entire bitext. Then we preordered the source side of the bitext and the tuning and development data sets using our preordering system, realigned the bitext and tuned a machine translation system using the preordered data. While preordering improved verb reordering in many cases, many other parts of the sentences were often also reordered which led to an overall decrease in translation qual154 ity. Therefore, we concluded that this system will require f</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-side classifier preordering for machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3900" citStr="Liang et al., 2006" startWordPosition="574" endWordPosition="577">, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our systems used up to three language models. 2.3.1 Constrained Language Models For En-De, we used lmplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Gi</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Vlad Lita</author>
<author>Abe Ittycheriah</author>
<author>Salim Roukos</author>
<author>Nanda Kambhatla</author>
</authors>
<title>tRuEcasIng.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8055" citStr="Lita et al., 2003" startWordPosition="1243" endWordPosition="1246">both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to 151 learn transformations between the raw output characters and the post-processed versions. For each test dataset, we trained a separate model on 500,000 sentences selected using the Feature Decay Algorithm for bitext selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline features, the eight hierarchical lexicalized reordering model features by Galley and Manning (2008), the log count of each rule, and an indicator for unique rules. On top of that, the model uses the following additional features of Green et al. (2014a). Rule indicator features: An ind</context>
</contexts>
<marker>Lita, Ittycheriah, Roukos, Kambhatla, 2003</marker>
<rawString>Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, and Nanda Kambhatla. 2003. tRuEcasIng. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="3087" citStr="Marcus et al., 1993" startWordPosition="454" endWordPosition="457">, CzEng, and parallel CommonCrawl corpora. For parallel CommonCrawl, we concatenated the English halves for various language pairs and then deduplicated at the sentence level. In addition, our unconstrained English-German system used German text extracted from the entire 2012, 2013, and winter 2013 CommonCrawl1 corpora by Buck et al. (2014). Tables 1 and 2 show the sizes of the preprocessed corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to tokenize the English and German data according to the Penn Treebank standard (Marcus et al., 1993). The French source data was tokenized similarly to the French Treebank 1http://commoncrawl.org 150 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and new</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzman</author>
<author>Stephan Vogel</author>
</authors>
<title>Optimizing for sentence-level BLEU+1 yields short translations.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="11333" citStr="Nakov et al., 2012" startWordPosition="1784" endWordPosition="1787">of this pair. Many of these features consider word classes instead of the actual tokens. For the target side, we used the same word classes as we used to train the class-based language model. For the source side, we trained word classes on all available data using the same method. 3.2 Tuning We used an online, adaptive tuning algorithm (Green et al., 2013c) to learn the feature weights. The loss function is an online variant of expected BLEU (Green et al., 2014a). As a sentence-level metric, we used the extended BLEU+1 metric that smooths the unigram precision as well as the reference length (Nakov et al., 2012). For feature selection, we used Li regularization. Each tuning epoch produces a different set of weights; we tried all of them on newstest2013, which was held out from the tuning set, then picked the weights that produced the best uncased BLEU score. 3.3 System Parameters We started off with the parameters of our systems for the WMT 2013 Translation Task (Green et al., 2013a) and optimized the Li-regularization strength. Both systems used the following tuning parameters: a 200-best list, a learning rate of 0.02 and a mini-batch size of 20. The En-De system 152 Track Stanford Best Rank Track S</context>
</contexts>
<marker>Nakov, Guzman, Vogel, 2012</marker>
<rawString>Preslav Nakov, Francisco Guzman, and Stephan Vogel. 2012. Optimizing for sentence-level BLEU+1 yields short translations. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="765" citStr="Och and Ney, 2004" startWordPosition="95" endWordPosition="98">pher D. Manning Computer Science Department, Stanford University {jneid,sebschu,spenceg,heafield,manning}@cs.stanford.edu Abstract We describe Stanford’s participation in the French-English and English-German tracks of the 2014 Workshop on Statistical Machine Translation (WMT). Our systems used large feature sets, word classes, and an optional unconstrained language model. Among constrained systems, ours performed the best according to uncased BLEU: 36.0% for French-English and 20.9% for English-German. 1 Introduction Phrasal (Green et al., 2014b) is a phrase-based machine translation system (Och and Ney, 2004) with an online, adaptive tuning algorithm (Green et al., 2013c) which allows efficient tuning of featurerich translation models. We improved upon the basic Phrasal system with sparse features over word classes, class-based language models, and a webscale language model. We submitted one constrained French-English (Fr-En) system, one unconstrained English-German (En-De) system with a huge language model, and one constrained English-German system without it. Each system was built using over 100,000 features and was tuned on over 10,000 sentences. This paper describes our submitted systems and d</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2011</date>
<booktitle>Linguistic Data Consortium, LDC2011T07.</booktitle>
<note>English gigaword fifth edition,</note>
<contexts>
<context position="2389" citStr="Parker et al., 2011" startWordPosition="342" endWordPosition="345">ommentary corpora provided by the evaluation. For monolingual data, we ∗These authors contributed equally. Sentences Tokens En-De 4.5M 222M Fr-En 36.3M 2.1B Table 1: Gross parallel corpus statistics after preprocessing. Constrained LM Unconstrained LM German 1.7B 38.9 B English 7.2B - Table 2: Number of tokens in pre-processed monolingual corpora used to estimate the language models. We split the constrained English data into two models: 3.7 billion tokens from Gigaword and 3.5 billion tokens from all other sources. used the provided news crawl data from all years, English Gigaword version 5 (Parker et al., 2011), and target sides of the parallel data. This includes English from the Yandex, CzEng, and parallel CommonCrawl corpora. For parallel CommonCrawl, we concatenated the English halves for various language pairs and then deduplicated at the sentence level. In addition, our unconstrained English-German system used German text extracted from the entire 2012, 2013, and winter 2013 CommonCrawl1 corpora by Buck et al. (2014). Tables 1 and 2 show the sizes of the preprocessed corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to </context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword fifth edition, june. Linguistic Data Consortium, LDC2011T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Smith</author>
<author>Hervé Saint-Amand</author>
<author>Magdalena Plamada</author>
<author>Philipp Koehn</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Dirt cheap web-scale parallel text from the common crawl.</title>
<date>2013</date>
<booktitle>In ACL. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1727" citStr="Smith et al., 2013" startWordPosition="238" endWordPosition="241">rained English-German (En-De) system with a huge language model, and one constrained English-German system without it. Each system was built using over 100,000 features and was tuned on over 10,000 sentences. This paper describes our submitted systems and discusses how the improvements affect translation quality. 2 Data Preparation &amp; Post-Processing We used all relevant data allowed by the constrained condition, with the exception of HindEnCorp and Wiki Headlines, which we deemed too noisy. Specifically, our parallel data consists of the Europarl version 7 (Koehn, 2005), parallel CommonCrawl (Smith et al., 2013), French-English UN, Giga-FrEn, and News Commentary corpora provided by the evaluation. For monolingual data, we ∗These authors contributed equally. Sentences Tokens En-De 4.5M 222M Fr-En 36.3M 2.1B Table 1: Gross parallel corpus statistics after preprocessing. Constrained LM Unconstrained LM German 1.7B 38.9 B English 7.2B - Table 2: Number of tokens in pre-processed monolingual corpora used to estimate the language models. We split the constrained English data into two models: 3.7 billion tokens from Gigaword and 3.5 billion tokens from all other sources. used the provided news crawl data fr</context>
</contexts>
<marker>Smith, Saint-Amand, Plamada, Koehn, Callison-Burch, Lopez, 2013</marker>
<rawString>Jason Smith, Hervé Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez. 2013. Dirt cheap web-scale parallel text from the common crawl. In ACL. Association for Computational Linguistics, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In ICLSP.</booktitle>
<contexts>
<context position="6956" citStr="Stolcke, 2002" startWordPosition="1070" endWordPosition="1071">which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest2008- 2012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. 2.5 Post-Processing Our post-processor recases and detokenizes system output. For the English-German system, we combined both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to 151 learn transformations between</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM—an extensible language modeling toolkit. In ICLSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6693" citStr="Uszkoreit and Brants (2008)" startWordPosition="1023" endWordPosition="1026">23 times as large as the rest of the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest2008- 2012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the </context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ed W D Whittaker</author>
<author>Philip C Woodland</author>
</authors>
<title>Efficient class-based language modelling for very large vocabularies.</title>
<date>2001</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="6619" citStr="Whittaker and Woodland (2001)" startWordPosition="1013" endWordPosition="1016">At 38.9 billion tokens after deduplication, this monolingual data is almost 23 times as large as the rest of the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest2008- 2012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system</context>
</contexts>
<marker>Whittaker, Woodland, 2001</marker>
<rawString>Ed W. D. Whittaker and Philip C. Woodland. 2001. Efficient class-based language modelling for very large vocabularies. In ICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
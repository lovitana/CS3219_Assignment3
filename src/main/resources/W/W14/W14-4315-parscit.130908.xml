<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.042780">
<title confidence="0.995004">
Free on-line speech recogniser based on Kaldi ASR toolkit producing
word posterior lattices
</title>
<author confidence="0.994574">
Ondˇrej Pl´atek and Filip Jurˇc´ıˇcek
</author>
<affiliation confidence="0.996148">
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.99394">
{oplatek, jurcicek}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999165">
This paper presents an extension of
the Kaldi automatic speech recognition
toolkit to support on-line recognition.
The resulting recogniser supports acous-
tic models trained using state-of-the-
art acoustic modelling techniques. As
the recogniser produces word posterior lat-
tices, it is particularly useful in statisti-
cal dialogue systems, which try to ex-
ploit uncertainty in the recogniser’s out-
put. Our experiments show that the on-
line recogniser performs significantly bet-
ter in terms of latency when compared to
a cloud-based recogniser.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999368666666667">
There are many choices of speech recognisers, but
we find no alternative with both a permissive li-
cense and on-line recognition suitable for a spo-
ken dialogue system. The Google speech recog-
nition service1 provides state-of-the-art quality for
many tasks (Morbini et al., 2013) and may be used
for free; however, the licensing conditions are not
clear, adaptation of acoustic and language models
to a task at hand is not possible and the service is
not officially supported.
Another option is Nuance cloud based recogni-
tion2; however, again adjustments to the system
are not possible. Moreover, it is a paid service.
When considering local ASR systems, we
found no viable alternatives either. The HTK
toolkit does not provide on-line large vocabulary
decoders suitable for real-time decoding. Open-
Julius can be used with custom-built acoustic and
</bodyText>
<footnote confidence="0.9934728">
1The API is available at https://www.google.
com/speech-api/v1/recognize, and its use described
in a blog post at http://mikepultz.com/2013/07/
google-speech-api-full-duplex-php-version/.
2http://www.nuancemobiledeveloper.com/
</footnote>
<bodyText confidence="0.999829466666667">
language models and for on-line decoding (Aki-
nobu, 2014). However, OpenJulius suffers from
software instability when producing lattices and
confusion networks; therefore, it is not suitable
for practical use. The RWTH decoder is not a free
software and a license must be purchased for com-
mercial applications (Rybach et al., 2011).
As a result, we implemented a lightweight
modification of the LatticeFasterDecoder from
the Kaldi toolkit and created an on-line recogniser
with an interface that is suitable for statistical dia-
logue systems. The Kaldi toolkit as well as the on-
line recogniser is distributed under the Apache
2.0 license3. Our on-line recogniser may use
acoustic models trained using the state-of-the-art
techniques, such as Linear Discriminant Analysis
(LDA), Maximum Likelihood Linear Transform
(MLLT), Boosted Maximum Mutual Information
(BMMI), Minimum Phone Error (MPE). It pro-
duces word posterior lattices which can be easily
converted into high quality n-best lists. The recog-
niser’s speed and latency can be effectively con-
trolled off-line by optimising a language model
and during decoding by beam thresholds.
In the next section, the Kaldi recognition
toolkit is briefly described. Section 3 describes
the implementation of the OnlineLatgenRecog-
niser. Section 4 evaluates the accuracy and speed
of the recogniser. Finally, Section 5 concludes this
work.
</bodyText>
<sectionHeader confidence="0.911832" genericHeader="method">
2 The Kaldi toolkit
</sectionHeader>
<bodyText confidence="0.999346666666666">
The Kaldi toolkit4 is a speech recognition toolkit
distributed under a free license (Povey et al.,
2011). The toolkit is based on Finite State Trans-
ducers, implements state-of-the-art acoustic mod-
elling techniques, is computationally efficient, and
is already widely adapted among research groups.
</bodyText>
<footnote confidence="0.999682">
3http://www.apache.org/licenses/
LICENSE-2.0
4http://sourceforge.net/projects/kaldi
</footnote>
<page confidence="0.961858">
108
</page>
<bodyText confidence="0.927337">
Proceedings of the SIGDIAL 2014 Conference, pages 108–112,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
Its only major drawback was the lack of on-line
recognition support. Therefore, it could not be
used directly in applications such as spoken dia-
logue systems. Kaldi includes an on-line recogni-
tion application; however, hard-wired timeout ex-
ceptions, audio source fixed to a sound card, and a
specialised 1-best decoder limit its use to demon-
stration of Kaldi recognition capabilities only.
</bodyText>
<sectionHeader confidence="0.998597" genericHeader="method">
3 OnlineLatgenRecogniser
</sectionHeader>
<bodyText confidence="0.999879172413793">
The standard Kaldi interface between the compo-
nents of the toolkit is based on a batch process-
ing paradigm, where the components assume that
the whole audio signal is available when recog-
nition starts. However, when performing on-line
recognition, one would like to take advantage of
the fact that the signal appears in small chunks and
can be processed incrementally. When properly
implemented, this significantly reduces recogniser
output latency.
into the recogniser’s buffer (line 11) and imme-
diately decoded (lines 12-14). If the audio data
is supplied in small enough chunks, the decod-
ing of queued data is finished before new data ar-
rives. When the recognition is finished, the recog-
niser prepares for lattice extraction (line 16). Line
20 shows how to obtain word posterior lattice as
an OpenFST object. The getAudio() function rep-
resents a separate process supplying speech data.
Please note that the recogniser’s latency is mainly
determined by the time spent in the GetLattice
function.
Please note that we do not present here the func-
tions controlling the input stream of audio chunks
passed to the decoder and processing the output
because these differ according to use case. An
example of a nontrivial use case is in a dialogue
system through a thin Python wrapper (see Sec-
tion 3.2).
</bodyText>
<figure confidence="0.981448375">
OnlineLatgenRecogniser rec;
rec.Setup(...);
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</figure>
<subsectionHeader confidence="0.984638">
3.1 C++ implementation
</subsectionHeader>
<bodyText confidence="0.984281714285714">
To achieve this, we implemented Kaldi’s Decod-
ableInterface supporting incremental speech pre-
processing, which includes speech parameterisa-
tion, feature transformations, and likelihood esti-
mation. In addition, we subclassed LatticeFaster-
Decoder and split the original batch processing in-
terface.
The newly implemented OnlineLatgenRecog-
niser makes use of our incremental speech pre-
processing and modified LatticeFasterDecoder. It
implements the following interface:
size_t decoded_now = 0;
size_t max_decoded = 10;
char *audio_array = NULL;
</bodyText>
<equation confidence="0.987654">
while (recognitionOn())
{
size_t audio_len = getAudio(audio_array);
rec.AudioIn(audio_array, audio_len);
do {
decoded_now = rec.Decode(max_decoded);
} while(decoded_now &gt; 0);
}
rec.PruneFinal();
double tot_lik;
fst::VectorFst&lt;fst::LogArc&gt; word_post_lat;
rec.GetLattice(&amp;word_post_lat, &amp;tot_lik);
rec.Reset();
</equation>
<listItem confidence="0.951547083333333">
Listing 1: Example of the decoder API
• AudioIn – queueing new audio for pre-
processing,
• Decode – decoding a fixed number of audio
frames,
• PruneFinal – preparing internal data struc-
tures for lattice extraction,
• GetLattice – extracting a word posterior lat-
tice and returning log likelihood of processed
audio,
• Reset – preparing the recogniser for a new ut-
terance,
</listItem>
<bodyText confidence="0.9997226">
The C++ example in Listing 1 shows a typi-
cal use of the OnlineLatgenRecogniser interface.
When audio data becomes available, it is queued
The source code of the OnlineLatgenRecog-
niser is available in Kaldi repository5.
</bodyText>
<subsectionHeader confidence="0.999035">
3.2 Python extension
</subsectionHeader>
<bodyText confidence="0.999925545454545">
In addition, we developed a Python extension ex-
porting the OnlineLatgenRecogniser C++ inter-
face. This can be used as an example of bringing
Kaldi’s on-line speech recognition functionality to
higher-level programming languages. This Python
extension is used in the Alex Dialogue Systems
Framework (ADSF, 2014), an open-source lan-
guage and domain independent framework for
developing spoken dialogue systems. The On-
lineLatgenRecogniser is deployed in an appli-
cation which provides information about public
</bodyText>
<footnote confidence="0.99882">
5https://sourceforge.net/p/kaldi/code/
HEAD/tree/sandbox/oplatek2/src/dec-wrap/
</footnote>
<page confidence="0.997637">
109
</page>
<bodyText confidence="0.959614">
transport and weather in the Czech republic and is
available on a public toll-free telephone number.
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992673">
4.1 Acoustic and language model training
</subsectionHeader>
<bodyText confidence="0.999946714285714">
The OnlineLatgenRecogniser is evaluated on
a corpus of audio data from the Public Transport
Information (PTI) domain. In PTI, users can inter-
act in Czech with a telephone-based dialogue sys-
tem to find public transport connections (UFAL-
DSG, 2014). The PTI corpus consist of approx-
imately 12k user utterances with a length vary-
ing between 0.4 s and 18 s with median around
3 s. The data were divided into training, develop-
ment, and test data where the corresponding data
sizes were 9496, 1188, 1188 respectively. For
evaluation, a domain specific the class-based lan-
guage model with a vocabulary size of approxi-
mately 52k and 559k n-grams was estimated from
the training data. Named entities e.g., cities or bus
stops, in class-based language model are expanded
before building a decoding graph.
Since the PTI acoustic data amounts to less then
5 hours, the acoustic training data was extended
by an additional 15 hours of telephone out-of-
domain data from the VYSTADIAL 2013 - Czech
corpus (Korvas et al., 2014). The acoustic mod-
els were obtained by BMMI discriminative train-
ing with LDA and MLLT feature transformations.
The scripts used to train the acoustic models are
publicly available in ASDF (2014) as well as in
Kaldi6 and a detailed description of the training
procedure is given in Korvas et al. (2014).
</bodyText>
<subsectionHeader confidence="0.777877">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999695">
We focus on evaluating the speed of the On-
lineLatgenRecogniser and its relationship with the
accuracy of the decoder, namely:
</bodyText>
<listItem confidence="0.999884666666667">
• Real Time Factor (RTF) of decoding – the ra-
tio of the recognition time to the duration of
the audio input,
• Latency – the delay between utterance end
and the availability of the recognition results,
• Word Error Rate (WER).
</listItem>
<bodyText confidence="0.7284635">
Accuracy and speed of the OnlineLatgenRecog-
niser are controlled by the max-active-states,
</bodyText>
<footnote confidence="0.9087795">
6http://sourceforge.net/p/kaldi/code/
HEAD/tree/trunk/egs/vystadial_en/
</footnote>
<bodyText confidence="0.99990996">
beam, and lattice-beam parameters (Povey et al.,
2011). Max-active-states limits the maximum
number of active tokens during decoding. Beam is
used during graph search to prune ASR hypothe-
ses at the state level. Lattice-beam is used when
producing word level lattices after the decoding is
finished. It is crucial to tune these parameters op-
timally to obtain good results.
In general, one aims for a setting RTF smaller
than 1.0. However, in practice, it is useful if
the RTF is even smaller because other processes
running on the machine can influence the amount
of available computational resources. Therefore,
we target the RTF of 0.6 in our setup.
We used grid search on the development set to
identify optimal parameters. Figure 1 (a) shows
the impact of the beam on the WER and RTF
measures. In this case, we set max-active-states
to 2000 in order to limit the worst case RTF to
0.6. Observing Figure 1 (a), we set beam to 13
as this setting balances the WER, 95th RTF per-
centile, and the average RTF. Figure 1 (b) shows
the impact of the lattice-beam on WER and la-
tency when beam is fixed to 13. We set lattice-
beam to 5 based on Figure 1 (b) to obtain the 95th
latency percentile of 200 ms, which is consid-
ered natural in a dialogue (Skantze and Schlangen,
2009). Lattice-beam does not affect WER, but
larger lattice-beam improves the oracle WER of
generated lattices (Povey et al., 2012).
Figure 2 shows the percentile graph of the RTF
and latency measures over the development set.
For example, the 95th percentile is the value of
a measure such that 95% of the data has the mea-
sure below that value. One can see from Fig-
ure 2 that 95% of development utterances is de-
coded with RTF under 0.6 and latency under 200
ms. The extreme values are typically caused by
decoding long noisy utterances where uncertainty
in decoding slows down the recogniser. Using this
setting, OnlineLatgenRecogniser decodes the ut-
terances with a WER of about 21%.
Please note that OnlineLatgenRecogniser only
extends the batch Kaldi decoder for incremental
speech processing interface. It uses the same code
as the batch Kaldi decoder to compute speech
parametrisation, frame likelihoods, and state-level
lattices. Therefore, the accuracy of OnlineLatgen-
Recogniser is equal to that of the batch Kaldi de-
coder given the same parameters.
</bodyText>
<page confidence="0.988071">
110
</page>
<figure confidence="0.999175">
b
25
a
25
1.0
24
0.8
23
0.6
22
0.4
21
0.2
0.08 9 10 11 12 13 14 15 49
beam
1000
95th latency percentile
Desired latency 200 ms
WER
800
23
600
22
400
21
200
01 2 3 4 5 6 7 8 9 1019
lattice-beam
95th RTF percentile
Average RTF
Desired 0.6 RTF
WER
20
24
20
</figure>
<figureCaption confidence="0.998881714285714">
Figure 1: The left graph (a) shows that WER decreases with increasing beam and the average RTF
linearly grows with the beam. Setting the maximum number of active states to 2000 stops the growth of
the 95th RTF percentile at 0.6, indicating that even in the worst case, we can guarantee an RTF around
0.6. The right graph (b) shows how latency grows in response to increasing lattice-beam.
Figure 2: The percentile graphs show RTF and Latency scores for development data for max-active-
sates=2000, beam=13, lattice-beam=5. Note that 95 % of utterances were decoded with the latency
lower that 200ms.
</figureCaption>
<figure confidence="0.999720185185185">
a
RTF
Desired 0.6 RTF
Critical 1.0 RTF
95th percentile
0 20 40 60 80 100
percentile
b
Latency
Desired latency 200 ms
95th percentile
0 20 40 60 80 100
percentile
800
700
600
500
400
300
200
100
0
2.0
1.5
1.0
0.5
0.0
</figure>
<bodyText confidence="0.999821">
In addition, we have also experimented with
Google ASR service on the same domain.
The Google ASR service decodes 95% of test ut-
terances with latency under 1900 ms and WER is
about 48%. The high latency is presumably caused
by the batch processing of audio data and net-
work latency, and the high WER is likely caused
by a mismatch between Google’s acoustic and lan-
guage models and the test data.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979705882353">
This work presented the OnlineLatgenRecogniser,
an extension of the Kaldi automatic speech recog-
nition toolkit. The OnlineLatgenRecogniser is dis-
tributed under the Apache 2.0 license, and there-
fore it is freely available for both research and
commercial applications. The recogniser and its
Python extension is stable and intensively used
in a publicly available spoken dialogue system
(UFAL-DSG, 2014). Thanks to the use of a stan-
dard Kaldi lattice decoder, the recogniser produces
high quality word posterior lattices. The training
scripts for the acoustic model and the OnlineLat-
genRecogniser code are currently being integrated
in the Kaldi toolkit. Future planned improvements
include implementing more sophisticated speech
parameterisation interface and feature transforma-
tions.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993971909090909">
We would also like to thank Daniel Povey and Ondˇrej Dusˇek
for their useful comments and discussions. We also thank the
anonymous reviewers for their helpful comments and sugges-
tions.
This research was funded by the Ministry of Education,
Youth and Sports of the Czech Republic under the grant
agreement LK11221, by the core research funding of Charles
University in Prague. The language resources presented in
this work are stored and distributed by the LINDAT/CLARIN
project of the Ministry of Education, Youth and Sports of the
Czech Republic (project LM2010013).
</bodyText>
<page confidence="0.99841">
111
</page>
<sectionHeader confidence="0.995758" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999734097560975">
ADSF. 2014. The Alex Dialogue Systems Framework.
https://github.com/UFAL-DSG/alex.
Lee Akinobu. 2014. Open-Source Large Vocabulary CSR
Engine Julius. http://julius.sourceforge.
jp/en_index.php.
Matˇej Korvas, Ondˇrej Pl´atek, Ondˇrej Duˇsek, Luk´aˇs ˇZilka, and
Filip Jurˇc´ıˇcek. 2014. Free English and Czech telephone
speech corpus shared under the CC-BY-SA 3.0 license.
In Proceedings of the Eigth International Conference on
Language Resources and Evaluation (LREC 2014).
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Ar-
stein, Doan Can, Panayiotis G. Georgiou, Shrikanth S.
Narayanan, Anton Leuski, and David Traum. 2013.
Which ASR should I choose for my dialogue system? In
Proc. SIGDIAL, August.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Bur-
get, Ondrej Glembek, Nagendra Goel, Mirko Hannemann,
Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. 2011.
The kaldi speech recognition toolkit. In Proc. ASRU,
pages 1–4.
Daniel Povey, Mirko Hannemann, Gilles Boulianne, Lukas
Burget, Arnab Ghoshal, Milos Janda, Martin Karafi´at,
Stefan Kombrink, Petr Motlicek, Yanmin Qian, et al.
2012. Generating exact lattices in the WFST framework.
In Acoustics, Speech and Signal Processing (ICASSP),
2012 IEEE International Conference on, pages 4213–
4216. IEEE.
David Rybach, Stefan Hahn, Patrick Lehnen, David Nolden,
Martin Sundermeyer, Zoltan T¨uske, Siemon Wiesler,
Ralf Schl¨uter, and Hermann Ney. 2011. RASR-The
RWTH Aachen University Open Source Speech Recogni-
tion Toolkit. In Proc. IEEEAutomatic Speech Recognition
and Understanding Workshop.
Gabriel Skantze and David Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proceedings of
the 12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 745–753.
Association for Computational Linguistics.
UFAL-DSG. 2014. The Alex Dialogue Systems Framework
- Public Transport Information. https://github.
com/UFAL-DSG/alex.
</reference>
<page confidence="0.998287">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.562367">
<title confidence="0.9755">Free on-line speech recogniser based on Kaldi ASR toolkit word posterior lattices</title>
<author confidence="0.712722">Ondˇrej Pl´atek</author>
<author confidence="0.712722">Filip</author>
<affiliation confidence="0.926816">Charles University in Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<abstract confidence="0.9953492">This paper presents an extension of the Kaldi automatic speech recognition toolkit to support on-line recognition. The resulting recogniser supports acoustic models trained using state-of-theart acoustic modelling techniques. As the recogniser produces word posterior lattices, it is particularly useful in statistical dialogue systems, which try to exploit uncertainty in the recogniser’s output. Our experiments show that the online recogniser performs significantly better in terms of latency when compared to a cloud-based recogniser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ADSF</author>
</authors>
<title>The Alex Dialogue Systems Framework.</title>
<date>2014</date>
<note>https://github.com/UFAL-DSG/alex.</note>
<contexts>
<context position="7447" citStr="ADSF, 2014" startWordPosition="1094" endWordPosition="1095">udio, • Reset – preparing the recogniser for a new utterance, The C++ example in Listing 1 shows a typical use of the OnlineLatgenRecogniser interface. When audio data becomes available, it is queued The source code of the OnlineLatgenRecogniser is available in Kaldi repository5. 3.2 Python extension In addition, we developed a Python extension exporting the OnlineLatgenRecogniser C++ interface. This can be used as an example of bringing Kaldi’s on-line speech recognition functionality to higher-level programming languages. This Python extension is used in the Alex Dialogue Systems Framework (ADSF, 2014), an open-source language and domain independent framework for developing spoken dialogue systems. The OnlineLatgenRecogniser is deployed in an application which provides information about public 5https://sourceforge.net/p/kaldi/code/ HEAD/tree/sandbox/oplatek2/src/dec-wrap/ 109 transport and weather in the Czech republic and is available on a public toll-free telephone number. 4 Evaluation 4.1 Acoustic and language model training The OnlineLatgenRecogniser is evaluated on a corpus of audio data from the Public Transport Information (PTI) domain. In PTI, users can interact in Czech with a tele</context>
</contexts>
<marker>ADSF, 2014</marker>
<rawString>ADSF. 2014. The Alex Dialogue Systems Framework. https://github.com/UFAL-DSG/alex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Akinobu</author>
</authors>
<title>Open-Source Large Vocabulary CSR Engine Julius.</title>
<date>2014</date>
<note>http://julius.sourceforge. jp/en_index.php.</note>
<contexts>
<context position="1968" citStr="Akinobu, 2014" startWordPosition="278" endWordPosition="280">gnition2; however, again adjustments to the system are not possible. Moreover, it is a paid service. When considering local ASR systems, we found no viable alternatives either. The HTK toolkit does not provide on-line large vocabulary decoders suitable for real-time decoding. OpenJulius can be used with custom-built acoustic and 1The API is available at https://www.google. com/speech-api/v1/recognize, and its use described in a blog post at http://mikepultz.com/2013/07/ google-speech-api-full-duplex-php-version/. 2http://www.nuancemobiledeveloper.com/ language models and for on-line decoding (Akinobu, 2014). However, OpenJulius suffers from software instability when producing lattices and confusion networks; therefore, it is not suitable for practical use. The RWTH decoder is not a free software and a license must be purchased for commercial applications (Rybach et al., 2011). As a result, we implemented a lightweight modification of the LatticeFasterDecoder from the Kaldi toolkit and created an on-line recogniser with an interface that is suitable for statistical dialogue systems. The Kaldi toolkit as well as the online recogniser is distributed under the Apache 2.0 license3. Our on-line recogn</context>
</contexts>
<marker>Akinobu, 2014</marker>
<rawString>Lee Akinobu. 2014. Open-Source Large Vocabulary CSR Engine Julius. http://julius.sourceforge. jp/en_index.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matˇej Korvas</author>
<author>Ondˇrej Pl´atek</author>
<author>Ondˇrej Duˇsek</author>
<author>Luk´aˇs ˇZilka</author>
<author>Filip Jurˇc´ıˇcek</author>
</authors>
<title>Free English and Czech telephone speech corpus shared under the CC-BY-SA 3.0 license.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eigth International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>Korvas, Pl´atek, Duˇsek, ˇZilka, Jurˇc´ıˇcek, 2014</marker>
<rawString>Matˇej Korvas, Ondˇrej Pl´atek, Ondˇrej Duˇsek, Luk´aˇs ˇZilka, and Filip Jurˇc´ıˇcek. 2014. Free English and Czech telephone speech corpus shared under the CC-BY-SA 3.0 license. In Proceedings of the Eigth International Conference on Language Resources and Evaluation (LREC 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Morbini</author>
<author>Kartik Audhkhasi</author>
<author>Kenji Sagae</author>
<author>Ron Arstein</author>
<author>Doan Can</author>
<author>Panayiotis G Georgiou</author>
<author>Shrikanth S Narayanan</author>
<author>Anton Leuski</author>
<author>David Traum</author>
</authors>
<title>Which ASR should I choose for my dialogue system?</title>
<date>2013</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<contexts>
<context position="1115" citStr="Morbini et al., 2013" startWordPosition="158" endWordPosition="161">ing techniques. As the recogniser produces word posterior lattices, it is particularly useful in statistical dialogue systems, which try to exploit uncertainty in the recogniser’s output. Our experiments show that the online recogniser performs significantly better in terms of latency when compared to a cloud-based recogniser. 1 Introduction There are many choices of speech recognisers, but we find no alternative with both a permissive license and on-line recognition suitable for a spoken dialogue system. The Google speech recognition service1 provides state-of-the-art quality for many tasks (Morbini et al., 2013) and may be used for free; however, the licensing conditions are not clear, adaptation of acoustic and language models to a task at hand is not possible and the service is not officially supported. Another option is Nuance cloud based recognition2; however, again adjustments to the system are not possible. Moreover, it is a paid service. When considering local ASR systems, we found no viable alternatives either. The HTK toolkit does not provide on-line large vocabulary decoders suitable for real-time decoding. OpenJulius can be used with custom-built acoustic and 1The API is available at https</context>
</contexts>
<marker>Morbini, Audhkhasi, Sagae, Arstein, Can, Georgiou, Narayanan, Leuski, Traum, 2013</marker>
<rawString>Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Arstein, Doan Can, Panayiotis G. Georgiou, Shrikanth S. Narayanan, Anton Leuski, and David Traum. 2013. Which ASR should I choose for my dialogue system? In Proc. SIGDIAL, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Povey</author>
<author>Arnab Ghoshal</author>
<author>Gilles Boulianne</author>
<author>Lukas Burget</author>
<author>Ondrej Glembek</author>
<author>Nagendra Goel</author>
<author>Mirko Hannemann</author>
<author>Petr Motlicek</author>
<author>Yanmin Qian</author>
<author>Petr Schwarz</author>
</authors>
<title>The kaldi speech recognition toolkit.</title>
<date>2011</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="3414" citStr="Povey et al., 2011" startWordPosition="495" endWordPosition="498">(MPE). It produces word posterior lattices which can be easily converted into high quality n-best lists. The recogniser’s speed and latency can be effectively controlled off-line by optimising a language model and during decoding by beam thresholds. In the next section, the Kaldi recognition toolkit is briefly described. Section 3 describes the implementation of the OnlineLatgenRecogniser. Section 4 evaluates the accuracy and speed of the recogniser. Finally, Section 5 concludes this work. 2 The Kaldi toolkit The Kaldi toolkit4 is a speech recognition toolkit distributed under a free license (Povey et al., 2011). The toolkit is based on Finite State Transducers, implements state-of-the-art acoustic modelling techniques, is computationally efficient, and is already widely adapted among research groups. 3http://www.apache.org/licenses/ LICENSE-2.0 4http://sourceforge.net/projects/kaldi 108 Proceedings of the SIGDIAL 2014 Conference, pages 108–112, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics Its only major drawback was the lack of on-line recognition support. Therefore, it could not be used directly in applications such as spoken dialogue systems. Kaldi includ</context>
<context position="9779" citStr="Povey et al., 2011" startWordPosition="1452" endWordPosition="1455">is given in Korvas et al. (2014). 4.2 Experiments We focus on evaluating the speed of the OnlineLatgenRecogniser and its relationship with the accuracy of the decoder, namely: • Real Time Factor (RTF) of decoding – the ratio of the recognition time to the duration of the audio input, • Latency – the delay between utterance end and the availability of the recognition results, • Word Error Rate (WER). Accuracy and speed of the OnlineLatgenRecogniser are controlled by the max-active-states, 6http://sourceforge.net/p/kaldi/code/ HEAD/tree/trunk/egs/vystadial_en/ beam, and lattice-beam parameters (Povey et al., 2011). Max-active-states limits the maximum number of active tokens during decoding. Beam is used during graph search to prune ASR hypotheses at the state level. Lattice-beam is used when producing word level lattices after the decoding is finished. It is crucial to tune these parameters optimally to obtain good results. In general, one aims for a setting RTF smaller than 1.0. However, in practice, it is useful if the RTF is even smaller because other processes running on the machine can influence the amount of available computational resources. Therefore, we target the RTF of 0.6 in our setup. We </context>
</contexts>
<marker>Povey, Ghoshal, Boulianne, Burget, Glembek, Goel, Hannemann, Motlicek, Qian, Schwarz, 2011</marker>
<rawString>Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al. 2011. The kaldi speech recognition toolkit. In Proc. ASRU, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Povey</author>
<author>Mirko Hannemann</author>
<author>Gilles Boulianne</author>
<author>Lukas Burget</author>
<author>Arnab Ghoshal</author>
<author>Milos Janda</author>
<author>Martin Karafi´at</author>
<author>Stefan Kombrink</author>
<author>Petr Motlicek</author>
<author>Yanmin Qian</author>
</authors>
<date>2012</date>
<booktitle>Generating exact lattices in the WFST framework. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on,</booktitle>
<pages>4213--4216</pages>
<publisher>IEEE.</publisher>
<marker>Povey, Hannemann, Boulianne, Burget, Ghoshal, Janda, Karafi´at, Kombrink, Motlicek, Qian, 2012</marker>
<rawString>Daniel Povey, Mirko Hannemann, Gilles Boulianne, Lukas Burget, Arnab Ghoshal, Milos Janda, Martin Karafi´at, Stefan Kombrink, Petr Motlicek, Yanmin Qian, et al. 2012. Generating exact lattices in the WFST framework. In Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, pages 4213– 4216. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rybach</author>
<author>Stefan Hahn</author>
<author>Patrick Lehnen</author>
<author>David Nolden</author>
<author>Martin Sundermeyer</author>
<author>Zoltan T¨uske</author>
<author>Siemon Wiesler</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>RASR-The RWTH Aachen University Open Source Speech Recognition Toolkit.</title>
<date>2011</date>
<booktitle>In Proc. IEEEAutomatic Speech Recognition and Understanding Workshop.</booktitle>
<marker>Rybach, Hahn, Lehnen, Nolden, Sundermeyer, T¨uske, Wiesler, Schl¨uter, Ney, 2011</marker>
<rawString>David Rybach, Stefan Hahn, Patrick Lehnen, David Nolden, Martin Sundermeyer, Zoltan T¨uske, Siemon Wiesler, Ralf Schl¨uter, and Hermann Ney. 2011. RASR-The RWTH Aachen University Open Source Speech Recognition Toolkit. In Proc. IEEEAutomatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>David Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>745--753</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10993" citStr="Skantze and Schlangen, 2009" startWordPosition="1671" endWordPosition="1674">our setup. We used grid search on the development set to identify optimal parameters. Figure 1 (a) shows the impact of the beam on the WER and RTF measures. In this case, we set max-active-states to 2000 in order to limit the worst case RTF to 0.6. Observing Figure 1 (a), we set beam to 13 as this setting balances the WER, 95th RTF percentile, and the average RTF. Figure 1 (b) shows the impact of the lattice-beam on WER and latency when beam is fixed to 13. We set latticebeam to 5 based on Figure 1 (b) to obtain the 95th latency percentile of 200 ms, which is considered natural in a dialogue (Skantze and Schlangen, 2009). Lattice-beam does not affect WER, but larger lattice-beam improves the oracle WER of generated lattices (Povey et al., 2012). Figure 2 shows the percentile graph of the RTF and latency measures over the development set. For example, the 95th percentile is the value of a measure such that 95% of the data has the measure below that value. One can see from Figure 2 that 95% of development utterances is decoded with RTF under 0.6 and latency under 200 ms. The extreme values are typically caused by decoding long noisy utterances where uncertainty in decoding slows down the recogniser. Using this </context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Gabriel Skantze and David Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 745–753. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>UFAL-DSG</author>
</authors>
<title>The Alex Dialogue Systems Framework - Public Transport Information.</title>
<date>2014</date>
<note>https://github. com/UFAL-DSG/alex.</note>
<contexts>
<context position="13928" citStr="UFAL-DSG, 2014" startWordPosition="2184" endWordPosition="2185">atency is presumably caused by the batch processing of audio data and network latency, and the high WER is likely caused by a mismatch between Google’s acoustic and language models and the test data. 5 Conclusion This work presented the OnlineLatgenRecogniser, an extension of the Kaldi automatic speech recognition toolkit. The OnlineLatgenRecogniser is distributed under the Apache 2.0 license, and therefore it is freely available for both research and commercial applications. The recogniser and its Python extension is stable and intensively used in a publicly available spoken dialogue system (UFAL-DSG, 2014). Thanks to the use of a standard Kaldi lattice decoder, the recogniser produces high quality word posterior lattices. The training scripts for the acoustic model and the OnlineLatgenRecogniser code are currently being integrated in the Kaldi toolkit. Future planned improvements include implementing more sophisticated speech parameterisation interface and feature transformations. Acknowledgments We would also like to thank Daniel Povey and Ondˇrej Dusˇek for their useful comments and discussions. We also thank the anonymous reviewers for their helpful comments and suggestions. This research wa</context>
</contexts>
<marker>UFAL-DSG, 2014</marker>
<rawString>UFAL-DSG. 2014. The Alex Dialogue Systems Framework - Public Transport Information. https://github. com/UFAL-DSG/alex.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.951009">
Extended HMM and Ranking models for Chinese Spelling Correction
</title>
<author confidence="0.9844985">
Jinhua Xiong, Qiao Zhao, Jianpeng Hou,
Qianbo Wang, Yuanzhuo Wang and Xueqi Cheng
</author>
<affiliation confidence="0.998273333333333">
CAS Key Laboratory of Network Data Science and Technology
Institute of Computing Technology, Chinese Academy of Sciences
University of Chinese Academy of Sciences
</affiliation>
<email confidence="0.99811">
xjh@ic.ac.cn, zhangqiao@software.ict.ac.cn
</email>
<sectionHeader confidence="0.995635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947052631579">
Spelling correction has been studied for
many decades, which can be classified
into two categories: (1) regular text
spelling correction, (2) query spelling
correction. Although the two tasks share
many common techniques, they have dif-
ferent concerns. This paper presents our
work on the CLP-2014 bake-off. The task
focuses on spelling checking on foreigner
Chinese essays. Compared to online
search query spelling checking task,
more complicated techniques can be ap-
plied for better performance. Therefore,
we proposed a unified framework for
Chinese essays spelling correction based
on extended HMM and ranker-based
models, together with a rule-based model
for further polishing. Our system showed
better performance on the test dataset.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969769230769">
The number of people learning Chinese as a For-
eign Language (CFL) is booming in recent dec-
ades, and the number is expected to become even
larger for the years to come1. Therefore spelling
correction tool to support such learners to correct
and polish their essays becomes very valuable.
Spelling correction has been studied for many
years on regular text and web search query. Alt-
hough the two tasks share many common tech-
niques, they have different concerns. Compared
to web search query spelling correction which
normally need to give corrections instantly,
complicated techniques can be applied to
</bodyText>
<footnote confidence="0.9984095">
1http://www.cipsc.org.cn/clp2014/webpage/en/four_bakeoff
s/Bakeoff2014cfp ChtSpellingCheck en.htm
</footnote>
<bodyText confidence="0.9002745">
spelling correction on essays, in order to improve
the performance. Spelling correction on Chinese
essays of CFL learners faces the following chal-
lenges:
</bodyText>
<listItem confidence="0.898721727272727">
(1) There is no word boundary between Chinese
word, which may result in the error on splitting,
and the error may accumulate.
(2) The number of error type is more than other
case, because CFL learners are prone to different
kinds of error which we can not imagine as a na-
tive speaker. Meanwhile, more errors can be
caused by various Chinese input methods. As
illustrated in Table 1, some errors can be found
only in the essays of CFL learners, e.g. the 3rd
and the last errors.
</listItem>
<table confidence="0.99409">
Error Types Misspelled Corrections
Homophone 聯合國公布 聯合國公佈
Near-homophone 好碼差不多 號碼差不多
一樣 一樣
Similar-shape 列如:家庭 例如:家庭
會變冷漠 會變冷漠
Others errors 每個禮拜每個 禮拜
1、3、5 一、三、五
受了都少苦 受了多少苦
</table>
<tableCaption confidence="0.999564">
Table 1. Examples of spelling error
</tableCaption>
<listItem confidence="0.717428333333333">
(3) Chinese language is continuously evolving,
for example, traditional Chinese and simple Chi-
nese may have different choices for the same
word. In some cases, it is very difficult to distin-
guish them. Therefore, online high quality corpus
is needed for decision-making.
</listItem>
<bodyText confidence="0.9993685">
To address the above challenges, we present a
unified framework, named HANSpeller, to com-
bine different methods for Chinese essays
spelling detecting and correction. The contribu-
</bodyText>
<page confidence="0.987529">
133
</page>
<note confidence="0.6531505">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 133–138,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.9999252">
tion of our approach is as follows: (1) A HMM-
based approach is used to segment sentences and
generate candidates for sentences spelling cor-
rection. (2) Under this framework, all kinds of
error types can be easily integrated for candi-
dates generating. We collected some error types
which only may be found in CFL learner essays,
and add them into candidates generating process.
And then ranking-based approach is used for
choosing candidates for correction. (3) In order
to address the evolving feature of Chinese, we
not only collect high quality Taiwan web pages
and also use search engine results to help deci-
sion-making on candidates.
The rest of the paper is organized as follow. In
Section 2, we introduce related work on spelling
checking. Then our unified framework approach
is discussed in detail in Section 3. Section 4 pre-
sents the detailed experiment on the task. Section
5 concludes the paper and discusses future work.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999937940476191">
Chinese essays spelling correction as a special
kind of spelling correction research effort has
been promoted by efforts such as the SIGHAN
bake-offs (Wu et al., 2013).
Spelling correction was first proposed for
English (Peterson, 1980). And it can be mainly
divided into single word and context-sensitive
spelling correction technology.
For the single word spelling error, it common-
ly uses dictionary-based method. It matches the
original word with all the words in dictionaries to
determine whether the word has spelling errors.
For the context-sensitive spelling errors, there
are two major kinds of processing methods:
Rule-based methods and Statistics-based meth-
ods. Rule-based methods use some rules generat-
ed fromrelevant grammars, the collocation of
words, syntactic knowledge, etc, for spelling cor-
rection. Mangu and Bill (1997) proposed a tran-
sition-based learning method for spelling correc-
tion. Their methods generated three types of
rules from training data, which constructs a high
performance and concise system for English. A
statistics-based method first finds related candi-
dates, and then ranks the candidates based on the
statistical model. Atwell and Elliott (1987) used
n-gram and part-of-speech language models for
spelling correction. Cucerzan and Brill (2004)
presented an iterative process for query spelling
check, using a query log and trust dictionary.
And the noisy channel mode is used to select the
best correction. Ahmad and Kondrak (2005) also
learned a spelling error model from search query
logs to improve the quality of query spelling
check. Li et al. (2006) applied distributional sim-
ilarity based models for query spelling correction.
Gao et al. (2010) presented a large scale ranker-
based system for search query spelling correction,
the ranker uses web scale language models and
many kinds of features for better performance,
including: surface-form similarity, phonetic-form
similarity, entity, dictionary, and frequency fea-
tures. Microsoft (2010) provides Microsoft web
n-gram services. Google (2010) has developed a
Java API for Google spelling check service.
As for Chinese spelling correction, an early
work was by (Chang, 1995), which used a char-
acter dictionary of similar shape, pronunciation,
meaning, and input-method-code to deal with the
spelling correction task. The system replaced
each character in the sentence with the similar
character in dictionary and calculated the proba-
bility of all modified sentences based on lan-
guage model.
Zhang et al. (2000) introduced a method that
can handle not only Chinese character substitu-
tion, but also insertion and deletion errors. They
distinguished the way of matching between the
Chinese and English, thus largely improved the
performance over the work of (Chang 1995).
Huang et al. (2007) used a word segmentation
tool (CKIP) to generate correction candidates,
and then to detect Chinese spelling errors.
Hung et al. (2008) introduced a method which
used the manually edited error templates to cor-
rect errors.
Zheng et al. (2011) found the fact that when
people typed Chinese Pinyins, there are several
wrong types. Then they introduced a method
based on a generative model and the typed wrong
types to correct spelling errors.
Liu et al. (2011) pointed out visually and pho-
nologically similar characters are major factors
for errors in Chinese text. And by defining ap-
propriate similarity measures that consider ex-
tended Cangjie codes, visually similar characters
can be quickly identified.
Note that all spelling correction methods re-
quire lexicons and/or language corpora. And
Chinese essays spelling correction has some dif-
ferent concerns with query spelling correction. In
our approach, we adopt the method based on sta-
tistics combining with lexicon and rule-based
methods.
</bodyText>
<page confidence="0.997393">
134
</page>
<sectionHeader confidence="0.934005" genericHeader="method">
3 The Unified Framework for Chinese
</sectionHeader>
<subsectionHeader confidence="0.913657">
Spelling Correction
</subsectionHeader>
<bodyText confidence="0.99994">
In this section we present a unified framework,
named HANSpeller, for Chinese spelling correc-
tion based on extended HMM and ranking mod-
els. The major idea of our approach is to model
the spelling correction process as a ranking and
decision-making problem. Generally speaking,
our approach has four major steps: Firstly the
spelling correction process generates lots of can-
didates for sentences being checked; and then a
ranking algorithm is applied to rank top-k correc-
tion candidates for later decision; the third step
conducts rule-based analysis for specific correc-
tion task, e.g. the correction rule of the usage of
three confusable words “的”, “地” and “得”. Fi-
nally, the system makes decision whether to out-
put the correction or not based on the previous
output and global constrains.
The system architecture is illustrated in Figure
1. This framework provides a unified approach
for spelling correction tasks, which can tailored
to different scenarios and can be regarded as a
language independent framework. To move to
another language scenario, you only need to col-
lect some language related corpus, but you don’t
need to be a language expert.
</bodyText>
<figure confidence="0.507277">
HMM-based Correction
Candidates Generating
</figure>
<figureCaption confidence="0.4661995">
Figure1. The Unified Framework (HANSpeller)
for Chinese Spelling Correction
</figureCaption>
<subsectionHeader confidence="0.995611">
3.1 Generating Candidates
</subsectionHeader>
<bodyText confidence="0.998405428571428">
Generating candidates of spelling correction task
is the basic part for the whole task, because it
determines the upper bound of precision and re-
call rate of the approach. The spelling correction
problem can be typically formulated under the
framework of noisy channel model. According to
such a model, the spelling correction task is to
find the correction with the highest probability of
yielding the misspelled input sentence. Formally,
given an “observed” sentence S which might
contain error characters, we need to find the cor-
rected sentence C with the highest probability of
different replacement C. Symbolically, it is rep-
resented by:
</bodyText>
<equation confidence="0.90113475">
(1)
By applying Bayes’ Rule, we can rewrite
Formula 1 as:
arg max (p(SIC)p(C) (2)
</equation>
<bodyText confidence="0.9999791">
where p (SIC) , called the “error model”, repre-
sents the chance that a correct Chinese character
could be written to the wrong one, while p(C) is
the n-gram language model which evaluates the
quality of the corrected Chinese sentence.
To solve the above problem, the HMM ap-
proach can be used. And the spelling correction
can then be ranked by multiplying the error mod-
el and language model.
The above one step method for Chinese essays
spelling correction faces the following challeng-
es: (1) For high quality spelling correction, the
training of HMM is not a trivial task. (2) The
long-span dependency in sentences makes first-
order hidden Markov model not enough to catch
context information. (3) Too many candidates
make the algorithm not efficient enough, and
right corrections may be concealed by the wrong
corrections.
To address the above issues, some extensions
have been made on HMM-based spelling correc-
tion approach. Firstly, the HMM-based method is
used only for candidates generating, not for final-
ly output correction generating. And all kinds of
possible error transformations can be integrated
into the framework of HMM approach, so as to
get high recall rate. Secondly, higher-order hid-
den Markov model is used to capture long-span
context dependency. Thirdly, a pruning dynamic
programming algorithm is adopted to dynamical-
</bodyText>
<figure confidence="0.994031454545455">
Preprocess
Input sentence
Global Decision-Making
for Correction
Output Correction
Result
Syntactic Rule based
Correction
Candidates Re-Ranking
Other Rules for
Exception
</figure>
<page confidence="0.989293">
135
</page>
<bodyText confidence="0.9920995">
ly select the best correction candidates for each
round of sentence segmentation and correction.
</bodyText>
<subsectionHeader confidence="0.999253">
3.2 Ranking Candidates
</subsectionHeader>
<bodyText confidence="0.998614055555556">
In the candidates generation phase, top-k best
candidates for a sentence are generated, but the
HMM-based framework does not have the flexi-
bility to incorporate a wide variety of features
useful for spelling correction, such as the online
search results and CKIP Parser results, which can
significantly improve the precision of spelling
correction.
Given the original sentence, our system first
creates a list of candidate sentences. The candi-
dates in the list will be re-ranked at this stage
based on the confidence score generated by a
ranker, herein by a SVM classifier. We choose
the top-2 candidates in the re-ranked candidate
list to make the final decision.
We use a lot of features in the re-ranking
phase. The features can be grouped into the fol-
lowing categories:
</bodyText>
<listItem confidence="0.873879214285714">
1) Language model features, which calcu-
late the n-gram probability of a candidate
sentence.
2) Dictionary features, which check wheth-
er parts of a candidate sentence can match
to one or more words or idioms in the
dictionaries.
3) Edit Distance features, which compute
the edit number and its weight, from the
original sentence to the candidate sentence.
4) Segmentation features, which use the re-
sults of the maximum matching segmenta-
tion algorithm and that of CKIP Parser
segmentation.
</listItem>
<bodyText confidence="0.8893106">
5) Online Resources features, which use
Bing or other search engine’s search re-
sults, when submitting the spelling correc-
tion part and the corresponding part of the
original sentence to the search engine.
</bodyText>
<subsectionHeader confidence="0.995891">
3.3 Rule-based Correction for Errors
</subsectionHeader>
<bodyText confidence="0.968705125">
As illustrated in Figure 1, the third step conducts
rule-based analysis for specific correction task.
One of most common errors is the usage of three
confusable words “的”, “地” and “得”. To cor-
rect such common errors, syntactic analysis is
needed. For other errors, some other specific
rules can be developed for them.
The following sentence contains an error of
Chinese syntax:
今天/我/穿著/剛/買/地/新/衣服。
Here the character “地” should be corrected to
another character “的”. To deal with such kind of
errors, sentence parsing must be done before the
syntactic rules are applied to check and correct
such errors. We have summarized three rules
according to Chinese grammar as follows:
</bodyText>
<listItem confidence="0.8959413">
1) The Chinese character “的” is the tag of
attributes, generally used in front of sub-
jects and objects. Words in front of “的”
are generally used to modify, restrict
things behind “的”.
2) The Chinese character “地” is adverbial
marker, usually used in front of predi-
cates(verbs, adjectives). Words in front of
“地” are generally used to describe actions
behind “地”.
</listItem>
<bodyText confidence="0.77837175">
3) The Chinese character “ 得 ” marks the
complement, generally used behind predi-
cates. The part follows “得” is generally
used to supplement the previous action.
</bodyText>
<subsectionHeader confidence="0.997678">
3.4 Decision-making on Corrections
</subsectionHeader>
<bodyText confidence="0.9999629">
Through the above processing steps, top candi-
dates for each sub-sentence have been generated.
To make the final decision on spelling correction,
global constrains should be considered, including
the whole error rate of the corpus, which error
type should be paid more weight than others,
which sub-sentence corrects should be output,
etc. Combining the above constrains together, the
system determines the final decision for spelling
corrections.
</bodyText>
<sectionHeader confidence="0.981406" genericHeader="method">
4 Experiment and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.989699">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.999853833333333">
The following corpora are used to train our mod-
el, including Taiwan Web as Corpus, SogouW
dictionary, a traditional Chinese dictionary of
words and idioms, a pinyin mapping table and a
cangjie code table of common words. The details
of them are described below.
</bodyText>
<sectionHeader confidence="0.28193" genericHeader="method">
1) Taiwan Web Pages as Corpus
</sectionHeader>
<bodyText confidence="0.999986285714286">
As we known, Taiwan web pages contain high
quality traditional Chinese text, so we gathered
pages from the Web under .tw domain to build
the corpus, containing around 3.2 million web
pages. And then the content extracted from these
pages is used to build traditional Chinese n-gram
model, where n is from 2 to 4.
</bodyText>
<sectionHeader confidence="0.456722" genericHeader="method">
2) SogouW Dictionary
</sectionHeader>
<page confidence="0.998163">
136
</page>
<bodyText confidence="0.997330142857143">
SougoW dictionary2 is built from the statistical
analysis of Chinese Internet corpus by Sogou
Search Engine. It contains about 150,000 high-
frequency words of the Chinese Internet. But
words in the corpus are simple Chinese charac-
ters; it is then translated into traditional Chinese
by Google translating service.
</bodyText>
<subsectionHeader confidence="0.36222">
3) Chinese Words and Idioms Dictionary
</subsectionHeader>
<bodyText confidence="0.99971225">
As introduced in [Chiu et al. 2013], we also ob-
tained the Chinese words 3and Chinese idioms4
published by Ministry of Education of Taiwan,
which are built from the dictionaries and related
books. There are 64,326 distinct Chinese words
and 48,030 distinct Chinese idioms. And we
combine these two dictionaries with SogouW
dictionary to build our trie tree dictionary.
</bodyText>
<listItem confidence="0.556054">
4) Pinyin and Canjie Code Tables
</listItem>
<bodyText confidence="0.999966625">
We collected more than 10000 pinyins of words
commonly used in Taiwan to build the homo-
phone and near-homophone words table, which
will be used in candidate generation phase. In
addition, cangjie code can be used to measure the
form/shape similarity between Chinese charac-
ters. Therefore, we collected cangjie codes to
build the table of Similar-form characters.
</bodyText>
<sectionHeader confidence="0.622432" genericHeader="method">
5) Segmentation Resources
</sectionHeader>
<bodyText confidence="0.9981295">
Besides using the Maximum Matching Method
for Chinese word segmentation, we also use the
CKIP Parser results to help ranking the candi-
dates. For example, the segmentation of “特續下
滑” is “特/續/下滑” while “持續下滑” is “持續/
下滑”. Thus the segmentation results of wrong
candidate sentence will have more words than
the correct one.
date sentence, because the first one is more
popular than the second one on the web corpus.
</bodyText>
<subsectionHeader confidence="0.997856">
4.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.9999529">
At the CLP-2014 bake-off, the evaluation task is
to correct errors in sentences. It is divided into
two related subtasks. One is error detection and
the other is error correction. There are 1062 sen-
tences with/without spelling errors. The evalua-
tion metrics, including false positive rate, accu-
racy rate, precision rate, recall rate and F1-score,
is provided by the Chinese Spelling Check Task
group. The confusion matrix as follow is to help
to calculate the related indicators.
</bodyText>
<table confidence="0.998408">
Confusion Matrix System Results
Positive Negative
(Error) (No Error)
Gold Positive TF FN
Standard
Negative FP TN
</table>
<tableCaption confidence="0.999361">
Table 2. Confusion Matrix
</tableCaption>
<bodyText confidence="0.7946025">
Each index calculation is as follows:
False Positive Rate (FPR) = FP / (FP + TN)
</bodyText>
<equation confidence="0.9779144">
Accuracy (A) =
(TP + TN) / (TP + TN + FP + FN)
Precision (P) = TP / (TP + FP)
Recall (R) = TP / (TP + FN)
F1-score = 2 * P * R / (P + R)
</equation>
<bodyText confidence="0.999865571428571">
Our system showed good performance on the
evaluation test. Among all 13 teams, our perfor-
mance ranks second place. The two submitted
test results are illustrated in Table 3. Meanwhile,
since such an open test is an extremely challeng-
ing task, there is still much room for further im-
provement.
</bodyText>
<sectionHeader confidence="0.991245" genericHeader="method">
6) Online Resource
</sectionHeader>
<bodyText confidence="0.9999361">
In addition to the above, we use the Bing search
results as one feature in candidates ranking phase,
which improve the performance obviously. For
example, the sentence “根據聯合國公布的數字”
has several candidate sentences, one of them may
be “根據聯合國公佈的數字”. If we use Bing to
search the error correction part and the corre-
sponding part of the original sentence “聯合國公
佈” and “聯合國公布”, the search results will be
obviously enough to identify the correct candi-
</bodyText>
<table confidence="0.998694625">
RUN1 RUN2
Detection Correction Detection Correction
Level Level Level Level
FPR 0.1525 0.1563
A 0.6149 0.5829 0.613 0.581
P 0.7148 0.676 0.7098 0.6706
R 0.3823 0.3183 0.3823 0.3183
F1 0.4982 0.4328 0.4969 0.4317
</table>
<tableCaption confidence="0.999687">
Table 3. Evaluation at CLP-2014 Bake-off
</tableCaption>
<sectionHeader confidence="0.903138" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<footnote confidence="0.9911115">
2 http://www.sogou.com/labs/dl/w.html
3http://www.edu.tw/files/site_content/m0001/pin/yu7.htm?o
pen
4 http://dict.idioms.moe.edu.tw/cydic/index.htm
</footnote>
<bodyText confidence="0.928794333333333">
This paper proposed a unified framework
(HANSpeller) for Chinese essays spelling cor-
rection based on extended HMM and ranker-
</bodyText>
<page confidence="0.993435">
137
</page>
<bodyText confidence="0.999955846153846">
based models. The rule-based strategy is used for
further correction polishing, and for final deci-
sion on whether outputs the correction or not.
Our approach has been evaluated at CLP-2014
bake-off on Chinese spelling correction task, and
made good performance with ranking second
among 13 teams.
Some interesting future works on Chinese
spelling correction include: (1) collecting and
considering more error types in the candidates
generating process, (2) how to better dealing
with the difference between traditional and sim-
ple Chinese.
</bodyText>
<sectionHeader confidence="0.99748" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.953048333333333">
This work was supported by the National Basic
Research Program of China (Grant No.
2014CB340406), the National High Technology
Research and Development Program of China
(Grant No. 2014AA015204) and the NSFC for
the Youth (Grant No. 61402442).
</bodyText>
<sectionHeader confidence="0.962296" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999937179104478">
Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee
(2013). Chinese Spelling Check Evaluation at
SIGHAN Bake-off 2013. Proceedings of the 7th
SIGHAN Workshop on Chinese Language Pro-
cessing (SIGHAN’13), Nagoya, Japan, 14 October,
2013, pp. 35-42.
Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, Yi-
Hsuan Chuang, Shih-Hung Wu, and Chia-Ying Lee.
2011. Visually and Phonologically similar charac-
ters in incorrect Chinese Words: analyses, Identifi-
cation, and Applications. ACM Transactions on
Asian Language Information Processing, 10(2),
10:1-39.
James L. Pterson. 1980. Computers programs for de-
tecting and correcting spelling errors. Communica-
tions of the ACM, pp 23: 676-687
Lidia Mangu and Eric Bill. 1997. Automatic rule ac-
quisition for spelling correction. In Proceeding of
the 14th International Conference on Machine
Learning, pp 187-194, San Francisco, CA
Eric Atwell and Stephen Elliott. 1987. Dealing with
ill-formed English text. In the Computational
Analysis of English: A Corpus-Based Approach,
London.
Ahmad, F., and Kondrak, G. 2005. Learning a
spelling error model from search query logs. In
HLT_EMNLP, pp 955-962.
Li, M., Zhu, M., Zhang, Y. and Zhou, M. 2006. Ex-
ploring distributional similarity based models for
query spelling correction. Proceedings of ACL
2006, pp 2025-1032.
Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk,
and Xu Sun. 2010. A large scale ranker-based sys-
tem for search query spelling correction. The 23rd
International Conference on Computational Lin-
guistics 2010 (COLING 2010). pp 358-366.
Microsoft Microsoft web n-gram services. 2010.
http://research.microsoft.com/web-ngram
Google. 2010. A Java API for Google spelling check
service. http//code.google.com/p/google-api-
spellingjava/
Chang-Huang Chang. 1995. A new approach for au-
tomatic Chinese spelling correction, Proceedings of
Natural Language Processing Pacific Rim Sympo-
sium. pp 278-283
Lei Zhang, Ming Zhou, Chang-Ning Huang, and Hui-
Hua Pan. 2000b. Automatic detecting/correcting
errors in Chinese text by an approximate word-
matching algorithm. In Proceedings of the 38the
Annual Meeting on Association for Computational
Linguistics, pp 248-254, Morristwon NJ.
S. Cucerzan and E. Brill. 2004. Spelling correction as
an iterative process that exploits the collective
knowledge of web users. In Proceedings of
EMNLP, volume 4, pp 293-300.
Huang, Chuen-Min and Wu, Mei-Chen and Chang,
Ching-Che. 2007, Error detection and correction
based on Chinese phonemic alphabet in Chinese
text. Modeling Decisions for Artificial Intelligence.
pp. 463-476.
Hung, Ta-Hung and Wu, Shih-Hung. 2008. Chinese
Essay Error Detection and Suggestion System.
Taiwan E-Learning Forum.
Y. Zheng, C. Li, and M. Sun. 2011. Chime: An effi-
cient error-tolerant Chinese pinyin input method. In
Proceedings of the 22nd International Joint Confer-
ence on Artificial Intelligence.
</reference>
<page confidence="0.997347">
138
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649318">
<title confidence="0.922555">Extended HMM and Ranking models for Chinese Spelling Correction</title>
<author confidence="0.9483645">Jinhua Xiong</author>
<author confidence="0.9483645">Qiao Zhao</author>
<author confidence="0.9483645">Jianpeng Qianbo Wang</author>
<author confidence="0.9483645">Yuanzhuo Wang</author>
<author confidence="0.9483645">Xueqi</author>
<affiliation confidence="0.996758666666667">CAS Key Laboratory of Network Data Science and Institute of Computing Technology, Chinese Academy of University of Chinese Academy of Sciences</affiliation>
<email confidence="0.771388">xjh@ic.ac.cn,zhangqiao@software.ict.ac.cn</email>
<abstract confidence="0.9991681">Spelling correction has been studied for many decades, which can be classified into two categories: (1) regular text spelling correction, (2) query spelling correction. Although the two tasks share many common techniques, they have different concerns. This paper presents our work on the CLP-2014 bake-off. The task focuses on spelling checking on foreigner Chinese essays. Compared to online search query spelling checking task, more complicated techniques can be applied for better performance. Therefore, we proposed a unified framework for Chinese essays spelling correction based on extended HMM and ranker-based models, together with a rule-based model for further polishing. Our system showed better performance on the test dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shih-Hung Wu</author>
<author>Chao-Lin Liu</author>
<author>Lung-Hao Lee</author>
</authors>
<title>Chinese Spelling Check Evaluation at SIGHAN Bake-off</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13),</booktitle>
<pages>35--42</pages>
<location>Nagoya,</location>
<contexts>
<context position="4358" citStr="Wu et al., 2013" startWordPosition="671" endWordPosition="674">ure of Chinese, we not only collect high quality Taiwan web pages and also use search engine results to help decision-making on candidates. The rest of the paper is organized as follow. In Section 2, we introduce related work on spelling checking. Then our unified framework approach is discussed in detail in Section 3. Section 4 presents the detailed experiment on the task. Section 5 concludes the paper and discusses future work. 2 Related work Chinese essays spelling correction as a special kind of spelling correction research effort has been promoted by efforts such as the SIGHAN bake-offs (Wu et al., 2013). Spelling correction was first proposed for English (Peterson, 1980). And it can be mainly divided into single word and context-sensitive spelling correction technology. For the single word spelling error, it commonly uses dictionary-based method. It matches the original word with all the words in dictionaries to determine whether the word has spelling errors. For the context-sensitive spelling errors, there are two major kinds of processing methods: Rule-based methods and Statistics-based methods. Rule-based methods use some rules generated fromrelevant grammars, the collocation of words, sy</context>
</contexts>
<marker>Wu, Liu, Lee, 2013</marker>
<rawString>Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee (2013). Chinese Spelling Check Evaluation at SIGHAN Bake-off 2013. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN’13), Nagoya, Japan, 14 October, 2013, pp. 35-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-Lin Liu</author>
<author>Min-Hua Lai</author>
<author>Kan-Wen Tien</author>
<author>YiHsuan Chuang</author>
<author>Shih-Hung Wu</author>
<author>Chia-Ying Lee</author>
</authors>
<title>Visually and Phonologically similar characters in incorrect Chinese Words: analyses, Identification, and Applications.</title>
<date>2011</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>10</volume>
<issue>2</issue>
<pages>10--1</pages>
<contexts>
<context position="7435" citStr="Liu et al. (2011)" startWordPosition="1141" endWordPosition="1144">They distinguished the way of matching between the Chinese and English, thus largely improved the performance over the work of (Chang 1995). Huang et al. (2007) used a word segmentation tool (CKIP) to generate correction candidates, and then to detect Chinese spelling errors. Hung et al. (2008) introduced a method which used the manually edited error templates to correct errors. Zheng et al. (2011) found the fact that when people typed Chinese Pinyins, there are several wrong types. Then they introduced a method based on a generative model and the typed wrong types to correct spelling errors. Liu et al. (2011) pointed out visually and phonologically similar characters are major factors for errors in Chinese text. And by defining appropriate similarity measures that consider extended Cangjie codes, visually similar characters can be quickly identified. Note that all spelling correction methods require lexicons and/or language corpora. And Chinese essays spelling correction has some different concerns with query spelling correction. In our approach, we adopt the method based on statistics combining with lexicon and rule-based methods. 134 3 The Unified Framework for Chinese Spelling Correction In thi</context>
</contexts>
<marker>Liu, Lai, Tien, Chuang, Wu, Lee, 2011</marker>
<rawString>Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, YiHsuan Chuang, Shih-Hung Wu, and Chia-Ying Lee. 2011. Visually and Phonologically similar characters in incorrect Chinese Words: analyses, Identification, and Applications. ACM Transactions on Asian Language Information Processing, 10(2), 10:1-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L Pterson</author>
</authors>
<title>Computers programs for detecting and correcting spelling errors.</title>
<date>1980</date>
<journal>Communications of the ACM, pp</journal>
<volume>23</volume>
<pages>676--687</pages>
<marker>Pterson, 1980</marker>
<rawString>James L. Pterson. 1980. Computers programs for detecting and correcting spelling errors. Communications of the ACM, pp 23: 676-687</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Bill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proceeding of the 14th International Conference on Machine Learning,</booktitle>
<pages>187--194</pages>
<location>San Francisco, CA</location>
<contexts>
<context position="5028" citStr="Mangu and Bill (1997)" startWordPosition="768" endWordPosition="771">ish (Peterson, 1980). And it can be mainly divided into single word and context-sensitive spelling correction technology. For the single word spelling error, it commonly uses dictionary-based method. It matches the original word with all the words in dictionaries to determine whether the word has spelling errors. For the context-sensitive spelling errors, there are two major kinds of processing methods: Rule-based methods and Statistics-based methods. Rule-based methods use some rules generated fromrelevant grammars, the collocation of words, syntactic knowledge, etc, for spelling correction. Mangu and Bill (1997) proposed a transition-based learning method for spelling correction. Their methods generated three types of rules from training data, which constructs a high performance and concise system for English. A statistics-based method first finds related candidates, and then ranks the candidates based on the statistical model. Atwell and Elliott (1987) used n-gram and part-of-speech language models for spelling correction. Cucerzan and Brill (2004) presented an iterative process for query spelling check, using a query log and trust dictionary. And the noisy channel mode is used to select the best co</context>
</contexts>
<marker>Mangu, Bill, 1997</marker>
<rawString>Lidia Mangu and Eric Bill. 1997. Automatic rule acquisition for spelling correction. In Proceeding of the 14th International Conference on Machine Learning, pp 187-194, San Francisco, CA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Atwell</author>
<author>Stephen Elliott</author>
</authors>
<title>Dealing with ill-formed English text.</title>
<date>1987</date>
<booktitle>In the Computational Analysis of English: A Corpus-Based Approach,</booktitle>
<location>London.</location>
<contexts>
<context position="5376" citStr="Atwell and Elliott (1987)" startWordPosition="820" endWordPosition="823">lling errors, there are two major kinds of processing methods: Rule-based methods and Statistics-based methods. Rule-based methods use some rules generated fromrelevant grammars, the collocation of words, syntactic knowledge, etc, for spelling correction. Mangu and Bill (1997) proposed a transition-based learning method for spelling correction. Their methods generated three types of rules from training data, which constructs a high performance and concise system for English. A statistics-based method first finds related candidates, and then ranks the candidates based on the statistical model. Atwell and Elliott (1987) used n-gram and part-of-speech language models for spelling correction. Cucerzan and Brill (2004) presented an iterative process for query spelling check, using a query log and trust dictionary. And the noisy channel mode is used to select the best correction. Ahmad and Kondrak (2005) also learned a spelling error model from search query logs to improve the quality of query spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker u</context>
</contexts>
<marker>Atwell, Elliott, 1987</marker>
<rawString>Eric Atwell and Stephen Elliott. 1987. Dealing with ill-formed English text. In the Computational Analysis of English: A Corpus-Based Approach, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ahmad</author>
<author>G Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs. In HLT_EMNLP,</title>
<date>2005</date>
<pages>955--962</pages>
<contexts>
<context position="5662" citStr="Ahmad and Kondrak (2005)" startWordPosition="864" endWordPosition="867">a transition-based learning method for spelling correction. Their methods generated three types of rules from training data, which constructs a high performance and concise system for English. A statistics-based method first finds related candidates, and then ranks the candidates based on the statistical model. Atwell and Elliott (1987) used n-gram and part-of-speech language models for spelling correction. Cucerzan and Brill (2004) presented an iterative process for query spelling check, using a query log and trust dictionary. And the noisy channel mode is used to select the best correction. Ahmad and Kondrak (2005) also learned a spelling error model from search query logs to improve the quality of query spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker uses web scale language models and many kinds of features for better performance, including: surface-form similarity, phonetic-form similarity, entity, dictionary, and frequency features. Microsoft (2010) provides Microsoft web n-gram services. Google (2010) has developed a Java API for</context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Ahmad, F., and Kondrak, G. 2005. Learning a spelling error model from search query logs. In HLT_EMNLP, pp 955-962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>M Zhu</author>
<author>Y Zhang</author>
<author>M Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>Proceedings of ACL</booktitle>
<pages>2025--1032</pages>
<contexts>
<context position="5786" citStr="Li et al. (2006)" startWordPosition="886" endWordPosition="889">onstructs a high performance and concise system for English. A statistics-based method first finds related candidates, and then ranks the candidates based on the statistical model. Atwell and Elliott (1987) used n-gram and part-of-speech language models for spelling correction. Cucerzan and Brill (2004) presented an iterative process for query spelling check, using a query log and trust dictionary. And the noisy channel mode is used to select the best correction. Ahmad and Kondrak (2005) also learned a spelling error model from search query logs to improve the quality of query spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker uses web scale language models and many kinds of features for better performance, including: surface-form similarity, phonetic-form similarity, entity, dictionary, and frequency features. Microsoft (2010) provides Microsoft web n-gram services. Google (2010) has developed a Java API for Google spelling check service. As for Chinese spelling correction, an early work was by (Chang, 1995), which used a charact</context>
</contexts>
<marker>Li, Zhu, Zhang, Zhou, 2006</marker>
<rawString>Li, M., Zhu, M., Zhang, Y. and Zhou, M. 2006. Exploring distributional similarity based models for query spelling correction. Proceedings of ACL 2006, pp 2025-1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaolong Li</author>
<author>Daniel Micol</author>
<author>Chris Quirk</author>
<author>Xu Sun</author>
</authors>
<title>A large scale ranker-based system for search query spelling correction.</title>
<date>2010</date>
<booktitle>The 23rd International Conference on Computational Linguistics</booktitle>
<pages>358--366</pages>
<contexts>
<context position="5882" citStr="Gao et al. (2010)" startWordPosition="900" endWordPosition="903">nds related candidates, and then ranks the candidates based on the statistical model. Atwell and Elliott (1987) used n-gram and part-of-speech language models for spelling correction. Cucerzan and Brill (2004) presented an iterative process for query spelling check, using a query log and trust dictionary. And the noisy channel mode is used to select the best correction. Ahmad and Kondrak (2005) also learned a spelling error model from search query logs to improve the quality of query spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker uses web scale language models and many kinds of features for better performance, including: surface-form similarity, phonetic-form similarity, entity, dictionary, and frequency features. Microsoft (2010) provides Microsoft web n-gram services. Google (2010) has developed a Java API for Google spelling check service. As for Chinese spelling correction, an early work was by (Chang, 1995), which used a character dictionary of similar shape, pronunciation, meaning, and input-method-code to deal with the s</context>
</contexts>
<marker>Gao, Li, Micol, Quirk, Sun, 2010</marker>
<rawString>Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk, and Xu Sun. 2010. A large scale ranker-based system for search query spelling correction. The 23rd International Conference on Computational Linguistics 2010 (COLING 2010). pp 358-366.</rawString>
</citation>
<citation valid="true">
<title>web n-gram services.</title>
<date>2010</date>
<institution>Microsoft Microsoft</institution>
<note>http://research.microsoft.com/web-ngram</note>
<contexts>
<context position="5882" citStr="(2010)" startWordPosition="903" endWordPosition="903"> candidates, and then ranks the candidates based on the statistical model. Atwell and Elliott (1987) used n-gram and part-of-speech language models for spelling correction. Cucerzan and Brill (2004) presented an iterative process for query spelling check, using a query log and trust dictionary. And the noisy channel mode is used to select the best correction. Ahmad and Kondrak (2005) also learned a spelling error model from search query logs to improve the quality of query spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker uses web scale language models and many kinds of features for better performance, including: surface-form similarity, phonetic-form similarity, entity, dictionary, and frequency features. Microsoft (2010) provides Microsoft web n-gram services. Google (2010) has developed a Java API for Google spelling check service. As for Chinese spelling correction, an early work was by (Chang, 1995), which used a character dictionary of similar shape, pronunciation, meaning, and input-method-code to deal with the s</context>
</contexts>
<marker>2010</marker>
<rawString>Microsoft Microsoft web n-gram services. 2010. http://research.microsoft.com/web-ngram</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>A Java API for Google spelling check service. http//code.google.com/p/google-apispellingjava/</title>
<date>2010</date>
<contexts>
<context position="6233" citStr="Google (2010)" startWordPosition="949" endWordPosition="950"> best correction. Ahmad and Kondrak (2005) also learned a spelling error model from search query logs to improve the quality of query spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker uses web scale language models and many kinds of features for better performance, including: surface-form similarity, phonetic-form similarity, entity, dictionary, and frequency features. Microsoft (2010) provides Microsoft web n-gram services. Google (2010) has developed a Java API for Google spelling check service. As for Chinese spelling correction, an early work was by (Chang, 1995), which used a character dictionary of similar shape, pronunciation, meaning, and input-method-code to deal with the spelling correction task. The system replaced each character in the sentence with the similar character in dictionary and calculated the probability of all modified sentences based on language model. Zhang et al. (2000) introduced a method that can handle not only Chinese character substitution, but also insertion and deletion errors. They distinguis</context>
</contexts>
<marker>Google, 2010</marker>
<rawString>Google. 2010. A Java API for Google spelling check service. http//code.google.com/p/google-apispellingjava/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang-Huang Chang</author>
</authors>
<title>A new approach for automatic Chinese spelling correction,</title>
<date>1995</date>
<booktitle>Proceedings of Natural Language Processing Pacific Rim Symposium.</booktitle>
<pages>278--283</pages>
<contexts>
<context position="6364" citStr="Chang, 1995" startWordPosition="971" endWordPosition="972">y spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker uses web scale language models and many kinds of features for better performance, including: surface-form similarity, phonetic-form similarity, entity, dictionary, and frequency features. Microsoft (2010) provides Microsoft web n-gram services. Google (2010) has developed a Java API for Google spelling check service. As for Chinese spelling correction, an early work was by (Chang, 1995), which used a character dictionary of similar shape, pronunciation, meaning, and input-method-code to deal with the spelling correction task. The system replaced each character in the sentence with the similar character in dictionary and calculated the probability of all modified sentences based on language model. Zhang et al. (2000) introduced a method that can handle not only Chinese character substitution, but also insertion and deletion errors. They distinguished the way of matching between the Chinese and English, thus largely improved the performance over the work of (Chang 1995). Huang</context>
</contexts>
<marker>Chang, 1995</marker>
<rawString>Chang-Huang Chang. 1995. A new approach for automatic Chinese spelling correction, Proceedings of Natural Language Processing Pacific Rim Symposium. pp 278-283</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Ming Zhou</author>
<author>Chang-Ning Huang</author>
<author>HuiHua Pan</author>
</authors>
<title>Automatic detecting/correcting errors in Chinese text by an approximate wordmatching algorithm.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38the Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>248--254</pages>
<location>Morristwon NJ.</location>
<contexts>
<context position="6700" citStr="Zhang et al. (2000)" startWordPosition="1021" endWordPosition="1024">similarity, phonetic-form similarity, entity, dictionary, and frequency features. Microsoft (2010) provides Microsoft web n-gram services. Google (2010) has developed a Java API for Google spelling check service. As for Chinese spelling correction, an early work was by (Chang, 1995), which used a character dictionary of similar shape, pronunciation, meaning, and input-method-code to deal with the spelling correction task. The system replaced each character in the sentence with the similar character in dictionary and calculated the probability of all modified sentences based on language model. Zhang et al. (2000) introduced a method that can handle not only Chinese character substitution, but also insertion and deletion errors. They distinguished the way of matching between the Chinese and English, thus largely improved the performance over the work of (Chang 1995). Huang et al. (2007) used a word segmentation tool (CKIP) to generate correction candidates, and then to detect Chinese spelling errors. Hung et al. (2008) introduced a method which used the manually edited error templates to correct errors. Zheng et al. (2011) found the fact that when people typed Chinese Pinyins, there are several wrong t</context>
</contexts>
<marker>Zhang, Zhou, Huang, Pan, 2000</marker>
<rawString>Lei Zhang, Ming Zhou, Chang-Ning Huang, and HuiHua Pan. 2000b. Automatic detecting/correcting errors in Chinese text by an approximate wordmatching algorithm. In Proceedings of the 38the Annual Meeting on Association for Computational Linguistics, pp 248-254, Morristwon NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>E Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>4</volume>
<pages>293--300</pages>
<contexts>
<context position="5474" citStr="Cucerzan and Brill (2004)" startWordPosition="833" endWordPosition="836">ased methods. Rule-based methods use some rules generated fromrelevant grammars, the collocation of words, syntactic knowledge, etc, for spelling correction. Mangu and Bill (1997) proposed a transition-based learning method for spelling correction. Their methods generated three types of rules from training data, which constructs a high performance and concise system for English. A statistics-based method first finds related candidates, and then ranks the candidates based on the statistical model. Atwell and Elliott (1987) used n-gram and part-of-speech language models for spelling correction. Cucerzan and Brill (2004) presented an iterative process for query spelling check, using a query log and trust dictionary. And the noisy channel mode is used to select the best correction. Ahmad and Kondrak (2005) also learned a spelling error model from search query logs to improve the quality of query spelling check. Li et al. (2006) applied distributional similarity based models for query spelling correction. Gao et al. (2010) presented a large scale rankerbased system for search query spelling correction, the ranker uses web scale language models and many kinds of features for better performance, including: surfac</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>S. Cucerzan and E. Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proceedings of EMNLP, volume 4, pp 293-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuen-Min Huang</author>
<author>Mei-Chen Wu</author>
<author>Ching-Che Chang</author>
</authors>
<title>Error detection and correction based on Chinese phonemic alphabet in Chinese text. Modeling Decisions for Artificial Intelligence.</title>
<date>2007</date>
<pages>463--476</pages>
<contexts>
<context position="6978" citStr="Huang et al. (2007)" startWordPosition="1065" endWordPosition="1068">1995), which used a character dictionary of similar shape, pronunciation, meaning, and input-method-code to deal with the spelling correction task. The system replaced each character in the sentence with the similar character in dictionary and calculated the probability of all modified sentences based on language model. Zhang et al. (2000) introduced a method that can handle not only Chinese character substitution, but also insertion and deletion errors. They distinguished the way of matching between the Chinese and English, thus largely improved the performance over the work of (Chang 1995). Huang et al. (2007) used a word segmentation tool (CKIP) to generate correction candidates, and then to detect Chinese spelling errors. Hung et al. (2008) introduced a method which used the manually edited error templates to correct errors. Zheng et al. (2011) found the fact that when people typed Chinese Pinyins, there are several wrong types. Then they introduced a method based on a generative model and the typed wrong types to correct spelling errors. Liu et al. (2011) pointed out visually and phonologically similar characters are major factors for errors in Chinese text. And by defining appropriate similarit</context>
</contexts>
<marker>Huang, Wu, Chang, 2007</marker>
<rawString>Huang, Chuen-Min and Wu, Mei-Chen and Chang, Ching-Che. 2007, Error detection and correction based on Chinese phonemic alphabet in Chinese text. Modeling Decisions for Artificial Intelligence. pp. 463-476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ta-Hung Hung</author>
<author>Shih-Hung Wu</author>
</authors>
<title>Chinese Essay Error Detection and Suggestion System. Taiwan E-Learning Forum.</title>
<date>2008</date>
<marker>Hung, Wu, 2008</marker>
<rawString>Hung, Ta-Hung and Wu, Shih-Hung. 2008. Chinese Essay Error Detection and Suggestion System. Taiwan E-Learning Forum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zheng</author>
<author>C Li</author>
<author>M Sun</author>
</authors>
<title>Chime: An efficient error-tolerant Chinese pinyin input method.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22nd International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7219" citStr="Zheng et al. (2011)" startWordPosition="1104" endWordPosition="1107">nd calculated the probability of all modified sentences based on language model. Zhang et al. (2000) introduced a method that can handle not only Chinese character substitution, but also insertion and deletion errors. They distinguished the way of matching between the Chinese and English, thus largely improved the performance over the work of (Chang 1995). Huang et al. (2007) used a word segmentation tool (CKIP) to generate correction candidates, and then to detect Chinese spelling errors. Hung et al. (2008) introduced a method which used the manually edited error templates to correct errors. Zheng et al. (2011) found the fact that when people typed Chinese Pinyins, there are several wrong types. Then they introduced a method based on a generative model and the typed wrong types to correct spelling errors. Liu et al. (2011) pointed out visually and phonologically similar characters are major factors for errors in Chinese text. And by defining appropriate similarity measures that consider extended Cangjie codes, visually similar characters can be quickly identified. Note that all spelling correction methods require lexicons and/or language corpora. And Chinese essays spelling correction has some diffe</context>
</contexts>
<marker>Zheng, Li, Sun, 2011</marker>
<rawString>Y. Zheng, C. Li, and M. Sun. 2011. Chime: An efficient error-tolerant Chinese pinyin input method. In Proceedings of the 22nd International Joint Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
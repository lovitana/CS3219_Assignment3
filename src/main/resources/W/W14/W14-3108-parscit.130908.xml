<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016458">
<title confidence="0.996946">
MUCK: A toolkit for extracting and visualizing semantic dimensions of
large text collections
</title>
<author confidence="0.993906">
Rebecca Weiss
</author>
<affiliation confidence="0.989367">
Stanford University
</affiliation>
<address confidence="0.961702">
Stanford, CA, 94305
</address>
<email confidence="0.999487">
rjweiss@stanford.edu
</email>
<sectionHeader confidence="0.993913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998990285714286">
Users with large text collections are of-
ten faced with one of two problems; ei-
ther they wish to retrieve a semantically-
relevant subset of data from the collection
for further scrutiny (needle-in-a-haystack)
or they wish to glean a high-level un-
derstanding of how a subset compares to
the parent corpus in the context of afore-
mentioned semantic dimensions (forest-
for-the-trees). In this paper, I describe
MUCK1, an open-source toolkit that ad-
dresses both of these problems through a
distributed text processing engine with an
interactive visualization interface.
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976">
As gathering large text collections grows increas-
ingly feasible for non-technical users, individu-
als such as journalists, marketing/communications
analysts, and social scientists are accumulating
vast quantities of documents in order to address
key strategy or research questions. But these
groups often lack the technical skills to work with
large text collections, in that the conventional ap-
proaches they employ (content analysis and indi-
vidual document scrutiny) are not suitable for the
scale of the data they have gathered. Thus, users
require tools with the capability to filter out irrel-
evant documents while drilling-down to the docu-
ments that they are most interested in investigating
with closer scrutiny. Furthermore, they require the
capability to then evaluate their subset in context,
as the contrast in attributes between their subset
and the full corpora can often address many rele-
vant questions.
This paper introduces a work-in-progress: the
development of a toolkit that aids non-technical
</bodyText>
<subsectionHeader confidence="0.953724">
1Mechanical Understanding of Contextual Knowledge
</subsectionHeader>
<bodyText confidence="0.999774833333333">
users of large text collections by combining se-
mantic search and semantic visualization methods.
The purpose of this toolkit is two-fold: first, to
ease the technical burden of working with large-
scale text collections by leveraging semantic infor-
mation for the purposes of filtering a large collec-
tion of text down to the select sample documents
that matter most to the user; second, to allow the
user to visually explore semantic attributes of their
subset in comparison to the rest of the text collec-
tion.
Thus, this toolkit comprises two components:
</bodyText>
<listItem confidence="0.984983111111111">
1. a distributed text processing engine that de-
creases the cost of annotating massive quan-
tities of text data for natural language infor-
mation
2. an interactive visualization interface that en-
ables exploration of the collection along se-
mantic dimensions, which then affords sub-
sequent document selection and subset-to-
corpora comparison
</listItem>
<bodyText confidence="0.999938777777778">
The text processing engine is extensible, en-
abling the future development of plug-ins to al-
low for tasks beyond the included natural language
processing tasks, such that future users can em-
bed any sentence- or document-level task to their
processing pipeline. The visualization interface is
built upon search engine technologies to decrease
search result latency to user requests, enabling a
high level of interactivity.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999630571428571">
The common theme of existing semantic search
and semantic visualization methods is to enable
the user to gain greater, meaningful insight into the
structure of their document collections through the
use of transparent, trustworthy methods (Chuang
et al., 2012; Ramage et al., 2009). The desired in-
sight can change depending on the intended task.
</bodyText>
<page confidence="0.985253">
53
</page>
<note confidence="0.301565">
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 53–58,
</note>
<page confidence="0.322659">
Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics
</page>
<bodyText confidence="0.999969910714286">
For some applications, users are understood to
have a need to find a smaller, relevant subset of
articles (or even a single article) in a vast collec-
tion of documents, which we can refer to as a
needle-in-a-haystack problem. For others, users
simply require the ability to gain a broad but de-
scriptive summary of a semantic concept that de-
scribes these text data, which we can refer to as a
forest-for-the-trees problem.
For example, marketers and social scientists of-
ten study news data, as the news constitute a vi-
tally important source of information that guide
the agendas of marketing strategy and inform
many theories underlying social behavior. How-
ever, their interests are answered at the level of
sentences or documents that contain the concepts
or entities that they care about. This need is often
not met through simple text querying, which can
return too many or too few relevant documents and
sentences. This is an example of a needle-in-a-
haystack problem, which has been previously ad-
dressed through the application of semantic search
(Guha et al., 2003). Much of the literature on
semantic search, in which semantic information
such as named entity, semantic web data, or simple
document categories are added to the individual-
level results of a simple query in order to bolster
the relevance of resulting query hits. This type
of information has proven to be useful in filtering
out irrelevant content for a wide array of informa-
tion retrieval tasks (Blanco et al., 2011; Pound et
al., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al.,
2009; Odijk et al., 2012).
Remaining in the same narrative, once a sub-
set of relevant documents has been created, these
users may wish to see how the semantic charac-
teristics of their subset contrast to the parent col-
lection from which it was drawn. A marketer may
have a desire to see how the tone of coverage in
news related to their client’s brand compares to
the news coverage of other brands of a similar
type. A social scientist may be interested to see
if one news organization covers more politicians
than other news organizations. This is an exam-
ple of a forest-for-the-trees problem. This type of
problem has been addressed through the applica-
tion of semantic visualization, which can be use-
ful for trend analysis and anomaly detection in text
corpora (Fisher et al., 2008; Chase et al., 1998;
Hearst and Karadi, 1997; Hearst, 1995; Ando et
al., 2000).
The toolkit outlined in this paper leverages both
of these techniques in order to facilitate the user’s
ability to gain meaningful insight into various se-
mantic attributes of their text collection while also
retrieving semantically relevant documents.
</bodyText>
<sectionHeader confidence="0.6027935" genericHeader="method">
3 Overview of System From User
Perspective
</sectionHeader>
<bodyText confidence="0.9989345">
The ordering of a user’s experience with this
toolkit is as follows:
</bodyText>
<listItem confidence="0.932025625">
1. Users begin with a collection of unstructured
text documents, which must be made avail-
able to the system (e.g., on a local or network
drive or as a list of URLs for remote content)
2. Users specify the types of semantic detail rel-
evant to their analysis (named entities, senti-
ment, etc.), and documents are then parsed,
annotated, and indexed.
3. Users interact with the visualization in or-
der to create the subset of documents or sen-
tences they are interested in according to se-
mantic dimensions of relevance
4. Once a view has been adequately configured
using the visual feedback, users are able to re-
trieve the documents or sentences referenced
in the visualization from the document store
</listItem>
<bodyText confidence="0.999685">
Items 2 and 3 are further elaborated in the sec-
tions on the backend and frontend.
</bodyText>
<sectionHeader confidence="0.994102" genericHeader="method">
4 Backend
</sectionHeader>
<bodyText confidence="0.999903933333333">
The distributed processing engine is driven by a
task planner, which is a framework for chaining
per-document tasks. As diagrammed in figure 1,
the system creates and distributes text processing
tasks needed to satisfy the user’s level of semantic
interest according to the dependencies between the
various integrated third-party text processing li-
braries. Additionally, this system does not possess
dependencies on additional third-party large-scale
processing frameworks or message queueing sys-
tems, which makes this toolkit useful for relatively
large (i.e. millions of documents) collections as it
does not require configuration of other technolo-
gies beyond maintaining a document store2 and a
search index.
</bodyText>
<footnote confidence="0.992831">
2http://www.mongodb.com
</footnote>
<page confidence="0.98852">
54
</page>
<table confidence="0.770076">
local URL RSS front end
files list feed
list
</table>
<figureCaption confidence="0.999392">
Figure 1: The architecture of the backend system.
</figureCaption>
<figure confidence="0.992818545454545">
document
extraction
task planner
remote worker pool
1 2 ... n
local worker pool
1 2 ... n
task resolver
document
store
index
</figure>
<bodyText confidence="0.997462580645161">
Task planner and resolver system The se-
mantic information extraction process occurs via
defining a series of tasks for each document. This
instantiates a virtual per-document queues of pro-
cessing tasks. These queues are maintained by
a task planner and resolver, which handles all of
the distribution of processing tasks through the
use of local or cloud resources3. This processing
model enables non-technical users to describe a
computationally-intensive, per-document process-
ing pipeline without having to perform any tech-
nical configuration beyond specifying the level of
processing detail output desired.
NLP task Currently, this system only incor-
porates the full Stanford CoreNLP pipeline4,
which processes each document into its (likely)
constituent sentences and tokens and annotates
each sentence and token for named entities,
parts-of-speech, dependency relations, and senti-
ment (Toutanova et al., 2003; Finkel et al., 2005;
De Marneffe et al., 2006; Raghunathan et al.,
2010; Lee et al., 2011; Lee et al., 2013; Re-
casens et al., 2013; Socher et al., 2013). This ex-
traction process is extensible, meaning that future
tasks can be defined and included in the processing
queue in the order determined by the dependen-
cies of the new processing technology. Additional
tasks at the sentence- or document-level, such as
simple text classification using the Stanford Clas-
sifier (Manning and Klein, 2003), are included in
the development roadmap.
</bodyText>
<footnote confidence="0.9994125">
3http://aws.amazon.com
4Using most recent version as of writing (v3.1)
</footnote>
<sectionHeader confidence="0.997235" genericHeader="method">
5 Frontend
</sectionHeader>
<bodyText confidence="0.99996764516129">
A semantic dimension of interest is mapped to a
dimension of the screen as a context pane, as di-
agrammed in figure 2. Corpora-level summaries
for each dimension are provided within each con-
text pane for each semantic category, whereas the
subset that the user interactively builds is visual-
ized in the focus pane of the screen. By brushing
each of semantic dimensions, the user can drill-
down to relevant data while also maintaining an
understanding of the semantic contrast between
their subset and the parent corpus.
This visualization design constitutes a multiple-
view system (Wang Baldonado et al., 2000), where
a single conceptual entity can be viewed from sev-
eral perspectives. In this case, the semantic con-
cepts extracted from the data can be portrayed in
several ways. This system maps semantic dimen-
sions to visualization components using the fol-
lowing interaction techniques:
Navigational slaving Users must first make an
initial selection for data by querying for a spe-
cific item of interest; a general text query (ideal
for phrase matching), a named entity, or even an
entity that served in a specific dependency relation
(such as the dependent of an nsubj relation). This
selection propagates through the remaining com-
ponents of the interface, such that the remaining
semantic dimensions are manipulated in the con-
text of the original query.
Focus + Context Users can increase their under-
standing of the subset by zooming into a relevant
</bodyText>
<page confidence="0.991155">
55
</page>
<figure confidence="0.999867222222222">
filter
context pane
(dimension1)
brush
brush
filter
primary navigational slaving pane (query)
context pane (dimension3)
filter
focus pane
filter
filter
filter
brush
brush
filter
context pane
(dimension2)
</figure>
<figureCaption confidence="0.999935">
Figure 2: The wireframe of the frontend system.
</figureCaption>
<bodyText confidence="0.9938032">
selection in a semantic dimension (e.g. time).
Brushing Users can further restrict their sub-
set by highlighting categories or ranges of interest
in semantic dimensions (e.g. document sources,
types of named entities). Brushing technique is
determined by whether the semantic concept is
categorical or continuous.
Filtering The brushing and context panes serve
as filters, which restrict the visualized subset to
only documents containing the intersection of all
brushed characteristics.
This visualization design is enabled through the
use of a distributed search engine5, which enables
the previously defined interactivity through three
behaviors:
Filters Search engines enable the restriction
of query results according to whether a query
matches the parameters of a filter, such as whether
a field contains text of a specific pattern.
Facets Search engines also can return subsets of
documents structured along a dimension of inter-
est, such as by document source types (if such in-
formation was originally included in the index).
Aggregations Aggregations allow for bucketing
of relevant data and metrics to be calculated per
</bodyText>
<footnote confidence="0.944666">
5http://www.elasticsearch.com
</footnote>
<bodyText confidence="0.998339230769231">
bucket. This allows the swift retrieval of docu-
ments in a variety of structures, providing the hi-
erarchical representation required for visualizing
a subset along multiple semantic dimensions de-
fined above.
Nesting All of these capabilities can be stacked
upon each other, allowing for the multiple view
system described above.
The visualization components are highly inter-
active, since the application is built upon a two-
way binding design paradigm6 between the DOM
and the RESTful API of the index (Bostock et al.,
2011).
</bodyText>
<sectionHeader confidence="0.985028" genericHeader="discussions">
6 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999973555555555">
This paper presents a work-in-progress on the de-
velopment of a system that enables the extraction
and visualization of large text collections along se-
mantic dimensions. This system is open-source
and extensible, so that additional per-document
processing tasks for future semantic extraction
procedures can be easily distributed. Additionally,
this system does not possess requirements beyond
maintaining a document store and a search index.
</bodyText>
<footnote confidence="0.995515">
6http://www.angularjs.org
</footnote>
<page confidence="0.997517">
56
</page>
<sectionHeader confidence="0.939289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998294220183486">
Rie Kubota Ando, Branimir K Boguraev, Roy J Byrd,
and Mary S Neff. 2000. Multi-document summa-
rization by visualizing topical content. In Proceed-
ings of the 2000 NAACL-ANLP Workshop on Auto-
matic Summarization, pages 79–98. Association for
Computational Linguistics.
Roi Blanco, Harry Halpin, Daniel M Herzig, Pe-
ter Mika, Jeffrey Pound, Henry S Thompson, and
T Tran Duc. 2011. Entity search evaluation over
structured web data. In Proceedings of the 1st inter-
national workshop on entity-oriented search work-
shop (SIGIR 2011), ACM, New York.
Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer.
2011. D3 data-driven documents. Visualization
and Computer Graphics, IEEE Transactions on,
17(12):2301–2309.
Penny Chase, Ray D’Amore, Nahum Gershon, Rod
Holland, Rob Hyland, Inderjeet Mani, Mark May-
bury, Andy Merlino, and Jim Rayson. 1998. Se-
mantic visualization. In ACL-COLING Workshop
on Content Visualization and Intermedia Represen-
tation.
Jason Chuang, Daniel Ramage, Christopher Manning,
and Jeffrey Heer. 2012. Interpretation and trust:
Designing model-driven visualizations for text anal-
ysis. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, pages 443–
452. ACM.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449–454.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Danyel Fisher, Aaron Hoff, George Robertson, and
Matthew Hurst. 2008. Narratives: A visualization
to track narrative events as they develop. In Visual
Analytics Science and Technology, 2008. VAST’08.
IEEE Symposium on, pages 115–122. IEEE.
Ramanathan Guha, Rob McCool, and Eric Miller.
2003. Semantic search. In Proceedings of the 12th
international conference on World Wide Web, pages
700–709. ACM.
Marti A Hearst and Chandu Karadi. 1997. Cat-a-cone:
an interactive interface for specifying searches and
viewing retrieval results using a large category hi-
erarchy. In ACM SIGIR Forum, volume 31, pages
246–255. ACM.
Marti A Hearst. 1995. Tilebars: visualization of term
distribution information in full text information ac-
cess. In Proceedings of the SIGCHI conference on
Human factors in computing systems, pages 59–66.
ACM Press/Addison-Wesley Publishing Co.
Marti A Hearst. 1999a. Untangling text data mining.
In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Com-
putational Linguistics, pages 3–10. Association for
Computational Linguistics.
Marti A Hearst. 1999b. The use of categories and
clusters for organizing retrieval results. In Natu-
ral language information retrieval, pages 333–374.
Springer.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve coref-
erence resolution system at the conll-2011 shared
task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34. Association for Computational
Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolution
based on entity-centric, precision-ranked rules.
Shixia Liu, Michelle X Zhou, Shimei Pan, Weihong
Qian, Weijia Cai, and Xiaoxiao Lian. 2009. Interac-
tive, topic-based visual text summarization and anal-
ysis. In Proceedings of the 18th ACM conference
on Information and knowledge management, pages
543–552. ACM.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In Proceedings of the 2003 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics on Human
Language Technology: Tutorials-Volume 5, pages
8–8. Association for Computational Linguistics.
Daan Odijk, Ork de Rooij, Maria-Hendrike Peetz,
Toine Pieters, Maarten de Rijke, and Stephen
Snelders. 2012. Semantic document selection.
In Theory and Practice of Digital Libraries, pages
215–221. Springer.
Jeffrey Pound, Peter Mika, and Hugo Zaragoza. 2010.
Ad-hoc object retrieval in the web of data. In Pro-
ceedings of the 19th international conference on
World wide web, pages 771–780. ACM.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 492–501.
Association for Computational Linguistics.
</reference>
<page confidence="0.980333">
57
</page>
<reference confidence="0.9995459375">
Daniel Ramage, Evan Rosen, Jason Chuang, Christo-
pher D Manning, and Daniel A McFarland. 2009.
Topic modeling for the social sciences. In NIPS
2009 Workshop on Applications for Topic Models:
Text and Beyond, volume 5.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 627–633.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Michelle Q Wang Baldonado, Allison Woodruff, and
Allan Kuchinsky. 2000. Guidelines for using multi-
ple views in information visualization. In Proceed-
ings of the working conference on Advanced visual
interfaces, pages 110–119. ACM.
</reference>
<page confidence="0.999258">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.251653">
<title confidence="0.9588905">MUCK: A toolkit for extracting and visualizing semantic dimensions large text collections</title>
<author confidence="0.572951">Rebecca</author>
<affiliation confidence="0.371863">Stanford</affiliation>
<address confidence="0.395755">Stanford, CA,</address>
<email confidence="0.997989">rjweiss@stanford.edu</email>
<abstract confidence="0.9994936">Users with large text collections are often faced with one of two problems; either they wish to retrieve a semanticallyrelevant subset of data from the collection for further scrutiny (needle-in-a-haystack) or they wish to glean a high-level understanding of how a subset compares to the parent corpus in the context of aforementioned semantic dimensions (forestfor-the-trees). In this paper, I describe an open-source toolkit that addresses both of these problems through a distributed text processing engine with an interactive visualization interface.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Branimir K Boguraev</author>
<author>Roy J Byrd</author>
<author>Mary S Neff</author>
</authors>
<title>Multi-document summarization by visualizing topical content.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization,</booktitle>
<pages>79--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6095" citStr="Ando et al., 2000" startWordPosition="964" endWordPosition="967">drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an example of a forest-for-the-trees problem. This type of problem has been addressed through the application of semantic visualization, which can be useful for trend analysis and anomaly detection in text corpora (Fisher et al., 2008; Chase et al., 1998; Hearst and Karadi, 1997; Hearst, 1995; Ando et al., 2000). The toolkit outlined in this paper leverages both of these techniques in order to facilitate the user’s ability to gain meaningful insight into various semantic attributes of their text collection while also retrieving semantically relevant documents. 3 Overview of System From User Perspective The ordering of a user’s experience with this toolkit is as follows: 1. Users begin with a collection of unstructured text documents, which must be made available to the system (e.g., on a local or network drive or as a list of URLs for remote content) 2. Users specify the types of semantic detail rele</context>
</contexts>
<marker>Ando, Boguraev, Byrd, Neff, 2000</marker>
<rawString>Rie Kubota Ando, Branimir K Boguraev, Roy J Byrd, and Mary S Neff. 2000. Multi-document summarization by visualizing topical content. In Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization, pages 79–98. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Blanco</author>
<author>Harry Halpin</author>
<author>Daniel M Herzig</author>
<author>Peter Mika</author>
<author>Jeffrey Pound</author>
<author>Henry S Thompson</author>
<author>T Tran Duc</author>
</authors>
<title>Entity search evaluation over structured web data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 1st international workshop on entity-oriented search workshop (SIGIR</booktitle>
<publisher>ACM,</publisher>
<location>New York.</location>
<contexts>
<context position="5170" citStr="Blanco et al., 2011" startWordPosition="802" endWordPosition="805"> many or too few relevant documents and sentences. This is an example of a needle-in-ahaystack problem, which has been previously addressed through the application of semantic search (Guha et al., 2003). Much of the literature on semantic search, in which semantic information such as named entity, semantic web data, or simple document categories are added to the individuallevel results of a simple query in order to bolster the relevance of resulting query hits. This type of information has proven to be useful in filtering out irrelevant content for a wide array of information retrieval tasks (Blanco et al., 2011; Pound et al., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al., 2009; Odijk et al., 2012). Remaining in the same narrative, once a subset of relevant documents has been created, these users may wish to see how the semantic characteristics of their subset contrast to the parent collection from which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizatio</context>
</contexts>
<marker>Blanco, Halpin, Herzig, Mika, Pound, Thompson, Duc, 2011</marker>
<rawString>Roi Blanco, Harry Halpin, Daniel M Herzig, Peter Mika, Jeffrey Pound, Henry S Thompson, and T Tran Duc. 2011. Entity search evaluation over structured web data. In Proceedings of the 1st international workshop on entity-oriented search workshop (SIGIR 2011), ACM, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bostock</author>
<author>Vadim Ogievetsky</author>
<author>Jeffrey Heer</author>
</authors>
<title>D3 data-driven documents.</title>
<date>2011</date>
<journal>Visualization and Computer Graphics, IEEE Transactions on,</journal>
<volume>17</volume>
<issue>12</issue>
<marker>Bostock, Ogievetsky, Heer, 2011</marker>
<rawString>Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011. D3 data-driven documents. Visualization and Computer Graphics, IEEE Transactions on, 17(12):2301–2309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penny Chase</author>
<author>Ray D’Amore</author>
<author>Nahum Gershon</author>
<author>Rod Holland</author>
<author>Rob Hyland</author>
<author>Inderjeet Mani</author>
<author>Mark Maybury</author>
<author>Andy Merlino</author>
<author>Jim Rayson</author>
</authors>
<title>Semantic visualization.</title>
<date>1998</date>
<booktitle>In ACL-COLING Workshop on Content Visualization and Intermedia Representation.</booktitle>
<marker>Chase, D’Amore, Gershon, Holland, Hyland, Mani, Maybury, Merlino, Rayson, 1998</marker>
<rawString>Penny Chase, Ray D’Amore, Nahum Gershon, Rod Holland, Rob Hyland, Inderjeet Mani, Mark Maybury, Andy Merlino, and Jim Rayson. 1998. Semantic visualization. In ACL-COLING Workshop on Content Visualization and Intermedia Representation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Chuang</author>
<author>Daniel Ramage</author>
<author>Christopher Manning</author>
<author>Jeffrey Heer</author>
</authors>
<title>Interpretation and trust: Designing model-driven visualizations for text analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>443--452</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3398" citStr="Chuang et al., 2012" startWordPosition="512" endWordPosition="515"> plug-ins to allow for tasks beyond the included natural language processing tasks, such that future users can embed any sentence- or document-level task to their processing pipeline. The visualization interface is built upon search engine technologies to decrease search result latency to user requests, enabling a high level of interactivity. 2 Related work The common theme of existing semantic search and semantic visualization methods is to enable the user to gain greater, meaningful insight into the structure of their document collections through the use of transparent, trustworthy methods (Chuang et al., 2012; Ramage et al., 2009). The desired insight can change depending on the intended task. 53 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 53–58, Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics For some applications, users are understood to have a need to find a smaller, relevant subset of articles (or even a single article) in a vast collection of documents, which we can refer to as a needle-in-a-haystack problem. For others, users simply require the ability to gain a broad but descriptive summary of </context>
</contexts>
<marker>Chuang, Ramage, Manning, Heer, 2012</marker>
<rawString>Jason Chuang, Daniel Ramage, Christopher Manning, and Jeffrey Heer. 2012. Interpretation and trust: Designing model-driven visualizations for text analysis. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 443– 452. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9141" citStr="Finkel et al., 2005" startWordPosition="1439" endWordPosition="1442"> tasks through the use of local or cloud resources3. This processing model enables non-technical users to describe a computationally-intensive, per-document processing pipeline without having to perform any technical configuration beyond specifying the level of processing detail output desired. NLP task Currently, this system only incorporates the full Stanford CoreNLP pipeline4, which processes each document into its (likely) constituent sentences and tokens and annotates each sentence and token for named entities, parts-of-speech, dependency relations, and sentiment (Toutanova et al., 2003; Finkel et al., 2005; De Marneffe et al., 2006; Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013; Recasens et al., 2013; Socher et al., 2013). This extraction process is extensible, meaning that future tasks can be defined and included in the processing queue in the order determined by the dependencies of the new processing technology. Additional tasks at the sentence- or document-level, such as simple text classification using the Stanford Classifier (Manning and Klein, 2003), are included in the development roadmap. 3http://aws.amazon.com 4Using most recent version as of writing (v3.1) 5 Frontend A </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danyel Fisher</author>
<author>Aaron Hoff</author>
<author>George Robertson</author>
<author>Matthew Hurst</author>
</authors>
<title>Narratives: A visualization to track narrative events as they develop.</title>
<date>2008</date>
<booktitle>In Visual Analytics Science and Technology, 2008. VAST’08. IEEE Symposium on,</booktitle>
<pages>115--122</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6016" citStr="Fisher et al., 2008" startWordPosition="950" endWordPosition="953">acteristics of their subset contrast to the parent collection from which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an example of a forest-for-the-trees problem. This type of problem has been addressed through the application of semantic visualization, which can be useful for trend analysis and anomaly detection in text corpora (Fisher et al., 2008; Chase et al., 1998; Hearst and Karadi, 1997; Hearst, 1995; Ando et al., 2000). The toolkit outlined in this paper leverages both of these techniques in order to facilitate the user’s ability to gain meaningful insight into various semantic attributes of their text collection while also retrieving semantically relevant documents. 3 Overview of System From User Perspective The ordering of a user’s experience with this toolkit is as follows: 1. Users begin with a collection of unstructured text documents, which must be made available to the system (e.g., on a local or network drive or as a list</context>
</contexts>
<marker>Fisher, Hoff, Robertson, Hurst, 2008</marker>
<rawString>Danyel Fisher, Aaron Hoff, George Robertson, and Matthew Hurst. 2008. Narratives: A visualization to track narrative events as they develop. In Visual Analytics Science and Technology, 2008. VAST’08. IEEE Symposium on, pages 115–122. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramanathan Guha</author>
<author>Rob McCool</author>
<author>Eric Miller</author>
</authors>
<title>Semantic search.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>700--709</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4753" citStr="Guha et al., 2003" startWordPosition="732" endWordPosition="735">cial scientists often study news data, as the news constitute a vitally important source of information that guide the agendas of marketing strategy and inform many theories underlying social behavior. However, their interests are answered at the level of sentences or documents that contain the concepts or entities that they care about. This need is often not met through simple text querying, which can return too many or too few relevant documents and sentences. This is an example of a needle-in-ahaystack problem, which has been previously addressed through the application of semantic search (Guha et al., 2003). Much of the literature on semantic search, in which semantic information such as named entity, semantic web data, or simple document categories are added to the individuallevel results of a simple query in order to bolster the relevance of resulting query hits. This type of information has proven to be useful in filtering out irrelevant content for a wide array of information retrieval tasks (Blanco et al., 2011; Pound et al., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al., 2009; Odijk et al., 2012). Remaining in the same narrative, once a subset of relevant documents has been created, these</context>
</contexts>
<marker>Guha, McCool, Miller, 2003</marker>
<rawString>Ramanathan Guha, Rob McCool, and Eric Miller. 2003. Semantic search. In Proceedings of the 12th international conference on World Wide Web, pages 700–709. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Chandu Karadi</author>
</authors>
<title>Cat-a-cone: an interactive interface for specifying searches and viewing retrieval results using a large category hierarchy.</title>
<date>1997</date>
<booktitle>In ACM SIGIR Forum,</booktitle>
<volume>31</volume>
<pages>246--255</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6061" citStr="Hearst and Karadi, 1997" startWordPosition="958" endWordPosition="961">he parent collection from which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an example of a forest-for-the-trees problem. This type of problem has been addressed through the application of semantic visualization, which can be useful for trend analysis and anomaly detection in text corpora (Fisher et al., 2008; Chase et al., 1998; Hearst and Karadi, 1997; Hearst, 1995; Ando et al., 2000). The toolkit outlined in this paper leverages both of these techniques in order to facilitate the user’s ability to gain meaningful insight into various semantic attributes of their text collection while also retrieving semantically relevant documents. 3 Overview of System From User Perspective The ordering of a user’s experience with this toolkit is as follows: 1. Users begin with a collection of unstructured text documents, which must be made available to the system (e.g., on a local or network drive or as a list of URLs for remote content) 2. Users specify</context>
</contexts>
<marker>Hearst, Karadi, 1997</marker>
<rawString>Marti A Hearst and Chandu Karadi. 1997. Cat-a-cone: an interactive interface for specifying searches and viewing retrieval results using a large category hierarchy. In ACM SIGIR Forum, volume 31, pages 246–255. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Tilebars: visualization of term distribution information in full text information access.</title>
<date>1995</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems,</booktitle>
<pages>59--66</pages>
<publisher>ACM Press/Addison-Wesley Publishing Co.</publisher>
<contexts>
<context position="6075" citStr="Hearst, 1995" startWordPosition="962" endWordPosition="963"> which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an example of a forest-for-the-trees problem. This type of problem has been addressed through the application of semantic visualization, which can be useful for trend analysis and anomaly detection in text corpora (Fisher et al., 2008; Chase et al., 1998; Hearst and Karadi, 1997; Hearst, 1995; Ando et al., 2000). The toolkit outlined in this paper leverages both of these techniques in order to facilitate the user’s ability to gain meaningful insight into various semantic attributes of their text collection while also retrieving semantically relevant documents. 3 Overview of System From User Perspective The ordering of a user’s experience with this toolkit is as follows: 1. Users begin with a collection of unstructured text documents, which must be made available to the system (e.g., on a local or network drive or as a list of URLs for remote content) 2. Users specify the types of </context>
</contexts>
<marker>Hearst, 1995</marker>
<rawString>Marti A Hearst. 1995. Tilebars: visualization of term distribution information in full text information access. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 59–66. ACM Press/Addison-Wesley Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Untangling text data mining.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>3--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5204" citStr="Hearst, 1999" startWordPosition="810" endWordPosition="811">entences. This is an example of a needle-in-ahaystack problem, which has been previously addressed through the application of semantic search (Guha et al., 2003). Much of the literature on semantic search, in which semantic information such as named entity, semantic web data, or simple document categories are added to the individuallevel results of a simple query in order to bolster the relevance of resulting query hits. This type of information has proven to be useful in filtering out irrelevant content for a wide array of information retrieval tasks (Blanco et al., 2011; Pound et al., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al., 2009; Odijk et al., 2012). Remaining in the same narrative, once a subset of relevant documents has been created, these users may wish to see how the semantic characteristics of their subset contrast to the parent collection from which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an example of a forest</context>
</contexts>
<marker>Hearst, 1999</marker>
<rawString>Marti A Hearst. 1999a. Untangling text data mining. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 3–10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>The use of categories and clusters for organizing retrieval results.</title>
<date>1999</date>
<booktitle>In Natural language information retrieval,</booktitle>
<pages>333--374</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5204" citStr="Hearst, 1999" startWordPosition="810" endWordPosition="811">entences. This is an example of a needle-in-ahaystack problem, which has been previously addressed through the application of semantic search (Guha et al., 2003). Much of the literature on semantic search, in which semantic information such as named entity, semantic web data, or simple document categories are added to the individuallevel results of a simple query in order to bolster the relevance of resulting query hits. This type of information has proven to be useful in filtering out irrelevant content for a wide array of information retrieval tasks (Blanco et al., 2011; Pound et al., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al., 2009; Odijk et al., 2012). Remaining in the same narrative, once a subset of relevant documents has been created, these users may wish to see how the semantic characteristics of their subset contrast to the parent collection from which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an example of a forest</context>
</contexts>
<marker>Hearst, 1999</marker>
<rawString>Marti A Hearst. 1999b. The use of categories and clusters for organizing retrieval results. In Natural language information retrieval, pages 333–374. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9211" citStr="Lee et al., 2011" startWordPosition="1452" endWordPosition="1455">l enables non-technical users to describe a computationally-intensive, per-document processing pipeline without having to perform any technical configuration beyond specifying the level of processing detail output desired. NLP task Currently, this system only incorporates the full Stanford CoreNLP pipeline4, which processes each document into its (likely) constituent sentences and tokens and annotates each sentence and token for named entities, parts-of-speech, dependency relations, and sentiment (Toutanova et al., 2003; Finkel et al., 2005; De Marneffe et al., 2006; Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013; Recasens et al., 2013; Socher et al., 2013). This extraction process is extensible, meaning that future tasks can be defined and included in the processing queue in the order determined by the dependencies of the new processing technology. Additional tasks at the sentence- or document-level, such as simple text classification using the Stanford Classifier (Manning and Klein, 2003), are included in the development roadmap. 3http://aws.amazon.com 4Using most recent version as of writing (v3.1) 5 Frontend A semantic dimension of interest is mapped to a dimension of the screen </context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<contexts>
<context position="9229" citStr="Lee et al., 2013" startWordPosition="1456" endWordPosition="1459">nical users to describe a computationally-intensive, per-document processing pipeline without having to perform any technical configuration beyond specifying the level of processing detail output desired. NLP task Currently, this system only incorporates the full Stanford CoreNLP pipeline4, which processes each document into its (likely) constituent sentences and tokens and annotates each sentence and token for named entities, parts-of-speech, dependency relations, and sentiment (Toutanova et al., 2003; Finkel et al., 2005; De Marneffe et al., 2006; Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013; Recasens et al., 2013; Socher et al., 2013). This extraction process is extensible, meaning that future tasks can be defined and included in the processing queue in the order determined by the dependencies of the new processing technology. Additional tasks at the sentence- or document-level, such as simple text classification using the Stanford Classifier (Manning and Klein, 2003), are included in the development roadmap. 3http://aws.amazon.com 4Using most recent version as of writing (v3.1) 5 Frontend A semantic dimension of interest is mapped to a dimension of the screen as a context pane,</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shixia Liu</author>
<author>Michelle X Zhou</author>
<author>Shimei Pan</author>
<author>Weihong Qian</author>
<author>Weijia Cai</author>
<author>Xiaoxiao Lian</author>
</authors>
<title>Interactive, topic-based visual text summarization and analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>543--552</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5238" citStr="Liu et al., 2009" startWordPosition="814" endWordPosition="817">f a needle-in-ahaystack problem, which has been previously addressed through the application of semantic search (Guha et al., 2003). Much of the literature on semantic search, in which semantic information such as named entity, semantic web data, or simple document categories are added to the individuallevel results of a simple query in order to bolster the relevance of resulting query hits. This type of information has proven to be useful in filtering out irrelevant content for a wide array of information retrieval tasks (Blanco et al., 2011; Pound et al., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al., 2009; Odijk et al., 2012). Remaining in the same narrative, once a subset of relevant documents has been created, these users may wish to see how the semantic characteristics of their subset contrast to the parent collection from which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an example of a forest-for-the-trees problem. This type </context>
</contexts>
<marker>Liu, Zhou, Pan, Qian, Cai, Lian, 2009</marker>
<rawString>Shixia Liu, Michelle X Zhou, Shimei Pan, Weihong Qian, Weijia Cai, and Xiaoxiao Lian. 2009. Interactive, topic-based visual text summarization and analysis. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 543–552. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Dan Klein</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Tutorials-Volume 5,</booktitle>
<pages>8--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9614" citStr="Manning and Klein, 2003" startWordPosition="1518" endWordPosition="1521">annotates each sentence and token for named entities, parts-of-speech, dependency relations, and sentiment (Toutanova et al., 2003; Finkel et al., 2005; De Marneffe et al., 2006; Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013; Recasens et al., 2013; Socher et al., 2013). This extraction process is extensible, meaning that future tasks can be defined and included in the processing queue in the order determined by the dependencies of the new processing technology. Additional tasks at the sentence- or document-level, such as simple text classification using the Stanford Classifier (Manning and Klein, 2003), are included in the development roadmap. 3http://aws.amazon.com 4Using most recent version as of writing (v3.1) 5 Frontend A semantic dimension of interest is mapped to a dimension of the screen as a context pane, as diagrammed in figure 2. Corpora-level summaries for each dimension are provided within each context pane for each semantic category, whereas the subset that the user interactively builds is visualized in the focus pane of the screen. By brushing each of semantic dimensions, the user can drilldown to relevant data while also maintaining an understanding of the semantic contrast b</context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>Christopher Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without magic. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Tutorials-Volume 5, pages 8–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daan Odijk</author>
<author>Ork de Rooij</author>
<author>Maria-Hendrike Peetz</author>
<author>Toine Pieters</author>
<author>Maarten de Rijke</author>
<author>Stephen Snelders</author>
</authors>
<title>Semantic document selection.</title>
<date>2012</date>
<booktitle>In Theory and Practice of Digital Libraries,</booktitle>
<pages>215--221</pages>
<publisher>Springer.</publisher>
<marker>Odijk, de Rooij, Peetz, Pieters, de Rijke, Snelders, 2012</marker>
<rawString>Daan Odijk, Ork de Rooij, Maria-Hendrike Peetz, Toine Pieters, Maarten de Rijke, and Stephen Snelders. 2012. Semantic document selection. In Theory and Practice of Digital Libraries, pages 215–221. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pound</author>
<author>Peter Mika</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Ad-hoc object retrieval in the web of data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>771--780</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5190" citStr="Pound et al., 2010" startWordPosition="806" endWordPosition="809">vant documents and sentences. This is an example of a needle-in-ahaystack problem, which has been previously addressed through the application of semantic search (Guha et al., 2003). Much of the literature on semantic search, in which semantic information such as named entity, semantic web data, or simple document categories are added to the individuallevel results of a simple query in order to bolster the relevance of resulting query hits. This type of information has proven to be useful in filtering out irrelevant content for a wide array of information retrieval tasks (Blanco et al., 2011; Pound et al., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al., 2009; Odijk et al., 2012). Remaining in the same narrative, once a subset of relevant documents has been created, these users may wish to see how the semantic characteristics of their subset contrast to the parent collection from which it was drawn. A marketer may have a desire to see how the tone of coverage in news related to their client’s brand compares to the news coverage of other brands of a similar type. A social scientist may be interested to see if one news organization covers more politicians than other news organizations. This is an examp</context>
</contexts>
<marker>Pound, Mika, Zaragoza, 2010</marker>
<rawString>Jeffrey Pound, Peter Mika, and Hugo Zaragoza. 2010. Ad-hoc object retrieval in the web of data. In Proceedings of the 19th international conference on World wide web, pages 771–780. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>492--501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9193" citStr="Raghunathan et al., 2010" startWordPosition="1448" endWordPosition="1451">ces3. This processing model enables non-technical users to describe a computationally-intensive, per-document processing pipeline without having to perform any technical configuration beyond specifying the level of processing detail output desired. NLP task Currently, this system only incorporates the full Stanford CoreNLP pipeline4, which processes each document into its (likely) constituent sentences and tokens and annotates each sentence and token for named entities, parts-of-speech, dependency relations, and sentiment (Toutanova et al., 2003; Finkel et al., 2005; De Marneffe et al., 2006; Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013; Recasens et al., 2013; Socher et al., 2013). This extraction process is extensible, meaning that future tasks can be defined and included in the processing queue in the order determined by the dependencies of the new processing technology. Additional tasks at the sentence- or document-level, such as simple text classification using the Stanford Classifier (Manning and Klein, 2003), are included in the development roadmap. 3http://aws.amazon.com 4Using most recent version as of writing (v3.1) 5 Frontend A semantic dimension of interest is mapped to a dimens</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 492–501. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Evan Rosen</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Daniel A McFarland</author>
</authors>
<title>Topic modeling for the social sciences.</title>
<date>2009</date>
<booktitle>In NIPS 2009 Workshop on Applications for Topic Models: Text and Beyond,</booktitle>
<volume>5</volume>
<contexts>
<context position="3420" citStr="Ramage et al., 2009" startWordPosition="516" endWordPosition="519">r tasks beyond the included natural language processing tasks, such that future users can embed any sentence- or document-level task to their processing pipeline. The visualization interface is built upon search engine technologies to decrease search result latency to user requests, enabling a high level of interactivity. 2 Related work The common theme of existing semantic search and semantic visualization methods is to enable the user to gain greater, meaningful insight into the structure of their document collections through the use of transparent, trustworthy methods (Chuang et al., 2012; Ramage et al., 2009). The desired insight can change depending on the intended task. 53 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 53–58, Baltimore, Maryland, USA, June 27, 2014. @c 2014 Association for Computational Linguistics For some applications, users are understood to have a need to find a smaller, relevant subset of articles (or even a single article) in a vast collection of documents, which we can refer to as a needle-in-a-haystack problem. For others, users simply require the ability to gain a broad but descriptive summary of a semantic concept tha</context>
</contexts>
<marker>Ramage, Rosen, Chuang, Manning, McFarland, 2009</marker>
<rawString>Daniel Ramage, Evan Rosen, Jason Chuang, Christopher D Manning, and Daniel A McFarland. 2009. Topic modeling for the social sciences. In NIPS 2009 Workshop on Applications for Topic Models: Text and Beyond, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>627--633</pages>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="9274" citStr="Socher et al., 2013" startWordPosition="1465" endWordPosition="1468">-intensive, per-document processing pipeline without having to perform any technical configuration beyond specifying the level of processing detail output desired. NLP task Currently, this system only incorporates the full Stanford CoreNLP pipeline4, which processes each document into its (likely) constituent sentences and tokens and annotates each sentence and token for named entities, parts-of-speech, dependency relations, and sentiment (Toutanova et al., 2003; Finkel et al., 2005; De Marneffe et al., 2006; Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013; Recasens et al., 2013; Socher et al., 2013). This extraction process is extensible, meaning that future tasks can be defined and included in the processing queue in the order determined by the dependencies of the new processing technology. Additional tasks at the sentence- or document-level, such as simple text classification using the Stanford Classifier (Manning and Klein, 2003), are included in the development roadmap. 3http://aws.amazon.com 4Using most recent version as of writing (v3.1) 5 Frontend A semantic dimension of interest is mapped to a dimension of the screen as a context pane, as diagrammed in figure 2. Corpora-level sum</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9120" citStr="Toutanova et al., 2003" startWordPosition="1435" endWordPosition="1438">stribution of processing tasks through the use of local or cloud resources3. This processing model enables non-technical users to describe a computationally-intensive, per-document processing pipeline without having to perform any technical configuration beyond specifying the level of processing detail output desired. NLP task Currently, this system only incorporates the full Stanford CoreNLP pipeline4, which processes each document into its (likely) constituent sentences and tokens and annotates each sentence and token for named entities, parts-of-speech, dependency relations, and sentiment (Toutanova et al., 2003; Finkel et al., 2005; De Marneffe et al., 2006; Raghunathan et al., 2010; Lee et al., 2011; Lee et al., 2013; Recasens et al., 2013; Socher et al., 2013). This extraction process is extensible, meaning that future tasks can be defined and included in the processing queue in the order determined by the dependencies of the new processing technology. Additional tasks at the sentence- or document-level, such as simple text classification using the Stanford Classifier (Manning and Klein, 2003), are included in the development roadmap. 3http://aws.amazon.com 4Using most recent version as of writing</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Q Wang Baldonado</author>
<author>Allison Woodruff</author>
<author>Allan Kuchinsky</author>
</authors>
<title>Guidelines for using multiple views in information visualization.</title>
<date>2000</date>
<booktitle>In Proceedings of the working conference on Advanced visual interfaces,</booktitle>
<pages>110--119</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10346" citStr="Baldonado et al., 2000" startWordPosition="1635" endWordPosition="1638">3.1) 5 Frontend A semantic dimension of interest is mapped to a dimension of the screen as a context pane, as diagrammed in figure 2. Corpora-level summaries for each dimension are provided within each context pane for each semantic category, whereas the subset that the user interactively builds is visualized in the focus pane of the screen. By brushing each of semantic dimensions, the user can drilldown to relevant data while also maintaining an understanding of the semantic contrast between their subset and the parent corpus. This visualization design constitutes a multipleview system (Wang Baldonado et al., 2000), where a single conceptual entity can be viewed from several perspectives. In this case, the semantic concepts extracted from the data can be portrayed in several ways. This system maps semantic dimensions to visualization components using the following interaction techniques: Navigational slaving Users must first make an initial selection for data by querying for a specific item of interest; a general text query (ideal for phrase matching), a named entity, or even an entity that served in a specific dependency relation (such as the dependent of an nsubj relation). This selection propagates t</context>
</contexts>
<marker>Baldonado, Woodruff, Kuchinsky, 2000</marker>
<rawString>Michelle Q Wang Baldonado, Allison Woodruff, and Allan Kuchinsky. 2000. Guidelines for using multiple views in information visualization. In Proceedings of the working conference on Advanced visual interfaces, pages 110–119. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000205">
<title confidence="0.998575">
A multimodal corpus for the evaluation of computational models for
(grounded) language acquisition
</title>
<author confidence="0.924423">
Judith Gaspersa, Maximilian Panznera, Andre Lemmeb, Philipp Cimianoa,
Katharina J. Rohlfing&apos;, Sebastian Wredeb
</author>
<affiliation confidence="0.8707">
aSemantic Computing Group, CITEC, Bielefeld University, Germany
</affiliation>
<bodyText confidence="0.202863">
{jgaspers|mpanzner|cimiano}@cit-ec.uni-bielefeld.de
bResearch Institute for Cognition and Robotics (CoR-Lab), Bielefeld University, Germany
{alemme|swrede}@cor-lab.uni-bielefeld.de
&apos;Emergentist Semantics Group, CITEC, Bielefeld University, Germany
kjr@uni-bielefeld.de
</bodyText>
<sectionHeader confidence="0.986556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998655">
This paper describes the design and ac-
quisition of a German multimodal cor-
pus for the development and evaluation of
computational models for (grounded) lan-
guage acquisition and algorithms enabling
corresponding capabilities in robots. The
corpus contains parallel data from multi-
ple speakers/actors, including speech, vi-
sual data from different perspectives and
body posture data. The corpus is designed
to support the development and evalua-
tion of models learning rather complex
grounded linguistic structures, e.g. syn-
tactic patterns, from sub-symbolic input.
It provides moreover a valuable resource
for evaluating algorithms addressing sev-
eral other learning processes, e.g. concept
formation or acquisition of manipulation
skills. The corpus will be made available
to the public.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998308">
Children acquire linguistic structures through ex-
posure to (spoken) language in a rich context and
environment. The semantics of language may be
learned by establishing connections between lin-
guistic structures and corresponding structures in
the environment, i.e. in different domains such
as the visual one (Harnad, 1990). Both with re-
spect to modeling language acquisition in chil-
dren and with respect to enabling corresponding
language acquisition capabilities in robots, which
may ideally be also grounded in their environment,
it is hence of great interest to explore i) how lin-
guistic structures of different levels of complexity,
e.g. words or grammatical phrases, can be derived
from speech input, ii) how structured representa-
tions for entities observed in the environment can
be derived, e.g. how concepts and structured rep-
resentations of actions can be formed, and iii) how
connections can be established between structured
representations derived from different domains. In
order to gain insights concerning the mechanisms
at play during language acquisition (LA), which
enable children to solve these learning tasks, mod-
els are needed which ideally cover several learning
tasks. For instance, they may cover the acquisition
of both words and grammatical rules as well as the
acquisition of their grounded meanings. Comple-
mentarily, data resources are needed which enable
the design and evaluation of these models by pro-
viding suitable parallel data.
Aiming to provide a basis for the development
and evaluation of LA models addressing the ac-
quisition of rather complex and grounded linguis-
tic structures, i.e. syntactic patterns, from sub-
symbolic input, we designed a German multi-
modal input corpus. The corpus consists of data of
multiple speakers/actors who performed actions in
front of a robot and described these actions while
executing them. Subjects were recorded, i.e. par-
allel data of speech, stereo vision (including the
view-perspective of the “infant”/robot) and body
postures were gathered. The resulting data hence
allow grounding of linguistic structures in both
vision and body postures. Among others, learn-
ing processes that may be evaluated using the cor-
pus include: acquisition of several linguistic struc-
tures, acquisition of visual structures, concept for-
mation, acquisition of generalized patterns which
abstract over different speakers and actors, estab-
lishment of correspondences between structures
</bodyText>
<page confidence="0.978666">
30
</page>
<note confidence="0.9698495">
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 30–37,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999906666666667">
from different domains, acquisition of manipula-
tion skills, and development of appropriate models
for the representations of actions.
This paper is organized as follows. Next, we
will provide background information concerning
computational models of LA. In Section 3, we
will then describe the corpus design and acqui-
sition, including the desired properties of the
collected data, corresponding experimental set-
tings and technical implementation. We will then
present the resulting data set and subsequently
conclude..
</bodyText>
<sectionHeader confidence="0.974483" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999984512195123">
To date, several models addressing LA learning
tasks have been proposed and evaluated using dif-
ferent copora. Yet, these models typically focus on
a subset or certain aspects of the LA learning tasks
mentioned in the previous section, often assuming
other learning tasks, e.g. those of lower complex-
ity, as already solved by the learner. For instance,
models addressing the acquisition of grammatical
constructions and their meaning (Kwiatkowski et
al., 2012; Alishahi and Stevenson, 2008; Gaspers
and Cimiano, in press; Chang and Maia, 2001)
typically learn from symbolic input. In particu-
lar, assuming that the child is already able to seg-
ment a speech signal into a stream of words and to
extract structured representations from the visual
context, such models typically explore learning
from sequences of words and symbolic descrip-
tions of the non-linguistic context. Models ad-
dressing the acquisition of word-like units directly
from a speech signal (R¨as¨anen, 2011; R¨as¨anen et
al., 2009) have also been explored. These, how-
ever, typically do not address learning of more
complex linguistic structures/constructions.
Taken together, lexical acquisition from speech
and syntactic acquisition have been mainly stud-
ied independently of each other, often assuming
that syntactic acquisition follows from knowledge
of words. However, learning processes might ac-
tually be interleaved, and top-down learning pro-
cesses may play an important role in LA. For
instance, with respect to computational learn-
ing from symbolic input, it has been shown that
knowledge of syntax can facilitate word learning
(Yu, 2006). Children may, for instance, also make
use of syntactic cues during speech segmentation
and/or word learning, but models addressing lexi-
cal acquisition from speech have to date mainly ig-
nored syntax (R¨as¨anen, 2012). Models addressing
the acquisition of syntactic patterns directly from
speech provide a basis for exploring to what extent
learning mechanisms might be interleaved in early
LA. Moreover, they allow to investigate the pos-
sible role of several top-down learning processes
which have to date been little explored.
Several corpora comprising interactions of chil-
dren with their caregivers have been collected. A
large such resource is the CHILDES data base
(MacWhinney, 2000), which contains transcribed
speech. Data from CHILDES have been often
used to evaluate models learning from symbolic
input, in particular models for syntactic acquisi-
tion from sequences of words; additional accom-
panying symbolic context representations have
been often created (semi–)automatically. More-
over, multimodal corpora containing caregiver-
child interactions have been recorded and anno-
tated (Bj¨orkenstam and Wirn, 2013; Yu et al.,
2008), thus also allowing to study the role of social
interaction and extra-linguistic cues in language
learning. By contrast, in this work we aim to pro-
vide a basis for developing and evaluating models
which address the acquisition of syntactic patterns
from speech. Hence, allowing to derive general-
ized patterns, linguistic units as well as the objects
and actions they refer to have to re-appear in the
data several times. Thus, in line with the CARE-
GIVER corpus (Altosaar et al., 2010) we did not
record caregiver-child interactions but attempted
to approximate speech used by caregivers with re-
spect to the learning task(s) at hand. However,
the focus of the CAREGIVER corpus is on mod-
els learning word-like units from speech. Thus,
a number of keywords were spoken in different
carrier sentences; speech is accompanied by only
limited non-linguistic context information in the
corpus. In contrast to CAREGIVER, we did not
restrict language use directly and recorded paral-
lel context information from different modalities,
focusing not only on the acquisition of word-like
units from speech and word-to-object mapping but
moreover on the acquisition of simple syntactic
patterns and mapping language to actions.
</bodyText>
<sectionHeader confidence="0.716372" genericHeader="method">
3 Corpus design and acquisition
</sectionHeader>
<bodyText confidence="0.99996775">
In this section, we will first describe the desired
properties of the corpus. Subsequently, we will
present the corresponding experimental settings,
used stimuli and procedure, the technical imple-
</bodyText>
<page confidence="0.999327">
31
</page>
<bodyText confidence="0.999891">
mentation of the robot behavior and the data ac-
quisition as well as the resulting corpus.
</bodyText>
<subsectionHeader confidence="0.998675">
3.1 Desired properties
</subsectionHeader>
<bodyText confidence="0.999971826086957">
Our goal was to design a corpus comprising multi-
modal data which supports the evaluation of com-
putational models addressing several LA learn-
ing tasks, and in particular the acquisition of
grounded syntactic patterns from sub-symbolic in-
put only as well as the development of compo-
nents supporting the acquisition of language by
robots. Thus, the main focus was to design the
corpus in such a way that the data acquisition
scenario was simplified enough to allow solving
the task of learning grounded syntactic patterns
from sub-symbolic input with the resulting data
set (which of course contains much less data when
compared to the innumerable natural language ex-
amples children receive when acquiring language
over several years). In particular, since the ac-
quisition of rather complex structures should be
enabled using sub-symbolic information, several
(repeated) examples for contained structures were
needed, allowing the formation of generalized rep-
resentations. Thus, we opted for a rather sim-
ple scenario. Specifically, the following properties
were taken into account:
</bodyText>
<listItem confidence="0.988545772727273">
• Rather few objects and actions were included
that could moreover be differentiated rather
easily from a visual point of view. How-
ever, in order to reflect differences between
actions, these differed i) with respect to
the number of their referents as well as ii)
with respect to their specificity to certain
objects. In particular, we included actions
which could be performed on different sub-
sets of the objects, ranging from specificity
to one certain object to being executed with
all of the objects.
• Objects and actions reappeared several times,
yielding several examples for each of them.
Repeated appearance is an essential aspect,
since the formation of generalized represen-
tations starting from continuous input re-
quires several observations in order to allow
abstraction over observed examples/different
actors and speakers.
• The scenario was designed such that it en-
couraged human subjects to use rather simple
</listItem>
<bodyText confidence="0.979295761904762">
syntactic patterns/short sentences. Yet, lan-
guage use was in principle unrestricted in or-
der to acquire rather natural data and to cap-
ture speaker-dependent differences. This also
reflects the input children receive in that par-
ents use rather simple language when talking
to children.
• Data were gathered from several human sub-
jects in order to allow for the evaluation of
generalization over different speakers (with
different acoustic properties and different
language use, e.g. different words for ob-
jects, different syntactic patterns with differ-
ent complexity, etc) as well as over different
actors in case of actions, since children inter-
act with different people and are able to solve
this task. Moreover, generalization to dif-
ferent speakers/actors is also important with
respect to learning in artificial agents which
should preferably not be operable by a single
person only.
</bodyText>
<listItem confidence="0.9341695">
• Parallel data were gathered in which ob-
jects and actions were explicitly named when
they were used. This is an important as-
pect because the corpus should allow learn-
</listItem>
<bodyText confidence="0.9608126">
ing connections between vision, i.e. objects
and actions, and speech (segments) referring
to these objects/actions, i.e. (sequences of
words) and syntactic patterns. It reflects the
input children receive in that caregivers also
explain/show objects directly to their chil-
dren and may show them how to use ob-
jects/perform actions in front of them (Rolf
et al., 2009; Schillingmann et al., 2009).
We opted for the collection of parallel data con-
cerning vision and body postures for human tutors.
Hence, the corpus allows grounding of linguistic
structures in both vision and body postures. In-
cluding body postures moreover allows the eval-
uation of algorithms showing manipulation skills
which is of interest with respect to learning in
robots.
We used stereo vision to allow computational
learners to reliably track object movement and in-
teraction using both visual and depth information.
With respect to vision, four cameras with two dif-
ferent perspectives were used: two static external
cameras as well as the robot’s two internal moving
cameras. The latter basically mimics the “infant”
view, i.e. while the external cameras were static,
</bodyText>
<page confidence="0.996449">
32
</page>
<bodyText confidence="0.9998844">
the robot moved its eyes (and thus the cameras)
and focused on the tutor’s hand performing the ac-
tions, thus reflecting how a child may focus her/his
attention to the important aspects of a scene/a per-
formance of her/his caregiver.
</bodyText>
<subsectionHeader confidence="0.994248">
3.2 Participants
</subsectionHeader>
<bodyText confidence="0.999915333333333">
A total of 27 adult human subjects participated in
data collection (7 male, 20 female, mean age: 26).
Subjects were paid for their participation.
</bodyText>
<subsectionHeader confidence="0.99735">
3.3 Experimental setting
</subsectionHeader>
<bodyText confidence="0.9987918">
Human subjects performed pre-defined actions
and simultaneously described their performances
in front of the robot iCub (Metta et al., 2008); Fig.
1 depicts a human subject interacting with iCub.
While interacting with iCub, human subjects’ be-
</bodyText>
<figureCaption confidence="0.998783">
Figure 2: Experimental sketch.
</figureCaption>
<bodyText confidence="0.998377">
interacting with a human (child), iCub provided
feedback (Nagai and Rohlfing, 2009; Fischer et
al., 2011). In particular, a gazing behavior was
implemented to make the robot appear attentively
following the tutoring.
</bodyText>
<subsectionHeader confidence="0.888911">
3.4 Stimuli
</subsectionHeader>
<bodyText confidence="0.98969075">
Data were gathered in the framework of a toy
cooking scenario. In particular, subjects prepared
several dishes in front of iCub using toy objects.
Specifically, 21 toy objects were chosen such that
</bodyText>
<figureCaption confidence="0.997357">
Figure 1: A human subject interacting with iCub.
</figureCaption>
<bodyText confidence="0.9714295">
havior was recorded. In particular, the following
data were recorded simultaneously:
</bodyText>
<listItem confidence="0.999789714285714">
• Speech/Audio (via a headset microphone)
• Vision/Video, static perspective (via two
cameras, allowing for stereo vision)
• iCub-Vision/Video, iCub’s (attentive) per-
spective (via iCub’s two internal cameras,
again allowing for stereo vision)
• Body postures (via a Kinect).
</listItem>
<bodyText confidence="0.99983875">
An experimental sketch showing the experimental
setting including the positions of the human sub-
ject and iCub, as well as camera and Kinect po-
sitions, is illustrated in Fig. 2. As can be seen,
the human subject was placed directly opposite to
iCub. The two external cameras and the Kincet
were placed slightly sloped opposite to the sub-
ject. Subjects were instructed about which actions
should be performed via a computer screen which
was operated by an experimentator.
In order to encourage subjects to perform the tu-
toring task rather naturally, i.e. just like they were
</bodyText>
<figureCaption confidence="0.99937">
Figure 3: Utilized objects.
</figureCaption>
<bodyText confidence="0.999457588235294">
they were rather easy to differentiate with respect
to color and/or form. The chosen objects were:
pizza, pita bread, plate, bowl, spaghetti, pepper,
vinegar, red pepper, lettuce leafs, tomato, onion,
cucumber, cheese, toast, salami, chillies, egg, an-
chovy, cutting board, knife, and mushrooms. The
objects are depicted in Fig. 3. Moreover, six dif-
ferent actions were chosen which could be exe-
cuted using these objects. Again, the goal was
to support rather easy identification visually (with
respect to their trajectories). The chosen actions
were: showing an object, cutting an object (egg
or tomato) into two pieces (with knife), placing
an object onto another one (plate, pizza, cutting
board, toast), putting an object into another one
(bowl, pita bread), pour vinegar, and strew pep-
per. Thus, most actions were object-specific to a
</bodyText>
<page confidence="0.998147">
33
</page>
<bodyText confidence="0.9999090625">
certain degree, i.e. they were to be executed with
a certain subset of the objects each. The show ac-
tion was to be executed using each of the objects.
Furthermore, 20 different dishes, i.e. preparation
processes each consisting of a sequence of actions,
were created (four dishes including salad, pizza,
pita bread, spaghetti and sandwich/toast, respec-
tively). This was done in order to gather rather
fluent/consistent courses of action and rather flu-
ent communication in case of descriptions. For in-
stance, one sequence for preparing a salad started
as follows: showing bowl, showing lettuce leafs,
putting lettuce leafs into bowl, showing cutting
board, showing knife, showing tomato, putting
tomato onto cutting board, cutting tomato into two
pieces, putting tomato pieces into bowl, etc.
</bodyText>
<subsectionHeader confidence="0.831712">
3.5 Procedure
</subsectionHeader>
<bodyText confidence="0.996009">
Subjects first prepared one dish while not being
recorded in order to get familiar with the task.
They were instructed to perform presented actions
and to describe their performance simultaneously.
Moreover, they were asked to name objects and
actions explicitly, since a goal of the corpus is to
allow learning connections between speech, vision
and body postures. Subjects were not asked to
use particular words or phrases, but were free to
make own choices. For instance, when being ex-
posed to a picture of the pita bread, they were sup-
posed to explicitly name the pita bread. Yet, they
were free to choose a suitable word (or sequence
of words), e.g. “Pita”, “Pitatasche”, “Teigtasche”,
“D¨onertasche”, “Brottasche”, etc.
Actions to be performed were presented to the
subjects via a computer screen; either one action
was presented or – in most cases – two actions
were presented at once to be executed one after
another. In most cases two actions were presented
in order to gain more fluent communications and
courses of action. In no case more than two ac-
tions were presented together because we wanted
subjects to focus on performance and not on re-
membering a certain course of action. Actions
were presented only in the form of pictures in or-
der to elicit rather natural language use. In par-
ticular, as mentioned previously, subjects could
choose freely how to name objects and actions. An
example for a screen/picture showing two actions
</bodyText>
<figureCaption confidence="0.8912742">
to be performed one after another is presented in
Fig. 4. An experimentator operated the screen,
i.e. guided the subjects through the sequences of
Figure 4: Example screen showing the actions
show red pepper and put red pepper into bowl.
</figureCaption>
<bodyText confidence="0.72863075">
actions. Subjects participated for approximately
one hour; only subject’s actual performances were
recorded, yielding approximately 20–30 minutes
of usable material per subject.
</bodyText>
<subsectionHeader confidence="0.995187">
3.6 Robot behavior
</subsectionHeader>
<bodyText confidence="0.999974588235294">
As mentioned previously, a gazing behavior was
implemented to make the robot appear attentively
following the tutoring. In particular, the robot’s
gaze followed a subject’s presentation of an action
by gazing at her/his right wrist. At times when
subjects did not move their hands (to present ac-
tions) the robot was looking around, i.e. it gazed
at random targets. In the following, the implemen-
tation of the robot behavior will be described in
more detail.
The experimental setup shown in Fig. 2 allows the
system to observe a person in front of the robot
iCub. While the presentation task was performed
by the person, the robot was supposed to gaze at
the right wrist of this person. Via the Kinect data
it was possible to acquire the body posture of the
robot’s interaction partner. We extracted the loca-
tion of the wrist and represented the Cartesian po-
sition in the coordinate system of the robot. This
position was then used as the target to generate
the head and eye movements. The movement was
executed by the iKinGaze module available in the
iCub software repository (Pattacini, 2010).
Next to this “tracking” behavior of the robot we
also used a “background” behavior. The “back-
ground” behavior then drew randomly new tar-
gets xtarg(in meter) from the uniform distribution
Q E [−1.5, −1,5] x [−0.2,0.2] x [0.2, 0, 4] in
front of the robot. After convergence to the tar-
get the behavior waited for t = 3 seconds before
a new target was drawn. The switch from “back-
ground” behavior to “tracking” behavior was trig-
gered if new targets arrived from the Kinect-based
tracking component. This behavior stayed active
</bodyText>
<page confidence="0.99763">
34
</page>
<bodyText confidence="0.999981">
as long as targets were received. If no targets were
arriving during t = 2 sec. after the gazing con-
verged on the last target, the “background” behav-
ior took over. Due to the difference in distance
between targets, the motion duration was different
as well. Therefore, time delays were added to the
target generation, which resulted in a more natural
behavior of the robot gazing.
</bodyText>
<sectionHeader confidence="0.95578" genericHeader="method">
4 Acquired data
</sectionHeader>
<bodyText confidence="0.99968885">
In order to record synchronized data from the
external sensors, the robot system and the ex-
perimental control software, we utilized a ded-
icated framework for the acquisition of multi-
modal human-robot interaction data sets (Wienke
et al., 2012). The framework and the underly-
ing technology (Wienke and Wrede, 2011) allows
to directly capture the network communication of
robotics software components. Through this ap-
proach, system-internal data from the iCub such as
its proprioception and stereo cameras images can
be synchronously captured and transformed into
an RETF1-described log-file format with explicit
time and type information. Moreover, additional
recording devices such as the Kinect sensors, the
external pair of stereo cams or the audio input
from a close-talk microphone are captured directly
with this system and stored persistently. An exam-
ple of the acquired parallel data is provided by Fig.
5 while Table 6 summarizes the technical aspects
of the acquired data.
The applied framework also supports the auto-
matic export and conversion of synchronized parts
of the multimodal data set to common formats
used by other 3rd party tools such as the annota-
tion tool ELAN (Sloetjes and Wittenburg, 2008)
used for ground truth annotation of the acquired
corpus. In this experiment, we additionally cap-
tured the logical state of the experiment control
software which allowed us to efficiently post-
process the raw data and, e.g., automatically pro-
vide cropped video files containing only single ut-
terances. A logical state corresponds to the image
seen at the screen by a human subject at a certain
time, showing the action(s) to be performed.
The acquired corpus contains in total 11.45
hours / approx. 2.3 TB of multimodal input data
recorded in 27 trials. Each trial was recorded in
about 1 hour of wallclock time and cropped to 20–
30 minutes of effective parallel data. While in 5
</bodyText>
<footnote confidence="0.877541">
1Robot Engineering Task-Force, cf. http://retf.info/
</footnote>
<figure confidence="0.396789">
c) d)
</figure>
<figureCaption confidence="0.919618">
Figure 5: Example of acquired parallel data com-
</figureCaption>
<bodyText confidence="0.988456451612903">
prising a) visual data from two static cameras, b)
visual data from two cameras contained in the
robot’s eyes, c) audio and d) body posture data
recorded by the Kinect. In this example the sub-
ject is preparing a sandwich, and currently strew-
ing pepper onto it.
cases not all of the parallel data streams are avail-
able due to difficulties with the robot and the wire-
less microphones, we decided to leave this data
in the corpus to evaluate machine learning pro-
cesses addressing learning from one or a subset of
the modalities only, e.g. blind segmentation of a
speech stream.
From the data logs, we exported audio (in AAC
format) and the 4 synchronized video (with H.264
encoding) files (MP4 container format) for each
trial with an additional ELAN project file for an-
notation. This annotation is currently carried out;
a screenshot of acquired data and corresponding
annotations in ELAN is depicted in Fig. 7. It
comprises annotation of errors, as well as start-
ing and end points for both presented actions and
spoken utterances. In particular, in case of speech
word transcriptions are added, while in case of vi-
sion actions are annotated in the form of predicate
logic formulas. Hence, once the corpus is prepro-
cessed, it is also suitable for the evaluation of mod-
els learning from symbolic input with respect to
data from one or more domains. For instance, one
could explore the acquisition of syntactic patterns
from speech by providing parallel visual context
</bodyText>
<page confidence="0.996387">
35
</page>
<table confidence="0.940182">
# Device Description Data type Frequency Dimension Throughput
1 Cam 1 Scene video rst.vision.Image ≈ 30 Hz 640 × 480 × 3 ≈ 28 MB/s
2 Cam 2 Scene video rst.vision.Image ≈ 30 Hz 640 × 480 × 3 ≈ 28 MB/s
3 Mic 1 Speech rst.audition.SoundChunk ≈ 50 kHz 1-2 ≈ 0.5 MB/s
</table>
<figure confidence="0.81853875">
4 iCub Cam 1 Ego left bottle/yarp::sig::Image ≈ 30 Hz 320 × 240 × 3 ≈ 7 MB/s
5 iCub Cam 2 Ego right bottle/yarp::sig::Image ≈ 30 Hz 320 × 240 × 3 ≈ 7 MB/s
6 Kinect Body posture TrackedPosture3DFloat2 ≈ 30 Hz 36 ≈ 6 kB/s
7 Control Logical state string ≈ 0.05 Hz - ≈ 5 B/s
</figure>
<figureCaption confidence="0.975281666666667">
Figure 6: Description of acquired data streams, type specifications, average frequency, data dimension
and throughput as measured during recording.
information either in sub-symbolic form or in the
form of predicate logic formulas.
Figure 7: Example of acquired data and corre-
sponding annotations in ELAN.
</figureCaption>
<bodyText confidence="0.999692933333333">
Word transcriptions for utterances for the whole
data set are not yet available. According to the ex-
perimentators’ impressions, most subjects indeed
used, as desired, rather short sentences. Further-
more, a few subjects tried to vary their linguis-
tic descriptions, i.e. to use different sentences
for each description. Thus, the corpus appears
to cover not only several examples of rather sim-
ple linguistic constructions with variations across
speakers, but moreover input examples with a
rather large degree of linguistic variation for a sin-
gle speaker, hence providing examples of more
challenging data.
We will make the corpus available to the public
once post-processing is completely finished.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999954529411765">
In this paper, we have described the design and
acquisition of a German multimodal data set for
the development and evaluation of grounded lan-
guage acquisition models and algorithms enabling
corresponding abilities in robots. The corpus con-
tains parallel data including speech, visual data
from four different cameras with different per-
spectives and body posture data from multiple
speakers/actors. Among others, learning pro-
cesses that may be evaluated using the corpus in-
clude: acquisition of several linguistic structures,
acquisition of visual structures, concept forma-
tion, acquisition of generalized patterns which ab-
stract over different speakers and actors, establish-
ment of correspondences between structures from
different domains and acquisition of manipulation
skills.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999825727272727">
We are deeply grateful to Jan Moringen, Michael
G¨otting and Stefan Kr¨uger for providing techni-
cal support. We wish to thank Luci Filinger,
Christina Lehwalder, Anne Nemeth and Frederike
Strunz for support in data collection and annota-
tion. This work has been funded by the German
Research Foundation DFG within the Collabora-
tive Research Center 673 Alignment in Communi-
cation and the Center of Excellence Cognitive In-
teraction Technology. Andre Lemme is funded by
FP7 under GA. No. 248311-AMARSi.
</bodyText>
<sectionHeader confidence="0.998912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995233">
Afra Alishahi and Suzanne Stevenson. 2008. A com-
putational model of early argument structure acqui-
sition. Cognitive Science, 32(5):789–834.
Toomas Altosaar, Louis ten Bosch, Guillaume Aimetti,
Christos Koniaris, Kris Demuynck, and Henk
van den Heuvel. 2010. A speech corpus for mod-
eling language acquisition: Caregiver. In Proceed-
</reference>
<page confidence="0.991163">
36
</page>
<reference confidence="0.995663465909091">
ings of the International Conference on Language
Resources and Evaluation.
Kristina Nilsson Bj¨orkenstam and Mats Wirn. 2013.
Multimodal annotation of parent-child interaction
in a free-play setting. In Proceedings of the Thir-
teenth International Conference on Intelligent Vir-
tual Agents.
Nancy C. Chang and Tiago V. Maia. 2001. Learn-
ing grammatical constructions. In Proceedings of
the 23rd Cognitive Science Society Conference.
Kerstin Fischer, Kilian Foth, Katharina J. Rohlfing,
and Britta Wrede. 2011. Mindful tutors: Linguis-
tic choice and action demonstration in speech to in-
fants and to a simulated robot. Interaction Studies,
12(1):134–161.
Judith Gaspers and Philipp Cimiano. in press. A
computational model for the item-based induction of
construction networks. Cognitive Science.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D: Nonlinear Phenomena, 42(1-3):335–
346.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-
moyer, and Mark Steedman. 2012. A probabilis-
tic model of syntactic and semantic acquisition from
child-directed utterances and their meanings. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for analyzing talk. Mahwah, NJ.
Giorgio Metta, Giulio Sandini, David Vernon, Lorenzo
Natale, and Francesco Nori. 2008. The iCub hu-
manoid robot: an open platform for research in em-
bodied cognition. In Proceedings of the 8th Work-
shop on Performance Metrics for Intelligent Sys-
tems, pages 50–56, New York, NY. ACM.
Yukie Nagai and Katharina J. Rohlfing. 2009. Com-
putational analysis of motionese toward scaffolding
robot action learning. IEEE Transactions on Au-
tonomous Mental Development, 1:44–54.
Ugo Pattacini. 2010. Modular Cartesian Controllers
for Humanoid Robots: Design and Implementation
on the iCub. Ph.D. thesis, RBCS, Istituto Italiano di
Tecnologia, Genova.
Okko R¨as¨anen, Unto K. Laine, and Toomas Altosaar.
2009. Computational language acquisition by sta-
tistical bottom-up processing. In Proceedings Inter-
speech.
Okko R¨as¨anen. 2011. A computational model of word
segmentation from continuous speech using transi-
tional probabilities of atomic acoustic events. Cog-
nition, 120:149176.
Okko R¨as¨anen. 2012. Computational modeling of
phonetic and lexical learning in early language ac-
quisition: existing models and future directions.
Speech Communication, 54:975–997.
Matthias Rolf, Marc Hanheide, and Katharina J. Rohlf-
ing. 2009. Attention via synchrony. making use of
multimodal cues in social learning. IEEE Transac-
tions on Autonomous Mental Development, 1:55–67.
Lars Schillingmann, Britta Wrede, and Katharina J.
Rohlfing. 2009. A computational model of acous-
tic packaging. IEEE Transactions on Autonomous
Mental Development, 1:226–237.
Han Sloetjes and Peter Wittenburg. 2008. Annota-
tion by category: Elan and iso dcr. In Proceed-
ings of the International Conference on Language
Resources and Evaluation.
Johannes Wienke and Sebastian Wrede. 2011. A Mid-
dleware for Collaborative Research in Experimen-
tal Robotics. In IEEE/SICE International Sympo-
sium on System Integration (SII2011), Kyoto, Japan.
IEEE.
Johannes Wienke, David Klotz, and Sebastian Wrede.
2012. A Framework for the Acquisition of Multi-
modal Human-Robot Interaction Data Sets with a
Whole-System Perspective. In LREC Workshop on
Multimodal Corpora for Machine Learning: How
should multimodal corpora deal with the situation?,
Istanbul, Turkey.
Chen Yu, Linda B. Smith, and Alfredo F. Pereira. 2008.
Grounding word learning in multimodal sensorimo-
tor interaction. In Proceedings of the 30th Annual
Conference of the Cognitive Science Society.
Chen Yu. 2006. Learning syntax-semantics mappings
to bootstrap word learning. In Proceedings of the
28th Annual Conference of the Cognitive Science
Society (2006) Key: citeulike:5276016.
</reference>
<page confidence="0.999606">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.861478">
<title confidence="0.9845535">A multimodal corpus for the evaluation of computational models (grounded) language acquisition</title>
<author confidence="0.9534505">Maximilian Andre Philipp J Sebastian</author>
<affiliation confidence="0.999477">Computing Group, CITEC, Bielefeld University, Institute for Cognition and Robotics (CoR-Lab), Bielefeld University, Semantics Group, CITEC, Bielefeld University,</affiliation>
<email confidence="0.995155">kjr@uni-bielefeld.de</email>
<abstract confidence="0.999114095238095">This paper describes the design and acquisition of a German multimodal corpus for the development and evaluation of computational models for (grounded) language acquisition and algorithms enabling corresponding capabilities in robots. The corpus contains parallel data from multiple speakers/actors, including speech, visual data from different perspectives and body posture data. The corpus is designed to support the development and evaluation of models learning rather complex grounded linguistic structures, e.g. syntactic patterns, from sub-symbolic input. It provides moreover a valuable resource for evaluating algorithms addressing several other learning processes, e.g. concept formation or acquisition of manipulation skills. The corpus will be made available to the public.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Afra Alishahi</author>
<author>Suzanne Stevenson</author>
</authors>
<title>A computational model of early argument structure acquisition.</title>
<date>2008</date>
<journal>Cognitive Science,</journal>
<volume>32</volume>
<issue>5</issue>
<contexts>
<context position="5011" citStr="Alishahi and Stevenson, 2008" startWordPosition="710" endWordPosition="713">ng experimental settings and technical implementation. We will then present the resulting data set and subsequently conclude.. 2 Background To date, several models addressing LA learning tasks have been proposed and evaluated using different copora. Yet, these models typically focus on a subset or certain aspects of the LA learning tasks mentioned in the previous section, often assuming other learning tasks, e.g. those of lower complexity, as already solved by the learner. For instance, models addressing the acquisition of grammatical constructions and their meaning (Kwiatkowski et al., 2012; Alishahi and Stevenson, 2008; Gaspers and Cimiano, in press; Chang and Maia, 2001) typically learn from symbolic input. In particular, assuming that the child is already able to segment a speech signal into a stream of words and to extract structured representations from the visual context, such models typically explore learning from sequences of words and symbolic descriptions of the non-linguistic context. Models addressing the acquisition of word-like units directly from a speech signal (R¨as¨anen, 2011; R¨as¨anen et al., 2009) have also been explored. These, however, typically do not address learning of more complex </context>
</contexts>
<marker>Alishahi, Stevenson, 2008</marker>
<rawString>Afra Alishahi and Suzanne Stevenson. 2008. A computational model of early argument structure acquisition. Cognitive Science, 32(5):789–834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toomas Altosaar</author>
<author>Louis ten Bosch</author>
<author>Guillaume Aimetti</author>
<author>Christos Koniaris</author>
<author>Kris Demuynck</author>
<author>Henk van den Heuvel</author>
</authors>
<title>A speech corpus for modeling language acquisition: Caregiver.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation.</booktitle>
<marker>Altosaar, Bosch, Aimetti, Koniaris, Demuynck, van den Heuvel, 2010</marker>
<rawString>Toomas Altosaar, Louis ten Bosch, Guillaume Aimetti, Christos Koniaris, Kris Demuynck, and Henk van den Heuvel. 2010. A speech corpus for modeling language acquisition: Caregiver. In Proceedings of the International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Nilsson Bj¨orkenstam</author>
<author>Mats Wirn</author>
</authors>
<title>Multimodal annotation of parent-child interaction in a free-play setting.</title>
<date>2013</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Intelligent Virtual Agents.</booktitle>
<marker>Bj¨orkenstam, Wirn, 2013</marker>
<rawString>Kristina Nilsson Bj¨orkenstam and Mats Wirn. 2013. Multimodal annotation of parent-child interaction in a free-play setting. In Proceedings of the Thirteenth International Conference on Intelligent Virtual Agents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy C Chang</author>
<author>Tiago V Maia</author>
</authors>
<title>Learning grammatical constructions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 23rd Cognitive Science Society Conference.</booktitle>
<contexts>
<context position="5065" citStr="Chang and Maia, 2001" startWordPosition="719" endWordPosition="722">l then present the resulting data set and subsequently conclude.. 2 Background To date, several models addressing LA learning tasks have been proposed and evaluated using different copora. Yet, these models typically focus on a subset or certain aspects of the LA learning tasks mentioned in the previous section, often assuming other learning tasks, e.g. those of lower complexity, as already solved by the learner. For instance, models addressing the acquisition of grammatical constructions and their meaning (Kwiatkowski et al., 2012; Alishahi and Stevenson, 2008; Gaspers and Cimiano, in press; Chang and Maia, 2001) typically learn from symbolic input. In particular, assuming that the child is already able to segment a speech signal into a stream of words and to extract structured representations from the visual context, such models typically explore learning from sequences of words and symbolic descriptions of the non-linguistic context. Models addressing the acquisition of word-like units directly from a speech signal (R¨as¨anen, 2011; R¨as¨anen et al., 2009) have also been explored. These, however, typically do not address learning of more complex linguistic structures/constructions. Taken together, l</context>
</contexts>
<marker>Chang, Maia, 2001</marker>
<rawString>Nancy C. Chang and Tiago V. Maia. 2001. Learning grammatical constructions. In Proceedings of the 23rd Cognitive Science Society Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kerstin Fischer</author>
<author>Kilian Foth</author>
<author>Katharina J Rohlfing</author>
<author>Britta Wrede</author>
</authors>
<title>Mindful tutors: Linguistic choice and action demonstration in speech to infants and to a simulated robot.</title>
<date>2011</date>
<journal>Interaction Studies,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="13826" citStr="Fischer et al., 2011" startWordPosition="2073" endWordPosition="2076">spects of a scene/a performance of her/his caregiver. 3.2 Participants A total of 27 adult human subjects participated in data collection (7 male, 20 female, mean age: 26). Subjects were paid for their participation. 3.3 Experimental setting Human subjects performed pre-defined actions and simultaneously described their performances in front of the robot iCub (Metta et al., 2008); Fig. 1 depicts a human subject interacting with iCub. While interacting with iCub, human subjects’ beFigure 2: Experimental sketch. interacting with a human (child), iCub provided feedback (Nagai and Rohlfing, 2009; Fischer et al., 2011). In particular, a gazing behavior was implemented to make the robot appear attentively following the tutoring. 3.4 Stimuli Data were gathered in the framework of a toy cooking scenario. In particular, subjects prepared several dishes in front of iCub using toy objects. Specifically, 21 toy objects were chosen such that Figure 1: A human subject interacting with iCub. havior was recorded. In particular, the following data were recorded simultaneously: • Speech/Audio (via a headset microphone) • Vision/Video, static perspective (via two cameras, allowing for stereo vision) • iCub-Vision/Video, </context>
</contexts>
<marker>Fischer, Foth, Rohlfing, Wrede, 2011</marker>
<rawString>Kerstin Fischer, Kilian Foth, Katharina J. Rohlfing, and Britta Wrede. 2011. Mindful tutors: Linguistic choice and action demonstration in speech to infants and to a simulated robot. Interaction Studies, 12(1):134–161.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Judith Gaspers</author>
<author>Philipp Cimiano</author>
</authors>
<title>in press. A computational model for the item-based induction of construction networks.</title>
<journal>Cognitive Science.</journal>
<marker>Gaspers, Cimiano, </marker>
<rawString>Judith Gaspers and Philipp Cimiano. in press. A computational model for the item-based induction of construction networks. Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stevan Harnad</author>
</authors>
<title>The symbol grounding problem. Physica D: Nonlinear Phenomena,</title>
<date>1990</date>
<pages>42--1</pages>
<contexts>
<context position="1675" citStr="Harnad, 1990" startWordPosition="214" endWordPosition="215">, e.g. syntactic patterns, from sub-symbolic input. It provides moreover a valuable resource for evaluating algorithms addressing several other learning processes, e.g. concept formation or acquisition of manipulation skills. The corpus will be made available to the public. 1 Introduction Children acquire linguistic structures through exposure to (spoken) language in a rich context and environment. The semantics of language may be learned by establishing connections between linguistic structures and corresponding structures in the environment, i.e. in different domains such as the visual one (Harnad, 1990). Both with respect to modeling language acquisition in children and with respect to enabling corresponding language acquisition capabilities in robots, which may ideally be also grounded in their environment, it is hence of great interest to explore i) how linguistic structures of different levels of complexity, e.g. words or grammatical phrases, can be derived from speech input, ii) how structured representations for entities observed in the environment can be derived, e.g. how concepts and structured representations of actions can be formed, and iii) how connections can be established betwe</context>
</contexts>
<marker>Harnad, 1990</marker>
<rawString>Stevan Harnad. 1990. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335– 346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Sharon Goldwater</author>
<author>Luke Zettlemoyer</author>
<author>Mark Steedman</author>
</authors>
<title>A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4981" citStr="Kwiatkowski et al., 2012" startWordPosition="706" endWordPosition="709">ollected data, corresponding experimental settings and technical implementation. We will then present the resulting data set and subsequently conclude.. 2 Background To date, several models addressing LA learning tasks have been proposed and evaluated using different copora. Yet, these models typically focus on a subset or certain aspects of the LA learning tasks mentioned in the previous section, often assuming other learning tasks, e.g. those of lower complexity, as already solved by the learner. For instance, models addressing the acquisition of grammatical constructions and their meaning (Kwiatkowski et al., 2012; Alishahi and Stevenson, 2008; Gaspers and Cimiano, in press; Chang and Maia, 2001) typically learn from symbolic input. In particular, assuming that the child is already able to segment a speech signal into a stream of words and to extract structured representations from the visual context, such models typically explore learning from sequences of words and symbolic descriptions of the non-linguistic context. Models addressing the acquisition of word-like units directly from a speech signal (R¨as¨anen, 2011; R¨as¨anen et al., 2009) have also been explored. These, however, typically do not add</context>
</contexts>
<marker>Kwiatkowski, Goldwater, Zettlemoyer, Steedman, 2012</marker>
<rawString>Tom Kwiatkowski, Sharon Goldwater, Luke Zettlemoyer, and Mark Steedman. 2012. A probabilistic model of syntactic and semantic acquisition from child-directed utterances and their meanings. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for analyzing talk.</title>
<date>2000</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="6821" citStr="MacWhinney, 2000" startWordPosition="986" endWordPosition="987">ech segmentation and/or word learning, but models addressing lexical acquisition from speech have to date mainly ignored syntax (R¨as¨anen, 2012). Models addressing the acquisition of syntactic patterns directly from speech provide a basis for exploring to what extent learning mechanisms might be interleaved in early LA. Moreover, they allow to investigate the possible role of several top-down learning processes which have to date been little explored. Several corpora comprising interactions of children with their caregivers have been collected. A large such resource is the CHILDES data base (MacWhinney, 2000), which contains transcribed speech. Data from CHILDES have been often used to evaluate models learning from symbolic input, in particular models for syntactic acquisition from sequences of words; additional accompanying symbolic context representations have been often created (semi–)automatically. Moreover, multimodal corpora containing caregiverchild interactions have been recorded and annotated (Bj¨orkenstam and Wirn, 2013; Yu et al., 2008), thus also allowing to study the role of social interaction and extra-linguistic cues in language learning. By contrast, in this work we aim to provide </context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>Brian MacWhinney. 2000. The CHILDES Project: Tools for analyzing talk. Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Metta</author>
<author>Giulio Sandini</author>
<author>David Vernon</author>
<author>Lorenzo Natale</author>
<author>Francesco Nori</author>
</authors>
<title>The iCub humanoid robot: an open platform for research in embodied cognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems,</booktitle>
<pages>50--56</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="13587" citStr="Metta et al., 2008" startWordPosition="2037" endWordPosition="2040">ant” view, i.e. while the external cameras were static, 32 the robot moved its eyes (and thus the cameras) and focused on the tutor’s hand performing the actions, thus reflecting how a child may focus her/his attention to the important aspects of a scene/a performance of her/his caregiver. 3.2 Participants A total of 27 adult human subjects participated in data collection (7 male, 20 female, mean age: 26). Subjects were paid for their participation. 3.3 Experimental setting Human subjects performed pre-defined actions and simultaneously described their performances in front of the robot iCub (Metta et al., 2008); Fig. 1 depicts a human subject interacting with iCub. While interacting with iCub, human subjects’ beFigure 2: Experimental sketch. interacting with a human (child), iCub provided feedback (Nagai and Rohlfing, 2009; Fischer et al., 2011). In particular, a gazing behavior was implemented to make the robot appear attentively following the tutoring. 3.4 Stimuli Data were gathered in the framework of a toy cooking scenario. In particular, subjects prepared several dishes in front of iCub using toy objects. Specifically, 21 toy objects were chosen such that Figure 1: A human subject interacting w</context>
</contexts>
<marker>Metta, Sandini, Vernon, Natale, Nori, 2008</marker>
<rawString>Giorgio Metta, Giulio Sandini, David Vernon, Lorenzo Natale, and Francesco Nori. 2008. The iCub humanoid robot: an open platform for research in embodied cognition. In Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems, pages 50–56, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yukie Nagai</author>
<author>Katharina J Rohlfing</author>
</authors>
<title>Computational analysis of motionese toward scaffolding robot action learning.</title>
<date>2009</date>
<journal>IEEE Transactions on Autonomous Mental Development,</journal>
<pages>1--44</pages>
<contexts>
<context position="13803" citStr="Nagai and Rohlfing, 2009" startWordPosition="2069" endWordPosition="2072">tention to the important aspects of a scene/a performance of her/his caregiver. 3.2 Participants A total of 27 adult human subjects participated in data collection (7 male, 20 female, mean age: 26). Subjects were paid for their participation. 3.3 Experimental setting Human subjects performed pre-defined actions and simultaneously described their performances in front of the robot iCub (Metta et al., 2008); Fig. 1 depicts a human subject interacting with iCub. While interacting with iCub, human subjects’ beFigure 2: Experimental sketch. interacting with a human (child), iCub provided feedback (Nagai and Rohlfing, 2009; Fischer et al., 2011). In particular, a gazing behavior was implemented to make the robot appear attentively following the tutoring. 3.4 Stimuli Data were gathered in the framework of a toy cooking scenario. In particular, subjects prepared several dishes in front of iCub using toy objects. Specifically, 21 toy objects were chosen such that Figure 1: A human subject interacting with iCub. havior was recorded. In particular, the following data were recorded simultaneously: • Speech/Audio (via a headset microphone) • Vision/Video, static perspective (via two cameras, allowing for stereo vision</context>
</contexts>
<marker>Nagai, Rohlfing, 2009</marker>
<rawString>Yukie Nagai and Katharina J. Rohlfing. 2009. Computational analysis of motionese toward scaffolding robot action learning. IEEE Transactions on Autonomous Mental Development, 1:44–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ugo Pattacini</author>
</authors>
<title>Modular Cartesian Controllers for Humanoid Robots:</title>
<date>2010</date>
<booktitle>Design and Implementation on the iCub. Ph.D. thesis, RBCS, Istituto Italiano di Tecnologia,</booktitle>
<location>Genova.</location>
<contexts>
<context position="19780" citStr="Pattacini, 2010" startWordPosition="3033" endWordPosition="3034">tup shown in Fig. 2 allows the system to observe a person in front of the robot iCub. While the presentation task was performed by the person, the robot was supposed to gaze at the right wrist of this person. Via the Kinect data it was possible to acquire the body posture of the robot’s interaction partner. We extracted the location of the wrist and represented the Cartesian position in the coordinate system of the robot. This position was then used as the target to generate the head and eye movements. The movement was executed by the iKinGaze module available in the iCub software repository (Pattacini, 2010). Next to this “tracking” behavior of the robot we also used a “background” behavior. The “background” behavior then drew randomly new targets xtarg(in meter) from the uniform distribution Q E [−1.5, −1,5] x [−0.2,0.2] x [0.2, 0, 4] in front of the robot. After convergence to the target the behavior waited for t = 3 seconds before a new target was drawn. The switch from “background” behavior to “tracking” behavior was triggered if new targets arrived from the Kinect-based tracking component. This behavior stayed active 34 as long as targets were received. If no targets were arriving during t =</context>
</contexts>
<marker>Pattacini, 2010</marker>
<rawString>Ugo Pattacini. 2010. Modular Cartesian Controllers for Humanoid Robots: Design and Implementation on the iCub. Ph.D. thesis, RBCS, Istituto Italiano di Tecnologia, Genova.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okko R¨as¨anen</author>
<author>Unto K Laine</author>
<author>Toomas Altosaar</author>
</authors>
<title>Computational language acquisition by statistical bottom-up processing.</title>
<date>2009</date>
<booktitle>In Proceedings Interspeech.</booktitle>
<marker>R¨as¨anen, Laine, Altosaar, 2009</marker>
<rawString>Okko R¨as¨anen, Unto K. Laine, and Toomas Altosaar. 2009. Computational language acquisition by statistical bottom-up processing. In Proceedings Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okko R¨as¨anen</author>
</authors>
<title>A computational model of word segmentation from continuous speech using transitional probabilities of atomic acoustic events.</title>
<date>2011</date>
<journal>Cognition,</journal>
<pages>120--149176</pages>
<marker>R¨as¨anen, 2011</marker>
<rawString>Okko R¨as¨anen. 2011. A computational model of word segmentation from continuous speech using transitional probabilities of atomic acoustic events. Cognition, 120:149176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okko R¨as¨anen</author>
</authors>
<title>Computational modeling of phonetic and lexical learning in early language acquisition: existing models and future directions.</title>
<date>2012</date>
<journal>Speech Communication,</journal>
<pages>54--975</pages>
<marker>R¨as¨anen, 2012</marker>
<rawString>Okko R¨as¨anen. 2012. Computational modeling of phonetic and lexical learning in early language acquisition: existing models and future directions. Speech Communication, 54:975–997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Rolf</author>
<author>Marc Hanheide</author>
<author>Katharina J Rohlfing</author>
</authors>
<title>Attention via synchrony. making use of multimodal cues in social learning.</title>
<date>2009</date>
<journal>IEEE Transactions on Autonomous Mental Development,</journal>
<pages>1--55</pages>
<contexts>
<context position="12249" citStr="Rolf et al., 2009" startWordPosition="1826" endWordPosition="1829">n artificial agents which should preferably not be operable by a single person only. • Parallel data were gathered in which objects and actions were explicitly named when they were used. This is an important aspect because the corpus should allow learning connections between vision, i.e. objects and actions, and speech (segments) referring to these objects/actions, i.e. (sequences of words) and syntactic patterns. It reflects the input children receive in that caregivers also explain/show objects directly to their children and may show them how to use objects/perform actions in front of them (Rolf et al., 2009; Schillingmann et al., 2009). We opted for the collection of parallel data concerning vision and body postures for human tutors. Hence, the corpus allows grounding of linguistic structures in both vision and body postures. Including body postures moreover allows the evaluation of algorithms showing manipulation skills which is of interest with respect to learning in robots. We used stereo vision to allow computational learners to reliably track object movement and interaction using both visual and depth information. With respect to vision, four cameras with two different perspectives were use</context>
</contexts>
<marker>Rolf, Hanheide, Rohlfing, 2009</marker>
<rawString>Matthias Rolf, Marc Hanheide, and Katharina J. Rohlfing. 2009. Attention via synchrony. making use of multimodal cues in social learning. IEEE Transactions on Autonomous Mental Development, 1:55–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Schillingmann</author>
<author>Britta Wrede</author>
<author>Katharina J Rohlfing</author>
</authors>
<title>A computational model of acoustic packaging.</title>
<date>2009</date>
<journal>IEEE Transactions on Autonomous Mental Development,</journal>
<pages>1--226</pages>
<contexts>
<context position="12278" citStr="Schillingmann et al., 2009" startWordPosition="1830" endWordPosition="1833"> which should preferably not be operable by a single person only. • Parallel data were gathered in which objects and actions were explicitly named when they were used. This is an important aspect because the corpus should allow learning connections between vision, i.e. objects and actions, and speech (segments) referring to these objects/actions, i.e. (sequences of words) and syntactic patterns. It reflects the input children receive in that caregivers also explain/show objects directly to their children and may show them how to use objects/perform actions in front of them (Rolf et al., 2009; Schillingmann et al., 2009). We opted for the collection of parallel data concerning vision and body postures for human tutors. Hence, the corpus allows grounding of linguistic structures in both vision and body postures. Including body postures moreover allows the evaluation of algorithms showing manipulation skills which is of interest with respect to learning in robots. We used stereo vision to allow computational learners to reliably track object movement and interaction using both visual and depth information. With respect to vision, four cameras with two different perspectives were used: two static external camera</context>
</contexts>
<marker>Schillingmann, Wrede, Rohlfing, 2009</marker>
<rawString>Lars Schillingmann, Britta Wrede, and Katharina J. Rohlfing. 2009. A computational model of acoustic packaging. IEEE Transactions on Autonomous Mental Development, 1:226–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Han Sloetjes</author>
<author>Peter Wittenburg</author>
</authors>
<title>Annotation by category: Elan and iso dcr.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="21920" citStr="Sloetjes and Wittenburg, 2008" startWordPosition="3381" endWordPosition="3384">file format with explicit time and type information. Moreover, additional recording devices such as the Kinect sensors, the external pair of stereo cams or the audio input from a close-talk microphone are captured directly with this system and stored persistently. An example of the acquired parallel data is provided by Fig. 5 while Table 6 summarizes the technical aspects of the acquired data. The applied framework also supports the automatic export and conversion of synchronized parts of the multimodal data set to common formats used by other 3rd party tools such as the annotation tool ELAN (Sloetjes and Wittenburg, 2008) used for ground truth annotation of the acquired corpus. In this experiment, we additionally captured the logical state of the experiment control software which allowed us to efficiently postprocess the raw data and, e.g., automatically provide cropped video files containing only single utterances. A logical state corresponds to the image seen at the screen by a human subject at a certain time, showing the action(s) to be performed. The acquired corpus contains in total 11.45 hours / approx. 2.3 TB of multimodal input data recorded in 27 trials. Each trial was recorded in about 1 hour of wall</context>
</contexts>
<marker>Sloetjes, Wittenburg, 2008</marker>
<rawString>Han Sloetjes and Peter Wittenburg. 2008. Annotation by category: Elan and iso dcr. In Proceedings of the International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Wienke</author>
<author>Sebastian Wrede</author>
</authors>
<title>A Middleware for Collaborative Research in Experimental Robotics.</title>
<date>2011</date>
<booktitle>In IEEE/SICE International Symposium on System Integration (SII2011),</booktitle>
<publisher>IEEE.</publisher>
<location>Kyoto, Japan.</location>
<contexts>
<context position="21018" citStr="Wienke and Wrede, 2011" startWordPosition="3241" endWordPosition="3244">he gazing converged on the last target, the “background” behavior took over. Due to the difference in distance between targets, the motion duration was different as well. Therefore, time delays were added to the target generation, which resulted in a more natural behavior of the robot gazing. 4 Acquired data In order to record synchronized data from the external sensors, the robot system and the experimental control software, we utilized a dedicated framework for the acquisition of multimodal human-robot interaction data sets (Wienke et al., 2012). The framework and the underlying technology (Wienke and Wrede, 2011) allows to directly capture the network communication of robotics software components. Through this approach, system-internal data from the iCub such as its proprioception and stereo cameras images can be synchronously captured and transformed into an RETF1-described log-file format with explicit time and type information. Moreover, additional recording devices such as the Kinect sensors, the external pair of stereo cams or the audio input from a close-talk microphone are captured directly with this system and stored persistently. An example of the acquired parallel data is provided by Fig. 5 </context>
</contexts>
<marker>Wienke, Wrede, 2011</marker>
<rawString>Johannes Wienke and Sebastian Wrede. 2011. A Middleware for Collaborative Research in Experimental Robotics. In IEEE/SICE International Symposium on System Integration (SII2011), Kyoto, Japan. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Wienke</author>
<author>David Klotz</author>
<author>Sebastian Wrede</author>
</authors>
<title>A Framework for the Acquisition of Multimodal Human-Robot Interaction Data Sets with a Whole-System Perspective.</title>
<date>2012</date>
<booktitle>In LREC Workshop on Multimodal Corpora for Machine Learning: How</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="20948" citStr="Wienke et al., 2012" startWordPosition="3230" endWordPosition="3233">ere received. If no targets were arriving during t = 2 sec. after the gazing converged on the last target, the “background” behavior took over. Due to the difference in distance between targets, the motion duration was different as well. Therefore, time delays were added to the target generation, which resulted in a more natural behavior of the robot gazing. 4 Acquired data In order to record synchronized data from the external sensors, the robot system and the experimental control software, we utilized a dedicated framework for the acquisition of multimodal human-robot interaction data sets (Wienke et al., 2012). The framework and the underlying technology (Wienke and Wrede, 2011) allows to directly capture the network communication of robotics software components. Through this approach, system-internal data from the iCub such as its proprioception and stereo cameras images can be synchronously captured and transformed into an RETF1-described log-file format with explicit time and type information. Moreover, additional recording devices such as the Kinect sensors, the external pair of stereo cams or the audio input from a close-talk microphone are captured directly with this system and stored persist</context>
</contexts>
<marker>Wienke, Klotz, Wrede, 2012</marker>
<rawString>Johannes Wienke, David Klotz, and Sebastian Wrede. 2012. A Framework for the Acquisition of Multimodal Human-Robot Interaction Data Sets with a Whole-System Perspective. In LREC Workshop on Multimodal Corpora for Machine Learning: How should multimodal corpora deal with the situation?, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
<author>Linda B Smith</author>
<author>Alfredo F Pereira</author>
</authors>
<title>Grounding word learning in multimodal sensorimotor interaction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="7268" citStr="Yu et al., 2008" startWordPosition="1046" endWordPosition="1049">xplored. Several corpora comprising interactions of children with their caregivers have been collected. A large such resource is the CHILDES data base (MacWhinney, 2000), which contains transcribed speech. Data from CHILDES have been often used to evaluate models learning from symbolic input, in particular models for syntactic acquisition from sequences of words; additional accompanying symbolic context representations have been often created (semi–)automatically. Moreover, multimodal corpora containing caregiverchild interactions have been recorded and annotated (Bj¨orkenstam and Wirn, 2013; Yu et al., 2008), thus also allowing to study the role of social interaction and extra-linguistic cues in language learning. By contrast, in this work we aim to provide a basis for developing and evaluating models which address the acquisition of syntactic patterns from speech. Hence, allowing to derive generalized patterns, linguistic units as well as the objects and actions they refer to have to re-appear in the data several times. Thus, in line with the CAREGIVER corpus (Altosaar et al., 2010) we did not record caregiver-child interactions but attempted to approximate speech used by caregivers with respect</context>
</contexts>
<marker>Yu, Smith, Pereira, 2008</marker>
<rawString>Chen Yu, Linda B. Smith, and Alfredo F. Pereira. 2008. Grounding word learning in multimodal sensorimotor interaction. In Proceedings of the 30th Annual Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Yu</author>
</authors>
<title>Learning syntax-semantics mappings to bootstrap word learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 28th Annual Conference of the Cognitive Science Society</booktitle>
<note>Key: citeulike:5276016.</note>
<contexts>
<context position="6132" citStr="Yu, 2006" startWordPosition="882" endWordPosition="883">e also been explored. These, however, typically do not address learning of more complex linguistic structures/constructions. Taken together, lexical acquisition from speech and syntactic acquisition have been mainly studied independently of each other, often assuming that syntactic acquisition follows from knowledge of words. However, learning processes might actually be interleaved, and top-down learning processes may play an important role in LA. For instance, with respect to computational learning from symbolic input, it has been shown that knowledge of syntax can facilitate word learning (Yu, 2006). Children may, for instance, also make use of syntactic cues during speech segmentation and/or word learning, but models addressing lexical acquisition from speech have to date mainly ignored syntax (R¨as¨anen, 2012). Models addressing the acquisition of syntactic patterns directly from speech provide a basis for exploring to what extent learning mechanisms might be interleaved in early LA. Moreover, they allow to investigate the possible role of several top-down learning processes which have to date been little explored. Several corpora comprising interactions of children with their caregive</context>
</contexts>
<marker>Yu, 2006</marker>
<rawString>Chen Yu. 2006. Learning syntax-semantics mappings to bootstrap word learning. In Proceedings of the 28th Annual Conference of the Cognitive Science Society (2006) Key: citeulike:5276016.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032807">
<title confidence="0.998603">
Using Maximum Entropy Models to Discriminate between Similar
Languages and Varieties
</title>
<author confidence="0.965845">
Jordi Porta and Jos´e-Luis Sancho
</author>
<affiliation confidence="0.96067">
Departamento de Tecnolog´ıa y Sistemas
Centro de Estudios de la Real Academia Espa˜nola
</affiliation>
<address confidence="0.944239">
c/ Serrano 187-189, 28002 Madrid
</address>
<email confidence="0.999393">
{porta,sancho}@rae.es
</email>
<sectionHeader confidence="0.99739" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995016">
DSLRAE is a hierarchical classifier for similar written languages and varieties based on
maximum-entropy (maxent) classifiers. In the first level, the text is classified into a language
group using a simple token-based maxent classifier. At the second level, a group-specific maxent
classifier is applied to classify the text as one of the languages or varieties within the previously
identified group. For each group of languages, the classifier uses a different kind and combination
of knowledge-poor features: token or character n-grams and ‘white lists’ of tokens. Features were
selected according to the results of applying ten-fold cross-validation over the training dataset.
The system presented in this article1 has been ranked second in the Discriminating Similar Lan-
guage (DSL) shared task co-located within the VarDial Workshop at COLING 2014 (Zampieri
et al., 2014).
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999813541666667">
Language identification (LI) can be defined as the task of determining the language of a written text.
LI is also a cross-cutting technology supporting many other text analysis tasks: sentiment analysis,
political tendency or topic classification. There are some interesting problems around written language
identification that have attracted some attention recently, as native language identification (NLI, Tetreault
et al., 2013), the identification of the country of origin or the discrimination between similar or closely
related languages (DSL, Tiedemann and Ljubeˇsi´c, 2012).
LI has reached a great success in discriminating between languages with unique character sets and lan-
guages belonging to different language groups or typologically distant. However, according to Zampieri
(2013), multilingualism, noisy or non-standard features in text and discrimination between similar lan-
guages, varieties or dialects remain as the major known bottlenecks in language identification. For this
reason, DSL can be considered as a sub-task in language identification. Interestingly enough, LI seems
to work well with what Kloss (1967) called abstandsprache or language by distance (because Basque
is an isolate, it is generally regarded as a distant language) but fails in dealing with ausbausprache or
language by development (a standard variety together with all varieties heteronomous with respect to it,
e. g. Basque Batua koin´e and the various vernacular dialects).
Mass media, educational centres, administrations and communications favour standard languages in-
stead of other varieties. Standard varieties of languages are then seen by sociolinguists and dialectologists
as political and cultural constructs (Trudgill, 2004). However, languages and varieties are not just sys-
tems for communication between individuals, they are also used by groups and they are a crucial part
of their identity and culture. Language variation is systematic, both inter- and intra-personal. It can be
related to political, social, geographical, situational, communicative or instrumental factors. Variation
within a language can be found at different levels: alphabet, orthography (diacritics), word structure
(syllable composition, morphology), lexical choice or even syntax. Similar or closely related languages
often reflect a common origin and are members of a dialect continuum (Bloomfield, 1935).
</bodyText>
<footnote confidence="0.976495">
1We wish to thank an anonimous reviewer for her valuable comments and suggestions.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.853643">
120
</page>
<note confidence="0.985765">
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 120–128,
Dublin, Ireland, August 23 2014.
</note>
<bodyText confidence="0.999286">
Solutions to language identification are often based either on generative or discriminative character
n-gram language models. While character-based methods provide a means to distinguish between differ-
ent languages on the basis of coarse-grained statistics on n-grams, it seems that discriminating between
similar languages needs more fine-grained distinctions not always reflected by n-gram character distribu-
tions. According to Tiedemann and Ljubeˇsi´c (2012), character-based n-gram methods fail for languages
with a high lexical overlap, since the more shared words between two languages, the more similar will
their n-gram character frequency profiles be.
</bodyText>
<table confidence="0.999500875">
Group Model Lang/Var Precision Recall Fl-score
A C 1-5 bs 0.930 0.889 0.909
hr 0.924 0.941 0.932
sr 0.929 0.953 0.941
B L 1 id 0.988 0.994 0.991
my 0.994 0.988 0.991
C T 1-2 cz 1.000 0.999 0.999
sk 0.999 1.000 0.999
D T 1-2 pt-BR 0.933 0.964 0.948
pt-PT 0.963 0.930 0.946
E T 1-2 es-AR 0.942 0.816 0.874
es-ES 0.837 0.949 0.890
F L 1 en-GB 0.959 0.411 0.575
en-US 0.643 0.932 0.761
Overall without F 0.949 0.947 0.947
Overall 0.926 0.932 0.928
</table>
<tableCaption confidence="0.999456">
Table 1: Macro-averaged Precision, Recall and Fi-score on the DSL training dataset resulting from 10-
</tableCaption>
<bodyText confidence="0.979894">
fold cross-validation using the best model for each group of languages o varieties. Model has a letter
code indicating the kind of elements considered: C (characters), T (tokens), L (tokens from the list of the
10,000 most frequent tokens), and a number indicating how many consecutive elements have been taken
in a feature: 1 (unigrams), 1-2 (unigrams and bigrams), 1-5 (sequences of length one to five).
</bodyText>
<sectionHeader confidence="0.971217" genericHeader="method">
2 Previous Approaches
</sectionHeader>
<bodyText confidence="0.9997555">
Although focused on formal languages, Gold (1967) is usually credited as the first to attempt compu-
tational language identification. In particular, two common LI approaches, namely n-gram language
models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language
identification was formulated as a sub-task of text categorization and varied approaches were explored.
Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994)
and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on
the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some
statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin
(2011) for an overview of some linguistically motivated models.
As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is
erroneously considered an easy and solved problem2, in part because of some general purpose systems
being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui
and Baldwin, 2012). While it is true that it is possible to obtain brilliant results for a small number of
languages (Baldwin and Lui, 2010) or typologically distant languages (Zampieri et al., 2013), accurately
discriminating among closely related languages or varieties of the same language has been repeatedly
reported as a bottleneck for language identification systems, in particular for those based on n-grams.
</bodyText>
<footnote confidence="0.999834666666667">
2See McNamee (2005) eloquent title.
3http://odur.let.rug.nl/vannoord/TextCat
4http://open.xerox.com/Services/LanguageIdentifier
</footnote>
<page confidence="0.997528">
121
</page>
<bodyText confidence="0.999949239130435">
Back in 2004, Padr´o and Padr´o concluded that “since the tested systems tend to fail when distinguishing
similar languages (e.g. Spanish and Catalan), further research could be done to solve these cases.”
Martins and Silva (2005) report similar difficulties in discriminating among European and Brazilian
Portuguese. Ranaivo-Malanc¸on (2006) motivates her work on the unsatisfactory performance of (then)
available language identifiers when dealing with close languages such as Malay and Indonesian. Ljubeˇsi´c
et al. (2007) do not even attempt to distinguish Bosnian from Croatian when developing a Croatian
identifier because of their closeness. Trieschnigg et al. (2012) come as an exception as they report
satisfactory results in identifying sixteen varieties of Dutch with TextCat.
Ranaivo-Malanc¸on (2006) presents a cascaded language identifier for Malay and Indonesian. It first
distinguishes Malay or Indonesian from other four European languages using trigrams extracted from
the most frequent words from each language. Texts classified as Malay or Indonesian are subsequently
scanned for some linguistic features (format of numbers and exclusive words), yielding a more precise
performance than TextCat.
Ljubeˇsi´c et al. (2007) also propose a cascaded identifier that relies on ‘black lists’ to discard non-
Balkan languages and a second order Markov model on n-grams to discriminate among them, aug-
mented with a ‘black list’ component that raises accuracy up to 0.99 when dealing with the most difficult
pair (Croatian and Serbian). This work is followed up in Tiedemann and Ljubeˇsi´c (2012) where 9%
of improvement over standard approaches is reported and where support for Bosnian discrimination is
included.
Huang and Lee (2008) use a bag of the most frequent words to build a voting identifier for three Chi-
nese varieties with a top accuracy of 0.929. More recently, Zampieri (2013) compares the performance
of n-gram based models to machine learning methods using bag of words when discriminating similar
languages and varieties obtaining comparable performance with both approaches.
Grouin et al. (2010) present the shared task DEFT 2010. Participants were challenged to identify
the decade, country (France and Canada) and newspaper for a set of journalistic texts. As far as the
country labeling is concerned, they report an upper 0.964 Fl-measure and an average of 0.767. Very
brief descriptions of the systems are also offered.
Zampieri and Gebre (2012) present a log-likelihood estimation method for language models built on
orthographical (character n-grams), lexical (word unigrams) and lexico-syntactic (word bigrams) fea-
tures. They report a 0.998 accuracy distinguishing European and Brazilian Portuguese with a language
model based on character 4-grams. This approach is adapted in Zampieri et al. (2013) to deal with Span-
ish varieties, where the role of knowledge-rich features (POS tags) is also explored. They report a 0.99
accuracy when binarily distinguishing Argentinean and Mexican Spanish with single words or bigrams.
Trieschnigg et al. (2012) compare the performance of TextCat to the nearest neighbour and nearest
prototype in combination with a cosine distance when distinguishing among sixteen varieties of Dutch.
They report a micro-average Fl-score of 0.799 (and a macro-average Fl-score of 0.527) with a top
Fl-score of 0.987 when dealing with Frisian.
Lui and Cook (2013) report experiments with different classifiers to map English documents to their
country of origin. An SVM classifier with bag of words is top ranked with a macro-average 0.911 Fi-
score in a cross-domain setting and 0.975 in an in-domain setting.
All these previous works (with the sole exception of Trieschnigg et al. (2012), where a general purpose
LI system yields a satisfactory performance) agree in the specificity of DSL regarding LI. Maybe because
of that, two level approaches are not uncommon. Features used to discriminate seem to be language-
group specific, altough word rather than character features seem to perform better (Zampieri and Gebre
(2012) report best results for character 4-grams, however, given that European and Brazilian Portuguese
do not completely share ortography).
</bodyText>
<sectionHeader confidence="0.984069" genericHeader="method">
3 Maximum Entropy Models and Feature Engineering
</sectionHeader>
<bodyText confidence="0.999545666666667">
Maximum Entropy modelling is a general purpose machine learning framework that has proven to be
highly expressive and powerful in many areas. Maximum Entropy (maxent) was first introduced into
natural language processing by Berger et al. (1996) and Della Pietra et al. (1997). Since its introduction,
</bodyText>
<page confidence="0.988718">
122
</page>
<bodyText confidence="0.9986815">
Maximum Entropy techniques and the more general framework of Random Fields have been applied
extensively to natural language processing problems, where maxent classifiers are commonly used as an
alternative to Naive Bayes classifiers. In maxent modelling, the probability that an example x is in a
class c is estimated from its bag of words (or n-grams) as:
</bodyText>
<equation confidence="0.9997892">
1 �
p(c|x) = Z exp
wci · fi(c, y)
N
y∈bow(x) i=1
</equation>
<bodyText confidence="0.999872">
where fi(c, y) are indicator functions, wci is the weight assigned to feature i in class c, and Z is a
normalization factor. Features are modelled by indicator functions fi(c, y), which are evaluated to one
when the feature i for a particular class c is true for a word y and zero otherwise. The following is an
example of an indicator function modelling the presence of a particular word in a class:
</bodyText>
<equation confidence="0.993801">
� = 1, c = en-GB ∧ y = ‘colour’
f 1(c, y) 0, otherwise
</equation>
<bodyText confidence="0.578162">
The class assigned to an example x is the most probable one:
</bodyText>
<equation confidence="0.991135">
cˆ = arg max p(c|x)
c∈C
</equation>
<bodyText confidence="0.999823310344828">
The maxent classifiers are implemented with the toolkit of Zhang Le (2004), and the parameters of the
model are estimated using Generalized Iterative Scaling (Darroch and Ratcli, 1972).
Having chosen a closed approach to the DSL shared task, no other resources than the text samples
given as training and development datasets have been used in features design. In this knowledge-poor
approach to the problem, the maxent classifier has been trained with token and character n-gram features.
Character-based features are obtained with a simple character tokenizer. However, for token-based fea-
tures, texts are tokenized using an orthographic tokenizer which splits punctuation from words. Several
bags of features have been considered during the experiments: single tokens (T1), single words from the
list of the 10,000 most frequent tokens (L1), token bigrams (T2), and n-grams of character sequences of
length from one to five (C1-5). We will also refer to the lists of the 10,000 most frequent words as ‘white
list’, which have a complementary role to the ‘black lists’ of Tiedemann and Ljubeˇsi´c (2012).
To determine which features are best suited to each group, we measured their performance using ten-
fold cross-validation on the training dataset and using the development dataset for testing. For group A,
best results were obtained using bag of features consisting of variable length character n-grams ranging
from one to five (C1-5). On group B, token bigrams (T2) performed slightly better in the development
set than in the training set than the ‘white list’ of tokens (L1), which seems to indicate a better general-
isation of the former on unseen examples. Results for group C were similar for all features considered.
Regarding groups D and E, token-based features got similar results, with slightly better results for token
bigrams. Finally, for English (group F) results were generally bad, reaching the ‘white list’ the better
results. Group F is known to contain more than a few misclassifications due to news cross citing be-
tween American and British press. Results for each group’s best model using ten-fold cross-validation
on the training dataset are shown in Table 1. All figures have been macro averaged, i.e., they have been
computed averaging the ten folds.
Because best results for each group are obtained with different feature sets, a new classifier is in-
troduced. This classifier determines the language/variety group of each example before applying its
particular group classifier. As can be seen in Table 2, the degree of token overlap between languages and
varieties of different groups is rather low compared with the degree of overlap within the same group.
Using only tokens, total accuracy is reached on the training dataset using cross validation. A classifier
applying several classifiers in the way we propose is known as a hierarchical two-level classifier.
</bodyText>
<sectionHeader confidence="0.990038" genericHeader="evaluation">
4 Evaluation and Error Analysis
</sectionHeader>
<bodyText confidence="0.9996885">
Having as a goal to assess the performance of the hierarchical maxent classifier with the DSL task
dataset, models were trained using all the examples provided in the training and development datasets.
</bodyText>
<page confidence="0.988314">
123
</page>
<table confidence="0.999637785714286">
bs hr sr id my sk cz pt-BR pt-PT es-AR es-ES en-GB en-US
bs 35.51 31.29 2.25 2.05 2.09 1.95 1.91 2.00 1.92 1.99 2.09 2.10
hr 41.18 2.47 2.21 2.15 2.04 2.08 2.20 2.12 2.16 2.42 2.39
sr 2.06 1.74 1.95 1.79 1.63 1.72 1.69 1.69 1.68 1.68
id 19.02 2.36 2.47 4.00 4.14 4.35 4.21 6.81 6.74
my 1.91 2.00 3.43 3.61 3.75 3.52 6.40 6.23
sk 9.45 2.12 2.15 2.20 2.22 2.55 2.56
cz 2.18 2.25 2.24 2.27 2.73 2.70
pt-BR 29.17 12.04 11.63 4.62 4.60
pt-PT 12.14 12.50 4.92 4.94
es-AR 30.91 5.52 5.52
es-ES 4.89 4.90
en-GB 32.76
en-US
</table>
<tableCaption confidence="0.957682">
Table 2: Lexical overlap between pairs of languages as a percentage. Only orthographic forms and
punctuation signs appearing more than once in the training dataset has been considered.
</tableCaption>
<table confidence="0.9999504375">
Group Model Lang/Var Precision Recall F1-score
A C 1-5 bs 0.903 0.875 0.889
hr 0.923 0.931 0.927
sr 0.928 0.951 0.939
B L 1 id 0.991 0.996 0.993
my 0.996 0.991 0.993
C T 1-2 cz 1.000 1.000 1.000
sk 1.000 1.000 1.000
D T 1-2 pt-BR 0.933 0.964 0.948
pt-PT 0.962 0.931 0.946
E T 1-2 es-AR 0.950 0.819 0.879
es-ES 0.840 0.957 0.895
F L 1 en-GB 0.486 0.713 0.578
en-US 0.463 0.247 0.322
Overall without F 0.948 0.948 0.947
Overall 0.875 0.870 0.872
</table>
<tableCaption confidence="0.999035">
Table 3: Macro-averaged Precision, Recall and F1-score on the DSL test dataset. Models are described
</tableCaption>
<bodyText confidence="0.980626222222222">
in Table 1.
Table 4 shows the confusion matrix for the classifier on the test dataset and Table 1 the results in terms
of precision, recall and F1-score for each language and variety. As can be seen in Table 4, no example
has been classified outside in a wrong group.
Tan et al. (2014) provide a baseline using a Naive Bayes classifier on character 5-grams. As can be
seen if Table 3 is compared with Table 4 of Tan et al. (2014), figures for group A are slightly below the
baseline, groups B and C achieve the same results, D and E groups get slightly better results with the
maxent classifier, and the biggest difference is found in group F, having better results Naive Bayes. The
overall result without group F is similar: an F1-score of 0.947 for maxent and 0.942 for Naive Bayes.
The DSL Corpus is composed of journalistic comparable texts to make the corpus suitable for discrim-
inating similar languages and languages varieties but not text types or genres. Tiedemann and Ljubeˇsi´c
(2012) avoid biases towards topic and domain by experimenting with parallel texts reaching an overall
accuracy of 90.3% for group A (br, hr, sr) using a ‘black list’ classifier and comparing its results with a
Naive Bayes approach. They found that the ‘black list’ classifier generalise better than the Naive Bayes
approach when moving from parallel to comparable corpora, since the former classifier is based on more
informative features than the later.
Results of ten-fold cross-validation on the training dataset for different feature settings for group E
(Spanish) were consistent with those of Zampieri et al. (2013), where word bigrams are reported to
</bodyText>
<page confidence="0.980247">
124
</page>
<bodyText confidence="0.695093428571429">
bs hr sr id my cz sk pt-BR pt-PT es-AR es-ES en-GB en-US
bs 875 61 64 0 0 0 0 0 0 0 0 0 0
hr 60 931 9 0 0 0 0 0 0 0 0 0 0
sr 33 16 951 0 0 0 0 0 0 0 0 0 0
id 0 0 0 996 4 0 0 0 0 0 0 0 0
my 0 0 0 9 991 0 0 0 0 0 0 0 0
cz 0 0 0 0 0 1,000 0 0 0 0 0 0 0
sk 0 0 0 0 0 0 1,000 0 0 0 0 0 0
pt-BR 0 0 0 0 0 0 0 964 36 0 0 0 0
pt-PT 0 0 0 0 0 0 0 69 931 0 0 0 0
es-AR 0 0 0 0 0 0 0 0 0 819 181 0 0
es-ES 0 0 0 0 0 0 0 0 0 43 957 0 0
en-GB 0 0 0 0 0 0 0 0 0 0 0 571 229
en-US 0 0 0 0 0 0 0 0 0 0 0 602 198
</bodyText>
<tableCaption confidence="0.786279">
Table 4: Confusion matrix for the hierarchical maxent classifier on languages and varieties in the DSL
test dataset. The 1,000 Bosnian texts have been classified as Bosnian (875), Croatian (61) and Serbian
(64).
</tableCaption>
<figure confidence="0.918968894736842">
Group Language/Variety Code
Bosnian bs
A Croatian hr
Serbian sr
Indonesian id
B
Malay my
Czech cz
C
Slovak sk
Brazilian Portuguese pt-BR
D
European Portuguese pt-PT
Argentine Spanish es-AR
E
European Spanish es-ES
British English en-GB
F
American English en-US
</figure>
<tableCaption confidence="0.952642">
Table 5: Languages and varieties groups and codes.
</tableCaption>
<bodyText confidence="0.999739888888889">
outperform character n-grams. Given that datasets are not identical, it is difficult to draw any conclusion
from the 1.2% difference in accuracy between DSLRAE and Zampieri et al. (2013). Manual inspection
of misclassified news suggests some textual properties that are specially challenging: a) high density of
foreign proper names (Russian, Baby, Pony, Jack, ...) may dilute the evidence provided by vernacular
words; b) conversely, low density of features specific to any variant (such as place or family names5,
demonyms, lexical choices) may be insufficient to drive the text to the right class; this is also the case of
some perfectly neutral sentences where a trained linguist could not spot any clue about their origin; c)
certain syntactical idiosyncrasies (for example Argentinian idioms la pasas bien, tal como muchas veces,
en exceso de) are not captured by bigrams; d) there are instances of cross-information, e. g., Argentinian
news about Spain and vice versa where maybe more of a topic rather than a variety is being detected
(e. g., news about Urdangarin or Fern´andez de Kirchner); e) there are some typos and misspellings
(carabanas, dosco) whose role remains unclear; e) finally, there is at least one text misclassified in the
gold standard: it is labeled as Argentinian but it was written by the Spanish EFE news agency. Some of
these difficulties cross-cut all language groups and are not specific to Spanish but rather to DSL as a task.
In contrast to what Zampieri and Gebre (2012) found, ten-fold cross-validation on the training dataset
for different feature settings on the DSL dataset did not find character n-grams to outperform word n-
grams for group D (Portuguese). It could be hypothesized that they used a unique source (newspaper)
for each variety and therefore rigid editorial conventions could be at play; moreover, the collections were
</bodyText>
<footnote confidence="0.820123">
5Zampieri and Gebre (2012) highlight the importance of proper nouns when using word n-grams.
</footnote>
<page confidence="0.997586">
125
</page>
<bodyText confidence="0.9997755">
three years distant, so topic consistency could also be compromised6. Manual inspection of mislabeled
sentences shows some already known categories: evidence diluted by foreign words (Red Brick Ware-
house, M´esz´aros, Fat Duck), poor evidence (Valongo, Sao Paulo) or cross-information (TAP, Brasilia).
There is, however, a Portuguese-specific issue: some texts obey the 1990 Orthographic Agreement7
which blurs the orthographic distinctions regarding diacritics or consonant clusters; in fact, one sentence
contains words following both standards (perspectiva and reproduc¸˜ao). It remains unexplained why
word bigrams did not capture the Brazilian preference for passive voice (foram rebaixados), auxiliary +
gerund chunks (estamos utilizando) or clitic dropping (lembro).
Despite findings by Tiedemann and Ljubeˇsi´c (2012), character n-grams performed better during ten-
fold cross-validation on the training dataset for different feature settings on the DSL dataset for group A
(Bosnian, Croatian and Serbian). Misclassified sentences involve failing to capture adapted place names
(Belgiji, ˇSvedskoj) or derivational choices (organiziranog).
Results of ten-fold cross-validation on the training dataset for different feature settings for group B
(Indonesian and Malay) top ranked word unigrams. Ranaivo-Malanc¸on (2006) uses number formatting
and exclusive word lists. It can be hypothesized that lexical overlap is low (see Table 2) and/or frequency
distributions are dissimilar thus allowing word unigrams to perform as well as ‘white lists’.
Languages of group C (Czech and Slovak) are dissimilar both orthographically and lexically. These
dissimilarities are surprisingly well captured by the top 10,000 most frequent words.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979842105263">
In this paper, we have shown that a hierarchical classifier is well suited to discriminate among different
language groups and languages or varieties therein. Different features are shown to better suit typological
traits of supported languages. A comparison to previous approaches is provided, when available.
In a multilingual setting, the effect of adding Galician to group D could be investigated. Focusing on
Spanish language, we plan to geographically expand the classifier to deal with all national varieties, a
much harder task as both Baldwin and Lui (2010) and Zampieri et al. (2013) remark. Moreover, the
classifier could be used, as Tiedemann and Ljubeˇsi´c (2012) suggest, to learn varieties discriminators to
label texts beyond national classes (e.g. both Caribbean and Andean Spanish cross-cut national borders
and, conversely, nations involved are known not to be dialectally uniform). Given that error analysis
showed that word bigrams fail to capture certain syntactical idiosyncrasies, a model with longer n-grams
and/or knowledge-richer features such as POS sequences could also be explored, although Zampieri et al.
(2013) report lower performance than knowledge-poor features. Finally, classification techniques such as
those described in Gyawali et al. (2013) may be used to discard translations when building monolingual,
vernacular corpora.
A diachronic expansion, such as Trieschnigg et al. (2012), is also in mind. Medieval Castilian coex-
isted with other Romance varieties such as Leonese or Aragonese whose features permeated Castilian
texts. Researchers are in need of a tool to properly classify diachronic texts to accurately describe older
stages of Spanish. Following the suggestion of Tiedemann and Ljubeˇsi´c (2012), we envisage the use of
parallel texts such as versions of the Bible from different areas to learn the differences among varieties.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997783571428571">
Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In HLT-
NAACL, pages 229–237.
Kenneth Beesley. 1988. Language identifier: A computer program for automatic natural-language identification
of on-line text. In Language at Crossroads: Proceedings of the Annual Conference of the American Translators
Association, pages 47–54.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39–71.
</reference>
<footnote confidence="0.9929875">
6Ljubeˇsi´c et al. (2007) warn against corpus-specific features.
7http://www.portaldalinguaportuguesa.org/acordo.php
</footnote>
<page confidence="0.994192">
126
</page>
<reference confidence="0.998948065217391">
Leonard Bloomfield. 1935. Language. Allen &amp; Unwin, London.
William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of 3rd Annual
Symposium on Document Analysis and Information Retrieval (SDAIR 94), pages 161–175.
J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathemat-
ical Statistics, 43(5):1470–1480.
Ted Dunning. 1994. Statistical identification of language. Technical report, Computing Research Laboratory.
New Mexico State University.
E. Mark Gold. 1967. Language identification in the limit. Information and Control, 10(5):447–474.
Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of the 3rd Interna-
tional Conference on Statistical Analysis of Textual Data (JADT 95), pages 263–268.
Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2014. Pr´esentation et
r´esultats du d´efi fouille de texte DEFT2010 : o`u et quand un article de presse a-t-il ´et´e ´ecrit ? In Proceedings
Atelier de clˆoture de la sixi`eme ´edition du D´efi Fouille de Textes (DEFT-2010), pages 1–15.
Binod Gyawali, Gabriela Ramirez, and Thamar Solorio. 2013. Native language identification: a simple n-gram
based approach. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational
Applications, pages 224–231.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive approach towards text source classification based on
top-bag-of-word similarity. In PACLIC, pages 404–410.
Norman C. Ingle. 1978. Language identification table. The author, Shoreham-by-Sea.
Heinz Kloss. 1967. Abstand languages and Ausbau languages. Anthropological Linguistics, 9(7):29–41.
Zhang Le, 2004. Maximum Entropy Modeling Toolkit for Python and C++, December.
Nikola Ljube&amp;quot;si´c, Nives Mikeli´c, and Damir Boras. 2007. Language identification: How to distinguish similar
languages. In Proceedings of the 29th International Conference on Information Technology Interfaces, pages
541–546.
Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In IJCNLP,
pages 553–561.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceed-
ings of the ACL 2012 System Demonstrations, pages 25–30.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of the
Australasian Language Technology Association Workshop 2013 (ALTA 2013), pages 5–15.
Bruno Martins and M´ario J. Silva. 2005. Language identification in web pages. In Proceedings of the 2005 ACM
Symposium on Applied Computing, pages 764–768.
Paul McNamee. 2005. Language identification: A solved problem suitable for undergraduate instruction. Journal
of Computing Sciences in Colleges, 20(3):94–101.
Muntsa Padr´o and Llu´ıs Padr´o. 2004. Comparing methods for language identification. Procesamiento del
Lenguaje Natural, 33:155–162.
S. A. Della Pietra, V. J. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 19(4):1–13.
Bali Ranaivo-Malanc¸on. 2006. Automatic Identification of Close Languages – Case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2(2):126–134.
Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. Merging comparable data sources for
the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on
Building and Using Comparable Corpora (BUCC).
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop on the Innovative Use of NLP for Building Educational
Applications, pages 48–57.
</reference>
<page confidence="0.97139">
127
</page>
<reference confidence="0.998805666666667">
J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619–2634.
R.B. Trieschnigg, D. Hiemstra, M. Theune, F.M.G. de Jong, and T. Meder. 2012. An exploration of language
identification techniques for the Dutch folktale database. In Proceedings of the Workshop on Adaptation of
Language Resources and Tools for Processing Cultural Heritage (LREC 2012), pages 47–51.
Peter Trudgill. 2004. Glocalisation and the Ausbau sociolinguistics of modern Europe. In Anna Duszak and
Urszula Okulska, editors, Speaking from the Margin: Global English from a European Perspective, pages 35–
49. Peter Lang, Frankfurt am Main.
Marcos Zampieri and Binyam Gebre. 2012. Automatic identification of language varieties: The case of Por-
tuguese. In Proceedings of KONVENS 2012, pages 233–237.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580–587.
Marcos Zampieri, Liling Tan, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. A report on the DSL shared task
2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects
(VarDial).
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they?
In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics
(CINTI2013), pages 37–41.
</reference>
<page confidence="0.996739">
128
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.293765">
<title confidence="0.9994745">Using Maximum Entropy Models to Discriminate between Languages and Varieties</title>
<author confidence="0.970022">Porta</author>
<affiliation confidence="0.9093055">Departamento de Tecnolog´ıa y de Estudios de la Real Academia</affiliation>
<address confidence="0.933016">c/ Serrano 187-189, 28002</address>
<abstract confidence="0.976018111111111">DSLRAE is a hierarchical classifier for similar written languages and varieties based on maximum-entropy (maxent) classifiers. In the first level, the text is classified into a language group using a simple token-based maxent classifier. At the second level, a group-specific maxent classifier is applied to classify the text as one of the languages or varieties within the previously identified group. For each group of languages, the classifier uses a different kind and combination of knowledge-poor features: token or character n-grams and ‘white lists’ of tokens. Features were selected according to the results of applying ten-fold cross-validation over the training dataset. system presented in this has been ranked second in the Discriminating Similar Lan-</abstract>
<note confidence="0.696471">guage (DSL) shared task co-located within the VarDial Workshop at COLING 2014 (Zampieri et al., 2014).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>229--237</pages>
<contexts>
<context position="6426" citStr="Baldwin and Lui (2010)" startWordPosition="941" endWordPosition="944">roaches, namely n-gram language models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui and Baldwin, 2012). While it is true that it is possible to obtain brilliant results for a small number of languages (Baldwin and Lui, 2010) or typologically distant languages (Zampieri et al., 2013), accurately discrim</context>
<context position="24219" citStr="Baldwin and Lui (2010)" startWordPosition="3860" endWordPosition="3863">,000 most frequent words. 5 Conclusions and Future Work In this paper, we have shown that a hierarchical classifier is well suited to discriminate among different language groups and languages or varieties therein. Different features are shown to better suit typological traits of supported languages. A comparison to previous approaches is provided, when available. In a multilingual setting, the effect of adding Galician to group D could be investigated. Focusing on Spanish language, we plan to geographically expand the classifier to deal with all national varieties, a much harder task as both Baldwin and Lui (2010) and Zampieri et al. (2013) remark. Moreover, the classifier could be used, as Tiedemann and Ljubeˇsi´c (2012) suggest, to learn varieties discriminators to label texts beyond national classes (e.g. both Caribbean and Andean Spanish cross-cut national borders and, conversely, nations involved are known not to be dialectally uniform). Given that error analysis showed that word bigrams fail to capture certain syntactical idiosyncrasies, a model with longer n-grams and/or knowledge-richer features such as POS sequences could also be explored, although Zampieri et al. (2013) report lower performan</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In HLTNAACL, pages 229–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Beesley</author>
</authors>
<title>Language identifier: A computer program for automatic natural-language identification of on-line text.</title>
<date>1988</date>
<booktitle>In Language at Crossroads: Proceedings of the Annual Conference of the American Translators Association,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="6058" citStr="Beesley (1988)" startWordPosition="884" endWordPosition="885">okens), and a number indicating how many consecutive elements have been taken in a feature: 1 (unigrams), 1-2 (unigrams and bigrams), 1-5 (sequences of length one to five). 2 Previous Approaches Although focused on formal languages, Gold (1967) is usually credited as the first to attempt computational language identification. In particular, two common LI approaches, namely n-gram language models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved proble</context>
</contexts>
<marker>Beesley, 1988</marker>
<rawString>Kenneth Beesley. 1988. Language identifier: A computer program for automatic natural-language identification of on-line text. In Language at Crossroads: Proceedings of the Annual Conference of the American Translators Association, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11862" citStr="Berger et al. (1996)" startWordPosition="1746" endWordPosition="1749">evel approaches are not uncommon. Features used to discriminate seem to be languagegroup specific, altough word rather than character features seem to perform better (Zampieri and Gebre (2012) report best results for character 4-grams, however, given that European and Brazilian Portuguese do not completely share ortography). 3 Maximum Entropy Models and Feature Engineering Maximum Entropy modelling is a general purpose machine learning framework that has proven to be highly expressive and powerful in many areas. Maximum Entropy (maxent) was first introduced into natural language processing by Berger et al. (1996) and Della Pietra et al. (1997). Since its introduction, 122 Maximum Entropy techniques and the more general framework of Random Fields have been applied extensively to natural language processing problems, where maxent classifiers are commonly used as an alternative to Naive Bayes classifiers. In maxent modelling, the probability that an example x is in a class c is estimated from its bag of words (or n-grams) as: 1 � p(c|x) = Z exp wci · fi(c, y) N y∈bow(x) i=1 where fi(c, y) are indicator functions, wci is the weight assigned to feature i in class c, and Z is a normalization factor. Feature</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Bloomfield</author>
</authors>
<date>1935</date>
<publisher>Language. Allen &amp; Unwin,</publisher>
<location>London.</location>
<contexts>
<context position="3553" citStr="Bloomfield, 1935" startWordPosition="504" endWordPosition="505"> just systems for communication between individuals, they are also used by groups and they are a crucial part of their identity and culture. Language variation is systematic, both inter- and intra-personal. It can be related to political, social, geographical, situational, communicative or instrumental factors. Variation within a language can be found at different levels: alphabet, orthography (diacritics), word structure (syllable composition, morphology), lexical choice or even syntax. Similar or closely related languages often reflect a common origin and are members of a dialect continuum (Bloomfield, 1935). 1We wish to thank an anonimous reviewer for her valuable comments and suggestions. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 120 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 120–128, Dublin, Ireland, August 23 2014. Solutions to language identification are often based either on generative or discriminative character n-gram language models. While character-based m</context>
</contexts>
<marker>Bloomfield, 1935</marker>
<rawString>Leonard Bloomfield. 1935. Language. Allen &amp; Unwin, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>N-gram-based text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR 94),</booktitle>
<pages>161--175</pages>
<contexts>
<context position="6174" citStr="Cavnar and Trenkle (1994)" startWordPosition="901" endWordPosition="904"> 1-2 (unigrams and bigrams), 1-5 (sequences of length one to five). 2 Previous Approaches Although focused on formal languages, Gold (1967) is usually credited as the first to attempt computational language identification. In particular, two common LI approaches, namely n-gram language models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 an</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In Proceedings of 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR 94), pages 161–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J. N. Darroch and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43(5):1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<journal>Information and Control,</journal>
<tech>Technical report,</tech>
<volume>10</volume>
<issue>5</issue>
<institution>Computing Research Laboratory. New Mexico State University.</institution>
<contexts>
<context position="6144" citStr="Dunning (1994)" startWordPosition="898" endWordPosition="899">ture: 1 (unigrams), 1-2 (unigrams and bigrams), 1-5 (sequences of length one to five). 2 Previous Approaches Although focused on formal languages, Gold (1967) is usually credited as the first to attempt computational language identification. In particular, two common LI approaches, namely n-gram language models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3,</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Technical report, Computing Research Laboratory. New Mexico State University. E. Mark Gold. 1967. Language identification in the limit. Information and Control, 10(5):447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Comparing two language identification schemes.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd International Conference on Statistical Analysis of Textual Data (JADT 95),</booktitle>
<pages>263--268</pages>
<contexts>
<context position="6195" citStr="Grefenstette (1995)" startWordPosition="905" endWordPosition="906">, 1-5 (sequences of length one to five). 2 Previous Approaches Although focused on formal languages, Gold (1967) is usually credited as the first to attempt computational language identification. In particular, two common LI approaches, namely n-gram language models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, lan</context>
</contexts>
<marker>Grefenstette, 1995</marker>
<rawString>Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of the 3rd International Conference on Statistical Analysis of Textual Data (JADT 95), pages 263–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Grouin</author>
<author>Dominic Forest</author>
<author>Lyne Da Sylva</author>
<author>Patrick Paroubek</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Pr´esentation et r´esultats du d´efi fouille de texte DEFT2010 : o`u et quand un article de presse a-t-il ´et´e ´ecrit ?</title>
<date>2014</date>
<booktitle>In Proceedings Atelier de clˆoture de la sixi`eme ´edition du D´efi Fouille de Textes (DEFT-2010),</booktitle>
<pages>1--15</pages>
<marker>Grouin, Forest, Sylva, Paroubek, Zweigenbaum, 2014</marker>
<rawString>Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2014. Pr´esentation et r´esultats du d´efi fouille de texte DEFT2010 : o`u et quand un article de presse a-t-il ´et´e ´ecrit ? In Proceedings Atelier de clˆoture de la sixi`eme ´edition du D´efi Fouille de Textes (DEFT-2010), pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binod Gyawali</author>
<author>Gabriela Ramirez</author>
<author>Thamar Solorio</author>
</authors>
<title>Native language identification: a simple n-gram based approach.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>224--231</pages>
<contexts>
<context position="24935" citStr="Gyawali et al. (2013)" startWordPosition="3961" endWordPosition="3964">ubeˇsi´c (2012) suggest, to learn varieties discriminators to label texts beyond national classes (e.g. both Caribbean and Andean Spanish cross-cut national borders and, conversely, nations involved are known not to be dialectally uniform). Given that error analysis showed that word bigrams fail to capture certain syntactical idiosyncrasies, a model with longer n-grams and/or knowledge-richer features such as POS sequences could also be explored, although Zampieri et al. (2013) report lower performance than knowledge-poor features. Finally, classification techniques such as those described in Gyawali et al. (2013) may be used to discard translations when building monolingual, vernacular corpora. A diachronic expansion, such as Trieschnigg et al. (2012), is also in mind. Medieval Castilian coexisted with other Romance varieties such as Leonese or Aragonese whose features permeated Castilian texts. Researchers are in need of a tool to properly classify diachronic texts to accurately describe older stages of Spanish. Following the suggestion of Tiedemann and Ljubeˇsi´c (2012), we envisage the use of parallel texts such as versions of the Bible from different areas to learn the differences among varieties.</context>
</contexts>
<marker>Gyawali, Ramirez, Solorio, 2013</marker>
<rawString>Binod Gyawali, Gabriela Ramirez, and Thamar Solorio. 2013. Native language identification: a simple n-gram based approach. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
<author>Lung-Hao Lee</author>
</authors>
<title>Contrastive approach towards text source classification based on top-bag-of-word similarity.</title>
<date>2008</date>
<booktitle>In PACLIC,</booktitle>
<pages>404--410</pages>
<contexts>
<context position="9100" citStr="Huang and Lee (2008)" startWordPosition="1324" endWordPosition="1327"> (format of numbers and exclusive words), yielding a more precise performance than TextCat. Ljubeˇsi´c et al. (2007) also propose a cascaded identifier that relies on ‘black lists’ to discard nonBalkan languages and a second order Markov model on n-grams to discriminate among them, augmented with a ‘black list’ component that raises accuracy up to 0.99 when dealing with the most difficult pair (Croatian and Serbian). This work is followed up in Tiedemann and Ljubeˇsi´c (2012) where 9% of improvement over standard approaches is reported and where support for Bosnian discrimination is included. Huang and Lee (2008) use a bag of the most frequent words to build a voting identifier for three Chinese varieties with a top accuracy of 0.929. More recently, Zampieri (2013) compares the performance of n-gram based models to machine learning methods using bag of words when discriminating similar languages and varieties obtaining comparable performance with both approaches. Grouin et al. (2010) present the shared task DEFT 2010. Participants were challenged to identify the decade, country (France and Canada) and newspaper for a set of journalistic texts. As far as the country labeling is concerned, they report a</context>
</contexts>
<marker>Huang, Lee, 2008</marker>
<rawString>Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive approach towards text source classification based on top-bag-of-word similarity. In PACLIC, pages 404–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman C Ingle</author>
</authors>
<title>Language identification table. The author,</title>
<date>1978</date>
<location>Shoreham-by-Sea.</location>
<contexts>
<context position="6234" citStr="Ingle (1978)" startWordPosition="911" endWordPosition="912">vious Approaches Although focused on formal languages, Gold (1967) is usually credited as the first to attempt computational language identification. In particular, two common LI approaches, namely n-gram language models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui and Baldwin, 2012). While i</context>
</contexts>
<marker>Ingle, 1978</marker>
<rawString>Norman C. Ingle. 1978. Language identification table. The author, Shoreham-by-Sea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinz Kloss</author>
</authors>
<title>Abstand languages and Ausbau languages. Anthropological Linguistics, 9(7):29–41. Zhang Le,</title>
<date>1967</date>
<contexts>
<context position="2297" citStr="Kloss (1967)" startWordPosition="327" endWordPosition="328">osely related languages (DSL, Tiedemann and Ljubeˇsi´c, 2012). LI has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant. However, according to Zampieri (2013), multilingualism, noisy or non-standard features in text and discrimination between similar languages, varieties or dialects remain as the major known bottlenecks in language identification. For this reason, DSL can be considered as a sub-task in language identification. Interestingly enough, LI seems to work well with what Kloss (1967) called abstandsprache or language by distance (because Basque is an isolate, it is generally regarded as a distant language) but fails in dealing with ausbausprache or language by development (a standard variety together with all varieties heteronomous with respect to it, e. g. Basque Batua koin´e and the various vernacular dialects). Mass media, educational centres, administrations and communications favour standard languages instead of other varieties. Standard varieties of languages are then seen by sociolinguists and dialectologists as political and cultural constructs (Trudgill, 2004). H</context>
</contexts>
<marker>Kloss, 1967</marker>
<rawString>Heinz Kloss. 1967. Abstand languages and Ausbau languages. Anthropological Linguistics, 9(7):29–41. Zhang Le, 2004. Maximum Entropy Modeling Toolkit for Python and C++, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikola Ljubesi´c</author>
<author>Nives Mikeli´c</author>
<author>Damir Boras</author>
</authors>
<title>Language identification: How to distinguish similar languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 29th International Conference on Information Technology Interfaces,</booktitle>
<pages>541--546</pages>
<marker>Ljubesi´c, Mikeli´c, Boras, 2007</marker>
<rawString>Nikola Ljube&amp;quot;si´c, Nives Mikeli´c, and Damir Boras. 2007. Language identification: How to distinguish similar languages. In Proceedings of the 29th International Conference on Information Technology Interfaces, pages 541–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Cross-domain feature selection for language identification.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>553--561</pages>
<contexts>
<context position="6453" citStr="Lui and Baldwin (2011)" startWordPosition="946" endWordPosition="949">uage models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui and Baldwin, 2012). While it is true that it is possible to obtain brilliant results for a small number of languages (Baldwin and Lui, 2010) or typologically distant languages (Zampieri et al., 2013), accurately discriminating among closely relat</context>
</contexts>
<marker>Lui, Baldwin, 2011</marker>
<rawString>Marco Lui and Timothy Baldwin. 2011. Cross-domain feature selection for language identification. In IJCNLP, pages 553–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="6825" citStr="Lui and Baldwin, 2012" startWordPosition="999" endWordPosition="1002">d this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui and Baldwin, 2012). While it is true that it is possible to obtain brilliant results for a small number of languages (Baldwin and Lui, 2010) or typologically distant languages (Zampieri et al., 2013), accurately discriminating among closely related languages or varieties of the same language has been repeatedly reported as a bottleneck for language identification systems, in particular for those based on n-grams. 2See McNamee (2005) eloquent title. 3http://odur.let.rug.nl/vannoord/TextCat 4http://open.xerox.com/Services/LanguageIdentifier 121 Back in 2004, Padr´o and Padr´o concluded that “since the tested syst</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the ACL 2012 System Demonstrations, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Paul Cook</author>
</authors>
<title>Classifying English documents by national dialect.</title>
<date>2013</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>5--15</pages>
<contexts>
<context position="10773" citStr="Lui and Cook (2013)" startWordPosition="1579" endWordPosition="1582">s adapted in Zampieri et al. (2013) to deal with Spanish varieties, where the role of knowledge-rich features (POS tags) is also explored. They report a 0.99 accuracy when binarily distinguishing Argentinean and Mexican Spanish with single words or bigrams. Trieschnigg et al. (2012) compare the performance of TextCat to the nearest neighbour and nearest prototype in combination with a cosine distance when distinguishing among sixteen varieties of Dutch. They report a micro-average Fl-score of 0.799 (and a macro-average Fl-score of 0.527) with a top Fl-score of 0.987 when dealing with Frisian. Lui and Cook (2013) report experiments with different classifiers to map English documents to their country of origin. An SVM classifier with bag of words is top ranked with a macro-average 0.911 Fiscore in a cross-domain setting and 0.975 in an in-domain setting. All these previous works (with the sole exception of Trieschnigg et al. (2012), where a general purpose LI system yields a satisfactory performance) agree in the specificity of DSL regarding LI. Maybe because of that, two level approaches are not uncommon. Features used to discriminate seem to be languagegroup specific, altough word rather than charact</context>
</contexts>
<marker>Lui, Cook, 2013</marker>
<rawString>Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013), pages 5–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Martins</author>
<author>M´ario J Silva</author>
</authors>
<title>Language identification in web pages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACM Symposium on Applied Computing,</booktitle>
<pages>764--768</pages>
<contexts>
<context position="7586" citStr="Martins and Silva (2005)" startWordPosition="1103" endWordPosition="1106">ly distant languages (Zampieri et al., 2013), accurately discriminating among closely related languages or varieties of the same language has been repeatedly reported as a bottleneck for language identification systems, in particular for those based on n-grams. 2See McNamee (2005) eloquent title. 3http://odur.let.rug.nl/vannoord/TextCat 4http://open.xerox.com/Services/LanguageIdentifier 121 Back in 2004, Padr´o and Padr´o concluded that “since the tested systems tend to fail when distinguishing similar languages (e.g. Spanish and Catalan), further research could be done to solve these cases.” Martins and Silva (2005) report similar difficulties in discriminating among European and Brazilian Portuguese. Ranaivo-Malanc¸on (2006) motivates her work on the unsatisfactory performance of (then) available language identifiers when dealing with close languages such as Malay and Indonesian. Ljubeˇsi´c et al. (2007) do not even attempt to distinguish Bosnian from Croatian when developing a Croatian identifier because of their closeness. Trieschnigg et al. (2012) come as an exception as they report satisfactory results in identifying sixteen varieties of Dutch with TextCat. Ranaivo-Malanc¸on (2006) presents a cascad</context>
</contexts>
<marker>Martins, Silva, 2005</marker>
<rawString>Bruno Martins and M´ario J. Silva. 2005. Language identification in web pages. In Proceedings of the 2005 ACM Symposium on Applied Computing, pages 764–768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul McNamee</author>
</authors>
<title>Language identification: A solved problem suitable for undergraduate instruction.</title>
<date>2005</date>
<journal>Journal of Computing Sciences in Colleges,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="7243" citStr="McNamee (2005)" startWordPosition="1064" endWordPosition="1065">red an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui and Baldwin, 2012). While it is true that it is possible to obtain brilliant results for a small number of languages (Baldwin and Lui, 2010) or typologically distant languages (Zampieri et al., 2013), accurately discriminating among closely related languages or varieties of the same language has been repeatedly reported as a bottleneck for language identification systems, in particular for those based on n-grams. 2See McNamee (2005) eloquent title. 3http://odur.let.rug.nl/vannoord/TextCat 4http://open.xerox.com/Services/LanguageIdentifier 121 Back in 2004, Padr´o and Padr´o concluded that “since the tested systems tend to fail when distinguishing similar languages (e.g. Spanish and Catalan), further research could be done to solve these cases.” Martins and Silva (2005) report similar difficulties in discriminating among European and Brazilian Portuguese. Ranaivo-Malanc¸on (2006) motivates her work on the unsatisfactory performance of (then) available language identifiers when dealing with close languages such as Malay an</context>
</contexts>
<marker>McNamee, 2005</marker>
<rawString>Paul McNamee. 2005. Language identification: A solved problem suitable for undergraduate instruction. Journal of Computing Sciences in Colleges, 20(3):94–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muntsa Padr´o</author>
<author>Llu´ıs Padr´o</author>
</authors>
<title>Comparing methods for language identification.</title>
<date>2004</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>33--155</pages>
<marker>Padr´o, Padr´o, 2004</marker>
<rawString>Muntsa Padr´o and Llu´ıs Padr´o. 2004. Comparing methods for language identification. Procesamiento del Lenguaje Natural, 33:155–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="11893" citStr="Pietra et al. (1997)" startWordPosition="1752" endWordPosition="1755">n. Features used to discriminate seem to be languagegroup specific, altough word rather than character features seem to perform better (Zampieri and Gebre (2012) report best results for character 4-grams, however, given that European and Brazilian Portuguese do not completely share ortography). 3 Maximum Entropy Models and Feature Engineering Maximum Entropy modelling is a general purpose machine learning framework that has proven to be highly expressive and powerful in many areas. Maximum Entropy (maxent) was first introduced into natural language processing by Berger et al. (1996) and Della Pietra et al. (1997). Since its introduction, 122 Maximum Entropy techniques and the more general framework of Random Fields have been applied extensively to natural language processing problems, where maxent classifiers are commonly used as an alternative to Naive Bayes classifiers. In maxent modelling, the probability that an example x is in a class c is estimated from its bag of words (or n-grams) as: 1 � p(c|x) = Z exp wci · fi(c, y) N y∈bow(x) i=1 where fi(c, y) are indicator functions, wci is the weight assigned to feature i in class c, and Z is a normalization factor. Features are modelled by indicator fun</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. A. Della Pietra, V. J. Della Pietra, and J. Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bali Ranaivo-Malanc¸on</author>
</authors>
<title>Automatic Identification of Close Languages – Case study: Malay and Indonesian.</title>
<date>2006</date>
<journal>ECTI Transactions on Computer and Information Technology,</journal>
<volume>2</volume>
<issue>2</issue>
<marker>Ranaivo-Malanc¸on, 2006</marker>
<rawString>Bali Ranaivo-Malanc¸on. 2006. Automatic Identification of Close Languages – Case study: Malay and Indonesian. ECTI Transactions on Computer and Information Technology, 2(2):126–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Marcos Zampieri</author>
<author>Nikola Ljubesi´c</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Merging comparable data sources for the discrimination of similar languages: The DSL corpus collection.</title>
<date>2014</date>
<booktitle>In Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC).</booktitle>
<marker>Tan, Zampieri, Ljubesi´c, Tiedemann, 2014</marker>
<rawString>Liling Tan, Marcos Zampieri, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. Merging comparable data sources for the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
</authors>
<title>A report on the first native language identification shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on the Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>48--57</pages>
<contexts>
<context position="1596" citStr="Tetreault et al., 2013" startWordPosition="225" endWordPosition="228"> in this article1 has been ranked second in the Discriminating Similar Language (DSL) shared task co-located within the VarDial Workshop at COLING 2014 (Zampieri et al., 2014). 1 Introduction Language identification (LI) can be defined as the task of determining the language of a written text. LI is also a cross-cutting technology supporting many other text analysis tasks: sentiment analysis, political tendency or topic classification. There are some interesting problems around written language identification that have attracted some attention recently, as native language identification (NLI, Tetreault et al., 2013), the identification of the country of origin or the discrimination between similar or closely related languages (DSL, Tiedemann and Ljubeˇsi´c, 2012). LI has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant. However, according to Zampieri (2013), multilingualism, noisy or non-standard features in text and discrimination between similar languages, varieties or dialects remain as the major known bottlenecks in language identification. For this reason, DSL can be considered as a su</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, 2013</marker>
<rawString>Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared task. In Proceedings of the Eighth Workshop on the Innovative Use of NLP for Building Educational Applications, pages 48–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
<author>Nikola Ljubesi´c</author>
</authors>
<title>Efficient discrimination between closely related languages.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>2619--2634</pages>
<marker>Tiedemann, Ljubesi´c, 2012</marker>
<rawString>J¨org Tiedemann and Nikola Ljube&amp;quot;si´c. 2012. Efficient discrimination between closely related languages. In Proceedings of COLING 2012, pages 2619–2634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Trieschnigg</author>
<author>D Hiemstra</author>
<author>M Theune</author>
<author>F M G de Jong</author>
<author>T Meder</author>
</authors>
<title>An exploration of language identification techniques for the Dutch folktale database.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Adaptation of Language Resources and Tools for Processing Cultural Heritage (LREC</booktitle>
<pages>47--51</pages>
<marker>Trieschnigg, Hiemstra, Theune, de Jong, Meder, 2012</marker>
<rawString>R.B. Trieschnigg, D. Hiemstra, M. Theune, F.M.G. de Jong, and T. Meder. 2012. An exploration of language identification techniques for the Dutch folktale database. In Proceedings of the Workshop on Adaptation of Language Resources and Tools for Processing Cultural Heritage (LREC 2012), pages 47–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Trudgill</author>
</authors>
<title>Glocalisation and the Ausbau sociolinguistics of modern Europe.</title>
<date>2004</date>
<booktitle>In Anna Duszak and Urszula Okulska, editors, Speaking from the Margin: Global English from a European Perspective,</booktitle>
<pages>35--49</pages>
<editor>Peter Lang,</editor>
<location>Frankfurt am Main.</location>
<contexts>
<context position="2894" citStr="Trudgill, 2004" startWordPosition="411" endWordPosition="412"> what Kloss (1967) called abstandsprache or language by distance (because Basque is an isolate, it is generally regarded as a distant language) but fails in dealing with ausbausprache or language by development (a standard variety together with all varieties heteronomous with respect to it, e. g. Basque Batua koin´e and the various vernacular dialects). Mass media, educational centres, administrations and communications favour standard languages instead of other varieties. Standard varieties of languages are then seen by sociolinguists and dialectologists as political and cultural constructs (Trudgill, 2004). However, languages and varieties are not just systems for communication between individuals, they are also used by groups and they are a crucial part of their identity and culture. Language variation is systematic, both inter- and intra-personal. It can be related to political, social, geographical, situational, communicative or instrumental factors. Variation within a language can be found at different levels: alphabet, orthography (diacritics), word structure (syllable composition, morphology), lexical choice or even syntax. Similar or closely related languages often reflect a common origi</context>
</contexts>
<marker>Trudgill, 2004</marker>
<rawString>Peter Trudgill. 2004. Glocalisation and the Ausbau sociolinguistics of modern Europe. In Anna Duszak and Urszula Okulska, editors, Speaking from the Margin: Global English from a European Perspective, pages 35– 49. Peter Lang, Frankfurt am Main.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebre</author>
</authors>
<title>Automatic identification of language varieties: The case of Portuguese.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS 2012,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="9832" citStr="Zampieri and Gebre (2012)" startWordPosition="1440" endWordPosition="1443">ccuracy of 0.929. More recently, Zampieri (2013) compares the performance of n-gram based models to machine learning methods using bag of words when discriminating similar languages and varieties obtaining comparable performance with both approaches. Grouin et al. (2010) present the shared task DEFT 2010. Participants were challenged to identify the decade, country (France and Canada) and newspaper for a set of journalistic texts. As far as the country labeling is concerned, they report an upper 0.964 Fl-measure and an average of 0.767. Very brief descriptions of the systems are also offered. Zampieri and Gebre (2012) present a log-likelihood estimation method for language models built on orthographical (character n-grams), lexical (word unigrams) and lexico-syntactic (word bigrams) features. They report a 0.998 accuracy distinguishing European and Brazilian Portuguese with a language model based on character 4-grams. This approach is adapted in Zampieri et al. (2013) to deal with Spanish varieties, where the role of knowledge-rich features (POS tags) is also explored. They report a 0.99 accuracy when binarily distinguishing Argentinean and Mexican Spanish with single words or bigrams. Trieschnigg et al. (</context>
<context position="11434" citStr="Zampieri and Gebre (2012)" startWordPosition="1684" endWordPosition="1687">lassifiers to map English documents to their country of origin. An SVM classifier with bag of words is top ranked with a macro-average 0.911 Fiscore in a cross-domain setting and 0.975 in an in-domain setting. All these previous works (with the sole exception of Trieschnigg et al. (2012), where a general purpose LI system yields a satisfactory performance) agree in the specificity of DSL regarding LI. Maybe because of that, two level approaches are not uncommon. Features used to discriminate seem to be languagegroup specific, altough word rather than character features seem to perform better (Zampieri and Gebre (2012) report best results for character 4-grams, however, given that European and Brazilian Portuguese do not completely share ortography). 3 Maximum Entropy Models and Feature Engineering Maximum Entropy modelling is a general purpose machine learning framework that has proven to be highly expressive and powerful in many areas. Maximum Entropy (maxent) was first introduced into natural language processing by Berger et al. (1996) and Della Pietra et al. (1997). Since its introduction, 122 Maximum Entropy techniques and the more general framework of Random Fields have been applied extensively to nat</context>
<context position="21427" citStr="Zampieri and Gebre (2012)" startWordPosition="3460" endWordPosition="3463">there are instances of cross-information, e. g., Argentinian news about Spain and vice versa where maybe more of a topic rather than a variety is being detected (e. g., news about Urdangarin or Fern´andez de Kirchner); e) there are some typos and misspellings (carabanas, dosco) whose role remains unclear; e) finally, there is at least one text misclassified in the gold standard: it is labeled as Argentinian but it was written by the Spanish EFE news agency. Some of these difficulties cross-cut all language groups and are not specific to Spanish but rather to DSL as a task. In contrast to what Zampieri and Gebre (2012) found, ten-fold cross-validation on the training dataset for different feature settings on the DSL dataset did not find character n-grams to outperform word ngrams for group D (Portuguese). It could be hypothesized that they used a unique source (newspaper) for each variety and therefore rigid editorial conventions could be at play; moreover, the collections were 5Zampieri and Gebre (2012) highlight the importance of proper nouns when using word n-grams. 125 three years distant, so topic consistency could also be compromised6. Manual inspection of mislabeled sentences shows some already known</context>
</contexts>
<marker>Zampieri, Gebre, 2012</marker>
<rawString>Marcos Zampieri and Binyam Gebre. 2012. Automatic identification of language varieties: The case of Portuguese. In Proceedings of KONVENS 2012, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
<author>Sascha Diwersy</author>
</authors>
<title>N-gram language models and POS distribution for the identification of Spanish varieties.</title>
<date>2013</date>
<booktitle>In Proceedings of TALN2013,</booktitle>
<pages>580--587</pages>
<contexts>
<context position="7006" citStr="Zampieri et al., 2013" startWordPosition="1029" endWordPosition="1032">oposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui and Baldwin, 2012). While it is true that it is possible to obtain brilliant results for a small number of languages (Baldwin and Lui, 2010) or typologically distant languages (Zampieri et al., 2013), accurately discriminating among closely related languages or varieties of the same language has been repeatedly reported as a bottleneck for language identification systems, in particular for those based on n-grams. 2See McNamee (2005) eloquent title. 3http://odur.let.rug.nl/vannoord/TextCat 4http://open.xerox.com/Services/LanguageIdentifier 121 Back in 2004, Padr´o and Padr´o concluded that “since the tested systems tend to fail when distinguishing similar languages (e.g. Spanish and Catalan), further research could be done to solve these cases.” Martins and Silva (2005) report similar diff</context>
<context position="10189" citStr="Zampieri et al. (2013)" startWordPosition="1489" endWordPosition="1492">country (France and Canada) and newspaper for a set of journalistic texts. As far as the country labeling is concerned, they report an upper 0.964 Fl-measure and an average of 0.767. Very brief descriptions of the systems are also offered. Zampieri and Gebre (2012) present a log-likelihood estimation method for language models built on orthographical (character n-grams), lexical (word unigrams) and lexico-syntactic (word bigrams) features. They report a 0.998 accuracy distinguishing European and Brazilian Portuguese with a language model based on character 4-grams. This approach is adapted in Zampieri et al. (2013) to deal with Spanish varieties, where the role of knowledge-rich features (POS tags) is also explored. They report a 0.99 accuracy when binarily distinguishing Argentinean and Mexican Spanish with single words or bigrams. Trieschnigg et al. (2012) compare the performance of TextCat to the nearest neighbour and nearest prototype in combination with a cosine distance when distinguishing among sixteen varieties of Dutch. They report a micro-average Fl-score of 0.799 (and a macro-average Fl-score of 0.527) with a top Fl-score of 0.987 when dealing with Frisian. Lui and Cook (2013) report experime</context>
<context position="18859" citStr="Zampieri et al. (2013)" startWordPosition="2932" endWordPosition="2935"> (2012) avoid biases towards topic and domain by experimenting with parallel texts reaching an overall accuracy of 90.3% for group A (br, hr, sr) using a ‘black list’ classifier and comparing its results with a Naive Bayes approach. They found that the ‘black list’ classifier generalise better than the Naive Bayes approach when moving from parallel to comparable corpora, since the former classifier is based on more informative features than the later. Results of ten-fold cross-validation on the training dataset for different feature settings for group E (Spanish) were consistent with those of Zampieri et al. (2013), where word bigrams are reported to 124 bs hr sr id my cz sk pt-BR pt-PT es-AR es-ES en-GB en-US bs 875 61 64 0 0 0 0 0 0 0 0 0 0 hr 60 931 9 0 0 0 0 0 0 0 0 0 0 sr 33 16 951 0 0 0 0 0 0 0 0 0 0 id 0 0 0 996 4 0 0 0 0 0 0 0 0 my 0 0 0 9 991 0 0 0 0 0 0 0 0 cz 0 0 0 0 0 1,000 0 0 0 0 0 0 0 sk 0 0 0 0 0 0 1,000 0 0 0 0 0 0 pt-BR 0 0 0 0 0 0 0 964 36 0 0 0 0 pt-PT 0 0 0 0 0 0 0 69 931 0 0 0 0 es-AR 0 0 0 0 0 0 0 0 0 819 181 0 0 es-ES 0 0 0 0 0 0 0 0 0 43 957 0 0 en-GB 0 0 0 0 0 0 0 0 0 0 0 571 229 en-US 0 0 0 0 0 0 0 0 0 0 0 602 198 Table 4: Confusion matrix for the hierarchical maxent classifie</context>
<context position="20106" citStr="Zampieri et al. (2013)" startWordPosition="3245" endWordPosition="3248">ies in the DSL test dataset. The 1,000 Bosnian texts have been classified as Bosnian (875), Croatian (61) and Serbian (64). Group Language/Variety Code Bosnian bs A Croatian hr Serbian sr Indonesian id B Malay my Czech cz C Slovak sk Brazilian Portuguese pt-BR D European Portuguese pt-PT Argentine Spanish es-AR E European Spanish es-ES British English en-GB F American English en-US Table 5: Languages and varieties groups and codes. outperform character n-grams. Given that datasets are not identical, it is difficult to draw any conclusion from the 1.2% difference in accuracy between DSLRAE and Zampieri et al. (2013). Manual inspection of misclassified news suggests some textual properties that are specially challenging: a) high density of foreign proper names (Russian, Baby, Pony, Jack, ...) may dilute the evidence provided by vernacular words; b) conversely, low density of features specific to any variant (such as place or family names5, demonyms, lexical choices) may be insufficient to drive the text to the right class; this is also the case of some perfectly neutral sentences where a trained linguist could not spot any clue about their origin; c) certain syntactical idiosyncrasies (for example Argenti</context>
<context position="24246" citStr="Zampieri et al. (2013)" startWordPosition="3865" endWordPosition="3868"> Conclusions and Future Work In this paper, we have shown that a hierarchical classifier is well suited to discriminate among different language groups and languages or varieties therein. Different features are shown to better suit typological traits of supported languages. A comparison to previous approaches is provided, when available. In a multilingual setting, the effect of adding Galician to group D could be investigated. Focusing on Spanish language, we plan to geographically expand the classifier to deal with all national varieties, a much harder task as both Baldwin and Lui (2010) and Zampieri et al. (2013) remark. Moreover, the classifier could be used, as Tiedemann and Ljubeˇsi´c (2012) suggest, to learn varieties discriminators to label texts beyond national classes (e.g. both Caribbean and Andean Spanish cross-cut national borders and, conversely, nations involved are known not to be dialectally uniform). Given that error analysis showed that word bigrams fail to capture certain syntactical idiosyncrasies, a model with longer n-grams and/or knowledge-richer features such as POS sequences could also be explored, although Zampieri et al. (2013) report lower performance than knowledge-poor feat</context>
</contexts>
<marker>Zampieri, Gebre, Diwersy, 2013</marker>
<rawString>Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580–587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Liling Tan</author>
<author>Nikola Ljubesi´c</author>
<author>J¨org Tiedemann</author>
</authors>
<title>A report on the DSL shared task</title>
<date>2014</date>
<booktitle>In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial).</booktitle>
<marker>Zampieri, Tan, Ljubesi´c, Tiedemann, 2014</marker>
<rawString>Marcos Zampieri, Liling Tan, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. A report on the DSL shared task 2014. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
</authors>
<title>Using bag-of-words to distinguish similar languages: How efficient are they?</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI2013),</booktitle>
<pages>37--41</pages>
<contexts>
<context position="1958" citStr="Zampieri (2013)" startWordPosition="278" endWordPosition="279">asks: sentiment analysis, political tendency or topic classification. There are some interesting problems around written language identification that have attracted some attention recently, as native language identification (NLI, Tetreault et al., 2013), the identification of the country of origin or the discrimination between similar or closely related languages (DSL, Tiedemann and Ljubeˇsi´c, 2012). LI has reached a great success in discriminating between languages with unique character sets and languages belonging to different language groups or typologically distant. However, according to Zampieri (2013), multilingualism, noisy or non-standard features in text and discrimination between similar languages, varieties or dialects remain as the major known bottlenecks in language identification. For this reason, DSL can be considered as a sub-task in language identification. Interestingly enough, LI seems to work well with what Kloss (1967) called abstandsprache or language by distance (because Basque is an isolate, it is generally regarded as a distant language) but fails in dealing with ausbausprache or language by development (a standard variety together with all varieties heteronomous with re</context>
<context position="6327" citStr="Zampieri (2013)" startWordPosition="926" endWordPosition="927">the first to attempt computational language identification. In particular, two common LI approaches, namely n-gram language models and white (or black) lists, echo Gold’s information presentation methods. In the 1990s, language identification was formulated as a sub-task of text categorization and varied approaches were explored. Beesley (1988) pioneered the use of character n-grams models, which were also used by Dunning (1994) and Cavnar and Trenkle (1994). Grefenstette (1995) compared this approach to Ingle (1978), based on the frequency of short words. The interested reader is referred to Zampieri (2013) for a review of some statistical and machine learning proposals and to both Baldwin and Lui (2010) and Lui and Baldwin (2011) for an overview of some linguistically motivated models. As Baldwin and Lui (2010) or Tiedemann and Ljubeˇsi´c (2012) point out, language identification is erroneously considered an easy and solved problem2, in part because of some general purpose systems being available, notably TextCat3, Xerox Language Identifier4 and, more recently, langid.py (Lui and Baldwin, 2012). While it is true that it is possible to obtain brilliant results for a small number of languages (Ba</context>
<context position="9255" citStr="Zampieri (2013)" startWordPosition="1354" endWordPosition="1355">ies on ‘black lists’ to discard nonBalkan languages and a second order Markov model on n-grams to discriminate among them, augmented with a ‘black list’ component that raises accuracy up to 0.99 when dealing with the most difficult pair (Croatian and Serbian). This work is followed up in Tiedemann and Ljubeˇsi´c (2012) where 9% of improvement over standard approaches is reported and where support for Bosnian discrimination is included. Huang and Lee (2008) use a bag of the most frequent words to build a voting identifier for three Chinese varieties with a top accuracy of 0.929. More recently, Zampieri (2013) compares the performance of n-gram based models to machine learning methods using bag of words when discriminating similar languages and varieties obtaining comparable performance with both approaches. Grouin et al. (2010) present the shared task DEFT 2010. Participants were challenged to identify the decade, country (France and Canada) and newspaper for a set of journalistic texts. As far as the country labeling is concerned, they report an upper 0.964 Fl-measure and an average of 0.767. Very brief descriptions of the systems are also offered. Zampieri and Gebre (2012) present a log-likeliho</context>
</contexts>
<marker>Zampieri, 2013</marker>
<rawString>Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they? In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics (CINTI2013), pages 37–41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
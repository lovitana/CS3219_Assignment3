<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000262">
<title confidence="0.9980965">
Improving Agreement and Disagreement Identification
in Online Discussions with A Socially-Tuned Sentiment Lexicon
</title>
<author confidence="0.998969">
Lu Wang
</author>
<affiliation confidence="0.9970075">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.775628">
Ithaca, NY 14853
</address>
<email confidence="0.999253">
luwang@cs.cornell.edu
</email>
<sectionHeader confidence="0.997394" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940238095238">
We study the problem of agreement and
disagreement detection in online discus-
sions. An isotonic Conditional Random
Fields (isotonic CRF) based sequential
model is proposed to make predictions
on sentence- or segment-level. We auto-
matically construct a socially-tuned lex-
icon that is bootstrapped from existing
general-purpose sentiment lexicons to fur-
ther improve the performance. We eval-
uate our agreement and disagreement tag-
ging model on two disparate online discus-
sion corpora – Wikipedia Talk pages and
online debates. Our model is shown to
outperform the state-of-the-art approaches
in both datasets. For example, the iso-
tonic CRF model achieves F1 scores of
0.74 and 0.67 for agreement and disagree-
ment detection, when a linear chain CRF
obtains 0.58 and 0.56 for the discussions
on Wikipedia Talk pages.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99981825">
We are in an era where people can easily voice and
exchange their opinions on the internet through
forums or social media. Mining public opinion
and the social interactions from online discus-
sions is an important task, which has a wide range
of applications. For example, by analyzing the
users’ attitude in forum posts on social and po-
litical problems, it is able to identify ideological
stance (Somasundaran and Wiebe, 2009) and user
relations (Qiu et al., 2013), and thus further dis-
cover subgroups (Hassan et al., 2012; Abu-Jbara
et al., 2012) with similar ideological viewpoint.
Meanwhile, catching the sentiment in the conver-
sation can help detect online disputes, reveal popu-
lar or controversial topics, and potentially disclose
the public opinion formation process.
</bodyText>
<author confidence="0.433433">
Claire Cardie
</author>
<affiliation confidence="0.9698925">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.571519">
Ithaca, NY 14853
</address>
<email confidence="0.990023">
cardie@cs.cornell.edu
</email>
<bodyText confidence="0.994953897435897">
In this work, we study the problem of agreement
and disagreement identification in online discus-
sions. Sentence-level agreement and disagreement
detection for this domain is challenging in its own
right due to the dynamic nature of online conversa-
tions, and the less formal, and usually very emo-
tional language used. As an example, consider a
snippet of discussion from Wikipedia Talk page
for article “Iraq War” where editors argue on the
correctness of the information in the opening para-
graph (Figure 1). “So what?” should presumably
be tagged as a negative sentence as should the sen-
tence “If you’re going to troll, do us all a favor
and stick to the guidelines.”. We hypothesize that
these, and other, examples will be difficult for the
tagger unless the context surrounding each sen-
tence is considered and in the absence of a sen-
timent lexicon tuned for conversational text (Ding
et al., 2008; Choi and Cardie, 2009).
As a result, we investigate isotonic Condi-
tional Random Fields (isotonic CRF) (Mao and
Lebanon, 2007) for the sentiment tagging task
since they preserve the advantages of the popu-
lar CRF sequential tagging models (Lafferty et
al., 2001) while providing an efficient mechanism
to encode domain knowledge — in our case, a
sentiment lexicon — through isotonic constraints
on the model parameters. In particular, we boot-
strap the construction of a sentiment lexicon from
Wikipedia talk pages using the lexical items in ex-
isting general-purpose sentiment lexicons as seeds
and in conjunction with an existing label propaga-
tion algorithm (Zhu and Ghahramani, 2002).1
To summarize, our chief contributions include:
(1) We propose an agreement and disagree-
ment identification model based on isotonic Con-
ditional Random Fields (Mao and Lebanon, 2007)
to identify users’ attitude in online discussion.
Our predictions that are made on the sentence-
</bodyText>
<footnote confidence="0.9900015">
1Our online discussion lexicon (Section 4) will be made
publicly available.
</footnote>
<page confidence="0.992023">
97
</page>
<note confidence="0.875816428571429">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
Zer0faults: So questions comments feedback welcome.
Other views etc. I just hope we can remove the assertations
that WMD’s were in fact the sole reason for the US invasion,
considering that HJ Res 114 covers many many reasons.
&gt;Mr. Tibbs: So basically what you want to do is remove all
</note>
<bodyText confidence="0.983377064516129">
mention of the cassus belli of the Iraq War and try to create
the false impression that this military action was as inevitable
as the sunrise.[NN] No. Just because things didn’t turn out the
way the Bush administration wanted doesn’t give you license
to rewrite history.[NN] ...
&gt;&gt;MONGO: Regardless, the article is an antiwar propa-
ganda tool.[NN] ...
&gt;&gt;&gt;Mr. Tibbs: So what?[NN] That wasn’t the cassus
belli and trying to give that impression After the Fact is
Untrue.[NN] Hell, the reason it wasn’t the cassus belli is be-
cause there are dictators in Africa that make Saddam look like
a pussycat...
&gt;&gt;Haizum: Start using the proper format or it’s over for your
comments.[N] If you’re going to troll, do us all a favor and
stick to the guidelines.[N] ...
Tmorton166: Hi, I wonder if, as an outsider to this debate I
can put my word in here. I considered mediating this discus-
sion however I’d prefer just to comment and leave it at that :).
I agree mostly with what Zer0faults is saying[PP]....
&gt;Mr. Tibbs: Here’s the problem with that.[NN] It’s not about
publicity or press coverage. It’s about the fact that the Iraq
disarmament crisis set off the 2003 Invasion of Iraq.... And
theres a huge problem with rewriting the intro as if the Iraq
disarmament crisis never happened.[NN]
&gt;&gt;Tmorton166:... To suggest in the opening paragraph that
the ONLY reason for the war was WMD’s is wrong - because
it simply isn’t.[NN] However I agree that the emphasis needs
to be on the armaments crisis because it was the reason sold
to the public and the major one used to justify the invasion but
it needs to acknowledge that there was at least 12 reasons for
the war as well.[PP] ...
</bodyText>
<figureCaption confidence="0.721829818181818">
Figure 1: Example discussion from wikipedia talk page
for article “Iraq War”, where editors discuss about the cor-
rectness of the information in the opening paragraph. We
only show some sentences that are relevant for demonstra-
tion. Other sentences are omitted by ellipsis. Names of ed-
itors are in bold. “&gt;” is an indicator for the reply structure,
where turns starting with &gt; are response for most previous
turn that with one less &gt;. We use “NN”, “N”, and “PP” to in-
dicate “strongly disagree”, “disagree”, and “strongly agree”.
Sentences in blue are examples whose sentiment is hard to
detect by an existing lexicon.
</figureCaption>
<bodyText confidence="0.999653108108108">
or segment-level, are able to discover fine-grained
sentiment flow within each turn, which can be fur-
ther applied in other applications, such as dispute
detection or argumentation structure analysis. We
employ two existing online discussion data sets:
the Authority and Alignment in Wikipedia Dis-
cussions (AAWD) corpus of Bender et al. (2011)
(Wikipedia talk pages) and the Internet Argu-
ment Corpus (IAC) of Walker et al. (2012a). Ex-
perimental results show that our model signifi-
cantly outperforms state-of-the-art methods on the
AAWD data (our F1 scores are 0.74 and 0.67 for
agreement and disagreement, vs. 0.58 and 0.56 for
the linear chain CRF approach) and IAC data (our
F1 scores are 0.61 and 0.78 for agreement and dis-
agreement, vs. 0.28 and 0.73 for SVM).
(2) Furthermore, we construct a new senti-
ment lexicon for online discussion. We show
that the learned lexicon significantly improves per-
formance over systems that use existing general-
purpose lexicons (i.e. MPQA lexicon (Wilson et
al., 2005), General Inquirer (Stone et al., 1966),
and SentiWordNet (Esuli and Sebastiani, 2006)).
Our lexicon is constructed from a very large-scale
discussion corpus based on Wikipedia talk page,
where previous work (Somasundaran and Wiebe,
2010) for constructing online discussion lexicon
relies on human annotations derived from limited
number of conversations.
In the remainder of the paper, we describe first
the related work (Section 2). Then we intro-
duce the sentence-level agreement and disagree-
ment identification model (Section 3) as well as
the label propagation algorithm for lexicon con-
struction (Section 4). After explain the experimen-
tal setup, we display the results and provide further
analysis in Section 6.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999155037037037">
Sentiment analysis has been utilized as a key en-
abling technique in a number of conversation-
based applications. Previous work mainly stud-
ies the attitudes in spoken meetings (Galley et al.,
2004; Hahn et al., 2006) or broadcast conversa-
tions (Wang et al., 2011) using Conditional Ran-
dom Fields (CRF) (Lafferty et al., 2001). Galley
et al. (2004) employ Conditional Markov models
to detect if discussants reach at an agreement in
spoken meetings. Each state in their model is an
individual turn and prediction is made on the turn-
level. In the same spirit, Wang et al. (2011) also
propose a sequential model based on CRF for de-
tecting agreements and disagreements in broadcast
conversations, where they primarily show the ef-
ficiency of prosodic features. While we also ex-
ploit a sequential model extended from CRFs, our
predictions are made for each sentence or segment
rather than at the turn-level. Moreover, we experi-
ment with online discussion datasets that exhibit
a more realistic distribution of disagreement vs.
agreement, where much more disagreement is ob-
served due to its function and the relation between
the participants. This renders the detection prob-
lem more challenging.
Only recently, agreement and disagreement de-
tection is studied for online discussion, especially
</bodyText>
<page confidence="0.997888">
98
</page>
<bodyText confidence="0.999984754385965">
for online debate. Abbott et al. (2011) investi-
gate different types of features based on depen-
dency relations as well as manually-labeled fea-
tures, such as if the participants are nice, nasty,
or sarcastic, and respect or insult the target par-
ticipants. Automatically inducing those features
from human annotation are challenging itself, so
it would be difficult to reproduce their work on
new datasets. We use only automatically gener-
ated features. Using the same dataset, Misra and
Walker (2013) study the effectiveness of topic-
independent features, e.g. discourse cues indicat-
ing agreement or negative opinion. Those cues,
which serve a similar purpose as a sentiment lex-
icon, are also constructed manually. In our work,
we create an online discussion lexicon automat-
ically and construct sentiment features based on
the lexicon. Also targeting online debate, Yin et
al. (2012) train a logistic regression classifier with
features aggregating posts from the same partici-
pant to predict the sentiment for each individual
post. This approach works only when the speaker
has enough posts on each topic, which is not ap-
plicable to newcomers. Hassan et al. (2010) focus
on predicting the attitude of participants towards
each other. They relate the sentiment words to
the second person pronoun, which produces strong
baselines. We also adopt their baselines in our
work. Although there are available datasets with
(dis)agreement annotated on Wikipedia talk pages,
we are not aware of any published work that uti-
lizes these annotations. Dialogue act recognition
on talk pages (Ferschke et al., 2012) might be the
most related.
While detecting agreement and disagreement in
conversations is useful on its own, it is also a key
component for related tasks, such as stance pre-
diction (Thomas et al., 2006; Somasundaran and
Wiebe, 2009; Walker et al., 2012b) and subgroup
detection (Hassan et al., 2012; Abu-Jbara et al.,
2012). For instance, Thomas et al. (2006) train an
agreement detection classifier with Support Vec-
tor Machines on congressional floor-debate tran-
scripts to determine whether the speeches repre-
sent support of or opposition to the proposed leg-
islation. Somasundaran and Wiebe (2009) design
various sentiment constraints for inclusion in an
integer linear programming framework for stance
classification. For subgroup detection, Abu-Jbara
et al. (2012) uses the polarity of the expressions in
the discussions and partition discussants into sub-
groups based on the intuition that people in the
same group should mostly agree with each other.
Though those work highly relies on the compo-
nent of agreement and disagreement detection, the
evaluation is always performed on the ultimate ap-
plication only.
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="method">
3 The Model
</sectionHeader>
<bodyText confidence="0.999979285714286">
We first give a brief overview on isotonic Con-
ditional Random Fields (isotonic CRF) (Mao and
Lebanon, 2007), which is used as the backbone
approach for our sentence- or segment-level agree-
ment and disagreement detection model. We defer
the explanation of online discussion lexicon con-
struction in Section 4.
</bodyText>
<subsectionHeader confidence="0.997171">
3.1 Problem Description
</subsectionHeader>
<bodyText confidence="0.9999835">
Consider a discussion comprised of sequential
turns uttered by the participants; each turn con-
sists of a sequence of text units, where each unit
can be a sentence or a segment of several sen-
tences. Our model takes as input the text units
x = {x1, · · · , xn} in the same turn, and outputs
a sequence of sentiment labels y = {y1, · · · , yn},
where yi E O, O = {NN, N, O, P, PP}. The la-
bels in O represent strongly disagree (NN), dis-
agree (N), neutral (O), agree (P), strongly agree
(PP), respectively. In addition, elements in the
partially ordered set O possess an ordinal relation
G. Here, we differentiate agreement and disagree-
ment with different intensity, because the output
of our classifier can be used for other applications,
such as dispute detection, where “strongly dis-
agree” (e.g. NN) plays an important role. Mean-
while, fine-grained sentiment labels potentially
provide richer context information for the sequen-
tial model employed for this task.
</bodyText>
<subsectionHeader confidence="0.998783">
3.2 Isotonic Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.9923175">
Conditional Random Fields (CRF) have been suc-
cessfully applied in numerous sequential labeling
tasks (Lafferty et al., 2001). Given a sequence
of utterances or segments x = {x1, · · · , xn}, ac-
cording to linear-chain CRF, the probability of the
labels y for x is given by:
</bodyText>
<equation confidence="0.904064">
1 � �
p(y|x) = Z(x) exp( λ(σ,τ)f(σ,τ)(yi−1, yi)
i σ,τ
µ(σ,w)g(σ,w)(yi, xi))
(1)
�
σ,w
+�
i
</equation>
<page confidence="0.975113">
99
</page>
<bodyText confidence="0.999892588235294">
f(σ,τ)(yi_1, yi) and g(σ,w)(yi, xi) are feature
functions. Given that yi_1, yi, xi take values of
Q, T, w, the functions are indexed by pairs (Q, T)
and (Q, w). A(σ,τ), µ(σ,w) are the parameters.
CRF, as defined above, is not appropriate for or-
dinal data like sentiment, because it ignores the
ordinal relation among sentiment labels. Isotonic
Conditional Random Fields (isotonic CRF) are
proposed by Mao and Lebanon (2007) to enforce a
set of monotonicity constraints on the parameters
that are consistent with the ordinal structure and
domain knowledge (in our case, a sentiment lexi-
con automatically constructed from online discus-
sions).
Given a lexicon M = Mp U Mn, where Mp
and Mn are two sets of features (usually words)
identified as strongly associated with positive sen-
timent and negative sentiment. The constraints are
encoded as below. For each feature w E Mp, iso-
tonic CRF enforces Q &lt; Q&apos; ==&gt;. µ(σ,w) &lt; µ(σ,,w).
Intuitively, the parameters µ(σ,w) are intimately
tied to the model probabilities. When a feature
such as “totally agree” is observed in the training
data, the feature parameter for µ(PP,totally agree) is
likely to increase. Similar constraints are also de-
fined on Mn. In this work, we boostrap the con-
struction of an online discussion sentiment lexicon
used as M in the isotonic CRF (see Section 4).
The parameters can be found by maximizing the
likelihood subject to the monotonicity constraints.
We adopt the re-parameterization from Mao and
Lebanon (2007) for a simpler optimization prob-
lem, and refer the readers to Mao and Lebanon
(2007) for more details.2
</bodyText>
<subsectionHeader confidence="0.989657">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.994817090909091">
The features used in sentiment prediction are listed
in Table 1. Features with numerical values are first
normalized by standardization, then binned into 5
categories.
Syntactic/Semantic Features. Dependency re-
lations have been shown to be effective for various
sentiment prediction tasks (Joshi and Penstein-
Ros´e, 2009; Somasundaran and Wiebe, 2009;
Hassan et al., 2010; Abu-Jbara et al., 2012). We
have two versions of dependency relation as fea-
tures, one being the original form, another gen-
</bodyText>
<footnote confidence="0.582726">
2The full implementation is based on MALLET (McCal-
lum, 2002). We thank Yi Mao for sharing the implementation
of the core learning algorithm.
</footnote>
<table confidence="0.902783684210526">
Lexical Features
- unigram/bigram
- num of words all uppercased
- num of words
Discourse Features
- initial uni-/bi-/trigram
- repeated punctuations
- hedging (Farkas et al., 2010)
- number of negators
Syntactic/Semantic Features
- unigram with POS tag
- dependency relation
Conversation Features
- quote overlap with target
- TFIDF similarity with target (remove quote first)
Sentiment Features
- connective + sentiment words
- sentiment dependency relation
- sentiment words
</table>
<tableCaption confidence="0.999582">
Table 1: Features used in sentiment prediction.
</tableCaption>
<bodyText confidence="0.990114066666667">
eralizing a word to its POS tag in turn. For in-
stance, “nsubj(wrong, you)” is generlized as the
“nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. We
use Stanford parser (de Marneffe et al., 2006) to
obtain parse trees and dependency relations.
Discourse Features. Previous work (Hirschberg
and Litman, 1993; Abbott et al., 2011) suggests
that discourse markers, such as what?, actually,
may have their use for expressing opinions. We
extract the initial unigram, bigram, and trigram of
each utterance as discourse features (Hirschberg
and Litman, 1993). Hedge words are collected
from the CoNLL-2012 shared task (Farkas et al.,
2010).
Conversation Features. Conversation features
encode some useful information regarding the
similarity between the current utterance(s) and the
sentences uttered by the target participant. TFIDF
similarity is computed. We also check if the cur-
rent utterance(s) quotes target sentences and com-
pute its length.
Sentiment Features. We gather connectives
from Penn Discourse TreeBank (Rashmi Prasad
and Webber, 2008) and combine them with any
sentiment word that precedes or follows it as
new features. Sentiment dependency relations are
the subset of dependency relations with sentiment
words. We replace those words with their polarity
equivalents. For example, relation “nsubj(wrong,
you)” becomes “nsubj(SentiWordneg, you)”.
</bodyText>
<page confidence="0.743816">
100
</page>
<table confidence="0.999755833333333">
POSITIVE
please elaborate, nod, await response, from experiences, anti-war, profits, promises of, is undisputed,
royalty, sunlight, conclusively, badges, prophecies, in vivo, tesla, pioneer, published material, from god,
plea for, lend itself, geek, intuition, morning, anti SentiWordneg, connected closely, Rel(undertake,
to), intelligibility, Rel(articles, detailed), of noting, for brevity, Rel(believer, am), endorsements, testable,
source carefully
NEGATIVE
: (, TOT, ?!!, in contrast, ought to, whatever, Rel(nothing, you), anyway, Rel(crap, your), by facts, pur-
porting, disproven, Rel(judgement, our), Rel(demonstrating, you), opt for, subdue to, disinformation,
tornado, heroin, Rel(newbies, the), Rel (intentional, is), pretext, watergate, folly, perjury, Rel(lock, ar-
ticle), contrast with, poke to, censoring information, partisanship, insurrection, bigot, Rel(informative,
less), clowns, Rel(feeling, mixed), never-ending
</table>
<tableCaption confidence="0.9965195">
Table 2: Example terms and relations from our online discussion lexicon. We choose for display terms
that do not contain any seed word.
</tableCaption>
<sectionHeader confidence="0.998025" genericHeader="method">
4 Online Discussion Sentiment Lexicon
Construction
</sectionHeader>
<bodyText confidence="0.900431733333333">
So far as we know, there is no lexicon available
for online discussions. Thus, we create from a
large-scale corpus via label propagation. The la-
bel propagation algorithm, proposed by Zhu and
Ghahramani (2002), is a semi-supervised learning
method. In general, it takes as input a set of seed
samples (e.g. sentiment words in our case), and
the similarity between pairwise samples, then it-
eratively assigns values to the unlabeled samples
(see Algorithm 1). The construction of graph G is
discussed in Section 4.1. Sample sentiment words
in the new lexicon are listed in Table 2.
Algorithm 1: The label propagation algo-
rithm (Zhu and Ghahramani, 2002) used for
constructing online discussion lexicon.
</bodyText>
<subsectionHeader confidence="0.961146">
4.1 Graph Construction
</subsectionHeader>
<bodyText confidence="0.997960916666666">
Node Set V . Traditional lexicons, like General
Inquirer (Stone et al., 1966), usually consist of po-
larized unigrams. As we mentioned in Section 1,
unigrams lack the capability of capturing the sen-
timent conveyed in online discussions. Instead,
bigrams, dependency relations, and even punctu-
ation can serve as supplement to the unigrams.
Therefore, we consider four types of text units as
nodes in the graph: unigrams, bigrams, depen-
dency relations, sentiment dependency relations.
Sentiment dependency relations are described in
Section 3.3. We replace all relation names with a
general label. Text units that appear in at least 10
discussions are retained as nodes to reduce noise.
Edge Set E. As Velikovich et al. (2010) and
Feng et al. (2013) notice, a dense graph with a
large number of nodes is susceptible to propagat-
ing noise, and will not scale well. We thus adopt
the algorithm in Feng et al. (2013) to construct
a sparsely connected graph. For each text unit t,
we first compute its representation vector a� using
Pairwise Mutual Information scores with respect
to the top 50 co-occuring text units. We define
“co-occur” as text units appearing in the same sen-
tence. An edge is created between two text units
t0 and t1 only if they ever co-occur. The similar-
ity between t0 and t1 is calculated as the Cosine
similarity between d0 and d1.
Seed Words. The seed sentiment are collected
from three existing lexicons: MPQA lexicon, Gen-
eral Inquirer, and SentiWordNet. Each word in
SentiWordNet is associated with a positive score
and a negative score; words with a polarity score
Input : G = (V, E), wiz E [0, 1], positive
seed words P, negative seed words
N, number of iterations T
</bodyText>
<equation confidence="0.998135714285714">
Output: v|−1
P {yi}i=0
yi=1.0,VviEP
yi = −1.0, Vvi E N
yi = 0.0, Vvi E/ PUN
fort = 1 ··· T do
rl
(vi,vj)∈E wijXyj
wi . , Vvi E V
� (vi,vj)∈E 7
yi = 1.0, Vvi E P
yi = −1.0, Vvi E N
end
yi =
</equation>
<page confidence="0.978961">
101
</page>
<bodyText confidence="0.9987705">
larger than 0.7 are retained. We remove words
with conflicting sentiments.
</bodyText>
<subsectionHeader confidence="0.965908">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999189285714286">
The graph is constructed based on Wikipedia talk
pages. We download the 2013-03-04 Wikipedia
data dump, which contains 4,412,582 talk pages.
Since we are interested in conversational lan-
guages, we filter out talk pages with fewer than
5 participants. This results in a dataset of 20,884
talk pages, from which the graph is constructed.
</bodyText>
<sectionHeader confidence="0.997543" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.838462">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.988052733333333">
Wikipedia Talk pages. The first dataset we use
is Authority and Alignment in Wikipedia Dis-
cussions (AAWD) corpus (Bender et al., 2011).
AAWD consists of 221 English Wikipedia discus-
sions with agreement and disagreement annota-
tions.3
The annotation of AAWD is made at utterance-
or turn-level, where a turn is defined as continu-
ous body of text uttered by the same participant.
Annotators either label each utterance as agree-
ment, disagreement or neutral, and select the cor-
responding spans of text, or label the full turn.
Each turn is annotated by two or three people. To
induce an utterance-level label for instances that
have only a turn-level label, we assume they have
the same label as the turn.
To train our sentiment model, we further trans-
form agreement and disagreement labels (i.e. 3-
way) into the 5-way labels. For utterances that
are annotated as agreement and have the text
span specified by at least two annotators, they are
treated as “strongly agree” (PP). If an utterance is
only selected as agreement by one annotator or it
gets the label by turn-level annotation, it is “agree”
(P). “Strongly disagree” (NN) and “disagree” (N)
are collected in the same way from disagreement
label. All others are neutral (O). In total, we have
16,501 utterances. 1,930 and 1,102 utterances are
labeled as “NN” and “N”. 532 and 99 of them are
“PP” and “P”. All other 12,648 are neutral sam-
ples. 4
3Bender et al. (2011) originally use positive alignment
and negative alignment to indicate two types of social moves.
They define those alignment moves as “agreeing or disagree-
ing” with the target. We thus use agreement and disagreement
instead of positive and negative alignment in this work.
4345 samples with both positive and negative labels are
treated as neutral.
Online Debate. The second dataset is the Inter-
net Argument Corpus (IAC) (Walker et al., 2012a)
collected from an online debate forum. Each dis-
cussion in IAC consists of multiple posts, where
we treat each post as a turn. Most posts (72.3%)
contain quoted content from the posts they target
at or other resources. A post can have more than
one quote, which naturally break the post into mul-
tiple segments. 1,806 discussions are annotated
with agreement and disagreement on the segment-
level from -5 to 5, with -5 as strongly disagree and
5 as strongly agree. We first compute the average
score for each segment among different annotators
and transform the score into sentiment label in the
following way. We treat [−5,−3] as NN (1595
segments), (−3, −1] as N (4548 segments), [1, 3)
as P (911 samples), [3, 5] as PP (199), all others as
O (290 segments).
In the test phase, utterances or segments pre-
dicted with NN or N are treated as disagreement;
the ones predicted as PP or P are agreement; O is
neutral.
</bodyText>
<subsectionHeader confidence="0.994356">
5.2 Comparison
</subsectionHeader>
<bodyText confidence="0.999862125">
We compare with two baselines. (1) Baseline (Po-
larity) is based on counting the sentiment words
from our lexicon. An utterance or segment is
predicted as agreement if it contains more posi-
tive words than negative words, or disagreement
if more negative words are observed. Other-
wise, it is neutral. (2) Baseline (Distance) is ex-
tended from (Hassan et al., 2010). Each sentiment
word is associated with the closest second per-
son pronoun, and a surface distance can be com-
puted between them. A classifier based on Sup-
port Vector Machines (Joachims, 1999) (SVM) is
trained with the features of sentiment words, min-
imum/maximum/average of the distances.
We also compare with two state-of-the-art
methods that are widely used in sentiment predic-
tion for conversations. The first one is an RBF
kernel SVM based approach, which has been used
for sentiment prediction (Hassan et al., 2010), and
(dis)agreement detection (Yin et al., 2012) in on-
line debates. The second is linear chain CRF,
which has been utilized for (dis)agreement iden-
tification in broadcast conversations (Wang et al.,
2011).
</bodyText>
<page confidence="0.995017">
102
</page>
<table confidence="0.999908692307692">
Strict F1 Soft F1
Agree Disagree Neutral Agree Disagree Neutral
Baseline (Polarity) 14.56 25.70 64.04 22.53 38.61 66.45
Baseline (Distance) 8.08 20.68 84.87 33.75 55.79 88.97
SVM (3-way) 26.76 35.79 77.39 44.62 52.56 80.84
+ downsampling 21.60 36.32 72.11 31.86 49.58 74.92
CRF (3-way) 20.99 23.85 85.28 56.28 56.37 89.41
CRF (5-way) 20.47 19.42 85.86 58.39 56.30 90.10
+ downsampling 24.26 31.28 77.12 47.30 46.24 80.18
isotonic CRF 24.32 21.95 86.26 68.18 62.53 88.87
+ downsampling 29.62 34.17 80.97 55.38 53.00 84.56
+ new lexicon 46.01 51.49 87.40 74.47 67.02 90.56
+ new lexicon + downsampling 47.90 49.61 81.60 64.97 58.97 84.04
</table>
<tableCaption confidence="0.997195">
Table 3: Strict and soft F1 scores for agreement and disagreement detection on Wikipedia talk pages
</tableCaption>
<bodyText confidence="0.976628">
(AAWD). All the numbers are multiplied by 100. In each column, bold entries (if any) are statistically
significantly higher than all the rest, and the italic entry has the highest absolute value. Our model based
on the isotonic CRF with the new lexicon produces significantly better results than all the other systems
for agreement and disagreement detection. Downsampling, however, is not always helpful.
</bodyText>
<sectionHeader confidence="0.999932" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999966">
In this section, we first show the experimental re-
sults on sentence- and segment-level agreement
and disagreement detection in two types of online
discussions – Wikipedia Talk pages and online de-
bates. Then we provide more detailed analysis for
the features used in our model. Furthermore, we
discuss several types of errors made in the model.
</bodyText>
<subsectionHeader confidence="0.998972">
6.1 Wikipedia Talk Pages
</subsectionHeader>
<bodyText confidence="0.99791347826087">
We evaluate the systems by standard F1 score on
each of the three categories: agreement, disagree-
ment, and neutral. For AAWD, we compute two
versions of F1 scores. Strict F1 is computed
against the true labels. For soft F1, if a sentence
is never labeled by any annotator on the sentence-
level and adopts its agreement/disagreement label
from the turn-level annotation, then it is treated as
a true positive when predicted as neutral.
Table 3 demonstrates our main results on the
Wikipedia Talk pages (AAWD dataset). With-
out downsampling, our isotonic CRF based sys-
tems with the new lexicon significantly outper-
form the compared approaches for agreement and
disagreement detection according to the paired-
t test (p &lt; 0.05). We also perform downsam-
pling by removing the turns only containing neu-
tral utterances. However, it does not always help
with performance. We suspect that, with less neu-
tral samples in the training data, the classifier is
less likely to make neutral predictions, which thus
decreases true positive predictions. For strict F-
scores on agreement/disagreement, downsampling
</bodyText>
<table confidence="0.9995107">
Agree Disagree Neu
Baseline (Polarity) 3.33 5.96 65.61
Baseline (Distance) 1.65 5.07 85.41
SVM (3-way) 25.62 69.10 31.47
+ new lexicon features 28.35 72.58 34.53
CRF (3-way) 29.46 74.81 31.93
CRF (5-way) 24.54 69.31 39.60
+ new lexicon features 28.85 71.81 39.14
isotonic CRF 53.40 76.77 44.10
+ new lexicon 61.49 77.80 51.43
</table>
<tableCaption confidence="0.979221">
Table 4: F1 scores for agreement and disagree-
</tableCaption>
<bodyText confidence="0.996952153846154">
ment detection on online debate (IAC). All the
numbers are multiplied by 100. In each column,
bold entries (if any) are statistically significantly
higher than all the rest, and the italic entry has the
highest absolute value except baselines. We have
two main observations: 1) Both of our models
based on isotonic CRF significantly outperform
other systems for agreement and disagreement de-
tection. 2) By adding the new lexicon, either as
features or constraints in isotonic CRF, all systems
achieve better F1 scores.
has mixed effect, but mostly we get slightly better
performance.
</bodyText>
<subsectionHeader confidence="0.994387">
6.2 Online Debates
</subsectionHeader>
<bodyText confidence="0.999948111111111">
Similarly, F1 scores for agreement, disagreement
and neutral for online debates (IAC dataset) are
displayed in Table 4. Both of our systems based
on isotonic CRF achieve significantly better F1
scores than the comparison. Especially, our sys-
tem with the new lexicon produces the best results.
For SVM and linear-chain CRF based systems, we
also add new sentiment features constructed from
the new lexicon as described in Section 3.3. We
</bodyText>
<page confidence="0.998637">
103
</page>
<bodyText confidence="0.998372">
can see that those sentiment features also boost the
performance for both of the compared approaches.
</bodyText>
<subsectionHeader confidence="0.999042">
6.3 Feature Evaluation
</subsectionHeader>
<bodyText confidence="0.999852384615385">
Moreover, we evaluate the effectiveness of fea-
tures by adding one type of features each time.
The results are listed in Table 5. As it can be seen,
the performance gets improved incrementally with
every new set of features.
We also utilize x2-test to highlight some of
the salient features on the two datasets. We can
see from Table 6 that, for online debates (IAC),
some features are highly topic related, such as “the
male” or “the scientist”. This observation concurs
with the conclusion in Misra and Walker (2013)
that features with topic information are indicative
for agreement and disagreement detection.
</bodyText>
<table confidence="0.999192166666667">
AAWD Agree Disagree Neu
Lex 40.77 52.90 79.65
Lex + Syn 68.18 63.91 88.87
Lex + Syn + Disc 70.93 63.69 89.32
Lex + Syn + Disc + Con 71.27 63.72 89.60
Lex + Syn + Disc + Con + Sent 74.47 67.02 90.56
IAC Agree Disagree Neu
Lex 56.65 75.35 45.72
Lex + Syn 54.16 75.13 46.12
Lex + Syn + Disc 54.27 76.41 47.60
Lex + Syn + Disc + Con 55.31 77.25 48.87
Lex + Syn + Disc + Con + Sent 61.49 77.80 51.43
</table>
<tableCaption confidence="0.991732">
Table 5: Results on Wikipedia talk page
</tableCaption>
<bodyText confidence="0.992705">
(AAWD) (with soft F1 score) and online de-
bate (IAC) with different feature sets (i.e Lexical,
Syntacitc/Semantic, Discourse, Conversation, and
Sentiment features) by using isotonic CRF. The
numbers in bold are statistically significantly
higher than the numbers above it (paired-t test,
p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.978643">
6.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999893615384615">
After a closer look at the data, we found two ma-
jor types of errors. Firstly, people express dis-
agreement not only by using opinionated words,
but also by providing contradictory example. This
needs a deeper understanding of the semantic in-
formation embedded in the text. Techniques like
textual entailment can be used in the further work.
Secondly, a sequence of sentences with sarcasm is
hard to detect. For instance, “Bravo, my friends!
Bravo! Goebbles would be proud of your abilities
to whitewash information.” We observe terms like
“Bravo”, “friends”, and “be proud of” that are in-
dicators for positive sentiment; however, they are
</bodyText>
<sectionHeader confidence="0.511848" genericHeader="method">
AAWD
</sectionHeader>
<bodyText confidence="0.964235625">
POSITIVE: agree, nsubj (agree, I), nsubj (right,
you), Rel (Sentimentpos, I), thanks, amod (idea,
good), nsubj(glad, I), good point, concur, happy
with, advmod (good, pretty), suggestionHedge
NEGATIVE: you, your, nsubj (negative, you),
numberOfNegator, don’t, nsubj (disagree, I),
actuallySentInitial, please stopSentInitial, what
?SentInitial, shouldHedge
</bodyText>
<sectionHeader confidence="0.695548" genericHeader="method">
IAC
</sectionHeader>
<bodyText confidence="0.800191636363636">
POSITIVE: amod (conclusion, logical), Rel (agree,
on), Rel (have, justified), Rel (work, out), one
mightSentInitial, to confirmHedge, women
NEGATIVE: their kind, the male, the female, the
scientist, according to, is stated, poss (understand-
ing, my), hellSentInitial, whateverSentInitial
Table 6: Relevant features by x2 test on AAWD
and IAC.
in sarcastic tone. We believe a model that is able
to detect sarcasm would further improve the per-
formance.
</bodyText>
<sectionHeader confidence="0.999008" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999958">
We present an agreement and disagreement detec-
tion model based on isotonic CRFs that outputs
labels at the sentence- or segment-level. We boot-
strap the construction of a sentiment lexicon for
online discussions, encoding it in the form of do-
main knowledge for the isotonic CRF learner. Our
sentiment-tagging model is shown to outperform
the state-of-the-art approaches on both Wikipedia
Talk pages and online debates.
Acknowledgments We heartily thank the Cornell
NLP Group and the reviewers for helpful com-
ments. This work was supported in part by NSF
grants IIS-0968450 and IIS-1314778, and DARPA
DEFT Grant FA8750-13-2-0015. The views and
conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily
representing the official policies or endorsements,
either expressed or implied, of NSF, DARPA or
the U.S. Government.
</bodyText>
<sectionHeader confidence="0.999213" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.971259">
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.
Fox Tree, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing disagree-
ment in informal political argument. In Proceedings of
the Workshop on Languages in Social Media, LSM ’11,
pages 2–11, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
</reference>
<page confidence="0.993446">
104
</page>
<reference confidence="0.999476583333333">
Dragomir Radev. 2012. Subgroup detection in ideo-
logical discussions. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ’12, pages 399–409,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Emily M. Bender, Jonathan T. Morgan, Meghan Oxley, Mark
Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and
Mari Ostendorf. 2011. Annotating social acts: Author-
ity claims and alignment moves in wikipedia talk pages.
In Proceedings of the Workshop on Languages in Social
Media, LSM ’11, pages 48–57, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yejin Choi and Claire Cardie. 2009. Adapting a polarity lex-
icon using integer linear programming for domain-specific
sentiment classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language Pro-
cessing: Volume 2 - Volume 2, EMNLP ’09, pages 590–
598, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure trees. In LREC.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic
lexicon-based approach to opinion mining. In Proceed-
ings of the 2008 International Conference on Web Search
and Data Mining, WSDM ’08, pages 231–240, New York,
NY, USA. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet:
A publicly available lexical resource for opinion mining.
In In Proceedings of the 5th Conference on Language Re-
sources and Evaluation (LREC06, pages 417–422.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The conll-2010 shared
task: Learning to detect hedges and their scope in nat-
ural language text. In Proceedings of the Fourteenth
Conference on Computational Natural Language Learn-
ing — Shared Task, CoNLL ’10: Shared Task, pages 1–
12, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of sentiment
beneath the surface meaning. In ACL, pages 1774–1784.
The Association for Computer Linguistics.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.
2012. Behind the article: Recognizing dialog acts in
wikipedia talk pages. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
Computational Linguistics, EACL ’12, pages 777–786,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Michel Galley, Kathleen McKeown, Julia Hirschberg, and
Elizabeth Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In ACL ’04:
Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, pages 669+, Morristown,
NJ, USA. Association for Computational Linguistics.
Sangyun Hahn, Richard Ladner, and Mari Ostendorf. 2006.
Agreement/disagreement classification: Exploiting unla-
beled data using contrast classifiers. In Proceedings of the
Human Language Technology Conference of the NAACL,
Companion Volume: Short Papers, pages 53–56, New
York City, USA, June. Association for Computational Lin-
guistics.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What’s with the attitude?: Identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’10, pages 1245–1255,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by mod-
eling positive and negative relations among participants.
In Proceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-CoNLL
’12, pages 59–70, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Julia Hirschberg and Diane Litman. 1993. Empirical studies
on the disambiguation of cue phrases. Comput. Linguist.,
19(3):501–530, September.
Thorsten Joachims. 1999. Advances in kernel meth-
ods. chapter Making Large-scale Support Vector Machine
Learning Practical, pages 169–184. MIT Press, Cam-
bridge, MA, USA.
Mahesh Joshi and Carolyn Penstein-Ros´e. 2009. Generaliz-
ing dependency features for opinion mining. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ’09, pages 313–316, Stroudsburg, PA, USA.
Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning, ICML ’01, pages 282–289, San
Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Yi Mao and Guy Lebanon. 2007. Isotonic conditional ran-
dom fields and local sentiment flow. In Advances in Neu-
ral Information Processing Systems.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.cs.umass.edu.
Amita Misra and Marilyn Walker. 2013. Topic independent
identification of agreement and disagreement in social me-
dia dialogue. In Proceedings of the SIGDIAL 2013 Con-
ference, pages 41–50, Metz, France, August. Association
for Computational Linguistics.
Minghui Qiu, Liu Yang, and Jing Jiang. 2013. Mining user
relations from online discussions using sentiment analy-
sis and probabilistic matrix factorization. In Proceedings
of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human
Language Technologies, pages 401–410, Atlanta, Georgia,
June. Association for Computational Linguistics.
Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi
Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.
2008. The penn discourse treebank 2.0. In Bente
Maegaard Joseph Mariani Jan Odijk Stelios Piperidis
Daniel Tapias Nicoletta Calzolari (Conference Chair),
</reference>
<page confidence="0.983357">
105
</page>
<reference confidence="0.999751605633803">
Khalid Choukri, editor, Proceedings of the Sixth Interna-
tional Conference on Language Resources and Evaluation
(LREC’08), Marrakech, Morocco, may. European Lan-
guage Resources Association (ELRA). http://www.lrec-
conf.org/proceedings/lrec2008/.
Swapna Somasundaran and Janyce Wiebe. 2009. Recogniz-
ing stances in online debates. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09,
pages 226–234, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Swapna Somasundaran and Janyce Wiebe. 2010. Recogniz-
ing stances in ideological on-line debates. In Proceedings
of the NAACL HLT 2010 Workshop on Computational Ap-
proaches to Analysis and Generation of Emotion in Text,
CAAGET ’10, pages 116–124, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and
Daniel M. Ogilvie. 1966. The General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press, Cam-
bridge, MA.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Proceedings of the
2006 Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’06, pages 327–335, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan,
and Ryan McDonald. 2010. The viability of web-derived
polarity lexicons. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics, HLT
’10, pages 777–785, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob Abbott,
and Joseph King. 2012a. A corpus for research on de-
liberation and debate. In Proceedings of the Eight Inter-
national Conference on Language Resources and Evalu-
ation (LREC’12), Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
Marilyn A. Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012b. Stance classification using dialogic prop-
erties of persuasion. In HLT-NAACL, pages 592–596. The
Association for Computational Linguistics.
Wen Wang, Sibel Yaman, Kristin Precoda, Colleen Richey,
and Geoffrey Raymond. 2011. Detection of agreement
and disagreement in broadcast conversations. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technolo-
gies: Short Papers - Volume 2, HLT ’11, pages 374–
378, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005.
Recognizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural
Language Processing, HLT ’05, pages 347–354, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris. 2012.
Unifying local and global agreement and disagreement
classification in online debates. In Proceedings of the
3rd Workshop in Computational Approaches to Subjec-
tivity and Sentiment Analysis, WASSA ’12, pages 61–
69, Stroudsburg, PA, USA. Association for Computational
Linguistics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation. In
Technical Report CMU-CALD-02-107.
</reference>
<page confidence="0.997299">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775078">
<title confidence="0.9993655">Improving Agreement and Disagreement in Online Discussions with A Socially-Tuned Sentiment Lexicon</title>
<author confidence="0.995164">Lu</author>
<affiliation confidence="0.9630795">Department of Computer Cornell</affiliation>
<address confidence="0.875827">Ithaca, NY</address>
<email confidence="0.999676">luwang@cs.cornell.edu</email>
<abstract confidence="0.998437727272727">We study the problem of agreement and disagreement detection in online discussions. An isotonic Conditional Random Fields (isotonic CRF) based sequential model is proposed to make predictions on sentenceor segment-level. We automatically construct a socially-tuned lexicon that is bootstrapped from existing general-purpose sentiment lexicons to further improve the performance. We evaluate our agreement and disagreement tagging model on two disparate online discussion corpora – Wikipedia Talk pages and online debates. Our model is shown to outperform the state-of-the-art approaches in both datasets. For example, the isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rob Abbott</author>
<author>Marilyn Walker</author>
<author>Pranav Anand</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Joseph King</author>
</authors>
<title>How can you say such things?!?: Recognizing disagreement in informal political argument.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>2--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9714" citStr="Abbott et al. (2011)" startWordPosition="1557" endWordPosition="1560">arily show the efficiency of prosodic features. While we also exploit a sequential model extended from CRFs, our predictions are made for each sentence or segment rather than at the turn-level. Moreover, we experiment with online discussion datasets that exhibit a more realistic distribution of disagreement vs. agreement, where much more disagreement is observed due to its function and the relation between the participants. This renders the detection problem more challenging. Only recently, agreement and disagreement detection is studied for online discussion, especially 98 for online debate. Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a</context>
<context position="17176" citStr="Abbott et al., 2011" startWordPosition="2764" endWordPosition="2767">res - unigram with POS tag - dependency relation Conversation Features - quote overlap with target - TFIDF similarity with target (remove quote first) Sentiment Features - connective + sentiment words - sentiment dependency relation - sentiment words Table 1: Features used in sentiment prediction. eralizing a word to its POS tag in turn. For instance, “nsubj(wrong, you)” is generlized as the “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations. Discourse Features. Previous work (Hirschberg and Litman, 1993; Abbott et al., 2011) suggests that discourse markers, such as what?, actually, may have their use for expressing opinions. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Hedge words are collected from the CoNLL-2012 shared task (Farkas et al., 2010). Conversation Features. Conversation features encode some useful information regarding the similarity between the current utterance(s) and the sentences uttered by the target participant. TFIDF similarity is computed. We also check if the current utterance(s) quotes target sentences and comput</context>
</contexts>
<marker>Abbott, Walker, Anand, Tree, Bowmani, King, 2011</marker>
<rawString>Rob Abbott, Marilyn Walker, Pranav Anand, Jean E. Fox Tree, Robeson Bowmani, and Joseph King. 2011. How can you say such things?!?: Recognizing disagreement in informal political argument. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 2–11, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Mona Diab</author>
<author>Pradeep Dasigi</author>
<author>Dragomir Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>399--409</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1594" citStr="Abu-Jbara et al., 2012" startWordPosition="242" endWordPosition="245">s 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu In this work, we study the problem of agreement and disagreement identification in online discussions. Sentence-level agreement and disagreement detection for this domain is challenging in its own right due to the dynamic nature of online conversations, and the less for</context>
<context position="11594" citStr="Abu-Jbara et al., 2012" startWordPosition="1855" endWordPosition="1858">h produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressions in the discussions and partition discussants into subgroups based on the intuition that people in the same group shoul</context>
<context position="16090" citStr="Abu-Jbara et al., 2012" startWordPosition="2595" endWordPosition="2598">ikelihood subject to the monotonicity constraints. We adopt the re-parameterization from Mao and Lebanon (2007) for a simpler optimization problem, and refer the readers to Mao and Lebanon (2007) for more details.2 3.3 Features The features used in sentiment prediction are listed in Table 1. Features with numerical values are first normalized by standardization, then binned into 5 categories. Syntactic/Semantic Features. Dependency relations have been shown to be effective for various sentiment prediction tasks (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Hassan et al., 2010; Abu-Jbara et al., 2012). We have two versions of dependency relation as features, one being the original form, another gen2The full implementation is based on MALLET (McCallum, 2002). We thank Yi Mao for sharing the implementation of the core learning algorithm. Lexical Features - unigram/bigram - num of words all uppercased - num of words Discourse Features - initial uni-/bi-/trigram - repeated punctuations - hedging (Farkas et al., 2010) - number of negators Syntactic/Semantic Features - unigram with POS tag - dependency relation Conversation Features - quote overlap with target - TFIDF similarity with target (rem</context>
</contexts>
<marker>Abu-Jbara, Diab, Dasigi, Radev, 2012</marker>
<rawString>Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and Dragomir Radev. 2012. Subgroup detection in ideological discussions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 399–409, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Jonathan T Morgan</author>
<author>Meghan Oxley</author>
<author>Mark Zachry</author>
<author>Brian Hutchinson</author>
<author>Alex Marin</author>
<author>Bin Zhang</author>
<author>Mari Ostendorf</author>
</authors>
<title>Annotating social acts: Authority claims and alignment moves in wikipedia talk pages.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media, LSM ’11,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6995" citStr="Bender et al. (2011)" startWordPosition="1123" endWordPosition="1126">ructure, where turns starting with &gt; are response for most previous turn that with one less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and disagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing ge</context>
<context position="22523" citStr="Bender et al., 2011" startWordPosition="3599" endWordPosition="3602">yi = −1.0, Vvi E N end yi = 101 larger than 0.7 are retained. We remove words with conflicting sentiments. 4.2 Data The graph is constructed based on Wikipedia talk pages. We download the 2013-03-04 Wikipedia data dump, which contains 4,412,582 talk pages. Since we are interested in conversational languages, we filter out talk pages with fewer than 5 participants. This results in a dataset of 20,884 talk pages, from which the graph is constructed. 5 Experimental Setup 5.1 Datasets Wikipedia Talk pages. The first dataset we use is Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011). AAWD consists of 221 English Wikipedia discussions with agreement and disagreement annotations.3 The annotation of AAWD is made at utteranceor turn-level, where a turn is defined as continuous body of text uttered by the same participant. Annotators either label each utterance as agreement, disagreement or neutral, and select the corresponding spans of text, or label the full turn. Each turn is annotated by two or three people. To induce an utterance-level label for instances that have only a turn-level label, we assume they have the same label as the turn. To train our sentiment model, we f</context>
<context position="23809" citStr="Bender et al. (2011)" startWordPosition="3820" endWordPosition="3823">nto the 5-way labels. For utterances that are annotated as agreement and have the text span specified by at least two annotators, they are treated as “strongly agree” (PP). If an utterance is only selected as agreement by one annotator or it gets the label by turn-level annotation, it is “agree” (P). “Strongly disagree” (NN) and “disagree” (N) are collected in the same way from disagreement label. All others are neutral (O). In total, we have 16,501 utterances. 1,930 and 1,102 utterances are labeled as “NN” and “N”. 532 and 99 of them are “PP” and “P”. All other 12,648 are neutral samples. 4 3Bender et al. (2011) originally use positive alignment and negative alignment to indicate two types of social moves. They define those alignment moves as “agreeing or disagreeing” with the target. We thus use agreement and disagreement instead of positive and negative alignment in this work. 4345 samples with both positive and negative labels are treated as neutral. Online Debate. The second dataset is the Internet Argument Corpus (IAC) (Walker et al., 2012a) collected from an online debate forum. Each discussion in IAC consists of multiple posts, where we treat each post as a turn. Most posts (72.3%) contain quo</context>
</contexts>
<marker>Bender, Morgan, Oxley, Zachry, Hutchinson, Marin, Zhang, Ostendorf, 2011</marker>
<rawString>Emily M. Bender, Jonathan T. Morgan, Meghan Oxley, Mark Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and Mari Ostendorf. 2011. Annotating social acts: Authority claims and alignment moves in wikipedia talk pages. In Proceedings of the Workshop on Languages in Social Media, LSM ’11, pages 48–57, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>590--598</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2846" citStr="Choi and Cardie, 2009" startWordPosition="440" endWordPosition="443">l language used. As an example, consider a snippet of discussion from Wikipedia Talk page for article “Iraq War” where editors argue on the correctness of the information in the opening paragraph (Figure 1). “So what?” should presumably be tagged as a negative sentence as should the sentence “If you’re going to troll, do us all a favor and stick to the guidelines.”. We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009). As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an exi</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Yejin Choi and Claire Cardie. 2009. Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 590– 598, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure trees.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure trees. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A holistic lexicon-based approach to opinion mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08,</booktitle>
<pages>231--240</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2822" citStr="Ding et al., 2008" startWordPosition="436" endWordPosition="439">ually very emotional language used. As an example, consider a snippet of discussion from Wikipedia Talk page for article “Iraq War” where editors argue on the correctness of the information in the opening paragraph (Figure 1). “So what?” should presumably be tagged as a negative sentence as should the sentence “If you’re going to troll, do us all a favor and stick to the guidelines.”. We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009). As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08, pages 231–240, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="7743" citStr="Esuli and Sebastiani, 2006" startWordPosition="1245" endWordPosition="1248">ur model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and disagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide </context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The conll-2010 shared task: Learning to detect hedges and their scope in natural language text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning — Shared Task, CoNLL ’10: Shared Task, pages 1– 12,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The conll-2010 shared task: Learning to detect hedges and their scope in natural language text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning — Shared Task, CoNLL ’10: Shared Task, pages 1– 12, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Jun Seok Kang</author>
<author>Polina Kuznetsova</author>
<author>Yejin Choi</author>
</authors>
<title>Connotation lexicon: A dash of sentiment beneath the surface meaning.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>1774--1784</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="20799" citStr="Feng et al. (2013)" startWordPosition="3287" endWordPosition="3290">ioned in Section 1, unigrams lack the capability of capturing the sentiment conveyed in online discussions. Instead, bigrams, dependency relations, and even punctuation can serve as supplement to the unigrams. Therefore, we consider four types of text units as nodes in the graph: unigrams, bigrams, dependency relations, sentiment dependency relations. Sentiment dependency relations are described in Section 3.3. We replace all relation names with a general label. Text units that appear in at least 10 discussions are retained as nodes to reduce noise. Edge Set E. As Velikovich et al. (2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph. For each text unit t, we first compute its representation vector a� using Pairwise Mutual Information scores with respect to the top 50 co-occuring text units. We define “co-occur” as text units appearing in the same sentence. An edge is created between two text units t0 and t1 only if they ever co-occur. The similarity between t0 and t1 is calculated as the Cosine similarity between d0 and d</context>
</contexts>
<marker>Feng, Kang, Kuznetsova, Choi, 2013</marker>
<rawString>Song Feng, Jun Seok Kang, Polina Kuznetsova, and Yejin Choi. 2013. Connotation lexicon: A dash of sentiment beneath the surface meaning. In ACL, pages 1774–1784. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Iryna Gurevych</author>
<author>Yevgen Chebotar</author>
</authors>
<title>Behind the article: Recognizing dialog acts in wikipedia talk pages.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>777--786</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11270" citStr="Ferschke et al., 2012" startWordPosition="1801" endWordPosition="1804">ant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) </context>
</contexts>
<marker>Ferschke, Gurevych, Chebotar, 2012</marker>
<rawString>Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar. 2012. Behind the article: Recognizing dialog acts in wikipedia talk pages. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 777–786, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Julia Hirschberg</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>669</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8583" citStr="Galley et al., 2004" startWordPosition="1375" endWordPosition="1378">ns derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model </context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Michel Galley, Kathleen McKeown, Julia Hirschberg, and Elizabeth Shriberg. 2004. Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 669+, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sangyun Hahn</author>
<author>Richard Ladner</author>
<author>Mari Ostendorf</author>
</authors>
<title>Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>53--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="8603" citStr="Hahn et al., 2006" startWordPosition="1379" endWordPosition="1382">ed number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model extended from CRFs, </context>
</contexts>
<marker>Hahn, Ladner, Ostendorf, 2006</marker>
<rawString>Sangyun Hahn, Richard Ladner, and Mari Ostendorf. 2006. Agreement/disagreement classification: Exploiting unlabeled data using contrast classifiers. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 53–56, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
</authors>
<title>What’s with the attitude?: Identifying sentences with attitude in online discussions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1245--1255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10835" citStr="Hassan et al. (2010)" startWordPosition="1734" endWordPosition="1737">t features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as s</context>
<context position="16065" citStr="Hassan et al., 2010" startWordPosition="2591" endWordPosition="2594">d by maximizing the likelihood subject to the monotonicity constraints. We adopt the re-parameterization from Mao and Lebanon (2007) for a simpler optimization problem, and refer the readers to Mao and Lebanon (2007) for more details.2 3.3 Features The features used in sentiment prediction are listed in Table 1. Features with numerical values are first normalized by standardization, then binned into 5 categories. Syntactic/Semantic Features. Dependency relations have been shown to be effective for various sentiment prediction tasks (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Hassan et al., 2010; Abu-Jbara et al., 2012). We have two versions of dependency relation as features, one being the original form, another gen2The full implementation is based on MALLET (McCallum, 2002). We thank Yi Mao for sharing the implementation of the core learning algorithm. Lexical Features - unigram/bigram - num of words all uppercased - num of words Discourse Features - initial uni-/bi-/trigram - repeated punctuations - hedging (Farkas et al., 2010) - number of negators Syntactic/Semantic Features - unigram with POS tag - dependency relation Conversation Features - quote overlap with target - TFIDF si</context>
<context position="25540" citStr="Hassan et al., 2010" startWordPosition="4116" endWordPosition="4119"> segments), [1, 3) as P (911 samples), [3, 5] as PP (199), all others as O (290 segments). In the test phase, utterances or segments predicted with NN or N are treated as disagreement; the ones predicted as PP or P are agreement; O is neutral. 5.2 Comparison We compare with two baselines. (1) Baseline (Polarity) is based on counting the sentiment words from our lexicon. An utterance or segment is predicted as agreement if it contains more positive words than negative words, or disagreement if more negative words are observed. Otherwise, it is neutral. (2) Baseline (Distance) is extended from (Hassan et al., 2010). Each sentiment word is associated with the closest second person pronoun, and a surface distance can be computed between them. A classifier based on Support Vector Machines (Joachims, 1999) (SVM) is trained with the features of sentiment words, minimum/maximum/average of the distances. We also compare with two state-of-the-art methods that are widely used in sentiment prediction for conversations. The first one is an RBF kernel SVM based approach, which has been used for sentiment prediction (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates. The second </context>
</contexts>
<marker>Hassan, Qazvinian, Radev, 2010</marker>
<rawString>Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev. 2010. What’s with the attitude?: Identifying sentences with attitude in online discussions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1245–1255, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Amjad Abu-Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Detecting subgroups in online discussions by modeling positive and negative relations among participants.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>59--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1569" citStr="Hassan et al., 2012" startWordPosition="238" endWordPosition="241">near chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu In this work, we study the problem of agreement and disagreement identification in online discussions. Sentence-level agreement and disagreement detection for this domain is challenging in its own right due to the dynamic nature of online conver</context>
<context position="11569" citStr="Hassan et al., 2012" startWordPosition="1851" endWordPosition="1854"> person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressions in the discussions and partition discussants into subgroups based on the intuition that peopl</context>
</contexts>
<marker>Hassan, Abu-Jbara, Radev, 2012</marker>
<rawString>Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev. 2012. Detecting subgroups in online discussions by modeling positive and negative relations among participants. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 59–70, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="17154" citStr="Hirschberg and Litman, 1993" startWordPosition="2760" endWordPosition="2763">tors Syntactic/Semantic Features - unigram with POS tag - dependency relation Conversation Features - quote overlap with target - TFIDF similarity with target (remove quote first) Sentiment Features - connective + sentiment words - sentiment dependency relation - sentiment words Table 1: Features used in sentiment prediction. eralizing a word to its POS tag in turn. For instance, “nsubj(wrong, you)” is generlized as the “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations. Discourse Features. Previous work (Hirschberg and Litman, 1993; Abbott et al., 2011) suggests that discourse markers, such as what?, actually, may have their use for expressing opinions. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Hedge words are collected from the CoNLL-2012 shared task (Farkas et al., 2010). Conversation Features. Conversation features encode some useful information regarding the similarity between the current utterance(s) and the sentences uttered by the target participant. TFIDF similarity is computed. We also check if the current utterance(s) quotes targe</context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Comput. Linguist., 19(3):501–530, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Advances in kernel methods. chapter Making Large-scale Support Vector Machine Learning Practical,</title>
<date>1999</date>
<pages>169--184</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="25731" citStr="Joachims, 1999" startWordPosition="4150" endWordPosition="4151">edicted as PP or P are agreement; O is neutral. 5.2 Comparison We compare with two baselines. (1) Baseline (Polarity) is based on counting the sentiment words from our lexicon. An utterance or segment is predicted as agreement if it contains more positive words than negative words, or disagreement if more negative words are observed. Otherwise, it is neutral. (2) Baseline (Distance) is extended from (Hassan et al., 2010). Each sentiment word is associated with the closest second person pronoun, and a surface distance can be computed between them. A classifier based on Support Vector Machines (Joachims, 1999) (SVM) is trained with the features of sentiment words, minimum/maximum/average of the distances. We also compare with two state-of-the-art methods that are widely used in sentiment prediction for conversations. The first one is an RBF kernel SVM based approach, which has been used for sentiment prediction (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates. The second is linear chain CRF, which has been utilized for (dis)agreement identification in broadcast conversations (Wang et al., 2011). 102 Strict F1 Soft F1 Agree Disagree Neutral Agree Disagree Neut</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Advances in kernel methods. chapter Making Large-scale Support Vector Machine Learning Practical, pages 169–184. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Carolyn Penstein-Ros´e</author>
</authors>
<title>Generalizing dependency features for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>313--316</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Joshi, Penstein-Ros´e, 2009</marker>
<rawString>Mahesh Joshi and Carolyn Penstein-Ros´e. 2009. Generalizing dependency features for opinion mining. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 313–316, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3084" citStr="Lafferty et al., 2001" startWordPosition="478" endWordPosition="481">e tagged as a negative sentence as should the sentence “If you’re going to troll, do us all a favor and stick to the guidelines.”. We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009). As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an existing label propagation algorithm (Zhu and Ghahramani, 2002).1 To summarize, our chief contributions include: (1) We propose an agreement and disagreement identification model based on isotonic Conditional Random Fields (Mao and Lebanon, </context>
<context position="8712" citStr="Lafferty et al., 2001" startWordPosition="1397" endWordPosition="1400">. Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model extended from CRFs, our predictions are made for each sentence or segment rather than at the turn-level. Moreover, we experiment </context>
<context position="13852" citStr="Lafferty et al., 2001" startWordPosition="2222" endWordPosition="2225"> In addition, elements in the partially ordered set O possess an ordinal relation G. Here, we differentiate agreement and disagreement with different intensity, because the output of our classifier can be used for other applications, such as dispute detection, where “strongly disagree” (e.g. NN) plays an important role. Meanwhile, fine-grained sentiment labels potentially provide richer context information for the sequential model employed for this task. 3.2 Isotonic Conditional Random Fields Conditional Random Fields (CRF) have been successfully applied in numerous sequential labeling tasks (Lafferty et al., 2001). Given a sequence of utterances or segments x = {x1, · · · , xn}, according to linear-chain CRF, the probability of the labels y for x is given by: 1 � � p(y|x) = Z(x) exp( λ(σ,τ)f(σ,τ)(yi−1, yi) i σ,τ µ(σ,w)g(σ,w)(yi, xi)) (1) � σ,w +� i 99 f(σ,τ)(yi_1, yi) and g(σ,w)(yi, xi) are feature functions. Given that yi_1, yi, xi take values of Q, T, w, the functions are indexed by pairs (Q, T) and (Q, w). A(σ,τ), µ(σ,w) are the parameters. CRF, as defined above, is not appropriate for ordinal data like sentiment, because it ignores the ordinal relation among sentiment labels. Isotonic Conditional R</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Mao</author>
<author>Guy Lebanon</author>
</authors>
<title>Isotonic conditional random fields and local sentiment flow.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2949" citStr="Mao and Lebanon, 2007" startWordPosition="456" endWordPosition="459">Iraq War” where editors argue on the correctness of the information in the opening paragraph (Figure 1). “So what?” should presumably be tagged as a negative sentence as should the sentence “If you’re going to troll, do us all a favor and stick to the guidelines.”. We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009). As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an existing label propagation algorithm (Zhu and Ghahramani, 2002).1 To summarize, our chief contributions in</context>
<context position="12504" citStr="Mao and Lebanon, 2007" startWordPosition="1995" endWordPosition="1998">us sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressions in the discussions and partition discussants into subgroups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. 3 The Model We first give a brief overview on isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007), which is used as the backbone approach for our sentence- or segment-level agreement and disagreement detection model. We defer the explanation of online discussion lexicon construction in Section 4. 3.1 Problem Description Consider a discussion comprised of sequential turns uttered by the participants; each turn consists of a sequence of text units, where each unit can be a sentence or a segment of several sentences. Our model takes as input the text units x = {x1, · · · , xn} in the same turn, and outputs a sequence of sentiment labels y = {y1, · · · , yn}, where yi E O, O = {NN, N, O, P, P</context>
<context position="14518" citStr="Mao and Lebanon (2007)" startWordPosition="2342" endWordPosition="2345"> x = {x1, · · · , xn}, according to linear-chain CRF, the probability of the labels y for x is given by: 1 � � p(y|x) = Z(x) exp( λ(σ,τ)f(σ,τ)(yi−1, yi) i σ,τ µ(σ,w)g(σ,w)(yi, xi)) (1) � σ,w +� i 99 f(σ,τ)(yi_1, yi) and g(σ,w)(yi, xi) are feature functions. Given that yi_1, yi, xi take values of Q, T, w, the functions are indexed by pairs (Q, T) and (Q, w). A(σ,τ), µ(σ,w) are the parameters. CRF, as defined above, is not appropriate for ordinal data like sentiment, because it ignores the ordinal relation among sentiment labels. Isotonic Conditional Random Fields (isotonic CRF) are proposed by Mao and Lebanon (2007) to enforce a set of monotonicity constraints on the parameters that are consistent with the ordinal structure and domain knowledge (in our case, a sentiment lexicon automatically constructed from online discussions). Given a lexicon M = Mp U Mn, where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive sentiment and negative sentiment. The constraints are encoded as below. For each feature w E Mp, isotonic CRF enforces Q &lt; Q&apos; ==&gt;. µ(σ,w) &lt; µ(σ,,w). Intuitively, the parameters µ(σ,w) are intimately tied to the model probabilities. When a feature s</context>
</contexts>
<marker>Mao, Lebanon, 2007</marker>
<rawString>Yi Mao and Guy Lebanon. 2007. Isotonic conditional random fields and local sentiment flow. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="16249" citStr="McCallum, 2002" startWordPosition="2623" endWordPosition="2625">rs to Mao and Lebanon (2007) for more details.2 3.3 Features The features used in sentiment prediction are listed in Table 1. Features with numerical values are first normalized by standardization, then binned into 5 categories. Syntactic/Semantic Features. Dependency relations have been shown to be effective for various sentiment prediction tasks (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Hassan et al., 2010; Abu-Jbara et al., 2012). We have two versions of dependency relation as features, one being the original form, another gen2The full implementation is based on MALLET (McCallum, 2002). We thank Yi Mao for sharing the implementation of the core learning algorithm. Lexical Features - unigram/bigram - num of words all uppercased - num of words Discourse Features - initial uni-/bi-/trigram - repeated punctuations - hedging (Farkas et al., 2010) - number of negators Syntactic/Semantic Features - unigram with POS tag - dependency relation Conversation Features - quote overlap with target - TFIDF similarity with target (remove quote first) Sentiment Features - connective + sentiment words - sentiment dependency relation - sentiment words Table 1: Features used in sentiment predic</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amita Misra</author>
<author>Marilyn Walker</author>
</authors>
<title>Topic independent identification of agreement and disagreement in social media dialogue.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>41--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France,</location>
<contexts>
<context position="10172" citStr="Misra and Walker (2013)" startWordPosition="1629" endWordPosition="1632">problem more challenging. Only recently, agreement and disagreement detection is studied for online discussion, especially 98 for online debate. Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each to</context>
<context position="30940" citStr="Misra and Walker (2013)" startWordPosition="4987" endWordPosition="4990">sentiment features also boost the performance for both of the compared approaches. 6.3 Feature Evaluation Moreover, we evaluate the effectiveness of features by adding one type of features each time. The results are listed in Table 5. As it can be seen, the performance gets improved incrementally with every new set of features. We also utilize x2-test to highlight some of the salient features on the two datasets. We can see from Table 6 that, for online debates (IAC), some features are highly topic related, such as “the male” or “the scientist”. This observation concurs with the conclusion in Misra and Walker (2013) that features with topic information are indicative for agreement and disagreement detection. AAWD Agree Disagree Neu Lex 40.77 52.90 79.65 Lex + Syn 68.18 63.91 88.87 Lex + Syn + Disc 70.93 63.69 89.32 Lex + Syn + Disc + Con 71.27 63.72 89.60 Lex + Syn + Disc + Con + Sent 74.47 67.02 90.56 IAC Agree Disagree Neu Lex 56.65 75.35 45.72 Lex + Syn 54.16 75.13 46.12 Lex + Syn + Disc 54.27 76.41 47.60 Lex + Syn + Disc + Con 55.31 77.25 48.87 Lex + Syn + Disc + Con + Sent 61.49 77.80 51.43 Table 5: Results on Wikipedia talk page (AAWD) (with soft F1 score) and online debate (IAC) with different fea</context>
</contexts>
<marker>Misra, Walker, 2013</marker>
<rawString>Amita Misra and Marilyn Walker. 2013. Topic independent identification of agreement and disagreement in social media dialogue. In Proceedings of the SIGDIAL 2013 Conference, pages 41–50, Metz, France, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minghui Qiu</author>
<author>Liu Yang</author>
<author>Jing Jiang</author>
</authors>
<title>Mining user relations from online discussions using sentiment analysis and probabilistic matrix factorization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>401--410</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1511" citStr="Qiu et al., 2013" startWordPosition="228" endWordPosition="231">0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu In this work, we study the problem of agreement and disagreement identification in online discussions. Sentence-level agreement and disagreement detection for this domain is challenging i</context>
</contexts>
<marker>Qiu, Yang, Jiang, 2013</marker>
<rawString>Minghui Qiu, Liu Yang, and Jing Jiang. 2013. Mining user relations from online discussions using sentiment analysis and probabilistic matrix factorization. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 401–410, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Lee</author>
</authors>
<title>Eleni Miltsakaki Livio Robaldo Aravind Joshi Rashmi Prasad, Nikhil Dinesh</title>
<date>2008</date>
<booktitle>In Bente Maegaard Joseph Mariani</booktitle>
<marker>Lee, 2008</marker>
<rawString>Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi Rashmi Prasad, Nikhil Dinesh and Bonnie Webber. 2008. The penn discourse treebank 2.0. In Bente Maegaard Joseph Mariani Jan Odijk Stelios Piperidis Daniel Tapias Nicoletta Calzolari (Conference Chair),</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<editor>Khalid Choukri, editor,</editor>
<location>Marrakech, Morocco,</location>
<marker></marker>
<rawString>Khalid Choukri, editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrecconf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>226--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1473" citStr="Somasundaran and Wiebe, 2009" startWordPosition="221" endWordPosition="224">isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 cardie@cs.cornell.edu In this work, we study the problem of agreement and disagreement identification in online discussions. Sentence-level agreement and disagreement dete</context>
<context position="11502" citStr="Somasundaran and Wiebe, 2009" startWordPosition="1840" endWordPosition="1843">rticipants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressions in the discussions and par</context>
<context position="16044" citStr="Somasundaran and Wiebe, 2009" startWordPosition="2587" endWordPosition="2590">4). The parameters can be found by maximizing the likelihood subject to the monotonicity constraints. We adopt the re-parameterization from Mao and Lebanon (2007) for a simpler optimization problem, and refer the readers to Mao and Lebanon (2007) for more details.2 3.3 Features The features used in sentiment prediction are listed in Table 1. Features with numerical values are first normalized by standardization, then binned into 5 categories. Syntactic/Semantic Features. Dependency relations have been shown to be effective for various sentiment prediction tasks (Joshi and PensteinRos´e, 2009; Somasundaran and Wiebe, 2009; Hassan et al., 2010; Abu-Jbara et al., 2012). We have two versions of dependency relation as features, one being the original form, another gen2The full implementation is based on MALLET (McCallum, 2002). We thank Yi Mao for sharing the implementation of the core learning algorithm. Lexical Features - unigram/bigram - num of words all uppercased - num of words Discourse Features - initial uni-/bi-/trigram - repeated punctuations - hedging (Farkas et al., 2010) - number of negators Syntactic/Semantic Features - unigram with POS tag - dependency relation Conversation Features - quote overlap w</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 226–234, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10,</booktitle>
<pages>116--124</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7895" citStr="Somasundaran and Wiebe, 2010" startWordPosition="1267" endWordPosition="1270"> and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and disagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applicat</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET ’10, pages 116–124, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7696" citStr="Stone et al., 1966" startWordPosition="1239" endWordPosition="1242">012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and disagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experime</context>
<context position="20130" citStr="Stone et al., 1966" startWordPosition="3180" endWordPosition="3183">y Zhu and Ghahramani (2002), is a semi-supervised learning method. In general, it takes as input a set of seed samples (e.g. sentiment words in our case), and the similarity between pairwise samples, then iteratively assigns values to the unlabeled samples (see Algorithm 1). The construction of graph G is discussed in Section 4.1. Sample sentiment words in the new lexicon are listed in Table 2. Algorithm 1: The label propagation algorithm (Zhu and Ghahramani, 2002) used for constructing online discussion lexicon. 4.1 Graph Construction Node Set V . Traditional lexicons, like General Inquirer (Stone et al., 1966), usually consist of polarized unigrams. As we mentioned in Section 1, unigrams lack the capability of capturing the sentiment conveyed in online discussions. Instead, bigrams, dependency relations, and even punctuation can serve as supplement to the unigrams. Therefore, we consider four types of text units as nodes in the graph: unigrams, bigrams, dependency relations, sentiment dependency relations. Sentiment dependency relations are described in Section 3.3. We replace all relation names with a general label. Text units that appear in at least 10 discussions are retained as nodes to reduce </context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>327--335</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11472" citStr="Thomas et al., 2006" startWordPosition="1836" endWordPosition="1839">ng the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressi</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 327–335, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of web-derived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>777--785</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20776" citStr="Velikovich et al. (2010)" startWordPosition="3282" endWordPosition="3285">olarized unigrams. As we mentioned in Section 1, unigrams lack the capability of capturing the sentiment conveyed in online discussions. Instead, bigrams, dependency relations, and even punctuation can serve as supplement to the unigrams. Therefore, we consider four types of text units as nodes in the graph: unigrams, bigrams, dependency relations, sentiment dependency relations. Sentiment dependency relations are described in Section 3.3. We replace all relation names with a general label. Text units that appear in at least 10 discussions are retained as nodes to reduce noise. Edge Set E. As Velikovich et al. (2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph. For each text unit t, we first compute its representation vector a� using Pairwise Mutual Information scores with respect to the top 50 co-occuring text units. We define “co-occur” as text units appearing in the same sentence. An edge is created between two text units t0 and t1 only if they ever co-occur. The similarity between t0 and t1 is calculated as the Cosine simi</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 777–785, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Jean Fox Tree</author>
<author>Pranav Anand</author>
<author>Rob Abbott</author>
<author>Joseph King</author>
</authors>
<title>A corpus for research on deliberation and debate.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="7080" citStr="Walker et al. (2012" startWordPosition="1138" endWordPosition="1141"> less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and disagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Sto</context>
<context position="11523" citStr="Walker et al., 2012" startWordPosition="1844" endWordPosition="1847"> They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressions in the discussions and partition discussants in</context>
<context position="24250" citStr="Walker et al., 2012" startWordPosition="3890" endWordPosition="3893">have 16,501 utterances. 1,930 and 1,102 utterances are labeled as “NN” and “N”. 532 and 99 of them are “PP” and “P”. All other 12,648 are neutral samples. 4 3Bender et al. (2011) originally use positive alignment and negative alignment to indicate two types of social moves. They define those alignment moves as “agreeing or disagreeing” with the target. We thus use agreement and disagreement instead of positive and negative alignment in this work. 4345 samples with both positive and negative labels are treated as neutral. Online Debate. The second dataset is the Internet Argument Corpus (IAC) (Walker et al., 2012a) collected from an online debate forum. Each discussion in IAC consists of multiple posts, where we treat each post as a turn. Most posts (72.3%) contain quoted content from the posts they target at or other resources. A post can have more than one quote, which naturally break the post into multiple segments. 1,806 discussions are annotated with agreement and disagreement on the segmentlevel from -5 to 5, with -5 as strongly disagree and 5 as strongly agree. We first compute the average score for each segment among different annotators and transform the score into sentiment label in the foll</context>
</contexts>
<marker>Walker, Tree, Anand, Abbott, King, 2012</marker>
<rawString>Marilyn Walker, Jean Fox Tree, Pranav Anand, Rob Abbott, and Joseph King. 2012a. A corpus for research on deliberation and debate. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Pranav Anand</author>
<author>Rob Abbott</author>
<author>Ricky Grant</author>
</authors>
<title>Stance classification using dialogic properties of persuasion.</title>
<date>2012</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>592--596</pages>
<contexts>
<context position="7080" citStr="Walker et al. (2012" startWordPosition="1138" endWordPosition="1141"> less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and disagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Sto</context>
<context position="11523" citStr="Walker et al., 2012" startWordPosition="1844" endWordPosition="1847"> They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressions in the discussions and partition discussants in</context>
<context position="24250" citStr="Walker et al., 2012" startWordPosition="3890" endWordPosition="3893">have 16,501 utterances. 1,930 and 1,102 utterances are labeled as “NN” and “N”. 532 and 99 of them are “PP” and “P”. All other 12,648 are neutral samples. 4 3Bender et al. (2011) originally use positive alignment and negative alignment to indicate two types of social moves. They define those alignment moves as “agreeing or disagreeing” with the target. We thus use agreement and disagreement instead of positive and negative alignment in this work. 4345 samples with both positive and negative labels are treated as neutral. Online Debate. The second dataset is the Internet Argument Corpus (IAC) (Walker et al., 2012a) collected from an online debate forum. Each discussion in IAC consists of multiple posts, where we treat each post as a turn. Most posts (72.3%) contain quoted content from the posts they target at or other resources. A post can have more than one quote, which naturally break the post into multiple segments. 1,806 discussions are annotated with agreement and disagreement on the segmentlevel from -5 to 5, with -5 as strongly disagree and 5 as strongly agree. We first compute the average score for each segment among different annotators and transform the score into sentiment label in the foll</context>
</contexts>
<marker>Walker, Anand, Abbott, Grant, 2012</marker>
<rawString>Marilyn A. Walker, Pranav Anand, Rob Abbott, and Ricky Grant. 2012b. Stance classification using dialogic properties of persuasion. In HLT-NAACL, pages 592–596. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Sibel Yaman</author>
<author>Kristin Precoda</author>
<author>Colleen Richey</author>
<author>Geoffrey Raymond</author>
</authors>
<title>Detection of agreement and disagreement in broadcast conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>374--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8650" citStr="Wang et al., 2011" startWordPosition="1387" endWordPosition="1390"> the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model extended from CRFs, our predictions are made for each sentence or s</context>
<context position="26265" citStr="Wang et al., 2011" startWordPosition="4232" endWordPosition="4235">omputed between them. A classifier based on Support Vector Machines (Joachims, 1999) (SVM) is trained with the features of sentiment words, minimum/maximum/average of the distances. We also compare with two state-of-the-art methods that are widely used in sentiment prediction for conversations. The first one is an RBF kernel SVM based approach, which has been used for sentiment prediction (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates. The second is linear chain CRF, which has been utilized for (dis)agreement identification in broadcast conversations (Wang et al., 2011). 102 Strict F1 Soft F1 Agree Disagree Neutral Agree Disagree Neutral Baseline (Polarity) 14.56 25.70 64.04 22.53 38.61 66.45 Baseline (Distance) 8.08 20.68 84.87 33.75 55.79 88.97 SVM (3-way) 26.76 35.79 77.39 44.62 52.56 80.84 + downsampling 21.60 36.32 72.11 31.86 49.58 74.92 CRF (3-way) 20.99 23.85 85.28 56.28 56.37 89.41 CRF (5-way) 20.47 19.42 85.86 58.39 56.30 90.10 + downsampling 24.26 31.28 77.12 47.30 46.24 80.18 isotonic CRF 24.32 21.95 86.26 68.18 62.53 88.87 + downsampling 29.62 34.17 80.97 55.38 53.00 84.56 + new lexicon 46.01 51.49 87.40 74.47 67.02 90.56 + new lexicon + downsam</context>
</contexts>
<marker>Wang, Yaman, Precoda, Richey, Raymond, 2011</marker>
<rawString>Wen Wang, Sibel Yaman, Kristin Precoda, Colleen Richey, and Geoffrey Raymond. 2011. Detection of agreement and disagreement in broadcast conversations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 374– 378, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7657" citStr="Wilson et al., 2005" startWordPosition="1233" endWordPosition="1236">rgument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and disagreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jie Yin</author>
<author>Paul Thomas</author>
<author>Nalin Narang</author>
<author>Cecile Paris</author>
</authors>
<title>Unifying local and global agreement and disagreement classification in online debates.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12,</booktitle>
<pages>61--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10554" citStr="Yin et al. (2012)" startWordPosition="1688" endWordPosition="1691">cally inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware o</context>
<context position="26109" citStr="Yin et al., 2012" startWordPosition="4207" endWordPosition="4210">Distance) is extended from (Hassan et al., 2010). Each sentiment word is associated with the closest second person pronoun, and a surface distance can be computed between them. A classifier based on Support Vector Machines (Joachims, 1999) (SVM) is trained with the features of sentiment words, minimum/maximum/average of the distances. We also compare with two state-of-the-art methods that are widely used in sentiment prediction for conversations. The first one is an RBF kernel SVM based approach, which has been used for sentiment prediction (Hassan et al., 2010), and (dis)agreement detection (Yin et al., 2012) in online debates. The second is linear chain CRF, which has been utilized for (dis)agreement identification in broadcast conversations (Wang et al., 2011). 102 Strict F1 Soft F1 Agree Disagree Neutral Agree Disagree Neutral Baseline (Polarity) 14.56 25.70 64.04 22.53 38.61 66.45 Baseline (Distance) 8.08 20.68 84.87 33.75 55.79 88.97 SVM (3-way) 26.76 35.79 77.39 44.62 52.56 80.84 + downsampling 21.60 36.32 72.11 31.86 49.58 74.92 CRF (3-way) 20.99 23.85 85.28 56.28 56.37 89.41 CRF (5-way) 20.47 19.42 85.86 58.39 56.30 90.10 + downsampling 24.26 31.28 77.12 47.30 46.24 80.18 isotonic CRF 24.3</context>
</contexts>
<marker>Yin, Thomas, Narang, Paris, 2012</marker>
<rawString>Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris. 2012. Unifying local and global agreement and disagreement classification in online debates. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12, pages 61– 69, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation. In</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107.</tech>
<contexts>
<context position="3506" citStr="Zhu and Ghahramani, 2002" startWordPosition="543" endWordPosition="546">nic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an existing label propagation algorithm (Zhu and Ghahramani, 2002).1 To summarize, our chief contributions include: (1) We propose an agreement and disagreement identification model based on isotonic Conditional Random Fields (Mao and Lebanon, 2007) to identify users’ attitude in online discussion. Our predictions that are made on the sentence1Our online discussion lexicon (Section 4) will be made publicly available. 97 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics Zer0faults: So questions</context>
<context position="19538" citStr="Zhu and Ghahramani (2002)" startWordPosition="3085" endWordPosition="3088">), Rel (intentional, is), pretext, watergate, folly, perjury, Rel(lock, article), contrast with, poke to, censoring information, partisanship, insurrection, bigot, Rel(informative, less), clowns, Rel(feeling, mixed), never-ending Table 2: Example terms and relations from our online discussion lexicon. We choose for display terms that do not contain any seed word. 4 Online Discussion Sentiment Lexicon Construction So far as we know, there is no lexicon available for online discussions. Thus, we create from a large-scale corpus via label propagation. The label propagation algorithm, proposed by Zhu and Ghahramani (2002), is a semi-supervised learning method. In general, it takes as input a set of seed samples (e.g. sentiment words in our case), and the similarity between pairwise samples, then iteratively assigns values to the unlabeled samples (see Algorithm 1). The construction of graph G is discussed in Section 4.1. Sample sentiment words in the new lexicon are listed in Table 2. Algorithm 1: The label propagation algorithm (Zhu and Ghahramani, 2002) used for constructing online discussion lexicon. 4.1 Graph Construction Node Set V . Traditional lexicons, like General Inquirer (Stone et al., 1966), usuall</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. In Technical Report CMU-CALD-02-107.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
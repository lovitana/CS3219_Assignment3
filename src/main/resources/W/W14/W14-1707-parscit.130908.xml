<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.934639">
CoNLL 2014 Shared Task: Grammatical Error Correction with
a Syntactic N-gram Language Model from a Big Corpora
</title>
<author confidence="0.93333">
S. David Hernandez Hiram Calvo
</author>
<affiliation confidence="0.896572">
Centro de Investigación en Centro de Investigación en
</affiliation>
<address confidence="0.619068">
Computación - IPN / México Computación - IPN / México
</address>
<email confidence="0.973593">
shernandez_b12@sagitario.cic.ipn.mx hcalvo@cic.ipn.mx
</email>
<sectionHeader confidence="0.993001" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984764705882">
We describe our approach to grammatical er-
ror correction presented in the CoNLL Shared
Task 2014. Our work is focused on error detec-
tion in sentences with a language model based
on syntactic tri-grams and bi-grams extracted
from dependency trees generated from 90% of
the English Wikipedia. Also, we add a naïve
module to error correction that outputs a set
of possible answers, those sentences are scored
using a syntactic n-gram language model. The
sentence with the best score is the final sug-
gestion of the system.
The system was ranked 11th, evidently this
is a very simple approach, but since the begin-
ning our main goal was to test the syntactic
n-gram language model with a big corpus to
future comparison.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999820327868853">
Grammatical error correction is a difficult task
to solve even for humans, because there are a
lot of phenomena that can occur in a sentence.
One example of the difficulty of the task is that
the annotators of the training and test data in
the NUCLE (Dahlmeier et al., 2013) differs
in the corrections that they made to the sen-
tences, those differences in the annotations are
mostly because depend on uncontrolled con-
ditions, such knowledge, emotional state and
the environment of the annotator at the mo-
ment that the task is performed. This time
the shared task is more difficult than the last
year (Ng and Wu et al., 2013) that considered
only five types of errors, and this time the task
consist into correct all the grammatical errors
in the NUCLE (Ng and Wu et al., 2014).
We are interested into test the behaviour of
different methods used in different NLP task
with the syntactic n-grams as a resource, in or-
der to set a baseline to future work. There is
work that probes that there is an improvement
using syntactic n-grams in (Sidorov and Ve-
lasquez et al., 2014) where the author uses syn-
tactic n-grams as machine learning features,
another example of the use of syntactic n-
grams occurred in the CoNLL 2013 Shared
Task in (Sidorov and Gupta et al., CoNLL
2013), but they used a different approach from
us.
Until the moment we do not have a com-
parison with the same method that we used in
this task using normal n-grams, still our hy-
pothesis is that syntactic n-grams allow us to
relate words that in a common n-gram model
wouldn’t be related and that can outperform
the results.
For example, in the sentence:
&amp;quot;Genetic risk refers more to your chance of
inheriting a disorder or disease .&amp;quot;
Some common tri-grams are &amp;quot;to your
chance&amp;quot;, &amp;quot;your chance of&amp;quot;, &amp;quot;chance of inher-
iting&amp;quot;. The word chance can not be related
to the words &amp;quot;disorder&amp;quot; or &amp;quot;disease&amp;quot;, unless we
use 5-grams or 7-grams, unlike with the syn-
tactic tri-grams that as can be appreciated in
the Table 3 the relation between this words are
normally included.
Another hypothesis is that a low probability
in a syntactic n-gram is an indicator that exist
a wrong token in the portion of a dependency
tree. A simple example of this intuition can be
seen in the Table 1 for the sentence &amp;quot;This will
, if not already , caused problems as there are
very limited spaces for us .&amp;quot; from the training
data in the NUCLE. The bold words are wrong
tokens annotated in the training data and the
numbers are the token number in the sentence.
As can be observed the low probability syn-
tactic tri-grams include the wrong tokens. The
problem is to establish a threshold in the prob-
</bodyText>
<page confidence="0.992079">
53
</page>
<note confidence="0.4747">
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 53–59,
Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.98629048">
qz Syntactic tri-grams to exclude the sentences with more than one
hundred token. At the end we parsed about
75 millions of sentences.
The dependency trees were generated as
Stanford typed dependencies (Marneffe et al.,
2006), in specific in the collapsed with prop-
agation version as described in (Marneffe et
al., 2008). One example of this kind of de-
pendencies can be seen in the Figure 2. As
can be observed the collapsed with propaga-
tion typed dependencies can break the tree, so
strictly this is a directed graph with the gram-
matical relations in the edges and the words of
the sentence in the nodes, though as conven-
tion we will continue referring it as a tree. In
total there are about 1,166 million grammati-
cal relations.
In the error detection phase we used
the information provided with the NUCLE
(Dahlmeier et al., 2013), specifically the to-
kens, POS and the grammatical relations from
the test data in CoNLL style. From the train-
ing data we only made some calculations about
the kinds of errors that occur with higher fre-
quency and we used this information to in-
clude some rules in the correction phase.
3 System description
3.1 Syntactic n-gram language model
We used the dependency trees from Wikipedia
corpus to generate the syntactic n-grams in the
non-continuous form as described in (Sidorov,
2013) and in the book (Sidorov, Book 2013),
but there is an significant difference, the cur-
rent work with syntactic n-grams was made
with the basic dependencies, and as we said
before, we are using the dependencies that
collapses the prepositions and propagates the
conjunctions. The tree in Figure 1 is in the Ba-
sic representation and the differences with the
collapsed and propagated dependencies can be
appreciated in the Figure 2.
This change allow us to increase the scope of
the relations between content words, but also
it makes difficult to find preposition errors, so
our system do not consider preposition correc-
tion.
The tables 2 and 3 show the syntactic tri-
grams generated whit each one of the depen-
dency representations, but without the rela-
tions for lack of space. As can be observed the
</bodyText>
<table confidence="0.997792458333334">
0.0 7are-12 spaces-15 us-17 True’
0.0 ’spaces-15 limited-14 us-17 False’
0.00004 ’caused-8 will-2 are-12 False’
0.00004 ’caused-8 will-2 not-5 False’
0.00004 ’caused-8 will-2 This-1 True’
0.00004 ’caused-8 will-2 problems-9 False’
0.00029 ’caused-8 not-5 are-12 False’
0.00047 ’caused-8 are-12 as-10 True’
0.00054 7are-12 spaces-15 limited-14 True’
0.00054 ’caused-8 are-12 spaces-15 True’
0.00057 ’caused-8 are-12 there-11 True’
0.00065 ’caused-8 problems-9 are-12 False’
0.00109 ’spaces-15 limited-14 very-13 True’
0.00194 ’caused-8 not-5 already-6 True’
0.00314 ’caused-8 not-5 problems-9 False’
0.00522 ’caused-8 not-5 if-4 True’
0.22841 7are-12 as-10 there-11 False’
0.375 7are-12 as-10 spaces-15 False’
0.75510 &apos;are-12 there-11 spaces-15 False’
1.0 ’ROOT-0 caused-8 are-12 True’
1.0 ’ROOT-0 caused-8 not-5 True’
1.0 ’ROOT-0 caused-8 problems-9 True’
1.0 ’ROOT-0 caused-8 will-2 True’
1.0 &apos;not-5 if-4 already-6 False’
</table>
<tableCaption confidence="0.927647">
Table 1: Ordered probabilities of syntactic tri-
</tableCaption>
<bodyText confidence="0.980635038461539">
grams. The wrong tokens are &amp;quot;caused&amp;quot;, &amp;quot;are&amp;quot;
and &amp;quot;spaces&amp;quot;.
abilities to consider as wrong a syntactic tri-
gram and separate the wrong tokens from the
correct ones.
2 Resources
For the language model we used a Wikipedia
dump as training data (Wikipedia, 2013)
and extracted the text with the Multithread
Wikipedia Extractor (Souza, 2012) then was
tokenized with the Stanford Tokenizer (Man-
ning et al., ). There are about 87 millions of
sentences and 1,480 millions of tokens.
To generate the dependency trees we used
the Stanford Parser 3.2 (Socher et al., 2013),
but for the syntactic n-gram language model
we only took 90% of the sentences randomly
chosen. The parsing task took a lot of time to
be made with our computing resources and we
had to use threads with the Stanford Parser,
unfortunately this increases the amount of
memory required by the software, so we had
54
Genetic risk refers more to your chance of in-
heriting a disorder or disease
ROOT-0
</bodyText>
<table confidence="0.617987761904762">
root
w1 w2 w3 Continuous
to-5 chance-7 of-8 True
to-5 chance-7 your-6 True
refers-3 to-5 chance-7 True
refers-3 risk-2 Genetic-1 True
of-8 inheriting-9 disorder-11 True
inheriting-9 disorder-11 a-10 True
inheriting-9 disorder-11 disease-13 True
inheriting-9 disorder-11 or-12 True
chance-7 of-8 inheriting-9 True
ROOT-0 refers-3 to-5 True
ROOT-0 refers-3 risk-2 True
ROOT-0 refers-3 more-4 True
refers-3 risk-2 to-5 False
refers-3 risk-2 more-4 False
refers-3 more-4 to-5 False
disorder-11 a-10 disease-13 False
disorder-11 a-10 or-12 False
disorder-11 or-12 disease-13 False
chance-7 your-6 of-8 False
</table>
<figure confidence="0.975578764705882">
nsubj dobj
risk-2 more-4 to-5
refers-3 prep
Genetic-1 chance-7
poss prep
your-6 of-8
pcomp
amod pobj
Table 2: Syntactic tri-grams from the basic
dependencies.
w1 w2 w3 Continuous
inheriting-9
dobj
a-10 or-12 disease-13
det cc
disorder-11
conj
</figure>
<figureCaption confidence="0.999929">
Figure 1: Basic dependencies.
</figureCaption>
<bodyText confidence="0.702021333333333">
Genetic risk refers more to your chance of in-
heriting a disorder or disease
ROOT-0
</bodyText>
<table confidence="0.979020235294118">
root
refers-3 chance-7 inheriting-9 True
refers-3 chance-7 your-6 True
refers-3 risk-2 Genetic-1 True
inheriting-9 disorder-11 a-10 True
inheriting-9 disorder-11 disease-13 True
chance-7 inheriting-9 disorder-11 True
chance-7 inheriting-9 disease-13 True
ROOT-0 refers-3 chance-7 True
ROOT-0 refers-3 risk-2 True
ROOT-0 refers-3 more-4 True
refers-3 risk-2 chance-7 False
refers-3 risk-2 more-4 False
refers-3 more-4 chance-7 False
inheriting-9 disorder-11 disease-13 False
disorder-11 a-10 disease-13 False
chance-7 your-6 inheriting-9 False
</table>
<tableCaption confidence="0.957069">
Table 3: Syntactic tri-grams from the col-
lapsed with propagation dependencies.
</tableCaption>
<figure confidence="0.534472857142857">
refers-3
prep_to
nsubj dobj
risk-2 more-4 chance-7
amod
your-6 inheriting-9
Genetic-1
</figure>
<figureCaption confidence="0.992554">
Figure 2: Collapsed dependencies with propa-
gation.
</figureCaption>
<bodyText confidence="0.998434333333333">
word &amp;quot;chance&amp;quot; in the basic dependencies is not
directly related with the words &amp;quot;disorder&amp;quot; and
&amp;quot;disease&amp;quot;, on the contrary with the collapsed
and propagated dependencies.
Next we show the maximum likelihood es-
timations that we calculated for this language
model. Where w1, w2, w3 E W and W is the
set of words of the sentence, r1, r2 E R with R
as the set of grammatical relations between the
words and c E {True, False}, with True rep-
resenting a continuous syntactic n-gram and
False a non-continuous syntactic n-gram.
In equation (1) we take the maximum value
between the probability estimation of the tri-
gram with and without grammatical relations
in order to favour the complete tri-gram.
Even with a big corpus as Wikipedia and
with the non-continuous syntactic tri-grams
these estimations can produce zeros in the
probabilities, then we have to draw upon a
back-off, so, we add other estimations.
</bodyText>
<figure confidence="0.9083946">
poss prepc_of
dobj
disorder-11 dobj
a-10 disease-13
det conj_or
</figure>
<page confidence="0.88576">
55
</page>
<equation confidence="0.9993602">
q1 = max(q(w1|w2, w3; r1, r2; c), (1)
q(w1|w2, w3; c))
q2 = max(q(w3|w1, w2; r1, r2; c),
(2)
q(w3|w1, w2; c))
</equation>
<bodyText confidence="0.999599333333333">
Notice that equation (2) is similar to (1),
both evaluate the same syntactic tri-gram, but
with a different word of interest.
</bodyText>
<equation confidence="0.999521333333333">
q3
�min(q(w2|w1; r1), q(w3|w2; r2)) i f c = True =
min(q(w2|w1; r1), q(w3|w1; r2)) if c = False
(3)
�min(q(w2|w1), q(w3|w2)) if c = True
q4 = min(q(w2|w1), q(w3|w1)) if c=False
(4)
q5 = max(q3, q4) (5)
�min(q(w1|w2; r1), q(w2|w3; r2)) if c = True
q6 =min(q(w1|w2; r1), q(w1|w3; r2)) if c = False
(6)
�min(q(w1|w2), q(w2|w3)) if c = True
q7 = min(q(w1|w2), q(w1|w3)) if c=False
(7)
q8 = max(q6, q7) (8)
</equation>
<bodyText confidence="0.999689388888889">
When the probabilities in equations (1) and
(2) are equal to zero, we add a back-off es-
timation based in syntactic bi-grams, since a
syntactic tri-gram is formed of two syntactic
bi-grams or grammatical relations with differ-
ent probabilities, but both or one of them can
contain wrong tokens, so we decided to penal-
ize the complete probability estimation of the
syntactic tri-gram by choosing the min proba-
bility between the two relations. In the equa-
tions (3), (4), (6) and (7) a min operation is
included to penalize the low probability in a
syntactic bi-gram that corresponds to a syn-
tactic tri-gram. In the equations (5) and (8)
the max operation plays the same role as in
equations (1) and (2).
The final expression of the model is shown
in equation (9).
</bodyText>
<equation confidence="0.527306">
⎧ q1 if q1 &gt; 0 and q2 &gt; 0
⎨⎪⎪⎪⎪⎪⎪⎪ q2 if q1 = 0 and q5 &gt; 0
qstri = q5 if q2 = 0 and q8 &gt; 0
q8 if q5 = 0
⎪⎪⎪⎪⎪⎪⎪⎩ Otherwise
0
(9)
</equation>
<bodyText confidence="0.936106">
Where qstri = q(w1, w2, w3; r1, r2; c) and
represents the probability of the syntactic tri-
gram.
The syntactic tri-grams continuous and non-
continuous produced a vast amount of data,
for that reason we only took about 1,660 mil-
lions of syntactic tri-grams to made the lan-
guage model. This data can be downloaded
from (Syntactic N-grams, 2014).
</bodyText>
<subsectionHeader confidence="0.999572">
3.2 Detection and correction
</subsectionHeader>
<bodyText confidence="0.999945">
In order to detect errors in the test data of NU-
CLE (Dahlmeier et al., 2013), we extract the
Stanford typed dependencies from the conll-
style file and to be congruent with the data of
our language model excluded the punct gram-
matical relations. Then we obtain the syn-
tactic tri-grams and probabilities of each sen-
tence. The assumption is that low probability
in a syntactic tri-gram makes it a candidate
to be wrong, since grammatical errors could
produce trees with portions where grammati-
cal relations are unseen in the training data or
with a low probability.
</bodyText>
<table confidence="0.991559941176471">
qi Syntactic tri-grams
0.0 refers-3 more-4 chance-7 False
0.0 refers-3 risk-2 chance-7 False
0.0 refers-3 chance-7 your-6 True
0.0 refers-3 chance-7 inheriting-9 True
0.00015 refers-3 risk-2 Genetic-1 True
0.00023 refers-3 risk-2 more-4 False
0.00355 chance-7 your-6 inheriting-9 False
0.00355 chance-7 inheriting-9 disorder-11 True
0.00609 inheriting-9 disorder-11 disease-13 True
0.00609 inheriting-9 disorder-11 a-10 True
0.00609 inheriting-9 disorder-11 disease-13 False
0.02128 disorder-11 a-10 disease-13 False
1.0 ROOT-0 refers-3 more-4 True
1.0 ROOT-0 refers-3 risk-2 True
1.0 ROOT-0 refers-3 chance-7 True
1.0 chance-7 inheriting-9 disease-13 True
</table>
<tableCaption confidence="0.843231">
Table 4: Ordered probabilities of the syntactic
tri-grams.
</tableCaption>
<bodyText confidence="0.990959">
To add the wrong syntactic tri-grams to a
set E we include two parameters, α = 0.0001
</bodyText>
<page confidence="0.992939">
56
</page>
<bodyText confidence="0.999913">
which is a threshold and � = 0.5 that is a
percentage. To decide whose syntactic tri-
grams must be in the set E, we sort them up-
wardly as in the table 4, if satisfy the condition
(qi &lt; α) and (qi &gt;= �qi+1) for i E 11, 2,..., un-
til the first exception } the syntactic tri-gram
is added to the set E. The fixed values of α
and � were selected by experimentation.
</bodyText>
<table confidence="0.9027942">
w1 w2 w3 Continuous
refers-3 more-4 chance-7 False
refers-3 risk-2 chance-7 False
refers-3 chance-7 your-6 True
refers-3 chance-7 inheriting-9 True
</table>
<tableCaption confidence="0.8064765">
Table 5: Set of possible wrong syntactic tri-
grams.
</tableCaption>
<bodyText confidence="0.999438666666667">
The syntactic tri-grams in the table 5 are
the selected as suspicious to be wrong with
the above considerations. All the tokens can
be part of a grammatical error, but to get re-
placement candidates of all of them can in-
crease the complexity of the task and with the
window of time that we had to accomplish the
task, so we decided to select words in the set E
to be considered as wrong tokens. We counted
the total amount of occurrences of each token
in the set E and took the two with higher val-
ues.
</bodyText>
<equation confidence="0.541017">
Count Tokens
4 refers-3
4 chance-7
1 more-4
1 risk-2
1 your-6
1 inheriting-9
</equation>
<tableCaption confidence="0.953241">
Table 6: Possible wrong tokens.
</tableCaption>
<bodyText confidence="0.999902484848485">
We chose the best candidates that can re-
place each word in the sentence and gener-
ate new sentences with each one of the can-
didates in his different combinations. Is easy
to see that can be a lot of sentences, consid-
ering that each word can have more than one
candidate and that each sentence could have
more than one wrong token to be replaced. To
obtain the candidates to each suspicious to-
ken we search in our training data, words that
start with the stemmed form of the selected to-
ken and that depends of the same word with
the same relation, also we add the lemmatized
word. The lemmatization was made with the
WordNetLemmatizer and the stemming with
LancasterStemmer, both from NLTK. Also we
applied as we said some naïve rules based on
the most frequent errors in the training cor-
pus from NUCLE, for example, when the sus-
picious token is a pronoun or a common verb
as &amp;quot;have&amp;quot; or &amp;quot;do&amp;quot; we replace them with their
conjugations.
For the example in table 6, we have the
respective candidates in table 7. Visibly the
word &amp;quot;chants&amp;quot; has nothing to do with the origi-
nal token to be replaced, it shows the main rea-
son of why we have low score, the rules used in
the correction phase are very simple. For this
example, the word &amp;quot;chance&amp;quot; stemmed with the
LancasterStemmer is &amp;quot;chant&amp;quot;, then the search
of words in the grammatical relations that de-
pends on the word &amp;quot;refers&amp;quot; and with the same
relation, outputs the word &amp;quot;chants&amp;quot;.
</bodyText>
<tableCaption confidence="0.50254125">
Tokens Candidates
refers refers
chance-7 chance, chants
Table 7: Candidates.
</tableCaption>
<bodyText confidence="0.999883">
The possible sentences generated for this ex-
ample are &amp;quot;Genetic risk refers more to your
chance of inheriting a disorder or disease .&amp;quot;
and &amp;quot;Genetic risk refers more to your chants
of inheriting a disorder or disease .&amp;quot;.
In this example the first sentence is the se-
lected as the answer by the system. As can
be appreciated the word chants just worsen
the second sentence. This capacity to discrim-
inate the wrong sentence is what draws our
attention to continue with future work.
With this conditions our system produced
3613 new sentences from the original 1312. To
choose the final answer from the set of pro-
posed sentences for each sentence, we only sum
all the probabilities of the syntactic tri-grams
of each sentence, naturally the sentence with a
higher mass of probability is the final proposed
answer.
</bodyText>
<sectionHeader confidence="0.99962" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.997729333333333">
Our official results in the CoNLL 2014 Shared
Task on grammatical error correction of the
NUCLE and evaluated with the official scorer
</bodyText>
<page confidence="0.995894">
57
</page>
<bodyText confidence="0.4805385">
(Dahlmeier and Ng, 2012) are shown in the ta-
ble 8. The organizers provide all the resources.
</bodyText>
<table confidence="0.99932825">
Without alternative annotation
Recall 2.85
Precision 11.28
F_0.5 7.09
With alternative annotation
Recall 3.17
Precision 11.66
F_0.5 7.59
</table>
<tableCaption confidence="0.9908145">
Table 8: Results in the CoNLL 2014 Shared
Task .
</tableCaption>
<bodyText confidence="0.999646833333333">
The scoring without alternative answers was
made with gold edits of the annotators and the
scoring with alternative annotation includes
answers proposed by 3 teams that participated
on the Shared Task and were judged by the
annotators.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999935">
The result of the system was not good or as we
expected, first because our approach is simple
and was motivated to test the use of a syntac-
tic n-grams language model, second because
the poor election of candidates to correct the
errors. However, this task gave us the oppor-
tunity to test the behaviour in different condi-
tions and now we have a reference to improve
our system.
</bodyText>
<sectionHeader confidence="0.999817" genericHeader="discussions">
6 Future work
</sectionHeader>
<bodyText confidence="0.999994176470588">
We have a lot of work to do, in order to sup-
port the use of this kind of resources. First
we have to compare the same method that
we used, but with a common n-gram language
model. Second is necessary to make a more
general language model that can be used with
syntactic 4-grams or more, and analyse how
this increase can affect the recall. Third find a
way to made more efficient the consult of the
resources.
Also we need to add a more wise method to
correct the detected errors, including prepo-
sitions. The fact that we did not take into
account this type of error does not mean that
is not possible to do it with this resources, so
we have to propose an alternative that takes
into account this and other types of errors.
</bodyText>
<sectionHeader confidence="0.990424" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.97660075">
Work done under partial support of Mexi-
can Government (CONACYT, SNI) and Insti-
tuto Politécnico Nacional, México (SIP-IPN,
COFAA-IPN, PIFI-IPN).
</bodyText>
<sectionHeader confidence="0.997531" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999671145833333">
Christopher Manning, Tim Grow, Teg Grenager,
Jenny Finkel, and John Bauer. PTBTokenizer
http://nlp.stanford.edu/software/tokenizer.shtml
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei
Wu 2013. Building a Large Annotated Corpus
of Learner English: The NUS Corpus of Learner
English. Proceedings of the Eighth Workshop on
Innovative Use of NLP for Building Educational
Applications (BEA 2013) . (pp. 22 – 31). At-
lanta, Georgia, USA.
Daniel Dahlmeier and Hwee Tou Ng 2012. Better
Evaluation for Grammatical Error Correction.
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies (NAACL 2012) . (pp. 568 – 572). Mon-
treal, Canada.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Chris-
tian Hadiwinoto and Joel Tetreault. 2013. The
CoNLL-2013 Shared Task on Grammatical Er-
ror Correction. Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task (CoNLL-2013 Shared
Task) .
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Chris-
tian Hadiwinoto, Raymond Hendy Susanto, and
Christopher Bryant 2014. The CoNLL-2014
Shared Task on Grammatical Error Correc-
tion. Proceedings of the Eighteenth Conference
on Computational Natural Language Learning:
Shared Task (CoNLL-2014 Shared Task) . Bal-
timore, Maryland, USA.
Leonardo Souza (leonardossz@gmail.com).
2012. Multithread-Wikipedia-Extractor
for SMP based architectures, Ver-
sion: 1.0 (October 15, 2012).
https://bitbucket.org/leonardossz/multithreaded-
wikipedia-extractor/wiki/Home
Marie Catherine de Marneffe and Christopher D.
Manning. 2008. Stanford Dependencies manual.
Marie Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating
Typed Dependency Parses from Phrase Struc-
ture Parses. In LREC 2006.
Grigori Sidorov Book 2013. Non-linear construc-
tion of n-grams in computational linguistics:
syntactic, filtered, and generalized n-grams. G.
Sidorov. 2013, 166 p.
</reference>
<page confidence="0.983456">
58
</page>
<reference confidence="0.99579590625">
Grigori Sidorov, Francisco Velasquez, Efstathios
Stamatatos, Alexander Gelbukh, Liliana
Chanona-Hernández. 2014. Syntactic N-grams
as Machine Learning Features for Natural
Language Processing I Expert Syst. Appl.. vol.
41, no. 3, pp. 853-860, 2014.
Grigori Sidorov, 2013. Syntactic Dependency
Based N-grams in Rule Based Automatic En-
glish as Second Language Grammar Correc-
tion. International Journal of Computational
Linguistics and Applications. vol. 4, no. 2, pp.
169-188, 2013.
Grigori Sidorov, Anubhav Gupta, Martin Tozer,
Dolors Catala, Angels Catena and Sandrine
Fuentes. 2013. Rule-based System for Auto-
matic Grammar Correction Using Syntactic N-
grams for English Language Learning (L2). Pro-
ceedings of the Seventeenth Conference on Com-
putational Natural Language Learning: Shared
Task. pp. 96-101, 2013.
Non-continuous Syntactic N-grams
from Wikipedia for the CoNLL
2014 Shared Task. 2014.
http://iarp.cic.ipn.mx/~dhernandez/conll2014/
http://sdavidhernandez.com/conll2014/
Richard Socher, John Bauer, Christopher D. Man-
ning, and Andrew Y. Ng. 2013. Parsing With
Compositional Vector Grammars. Proceedings
of ACL 2013
Wikipedia English dump. 2013.
enwiki-20130904-pages-articles.xml.bz2
http://en.wikipedia.org/wiki/Wikipedia:Database_download
</reference>
<page confidence="0.998842">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.182821">
<title confidence="0.91193">CoNLL 2014 Shared Task: Grammatical Error Correction with a Syntactic N-gram Language Model from a Big Corpora</title>
<author confidence="0.998943">S David Hernandez Hiram Calvo</author>
<affiliation confidence="0.968371">Centro de Investigación en Centro de Investigación en</affiliation>
<abstract confidence="0.93356435">Computación - IPN / México Computación - IPN / México shernandez_b12@sagitario.cic.ipn.mx hcalvo@cic.ipn.mx Abstract We describe our approach to grammatical error correction presented in the CoNLL Shared Task 2014. Our work is focused on error detection in sentences with a language model based on syntactic tri-grams and bi-grams extracted from dependency trees generated from 90% of the English Wikipedia. Also, we add a naïve module to error correction that outputs a set of possible answers, those sentences are scored using a syntactic n-gram language model. The sentence with the best score is the final suggestion of the system. The system was ranked 11th, evidently this is a very simple approach, but since the beginning our main goal was to test the syntactic n-gram language model with a big corpus to future comparison.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Christopher Manning</author>
<author>Tim Grow</author>
<author>Teg Grenager</author>
<author>Jenny Finkel</author>
<author>John Bauer</author>
</authors>
<note>PTBTokenizer http://nlp.stanford.edu/software/tokenizer.shtml</note>
<marker>Manning, Grow, Grenager, Finkel, Bauer, </marker>
<rawString>Christopher Manning, Tim Grow, Teg Grenager, Jenny Finkel, and John Bauer. PTBTokenizer http://nlp.stanford.edu/software/tokenizer.shtml</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English.</title>
<date>2013</date>
<booktitle>Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013) . (pp. 22 – 31).</booktitle>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="1317" citStr="Dahlmeier et al., 2013" startWordPosition="213" endWordPosition="216">ose sentences are scored using a syntactic n-gram language model. The sentence with the best score is the final suggestion of the system. The system was ranked 11th, evidently this is a very simple approach, but since the beginning our main goal was to test the syntactic n-gram language model with a big corpus to future comparison. 1 Introduction Grammatical error correction is a difficult task to solve even for humans, because there are a lot of phenomena that can occur in a sentence. One example of the difficulty of the task is that the annotators of the training and test data in the NUCLE (Dahlmeier et al., 2013) differs in the corrections that they made to the sentences, those differences in the annotations are mostly because depend on uncontrolled conditions, such knowledge, emotional state and the environment of the annotator at the moment that the task is performed. This time the shared task is more difficult than the last year (Ng and Wu et al., 2013) that considered only five types of errors, and this time the task consist into correct all the grammatical errors in the NUCLE (Ng and Wu et al., 2014). We are interested into test the behaviour of different methods used in different NLP task with t</context>
<context position="4702" citStr="Dahlmeier et al., 2013" startWordPosition="806" endWordPosition="809">ffe et al., 2006), in specific in the collapsed with propagation version as described in (Marneffe et al., 2008). One example of this kind of dependencies can be seen in the Figure 2. As can be observed the collapsed with propagation typed dependencies can break the tree, so strictly this is a directed graph with the grammatical relations in the edges and the words of the sentence in the nodes, though as convention we will continue referring it as a tree. In total there are about 1,166 million grammatical relations. In the error detection phase we used the information provided with the NUCLE (Dahlmeier et al., 2013), specifically the tokens, POS and the grammatical relations from the test data in CoNLL style. From the training data we only made some calculations about the kinds of errors that occur with higher frequency and we used this information to include some rules in the correction phase. 3 System description 3.1 Syntactic n-gram language model We used the dependency trees from Wikipedia corpus to generate the syntactic n-grams in the non-continuous form as described in (Sidorov, 2013) and in the book (Sidorov, Book 2013), but there is an significant difference, the current work with syntactic n-gr</context>
<context position="12570" citStr="Dahlmeier et al., 2013" startWordPosition="2076" endWordPosition="2079">ression of the model is shown in equation (9). ⎧ q1 if q1 &gt; 0 and q2 &gt; 0 ⎨⎪⎪⎪⎪⎪⎪⎪ q2 if q1 = 0 and q5 &gt; 0 qstri = q5 if q2 = 0 and q8 &gt; 0 q8 if q5 = 0 ⎪⎪⎪⎪⎪⎪⎪⎩ Otherwise 0 (9) Where qstri = q(w1, w2, w3; r1, r2; c) and represents the probability of the syntactic trigram. The syntactic tri-grams continuous and noncontinuous produced a vast amount of data, for that reason we only took about 1,660 millions of syntactic tri-grams to made the language model. This data can be downloaded from (Syntactic N-grams, 2014). 3.2 Detection and correction In order to detect errors in the test data of NUCLE (Dahlmeier et al., 2013), we extract the Stanford typed dependencies from the conllstyle file and to be congruent with the data of our language model excluded the punct grammatical relations. Then we obtain the syntactic tri-grams and probabilities of each sentence. The assumption is that low probability in a syntactic tri-gram makes it a candidate to be wrong, since grammatical errors could produce trees with portions where grammatical relations are unseen in the training data or with a low probability. qi Syntactic tri-grams 0.0 refers-3 more-4 chance-7 False 0.0 refers-3 risk-2 chance-7 False 0.0 refers-3 chance-7</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013) . (pp. 22 – 31). Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better Evaluation for Grammatical Error Correction.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2012) .</booktitle>
<pages>568--572</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="17510" citStr="Dahlmeier and Ng, 2012" startWordPosition="2926" endWordPosition="2929">sentence. This capacity to discriminate the wrong sentence is what draws our attention to continue with future work. With this conditions our system produced 3613 new sentences from the original 1312. To choose the final answer from the set of proposed sentences for each sentence, we only sum all the probabilities of the syntactic tri-grams of each sentence, naturally the sentence with a higher mass of probability is the final proposed answer. 4 Evaluation Our official results in the CoNLL 2014 Shared Task on grammatical error correction of the NUCLE and evaluated with the official scorer 57 (Dahlmeier and Ng, 2012) are shown in the table 8. The organizers provide all the resources. Without alternative annotation Recall 2.85 Precision 11.28 F_0.5 7.09 With alternative annotation Recall 3.17 Precision 11.66 F_0.5 7.59 Table 8: Results in the CoNLL 2014 Shared Task . The scoring without alternative answers was made with gold edits of the annotators and the scoring with alternative annotation includes answers proposed by 3 teams that participated on the Shared Task and were judged by the annotators. 5 Conclusions The result of the system was not good or as we expected, first because our approach is simple a</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng 2012. Better Evaluation for Grammatical Error Correction. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2012) . (pp. 568 – 572). Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<date>2013</date>
<booktitle>The CoNLL-2013 Shared Task on Grammatical Error Correction. Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2013 Shared Task) .</booktitle>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto and Joel Tetreault. 2013. The CoNLL-2013 Shared Task on Grammatical Error Correction. Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2013 Shared Task) .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
<author>Christopher Bryant</author>
</authors>
<date>2014</date>
<booktitle>The CoNLL-2014 Shared Task on Grammatical Error Correction. Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task) .</booktitle>
<location>Baltimore, Maryland, USA.</location>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant 2014. The CoNLL-2014 Shared Task on Grammatical Error Correction. Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task) . Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Souza</author>
</authors>
<title>Multithread-Wikipedia-Extractor for SMP based architectures,</title>
<date>2012</date>
<location>Version:</location>
<note>https://bitbucket.org/leonardossz/multithreadedwikipedia-extractor/wiki/Home</note>
<contexts>
<context position="7288" citStr="Souza, 2012" startWordPosition="1206" endWordPosition="1207">spaces-15 False’ 0.75510 &apos;are-12 there-11 spaces-15 False’ 1.0 ’ROOT-0 caused-8 are-12 True’ 1.0 ’ROOT-0 caused-8 not-5 True’ 1.0 ’ROOT-0 caused-8 problems-9 True’ 1.0 ’ROOT-0 caused-8 will-2 True’ 1.0 &apos;not-5 if-4 already-6 False’ Table 1: Ordered probabilities of syntactic trigrams. The wrong tokens are &amp;quot;caused&amp;quot;, &amp;quot;are&amp;quot; and &amp;quot;spaces&amp;quot;. abilities to consider as wrong a syntactic trigram and separate the wrong tokens from the correct ones. 2 Resources For the language model we used a Wikipedia dump as training data (Wikipedia, 2013) and extracted the text with the Multithread Wikipedia Extractor (Souza, 2012) then was tokenized with the Stanford Tokenizer (Manning et al., ). There are about 87 millions of sentences and 1,480 millions of tokens. To generate the dependency trees we used the Stanford Parser 3.2 (Socher et al., 2013), but for the syntactic n-gram language model we only took 90% of the sentences randomly chosen. The parsing task took a lot of time to be made with our computing resources and we had to use threads with the Stanford Parser, unfortunately this increases the amount of memory required by the software, so we had 54 Genetic risk refers more to your chance of inheriting a disor</context>
</contexts>
<marker>Souza, 2012</marker>
<rawString>Leonardo Souza (leonardossz@gmail.com). 2012. Multithread-Wikipedia-Extractor for SMP based architectures, Version: 1.0 (October 15, 2012). https://bitbucket.org/leonardossz/multithreadedwikipedia-extractor/wiki/Home</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2008</date>
<note>Stanford Dependencies manual.</note>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie Catherine de Marneffe and Christopher D. Manning. 2008. Stanford Dependencies manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sidorov</author>
</authors>
<title>Grigori Sidorov Book 2013. Non-linear construction of n-grams in computational linguistics: syntactic, filtered, and generalized</title>
<date>2013</date>
<pages>166</pages>
<contexts>
<context position="5187" citStr="Sidorov, 2013" startWordPosition="889" endWordPosition="890">lion grammatical relations. In the error detection phase we used the information provided with the NUCLE (Dahlmeier et al., 2013), specifically the tokens, POS and the grammatical relations from the test data in CoNLL style. From the training data we only made some calculations about the kinds of errors that occur with higher frequency and we used this information to include some rules in the correction phase. 3 System description 3.1 Syntactic n-gram language model We used the dependency trees from Wikipedia corpus to generate the syntactic n-grams in the non-continuous form as described in (Sidorov, 2013) and in the book (Sidorov, Book 2013), but there is an significant difference, the current work with syntactic n-grams was made with the basic dependencies, and as we said before, we are using the dependencies that collapses the prepositions and propagates the conjunctions. The tree in Figure 1 is in the Basic representation and the differences with the collapsed and propagated dependencies can be appreciated in the Figure 2. This change allow us to increase the scope of the relations between content words, but also it makes difficult to find preposition errors, so our system do not consider p</context>
</contexts>
<marker>Sidorov, 2013</marker>
<rawString>Grigori Sidorov Book 2013. Non-linear construction of n-grams in computational linguistics: syntactic, filtered, and generalized n-grams. G. Sidorov. 2013, 166 p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
<author>Francisco Velasquez</author>
</authors>
<title>Efstathios Stamatatos, Alexander Gelbukh, Liliana Chanona-Hernández.</title>
<date>2014</date>
<journal>Appl..</journal>
<volume>41</volume>
<pages>853--860</pages>
<marker>Sidorov, Velasquez, 2014</marker>
<rawString>Grigori Sidorov, Francisco Velasquez, Efstathios Stamatatos, Alexander Gelbukh, Liliana Chanona-Hernández. 2014. Syntactic N-grams as Machine Learning Features for Natural Language Processing I Expert Syst. Appl.. vol. 41, no. 3, pp. 853-860, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
</authors>
<title>Syntactic Dependency Based N-grams in Rule Based Automatic English as Second Language Grammar Correction.</title>
<date>2013</date>
<journal>International Journal of Computational Linguistics and Applications.</journal>
<volume>4</volume>
<pages>169--188</pages>
<contexts>
<context position="5187" citStr="Sidorov, 2013" startWordPosition="889" endWordPosition="890">lion grammatical relations. In the error detection phase we used the information provided with the NUCLE (Dahlmeier et al., 2013), specifically the tokens, POS and the grammatical relations from the test data in CoNLL style. From the training data we only made some calculations about the kinds of errors that occur with higher frequency and we used this information to include some rules in the correction phase. 3 System description 3.1 Syntactic n-gram language model We used the dependency trees from Wikipedia corpus to generate the syntactic n-grams in the non-continuous form as described in (Sidorov, 2013) and in the book (Sidorov, Book 2013), but there is an significant difference, the current work with syntactic n-grams was made with the basic dependencies, and as we said before, we are using the dependencies that collapses the prepositions and propagates the conjunctions. The tree in Figure 1 is in the Basic representation and the differences with the collapsed and propagated dependencies can be appreciated in the Figure 2. This change allow us to increase the scope of the relations between content words, but also it makes difficult to find preposition errors, so our system do not consider p</context>
</contexts>
<marker>Sidorov, 2013</marker>
<rawString>Grigori Sidorov, 2013. Syntactic Dependency Based N-grams in Rule Based Automatic English as Second Language Grammar Correction. International Journal of Computational Linguistics and Applications. vol. 4, no. 2, pp. 169-188, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
<author>Anubhav Gupta</author>
<author>Martin Tozer</author>
<author>Dolors Catala</author>
</authors>
<title>Angels Catena and Sandrine Fuentes.</title>
<date>2013</date>
<booktitle>(L2). Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<pages>96--101</pages>
<marker>Sidorov, Gupta, Tozer, Catala, 2013</marker>
<rawString>Grigori Sidorov, Anubhav Gupta, Martin Tozer, Dolors Catala, Angels Catena and Sandrine Fuentes. 2013. Rule-based System for Automatic Grammar Correction Using Syntactic Ngrams for English Language Learning (L2). Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task. pp. 96-101, 2013.</rawString>
</citation>
<citation valid="true">
<title>Non-continuous Syntactic N-grams from Wikipedia for the CoNLL</title>
<date>2014</date>
<note>http://sdavidhernandez.com/conll2014/</note>
<marker>2014</marker>
<rawString>Non-continuous Syntactic N-grams from Wikipedia for the CoNLL 2014 Shared Task. 2014. http://sdavidhernandez.com/conll2014/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing With Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="7513" citStr="Socher et al., 2013" startWordPosition="1243" endWordPosition="1246">eady-6 False’ Table 1: Ordered probabilities of syntactic trigrams. The wrong tokens are &amp;quot;caused&amp;quot;, &amp;quot;are&amp;quot; and &amp;quot;spaces&amp;quot;. abilities to consider as wrong a syntactic trigram and separate the wrong tokens from the correct ones. 2 Resources For the language model we used a Wikipedia dump as training data (Wikipedia, 2013) and extracted the text with the Multithread Wikipedia Extractor (Souza, 2012) then was tokenized with the Stanford Tokenizer (Manning et al., ). There are about 87 millions of sentences and 1,480 millions of tokens. To generate the dependency trees we used the Stanford Parser 3.2 (Socher et al., 2013), but for the syntactic n-gram language model we only took 90% of the sentences randomly chosen. The parsing task took a lot of time to be made with our computing resources and we had to use threads with the Stanford Parser, unfortunately this increases the amount of memory required by the software, so we had 54 Genetic risk refers more to your chance of inheriting a disorder or disease ROOT-0 root w1 w2 w3 Continuous to-5 chance-7 of-8 True to-5 chance-7 your-6 True refers-3 to-5 chance-7 True refers-3 risk-2 Genetic-1 True of-8 inheriting-9 disorder-11 True inheriting-9 disorder-11 a-10 True</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing With Compositional Vector Grammars. Proceedings of ACL 2013</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia English dump</author>
</authors>
<date>2013</date>
<marker>dump, 2013</marker>
<rawString>Wikipedia English dump. 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
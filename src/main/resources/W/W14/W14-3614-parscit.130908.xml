<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000837">
<title confidence="0.998909">
A Pipeline Approach to Supervised Error Correction
for the QALB-2014 Shared Task
</title>
<author confidence="0.837355">
Nadi Tomeh† Nizar Habash$ Ramy Eskander* Joseph Le Roux†
</author>
<email confidence="0.8188965">
{nadi.tomeh,leroux}@lipn.univ-paris13.fr†
nizar.habash@nyu.edu$, ramy@ccls.columbia.edu*
</email>
<author confidence="0.609788">
†Universit´e Paris 13, Sorbonne Paris Cit´e, LIPN, Villetaneuse, France
</author>
<affiliation confidence="0.978953">
$Computer Science Department, New York University Abu Dhabi
*Center for Computational Learning Systems, Columbia University
</affiliation>
<sectionHeader confidence="0.98755" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897636363636">
This paper describes our submission to
the ANLP-2014 shared task on auto-
matic Arabic error correction. We present
a pipeline approach integrating an er-
ror detection model, a combination of
character- and word-level translation mod-
els, a reranking model and a punctuation
insertion model. We achieve an F1 score
of 62.8% on the development set of the
QALB corpus, and 58.6% on the official
test set.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957023809524">
Devising algorithms for automatic error correction
generated considerable interest in the community
since the early 1960s (Kukich, 1992) for at least
two reasons. First, typical NLP tools lack in ro-
bustness against errors in their input. This sen-
sitivity jeopardizes their usefulness especially for
unedited text, which is prevalent on the web. Sec-
ond, automated spell and grammar checkers facil-
itate text editing and can be of great help to non-
native speakers of a language. Several resources
and shared tasks appeared recently, including the
HOO task (Dale and Kilgarriff, 2010) and the
CoNLL task on grammatical error correction (Ng
et al., 2013b). In this paper we describe our partic-
ipation to the first shared task on automatic error
correction for Arabic (Mohit et al., 2014).
While non-word errors are relatively easy to
handle, the task is more challenging for gram-
matical and semantic errors. Detecting and cor-
recting such errors require context-sensitive ap-
proaches in order to capture the dependencies be-
tween the words of a text at various lexical and se-
mantic levels. All the more so for Arabic which
brings dependence down to the morphological
level (Habash, 2010).
A particularity interesting approach to error cor-
rection relies on statistical machine translation
(SMT) (Brockett et al., 2006), due to its context-
sensitivity and data-driven aspect. Therefore, the
pipeline system which we describe in Section 2
has as its core a phrase-based SMT component
(PBSMT) (Section 2.3). Nevertheless, several fac-
tors may hinder the success of this approach, such
as data sparsity, discrepancies between transla-
tion and error correction tasks, and the difficulty
of incorporating context-sensitive features into the
SMT decoder.
We address all these issues in our system which
achieves a better correction quality than a simple
word-level PBSMT baseline on the QALB corpus
(Zaghouani et al., 2014) as we show in our exper-
iments in Section 3.
</bodyText>
<sectionHeader confidence="0.756637" genericHeader="introduction">
2 Pipeline Approach to Error Correction
</sectionHeader>
<bodyText confidence="0.999471">
The PBSMT system accounts for context by learn-
ing, from a parallel corpus of annotated errors,
mappings from erroneous multi-word segments of
text to their corrections, and using a language
model to help select the suitable corrections in
context when multiple alternatives are present.
Furthermore, since the SMT approach is data-
driven, it is possible to address multiple types of
errors at once, as long as examples of them appear
in the training corpus. These errors may include
non-word errors, wrong lexical choices and gram-
matical errors, and can also handle normalization
issues (Yvon, 2010).
One major issue is data sparsity, since large
amount of labeled training data is necessary to
provide reliable statistics of all error types. We ad-
</bodyText>
<page confidence="0.975344">
114
</page>
<bodyText confidence="0.956514407407407">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 114–120,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
dress this issue by backing-off the word-level PB-
SMT model with a character-level correction com-
ponent, for which richer statistics can be obtained.
Another issue may stem from the inherent dif-
ference in nature between error correction and
translation. Unlike translation, the input and out-
put vocabularies in the correction task overlap sig-
nificantly, and the majority of input words are typi-
cally correct and are copied unmodified to the out-
put. The SMT system should handle correct words
by selecting their identities from all possible op-
tions, which may fail resulting in over-correction.
To help the SMT decoder decide, we augment our
pipeline with a problem zone detection compo-
nent, which supplies prior information on which
input words need to be corrected.
The final issue concerns the difficulty of incor-
porating features that require context across phrase
boundaries into the SMT decoder. A straightfor-
ward alternative is to use such features to rerank
the hypotheses in the SMT n-best hypotheses lists.
Since punctuation is particularity noisy in Ara-
bic data, we add a specialized punctuation inser-
tion component to our pipeline, depicted in Figure
1.
</bodyText>
<subsectionHeader confidence="0.851629">
2.1 Error Detection
</subsectionHeader>
<bodyText confidence="0.999946477272727">
We formalize the error detection problem as a
sequence labeling problem (Habash and Roth,
2011). Errors are classified into substitution, in-
sertion and deletion errors. Substitutions involve
an incorrect word form that should be replaced by
another correct form. Insertions are words that
are incorrectly added into the text and should be
deleted. Deletions are simply missing words that
should be added.
We group all error classes into a simple binary
problem tag: a word from the input text is tagged
as “PROB” if it is the result of an insertion or
a substitution of a word. Deleted words, which
cannot be tagged themselves, cause their adjacent
words to be marked as PROB instead. In this way,
the subsequent components in the pipeline can be
alerted to the possibility of a missing word via its
surroundings. Any words not marked as PROB are
given an “OK” tag.
Gold tags, necessary for training, can be gener-
ated by comparing the text to its correction using
some sequence alignment technique, for which we
use SCLITE (Fiscus, 1998).
For this task, we use Yamcha (Kudo and Mat-
sumoto, 2003) to train an SVM classifier using
morphological and lexical features. We employ
a quadratic polynomial kernel. The static feature
window context size is set to +/- 2 words; the pre-
vious two (dynamic) predicted tags are also used
as features.
The feature set includes the surface forms and
their normalization after “Alef”, “Ya” and digit
normalization, the POS tags and the lemmas of the
words. These morphological features are obtained
using MADA 3.0 (Habash et al., 2009).1 We also
use a set of word, POS and lemma 3-gram lan-
guage models scores as features. These LMs are
built using SRILM (Stolcke, 2002).
The error detection component is integrated into
the pipeline by concatenating the predicted tags
with the words of the input text. The SMT model
uses this additional information to learn distinct
mappings conditional on the predicted correctness
of words.
</bodyText>
<subsectionHeader confidence="0.998209">
2.2 Character-level Back-off Correction
</subsectionHeader>
<bodyText confidence="0.999992272727273">
Each word that is labeled as error (PROB) in the
output of the error detection component is mapped
to multiple possible corrections using a weighted
finite-state transducer similar to the transducers
used in speech recognition (Mohri et al., 2002).
The WFST, for which we used OpenFST (Al-
lauzen et al., 2007), operates on the character
level, and the character mapping is many-to-many
(similar to the phrase-based SMT framework).
The score of each proposed correction is a com-
bination of the scores of character mappings used
to build it. The list is filtered using WFST scores
and an additional character-level LM score. The
result is a list of error-tagged words and their cor-
rection suggestions, which constitutes a small on-
the-fly phrase table used to back-off primary PB-
SMT table.
During training, the mapping dictionary is
learned from the training after aligning it at the
character level using SCLITE. Mapping weights
are computed as their normalized frequencies in
the aligned training corpus.
</bodyText>
<subsectionHeader confidence="0.998876">
2.3 Word-level PBSMT Correction
</subsectionHeader>
<bodyText confidence="0.999294666666667">
We formalize the correction process as a phrase-
based statistical machine translation problem
(Koehn et al., 2003), at the word-level, and solve
</bodyText>
<footnote confidence="0.940486666666667">
1We did not use MADAMIRA (the newest version of
MADA) since it was not available when this component was
built.
</footnote>
<page confidence="0.994944">
115
</page>
<figure confidence="0.999511941176471">
Input Error-tagged text
Error
Detection
Character-level
Correction
Back-off
Primary
Word-level
PBSMT Correction
Phrase
tables
Reranked best hypothesis Output
N-best
Reranking
Punctuation
Insertion
N-best hypotheses
</figure>
<figureCaption confidence="0.999397">
Figure 1: Input text is run through the error detection component which labels the problematic words.
</figureCaption>
<bodyText confidence="0.99206685">
The labeled text is then fed to the character-level correction components which constructs a back-off
phrase table. The PBSMT component then uses two phrase tables to generate n-best correction hy-
potheses. The reranking component selects the best hypothesis, and pass it to the punctuation insertion
component in order to produce the final output.
it using Moses, a well-known PBSMT tool (Koehn
et al., 2007). The decoder constructs a correction
hypothesis by first segmenting the input text into
phrases, and mapping each phrase into its best cor-
rection using a combination of scores including a
context-sensitive LM score.
Unlike translation, error correction is mainly
monotonic, therefore we set disallow reordering
by setting the distortion limit in Moses to 0.2
When no mapping can be found for a given
phrase in the primary phrase table, the decoder
looks it up in the back-off model. The decoder
searches the space of all possible correction hy-
potheses, resulting from alternative segmentations
and mappings, and returns the list of n-best scor-
ing hypotheses.
</bodyText>
<subsectionHeader confidence="0.998815">
2.4 N-best List Reranking
</subsectionHeader>
<bodyText confidence="0.999973692307692">
In this step, we combine LM information with lin-
guistically and semantically motivated features us-
ing learning to rank methods (Tomeh et al., 2013).
Discriminative reranking (Liu, 2009) allows each
hypothesis to be represented as an arbitrary set of
features without the need to explicitly model their
interactions. Therefore, the system benefits from
global and potentially complex features which are
not available to the baseline decoder.
Each hypothesis in an n-best list is represented
by a d-dimensional feature vector. Word error rate
(WER) is computed for each hypotheses by com-
paring it to the reference correction. The resulting
</bodyText>
<footnote confidence="0.6574575">
2Only 0.14% of edits in the QALB corpus are actually
reordering.
</footnote>
<bodyText confidence="0.999872666666667">
scored n-best list is used for supervised training
of a reranking model. We employ a pairwise ap-
proach to ranking which takes pairs of hypotheses
as instances in learning, and formalizes the rank-
ing problem as pairwise classification.
For this task we use RankSVM (Joachims,
2002) which is a method based on Support Vec-
tor Machines (SVMs). We use only linear kernels
to keep complexity low. We use a rich set of fea-
tures including LM scores on surface forms, POS
tags and lemmas. We also use a feature based on a
global model of the semantic coherence of the hy-
potheses (Tomeh et al., 2013). The new top ranked
hypothesis is the output of this step which is then
fed to the next component.
</bodyText>
<subsectionHeader confidence="0.975019">
2.5 Punctuation Insertion
</subsectionHeader>
<bodyText confidence="0.999837">
We developed a model that predicts the occurrence
of periods and commas in a given Arabic text.
The core model is a decision tree classifier trained
on the QALB parallel training data using WEKA
(Hall et al., 2009). For each space between two
words, the classifier decides whether or not to in-
sert a punctuation mark, using a window size of
three words surrounding the underlying space.
The model uses the following features:
</bodyText>
<listItem confidence="0.9921615">
• A class punctuation feature, that is whether to
insert a period, a comma or none at the cur-
rent space location;
• The part-of-speech of the previous word;
• The existence of a conjunctive or connective
proclitic in the following word; that is a “wa”
</listItem>
<page confidence="0.990528">
116
</page>
<table confidence="0.635228857142857">
Precision−Recall Curve
Pression
AUC= 0.715
PRBE= 0.483, Cutoff= −0.349
Prec@rec(0.800)= 0.345, Cutoff= −1.045
0.0 0.2 0.4 0.6 0.8 1.0
Recall
</table>
<figureCaption confidence="0.9989475">
Figure 2: Evaluation of the error detection com-
ponent. AUC: Area Under the Curve, PRBE:
</figureCaption>
<bodyText confidence="0.955922714285714">
precision-recall break-even point. Classifier
thresholds are displayed on the right vertical axis.
or “fa” proclitic that is either a conjunction, a
sub-conjunction or a connective particle.
We obtain POS and proclitic information using
MADAMIRA (Pasha et al., 2014). The output of
this component is the final output of the system.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999862">
All the models we use in our pipeline are trained
in a supervised way using the training part of the
QALB corpus (Zaghouani et al., 2014), while we
reserve the development part of the corpus for test-
ing.
</bodyText>
<subsectionHeader confidence="0.956971">
3.1 Error detection
</subsectionHeader>
<bodyText confidence="0.999992">
We evaluate the error detection binary classifier in
terms of standard classification measures as shown
in Figure 2. Each point on the curve is computed
by selecting a threshold on the classifier score.
The threshold we use correspond to recall equal
to 80%, at which the precision is very low which
leaves much room for improvement in the perfor-
mance of the error detection component.
</bodyText>
<subsectionHeader confidence="0.994881">
3.2 Character-level correction
</subsectionHeader>
<bodyText confidence="0.999422428571429">
We evaluate the character-level correction model
by measuring the percentage of erroneous phrases
that have been mapped to their in-context refer-
ence corrections. We found this percentage to be
41% on QALB dev data. We limit the size of
such phrases to one in order to focus on out-of-
vocabulary words.
</bodyText>
<subsectionHeader confidence="0.997506">
3.3 Punctuation insertion
</subsectionHeader>
<bodyText confidence="0.999975736842105">
To evaluate the punctuation insertion indepen-
dently from the pipeline, we first remove the pe-
riods and commas from input text. Considering
only the locations where periods and commas ex-
ist, our model gives a recall of 49% and a precision
of 53%, giving an F1-score of 51%.
When we apply our punctuation model in the
correction pipeline, we find that it is always better
to keep the already existing periods and commas
in the input text instead of overwriting them by
the model prediction.
While developing the model, we ran experi-
ments where we train the complete list of fea-
tures produced by MADAMIRA; that is part-of-
speech, gender, number, person, aspect, voice,
case, mood, state, proclitics and enclitics. This
was done for two preceding words and two follow-
ing words. However, the results were significantly
outperformed by our final set-up.
</bodyText>
<subsectionHeader confidence="0.996004">
3.4 The pipeline
</subsectionHeader>
<bodyText confidence="0.998300166666667">
The performance of the pipeline is evaluated in
terms of precision, recall and F1 as computed by
the M2 Scorer (Dahlmeier and Ng, 2012b). The
results presented in Table 1 show that a simple
PBSMT baseline achieves relatively good perfor-
mance compared to more sophisticated models.
The character-level back-off model helps by im-
proving recall at the expense of decreased preci-
sion. The error detection component hurts the per-
formance which could be explained by its intrin-
sic bad performance. Since more investigation is
needed to clarify on this point, we drop this com-
ponent from our submission. Both reranking and
punctuation insertion improve the performance.
Our system submission to the shared task (back-
off+PBSMT+Rank+PI) resulted in an F1 score of
58.6% on the official test set, with a precision of
76.9% and a recall of 47.3%.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9870364">
Both rule-based and data-driven approaches to
error correction can be found in the literature
(Sidorov et al., 2013; Berend et al., 2013; Yi et
al., 2013) as well as hybridization of them (Putra
and Szabo, 2013). Unlike our approach, most of
</bodyText>
<figure confidence="0.9454545">
0.2 0.4 0.6 0.8 1.0
−8.33 −5.02 −1.7 1.61 4.93
117
System PR RC F1
PBSMT
backoff+PBSMT
ED+backoff+PBSMT
backoff+PBSMT+Rank
backoff+PBSMT+Rank+PI
75.5 49.5 59.8
74.1 51.8 60.9
61.3 45.4 52.2
75.7 52.1 61.7
74.9 54.2 62.8
</figure>
<bodyText confidence="0.977506666666667">
Foundation). The statements made herein are
solely the responsibility of the authors.
Nizar Habash performed most of his contri-
bution to this paper while he was at the Center
for Computational Learning Systems at Columbia
University.
Table 1: Pipeline precision, recall and F1 scores.
ED: error detection, PI: punctuation insertion.
the proposed systems build distinct models to ad-
dress individual types of errors (see the CoNLL-
2013, 2014 proceedings (Ng et al., 2013a; Ng
et al., 2014), and combine them afterwords us-
ing Integer Linear Programming for instance (Ro-
zovskaya et al., 2013). This approach is relatively
time-consuming when the number of error types
increases.
Interest in models that target all errors at once
has increased, using either multi-class classifiers
(Farra et al., 2014; Jia et al., 2013), of-the-shelf
SMT techniques (Brockett et al., 2006; Mizu-
moto et al., 2011; Yuan and Felice, 2013; Buys
and van der Merwe, 2013; Buys and van der
Merwe, 2013), or building specialized decoders
(Dahlmeier and Ng, 2012a).
Our system addresses the weaknesses of the
SMT approach using additional components in a
pipeline architecture. Similar work on word-level
and character-level model combination has been
done in the context of translation between closely
related languages (Nakov and Tiedemann, 2012).
A character-level correction model has also been
considered to reduce the out-of-vocabulary rate in
translation systems (Habash, 2008).
</bodyText>
<sectionHeader confidence="0.973006" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999987">
We described a pipeline approach based on
phrase-based SMT with n-best list reranking. We
showed that backing-off word-level model with a
character-level model improves the performance
by ameliorating the recall of the system.
The main focus of our future work will be on
better integration of the error detection model, and
on exploring alternative methods for combining
the character and the word models.
</bodyText>
<sectionHeader confidence="0.98983" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.979415">
This material is partially based on research funded
by grant NPRP-4-1058-1-168 from the Qatar Na-
tional Research Fund (a member of the Qatar
</bodyText>
<sectionHeader confidence="0.991006" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.729789926829268">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. Openfst: A
general and efficient weighted finite-state transducer
library. In CIAA, pages 11–23.
Gabor Berend, Veronika Vincze, Sina Zarrieß, and
Rich´ard Farkas. 2013. Lfg-based features for noun
number and article grammatical errors. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning: Shared Task,
pages 62–67, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting esl errors using phrasal
smt techniques. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th Annual Meeting of the Association
for Computational Linguistics, ACL-44, pages 249–
256, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jan Buys and Brink van der Merwe. 2013. A tree
transducer model for grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 43–51, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beam-
search decoder for grammatical error correction. In
EMNLP-CoNLL, pages 568–578.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better
evaluation for grammatical error correction. In HLT-
NAACL, pages 568–572.
Robert Dale and Adam Kilgarriff. 2010. Helping our
own: Text massaging for computational linguistics
as a new shared task. In INLG.
Noura Farra, Nadi Tomeh, Alla Rozovskaya, and Nizar
Habash. 2014. Generalized character-level spelling
error correction. In ACL (2), pages 161–167.
Jon Fiscus. 1998. Speech Recognition Scor-
ing Toolkit (SCTK). National Institute of Standard
Technology (NIST). http://www.itl.nist.
gov/iad/mig/tools/.
</reference>
<footnote confidence="0.77874">
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
arabic handwriting recognition. In Proceedings of
</footnote>
<page confidence="0.986005">
118
</page>
<reference confidence="0.997909909090908">
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ’11, pages 875–884, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2008. Four Techniques for Online
Handling of Out-of-Vocabulary Words in Arabic-
English Statistical Machine Translation. In Pro-
ceedings of ACL-08: HLT, Short Papers, pages 57–
60, Columbus, Ohio.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan &amp; Claypool Publish-
ers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explor. Newsl., 11(1):10–18, November.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Gram-
matical error correction as multiclass classification
with single model. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 74–81, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ’02,
pages 133–142.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference (HLT/NAACL), pages 127–133,
Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Comput. Surv.,
24(4):377–439, December.
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval. Now Publishers Inc., Hanover, MA,
USA.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revision
log of language learning sns for automated japanese
error correction of second language learners. In IJC-
NLP, pages 147–155.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech &amp; Language,
16(1):69–88.
Preslav Nakov and J¨org Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In ACL (2), pages 301–305.
Hwee Tou Ng, Joel Tetreault, Siew Mei Wu, Yuanbin
Wu, and Christian Hadiwinoto, editors. 2013a. Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task.
Association for Computational Linguistics, Sofia,
Bulgaria, August.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013b. The conll-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1–12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant, editors. 2014. Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning: Shared Task. Association for
Computational Linguistics, Baltimore, Maryland,
June.
Arfath Pasha, Mohamed Al-Badrashiny, Mona T. Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. Madamira: A fast, comprehensive tool for
morphological analysis and disambiguation of ara-
bic. In LREC, pages 1094–1101.
Desmond Darma Putra and Lili Szabo. 2013. Uds
at conll 2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 88–95,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.990255">
119
</page>
<reference confidence="0.999792314814815">
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The university of illinois sys-
tem in the conll-2013 shared task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 13–19,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Grigori Sidorov, Anubhav Gupta, Martin Tozer, Do-
lors Catala, Angels Catena, and Sandrine Fuentes.
2013. Rule-based system for automatic grammar
correction using syntactic n-grams for english lan-
guage learning (l2). In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 96–101, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901–904, Denver,
CO.
Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra,
Pradeep Dasigi, and Mona Diab. 2013. Reranking
with linguistic and semantic features for arabic op-
tical character recognition. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
549–555, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim.
2013. Kunlp grammatical error correction system
for conll-2013 shared task. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 123–127,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zheng Yuan and Mariano Felice. 2013. Constrained
grammatical error correction using statistical ma-
chine translation. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 52–61, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Franc¸ois Yvon. 2010. Rewriting the orthography
of sms messages. Natural Language Engineering,
16:133–159, 3.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale arabic error annotation: Guidelines and
framework. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Eval-
uation (LREC’14), Reykjavik, Iceland, May. Euro-
pean Language Resources Association (ELRA).
</reference>
<page confidence="0.996255">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595073">
<title confidence="0.9977025">A Pipeline Approach to Supervised Error for the QALB-2014 Shared Task</title>
<author confidence="0.997868">Joseph Le</author>
<affiliation confidence="0.861070666666667">Paris 13, Sorbonne Paris Cit´e, LIPN, Villetaneuse, Science Department, New York University Abu for Computational Learning Systems, Columbia University</affiliation>
<abstract confidence="0.997406416666667">This paper describes our submission to the ANLP-2014 shared task on automatic Arabic error correction. We present a pipeline approach integrating an error detection model, a combination of characterand word-level translation models, a reranking model and a punctuation model. We achieve an of 62.8% on the development set of the QALB corpus, and 58.6% on the official test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>Openfst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In CIAA,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="7258" citStr="Allauzen et al., 2007" startWordPosition="1144" endWordPosition="1148">(Stolcke, 2002). The error detection component is integrated into the pipeline by concatenating the predicted tags with the words of the input text. The SMT model uses this additional information to learn distinct mappings conditional on the predicted correctness of words. 2.2 Character-level Back-off Correction Each word that is labeled as error (PROB) in the output of the error detection component is mapped to multiple possible corrections using a weighted finite-state transducer similar to the transducers used in speech recognition (Mohri et al., 2002). The WFST, for which we used OpenFST (Allauzen et al., 2007), operates on the character level, and the character mapping is many-to-many (similar to the phrase-based SMT framework). The score of each proposed correction is a combination of the scores of character mappings used to build it. The list is filtered using WFST scores and an additional character-level LM score. The result is a list of error-tagged words and their correction suggestions, which constitutes a small onthe-fly phrase table used to back-off primary PBSMT table. During training, the mapping dictionary is learned from the training after aligning it at the character level using SCLITE</context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In CIAA, pages 11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabor Berend</author>
<author>Veronika Vincze</author>
<author>Sina Zarrieß</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Lfg-based features for noun number and article grammatical errors.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>62--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="15152" citStr="Berend et al., 2013" startWordPosition="2428" endWordPosition="2431">d precision. The error detection component hurts the performance which could be explained by its intrinsic bad performance. Since more investigation is needed to clarify on this point, we drop this component from our submission. Both reranking and punctuation insertion improve the performance. Our system submission to the shared task (backoff+PBSMT+Rank+PI) resulted in an F1 score of 58.6% on the official test set, with a precision of 76.9% and a recall of 47.3%. 4 Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013). Unlike our approach, most of 0.2 0.4 0.6 0.8 1.0 −8.33 −5.02 −1.7 1.61 4.93 117 System PR RC F1 PBSMT backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI 75.5 49.5 59.8 74.1 51.8 60.9 61.3 45.4 52.2 75.7 52.1 61.7 74.9 54.2 62.8 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Table 1: Pipeline precision, recall and </context>
</contexts>
<marker>Berend, Vincze, Zarrieß, Farkas, 2013</marker>
<rawString>Gabor Berend, Veronika Vincze, Sina Zarrieß, and Rich´ard Farkas. 2013. Lfg-based features for noun number and article grammatical errors. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 62–67, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting esl errors using phrasal smt techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>249--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2155" citStr="Brockett et al., 2006" startWordPosition="322" endWordPosition="325">cipation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014). While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors. Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels. All the more so for Arabic which brings dependence down to the morphological level (Habash, 2010). A particularity interesting approach to error correction relies on statistical machine translation (SMT) (Brockett et al., 2006), due to its contextsensitivity and data-driven aspect. Therefore, the pipeline system which we describe in Section 2 has as its core a phrase-based SMT component (PBSMT) (Section 2.3). Nevertheless, several factors may hinder the success of this approach, such as data sparsity, discrepancies between translation and error correction tasks, and the difficulty of incorporating context-sensitive features into the SMT decoder. We address all these issues in our system which achieves a better correction quality than a simple word-level PBSMT baseline on the QALB corpus (Zaghouani et al., 2014) as w</context>
<context position="16344" citStr="Brockett et al., 2006" startWordPosition="2617" endWordPosition="2620">peline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Conclusio</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting esl errors using phrasal smt techniques. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 249– 256, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Buys</author>
<author>Brink van der Merwe</author>
</authors>
<title>A tree transducer model for grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>43--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Buys, van der Merwe, 2013</marker>
<rawString>Jan Buys and Brink van der Merwe. 2013. A tree transducer model for grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 43–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A beamsearch decoder for grammatical error correction.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>568--578</pages>
<contexts>
<context position="14299" citStr="Dahlmeier and Ng, 2012" startWordPosition="2290" endWordPosition="2293">p the already existing periods and commas in the input text instead of overwriting them by the model prediction. While developing the model, we ran experiments where we train the complete list of features produced by MADAMIRA; that is part-ofspeech, gender, number, person, aspect, voice, case, mood, state, proclitics and enclitics. This was done for two preceding words and two following words. However, the results were significantly outperformed by our final set-up. 3.4 The pipeline The performance of the pipeline is evaluated in terms of precision, recall and F1 as computed by the M2 Scorer (Dahlmeier and Ng, 2012b). The results presented in Table 1 show that a simple PBSMT baseline achieves relatively good performance compared to more sophisticated models. The character-level back-off model helps by improving recall at the expense of decreased precision. The error detection component hurts the performance which could be explained by its intrinsic bad performance. Since more investigation is needed to clarify on this point, we drop this component from our submission. Both reranking and punctuation insertion improve the performance. Our system submission to the shared task (backoff+PBSMT+Rank+PI) result</context>
<context position="16509" citStr="Dahlmeier and Ng, 2012" startWordPosition="2646" endWordPosition="2649">rors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Conclusion and Future Work We described a pipeline approach based on phrase-based SMT with n-best list reranking. We showed that backing-off word-level model with a character</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012a. A beamsearch decoder for grammatical error correction. In EMNLP-CoNLL, pages 568–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>568--572</pages>
<contexts>
<context position="14299" citStr="Dahlmeier and Ng, 2012" startWordPosition="2290" endWordPosition="2293">p the already existing periods and commas in the input text instead of overwriting them by the model prediction. While developing the model, we ran experiments where we train the complete list of features produced by MADAMIRA; that is part-ofspeech, gender, number, person, aspect, voice, case, mood, state, proclitics and enclitics. This was done for two preceding words and two following words. However, the results were significantly outperformed by our final set-up. 3.4 The pipeline The performance of the pipeline is evaluated in terms of precision, recall and F1 as computed by the M2 Scorer (Dahlmeier and Ng, 2012b). The results presented in Table 1 show that a simple PBSMT baseline achieves relatively good performance compared to more sophisticated models. The character-level back-off model helps by improving recall at the expense of decreased precision. The error detection component hurts the performance which could be explained by its intrinsic bad performance. Since more investigation is needed to clarify on this point, we drop this component from our submission. Both reranking and punctuation insertion improve the performance. Our system submission to the shared task (backoff+PBSMT+Rank+PI) result</context>
<context position="16509" citStr="Dahlmeier and Ng, 2012" startWordPosition="2646" endWordPosition="2649">rors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Conclusion and Future Work We described a pipeline approach based on phrase-based SMT with n-best list reranking. We showed that backing-off word-level model with a character</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better evaluation for grammatical error correction. In HLTNAACL, pages 568–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping our own: Text massaging for computational linguistics as a new shared task.</title>
<date>2010</date>
<booktitle>In INLG.</booktitle>
<contexts>
<context position="1426" citStr="Dale and Kilgarriff, 2010" startWordPosition="204" endWordPosition="207">8.6% on the official test set. 1 Introduction Devising algorithms for automatic error correction generated considerable interest in the community since the early 1960s (Kukich, 1992) for at least two reasons. First, typical NLP tools lack in robustness against errors in their input. This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web. Second, automated spell and grammar checkers facilitate text editing and can be of great help to nonnative speakers of a language. Several resources and shared tasks appeared recently, including the HOO task (Dale and Kilgarriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b). In this paper we describe our participation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014). While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors. Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels. All the more so for Arabic which brings dependence down to the morphological level (Habash, 2010).</context>
</contexts>
<marker>Dale, Kilgarriff, 2010</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2010. Helping our own: Text massaging for computational linguistics as a new shared task. In INLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noura Farra</author>
<author>Nadi Tomeh</author>
<author>Alla Rozovskaya</author>
<author>Nizar Habash</author>
</authors>
<title>Generalized character-level spelling error correction.</title>
<date>2014</date>
<booktitle>In ACL (2),</booktitle>
<pages>161--167</pages>
<contexts>
<context position="16273" citStr="Farra et al., 2014" startWordPosition="2606" endWordPosition="2609">r Computational Learning Systems at Columbia University. Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the ou</context>
</contexts>
<marker>Farra, Tomeh, Rozovskaya, Habash, 2014</marker>
<rawString>Noura Farra, Nadi Tomeh, Alla Rozovskaya, and Nizar Habash. 2014. Generalized character-level spelling error correction. In ACL (2), pages 161–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Fiscus</author>
</authors>
<title>Speech Recognition Scoring Toolkit (SCTK).</title>
<date>1998</date>
<booktitle>National Institute of Standard Technology (NIST). http://www.itl.nist. gov/iad/mig/tools/.</booktitle>
<contexts>
<context position="5987" citStr="Fiscus, 1998" startWordPosition="940" endWordPosition="941">classes into a simple binary problem tag: a word from the input text is tagged as “PROB” if it is the result of an insertion or a substitution of a word. Deleted words, which cannot be tagged themselves, cause their adjacent words to be marked as PROB instead. In this way, the subsequent components in the pipeline can be alerted to the possibility of a missing word via its surroundings. Any words not marked as PROB are given an “OK” tag. Gold tags, necessary for training, can be generated by comparing the text to its correction using some sequence alignment technique, for which we use SCLITE (Fiscus, 1998). For this task, we use Yamcha (Kudo and Matsumoto, 2003) to train an SVM classifier using morphological and lexical features. We employ a quadratic polynomial kernel. The static feature window context size is set to +/- 2 words; the previous two (dynamic) predicted tags are also used as features. The feature set includes the surface forms and their normalization after “Alef”, “Ya” and digit normalization, the POS tags and the lemmas of the words. These morphological features are obtained using MADA 3.0 (Habash et al., 2009).1 We also use a set of word, POS and lemma 3-gram language models sco</context>
</contexts>
<marker>Fiscus, 1998</marker>
<rawString>Jon Fiscus. 1998. Speech Recognition Scoring Toolkit (SCTK). National Institute of Standard Technology (NIST). http://www.itl.nist. gov/iad/mig/tools/.</rawString>
</citation>
<citation valid="false">
<title>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</title>
<volume>1</volume>
<pages>875--884</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 875–884, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization.</title>
<date>2009</date>
<booktitle>In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools. The MEDAR Consortium,</booktitle>
<contexts>
<context position="6517" citStr="Habash et al., 2009" startWordPosition="1026" endWordPosition="1029">correction using some sequence alignment technique, for which we use SCLITE (Fiscus, 1998). For this task, we use Yamcha (Kudo and Matsumoto, 2003) to train an SVM classifier using morphological and lexical features. We employ a quadratic polynomial kernel. The static feature window context size is set to +/- 2 words; the previous two (dynamic) predicted tags are also used as features. The feature set includes the surface forms and their normalization after “Alef”, “Ya” and digit normalization, the POS tags and the lemmas of the words. These morphological features are obtained using MADA 3.0 (Habash et al., 2009).1 We also use a set of word, POS and lemma 3-gram language models scores as features. These LMs are built using SRILM (Stolcke, 2002). The error detection component is integrated into the pipeline by concatenating the predicted tags with the words of the input text. The SMT model uses this additional information to learn distinct mappings conditional on the predicted correctness of words. 2.2 Character-level Back-off Correction Each word that is labeled as error (PROB) in the output of the error detection component is mapped to multiple possible corrections using a weighted finite-state trans</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>Nizar Habash, Owen Rambow, and Ryan Roth. 2009. MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. In Khalid Choukri and Bente Maegaard, editors, Proceedings of the Second International Conference on Arabic Language Resources and Tools. The MEDAR Consortium, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four Techniques for Online Handling of Out-of-Vocabulary Words in ArabicEnglish Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>57--60</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="16931" citStr="Habash, 2008" startWordPosition="2706" endWordPosition="2707">ues (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Conclusion and Future Work We described a pipeline approach based on phrase-based SMT with n-best list reranking. We showed that backing-off word-level model with a character-level model improves the performance by ameliorating the recall of the system. The main focus of our future work will be on better integration of the error detection model, and on exploring alternative methods for combining the character and the word models. Acknowledgments This material is partially based on research funded by grant NPRP-4-1058-1-168 from the Qatar National Research Fund (a member of the Qatar Refere</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four Techniques for Online Handling of Out-of-Vocabulary Words in ArabicEnglish Statistical Machine Translation. In Proceedings of ACL-08: HLT, Short Papers, pages 57– 60, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="2025" citStr="Habash, 2010" startWordPosition="306" endWordPosition="307">garriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b). In this paper we describe our participation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014). While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors. Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels. All the more so for Arabic which brings dependence down to the morphological level (Habash, 2010). A particularity interesting approach to error correction relies on statistical machine translation (SMT) (Brockett et al., 2006), due to its contextsensitivity and data-driven aspect. Therefore, the pipeline system which we describe in Section 2 has as its core a phrase-based SMT component (PBSMT) (Section 2.3). Nevertheless, several factors may hinder the success of this approach, such as data sparsity, discrepancies between translation and error correction tasks, and the difficulty of incorporating context-sensitive features into the SMT decoder. We address all these issues in our system w</context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11289" citStr="Hall et al., 2009" startWordPosition="1789" endWordPosition="1792">ort Vector Machines (SVMs). We use only linear kernels to keep complexity low. We use a rich set of features including LM scores on surface forms, POS tags and lemmas. We also use a feature based on a global model of the semantic coherence of the hypotheses (Tomeh et al., 2013). The new top ranked hypothesis is the output of this step which is then fed to the next component. 2.5 Punctuation Insertion We developed a model that predicts the occurrence of periods and commas in a given Arabic text. The core model is a decision tree classifier trained on the QALB parallel training data using WEKA (Hall et al., 2009). For each space between two words, the classifier decides whether or not to insert a punctuation mark, using a window size of three words surrounding the underlying space. The model uses the following features: • A class punctuation feature, that is whether to insert a period, a comma or none at the current space location; • The part-of-speech of the previous word; • The existence of a conjunctive or connective proclitic in the following word; that is a “wa” 116 Precision−Recall Curve Pression AUC= 0.715 PRBE= 0.483, Cutoff= −0.349 Prec@rec(0.800)= 0.345, Cutoff= −1.045 0.0 0.2 0.4 0.6 0.8 1.</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Peilu Wang</author>
<author>Hai Zhao</author>
</authors>
<title>Grammatical error correction as multiclass classification with single model.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>74--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="16292" citStr="Jia et al., 2013" startWordPosition="2610" endWordPosition="2613">ning Systems at Columbia University. Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rat</context>
</contexts>
<marker>Jia, Wang, Zhao, 2013</marker>
<rawString>Zhongye Jia, Peilu Wang, and Hai Zhao. 2013. Grammatical error correction as multiclass classification with single model. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 74–81, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="10639" citStr="Joachims, 2002" startWordPosition="1671" endWordPosition="1672">tially complex features which are not available to the baseline decoder. Each hypothesis in an n-best list is represented by a d-dimensional feature vector. Word error rate (WER) is computed for each hypotheses by comparing it to the reference correction. The resulting 2Only 0.14% of edits in the QALB corpus are actually reordering. scored n-best list is used for supervised training of a reranking model. We employ a pairwise approach to ranking which takes pairs of hypotheses as instances in learning, and formalizes the ranking problem as pairwise classification. For this task we use RankSVM (Joachims, 2002) which is a method based on Support Vector Machines (SVMs). We use only linear kernels to keep complexity low. We use a rich set of features including LM scores on surface forms, POS tags and lemmas. We also use a feature based on a global model of the semantic coherence of the hypotheses (Tomeh et al., 2013). The new top ranked hypothesis is the output of this step which is then fed to the next component. 2.5 Punctuation Insertion We developed a model that predicts the occurrence of periods and commas in a given Arabic text. The core model is a decision tree classifier trained on the QALB par</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’02, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL),</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="8098" citStr="Koehn et al., 2003" startWordPosition="1276" endWordPosition="1279">build it. The list is filtered using WFST scores and an additional character-level LM score. The result is a list of error-tagged words and their correction suggestions, which constitutes a small onthe-fly phrase table used to back-off primary PBSMT table. During training, the mapping dictionary is learned from the training after aligning it at the character level using SCLITE. Mapping weights are computed as their normalized frequencies in the aligned training corpus. 2.3 Word-level PBSMT Correction We formalize the correction process as a phrasebased statistical machine translation problem (Koehn et al., 2003), at the word-level, and solve 1We did not use MADAMIRA (the newest version of MADA) since it was not available when this component was built. 115 Input Error-tagged text Error Detection Character-level Correction Back-off Primary Word-level PBSMT Correction Phrase tables Reranked best hypothesis Output N-best Reranking Punctuation Insertion N-best hypotheses Figure 1: Input text is run through the error detection component which labels the problematic words. The labeled text is then fed to the character-level correction components which constructs a back-off phrase table. The PBSMT component </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-based Translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL), pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8970" citStr="Koehn et al., 2007" startWordPosition="1406" endWordPosition="1409">ction Phrase tables Reranked best hypothesis Output N-best Reranking Punctuation Insertion N-best hypotheses Figure 1: Input text is run through the error detection component which labels the problematic words. The labeled text is then fed to the character-level correction components which constructs a back-off phrase table. The PBSMT component then uses two phrase tables to generate n-best correction hypotheses. The reranking component selects the best hypothesis, and pass it to the punctuation insertion component in order to produce the final output. it using Moses, a well-known PBSMT tool (Koehn et al., 2007). The decoder constructs a correction hypothesis by first segmenting the input text into phrases, and mapping each phrase into its best correction using a combination of scores including a context-sensitive LM score. Unlike translation, error correction is mainly monotonic, therefore we set disallow reordering by setting the distortion limit in Moses to 0.2 When no mapping can be found for a given phrase in the primary phrase table, the decoder looks it up in the back-off model. The decoder searches the space of all possible correction hypotheses, resulting from alternative segmentations and m</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Comput. Surv.,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="982" citStr="Kukich, 1992" startWordPosition="133" endWordPosition="134">r Computational Learning Systems, Columbia University Abstract This paper describes our submission to the ANLP-2014 shared task on automatic Arabic error correction. We present a pipeline approach integrating an error detection model, a combination of character- and word-level translation models, a reranking model and a punctuation insertion model. We achieve an F1 score of 62.8% on the development set of the QALB corpus, and 58.6% on the official test set. 1 Introduction Devising algorithms for automatic error correction generated considerable interest in the community since the early 1960s (Kukich, 1992) for at least two reasons. First, typical NLP tools lack in robustness against errors in their input. This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web. Second, automated spell and grammar checkers facilitate text editing and can be of great help to nonnative speakers of a language. Several resources and shared tasks appeared recently, including the HOO task (Dale and Kilgarriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b). In this paper we describe our participation to the first shared task on automatic er</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Comput. Surv., 24(4):377–439, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
</authors>
<title>Learning to Rank for Information Retrieval.</title>
<date>2009</date>
<publisher>Now Publishers Inc.,</publisher>
<location>Hanover, MA, USA.</location>
<contexts>
<context position="9841" citStr="Liu, 2009" startWordPosition="1544" endWordPosition="1545">ly monotonic, therefore we set disallow reordering by setting the distortion limit in Moses to 0.2 When no mapping can be found for a given phrase in the primary phrase table, the decoder looks it up in the back-off model. The decoder searches the space of all possible correction hypotheses, resulting from alternative segmentations and mappings, and returns the list of n-best scoring hypotheses. 2.4 N-best List Reranking In this step, we combine LM information with linguistically and semantically motivated features using learning to rank methods (Tomeh et al., 2013). Discriminative reranking (Liu, 2009) allows each hypothesis to be represented as an arbitrary set of features without the need to explicitly model their interactions. Therefore, the system benefits from global and potentially complex features which are not available to the baseline decoder. Each hypothesis in an n-best list is represented by a d-dimensional feature vector. Word error rate (WER) is computed for each hypotheses by comparing it to the reference correction. The resulting 2Only 0.14% of edits in the QALB corpus are actually reordering. scored n-best list is used for supervised training of a reranking model. We employ</context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Now Publishers Inc., Hanover, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoya Mizumoto</author>
<author>Mamoru Komachi</author>
<author>Masaaki Nagata</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Mining revision log of language learning sns for automated japanese error correction of second language learners.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="16367" citStr="Mizumoto et al., 2011" startWordPosition="2621" endWordPosition="2625">l and F1 scores. ED: error detection, PI: punctuation insertion. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Conclusion and Future Work We de</context>
</contexts>
<marker>Mizumoto, Komachi, Nagata, Matsumoto, 2011</marker>
<rawString>Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2011. Mining revision log of language learning sns for automated japanese error correction of second language learners. In IJCNLP, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Alla Rozovskaya</author>
<author>Nizar Habash</author>
<author>Wajdi Zaghouani</author>
<author>Ossama Obeid</author>
</authors>
<title>The First QALB Shared Task on Automatic Text Correction for Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing,</booktitle>
<location>Doha, Qatar,</location>
<contexts>
<context position="1628" citStr="Mohit et al., 2014" startWordPosition="239" endWordPosition="242">First, typical NLP tools lack in robustness against errors in their input. This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web. Second, automated spell and grammar checkers facilitate text editing and can be of great help to nonnative speakers of a language. Several resources and shared tasks appeared recently, including the HOO task (Dale and Kilgarriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b). In this paper we describe our participation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014). While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors. Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels. All the more so for Arabic which brings dependence down to the morphological level (Habash, 2010). A particularity interesting approach to error correction relies on statistical machine translation (SMT) (Brockett et al., 2006), due to its contextsensitivity and data-driven aspect. Therefore, the pi</context>
</contexts>
<marker>Mohit, Rozovskaya, Habash, Zaghouani, Obeid, 2014</marker>
<rawString>Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid. 2014. The First QALB Shared Task on Automatic Text Correction for Arabic. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7197" citStr="Mohri et al., 2002" startWordPosition="1133" endWordPosition="1136">odels scores as features. These LMs are built using SRILM (Stolcke, 2002). The error detection component is integrated into the pipeline by concatenating the predicted tags with the words of the input text. The SMT model uses this additional information to learn distinct mappings conditional on the predicted correctness of words. 2.2 Character-level Back-off Correction Each word that is labeled as error (PROB) in the output of the error detection component is mapped to multiple possible corrections using a weighted finite-state transducer similar to the transducers used in speech recognition (Mohri et al., 2002). The WFST, for which we used OpenFST (Allauzen et al., 2007), operates on the character level, and the character mapping is many-to-many (similar to the phrase-based SMT framework). The score of each proposed correction is a combination of the scores of character mappings used to build it. The list is filtered using WFST scores and an additional character-level LM score. The result is a list of error-tagged words and their correction suggestions, which constitutes a small onthe-fly phrase table used to back-off primary PBSMT table. During training, the mapping dictionary is learned from the t</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech &amp; Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Combining word-level and character-level models for machine translation between closely-related languages.</title>
<date>2012</date>
<booktitle>In ACL (2),</booktitle>
<pages>301--305</pages>
<contexts>
<context position="16795" citStr="Nakov and Tiedemann, 2012" startWordPosition="2686" endWordPosition="2689">s that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Conclusion and Future Work We described a pipeline approach based on phrase-based SMT with n-best list reranking. We showed that backing-off word-level model with a character-level model improves the performance by ameliorating the recall of the system. The main focus of our future work will be on better integration of the error detection model, and on exploring alternative methods for combining the character and the word models. Acknowledgments This mater</context>
</contexts>
<marker>Nakov, Tiedemann, 2012</marker>
<rawString>Preslav Nakov and J¨org Tiedemann. 2012. Combining word-level and character-level models for machine translation between closely-related languages. In ACL (2), pages 301–305.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>2013a. Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics,</booktitle>
<editor>Hwee Tou Ng, Joel Tetreault, Siew Mei Wu, Yuanbin Wu, and Christian Hadiwinoto, editors.</editor>
<location>Sofia, Bulgaria,</location>
<marker></marker>
<rawString>Hwee Tou Ng, Joel Tetreault, Siew Mei Wu, Yuanbin Wu, and Christian Hadiwinoto, editors. 2013a. Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<title>The conll2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1494" citStr="Ng et al., 2013" startWordPosition="216" endWordPosition="219"> error correction generated considerable interest in the community since the early 1960s (Kukich, 1992) for at least two reasons. First, typical NLP tools lack in robustness against errors in their input. This sensitivity jeopardizes their usefulness especially for unedited text, which is prevalent on the web. Second, automated spell and grammar checkers facilitate text editing and can be of great help to nonnative speakers of a language. Several resources and shared tasks appeared recently, including the HOO task (Dale and Kilgarriff, 2010) and the CoNLL task on grammatical error correction (Ng et al., 2013b). In this paper we describe our participation to the first shared task on automatic error correction for Arabic (Mohit et al., 2014). While non-word errors are relatively easy to handle, the task is more challenging for grammatical and semantic errors. Detecting and correcting such errors require context-sensitive approaches in order to capture the dependencies between the words of a text at various lexical and semantic levels. All the more so for Arabic which brings dependence down to the morphological level (Habash, 2010). A particularity interesting approach to error correction relies on </context>
<context position="15945" citStr="Ng et al., 2013" startWordPosition="2555" endWordPosition="2558"> backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI 75.5 49.5 59.8 74.1 51.8 60.9 61.3 45.4 52.2 75.7 52.1 61.7 74.9 54.2 62.8 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknes</context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013b. The conll2013 shared task on grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>2014</date>
<booktitle>Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics,</booktitle>
<editor>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant, editors.</editor>
<location>Baltimore, Maryland,</location>
<marker>2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant, editors. 2014. Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arfath Pasha</author>
<author>Mohamed Al-Badrashiny</author>
<author>Mona T Diab</author>
<author>Ahmed El Kholy</author>
<author>Ramy Eskander</author>
<author>Nizar Habash</author>
<author>Manoj Pooleery</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic.</title>
<date>2014</date>
<booktitle>In LREC,</booktitle>
<pages>1094--1101</pages>
<marker>Pasha, Al-Badrashiny, Diab, El Kholy, Eskander, Habash, Pooleery, Rambow, Roth, 2014</marker>
<rawString>Arfath Pasha, Mohamed Al-Badrashiny, Mona T. Diab, Ahmed El Kholy, Ramy Eskander, Nizar Habash, Manoj Pooleery, Owen Rambow, and Ryan Roth. 2014. Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic. In LREC, pages 1094–1101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Darma Putra</author>
<author>Lili Szabo</author>
</authors>
<title>Uds at conll 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>88--95</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="15227" citStr="Putra and Szabo, 2013" startWordPosition="2442" endWordPosition="2445">ould be explained by its intrinsic bad performance. Since more investigation is needed to clarify on this point, we drop this component from our submission. Both reranking and punctuation insertion improve the performance. Our system submission to the shared task (backoff+PBSMT+Rank+PI) resulted in an F1 score of 58.6% on the official test set, with a precision of 76.9% and a recall of 47.3%. 4 Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013). Unlike our approach, most of 0.2 0.4 0.6 0.8 1.0 −8.33 −5.02 −1.7 1.61 4.93 117 System PR RC F1 PBSMT backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI 75.5 49.5 59.8 74.1 51.8 60.9 61.3 45.4 52.2 75.7 52.1 61.7 74.9 54.2 62.8 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. the proposed sys</context>
</contexts>
<marker>Putra, Szabo, 2013</marker>
<rawString>Desmond Darma Putra and Lili Szabo. 2013. Uds at conll 2013 shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 88–95, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Kai-Wei Chang</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>The university of illinois system in the conll-2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>13--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="16065" citStr="Rozovskaya et al., 2013" startWordPosition="2574" endWordPosition="2578">4 52.2 75.7 52.1 61.7 74.9 54.2 62.8 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Table 1: Pipeline precision, recall and F1 scores. ED: error detection, PI: punctuation insertion. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, and Dan Roth. 2013. The university of illinois system in the conll-2013 shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13–19, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
<author>Anubhav Gupta</author>
<author>Martin Tozer</author>
<author>Dolors Catala</author>
<author>Angels Catena</author>
<author>Sandrine Fuentes</author>
</authors>
<title>Rule-based system for automatic grammar correction using syntactic n-grams for english language learning (l2).</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>96--101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="15131" citStr="Sidorov et al., 2013" startWordPosition="2424" endWordPosition="2427">he expense of decreased precision. The error detection component hurts the performance which could be explained by its intrinsic bad performance. Since more investigation is needed to clarify on this point, we drop this component from our submission. Both reranking and punctuation insertion improve the performance. Our system submission to the shared task (backoff+PBSMT+Rank+PI) resulted in an F1 score of 58.6% on the official test set, with a precision of 76.9% and a recall of 47.3%. 4 Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013). Unlike our approach, most of 0.2 0.4 0.6 0.8 1.0 −8.33 −5.02 −1.7 1.61 4.93 117 System PR RC F1 PBSMT backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI 75.5 49.5 59.8 74.1 51.8 60.9 61.3 45.4 52.2 75.7 52.1 61.7 74.9 54.2 62.8 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Table 1: Pipeline p</context>
</contexts>
<marker>Sidorov, Gupta, Tozer, Catala, Catena, Fuentes, 2013</marker>
<rawString>Grigori Sidorov, Anubhav Gupta, Martin Tozer, Dolors Catala, Angels Catena, and Sandrine Fuentes. 2013. Rule-based system for automatic grammar correction using syntactic n-grams for english language learning (l2). In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 96–101, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="6651" citStr="Stolcke, 2002" startWordPosition="1053" endWordPosition="1054">, 2003) to train an SVM classifier using morphological and lexical features. We employ a quadratic polynomial kernel. The static feature window context size is set to +/- 2 words; the previous two (dynamic) predicted tags are also used as features. The feature set includes the surface forms and their normalization after “Alef”, “Ya” and digit normalization, the POS tags and the lemmas of the words. These morphological features are obtained using MADA 3.0 (Habash et al., 2009).1 We also use a set of word, POS and lemma 3-gram language models scores as features. These LMs are built using SRILM (Stolcke, 2002). The error detection component is integrated into the pipeline by concatenating the predicted tags with the words of the input text. The SMT model uses this additional information to learn distinct mappings conditional on the predicted correctness of words. 2.2 Character-level Back-off Correction Each word that is labeled as error (PROB) in the output of the error detection component is mapped to multiple possible corrections using a weighted finite-state transducer similar to the transducers used in speech recognition (Mohri et al., 2002). The WFST, for which we used OpenFST (Allauzen et al.</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadi Tomeh</author>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
<author>Noura Farra</author>
<author>Pradeep Dasigi</author>
<author>Mona Diab</author>
</authors>
<title>Reranking with linguistic and semantic features for arabic optical character recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>549--555</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="9803" citStr="Tomeh et al., 2013" startWordPosition="1538" endWordPosition="1541">e. Unlike translation, error correction is mainly monotonic, therefore we set disallow reordering by setting the distortion limit in Moses to 0.2 When no mapping can be found for a given phrase in the primary phrase table, the decoder looks it up in the back-off model. The decoder searches the space of all possible correction hypotheses, resulting from alternative segmentations and mappings, and returns the list of n-best scoring hypotheses. 2.4 N-best List Reranking In this step, we combine LM information with linguistically and semantically motivated features using learning to rank methods (Tomeh et al., 2013). Discriminative reranking (Liu, 2009) allows each hypothesis to be represented as an arbitrary set of features without the need to explicitly model their interactions. Therefore, the system benefits from global and potentially complex features which are not available to the baseline decoder. Each hypothesis in an n-best list is represented by a d-dimensional feature vector. Word error rate (WER) is computed for each hypotheses by comparing it to the reference correction. The resulting 2Only 0.14% of edits in the QALB corpus are actually reordering. scored n-best list is used for supervised tr</context>
</contexts>
<marker>Tomeh, Habash, Roth, Farra, Dasigi, Diab, 2013</marker>
<rawString>Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra, Pradeep Dasigi, and Mona Diab. 2013. Reranking with linguistic and semantic features for arabic optical character recognition. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 549–555, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bong-Jun Yi</author>
<author>Ho-Chang Lee</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Kunlp grammatical error correction system for conll-2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>123--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="15170" citStr="Yi et al., 2013" startWordPosition="2432" endWordPosition="2435">r detection component hurts the performance which could be explained by its intrinsic bad performance. Since more investigation is needed to clarify on this point, we drop this component from our submission. Both reranking and punctuation insertion improve the performance. Our system submission to the shared task (backoff+PBSMT+Rank+PI) resulted in an F1 score of 58.6% on the official test set, with a precision of 76.9% and a recall of 47.3%. 4 Related Work Both rule-based and data-driven approaches to error correction can be found in the literature (Sidorov et al., 2013; Berend et al., 2013; Yi et al., 2013) as well as hybridization of them (Putra and Szabo, 2013). Unlike our approach, most of 0.2 0.4 0.6 0.8 1.0 −8.33 −5.02 −1.7 1.61 4.93 117 System PR RC F1 PBSMT backoff+PBSMT ED+backoff+PBSMT backoff+PBSMT+Rank backoff+PBSMT+Rank+PI 75.5 49.5 59.8 74.1 51.8 60.9 61.3 45.4 52.2 75.7 52.1 61.7 74.9 54.2 62.8 Foundation). The statements made herein are solely the responsibility of the authors. Nizar Habash performed most of his contribution to this paper while he was at the Center for Computational Learning Systems at Columbia University. Table 1: Pipeline precision, recall and F1 scores. ED: err</context>
</contexts>
<marker>Yi, Lee, Rim, 2013</marker>
<rawString>Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim. 2013. Kunlp grammatical error correction system for conll-2013 shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 123–127, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng Yuan</author>
<author>Mariano Felice</author>
</authors>
<title>Constrained grammatical error correction using statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>52--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="16390" citStr="Yuan and Felice, 2013" startWordPosition="2626" endWordPosition="2629">ror detection, PI: punctuation insertion. the proposed systems build distinct models to address individual types of errors (see the CoNLL2013, 2014 proceedings (Ng et al., 2013a; Ng et al., 2014), and combine them afterwords using Integer Linear Programming for instance (Rozovskaya et al., 2013). This approach is relatively time-consuming when the number of error types increases. Interest in models that target all errors at once has increased, using either multi-class classifiers (Farra et al., 2014; Jia et al., 2013), of-the-shelf SMT techniques (Brockett et al., 2006; Mizumoto et al., 2011; Yuan and Felice, 2013; Buys and van der Merwe, 2013; Buys and van der Merwe, 2013), or building specialized decoders (Dahlmeier and Ng, 2012a). Our system addresses the weaknesses of the SMT approach using additional components in a pipeline architecture. Similar work on word-level and character-level model combination has been done in the context of translation between closely related languages (Nakov and Tiedemann, 2012). A character-level correction model has also been considered to reduce the out-of-vocabulary rate in translation systems (Habash, 2008). 5 Conclusion and Future Work We described a pipeline appr</context>
</contexts>
<marker>Yuan, Felice, 2013</marker>
<rawString>Zheng Yuan and Mariano Felice. 2013. Constrained grammatical error correction using statistical machine translation. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 52–61, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Yvon</author>
</authors>
<title>Rewriting the orthography of sms messages.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<pages>3</pages>
<contexts>
<context position="3433" citStr="Yvon, 2010" startWordPosition="526" endWordPosition="527">r Correction The PBSMT system accounts for context by learning, from a parallel corpus of annotated errors, mappings from erroneous multi-word segments of text to their corrections, and using a language model to help select the suitable corrections in context when multiple alternatives are present. Furthermore, since the SMT approach is datadriven, it is possible to address multiple types of errors at once, as long as examples of them appear in the training corpus. These errors may include non-word errors, wrong lexical choices and grammatical errors, and can also handle normalization issues (Yvon, 2010). One major issue is data sparsity, since large amount of labeled training data is necessary to provide reliable statistics of all error types. We ad114 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 114–120, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics dress this issue by backing-off the word-level PBSMT model with a character-level correction component, for which richer statistics can be obtained. Another issue may stem from the inherent difference in nature between error correction and translation. Unlike transla</context>
</contexts>
<marker>Yvon, 2010</marker>
<rawString>Franc¸ois Yvon. 2010. Rewriting the orthography of sms messages. Natural Language Engineering, 16:133–159, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wajdi Zaghouani</author>
<author>Behrang Mohit</author>
<author>Nizar Habash</author>
<author>Ossama Obeid</author>
<author>Nadi Tomeh</author>
<author>Alla Rozovskaya</author>
<author>Noura Farra</author>
<author>Sarah Alkuhlani</author>
<author>Kemal Oflazer</author>
</authors>
<title>Large scale arabic error annotation: Guidelines and framework.</title>
<date>2014</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="2750" citStr="Zaghouani et al., 2014" startWordPosition="413" endWordPosition="416">SMT) (Brockett et al., 2006), due to its contextsensitivity and data-driven aspect. Therefore, the pipeline system which we describe in Section 2 has as its core a phrase-based SMT component (PBSMT) (Section 2.3). Nevertheless, several factors may hinder the success of this approach, such as data sparsity, discrepancies between translation and error correction tasks, and the difficulty of incorporating context-sensitive features into the SMT decoder. We address all these issues in our system which achieves a better correction quality than a simple word-level PBSMT baseline on the QALB corpus (Zaghouani et al., 2014) as we show in our experiments in Section 3. 2 Pipeline Approach to Error Correction The PBSMT system accounts for context by learning, from a parallel corpus of annotated errors, mappings from erroneous multi-word segments of text to their corrections, and using a language model to help select the suitable corrections in context when multiple alternatives are present. Furthermore, since the SMT approach is datadriven, it is possible to address multiple types of errors at once, as long as examples of them appear in the training corpus. These errors may include non-word errors, wrong lexical ch</context>
<context position="12469" citStr="Zaghouani et al., 2014" startWordPosition="1984" endWordPosition="1987">.345, Cutoff= −1.045 0.0 0.2 0.4 0.6 0.8 1.0 Recall Figure 2: Evaluation of the error detection component. AUC: Area Under the Curve, PRBE: precision-recall break-even point. Classifier thresholds are displayed on the right vertical axis. or “fa” proclitic that is either a conjunction, a sub-conjunction or a connective particle. We obtain POS and proclitic information using MADAMIRA (Pasha et al., 2014). The output of this component is the final output of the system. 3 Experiments All the models we use in our pipeline are trained in a supervised way using the training part of the QALB corpus (Zaghouani et al., 2014), while we reserve the development part of the corpus for testing. 3.1 Error detection We evaluate the error detection binary classifier in terms of standard classification measures as shown in Figure 2. Each point on the curve is computed by selecting a threshold on the classifier score. The threshold we use correspond to recall equal to 80%, at which the precision is very low which leaves much room for improvement in the performance of the error detection component. 3.2 Character-level correction We evaluate the character-level correction model by measuring the percentage of erroneous phrase</context>
</contexts>
<marker>Zaghouani, Mohit, Habash, Obeid, Tomeh, Rozovskaya, Farra, Alkuhlani, Oflazer, 2014</marker>
<rawString>Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Ossama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014. Large scale arabic error annotation: Guidelines and framework. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland, May. European Language Resources Association (ELRA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
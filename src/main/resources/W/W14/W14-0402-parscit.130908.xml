<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.981776">
Focused Web Corpus Crawling
</title>
<author confidence="0.850407">
Roland Schäfer
</author>
<affiliation confidence="0.56264">
Freie Universität Berlin
roland.schaefer
@fu-berlin.de
</affiliation>
<author confidence="0.518343">
Adrien Barbaresi
</author>
<affiliation confidence="0.3304235">
ENS Lyon
adrien.barbaresi
</affiliation>
<email confidence="0.904663">
@ens.lyon.org
</email>
<author confidence="0.683999">
Felix Bildhauer
</author>
<affiliation confidence="0.4982455">
Freie Universität Berlin
felix.bildhauer
</affiliation>
<email confidence="0.360247">
@fu-berlin.de
</email>
<sectionHeader confidence="0.987066" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984291666667">
In web corpus construction, crawling is a
necessary step, and it is probably the most
costly of all, because it requires expen-
sive bandwidth usage, and excess crawl-
ing increases storage requirements. Ex-
cess crawling results from the fact that the
web contains a lot of redundant content
(duplicates and near-duplicates), as well
as other material not suitable or desirable
for inclusion in web corpora or web in-
dexes (for example, pages with little text
or virtually no text at all). An optimized
crawler for web corpus construction would
ideally avoid crawling such content in the
first place, saving bandwidth, storage, and
post-processing costs. In this paper, we
show in three experiments that two simple
scores are suitable to improve the ratio be-
tween corpus size and crawling effort for
web corpus construction. The first score
is related to overall text quality of the page
containing the link, the other one is related
to the likelihood that the local block en-
closing a link is boilerplate.
</bodyText>
<sectionHeader confidence="0.805036" genericHeader="keywords">
1 Crawl Optimization and Yield Ratios
</sectionHeader>
<bodyText confidence="0.999862295454545">
Optimizing a crawling strategy consists in maxi-
mizing its weighted coverage WC(t) at any time
t during a crawl (Olston and Najork, 2010, 29),
i. e., the summed weight of the documents down-
loaded until t, where the weight of each crawled
document is calculated as a measure of the useful-
ness of the document relative to the purpose of the
crawl. To maximize WC, it is vital to guess the
weight of the documents behind harvested links
before download, such that documents with poten-
tially lesser weight have a lower probability of be-
ing downloaded. So-called focused crawlers (in a
broad sense) are designed to maximize WC with
respect to some specific definition of document
weight, for example when documents with a high
search-engine relevance (measured as its Page-
Rank or a similar score), documents about specific
subjects, or documents in a specific language are
desired (Chakrabarti et al., 1999; Menczer et al.,
2004; Baykan et al., 2008; Safran et al., 2012).
For our purpose, i. e., web corpus crawling, a doc-
ument with a high weight can simply be defined as
one which is not removed from the corpus by the
post-processing tools due to low linguistic qual-
ity and/or a document which contributes a high
amount of text to the corpus. Recently, an inter-
esting approach to crawl optimization along such
lines was suggested which relies on statistics about
the corpus yield from known hosts (Suchomel
and Pomikálek, 2012). Under this approach, the
weight (rather of a whole web host) is taken to be
the ratio of good documents from the host remain-
ing in the corpus after a specific post-processing
chain has been applied to the documents. Har-
vested URLs pointing to certain hosts are priori-
tized accordingly. We follow a similar route like
Suchomel and Pomikálek, but look at document-
local features instead of host statistics.
Throughout this paper, we refer to the yield ra-
tio instead of WC, although they are related no-
tions. We define the yield ratio Yd for a set Dc of
crawled unprocessed documents and a set Dr of
retained documents after filtering and processing
for inclusion in a corpus, with Dr C Dc, as:
</bodyText>
<equation confidence="0.89252">
Yd = |Dr |(1)
|Dc|
</equation>
<bodyText confidence="0.617859">
For example, a document yield ratio Yd = 0.21
</bodyText>
<page confidence="0.879729">
9
</page>
<bodyText confidence="0.969892">
Felix Bildhauer &amp; Roland Schäfer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 9–15,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
means that 21% of the crawled documents sur-
vived the cleaning procedure (i. e., were not clas-
sified as duplicates or spam, were long enough,
written in the target language, etc.) and ended up
in the corpus. In order to maximize Yd, 79% of
the documents should not have been downloaded
in the first place in this example. A parallel defini-
tion is assumed for Yb for the respective amounts
of bytes. The document yield ratio is easier to in-
terpret because the byte yield ratio depends on the
amount of markup which has to be stripped, and
which might vary independently of the quality of
the downloaded web pages.
Obviously, the yield ratio – like the weighted
coverage – depends highly on the definition of
what a good document is, i. e., what the goal of
the crawl is. We assume, similar to Suchomel and
Pomikálek’s approach, that our tools reliably filter
out documents that are interesting documents for
inclusion a corpus, and that calculating a yield ra-
tio based on the output of those tools is therefore
reasonable.1
</bodyText>
<sectionHeader confidence="0.919325" genericHeader="introduction">
2 Experiment 1: Seed and Crawl Quality
</sectionHeader>
<bodyText confidence="0.999883636363637">
In this experiment, we examine the correlation be-
tween the yield ratio of crawler seed URLs and
the yield ratio of short Breadth-First Search (BFS)
crawls based on those URLs. We used the Her-
itrix (1.14) web crawler (Mohr et al., 2004) and
an older version of the texrex web page clean-
ing toolkit (Schäfer and Bildhauer, 2012). The
tools perform, among other things, boilerplate de-
tection and text quality evaluation in the form of
the so-called Badness score (Schäfer et al., 2013).
A document receives a low Badness score if the
most frequent function words of the target lan-
guage have a high enough frequency in the doc-
ument. The Badness score is based on previous
ideas from language identification and web doc-
ument filtering (Grefenstette, 1995; Baroni et al.,
2009).
Originally, this experiment was carried out in
the context of an evaluation of sources of differ-
ent seed URLs for crawls. In a preliminary step,
we began by collecting seed URLs from various
sources:
</bodyText>
<footnote confidence="0.9665634">
1This claim should be backed up by forms of ex-
trinsic/task-based evaluation (Schäfer and Bildhauer, 2013,
p. 104 ff). Such an evaluation (in the form of a collocation ex-
traction task) was recently presented for our corpora in work
by Stefan Evert (Biemann et al., 2013).
</footnote>
<listItem confidence="0.9983502">
1. the DMOZ directory
2. the Etools meta search engine
3. the FriendFeed social service aggregator
4. the identi.ca social bookmarking service
5. Wikipedia dumps
</listItem>
<bodyText confidence="0.999713">
We scraped the content behind the URLs and
ran a state-of-the-art language identifier (Lui and
Baldwin, 2012) on it in order to obtain language-
classified seed URLs (Barbaresi, 2013).2 We then
looked specifically at the following languages as-
sociated as the single dominant language with at
least one top-level domain (TLD):
</bodyText>
<listItem confidence="0.99584075">
1. Dutch (.nl)
2. French (.fr)
3. Indonesian (.id)
4. Swedish (.se)
</listItem>
<bodyText confidence="0.999984379310345">
We randomly sampled 1, 000 seed URLs for
each of the 20 permutations of seed sources
and languages/TLDs, downloaded them and used
texrex to determine the document yield ratio
for the documents behind the 1, 000 seeds. The
software was configured to perform boilerplate re-
moval, removal of documents based on high Bad-
ness scores, perfect duplicate removal, and dele-
tion of documents shorter than 1, 000 characters
(after boilerplate removal). Then, we crawled
the respective TLDs, starting the crawls with the
1, 000 seed URLs, respectively. In each crawl, we
downloaded 2 GB of raw data, cleaned them, and
calculated the document yield ratio using the same
configuration of texrex as we used for cleaning
the seed documents. Figure 1 plots the data and an
appropriate linear model.
We see that there is a strong correlation (ad-
justed R2 = 0.7831) between the yield ratio of
the documents behind the seed URLs and the yield
ratio of the documents found by using the seeds
for BFS crawling. It follows that giving high pri-
ority to links from pages which are themselves
considered high-quality documents by the post-
processing tools will likely lead to more efficient
crawling. Since there is no fundamental distinc-
tion between initial URL seeds and URLs har-
vested at a later time during the crawl, this effect
is likely to extend to the whole run time of a crawl.
</bodyText>
<footnote confidence="0.977821">
2See also Barbaresi, this volume.
</footnote>
<page confidence="0.995312">
10
</page>
<figureCaption confidence="0.985705">
Figure 1: Yield ratio Yd of the crawls (y axis) plot-
</figureCaption>
<bodyText confidence="0.9807508">
ted against the yield ratio of the documents be-
hind the crawls’ 1,000 seeds (x axis). (Higher Yd
is better.) Linear model: Intercept = −0.0098,
Coefficient = 0.6332, R2 = 0.7831 (adjusted),
p &lt; 0.001 (ANOVA).
</bodyText>
<sectionHeader confidence="0.980808" genericHeader="method">
3 Experiment 2: Crawling
with Cyclic URL Selection
</sectionHeader>
<bodyText confidence="0.950230951612903">
Using the same configuration of tools as in Sec-
tion 2, we performed a crawl targeting Flem-
ish documents in the Belgian . be national TLD,
which hosts both Flemish and French documents
in substantial proportions. Usually, even under
more favorable conditions (i. e., when we crawl a
TLD which contains mostly documents in the tar-
get language), the yield ratio of a BFS crawl de-
creases rapidly in the initial phase, then staying at
a low level (Schäfer and Bildhauer, 2013, p. 31).
Figure 2 illustrates this with an analysis of a . de
BFS crawl from late 2011, also processed with the
same tools as mentioned in Section 2. Notice that
the . de domain hosts German documents almost
exclusively.
The interesting complication in this experiment
is thus the non-target language present in the
TLD scope of the crawler and the related question
whether, simply speaking, predominantly Flemish
documents link to other predominantly Flemish
documents rather than French documents. Since
the Badness score (calculated as described in Sec-
tion 2) includes a form of language identification,
the yield ratio takes into account this additional
complication.
We tested whether the decline of the yield ra-
tio could be compensated for by selecting “high
quality” URLs in the following manner: The crawl
progressed in five phases. In the first short burn-
in phase, we crawled 1, 000, 000 documents, and
in each of the second to fifth phase, we crawled
10, 000, 000 documents. After each phase, the
Figure 2: Yield ratio (y axis) over time for a
BFS crawl in .de in November/December 2011
started with 231,484 seed URLs scraped from
Bing. The yield ratio was calculated at 1, 000
snapshots of 400 MB of data (= one Heritrix ARC
file). For snapshots s1..s500: Yd = 0.141, for
snapshots s501..s1000: Yd = 0.071. The vertical
bar marks the point at which the seeds were ex-
hausted. (Schäfer and Bildhauer, 2013, p. 31)
crawl was halted, the crawler frontier was emptied,
and the crawl was then re-started with a selection
of the URLs harvested in the previous phase. Only
those URLs were used which came from docu-
ments with a Badness score of 10 or lower (= doc-
uments in which the distribution of the most fre-
quent function words fits the expected distribution
for Flemish very well, cf. Section 2), and from text
blocks with a boilerplate score (Schäfer and Bild-
hauer, 2012) in [0.5, 1] (= likely not boilerplate).
Additionally, it was made sure that no URLs were
re-used between the five phases. The very promis-
ing results are plotted in Figure 3.
Figure 3: Yield ratio over crawl time with cyclic
URL selection in the .be TLD. The x axis shows
the crawl progression in snapshots of 400 MB of
raw crawled data (= one Heritrix ARC file). The y
axis shows the yield ratio for each snapshot. The
five phases are clearly distinguishable by the sud-
den increases in yield ratio.
0 500 1000 1500 2000
</bodyText>
<page confidence="0.973107">
11
</page>
<table confidence="0.946618">
phase adjusted R2 p (ANOVA)
1 0.8288 &lt; 0.001
2 0.9187 &lt; 0.001
3 0.8308 &lt; 0.001
4 0.9125 &lt; 0.001
5 0.9025 &lt; 0.001
</table>
<tableCaption confidence="0.997619">
Table 1: Fit of linear models for the decrease in
</tableCaption>
<bodyText confidence="0.9964195">
the yield ratios of the first 100 snapshots in each
of the five phases of the .be crawl. For the first
phase, only 50 snapshots were crawled and fitted.
The decline of the yield ratio is almost linear
for the first 100 snapshots in the five phases (cf.
Table 1), where each phase has roughly 500 snap-
shots in total, and one snapshot corresponds to
400 MB of downloaded raw data. After this de-
cline, the yield ratio remains at low levels around
0.05. Cyclic URL selection, however, repeatedly
manages to push the yield ratio to above 0.2 for a
short period. The subsequent sharp decline shows
that link selection/prioritization should rather be
implemented in the crawler frontier management
in order to achieve a constant effect over longer
crawls (cf. Section 5).
</bodyText>
<sectionHeader confidence="0.996067" genericHeader="evaluation">
4 Experiment 3: Internal Crawl Analysis
</sectionHeader>
<bodyText confidence="0.999943954545454">
For the last experiment, we used the most recent
version of the texrex toolkit, which writes full
link structures for the processed documents as a
by-product.3 An internal analysis of a small por-
tion of a crawled data set from the German TLD
was performed, which is part of the raw mate-
rial of the DECOW corpus (Schäfer and Bild-
hauer, 2012). The data set contains 11, 557,695
crawled HTML documents and 81, 255, 876 http
links extracted from the crawled documents (only
&lt;a&gt; tags). Among the link URLs in the sam-
ple, 711,092 are actually links to documents in
the sample, so we could analyze exactly those
711, 092 links. It should be noticed that we only
looked at links to different hosts, such that host-
internal links (navigation to “Home”, etc.) are not
included in the analysis.
In this experiment, we were interested specif-
ically in the many documents which we usually
discard right away simply because they are either
very short (below 2 KB of unstripped HTML) or
perfect duplicates of other documents. This is a
</bodyText>
<footnote confidence="0.7287465">
3The new version (release name hyperhyper) has been
released and documented at http://texrex.sf.net/.
</footnote>
<bodyText confidence="0.771061470588235">
positives negatives
true 69,273 342,430
false 237,959 61,430
Table 2: Confusion matrix for binary download
decisions based on the Badness of the document
containing the URL for the DECOW crawl sam-
ple described in Section 4. Badness threshold at
10. Precision=0.225, Recall=0.530, F1=0.316.
step of document selection which usually precedes
the cleansing used for the experiments described
in Sections 2 and 3. The analysis shows that of the
711,092 link URLs in the sample, 130,703 point
to documents which are not perfect duplicates of
other documents and which are over 2 KB long.
580, 389 of them point to documents which do not
satisfy these criteria. We then evaluated the quality
of the link environments in terms of their Badness
</bodyText>
<figureCaption confidence="0.626999888888889">
and boilerplate scores. The results are shown in
Figures 4 and 5.4
Figure 4: Badness scores of the links in the crawl
analysis described in Section 4. The x axis shows
the Badness scores of the documents which linked
to the retained (“good”) and the deleted (“bad”)
documents. The y axis shows the proportion of
retained/deleted documents for which the Badness
score is ≥ x. (Lower Badness scores are better.)
</figureCaption>
<bodyText confidence="0.998436230769231">
The observable correlation between the quality
of a link’s context and the quality of the page be-
hind the link is stronger for the boilerplate score
than for the Badness score. For example, had
we only followed links from documents with a
Badness score of 10 or lower (= better), then
4Notice that the older version of texrex used in the
experiments described in Sections 2 and 3 assigns a boiler-
plate score of 1 to text blocks which are most likely good
text, while the new texrex-hyperhyper assigns 1 to text
blocks which are most likely boilerplate. Take this into ac-
count when comparing the thresholds mentioned there and
those reported here.
</bodyText>
<figure confidence="0.953872333333333">
0 5 10 15 20 25 30 35 40 45 50
retained
deleted
</figure>
<page confidence="0.695994">
12
</page>
<figureCaption confidence="0.941899625">
Figure 5: Boilerplate scores of the links in the
crawl analysis described in Section 4. The x axis
shows the boilerplate scores of the blocks which
linked to the retained (“good”) and the deleted
(“bad”) documents. The y axis shows the propor-
tion of retained/deleted documents for which the
boilerplate score is ≥ x. (Lower boilerplate scores
are better.)
</figureCaption>
<bodyText confidence="0.566972">
positives negatives
true 83,650 522,350
false 58,039 47,053
</bodyText>
<tableCaption confidence="0.785783">
Table 3: Confusion matrix for binary down-
</tableCaption>
<bodyText confidence="0.9409926875">
load decisions based on the boilerplate score of
the block containing the URL for the DECOW
crawl sample described in Section 4. Boilerplate
threshold at 0.5. Precision=0.590, Recall=0.640,
F1=0.614.
0.59 × 580,389 = 342,430 bad documents would
not have been downloaded, but at the same time
0.47×130, 703 = 61, 430 good documents would
have been lost. Tables 2 and 3 show a confusion
matrix for a reasonable Badness threshold (10) and
a reasonable boilerplate threshold (0.5). Obvi-
ously, if we use Badness and boilerplate scores of
the link context to make a binary download deci-
sion, the accuracy is much too low, which is why
we suggest to merely prioritize URLs instead of
discarding them, cf. Section 5.
</bodyText>
<sectionHeader confidence="0.991146" genericHeader="conclusions">
5 Conclusion and
</sectionHeader>
<subsectionHeader confidence="0.935937">
Planned Crawler Architecture
</subsectionHeader>
<bodyText confidence="0.999972254545455">
We have shown that two standard cleaning algo-
rithms used in web corpus construction, i. e., text
quality evaluation based on frequent short words
and boilerplate detection (as implemented in the
texrex toolkit) have a high potential for optimiz-
ing web corpus crawling through the prioritization
of harvested URLs in a crawler system.
We are now in the process of designing a custom
web corpus crawler system called HeidiX, which
integrates the texrex post-processing tools for
weight estimation based on the methods described
in this paper. Cf. Figure 6, which schematically
shows the current design draft.5
HeidiX is designed with a system of ranked
URL back queues for harvested links (cf.
UrlQueues). Each queue holds URLs for which
the weight estimation is within a specifiable in-
terval, such that the most promising URLs are in
one queue, etc. The actual downloading is per-
formed by massively parallel fetcher threads in
the FetcherPool, which (in the final software) will
talk to a DNS cacher and a politeness manager,
which handles caching of Robots Exclusion In-
formation and politeness intervals. The fetcher
threads pop URLs from one of the ranked queues,
which is selected randomly with prior probabili-
ties inversely proportional to the rank of the queue.
Thus, promising URLs are popped more often and
less promising ones less often.
For guessing the weight, pluggable modules
can be used and combined in the Focused-
Walker container. Currently, we have the stan-
dard UrlSeenFilter, which is based on our own
self-scaling Bloom Filter implementation (Bloom,
1970; Almeida et al., 2007), and which pre-
vents any URL from being queued more than
once. We have plans for a URL-based language
guesser (Baykan et al., 2008) in the form of
the LanguagePredictor, and a prioritizer based
on the yield from specific hosts as described in
Suchomel and Pomikálek (2012) in the form of
the HostYieldPrioritizer, which reads statistics di-
rectly from the texrex module. The texrex
module extracts all hyperlinks from processed
documents and tags them with the quality scores
described in this paper, such that the QualityPri-
oritizer module can adjust the expected weight of
the document behind each URL.
The HeidiX architecture also features an al-
ternative queueing strategy in the form of the
RandomWalker, which allows users to obtain uni-
form random samples from the web based on ex-
isting algorithms (Henzinger et al., 2000; Rus-
mevichientong et al., 2001). Since obtaining such
samples is a goal which is mostly orthogonal to the
</bodyText>
<footnote confidence="0.96475525">
5Like texrex, it is written entirely in the FreePascal
dialect of ObjectPascal (http://freepascal.org/),
uses only very few additional C libraries, and will be released
under the GPL 3.
</footnote>
<page confidence="0.997741">
13
</page>
<figure confidence="0.999751736842105">
HostYieldPrioritizer
LanguagePredictor
QualityPrioritizer
FocusedWalker
UrlSeenFilter
Statistics
URL
RandomWalker
(CLARAx)
FetcherPool
UrlQueues
texrex
HTML
PolitenessManager
SnapshotKeeper
DNSCacher
Corpus
WWW
Snapshots
</figure>
<figureCaption confidence="0.96173975">
Figure 6: HeidiX Crawler Architecture. Grayed modules are done as of March 2014. The Focused-
Walker implements an “efficiently locate good corpus document” URL prioritization scheme; the Ran-
domWalker implements bias-corrected Random Walk URL selection for obtaining uniform random sam-
ples.
</figureCaption>
<bodyText confidence="0.9999648">
one assumed in this paper, we do not discuss this
further here. Finally, a SnapshotKeeper module
allows users to halt and continue crawls by writ-
ing/reading the current state of the relevant com-
ponents to/from disk.
We hope that HeidiX will become a valuable
tool in both the efficient construction of very large
web corpora (FocusedWalker) and the construc-
tion of smaller unbiased reference samples as well
as web analysis (RandomWalker).
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999705981132076">
Paulo Sérgio Almeida, Carlos Baquero, Nuno
Preguiça, and David Hutchison. 2007. Scalable
bloom filters. Information Processing Letters,
101:255–261.
Adrien Barbaresi. 2013. Crawling microblogging ser-
vices to gather language-classified urls. workflow
and case study. In 51st Annual Meeting of the As-
sociation for Computational Linguistics Proceed-
ings of the Student Research Workshop, pages 9–15,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209–226.
Eda Baykan, Monika Henzinger, and Ingmar Weber.
2008. Web page language identification based on
URLs. In Proceedings of the VLDB Endowment,
pages 176–187.
Chris Biemann, Felix Bildhauer, Stefan Evert, Dirk
Goldhahn, Uwe Quasthoff, Roland Schäfer, Jo-
hannes Simon, Leonard Swiezinski, and Torsten
Zesch. 2013. Scalable construction of high-quality
web corpora. Journal for Language Technology and
Computational Linguistics, 28(2):23–60.
Burton Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of
ACM, 13(7):422–426.
Soumen Chakrabarti, Martin van den Berg, and Byron
Dom. 1999. Focused crawling: a new approach
to topic-specific web resource discovery. Computer
Networks, 31:1623–1640.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of the 3rd In-
ternation conference on Statistical Analysis of Tex-
tual Data (JADT 1995), pages 263–268, Rome.
Monika R. Henzinger, Allan Heydon, Michael Mitzen-
macher, and Marc Najork. 2000. On near-uniform
URL sampling. In Proceedings of the 9th Inter-
national World Wide Web conference on Computer
Networks: The International Journal of Computer
and Telecommunications Networking, pages 295–
308. North-Holland Publishing Co.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
Off-the-shelf Language Identification Tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012),
Jeju, Republic of Korea.
Filippo Menczer, Gautam Pant, and Padmini Srini-
vasan. 2004. Topical web crawlers: Evaluating
adaptive algorithms. ACM Trans. Internet Technol.,
4(4):378–419.
</reference>
<page confidence="0.984342">
14
</page>
<reference confidence="0.999469071428572">
Gordon Mohr, Michael Stack, Igor Ranitovic, Dan Av-
ery, and Michele Kimpton. 2004. Introduction
to Heritrix, an archival quality web crawler. In
Proceedings of the 4th International Web Archiving
Workshop (IWAW’04).
Christopher Olston and Marc Najork. 2010. Web
Crawling, volume 4(3) of Foundations and Trends
in Information Retrieval. now Publishers, Hanover,
MA.
Paat Rusmevichientong, David M. Pennock, Steve
Lawrence, and C. Lee Giles. 2001. Methods for
sampling pages uniformly from the World Wide
Web. In In AAAI Fall Symposium on Using Uncer-
tainty Within Computation, pages 121–128.
M.S. Safran, A. Althagafi, and Dunren Che. 2012.
Improving relevance prediction for focused Web
crawlers. In IEEE/ACIS 11th International Confer-
ence on Computer and Information Science (ICIS),
2012, pages 161–166.
Roland Schäfer and Felix Bildhauer. 2012. Build-
ing large corpora from the web using a new ef-
ficient tool chain. In Nicoletta Calzolari, Khalid
Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Eight
International Conference on Language Resources
and Evaluation (LREC’12), pages 486–493, Istan-
bul. ELRA.
Roland Schäfer and Felix Bildhauer. 2013. Web Cor-
pus Construction. Synthesis Lectures on Human
Language Technologies. Morgan and Claypool, San
Francisco.
Roland Schäfer, Adrien Barbaresi, and Felix Bildhauer.
2013. The good, the bad, and the hazy: Design de-
cisions in web corpus construction. In Stefan Evert,
Egon Stemle, and Paul Rayson, editors, Proceedings
of the 8th Web as Corpus Workshop (WAC-8), pages
7–15, Lancaster. SIGWAC.
Vít Suchomel and Jan Pomikálek. 2012. Effcient Web
crawling for large text corpora. In Adam Kilgarriff
and Serge Sharoff, editors, Proceedings of the sev-
enth Web as Corpus Workshop, pages 40–44.
</reference>
<page confidence="0.997923">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.999343">Focused Web Corpus Crawling</title>
<author confidence="0.7431985">Roland Freie Universität</author>
<email confidence="0.888706">@fu-berlin.de</email>
<author confidence="0.92892">Adrien</author>
<affiliation confidence="0.463957">ENS</affiliation>
<email confidence="0.856695">@ens.lyon.org</email>
<author confidence="0.756177">Felix Freie Universität</author>
<email confidence="0.957032">@fu-berlin.de</email>
<abstract confidence="0.972642770562771">In web corpus construction, crawling is a necessary step, and it is probably the most costly of all, because it requires expensive bandwidth usage, and excess crawling increases storage requirements. Excess crawling results from the fact that the web contains a lot of redundant content (duplicates and near-duplicates), as well as other material not suitable or desirable for inclusion in web corpora or web indexes (for example, pages with little text or virtually no text at all). An optimized crawler for web corpus construction would ideally avoid crawling such content in the first place, saving bandwidth, storage, and post-processing costs. In this paper, we show in three experiments that two simple scores are suitable to improve the ratio between corpus size and crawling effort for web corpus construction. The first score is related to overall text quality of the page containing the link, the other one is related to the likelihood that the local block enclosing a link is boilerplate. 1 Crawl Optimization and Yield Ratios Optimizing a crawling strategy consists in maxiits weighted coverage any time a crawl (Olston and Najork, 2010, 29), i. e., the summed weight of the documents downuntil where the weight of each crawled document is calculated as a measure of the usefulness of the document relative to the purpose of the To maximize it is vital to guess the weight of the documents behind harvested links download, such that documents with potentially lesser weight have a lower probability of being downloaded. So-called focused crawlers (in a sense) are designed to maximize respect to some specific definition of document weight, for example when documents with a high search-engine relevance (measured as its Page- Rank or a similar score), documents about specific subjects, or documents in a specific language are desired (Chakrabarti et al., 1999; Menczer et al., 2004; Baykan et al., 2008; Safran et al., 2012). For our purpose, i. e., web corpus crawling, a document with a high weight can simply be defined as one which is not removed from the corpus by the post-processing tools due to low linguistic quality and/or a document which contributes a high amount of text to the corpus. Recently, an interesting approach to crawl optimization along such lines was suggested which relies on statistics about the corpus yield from known hosts (Suchomel and Pomikálek, 2012). Under this approach, the weight (rather of a whole web host) is taken to be the ratio of good documents from the host remaining in the corpus after a specific post-processing chain has been applied to the documents. Harvested URLs pointing to certain hosts are prioritized accordingly. We follow a similar route like Suchomel and Pomikálek, but look at documentlocal features instead of host statistics. this paper, we refer to the raof although they are related no- We define the yield ratio a set of unprocessed documents and a set of retained documents after filtering and processing inclusion in a corpus, with C as: (1) example, a document yield ratio 9 Bildhauer &amp; Roland Schäfer (eds.), of the 9th Web as Corpus Workshop (WaC-9) @ EACL pages 9–15, Sweden, April 26 2014. Association for Computational Linguistics means that 21% of the crawled documents survived the cleaning procedure (i. e., were not classified as duplicates or spam, were long enough, written in the target language, etc.) and ended up the corpus. In order to maximize 79% of the documents should not have been downloaded in the first place in this example. A parallel definiis assumed for the respective amounts of bytes. The document yield ratio is easier to interpret because the byte yield ratio depends on the amount of markup which has to be stripped, and which might vary independently of the quality of the downloaded web pages. Obviously, the yield ratio – like the weighted coverage – depends highly on the definition of what a good document is, i. e., what the goal of the crawl is. We assume, similar to Suchomel and Pomikálek’s approach, that our tools reliably filter out documents that are interesting documents for inclusion a corpus, and that calculating a yield ratio based on the output of those tools is therefore 2 Experiment 1: Seed and Crawl Quality In this experiment, we examine the correlation between the yield ratio of crawler seed URLs and the yield ratio of short Breadth-First Search (BFS) crawls based on those URLs. We used the Heritrix (1.14) web crawler (Mohr et al., 2004) and older version of the page cleaning toolkit (Schäfer and Bildhauer, 2012). The tools perform, among other things, boilerplate detection and text quality evaluation in the form of the so-called Badness score (Schäfer et al., 2013). A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls. In a preliminary step, we began by collecting seed URLs from various sources: claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. 104 ff). Such an evaluation (in the form of a collocation extraction task) was recently presented for our corpora in work by Stefan Evert (Biemann et al., 2013). the the search engine the service aggregator the bookmarking service Wikipedia We scraped the content behind the URLs and ran a state-of-the-art language identifier (Lui and Baldwin, 2012) on it in order to obtain languageseed URLs (Barbaresi, We then looked specifically at the following languages associated as the single dominant language with at least one top-level domain (TLD): Dutch French Indonesian Swedish randomly sampled seed URLs for each of the 20 permutations of seed sources and languages/TLDs, downloaded them and used determine the document yield ratio the documents behind the seeds. The software was configured to perform boilerplate removal, removal of documents based on high Badness scores, perfect duplicate removal, and deleof documents shorter than characters (after boilerplate removal). Then, we crawled the respective TLDs, starting the crawls with the seed URLs, respectively. In each crawl, we downloaded 2 GB of raw data, cleaned them, and calculated the document yield ratio using the same of we used for cleaning the seed documents. Figure 1 plots the data and an appropriate linear model. We see that there is a strong correlation (ad- = between the yield ratio of the documents behind the seed URLs and the yield ratio of the documents found by using the seeds for BFS crawling. It follows that giving high priority to links from pages which are themselves considered high-quality documents by the postprocessing tools will likely lead to more efficient crawling. Since there is no fundamental distinction between initial URL seeds and URLs harvested at a later time during the crawl, this effect is likely to extend to the whole run time of a crawl. also Barbaresi, this volume. 10 1: Yield ratio the crawls (y axis) plotted against the yield ratio of the documents bethe crawls’ 1,000 seeds (x axis). (Higher better.) Linear model: = (adjusted), &lt; (ANOVA). 3 Experiment 2: Crawling with Cyclic URL Selection Using the same configuration of tools as in Section 2, we performed a crawl targeting Flemdocuments in the Belgian be TLD, which hosts both Flemish and French documents in substantial proportions. Usually, even under more favorable conditions (i. e., when we crawl a TLD which contains mostly documents in the target language), the yield ratio of a BFS crawl decreases rapidly in the initial phase, then staying at a low level (Schäfer and Bildhauer, 2013, p. 31). 2 illustrates this with an analysis of a de BFS crawl from late 2011, also processed with the same tools as mentioned in Section 2. Notice that de hosts German documents almost exclusively. The interesting complication in this experiment is thus the non-target language present in the TLD scope of the crawler and the related question simply predominantly Flemish documents link to other predominantly Flemish rather than documents. Since the Badness score (calculated as described in Sec- 2) includes a form language identification, the yield ratio takes into account this additional complication. We tested whether the decline of the yield ratio could be compensated for by selecting “high quality” URLs in the following manner: The crawl progressed in five phases. In the first short burnphase, we crawled documents, and in each of the second to fifth phase, we crawled documents. After each phase, the Figure 2: Yield ratio (y axis) over time for a crawl in November/December 2011 with seed URLs scraped from The yield ratio was calculated at snapshots of 400 MB of data (= one Heritrix ARC For snapshots for The vertical bar marks the point at which the seeds were exhausted. (Schäfer and Bildhauer, 2013, p. 31) crawl was halted, the crawler frontier was emptied, and the crawl was then re-started with a selection of the URLs harvested in the previous phase. Only those URLs were used which came from documents with a Badness score of 10 or lower (= documents in which the distribution of the most frequent function words fits the expected distribution for Flemish very well, cf. Section 2), and from text blocks with a boilerplate score (Schäfer and Bild- 2012) in (= likely not boilerplate). Additionally, it was made sure that no URLs were re-used between the five phases. The very promising results are plotted in Figure 3. Figure 3: Yield ratio over crawl time with cyclic selection in the The x axis shows the crawl progression in snapshots of 400 MB of raw crawled data (= one Heritrix ARC file). The y axis shows the yield ratio for each snapshot. The five phases are clearly distinguishable by the sudden increases in yield ratio. 500 1000 1500 2000 11 adjusted p 1 2 3 4 5 Table 1: Fit of linear models for the decrease in the yield ratios of the first 100 snapshots in each the five phases of the For the first phase, only 50 snapshots were crawled and fitted. The decline of the yield ratio is almost linear for the first 100 snapshots in the five phases (cf. Table 1), where each phase has roughly 500 snapshots in total, and one snapshot corresponds to 400 MB of downloaded raw data. After this decline, the yield ratio remains at low levels around Cyclic URL selection, however, repeatedly to push the yield ratio to above for a short period. The subsequent sharp decline shows that link selection/prioritization should rather be implemented in the crawler frontier management in order to achieve a constant effect over longer crawls (cf. Section 5). 4 Experiment 3: Internal Crawl Analysis For the last experiment, we used the most recent of the which writes full link structures for the processed documents as a An internal analysis of a small portion of a crawled data set from the German TLD was performed, which is part of the raw material of the DECOW corpus (Schäfer and Bild- 2012). The data set contains HTML documents and links extracted from the crawled documents (only Among the link URLs in the samare actually links to documents in the sample, so we could analyze exactly those links. It should be noticed that we only looked at links to different hosts, such that hostinternal links (navigation to “Home”, etc.) are not included in the analysis. In this experiment, we were interested specifically in the many documents which we usually discard right away simply because they are either very short (below 2 KB of unstripped HTML) or perfect duplicates of other documents. This is a new version (release name has been and documented at positives negatives Table 2: Confusion matrix for binary download decisions based on the Badness of the document containing the URL for the DECOW crawl sample described in Section 4. Badness threshold at step of document selection which usually precedes the cleansing used for the experiments described in Sections 2 and 3. The analysis shows that of the link URLs in the sample, point to documents which are not perfect duplicates of other documents and which are over 2 KB long. of them point to documents which do not satisfy these criteria. We then evaluated the quality of the link environments in terms of their Badness and boilerplate scores. The results are shown in 4 and Figure 4: Badness scores of the links in the crawl analysis described in Section 4. The x axis shows the Badness scores of the documents which linked to the retained (“good”) and the deleted (“bad”) documents. The y axis shows the proportion of retained/deleted documents for which the Badness is (Lower Badness scores are better.) correlation between the quality of a link’s context and the quality of the page bethe is stronger for the boilerplate score than for the Badness score. For example, had we only followed links from documents with a Badness score of 10 or lower (= better), then that the older version of in the experiments described in Sections 2 and 3 assigns a boilerscore of text blocks which are most likely good while the new text blocks which are most likely boilerplate. Take this into account when comparing the thresholds mentioned there and those reported here. 5 10 15 20 25 30 35 40 45 50 retained deleted 12 Figure 5: Boilerplate scores of the links in the crawl analysis described in Section 4. The x axis shows the boilerplate scores of the blocks which linked to the retained (“good”) and the deleted (“bad”) documents. The y axis shows the proportion of retained/deleted documents for which the score is (Lower boilerplate scores are better.) positives negatives Table 3: Confusion matrix for binary download decisions based on the boilerplate score of the block containing the URL for the DECOW crawl sample described in Section 4. Boilerplate at = bad documents would not have been downloaded, but at the same time = good documents would have been lost. Tables 2 and 3 show a confusion matrix for a reasonable Badness threshold (10) and reasonable boilerplate threshold Obviously, if we use Badness and boilerplate scores of the link context to make a binary download decision, the accuracy is much too low, which is why we suggest to merely prioritize URLs instead of discarding them, cf. Section 5. 5 Conclusion and Planned Crawler Architecture We have shown that two standard cleaning algorithms used in web corpus construction, i. e., text quality evaluation based on frequent short words and boilerplate detection (as implemented in the have a high potential for optimizing web corpus crawling through the prioritization of harvested URLs in a crawler system. We are now in the process of designing a custom corpus crawler system called which the tools for weight estimation based on the methods described in this paper. Cf. Figure 6, which schematically the current design designed with a system of ranked URL back queues for harvested links (cf. Each queue holds URLs for which the weight estimation is within a specifiable interval, such that the most promising URLs are in one queue, etc. The actual downloading is performed by massively parallel fetcher threads in which (in the final software) will talk to a DNS cacher and a politeness manager, which handles caching of Robots Exclusion Information and politeness intervals. The fetcher threads pop URLs from one of the ranked queues, which is selected randomly with prior probabilities inversely proportional to the rank of the queue. Thus, promising URLs are popped more often and less promising ones less often. For guessing the weight, pluggable modules be used and combined in the Focused- Currently, we have the stanwhich is based on our own self-scaling Bloom Filter implementation (Bloom, 1970; Almeida et al., 2007), and which prevents any URL from being queued more than once. We have plans for a URL-based language guesser (Baykan et al., 2008) in the form of and a prioritizer based on the yield from specific hosts as described in Suchomel and Pomikálek (2012) in the form of which reads statistics difrom the The module extracts all hyperlinks from processed documents and tags them with the quality scores in this paper, such that the QualityPrican adjust the expected weight of the document behind each URL. also features an alternative queueing strategy in the form of the which allows users to obtain uniform random samples from the web based on existing algorithms (Henzinger et al., 2000; Rusmevichientong et al., 2001). Since obtaining such samples is a goal which is mostly orthogonal to the it is written entirely in the FreePascal of ObjectPascal uses only very few additional C libraries, and will be released under the GPL 3. 13 HostYieldPrioritizer LanguagePredictor QualityPrioritizer FocusedWalker UrlSeenFilter Statistics URL RandomWalker (CLARAx) FetcherPool UrlQueues texrex HTML PolitenessManager SnapshotKeeper DNSCacher Corpus WWW Snapshots 6: Architecture. Grayed modules are done as of March 2014. The Focused- Walker implements an “efficiently locate good corpus document” URL prioritization scheme; the RandomWalker implements bias-corrected Random Walk URL selection for obtaining uniform random samples. one assumed in this paper, we do not discuss this here. Finally, a allows users to halt and continue crawls by writing/reading the current state of the relevant components to/from disk. hope that become a valuable tool in both the efficient construction of very large corpora and the construction of smaller unbiased reference samples as well web analysis References Paulo Sérgio Almeida, Carlos Baquero, Nuno Preguiça, and David Hutchison. 2007. Scalable filters. Processing 101:255–261. Adrien Barbaresi. 2013. Crawling microblogging services to gather language-classified urls. workflow case study. In Annual Meeting of the Association for Computational Linguistics Proceedof the Student Research pages 9–15, Sofia, Bulgaria, August. Association for Computational Linguistics.</abstract>
<author confidence="0.747531333333333">The WaCky Wide Web A collection of very large linguistically pro-</author>
<note confidence="0.906449666666667">web-crawled corpora. Resources 43(3):209–226. Eda Baykan, Monika Henzinger, and Ingmar Weber. 2008. Web page language identification based on In of the VLDB pages 176–187.</note>
<author confidence="0.534555">Chris Biemann</author>
<author confidence="0.534555">Felix Bildhauer</author>
<author confidence="0.534555">Stefan Evert</author>
<author confidence="0.534555">Dirk Goldhahn</author>
<author confidence="0.534555">Uwe Quasthoff</author>
<author confidence="0.534555">Roland Schäfer</author>
<author confidence="0.534555">Jo-</author>
<email confidence="0.310508">hannesSimon,LeonardSwiezinski,andTorsten</email>
<abstract confidence="0.837799615384615">Zesch. 2013. Scalable construction of high-quality corpora. for Language Technology and 28(2):23–60. Burton Bloom. 1970. Space/time trade-offs in hash with allowable errors. of 13(7):422–426. Soumen Chakrabarti, Martin van den Berg, and Byron Dom. 1999. Focused crawling: a new approach topic-specific web resource discovery. 31:1623–1640. Gregory Grefenstette. 1995. Comparing two language schemes. In of the 3rd Internation conference on Statistical Analysis of Tex-</abstract>
<note confidence="0.817006">Data (JADT pages 263–268, Rome.</note>
<author confidence="0.74548">Monika R Henzinger</author>
<author confidence="0.74548">Allan Heydon</author>
<author confidence="0.74548">Michael Mitzen-</author>
<email confidence="0.447814">macher,andMarcNajork.2000.Onnear-uniform</email>
<note confidence="0.87003425">sampling. In of the 9th International World Wide Web conference on Computer Networks: The International Journal of Computer Telecommunications pages 295– 308. North-Holland Publishing Co. Marco Lui and Timothy Baldwin. 2012. langid.py: An Language Identification Tool. In Proceedings of the 50th Annual Meeting of the Assofor Computational Linguistics (ACL Jeju, Republic of Korea. Filippo Menczer, Gautam Pant, and Padmini Srinivasan. 2004. Topical web crawlers: Evaluating algorithms. Trans. Internet 4(4):378–419. 14 Gordon Mohr, Michael Stack, Igor Ranitovic, Dan Av-</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paulo Sérgio Almeida</author>
<author>Carlos Baquero</author>
<author>Nuno Preguiça</author>
<author>David Hutchison</author>
</authors>
<date>2007</date>
<booktitle>Scalable bloom filters. Information Processing Letters,</booktitle>
<pages>101--255</pages>
<contexts>
<context position="17844" citStr="Almeida et al., 2007" startWordPosition="3023" endWordPosition="3026">will talk to a DNS cacher and a politeness manager, which handles caching of Robots Exclusion Information and politeness intervals. The fetcher threads pop URLs from one of the ranked queues, which is selected randomly with prior probabilities inversely proportional to the rank of the queue. Thus, promising URLs are popped more often and less promising ones less often. For guessing the weight, pluggable modules can be used and combined in the FocusedWalker container. Currently, we have the standard UrlSeenFilter, which is based on our own self-scaling Bloom Filter implementation (Bloom, 1970; Almeida et al., 2007), and which prevents any URL from being queued more than once. We have plans for a URL-based language guesser (Baykan et al., 2008) in the form of the LanguagePredictor, and a prioritizer based on the yield from specific hosts as described in Suchomel and Pomikálek (2012) in the form of the HostYieldPrioritizer, which reads statistics directly from the texrex module. The texrex module extracts all hyperlinks from processed documents and tags them with the quality scores described in this paper, such that the QualityPrioritizer module can adjust the expected weight of the document behind each U</context>
</contexts>
<marker>Almeida, Baquero, Preguiça, Hutchison, 2007</marker>
<rawString>Paulo Sérgio Almeida, Carlos Baquero, Nuno Preguiça, and David Hutchison. 2007. Scalable bloom filters. Information Processing Letters, 101:255–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrien Barbaresi</author>
</authors>
<title>Crawling microblogging services to gather language-classified urls. workflow and case study.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,</booktitle>
<pages>9--15</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6309" citStr="Barbaresi, 2013" startWordPosition="1058" endWordPosition="1059">: 1This claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. 104 ff). Such an evaluation (in the form of a collocation extraction task) was recently presented for our corpora in work by Stefan Evert (Biemann et al., 2013). 1. the DMOZ directory 2. the Etools meta search engine 3. the FriendFeed social service aggregator 4. the identi.ca social bookmarking service 5. Wikipedia dumps We scraped the content behind the URLs and ran a state-of-the-art language identifier (Lui and Baldwin, 2012) on it in order to obtain languageclassified seed URLs (Barbaresi, 2013).2 We then looked specifically at the following languages associated as the single dominant language with at least one top-level domain (TLD): 1. Dutch (.nl) 2. French (.fr) 3. Indonesian (.id) 4. Swedish (.se) We randomly sampled 1, 000 seed URLs for each of the 20 permutations of seed sources and languages/TLDs, downloaded them and used texrex to determine the document yield ratio for the documents behind the 1, 000 seeds. The software was configured to perform boilerplate removal, removal of documents based on high Badness scores, perfect duplicate removal, and deletion of documents shorter</context>
</contexts>
<marker>Barbaresi, 2013</marker>
<rawString>Adrien Barbaresi. 2013. Crawling microblogging services to gather language-classified urls. workflow and case study. In 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop, pages 9–15, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="5493" citStr="Baroni et al., 2009" startWordPosition="922" endWordPosition="925">wls based on those URLs. We used the Heritrix (1.14) web crawler (Mohr et al., 2004) and an older version of the texrex web page cleaning toolkit (Schäfer and Bildhauer, 2012). The tools perform, among other things, boilerplate detection and text quality evaluation in the form of the so-called Badness score (Schäfer et al., 2013). A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls. In a preliminary step, we began by collecting seed URLs from various sources: 1This claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. 104 ff). Such an evaluation (in the form of a collocation extraction task) was recently presented for our corpora in work by Stefan Evert (Biemann et al., 2013). 1. the DMOZ directory 2. the Etools meta search engine 3. the FriendFeed social service aggregator 4. the identi.ca social book</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed web-crawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eda Baykan</author>
<author>Monika Henzinger</author>
<author>Ingmar Weber</author>
</authors>
<title>Web page language identification based on URLs.</title>
<date>2008</date>
<booktitle>In Proceedings of the VLDB Endowment,</booktitle>
<pages>176--187</pages>
<contexts>
<context position="2216" citStr="Baykan et al., 2008" startWordPosition="352" endWordPosition="355">urpose of the crawl. To maximize WC, it is vital to guess the weight of the documents behind harvested links before download, such that documents with potentially lesser weight have a lower probability of being downloaded. So-called focused crawlers (in a broad sense) are designed to maximize WC with respect to some specific definition of document weight, for example when documents with a high search-engine relevance (measured as its PageRank or a similar score), documents about specific subjects, or documents in a specific language are desired (Chakrabarti et al., 1999; Menczer et al., 2004; Baykan et al., 2008; Safran et al., 2012). For our purpose, i. e., web corpus crawling, a document with a high weight can simply be defined as one which is not removed from the corpus by the post-processing tools due to low linguistic quality and/or a document which contributes a high amount of text to the corpus. Recently, an interesting approach to crawl optimization along such lines was suggested which relies on statistics about the corpus yield from known hosts (Suchomel and Pomikálek, 2012). Under this approach, the weight (rather of a whole web host) is taken to be the ratio of good documents from the host</context>
<context position="17975" citStr="Baykan et al., 2008" startWordPosition="3047" endWordPosition="3050">The fetcher threads pop URLs from one of the ranked queues, which is selected randomly with prior probabilities inversely proportional to the rank of the queue. Thus, promising URLs are popped more often and less promising ones less often. For guessing the weight, pluggable modules can be used and combined in the FocusedWalker container. Currently, we have the standard UrlSeenFilter, which is based on our own self-scaling Bloom Filter implementation (Bloom, 1970; Almeida et al., 2007), and which prevents any URL from being queued more than once. We have plans for a URL-based language guesser (Baykan et al., 2008) in the form of the LanguagePredictor, and a prioritizer based on the yield from specific hosts as described in Suchomel and Pomikálek (2012) in the form of the HostYieldPrioritizer, which reads statistics directly from the texrex module. The texrex module extracts all hyperlinks from processed documents and tags them with the quality scores described in this paper, such that the QualityPrioritizer module can adjust the expected weight of the document behind each URL. The HeidiX architecture also features an alternative queueing strategy in the form of the RandomWalker, which allows users to o</context>
</contexts>
<marker>Baykan, Henzinger, Weber, 2008</marker>
<rawString>Eda Baykan, Monika Henzinger, and Ingmar Weber. 2008. Web page language identification based on URLs. In Proceedings of the VLDB Endowment, pages 176–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Felix Bildhauer</author>
<author>Stefan Evert</author>
<author>Dirk Goldhahn</author>
<author>Uwe Quasthoff</author>
<author>Roland Schäfer</author>
<author>Johannes Simon</author>
<author>Leonard Swiezinski</author>
<author>Torsten Zesch</author>
</authors>
<title>Scalable construction of high-quality web corpora.</title>
<date>2013</date>
<booktitle>Journal for Language Technology and Computational Linguistics,</booktitle>
<pages>28--2</pages>
<contexts>
<context position="5964" citStr="Biemann et al., 2013" startWordPosition="1002" endWordPosition="1005">cument. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls. In a preliminary step, we began by collecting seed URLs from various sources: 1This claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. 104 ff). Such an evaluation (in the form of a collocation extraction task) was recently presented for our corpora in work by Stefan Evert (Biemann et al., 2013). 1. the DMOZ directory 2. the Etools meta search engine 3. the FriendFeed social service aggregator 4. the identi.ca social bookmarking service 5. Wikipedia dumps We scraped the content behind the URLs and ran a state-of-the-art language identifier (Lui and Baldwin, 2012) on it in order to obtain languageclassified seed URLs (Barbaresi, 2013).2 We then looked specifically at the following languages associated as the single dominant language with at least one top-level domain (TLD): 1. Dutch (.nl) 2. French (.fr) 3. Indonesian (.id) 4. Swedish (.se) We randomly sampled 1, 000 seed URLs for eac</context>
</contexts>
<marker>Biemann, Bildhauer, Evert, Goldhahn, Quasthoff, Schäfer, Simon, Swiezinski, Zesch, 2013</marker>
<rawString>Chris Biemann, Felix Bildhauer, Stefan Evert, Dirk Goldhahn, Uwe Quasthoff, Roland Schäfer, Johannes Simon, Leonard Swiezinski, and Torsten Zesch. 2013. Scalable construction of high-quality web corpora. Journal for Language Technology and Computational Linguistics, 28(2):23–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burton Bloom</author>
</authors>
<title>Space/time trade-offs in hash coding with allowable errors.</title>
<date>1970</date>
<journal>Communications of ACM,</journal>
<volume>13</volume>
<issue>7</issue>
<contexts>
<context position="17821" citStr="Bloom, 1970" startWordPosition="3021" endWordPosition="3022">al software) will talk to a DNS cacher and a politeness manager, which handles caching of Robots Exclusion Information and politeness intervals. The fetcher threads pop URLs from one of the ranked queues, which is selected randomly with prior probabilities inversely proportional to the rank of the queue. Thus, promising URLs are popped more often and less promising ones less often. For guessing the weight, pluggable modules can be used and combined in the FocusedWalker container. Currently, we have the standard UrlSeenFilter, which is based on our own self-scaling Bloom Filter implementation (Bloom, 1970; Almeida et al., 2007), and which prevents any URL from being queued more than once. We have plans for a URL-based language guesser (Baykan et al., 2008) in the form of the LanguagePredictor, and a prioritizer based on the yield from specific hosts as described in Suchomel and Pomikálek (2012) in the form of the HostYieldPrioritizer, which reads statistics directly from the texrex module. The texrex module extracts all hyperlinks from processed documents and tags them with the quality scores described in this paper, such that the QualityPrioritizer module can adjust the expected weight of the</context>
</contexts>
<marker>Bloom, 1970</marker>
<rawString>Burton Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Communications of ACM, 13(7):422–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
<author>Martin van den Berg</author>
<author>Byron Dom</author>
</authors>
<title>Focused crawling: a new approach to topic-specific web resource discovery.</title>
<date>1999</date>
<journal>Computer Networks,</journal>
<pages>31--1623</pages>
<marker>Chakrabarti, van den Berg, Dom, 1999</marker>
<rawString>Soumen Chakrabarti, Martin van den Berg, and Byron Dom. 1999. Focused crawling: a new approach to topic-specific web resource discovery. Computer Networks, 31:1623–1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Comparing two language identification schemes.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Internation conference on Statistical Analysis of Textual Data (JADT</booktitle>
<pages>263--268</pages>
<location>Rome.</location>
<contexts>
<context position="5471" citStr="Grefenstette, 1995" startWordPosition="920" endWordPosition="921">rst Search (BFS) crawls based on those URLs. We used the Heritrix (1.14) web crawler (Mohr et al., 2004) and an older version of the texrex web page cleaning toolkit (Schäfer and Bildhauer, 2012). The tools perform, among other things, boilerplate detection and text quality evaluation in the form of the so-called Badness score (Schäfer et al., 2013). A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls. In a preliminary step, we began by collecting seed URLs from various sources: 1This claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. 104 ff). Such an evaluation (in the form of a collocation extraction task) was recently presented for our corpora in work by Stefan Evert (Biemann et al., 2013). 1. the DMOZ directory 2. the Etools meta search engine 3. the FriendFeed social service aggregator 4. the</context>
</contexts>
<marker>Grefenstette, 1995</marker>
<rawString>Gregory Grefenstette. 1995. Comparing two language identification schemes. In Proceedings of the 3rd Internation conference on Statistical Analysis of Textual Data (JADT 1995), pages 263–268, Rome.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monika R Henzinger</author>
<author>Allan Heydon</author>
<author>Michael Mitzenmacher</author>
<author>Marc Najork</author>
</authors>
<title>On near-uniform URL sampling.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th International World Wide Web conference on Computer Networks: The International Journal of Computer and Telecommunications Networking,</booktitle>
<pages>295--308</pages>
<publisher>North-Holland Publishing Co.</publisher>
<contexts>
<context position="18669" citStr="Henzinger et al., 2000" startWordPosition="3159" endWordPosition="3162">eld from specific hosts as described in Suchomel and Pomikálek (2012) in the form of the HostYieldPrioritizer, which reads statistics directly from the texrex module. The texrex module extracts all hyperlinks from processed documents and tags them with the quality scores described in this paper, such that the QualityPrioritizer module can adjust the expected weight of the document behind each URL. The HeidiX architecture also features an alternative queueing strategy in the form of the RandomWalker, which allows users to obtain uniform random samples from the web based on existing algorithms (Henzinger et al., 2000; Rusmevichientong et al., 2001). Since obtaining such samples is a goal which is mostly orthogonal to the 5Like texrex, it is written entirely in the FreePascal dialect of ObjectPascal (http://freepascal.org/), uses only very few additional C libraries, and will be released under the GPL 3. 13 HostYieldPrioritizer LanguagePredictor QualityPrioritizer FocusedWalker UrlSeenFilter Statistics URL RandomWalker (CLARAx) FetcherPool UrlQueues texrex HTML PolitenessManager SnapshotKeeper DNSCacher Corpus WWW Snapshots Figure 6: HeidiX Crawler Architecture. Grayed modules are done as of March 2014. Th</context>
</contexts>
<marker>Henzinger, Heydon, Mitzenmacher, Najork, 2000</marker>
<rawString>Monika R. Henzinger, Allan Heydon, Michael Mitzenmacher, and Marc Najork. 2000. On near-uniform URL sampling. In Proceedings of the 9th International World Wide Web conference on Computer Networks: The International Journal of Computer and Telecommunications Networking, pages 295– 308. North-Holland Publishing Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An Off-the-shelf Language Identification Tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju,</booktitle>
<location>Republic of</location>
<contexts>
<context position="6237" citStr="Lui and Baldwin, 2012" startWordPosition="1044" endWordPosition="1047">. In a preliminary step, we began by collecting seed URLs from various sources: 1This claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. 104 ff). Such an evaluation (in the form of a collocation extraction task) was recently presented for our corpora in work by Stefan Evert (Biemann et al., 2013). 1. the DMOZ directory 2. the Etools meta search engine 3. the FriendFeed social service aggregator 4. the identi.ca social bookmarking service 5. Wikipedia dumps We scraped the content behind the URLs and ran a state-of-the-art language identifier (Lui and Baldwin, 2012) on it in order to obtain languageclassified seed URLs (Barbaresi, 2013).2 We then looked specifically at the following languages associated as the single dominant language with at least one top-level domain (TLD): 1. Dutch (.nl) 2. French (.fr) 3. Indonesian (.id) 4. Swedish (.se) We randomly sampled 1, 000 seed URLs for each of the 20 permutations of seed sources and languages/TLDs, downloaded them and used texrex to determine the document yield ratio for the documents behind the 1, 000 seeds. The software was configured to perform boilerplate removal, removal of documents based on high Badn</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An Off-the-shelf Language Identification Tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filippo Menczer</author>
<author>Gautam Pant</author>
<author>Padmini Srinivasan</author>
</authors>
<title>Topical web crawlers: Evaluating adaptive algorithms.</title>
<date>2004</date>
<journal>ACM Trans. Internet Technol.,</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="2195" citStr="Menczer et al., 2004" startWordPosition="348" endWordPosition="351">ment relative to the purpose of the crawl. To maximize WC, it is vital to guess the weight of the documents behind harvested links before download, such that documents with potentially lesser weight have a lower probability of being downloaded. So-called focused crawlers (in a broad sense) are designed to maximize WC with respect to some specific definition of document weight, for example when documents with a high search-engine relevance (measured as its PageRank or a similar score), documents about specific subjects, or documents in a specific language are desired (Chakrabarti et al., 1999; Menczer et al., 2004; Baykan et al., 2008; Safran et al., 2012). For our purpose, i. e., web corpus crawling, a document with a high weight can simply be defined as one which is not removed from the corpus by the post-processing tools due to low linguistic quality and/or a document which contributes a high amount of text to the corpus. Recently, an interesting approach to crawl optimization along such lines was suggested which relies on statistics about the corpus yield from known hosts (Suchomel and Pomikálek, 2012). Under this approach, the weight (rather of a whole web host) is taken to be the ratio of good do</context>
</contexts>
<marker>Menczer, Pant, Srinivasan, 2004</marker>
<rawString>Filippo Menczer, Gautam Pant, and Padmini Srinivasan. 2004. Topical web crawlers: Evaluating adaptive algorithms. ACM Trans. Internet Technol., 4(4):378–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon Mohr</author>
<author>Michael Stack</author>
<author>Igor Ranitovic</author>
<author>Dan Avery</author>
<author>Michele Kimpton</author>
</authors>
<title>Introduction to Heritrix, an archival quality web crawler.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Web Archiving Workshop (IWAW’04).</booktitle>
<contexts>
<context position="4957" citStr="Mohr et al., 2004" startWordPosition="832" endWordPosition="835">highly on the definition of what a good document is, i. e., what the goal of the crawl is. We assume, similar to Suchomel and Pomikálek’s approach, that our tools reliably filter out documents that are interesting documents for inclusion a corpus, and that calculating a yield ratio based on the output of those tools is therefore reasonable.1 2 Experiment 1: Seed and Crawl Quality In this experiment, we examine the correlation between the yield ratio of crawler seed URLs and the yield ratio of short Breadth-First Search (BFS) crawls based on those URLs. We used the Heritrix (1.14) web crawler (Mohr et al., 2004) and an older version of the texrex web page cleaning toolkit (Schäfer and Bildhauer, 2012). The tools perform, among other things, boilerplate detection and text quality evaluation in the form of the so-called Badness score (Schäfer et al., 2013). A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of </context>
</contexts>
<marker>Mohr, Stack, Ranitovic, Avery, Kimpton, 2004</marker>
<rawString>Gordon Mohr, Michael Stack, Igor Ranitovic, Dan Avery, and Michele Kimpton. 2004. Introduction to Heritrix, an archival quality web crawler. In Proceedings of the 4th International Web Archiving Workshop (IWAW’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Olston</author>
<author>Marc Najork</author>
</authors>
<title>Web Crawling,</title>
<date>2010</date>
<booktitle>of Foundations and Trends in Information Retrieval. now Publishers,</booktitle>
<volume>4</volume>
<issue>3</issue>
<location>Hanover, MA.</location>
<contexts>
<context position="1408" citStr="Olston and Najork, 2010" startWordPosition="216" endWordPosition="219">id crawling such content in the first place, saving bandwidth, storage, and post-processing costs. In this paper, we show in three experiments that two simple scores are suitable to improve the ratio between corpus size and crawling effort for web corpus construction. The first score is related to overall text quality of the page containing the link, the other one is related to the likelihood that the local block enclosing a link is boilerplate. 1 Crawl Optimization and Yield Ratios Optimizing a crawling strategy consists in maximizing its weighted coverage WC(t) at any time t during a crawl (Olston and Najork, 2010, 29), i. e., the summed weight of the documents downloaded until t, where the weight of each crawled document is calculated as a measure of the usefulness of the document relative to the purpose of the crawl. To maximize WC, it is vital to guess the weight of the documents behind harvested links before download, such that documents with potentially lesser weight have a lower probability of being downloaded. So-called focused crawlers (in a broad sense) are designed to maximize WC with respect to some specific definition of document weight, for example when documents with a high search-engine </context>
</contexts>
<marker>Olston, Najork, 2010</marker>
<rawString>Christopher Olston and Marc Najork. 2010. Web Crawling, volume 4(3) of Foundations and Trends in Information Retrieval. now Publishers, Hanover, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paat Rusmevichientong</author>
<author>David M Pennock</author>
<author>Steve Lawrence</author>
<author>C Lee Giles</author>
</authors>
<title>Methods for sampling pages uniformly from the World Wide Web. In</title>
<date>2001</date>
<booktitle>In AAAI Fall Symposium on Using Uncertainty Within Computation,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="18701" citStr="Rusmevichientong et al., 2001" startWordPosition="3163" endWordPosition="3167">as described in Suchomel and Pomikálek (2012) in the form of the HostYieldPrioritizer, which reads statistics directly from the texrex module. The texrex module extracts all hyperlinks from processed documents and tags them with the quality scores described in this paper, such that the QualityPrioritizer module can adjust the expected weight of the document behind each URL. The HeidiX architecture also features an alternative queueing strategy in the form of the RandomWalker, which allows users to obtain uniform random samples from the web based on existing algorithms (Henzinger et al., 2000; Rusmevichientong et al., 2001). Since obtaining such samples is a goal which is mostly orthogonal to the 5Like texrex, it is written entirely in the FreePascal dialect of ObjectPascal (http://freepascal.org/), uses only very few additional C libraries, and will be released under the GPL 3. 13 HostYieldPrioritizer LanguagePredictor QualityPrioritizer FocusedWalker UrlSeenFilter Statistics URL RandomWalker (CLARAx) FetcherPool UrlQueues texrex HTML PolitenessManager SnapshotKeeper DNSCacher Corpus WWW Snapshots Figure 6: HeidiX Crawler Architecture. Grayed modules are done as of March 2014. The FocusedWalker implements an “e</context>
</contexts>
<marker>Rusmevichientong, Pennock, Lawrence, Giles, 2001</marker>
<rawString>Paat Rusmevichientong, David M. Pennock, Steve Lawrence, and C. Lee Giles. 2001. Methods for sampling pages uniformly from the World Wide Web. In In AAAI Fall Symposium on Using Uncertainty Within Computation, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Safran</author>
<author>A Althagafi</author>
<author>Dunren Che</author>
</authors>
<title>Improving relevance prediction for focused Web crawlers.</title>
<date>2012</date>
<booktitle>In IEEE/ACIS 11th International Conference on Computer and Information Science (ICIS),</booktitle>
<pages>161--166</pages>
<contexts>
<context position="2238" citStr="Safran et al., 2012" startWordPosition="356" endWordPosition="359">To maximize WC, it is vital to guess the weight of the documents behind harvested links before download, such that documents with potentially lesser weight have a lower probability of being downloaded. So-called focused crawlers (in a broad sense) are designed to maximize WC with respect to some specific definition of document weight, for example when documents with a high search-engine relevance (measured as its PageRank or a similar score), documents about specific subjects, or documents in a specific language are desired (Chakrabarti et al., 1999; Menczer et al., 2004; Baykan et al., 2008; Safran et al., 2012). For our purpose, i. e., web corpus crawling, a document with a high weight can simply be defined as one which is not removed from the corpus by the post-processing tools due to low linguistic quality and/or a document which contributes a high amount of text to the corpus. Recently, an interesting approach to crawl optimization along such lines was suggested which relies on statistics about the corpus yield from known hosts (Suchomel and Pomikálek, 2012). Under this approach, the weight (rather of a whole web host) is taken to be the ratio of good documents from the host remaining in the corp</context>
</contexts>
<marker>Safran, Althagafi, Che, 2012</marker>
<rawString>M.S. Safran, A. Althagafi, and Dunren Che. 2012. Improving relevance prediction for focused Web crawlers. In IEEE/ACIS 11th International Conference on Computer and Information Science (ICIS), 2012, pages 161–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Schäfer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<pages>486--493</pages>
<editor>In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul. ELRA.</location>
<contexts>
<context position="5048" citStr="Schäfer and Bildhauer, 2012" startWordPosition="848" endWordPosition="851">crawl is. We assume, similar to Suchomel and Pomikálek’s approach, that our tools reliably filter out documents that are interesting documents for inclusion a corpus, and that calculating a yield ratio based on the output of those tools is therefore reasonable.1 2 Experiment 1: Seed and Crawl Quality In this experiment, we examine the correlation between the yield ratio of crawler seed URLs and the yield ratio of short Breadth-First Search (BFS) crawls based on those URLs. We used the Heritrix (1.14) web crawler (Mohr et al., 2004) and an older version of the texrex web page cleaning toolkit (Schäfer and Bildhauer, 2012). The tools perform, among other things, boilerplate detection and text quality evaluation in the form of the so-called Badness score (Schäfer et al., 2013). A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls. In a preliminary step, we began</context>
<context position="10580" citStr="Schäfer and Bildhauer, 2012" startWordPosition="1787" endWordPosition="1791">shots s1..s500: Yd = 0.141, for snapshots s501..s1000: Yd = 0.071. The vertical bar marks the point at which the seeds were exhausted. (Schäfer and Bildhauer, 2013, p. 31) crawl was halted, the crawler frontier was emptied, and the crawl was then re-started with a selection of the URLs harvested in the previous phase. Only those URLs were used which came from documents with a Badness score of 10 or lower (= documents in which the distribution of the most frequent function words fits the expected distribution for Flemish very well, cf. Section 2), and from text blocks with a boilerplate score (Schäfer and Bildhauer, 2012) in [0.5, 1] (= likely not boilerplate). Additionally, it was made sure that no URLs were re-used between the five phases. The very promising results are plotted in Figure 3. Figure 3: Yield ratio over crawl time with cyclic URL selection in the .be TLD. The x axis shows the crawl progression in snapshots of 400 MB of raw crawled data (= one Heritrix ARC file). The y axis shows the yield ratio for each snapshot. The five phases are clearly distinguishable by the sudden increases in yield ratio. 0 500 1000 1500 2000 11 phase adjusted R2 p (ANOVA) 1 0.8288 &lt; 0.001 2 0.9187 &lt; 0.001 3 0.8308 &lt; 0.0</context>
<context position="12412" citStr="Schäfer and Bildhauer, 2012" startWordPosition="2119" endWordPosition="2123"> ratio to above 0.2 for a short period. The subsequent sharp decline shows that link selection/prioritization should rather be implemented in the crawler frontier management in order to achieve a constant effect over longer crawls (cf. Section 5). 4 Experiment 3: Internal Crawl Analysis For the last experiment, we used the most recent version of the texrex toolkit, which writes full link structures for the processed documents as a by-product.3 An internal analysis of a small portion of a crawled data set from the German TLD was performed, which is part of the raw material of the DECOW corpus (Schäfer and Bildhauer, 2012). The data set contains 11, 557,695 crawled HTML documents and 81, 255, 876 http links extracted from the crawled documents (only &lt;a&gt; tags). Among the link URLs in the sample, 711,092 are actually links to documents in the sample, so we could analyze exactly those 711, 092 links. It should be noticed that we only looked at links to different hosts, such that hostinternal links (navigation to “Home”, etc.) are not included in the analysis. In this experiment, we were interested specifically in the many documents which we usually discard right away simply because they are either very short (belo</context>
</contexts>
<marker>Schäfer, Bildhauer, 2012</marker>
<rawString>Roland Schäfer and Felix Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), pages 486–493, Istanbul. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Schäfer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Web Corpus Construction. Synthesis Lectures on Human Language Technologies. Morgan and Claypool,</title>
<date>2013</date>
<location>San Francisco.</location>
<contexts>
<context position="5799" citStr="Schäfer and Bildhauer, 2013" startWordPosition="972" endWordPosition="975">ess score (Schäfer et al., 2013). A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls. In a preliminary step, we began by collecting seed URLs from various sources: 1This claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. 104 ff). Such an evaluation (in the form of a collocation extraction task) was recently presented for our corpora in work by Stefan Evert (Biemann et al., 2013). 1. the DMOZ directory 2. the Etools meta search engine 3. the FriendFeed social service aggregator 4. the identi.ca social bookmarking service 5. Wikipedia dumps We scraped the content behind the URLs and ran a state-of-the-art language identifier (Lui and Baldwin, 2012) on it in order to obtain languageclassified seed URLs (Barbaresi, 2013).2 We then looked specifically at the following languages associated as the single dominan</context>
<context position="8700" citStr="Schäfer and Bildhauer, 2013" startWordPosition="1466" endWordPosition="1469">ter.) Linear model: Intercept = −0.0098, Coefficient = 0.6332, R2 = 0.7831 (adjusted), p &lt; 0.001 (ANOVA). 3 Experiment 2: Crawling with Cyclic URL Selection Using the same configuration of tools as in Section 2, we performed a crawl targeting Flemish documents in the Belgian . be national TLD, which hosts both Flemish and French documents in substantial proportions. Usually, even under more favorable conditions (i. e., when we crawl a TLD which contains mostly documents in the target language), the yield ratio of a BFS crawl decreases rapidly in the initial phase, then staying at a low level (Schäfer and Bildhauer, 2013, p. 31). Figure 2 illustrates this with an analysis of a . de BFS crawl from late 2011, also processed with the same tools as mentioned in Section 2. Notice that the . de domain hosts German documents almost exclusively. The interesting complication in this experiment is thus the non-target language present in the TLD scope of the crawler and the related question whether, simply speaking, predominantly Flemish documents link to other predominantly Flemish documents rather than French documents. Since the Badness score (calculated as described in Section 2) includes a form of language identifi</context>
<context position="10115" citStr="Schäfer and Bildhauer, 2013" startWordPosition="1705" endWordPosition="1708">ollowing manner: The crawl progressed in five phases. In the first short burnin phase, we crawled 1, 000, 000 documents, and in each of the second to fifth phase, we crawled 10, 000, 000 documents. After each phase, the Figure 2: Yield ratio (y axis) over time for a BFS crawl in .de in November/December 2011 started with 231,484 seed URLs scraped from Bing. The yield ratio was calculated at 1, 000 snapshots of 400 MB of data (= one Heritrix ARC file). For snapshots s1..s500: Yd = 0.141, for snapshots s501..s1000: Yd = 0.071. The vertical bar marks the point at which the seeds were exhausted. (Schäfer and Bildhauer, 2013, p. 31) crawl was halted, the crawler frontier was emptied, and the crawl was then re-started with a selection of the URLs harvested in the previous phase. Only those URLs were used which came from documents with a Badness score of 10 or lower (= documents in which the distribution of the most frequent function words fits the expected distribution for Flemish very well, cf. Section 2), and from text blocks with a boilerplate score (Schäfer and Bildhauer, 2012) in [0.5, 1] (= likely not boilerplate). Additionally, it was made sure that no URLs were re-used between the five phases. The very pro</context>
</contexts>
<marker>Schäfer, Bildhauer, 2013</marker>
<rawString>Roland Schäfer and Felix Bildhauer. 2013. Web Corpus Construction. Synthesis Lectures on Human Language Technologies. Morgan and Claypool, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Schäfer</author>
<author>Adrien Barbaresi</author>
<author>Felix Bildhauer</author>
</authors>
<title>The good, the bad, and the hazy: Design decisions in web corpus construction.</title>
<date>2013</date>
<booktitle>Proceedings of the 8th Web as Corpus Workshop (WAC-8),</booktitle>
<pages>7--15</pages>
<editor>In Stefan Evert, Egon Stemle, and Paul Rayson, editors,</editor>
<publisher>SIGWAC.</publisher>
<location>Lancaster.</location>
<contexts>
<context position="5204" citStr="Schäfer et al., 2013" startWordPosition="873" endWordPosition="876">us, and that calculating a yield ratio based on the output of those tools is therefore reasonable.1 2 Experiment 1: Seed and Crawl Quality In this experiment, we examine the correlation between the yield ratio of crawler seed URLs and the yield ratio of short Breadth-First Search (BFS) crawls based on those URLs. We used the Heritrix (1.14) web crawler (Mohr et al., 2004) and an older version of the texrex web page cleaning toolkit (Schäfer and Bildhauer, 2012). The tools perform, among other things, boilerplate detection and text quality evaluation in the form of the so-called Badness score (Schäfer et al., 2013). A document receives a low Badness score if the most frequent function words of the target language have a high enough frequency in the document. The Badness score is based on previous ideas from language identification and web document filtering (Grefenstette, 1995; Baroni et al., 2009). Originally, this experiment was carried out in the context of an evaluation of sources of different seed URLs for crawls. In a preliminary step, we began by collecting seed URLs from various sources: 1This claim should be backed up by forms of extrinsic/task-based evaluation (Schäfer and Bildhauer, 2013, p. </context>
</contexts>
<marker>Schäfer, Barbaresi, Bildhauer, 2013</marker>
<rawString>Roland Schäfer, Adrien Barbaresi, and Felix Bildhauer. 2013. The good, the bad, and the hazy: Design decisions in web corpus construction. In Stefan Evert, Egon Stemle, and Paul Rayson, editors, Proceedings of the 8th Web as Corpus Workshop (WAC-8), pages 7–15, Lancaster. SIGWAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vít Suchomel</author>
<author>Jan Pomikálek</author>
</authors>
<title>Effcient Web crawling for large text corpora.</title>
<date>2012</date>
<booktitle>In Adam Kilgarriff and Serge Sharoff, editors, Proceedings of the seventh Web as Corpus Workshop,</booktitle>
<pages>40--44</pages>
<contexts>
<context position="2697" citStr="Suchomel and Pomikálek, 2012" startWordPosition="435" endWordPosition="438">uments about specific subjects, or documents in a specific language are desired (Chakrabarti et al., 1999; Menczer et al., 2004; Baykan et al., 2008; Safran et al., 2012). For our purpose, i. e., web corpus crawling, a document with a high weight can simply be defined as one which is not removed from the corpus by the post-processing tools due to low linguistic quality and/or a document which contributes a high amount of text to the corpus. Recently, an interesting approach to crawl optimization along such lines was suggested which relies on statistics about the corpus yield from known hosts (Suchomel and Pomikálek, 2012). Under this approach, the weight (rather of a whole web host) is taken to be the ratio of good documents from the host remaining in the corpus after a specific post-processing chain has been applied to the documents. Harvested URLs pointing to certain hosts are prioritized accordingly. We follow a similar route like Suchomel and Pomikálek, but look at documentlocal features instead of host statistics. Throughout this paper, we refer to the yield ratio instead of WC, although they are related notions. We define the yield ratio Yd for a set Dc of crawled unprocessed documents and a set Dr of re</context>
<context position="18116" citStr="Suchomel and Pomikálek (2012)" startWordPosition="3070" endWordPosition="3073">al to the rank of the queue. Thus, promising URLs are popped more often and less promising ones less often. For guessing the weight, pluggable modules can be used and combined in the FocusedWalker container. Currently, we have the standard UrlSeenFilter, which is based on our own self-scaling Bloom Filter implementation (Bloom, 1970; Almeida et al., 2007), and which prevents any URL from being queued more than once. We have plans for a URL-based language guesser (Baykan et al., 2008) in the form of the LanguagePredictor, and a prioritizer based on the yield from specific hosts as described in Suchomel and Pomikálek (2012) in the form of the HostYieldPrioritizer, which reads statistics directly from the texrex module. The texrex module extracts all hyperlinks from processed documents and tags them with the quality scores described in this paper, such that the QualityPrioritizer module can adjust the expected weight of the document behind each URL. The HeidiX architecture also features an alternative queueing strategy in the form of the RandomWalker, which allows users to obtain uniform random samples from the web based on existing algorithms (Henzinger et al., 2000; Rusmevichientong et al., 2001). Since obtaini</context>
</contexts>
<marker>Suchomel, Pomikálek, 2012</marker>
<rawString>Vít Suchomel and Jan Pomikálek. 2012. Effcient Web crawling for large text corpora. In Adam Kilgarriff and Serge Sharoff, editors, Proceedings of the seventh Web as Corpus Workshop, pages 40–44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
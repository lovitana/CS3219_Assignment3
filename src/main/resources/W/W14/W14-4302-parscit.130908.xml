<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005458">
<title confidence="0.9889455">
Crowdsourcing Street-level Geographic Information Using a
Spoken Dialogue System
</title>
<author confidence="0.995066">
Raveesh Meena Johan Boye Gabriel Skantze Joakim Gustafson
</author>
<affiliation confidence="0.99941">
KTH Royal Institute of Technology
School of Computer Science and Communication
</affiliation>
<address confidence="0.860962">
Stockholm, Sweden
</address>
<email confidence="0.996089">
{raveesh, jboye}@csc.kth.se, {gabriel, jocke}@speech.kth.se
</email>
<sectionHeader confidence="0.995626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992875">
We present a technique for crowd-
sourcing street-level geographic infor-
mation using spoken natural language. In
particular, we are interested in obtaining
first-person-view information about what
can be seen from different positions in
the city. This information can then for
example be used for pedestrian routing
services. The approach has been tested in
the lab using a fully implemented spoken
dialogue system, and has shown promis-
ing results.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970877192983">
Crowdsourcing is increasingly being used in
speech processing for tasks such as speech data
acquisition, transcription/labeling, and assess-
ment of speech technology, e.g. spoken dialogue
systems (Parent &amp; Eskenazi, 2011). However,
we are not aware of any attempts where a dia-
logue system is the vehicle for crowdsourcing
rather than the object of study, that is, where a
spoken dialogue system is used to collect infor-
mation from a large body of users. A task where
such crowdsourcing dialogue systems would be
useful is to populate geographic databases. While
there are now open databases with geographic
information, such as OpenStreetMap (Haklay &amp;
Weber, 2008), these are typically intended for
map drawing, and therefore lack detailed street-
level information about city landmarks, such as
colors and height of buildings, ornamentations,
facade materials, balconies, conspicuous signs,
etc. Such information could for example be very
useful for pedestrian navigation (Tom &amp; Denis,
2003; Ross et al., 2004). With the current grow-
ing usage of smartphones, we might envisage a
community of users using their phones to con-
tribute information to geographic databases, an-
notating cities to a great level of detail, using
multi-modal method including speech. The key
reason for using speech for map annotation is
convenience; it is easy to talk into a mobile
phone while walking down the street, so a user
with a little experience will not be slowed down
by the activity of interacting with a database.
This way, useful information could be obtained
that is really hard to add offline, sitting in front
of one’s PC using a map interface, things like:
Can you see X from this point? Is there a big
sign over the entrance of the restaurant? What
color is the building on your right?
Another advantage of using a spoken dialogue
system is that the users could be asked to freely
describe objects they consider important in their
current view. In this way, the system could learn
new objects not anticipated by the system de-
signers, and their associated properties.
In this paper we present a proof-of-concept
study of how a spoken dialogue system could be
used to enrich geographic databases by
crowdsourcing. To our knowledge, this is the
first attempt at using spoken dialogue systems
for crowdsourcing in this way. In Section 2, we
elaborate on the need of spoken dialogue systems
for crowdsourcing geographic information. In
Section 3 we describe the dialogue system im-
plementation. Section 4 presents our in-lab
crowdsourcing experiment. We present an analy-
sis of crowd-sourced data in Section 5, and dis-
cuss directions for future work in Section 6.
</bodyText>
<sectionHeader confidence="0.842267" genericHeader="method">
2 The pedestrian routing domain
</sectionHeader>
<bodyText confidence="0.972013">
Routing systems have been around quite some
time for car navigation, but systems for pedestri-
</bodyText>
<page confidence="0.944144">
2
</page>
<note confidence="0.725771">
Proceedings of the SIGDIAL 2014 Conference, pages 2–11,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999961786666667">
an routing are relatively new and are still in their
nascent stage (Bartie &amp; Mackaness, 2006; Krug
et al., 2003; Janarthanam et al., 2012; Boye et al.,
2014). In the case of pedestrian navigation, it is
preferable for way-finding systems to base their
instructions on landmarks, by which we under-
stand distinctive objects in the city environment.
Studies have shown that the inclusion of land-
marks into system-generated instructions for a
pedestrian raises the user’s confidence in the sys-
tem, compared to only left-right instructions
(Tom &amp; Denis, 2003; Ross et al., 2004).
Basing routing instructions on landmarks
means that the routing system would, for exam-
ple, generate an instruction “Go towards the red
brick building” (where, in this case, “the red
brick building” is the landmark), rather than
“Turn slightly left here” or “Go north 200 me-
ters”. This strategy for providing instructions
places certain requirements on the geographic
database: It has to include many landmarks and
many details about them as well, so that the sys-
tem can generate clear and un-ambiguous in-
structions. However, the information contained
in current databases is still both sparse and
coarse-grained in many cases.
Our starting point is a pedestrian routing sys-
tem we designed and implemented, using the
landmark-based approach to instruction-giving
(Boye et al., 2014). The system performs visibil-
ity calculations whenever the pedestrian ap-
proaches a waypoint, in order to compute the set
of landmarks that are visible for the user from his
current position. OpenStreetMap (Haklay &amp; We-
ber, 2008) is used as the data source. Figure 1
shows a typical situation in pedestrian routing
session. The blue dot indicates the user’s position
and the blue arrow her direction. Figure 2 shows
the same situation in a first-person perspective.
The system can now compute the set of visible
landmarks, such as buildings and traffic lights,
along with distances and angles to those land-
marks. The angle to a building is given as an in-
terval in degrees relative to the direction of the
user (e.g. 90° left to 30° left). This is exemplified
in Figure 1, where four different buildings are in
view (with field of view marked with numbers
1–4). Landmarks that are not buildings are con-
sidered to be a single point, and hence the rela-
tive angle can be given as a single number.
When comparing the map with the street view
picture, it becomes obvious that the “SEB” bank
office is very hard to see and probably not very
suitable to use as a landmark in route descrip-
tions. On the other hand, the database does not
contain the fact that the building has six stories
and a façade made of yellow bricks, something
that would be easily recognizable for the pedes-
trian. This is not due to any shortcoming of the
OpenStreetMap database; it just goes to show
that the database has been constructed with map
drawing in mind, rather than pedestrian routing.
There are also some other notable omissions in
the database; e.g. the shop on the corner, visible
right in front of the user, is not present in the da-
tabase. Since OpenStreetMap is crowd-sourced,
there is no guarantee as to which information
will be present in the database, and which will
not. This also highlights the limitation of existing
approaches to crowd-sourcing geographic infor-
mation: Some useful information is difficult to
add off-line, using a map interface on a PC. On
the other hand, it would be a straightforward
matter given the kind of crowd-sourcing spoken
dialogue system we present next.
</bodyText>
<figureCaption confidence="0.987536">
Figure 1: A pedestrian routing scenario
Figure 2: The visual scene corresponding to the
pedestrian routing scenario in Figure 1
</figureCaption>
<sectionHeader confidence="0.968208" genericHeader="method">
3 A dialogue system for crowd-sourcing
</sectionHeader>
<bodyText confidence="0.999936333333333">
To verify the potential of the ideas discussed
above, we implemented a spoken dialogue sys-
tem that can engage in spoken conversation with
</bodyText>
<page confidence="0.997066">
3
</page>
<bodyText confidence="0.999954819148937">
users and learn details about landmarks in visual
scenes (such as Figure 2). To identify the kind of
details in a visual scene that the system could
potentially ask the users, we first conducted a
preliminary informal crowd-sourcing dialogue:
one person (the receiver), was instructed to seek
information that could be useful for pedestrian
navigation from the other person (the giver).
The receiver only had access to information
available in the maps from OpenStreetMap, as in
Figure 1, but without any marking of field of
views, whereas the giver only had access to the
corresponding visual scene (as in Figure 2). In-
teraction data from eight such dialogues (from
four participants, and four different visual
scenes) suggested that in a city environment,
buildings are prominent landmarks and much of
the interaction involves their properties such as
color, number of stories, color of roof, signs or
ornamentations on buildings, whether it has
shops, etc. Seeking further details on mentioned
signs, shops, and entities (whether mapped or
unmapped) proved to be a useful strategy to ob-
tain information. We also noted that asking for
open-ended questions, such as “Is there anything
else in this scene that I should be aware of?”
towards the end has the potential of revealing
unknown landmarks and details in the map.
Obtaining specific details about known objects
from the user corresponds to slot-filling in a dia-
logue system, where the dialogue system seeks a
value for a certain slot (= attribute). By engaging
in an open-ended interaction the system could
also obtain general details to identify new slot-
value pairs. Although slots could be in some cas-
es be multi-valued (e.g., a building could have
both color red and yellow), we have here made
the simplifying assumption that they are single
valued. Since users may not always be able to
specify values for slots we treat no-value as a
valid slot-value for all type of slots.
We also wanted the system to automatically
learn the most reliable values for the slots, over
several interactions. As the system interacts with
new users, it is likely that the system will obtain
a range of values for certain slots. The variability
of the answers could appear for various reasons:
users may have differences in perception about
slot-values such as colors, some users might
misunderstand what building is being talked
about, and errors in speech recognition might
result in the wrong slot values. Some of these
values may therefore be in agreement with those
given by other users, while some may differ
slightly or be in complete contradiction. Thus the
system should be able to keep a record of all the
various slot-values obtained (including the dis-
puted ones), identify slot-values that need to be
clarified, and engage in a dialogue with users for
clarification.
In view of these requirements, we have de-
signed our crowd-sourcing dialogue system to be
able to (1) take and retain initiative during the
interactions for slot-filling, (2) behave as a re-
sponsive listener when engaging in open-ended
dialogue, and (3) ask wh– and yes–no questions
for seeking and clarifying slot-values, respective-
ly. Thus when performing the slot-filling task,
the system mainly asks questions, acknowledges,
or clarifies the concepts learned for the slot-
values. Apart from requesting repetitions, the
user cannot ask any questions or by other means
take the initiative. A summary of all the attrib-
utes and corresponding system prompts is pre-
sented in Appendix A.
The top half of Figure 3 illustrates the key
components of the dialogue system. The Dia-
logue Manager queries the Scene Manager (SM)
for slots to be filled or slot-values to be clarified,
engages in dialogue with users to learn/clarify
slot-values, and informs the SM about the values
obtained for these slots. The SM manages a list
of scenes and the predefined slots – for each type
of landmark in visual scenes – that need to be
filled, maintains a record of slot-values obtained
from all the users, and identifies slot-values with
majority vote as the current reliable slot-value.
To achieve these objectives, the scene manager
uses an XML representation of visual scenes. In
this representation, landmarks (e.g., buildings,
junctions, etc.) – automatically acquired through
the OpenStreetMap database and the visibility
computations mentioned in Section 2 – are
stored as scene-objects (cf. Figure 4).
</bodyText>
<figureCaption confidence="0.997655">
Figure 3: Dialogue system architecture
</figureCaption>
<bodyText confidence="0.99698">
The Dialogue Manager (DM) uses scene-
object attributes, such as type, angle or interval
of a building, to generate referential expressions,
such as “Do you see a building on the far left?”
</bodyText>
<page confidence="0.98981">
4
</page>
<bodyText confidence="0.999931">
or “Do you see a shop on the left?” to draw the
users’ attention to the intended landmark in the
scene. During the course of interaction, the Sce-
ne Manager (SM) extends scene-objects with a
set of predefined attributes (= slots) that we iden-
tified in the preliminary study, along with their
various slot-values (cf. Figure 5). For each slot,
the SM keeps a record of slot-values obtained
through wh– questions as well as the ones dis-
puted by the users in yes–no questions (cf. ob-
tained and disputed tags in the XML), and
uses their tally to identify the slot-value in major-
ity. The system assumes this slot-value (or one of
them in case of a tie) as its best estimate of a
slot-value pair, which it could clarify with anoth-
er user using a yes–no query. During the slot-
filling mode the DM switches to open-ended in-
teraction mode to seek general details (using
prompts such as “Could you describe it/them?”),
if the user suggests/agrees that there are signs
on/at a scene-object, or a building has shops or
restaurants. Once all the slots for all the scene-
objects in a visual scene have been queried, the
DM once again switches to the open-ended inter-
action mode and queries the users whether there
are any other relevant signs or landmarks that the
system may have missed and should be aware of.
On completion of the open-ended queries the SM
selects the next visual scene, and the DM engag-
es in a new dialogue.
</bodyText>
<figure confidence="0.950515954545454">
&lt;scene xmlns=&amp;quot;cityCS.scene&amp;quot; name=&amp;quot; view7.jpg&amp;quot; lat=&amp;quot;59.34501&amp;quot;
lon=&amp;quot;18.0614&amp;quot; fovl=&amp;quot;-60&amp;quot; fovr=&amp;quot;60&amp;quot; bearing=&amp;quot;320&amp;quot; dist=&amp;quot;100&amp;quot;&gt;
&lt;scene-object&gt;
&lt;id&gt;35274588&lt;/id&gt; &lt;type&gt;building&lt;/type&gt;
&lt;from&gt;-60&lt;/from&gt; &lt;end&gt;-39&lt;/end&gt;
&lt;/scene-object&gt;
&lt;scene-object&gt;
&lt;id&gt;538907080&lt;/id&gt; &lt;type&gt;shop&lt;/type&gt;
&lt;distance&gt;34.82&lt;/distance&gt;
&lt;angle&gt;-39&lt;/angle&gt; &lt;bearing&gt;281&lt;/bearing&gt;
&lt;/scene-object&gt;
&lt;scene-object&gt;
&lt;id&gt;280604&lt;/id&gt; &lt;type&gt;building&lt;/type&gt;
&lt;from&gt;-38&lt;/from&gt; &lt;end&gt;6&lt;/end&gt;
&lt;/scene-object&gt;
&lt;scene-object&gt;
&lt;id&gt;193906&lt;/id&gt; &lt;type&gt;traffic_signals&lt;/type&gt;
&lt;distance&gt;40.77&lt;/distance&gt;
&lt;angle&gt;-14&lt;/angle&gt; &lt;bearing&gt;306&lt;/bearing&gt;
&lt;/scene-object&gt;
...
&lt;/scene&gt;
</figure>
<figureCaption confidence="0.999941">
Figure 4: XML representation of visual scenes
</figureCaption>
<bodyText confidence="0.999965428571429">
For speech recognition and semantic interpre-
tation the system uses a context-free grammar
with semantic tags (SRGS1), tailored for the do-
main. The output of semantic interpretation is a
concept. If the concept type matches the type of
the slot, the dialogue manager informs the scene
manager about the obtained slot-value. If the
</bodyText>
<footnote confidence="0.604576">
1 http://www.w3.org/TR/speech-grammar/
</footnote>
<bodyText confidence="0.996644">
concept type is inappropriate the DM queries the
user once more (albeit using different utterance
forms). If still no appropriate concept is learned
the DM requests the SM for the next slot and
proceeds with the dialogue. For speech synthesis,
we use the CereVoice system developed by
CereProc2. The dialogue system has been imple-
mented using the IrisTK framework (Skantze &amp;
Al Moubayed, 2012).
</bodyText>
<figure confidence="0.999266235294118">
&lt;scene-object&gt;
&lt;id&gt;35274588&lt;/id&gt; &lt;type&gt;building&lt;/type&gt;
&lt;from&gt;-60&lt;/from&gt; &lt;end&gt;-39&lt;/end&gt;
&lt;slot slotName=&amp;quot;VISIBLE&amp;quot;&gt;... &lt;/slot&gt;
&lt;slot slotName=&amp;quot;COLOR&amp;quot;&gt;
&lt;obtained&gt;
&lt;value slotValue=&amp;quot;Green&amp;quot;&gt;
&lt;userlist&gt;
&lt;usrDtls uid=&amp;quot;u01&amp;quot; asrCnf=&amp;quot;0.06&amp;quot; qType=&amp;quot;WH&amp;quot;/&gt;
&lt;/userlist&gt;
&lt;/value&gt;
&lt;value slotValue=&amp;quot;no-value&amp;quot;&gt;
&lt;userlist&gt;
&lt;usrDtls uid=&amp;quot;u02&amp;quot; asrCnf=&amp;quot;0.46&amp;quot; qType =&amp;quot;WH&amp;quot;/&gt;
&lt;/userlist&gt;
&lt;/value&gt;
&lt;value slotValue=&amp;quot;Gray&amp;quot;&gt;
&lt;userlist&gt;
&lt;usrDtls uid=&amp;quot;u03&amp;quot; asrCnf=&amp;quot;0.19&amp;quot; qType =&amp;quot;WH&amp;quot;/&gt;
&lt;/userlist&gt;
&lt;/value&gt;
&lt;/obtained&gt;
&lt;disputed&gt;
&lt;value slotValue=&amp;quot;Green&amp;quot;&gt;
&lt;userlist&gt;
&lt;usrDtls uid=&amp;quot;u02&amp;quot; asrCnf=&amp;quot;0.92&amp;quot; qType =&amp;quot;YN&amp;quot;/&gt;
&lt;/userlist&gt;
&lt;/value&gt;
&lt;/disputed&gt;
&lt;/slot&gt;
&lt;slot slotName=&amp;quot;STORIES&amp;quot;&gt;... &lt;/slot&gt;
&lt;slot slotName=&amp;quot;ROOF_COLOR&amp;quot;&gt;... &lt;/slot&gt;
...
&lt;/scene-object&gt;
</figure>
<figureCaption confidence="0.999962">
Figure 5: Every slot-value is recorded
</figureCaption>
<bodyText confidence="0.999973619047619">
In contrast to the slot-filling mode, when en-
gaging in an open-ended interaction, the system
leaves the initiative to the user and behaves as a
responsive listener. That is, the system only pro-
duces feedback responses, such as backchannels
(e.g., okay, mh-hmm, uh-huh), repetition requests
for longer speaker turns (e.g., could you repeat
that?), or continuation prompts such as “any-
thing else?” until the user is finished speaking.
Unless the system recognized an explicit closing
statement from the user (e.g., “I can’t”), the sys-
tem encourages the user to continue the descrip-
tions for 2 to 4 turns (chosen randomly).
To detect appropriate locations in users’
speech where the system should give feedback
response, the system uses a trained data-driven
model (Meena et al., 2013). When the voice ac-
tivity detector detects a silence of 200 ms in us-
ers’ speech, the model uses prosodic, contextual
and lexico-syntactic features from the preceding
speech segment to decide whether the system
</bodyText>
<footnote confidence="0.963786">
2 https://www.cereproc.com/
</footnote>
<page confidence="0.989911">
5
</page>
<bodyText confidence="0.999993857142857">
should produce a feedback response. The lower
half of Figure 3 shows the additional components
of the dialogue system used in open-ended inter-
action mode. In this mode, the ASR system uses
a language model that is trained on interactions
from a related domain (verbal route descrip-
tions), in parallel to the SRGS grammar.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="method">
4 In-lab crowd-sourcing experiment
</sectionHeader>
<bodyText confidence="0.9999662">
Nine visual scenes (wide-angle pictures in first-
person perspective and taken in Stockholm city,
cf. Figure 2) were used for the task of
crowdsourcing. Fifteen human participants (4
females and 11 males) participated in the
crowdsourcing exercise. All participants either
studied or worked at the School of Computer
Science and Communication, KTH, Stockholm.
Participants were placed in front of a computer
display and were told that the system will engage
them in a spoken conversation to seek or clarify
details about landmarks and other objects in vis-
ual scenes. They were told that the details would
be used for pedestrian routing and therefore they
are free to choose and specify details (in open-
ended questions) that they thought would be use-
ful when giving route instructions to another per-
son.
Each participant did the nine visual scenes in
the same order, with a 1 minute pause between
each of them. The first visual scene was used as
a trial in order to familiarize participants with the
interaction scenario. For this reason, the trial in-
teraction was specifically designed to engage the
participants in both wh– and yes–no type ques-
tions. We also wanted to investigate whether the
use of wh– or yes–no questions alone has any
impact on the interactions. For the remaining
eight interactions, the dialogue system therefore
alternatively used wh– and yes–no questions
throughout the whole dialogue. The order of que-
ry types was switched for each successive partic-
ipant. Thus visual scenes presented with wh–
queries to a participant pi were presented to par-
ticipant pi+1 with only yes–no queries, and vice-
versa. After each interaction participants were
asked (1) whether the system appeared to be fa-
miliar with the visual scene; and (2) whether the
interactions went smoothly.
An example of a conversation with wh–type
queries is presented in Table 1. The second col-
umn indicates the various types of utterances
(dialogue acts) used by the system during this
interaction. Note that during the slot-filling
mode, the system has the initiative, whereas in
the open-ended queries (cf. rows 15–22 in Table
1) the system gives the initiative to the partici-
pant and only produces feedback responses. Ta-
ble 2 presents a conversation with yes–no queries
following the conversation in Table 1. In a con-
versation with only yes–no queries, a wh– query
may be presented if a slot has been observed
with no-value (cf. rows 9–10 in Table 1) or when
the participant disputes the system’s current best
estimate (cf. rows 6–9 in Table 2).
</bodyText>
<figure confidence="0.992787298507463">
S Introduction Could you help me with some de-
tails about this view?
P Yes
S wh– query Do you see a building on the far
left?
P Yes
P I don’t know [I do no]
P There is a bike painted on street
just in front of me
[ is the right pink university in front
of may ]
S Backchannel
S wh– query
Mh-hmm
What color does the building have?
P Cream color [Green color]
S Reprise
S wh– query
Green
How many floors is this building?
...
S wh– query
Are there any important signs in
this scene that I may have missed?
S
Uh-huh
Backchannel
S wh– query
Is there anything peculiar in this
scene that I should be familiar with
P No
S
Introduction
Could you help me with some details
about this view?
S
yes–no
query
Is the building on the far left visible to
you?
S
Mh-hmm
Backchannel
S
yes–no
query
Does this building have green color?
S
Uh-huh
Backchannel
S wh– query
But than what color is the building?
S
Anything else?
Backchannel
S
Mh-hmm
Backchannel
S
Backchannel
S
Okay, thanks
Ending
P Yeah its white paint [Yes light pink]
P That’s it [that that]
P I am done [okay]
Ehm
</figure>
<figureCaption confidence="0.349011">
Table 1: An example interaction with wh– que-
ries for the visual scene in Figure 2. S: system, P:
participant (here, u01 from Figure 5). ASR re-
sults are shown within square brackets.
</figureCaption>
<figure confidence="0.696269318181818">
P Yes
P Yes
P No
6
P white [light]
S Repeat Sorry, what color did you say?
P white [light]
S wh– query How many stories is this building?
P I don’t know [I do rental]
S Backchannel Ehm
...
S wh– query Are there any important signs in this
scene that I may have missed?
P Ehm no [nine no]
S Backchannel Ehm
S yes–no I am informed that there are other
query important landmarks or structures in
this scene that I should be aware of,
is it true?
P No
S Backchannel uh-huh
S Ending Okay, thanks
</figure>
<figureCaption confidence="0.8837064">
Table 2: An example interaction with yes–no
queries corresponding to the visual scene in Fig-
ure 2. S: system, P: participant (here u02 from
Figure 5). ASR results are shown within square
brackets.
</figureCaption>
<sectionHeader confidence="0.866109" genericHeader="method">
5 Data analysis
</sectionHeader>
<bodyText confidence="0.9999094">
We analyzed the data (15 x 8 interactions) col-
lected from the experiment along the following
tracks: first, we compare the majority value of
the slots to the ground truth as given by a human
annotator; second, we explore how the ground
truth of slot-values could be estimated automati-
cally; third, we also analyzed the instances where
the participants disputed the system’s current
estimate of slot-values; and fourth, we examined
the post-experimental questionnaires.
</bodyText>
<subsectionHeader confidence="0.998562">
5.1 Rate of learning slot-values
</subsectionHeader>
<bodyText confidence="0.999930714285714">
A total of 197 slots were learned in the exper-
iment. We analyzed how many slot-values had
been correctly retrieved after 1, 2... 15 users. In
Figure 6, the curve “Majority” illustrates the
fraction of slot-values correctly learned with
each new user, under the assumption that the
slot-values with majority votes – from all the 15
users – constitute the ground truth. Thus after
interacting with the first user the system had ob-
tained 67.0% of slot-values correctly (according
to the majority) and 96.4% of slot-values after
interacting with the first six users. Another eight
users, or fourteen in total, were required to learn
all the slot-values correctly. The progression
curve thus provides an estimate of how many
users are required to achieve a specific percent-
age of slot-values correctly if majority is to be
considered the ground truth. The curve “Not-in-
Majority” indicates the number of slot with val-
ues that were not in the majority. Thus after in-
teracting with the first user 20.8% of slot-values
the system had obtained were not in majority and
could be treated as incorrect. Note that the curves
Majority and Not-in-Majority do not sum up to
100%, this is because we consider no-value as a
valid slot-value, and treat the slot as unfilled. For
example, 12.2% of the slots remained unfilled
after interacting with the first user.
</bodyText>
<figureCaption confidence="0.991694">
Figure 6: Rate of learning slot-values with two differ-
ent estimates of ground truth
</figureCaption>
<bodyText confidence="0.999994870967742">
We also investigated how close the majority is
to the actual truth. A human annotator (one of the
coauthors) labeled all the obtained slot-values as
either sensible or insensible, based on the com-
bined knowledge from the corresponding maps,
the visual scenes, and the set of obtained values.
Thus a slot could have many sensible values. For
example, various parts of a building could be
painted in different colors. The progression
curves “Sensible” and “Insensible” in Figure 6
illustrate the fraction of total slots for which the
learned values were actually correct and incor-
rect, respectively. While the curve for sensible
values follows the same pattern as the progres-
sion curve for majority as the estimate of ground
truth, the percent of slot-values that were actually
correct is always lower than the majority as
ground truth, and it never reached 100%. The
constant gap between the two curves suggests
that some slot-values learned by the majority
were not actually the ground truth. What led the
majority into giving incorrect slot-values is left
as a topic for future work.
As mentioned earlier, much of the slot-filling
interaction involved buildings and their proper-
ties. Figure 7 illustrates that sensible values for
most slots, pertaining to whether a building is
visible, whether it is residential, whether it has
shops, and the color of roof were obtained by
interacting with only few participants. In con-
trast, properties such as color of the building and
</bodyText>
<page confidence="0.998846">
7
</page>
<bodyText confidence="0.99959893939394">
number of stories required many more partici-
pants. This could be attributed to the fact that
participants may have differences in perception
about slot-values. As regards to whether there are
signs on buildings, we observed that the recall is
relatively low. This is largely due to lack of
common ground among participants about what
could be considered a sign. Our intentions with
designing this prompt was to retrieve any peculi-
ar detail on the building that is easy to locate: for
us a sign suggesting a name of restaurant is as
useful as the knowledge that the building has
blue sunshade on the windows. Some partici-
pants understood this while other didn’t.
sive: A building might of course have more than
one color, and in many cases more than one color
name might be appropriate even though the
building has only one dominating color (e.g. to
describe the color either as “brown” and “red”
might be acceptable to most people). Figure 8
shows the incremental estimates for different
colors for a certain building (OpenStreetMap ID
163966736) after 1, 2... 15 subjects had been
asked. The answer from the first subject was er-
roneously recognized as “pink”. The next 9 sub-
jects all referred to the building as “brown”.
Among the final subjects, 3 subjects referred to
building as “red”, and 2 subjects as “brown”. The
final truth estimates are 0.98 for “brown”, 0.002
for “red”, and 0.00005 for “pink”. The diagram
shows that if the certainty threshold is set to 0.8,
the value “brown” would have been established
already after 4 subjects.
</bodyText>
<figureCaption confidence="0.941539">
Figure 7: Learning rate of various slots for land-
mark type building
</figureCaption>
<subsectionHeader confidence="0.986777">
5.2 Estimated ground truth of slot-values
</subsectionHeader>
<bodyText confidence="0.9999186">
The 15 subjects in the in-lab experiment were all
asked for the same information. In a real applica-
tion, however, we want the system to only ask
for slots for which it has insufficient or conflict-
ing information. If the ground truth of a certain
slot-value pair can be estimated with a certainty
exceeding some threshold (given the quality re-
quirements of the database, say 0.8), the system
can consider the matter settled, and need not ask
about that slot again. We therefore want to esti-
mate the ground truth of slot-values along with a
certainty measure. To this end, we use the
CityCrowdSource Trust software package
(Dickens &amp; Lupu, 2014), which is based on the
probabilistic approach for supervised learning
when we have multiple annotators providing la-
bels (possibly noisy) but no absolute gold stand-
ard, presented in Raykar et al. (2009).
Using this approach, a question concerning the
color of a building, say with ID 24, (e.g. “What
color is the building?”) would be translated into
several binary predicates COLOR_Red(24),
COLOR_Brown(24), COLOR_Orange(24), etc.
The justification for this binary encoding is that
the different color values are not mutually exclu-
</bodyText>
<figureCaption confidence="0.980383">
Figure 8: Probabilities of different estimated ground
truth values for the color of a certain building
</figureCaption>
<subsectionHeader confidence="0.898008">
5.3 Disputed slot-values
</subsectionHeader>
<bodyText confidence="0.999964857142857">
We also examined all system questions of
yes–no type that received negative answers, i.e.
instances where the participants disputed the sys-
tem’s current best estimate (based on majority
vote) of a slot-value. Among the 95 such in-
stances, the system’s current best estimate was
actually insensible only on 43 occasions. In 30 of
these instances the participants provided a recti-
fied slot-value that was sensible. For the remain-
ing 13 instances the new slot-values proposed by
the participant were actually insensible. There
were 52 instances of false disputations, i.e. the
system’s current estimate of a slot-value was
sensible, but the participants disputed it. 6 of the-
se occurrences were due to errors in speech
recognition, but for the remaining 46 occasions,
error in grounding the intended landmark (15),
users’ perception of slot-values (3), and ambigui-
ty in what the annotator terms as sensible slot-
values (28), (e.g. whether there are signs on a
building (as discussed in Section 5.1)) were iden-
</bodyText>
<page confidence="0.991745">
8
</page>
<bodyText confidence="0.999940666666667">
tified as the main reasons. This suggests that
slots (i.e. attributes) that are often disputed may
not be easily understood by users.
</bodyText>
<subsectionHeader confidence="0.958037">
5.4 Post-experimental questionnaire
</subsectionHeader>
<bodyText confidence="0.999921818181818">
As described above, the participants filled in a
questionnaire after each interaction. They were
asked to rate the system’s familiarity with the
visual scene based on the questions asked. A
Mann–Whitney U test suggests that participants’
perception of the system’s familiarity with the
visual scene was significantly higher for interac-
tions with yes–no queries than interactions with
wh– queries (U=1769.5, p= 0.007). This result
has implications for the design choice for sys-
tems that provide as well as ask for information
from users. For example, a pedestrian routing
system can already be used to offer routing in-
structions as well as crowdsourcing information.
The system is more likely to give an impression
of familiarity with the surrounding, to the user,
by asking yes–no type questions than wh–
questions. This may influence a user’s confi-
dence or trust in using the routing system.
Since yes–no questions expect a “yes” or
“no” in response, we therefore hypothesized that
interactions with yes–no questions would be per-
ceived smoother in comparison to interactions
with wh– questions. However, a Mann–Whitney
U test suggests that the participants perceived no
significant difference between the two interac-
tion types (U=1529.0, p= 0.248). Feedback
comments from participants suggest that abrupt
ending of open-ended interactions by the system
(due to the simplistic model of detecting whether
the user has anything more to say) gave users an
impression that the system is not allowing them
to speak.
</bodyText>
<sectionHeader confidence="0.996773" genericHeader="method">
6 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999989102040816">
We have presented a proof-of-concept study on
using a spoken dialogue system for crowd-
sourcing street-level geographic information. To
our knowledge, this is the first attempt at using
spoken dialogue systems for crowdsourcing in
this way. The system is fully automatic, in the
sense that it (i) starts with minimal details – ob-
tained from OpenStreetMap – about a visual sce-
ne, (ii) prompts users with wh– questions to ob-
tain values for a predefined set of attributes; and
(iii) assumes attribute-values with majority vote
as its beliefs, and engages in yes–no questions
with new participants to confirm them. In a data
collection experiment, we have observed that
after interacting with only 6 human participants
the system acquires more than 80% of the slots
with actually sensible values.
We have also shown that the majority vote (as
perceived by the system) could also be incorrect.
To mitigate this, we have explored the use of the
CityCrowdSource Trust software package
(Dickens &amp; Lupu, 2014) for obtaining the proba-
bilistic estimate of the ground truth of slot-values
in a real crowd-sourcing system. However, it is
important not only to consider the ground truth
probabilities per se, but also on how many con-
tributing users the estimate is based and the qual-
ity of information obtained. We will explore the-
se two issues in future work.
We have observed that through open-ended
prompts, the system could potentially collect a
large amount of details about the visual scenes.
Since we did not use any automatic interpretation
of these answers, we transcribed key concepts in
participants’ speech in order to obtain an esti-
mate of this. However, it is not obvious how to
quantify the number of concepts. For example,
we have learned that in Figure 2, at the junction
ahead, there is: a traffic-sign, a speed-limit sign,
a sign with yellow color, a sign with red color, a
sign with red boarder, a sign that is round, a sign
with some text, the text says 50. These are details
obtained in pieces from various participants.
Looking at Figure 2 one can see that these pieces
when put together refer to the speed-limit sign
mounted on the traffic-signal at the junction.
How to assimilate these pieces together into a
unified concept is a task that we have left for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.965657" genericHeader="method">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999748666666667">
We would like to thank the participants of the in-
lab crowd-sourcing experiment. This work is
supported by the EIT KIC project
“CityCrowdSource”, and the Swedish research
council (VR) project Incremental processing in
multimodal conversational systems (2011-6237).
</bodyText>
<sectionHeader confidence="0.994872" genericHeader="method">
Reference
</sectionHeader>
<reference confidence="0.999782125">
Bartie, P. J., &amp; Mackaness, W. A. (2006). Develop-
ment of a Speech-Based Augmented Reality Sys-
tem to Support Exploration of Cityscape. Transac-
tions in GIS, 10(1), 63-86.
Boye, J., Fredriksson, M., Götze, J., Gustafson, J., &amp;
Königsmann, J. (2014). Walk This Way: Spatial
Grounding for City Exploration. In Mariani, J.,
Rosset, S., Garnier-Rizet, M., &amp; Devillers, L.
</reference>
<page confidence="0.947085">
9
</page>
<reference confidence="0.998548083333333">
(Eds.), Natural Interaction with Robots, Knowbots
and Smartphones (pp. 59-67). Springer New York.
Dickens, L., &amp; Lupu, E. (2014). Trust service final
deliverable report. Technical Report, Imperial Col-
lege, UK.
Haklay, M., &amp; Weber, P. (2008). OpenStreetMap:
User-Generated Street Maps. IEEE Pervasive
Computing, 7(4), 12-18.
Janarthanam, S., Lemon, O., Liu, X., Bartie, P.,
Mackaness, W., Dalmas, T., &amp; Goetze, J. (2012).
Integrating Location, Visibility, and Question-
Answering in a Spoken Dialogue System for Pe-
destrian City Exploration. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (pp. 134-136). Seoul,
South Korea: Association for Computational Lin-
guistics.
Krug, K., Mountain, D., &amp; Phan, D. (2003). Webpark:
Location-based services for mobile users in pro-
tected areas.. GeoInformatics, 26-29.
Parent, G., &amp; Eskenazi, M. (2011). Speaking to the
Crowd: Looking at Past Achievements in Using
Crowdsourcing for Speech and Predicting Future
Challenges. In INTERSPEECH (pp. 3037-3040).
ISCA.
Raykar, V. C., Yu, S., Zhao, L. H., Jerebko, A., Flor-
in, C., Valadez, G. H., Bogoni, L., &amp; Moy, L.
(2009). Supervised Learning from Multiple Ex-
perts: Whom to Trust when Everyone Lies a Bit. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning (pp. 889-896). New
York, NY, USA: ACM.
Ross, T., May, A., &amp; Thompson, S. (2004). The Use
of Landmarks in Pedestrian Navigation Instructions
and the Effects of Context. In Brewster, S., &amp; Dun-
lop, M. (Eds.), Mobile Human-Computer Interac-
tion - MobileHCI 2004 (pp. 300-304). Springer
Berlin Heidelberg.
Skantze, G., &amp; Al Moubayed, S. (2012). IrisTK: a
statechart-based toolkit for multi-party face-to-face
interaction. In Proceedings of ICMI. Santa Monica,
CA.
Tom, A., &amp; Denis, M. (2003). Referring to Landmark
or Street Information in Route Directions: What
Difference Does It Make?. In Kuhn, W., Worboys,
M., &amp; Timpf, S. (Eds.), Spatial Information Theo-
ry. Foundations of Geographic Information Sci-
ence (pp. 362-374). Springer Berlin Heidelberg.
</reference>
<page confidence="0.99931">
10
</page>
<sectionHeader confidence="0.99037" genericHeader="method">
Appendix A
</sectionHeader>
<bodyText confidence="0.999317823529412">
The table below lists slots (= landmark attributes) and the corresponding wh– and yes–no system questions. For
attributes marked with * the dialogue manager switches to open-ended interaction mode.
Slot (=attribute ) System wh– questions System yes–no questions
Visible: whether a particular  Do you see a building on the far left?  Is the building on the far right visible to
landmark is visible from this  Do you see another building in front of you?
view. you?  I think there is another building in front of
 Is there a junction on the right? you, do you see it?
 Do you see a traffic-signal ahead?  Can you see the junction on the right?
 Are you able to see the traffic-signal
ahead?
Color of the building  What color does the building have?  I think this building is red in color, what do
 What color is the building? you think?
 Does this building have red color?
Size of the building (in num-  How many floors do you think are  I think there are six floors in this building,
ber of stories) there in this building what do you think?
 How many stories is this building  Is this building six storied?
Color of the building’s roof  What color does the roof of this build-  I think the roof of this building is orange in
ing have? color, what do you think? Do you think that the roof of this building
 What color is the roof of this building? is orange?
Signs or ornamentation on the  Do you see any signs or decorations  I think there is a sign or some decoration
building on this building? on this building, do you see it?
 There may be a sign or a name on this
building, do you see it?
Shops or restaurants in the  Are there any shops or restaurants in  I am informed that there are some shops or
building this building? restaurants in this building, is it true?
 I think there are some shops or restaurants
in this building, what do you think?
Signs at landmarks  Are there any important signs at the  I believe there is a sign at this junc-
junction/crossing? tion/crossing, do you see it?
 Do you see the sign at this junc-
tion/crossing?
*Description of sign  Could you describe this sign?  Could you describe this sign?
 What does this sign look like?  What does this sign look like?
 Does the sign say something?  Does the sign say something?
*Signs in the visual scene  Are there any important signs in this  There are some important signs in this
scene that I may have missed? scene that could be useful for my
 Have I missed any relevant signs in knowledge, am I right?
this scene?  I am informed that there are some signs in
this scene that are relevant for me, is it
true?
*Landmarks in the visual sce-  Are there any other important build-  I am informed that there are some im-
ne ings or relevant structures in this scene portant landmarks or structures in this sce-
that I should be aware of? ne that I should be aware of, is it true?
 Is there anything particular in this  I have been told that there are some other
scene that I should be familiar with? things in this scene that I are relevant for
 Have I missed any relevant buildings me, is it true?
or landmarks in this scene?  I believe I have missed some relevant
landmarks in this scene, am I right?
*Description of unknown  Could you describe it?  Could you describe it?
landmarks e.g. shop, restau-  Could you describe them?  Could you describe them?
rant, building, etc.  How do they look like?  How do they look like?
</bodyText>
<page confidence="0.995033">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788699">
<title confidence="0.9987585">Crowdsourcing Street-level Geographic Information Using a Spoken Dialogue System</title>
<author confidence="0.996344">Raveesh Meena Johan Boye Gabriel Skantze Joakim Gustafson</author>
<affiliation confidence="0.9760765">KTH Royal Institute of Technology School of Computer Science and Communication</affiliation>
<address confidence="0.859921">Stockholm, Sweden</address>
<email confidence="0.989521">raveesh@speech.kth.se</email>
<email confidence="0.989521">jboye}@csc.kth.se@speech.kth.se</email>
<email confidence="0.989521">{gabriel@speech.kth.se</email>
<email confidence="0.989521">jocke@speech.kth.se</email>
<abstract confidence="0.998034692307692">We present a technique for crowdsourcing street-level geographic information using spoken natural language. In particular, we are interested in obtaining first-person-view information about what can be seen from different positions in the city. This information can then for example be used for pedestrian routing services. The approach has been tested in the lab using a fully implemented spoken dialogue system, and has shown promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P J Bartie</author>
<author>W A Mackaness</author>
</authors>
<title>Development of a Speech-Based Augmented Reality System to Support Exploration of Cityscape.</title>
<date>2006</date>
<journal>Transactions in GIS,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>63--86</pages>
<contexts>
<context position="3778" citStr="Bartie &amp; Mackaness, 2006" startWordPosition="587" endWordPosition="590">ourcing geographic information. In Section 3 we describe the dialogue system implementation. Section 4 presents our in-lab crowdsourcing experiment. We present an analysis of crowd-sourced data in Section 5, and discuss directions for future work in Section 6. 2 The pedestrian routing domain Routing systems have been around quite some time for car navigation, but systems for pedestri2 Proceedings of the SIGDIAL 2014 Conference, pages 2–11, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics an routing are relatively new and are still in their nascent stage (Bartie &amp; Mackaness, 2006; Krug et al., 2003; Janarthanam et al., 2012; Boye et al., 2014). In the case of pedestrian navigation, it is preferable for way-finding systems to base their instructions on landmarks, by which we understand distinctive objects in the city environment. Studies have shown that the inclusion of landmarks into system-generated instructions for a pedestrian raises the user’s confidence in the system, compared to only left-right instructions (Tom &amp; Denis, 2003; Ross et al., 2004). Basing routing instructions on landmarks means that the routing system would, for example, generate an instruction “G</context>
</contexts>
<marker>Bartie, Mackaness, 2006</marker>
<rawString>Bartie, P. J., &amp; Mackaness, W. A. (2006). Development of a Speech-Based Augmented Reality System to Support Exploration of Cityscape. Transactions in GIS, 10(1), 63-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boye</author>
<author>M Fredriksson</author>
<author>J Götze</author>
<author>J Gustafson</author>
<author>J Königsmann</author>
</authors>
<title>Walk This Way: Spatial Grounding for City Exploration. In</title>
<date>2014</date>
<booktitle>Natural Interaction with Robots, Knowbots and Smartphones</booktitle>
<pages>59--67</pages>
<publisher>Springer</publisher>
<location>New York.</location>
<contexts>
<context position="3843" citStr="Boye et al., 2014" startWordPosition="599" endWordPosition="602">ystem implementation. Section 4 presents our in-lab crowdsourcing experiment. We present an analysis of crowd-sourced data in Section 5, and discuss directions for future work in Section 6. 2 The pedestrian routing domain Routing systems have been around quite some time for car navigation, but systems for pedestri2 Proceedings of the SIGDIAL 2014 Conference, pages 2–11, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics an routing are relatively new and are still in their nascent stage (Bartie &amp; Mackaness, 2006; Krug et al., 2003; Janarthanam et al., 2012; Boye et al., 2014). In the case of pedestrian navigation, it is preferable for way-finding systems to base their instructions on landmarks, by which we understand distinctive objects in the city environment. Studies have shown that the inclusion of landmarks into system-generated instructions for a pedestrian raises the user’s confidence in the system, compared to only left-right instructions (Tom &amp; Denis, 2003; Ross et al., 2004). Basing routing instructions on landmarks means that the routing system would, for example, generate an instruction “Go towards the red brick building” (where, in this case, “the red </context>
</contexts>
<marker>Boye, Fredriksson, Götze, Gustafson, Königsmann, 2014</marker>
<rawString>Boye, J., Fredriksson, M., Götze, J., Gustafson, J., &amp; Königsmann, J. (2014). Walk This Way: Spatial Grounding for City Exploration. In Mariani, J., Rosset, S., Garnier-Rizet, M., &amp; Devillers, L. (Eds.), Natural Interaction with Robots, Knowbots and Smartphones (pp. 59-67). Springer New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dickens</author>
<author>E Lupu</author>
</authors>
<title>Trust service final deliverable report.</title>
<date>2014</date>
<tech>Technical Report,</tech>
<location>Imperial College, UK.</location>
<contexts>
<context position="27314" citStr="Dickens &amp; Lupu, 2014" startWordPosition="4418" endWordPosition="4421"> in-lab experiment were all asked for the same information. In a real application, however, we want the system to only ask for slots for which it has insufficient or conflicting information. If the ground truth of a certain slot-value pair can be estimated with a certainty exceeding some threshold (given the quality requirements of the database, say 0.8), the system can consider the matter settled, and need not ask about that slot again. We therefore want to estimate the ground truth of slot-values along with a certainty measure. To this end, we use the CityCrowdSource Trust software package (Dickens &amp; Lupu, 2014), which is based on the probabilistic approach for supervised learning when we have multiple annotators providing labels (possibly noisy) but no absolute gold standard, presented in Raykar et al. (2009). Using this approach, a question concerning the color of a building, say with ID 24, (e.g. “What color is the building?”) would be translated into several binary predicates COLOR_Red(24), COLOR_Brown(24), COLOR_Orange(24), etc. The justification for this binary encoding is that the different color values are not mutually excluFigure 8: Probabilities of different estimated ground truth values fo</context>
<context position="31688" citStr="Dickens &amp; Lupu, 2014" startWordPosition="5108" endWordPosition="5111">(ii) prompts users with wh– questions to obtain values for a predefined set of attributes; and (iii) assumes attribute-values with majority vote as its beliefs, and engages in yes–no questions with new participants to confirm them. In a data collection experiment, we have observed that after interacting with only 6 human participants the system acquires more than 80% of the slots with actually sensible values. We have also shown that the majority vote (as perceived by the system) could also be incorrect. To mitigate this, we have explored the use of the CityCrowdSource Trust software package (Dickens &amp; Lupu, 2014) for obtaining the probabilistic estimate of the ground truth of slot-values in a real crowd-sourcing system. However, it is important not only to consider the ground truth probabilities per se, but also on how many contributing users the estimate is based and the quality of information obtained. We will explore these two issues in future work. We have observed that through open-ended prompts, the system could potentially collect a large amount of details about the visual scenes. Since we did not use any automatic interpretation of these answers, we transcribed key concepts in participants’ sp</context>
</contexts>
<marker>Dickens, Lupu, 2014</marker>
<rawString>Dickens, L., &amp; Lupu, E. (2014). Trust service final deliverable report. Technical Report, Imperial College, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Haklay</author>
<author>P Weber</author>
</authors>
<title>OpenStreetMap: User-Generated Street Maps.</title>
<date>2008</date>
<journal>IEEE Pervasive Computing,</journal>
<volume>7</volume>
<issue>4</issue>
<pages>12--18</pages>
<contexts>
<context position="1430" citStr="Haklay &amp; Weber, 2008" startWordPosition="205" endWordPosition="208">n speech processing for tasks such as speech data acquisition, transcription/labeling, and assessment of speech technology, e.g. spoken dialogue systems (Parent &amp; Eskenazi, 2011). However, we are not aware of any attempts where a dialogue system is the vehicle for crowdsourcing rather than the object of study, that is, where a spoken dialogue system is used to collect information from a large body of users. A task where such crowdsourcing dialogue systems would be useful is to populate geographic databases. While there are now open databases with geographic information, such as OpenStreetMap (Haklay &amp; Weber, 2008), these are typically intended for map drawing, and therefore lack detailed streetlevel information about city landmarks, such as colors and height of buildings, ornamentations, facade materials, balconies, conspicuous signs, etc. Such information could for example be very useful for pedestrian navigation (Tom &amp; Denis, 2003; Ross et al., 2004). With the current growing usage of smartphones, we might envisage a community of users using their phones to contribute information to geographic databases, annotating cities to a great level of detail, using multi-modal method including speech. The key </context>
<context position="5269" citStr="Haklay &amp; Weber, 2008" startWordPosition="822" endWordPosition="826">include many landmarks and many details about them as well, so that the system can generate clear and un-ambiguous instructions. However, the information contained in current databases is still both sparse and coarse-grained in many cases. Our starting point is a pedestrian routing system we designed and implemented, using the landmark-based approach to instruction-giving (Boye et al., 2014). The system performs visibility calculations whenever the pedestrian approaches a waypoint, in order to compute the set of landmarks that are visible for the user from his current position. OpenStreetMap (Haklay &amp; Weber, 2008) is used as the data source. Figure 1 shows a typical situation in pedestrian routing session. The blue dot indicates the user’s position and the blue arrow her direction. Figure 2 shows the same situation in a first-person perspective. The system can now compute the set of visible landmarks, such as buildings and traffic lights, along with distances and angles to those landmarks. The angle to a building is given as an interval in degrees relative to the direction of the user (e.g. 90° left to 30° left). This is exemplified in Figure 1, where four different buildings are in view (with field of</context>
</contexts>
<marker>Haklay, Weber, 2008</marker>
<rawString>Haklay, M., &amp; Weber, P. (2008). OpenStreetMap: User-Generated Street Maps. IEEE Pervasive Computing, 7(4), 12-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
<author>X Liu</author>
<author>P Bartie</author>
<author>W Mackaness</author>
<author>T Dalmas</author>
<author>J Goetze</author>
</authors>
<title>Integrating Location, Visibility, and QuestionAnswering in a Spoken Dialogue System for Pedestrian City Exploration.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</booktitle>
<pages>134--136</pages>
<institution>Seoul, South Korea: Association for Computational Linguistics.</institution>
<contexts>
<context position="3823" citStr="Janarthanam et al., 2012" startWordPosition="595" endWordPosition="598">we describe the dialogue system implementation. Section 4 presents our in-lab crowdsourcing experiment. We present an analysis of crowd-sourced data in Section 5, and discuss directions for future work in Section 6. 2 The pedestrian routing domain Routing systems have been around quite some time for car navigation, but systems for pedestri2 Proceedings of the SIGDIAL 2014 Conference, pages 2–11, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics an routing are relatively new and are still in their nascent stage (Bartie &amp; Mackaness, 2006; Krug et al., 2003; Janarthanam et al., 2012; Boye et al., 2014). In the case of pedestrian navigation, it is preferable for way-finding systems to base their instructions on landmarks, by which we understand distinctive objects in the city environment. Studies have shown that the inclusion of landmarks into system-generated instructions for a pedestrian raises the user’s confidence in the system, compared to only left-right instructions (Tom &amp; Denis, 2003; Ross et al., 2004). Basing routing instructions on landmarks means that the routing system would, for example, generate an instruction “Go towards the red brick building” (where, in </context>
</contexts>
<marker>Janarthanam, Lemon, Liu, Bartie, Mackaness, Dalmas, Goetze, 2012</marker>
<rawString>Janarthanam, S., Lemon, O., Liu, X., Bartie, P., Mackaness, W., Dalmas, T., &amp; Goetze, J. (2012). Integrating Location, Visibility, and QuestionAnswering in a Spoken Dialogue System for Pedestrian City Exploration. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (pp. 134-136). Seoul, South Korea: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krug</author>
<author>D Mountain</author>
<author>D Phan</author>
</authors>
<title>Webpark: Location-based services for mobile users in protected areas..</title>
<date>2003</date>
<journal>GeoInformatics,</journal>
<pages>26--29</pages>
<contexts>
<context position="3797" citStr="Krug et al., 2003" startWordPosition="591" endWordPosition="594">tion. In Section 3 we describe the dialogue system implementation. Section 4 presents our in-lab crowdsourcing experiment. We present an analysis of crowd-sourced data in Section 5, and discuss directions for future work in Section 6. 2 The pedestrian routing domain Routing systems have been around quite some time for car navigation, but systems for pedestri2 Proceedings of the SIGDIAL 2014 Conference, pages 2–11, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics an routing are relatively new and are still in their nascent stage (Bartie &amp; Mackaness, 2006; Krug et al., 2003; Janarthanam et al., 2012; Boye et al., 2014). In the case of pedestrian navigation, it is preferable for way-finding systems to base their instructions on landmarks, by which we understand distinctive objects in the city environment. Studies have shown that the inclusion of landmarks into system-generated instructions for a pedestrian raises the user’s confidence in the system, compared to only left-right instructions (Tom &amp; Denis, 2003; Ross et al., 2004). Basing routing instructions on landmarks means that the routing system would, for example, generate an instruction “Go towards the red b</context>
</contexts>
<marker>Krug, Mountain, Phan, 2003</marker>
<rawString>Krug, K., Mountain, D., &amp; Phan, D. (2003). Webpark: Location-based services for mobile users in protected areas.. GeoInformatics, 26-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Parent</author>
<author>M Eskenazi</author>
</authors>
<title>Speaking to the Crowd: Looking at Past Achievements in Using Crowdsourcing for Speech and Predicting Future Challenges.</title>
<date>2011</date>
<booktitle>In INTERSPEECH</booktitle>
<pages>3037--3040</pages>
<contexts>
<context position="987" citStr="Parent &amp; Eskenazi, 2011" startWordPosition="131" endWordPosition="134">raphic information using spoken natural language. In particular, we are interested in obtaining first-person-view information about what can be seen from different positions in the city. This information can then for example be used for pedestrian routing services. The approach has been tested in the lab using a fully implemented spoken dialogue system, and has shown promising results. 1 Introduction Crowdsourcing is increasingly being used in speech processing for tasks such as speech data acquisition, transcription/labeling, and assessment of speech technology, e.g. spoken dialogue systems (Parent &amp; Eskenazi, 2011). However, we are not aware of any attempts where a dialogue system is the vehicle for crowdsourcing rather than the object of study, that is, where a spoken dialogue system is used to collect information from a large body of users. A task where such crowdsourcing dialogue systems would be useful is to populate geographic databases. While there are now open databases with geographic information, such as OpenStreetMap (Haklay &amp; Weber, 2008), these are typically intended for map drawing, and therefore lack detailed streetlevel information about city landmarks, such as colors and height of buildi</context>
</contexts>
<marker>Parent, Eskenazi, 2011</marker>
<rawString>Parent, G., &amp; Eskenazi, M. (2011). Speaking to the Crowd: Looking at Past Achievements in Using Crowdsourcing for Speech and Predicting Future Challenges. In INTERSPEECH (pp. 3037-3040). ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V C Raykar</author>
<author>S Yu</author>
<author>L H Zhao</author>
<author>A Jerebko</author>
<author>C Florin</author>
<author>G H Valadez</author>
<author>L Bogoni</author>
<author>L Moy</author>
</authors>
<title>Supervised Learning from Multiple Experts: Whom to Trust when Everyone Lies a Bit.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning</booktitle>
<pages>889--896</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA:</location>
<contexts>
<context position="27516" citStr="Raykar et al. (2009)" startWordPosition="4450" endWordPosition="4453">d truth of a certain slot-value pair can be estimated with a certainty exceeding some threshold (given the quality requirements of the database, say 0.8), the system can consider the matter settled, and need not ask about that slot again. We therefore want to estimate the ground truth of slot-values along with a certainty measure. To this end, we use the CityCrowdSource Trust software package (Dickens &amp; Lupu, 2014), which is based on the probabilistic approach for supervised learning when we have multiple annotators providing labels (possibly noisy) but no absolute gold standard, presented in Raykar et al. (2009). Using this approach, a question concerning the color of a building, say with ID 24, (e.g. “What color is the building?”) would be translated into several binary predicates COLOR_Red(24), COLOR_Brown(24), COLOR_Orange(24), etc. The justification for this binary encoding is that the different color values are not mutually excluFigure 8: Probabilities of different estimated ground truth values for the color of a certain building 5.3 Disputed slot-values We also examined all system questions of yes–no type that received negative answers, i.e. instances where the participants disputed the system’</context>
</contexts>
<marker>Raykar, Yu, Zhao, Jerebko, Florin, Valadez, Bogoni, Moy, 2009</marker>
<rawString>Raykar, V. C., Yu, S., Zhao, L. H., Jerebko, A., Florin, C., Valadez, G. H., Bogoni, L., &amp; Moy, L. (2009). Supervised Learning from Multiple Experts: Whom to Trust when Everyone Lies a Bit. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 889-896). New York, NY, USA: ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ross</author>
<author>A May</author>
<author>S Thompson</author>
</authors>
<title>The Use of Landmarks in Pedestrian Navigation Instructions and the Effects of Context. In</title>
<date>2004</date>
<pages>300--304</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="1775" citStr="Ross et al., 2004" startWordPosition="255" endWordPosition="258">tem is used to collect information from a large body of users. A task where such crowdsourcing dialogue systems would be useful is to populate geographic databases. While there are now open databases with geographic information, such as OpenStreetMap (Haklay &amp; Weber, 2008), these are typically intended for map drawing, and therefore lack detailed streetlevel information about city landmarks, such as colors and height of buildings, ornamentations, facade materials, balconies, conspicuous signs, etc. Such information could for example be very useful for pedestrian navigation (Tom &amp; Denis, 2003; Ross et al., 2004). With the current growing usage of smartphones, we might envisage a community of users using their phones to contribute information to geographic databases, annotating cities to a great level of detail, using multi-modal method including speech. The key reason for using speech for map annotation is convenience; it is easy to talk into a mobile phone while walking down the street, so a user with a little experience will not be slowed down by the activity of interacting with a database. This way, useful information could be obtained that is really hard to add offline, sitting in front of one’s </context>
<context position="4259" citStr="Ross et al., 2004" startWordPosition="664" endWordPosition="667">14 Association for Computational Linguistics an routing are relatively new and are still in their nascent stage (Bartie &amp; Mackaness, 2006; Krug et al., 2003; Janarthanam et al., 2012; Boye et al., 2014). In the case of pedestrian navigation, it is preferable for way-finding systems to base their instructions on landmarks, by which we understand distinctive objects in the city environment. Studies have shown that the inclusion of landmarks into system-generated instructions for a pedestrian raises the user’s confidence in the system, compared to only left-right instructions (Tom &amp; Denis, 2003; Ross et al., 2004). Basing routing instructions on landmarks means that the routing system would, for example, generate an instruction “Go towards the red brick building” (where, in this case, “the red brick building” is the landmark), rather than “Turn slightly left here” or “Go north 200 meters”. This strategy for providing instructions places certain requirements on the geographic database: It has to include many landmarks and many details about them as well, so that the system can generate clear and un-ambiguous instructions. However, the information contained in current databases is still both sparse and c</context>
</contexts>
<marker>Ross, May, Thompson, 2004</marker>
<rawString>Ross, T., May, A., &amp; Thompson, S. (2004). The Use of Landmarks in Pedestrian Navigation Instructions and the Effects of Context. In Brewster, S., &amp; Dunlop, M. (Eds.), Mobile Human-Computer Interaction - MobileHCI 2004 (pp. 300-304). Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>Al Moubayed</author>
<author>S</author>
</authors>
<title>IrisTK: a statechart-based toolkit for multi-party face-to-face interaction.</title>
<date>2012</date>
<booktitle>In Proceedings of ICMI.</booktitle>
<location>Santa Monica, CA.</location>
<marker>Skantze, Moubayed, S, 2012</marker>
<rawString>Skantze, G., &amp; Al Moubayed, S. (2012). IrisTK: a statechart-based toolkit for multi-party face-to-face interaction. In Proceedings of ICMI. Santa Monica, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tom</author>
<author>M Denis</author>
</authors>
<title>Referring to Landmark or Street Information in Route Directions: What Difference Does It Make?. In</title>
<date>2003</date>
<journal>Foundations of Geographic Information Science</journal>
<pages>362--374</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="1755" citStr="Tom &amp; Denis, 2003" startWordPosition="251" endWordPosition="254">spoken dialogue system is used to collect information from a large body of users. A task where such crowdsourcing dialogue systems would be useful is to populate geographic databases. While there are now open databases with geographic information, such as OpenStreetMap (Haklay &amp; Weber, 2008), these are typically intended for map drawing, and therefore lack detailed streetlevel information about city landmarks, such as colors and height of buildings, ornamentations, facade materials, balconies, conspicuous signs, etc. Such information could for example be very useful for pedestrian navigation (Tom &amp; Denis, 2003; Ross et al., 2004). With the current growing usage of smartphones, we might envisage a community of users using their phones to contribute information to geographic databases, annotating cities to a great level of detail, using multi-modal method including speech. The key reason for using speech for map annotation is convenience; it is easy to talk into a mobile phone while walking down the street, so a user with a little experience will not be slowed down by the activity of interacting with a database. This way, useful information could be obtained that is really hard to add offline, sittin</context>
<context position="4239" citStr="Tom &amp; Denis, 2003" startWordPosition="660" endWordPosition="663">-20 June 2014. c�2014 Association for Computational Linguistics an routing are relatively new and are still in their nascent stage (Bartie &amp; Mackaness, 2006; Krug et al., 2003; Janarthanam et al., 2012; Boye et al., 2014). In the case of pedestrian navigation, it is preferable for way-finding systems to base their instructions on landmarks, by which we understand distinctive objects in the city environment. Studies have shown that the inclusion of landmarks into system-generated instructions for a pedestrian raises the user’s confidence in the system, compared to only left-right instructions (Tom &amp; Denis, 2003; Ross et al., 2004). Basing routing instructions on landmarks means that the routing system would, for example, generate an instruction “Go towards the red brick building” (where, in this case, “the red brick building” is the landmark), rather than “Turn slightly left here” or “Go north 200 meters”. This strategy for providing instructions places certain requirements on the geographic database: It has to include many landmarks and many details about them as well, so that the system can generate clear and un-ambiguous instructions. However, the information contained in current databases is sti</context>
</contexts>
<marker>Tom, Denis, 2003</marker>
<rawString>Tom, A., &amp; Denis, M. (2003). Referring to Landmark or Street Information in Route Directions: What Difference Does It Make?. In Kuhn, W., Worboys, M., &amp; Timpf, S. (Eds.), Spatial Information Theory. Foundations of Geographic Information Science (pp. 362-374). Springer Berlin Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
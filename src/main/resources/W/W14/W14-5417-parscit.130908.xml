<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.070033">
<title confidence="0.967463">
Towards Succinct and Relevant Image Descriptions
</title>
<author confidence="0.993251">
Desmond Elliott
</author>
<affiliation confidence="0.999431333333333">
Institute of Language, Communication, and Computation
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.983633">
d.elliott@ed.ac.uk
</email>
<bodyText confidence="0.999806837837838">
What does it mean to produce a good description of an image? Is a description good because it
correctly identifies all of the objects in the image, because it describes the interesting attributes of the
objects, or because it is short, yet informative? Grice’s Cooperative Principle, stated as “Make your
contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction
of the talk exchange in which you are engaged” (Grice, 1975), alongside other ideas of pragmatics in
communication, have proven useful in thinking about language generation (Hovy, 1987; McKeown et
al., 1995). The Cooperative Principle provides one possible framework for thinking about the generation
and evaluation of image descriptions.1
The immediate question is whether automatic image description is within the scope of the Cooperative
Principle. Consider the task of searching for images using natural language, where the purpose of the
exchange is for the user to quickly and accurately find images that match their information needs. In this
scenario, the user formulates a complete sentence query to express their needs, e.g. A sheepdog chasing
sheep in afield, and initiates an exchange with the system in the form of a sequence of one-shot con-
versations. In this exchange, both participants can describe images in natural language, and a successful
outcome relies on each participant succinctly and correctly expressing their beliefs about the images. It
follows from this that we can think of image description as facilitating communication between people
and computers, and thus take advantage of the Principle’s maxims of Quantity, Quality, Relevance, and
Manner in guiding the development and evaluation of automatic image description models.
An overview of the image description literature from the perspective of Grice’s maxims can be found
in Table 1. The most apparent ommission is the lack of research devoted to generating minimally infor-
mative descriptions: the maxim of Quantity. Attending to this maxim will become increasingly important
as the quality and coverage of object, attribute, and scene detectors increases. It would be undesirable to
develop models that describe every detected object in an image because that would be likely to violate the
maxim of Quantity (Spain and Perona, 2010). Similarly, if it is possible to associate an accurate attribute
with each object in the image, it will be important to be sparing in the application of those attributes: is
it relevant to describe “furry” sheep when there are no sheared sheep in an image?
How should image description models be evaluated with respect to the maxims of the Cooperative
Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram
BLEU and human judgements of semantic correctness (see Hodosh et al. (2013) for discussion of framing
image description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-based
measures against human judgements). The semantic correctness judgements task typically present a
variant of “Rate the relevance of the description for this image”, which only evaluates the description vis-
`a-vis the maxim of Relevance. One exception is the study of Mitchell et al. (2012), in which judgements
about the ordering of noun phrases (the maxim of Manner) were also collected. The importance of
being able to evaluate according to multiple maxims becomes clearer as computer vision becomes more
accurate. It seems intuitive that a model that describes and relates every object in the image could
be characterised as generating Relevant and Quality descriptions, but not necessarily descriptions of
</bodyText>
<footnote confidence="0.96155325">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://
creativecommons.org/licenses/by/4.0/
1This discussion primarily applies to image descriptions, and not to image captions. See (Hodosh et al., 2013) and (Panof-
sky, 1939) for a discussion of the differences between descriptions and captions.
</footnote>
<page confidence="0.942649">
109
</page>
<note confidence="0.849898">
Proceedings of the 25th International Conference on Computational Linguistics, pages 109–111,
Dublin, Ireland, August 23-29 2014.
</note>
<table confidence="0.920399">
Category Maxim Attention in the literature
Be as informative as required ???
Quantity Do not be more informative than ???
required
Quality Do not say what you All models exploit some kind of corpus data to
believe is false construct descriptions that are maximally probable
Do not say that for which (Yang et al., 2011; Li et al., 2011; Kuznetsova et al.,
you lack evidence 2012; Le et al., 2013). These approaches typically
use language modelling to construct hypotheses
based on the available evidence, but may eventually
be false.
No models try to generate irrelevant descriptions.
Relevance Be relevant Dodge et al. (2012) explored the separation between
what can be seen/not seen in an image/caption pair.
Avoid obscure expressions No model has been deliberately obscure.
Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to
describe and distinguish objects.
Be brief ???
Mitchell et al. (2012) and Elliott and Keller (2013)
</table>
<tableCaption confidence="0.70046025">
Be orderly explicitly try to predict the best ordering of objects
in the final description.
Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we
are unaware of any models that implicitly or explicitly claim to address this type of maxim.
</tableCaption>
<bodyText confidence="0.998522222222222">
adequate Quantity. It is not clear that current human judgements capture this distinction, yet the gold-
standard crowdsourced descriptions almost certainly do conform to the maxim of sufficient Quantity. A
further important consideration is how to obtain human judgements for multiple maxims without making
the studies prohibitively expensive.
Using Grice’s maxims to think about image description from the perspective of enabling effective
communication helps us reconsider the state of the art of automatic image description and directions
for future research. In particular, we identified the open problems of determining the minimum and
most relevant aspects of an image, and the challenges of conducting human evaluations along alternative
dimensions to semantic correctness.
</bodyText>
<sectionHeader confidence="0.984581" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9876685">
S. Frank, D. Frassinelli, and the anonymous reviewers provided valuable feedback on this paper. The
research is funded by ERC Starting Grant SYNPROC No. 203427.
</bodyText>
<sectionHeader confidence="0.994237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991314">
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, Yejin
Choi, Hal Daum´e III, Alex Berg, and Tamara Berg. 2012. Detecting visual text. In Proceedings of the 2012
Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 762–772, Montr´eal, Canada.
Desmond Elliott and Frank Keller. 2013. Image Description using Visual Dependency Representations. In Pro-
ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302,
Seattle, Washington, U.S.A.
</reference>
<page confidence="0.996544">
110
</page>
<reference confidence="0.9921144375">
Desmond Elliott and Frank Keller. 2014. Comparing Automatic Evaluation Measures for Image Description.
In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 452–457,
Baltimore, Maryland, U.S.A.
H. Paul Grice. 1975. Logic and Conversation. In P. Cole and J. L. Morgan, editors, Syntax and Semantics 3:
Speech Arts, pages 41–58. Academic Press, Inc.
Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data,
Models and Evaluation Metrics. Journal of Artificial Intelligence Research, 47:853–899.
E Hovy. 1987. Generating natural language under pragmatic constraints. Journal of Pragmatics, 11(6):689–719.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg.
2011. Baby talk: Understanding and generating simple image descriptions. In 2011 IEEE Conference on
Computer Vision and Pattern Recognition, pages 1601–1608, Colorado Springs, Colorado, U.S.A.
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective
Generation of Natural Image Descriptions. In Proceedings of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 359–368, Jeju Island, South Korea.
Dieu Thu Le, Jasper Uijlings, and Raffaella Bernardi. 2013. Exploiting language models for visual recognition.
In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769–779,
Seattle, Washington, U.S.A.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image
descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural
Language Learning, pages 220–228, Portland, Oregon, U.S.A.
K McKeown, J Robin, and K Kukich. 1995. Generating concise natural language summaries. Information
Processing &amp; Management, 31(5):703–733.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Alyssa Mensch, Alex Berg, Tamara
Berg, and Hal Daum. 2012. Midge : Generating Image Descriptions From Computer Vision Detections. In
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,
pages 747–756, Avignon, France.
Erwin Panofsky. 1939. Studies in Iconology. Oxford University Press.
Merrielle Spain and Pietro Perona. 2010. Measuring and Predicting Object Importance. International Journal of
Computer Vision, 91(1):59–76.
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-Guided Sentence Generation
of Natural Images. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
pages 444–454, Edinburgh, Scotland, UK.
</reference>
<page confidence="0.998795">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.010552">
<title confidence="0.999895">Towards Succinct and Relevant Image Descriptions</title>
<author confidence="0.965173">Desmond</author>
<affiliation confidence="0.994076333333333">Institute of Language, Communication, and School of University of</affiliation>
<abstract confidence="0.985217788732394">d.elliott@ed.ac.uk does it mean to produce a of an image? Is a description good because it correctly identifies all of the objects in the image, because it describes the interesting attributes of the objects, or because it is short, yet informative? Grice’s Cooperative Principle, stated as “Make your contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged” (Grice, 1975), alongside other ideas of pragmatics in communication, have proven useful in thinking about language generation (Hovy, 1987; McKeown et al., 1995). The Cooperative Principle provides one possible framework for thinking about the generation evaluation of image The immediate question is whether automatic image description is within the scope of the Cooperative Principle. Consider the task of searching for images using natural language, where the purpose of the exchange is for the user to quickly and accurately find images that match their information needs. In this the user formulates a complete sentence query to express their needs, e.g. sheepdog chasing in and initiates an exchange with the system in the form of a sequence of conversations. In this exchange, both participants can describe images in natural language, and a successful outcome relies on each participant succinctly and correctly expressing their beliefs about the images. It follows from this that we can think of image description as facilitating communication between people and computers, and thus take advantage of the Principle’s maxims of Quantity, Quality, Relevance, and Manner in guiding the development and evaluation of automatic image description models. An overview of the image description literature from the perspective of Grice’s maxims can be found in Table 1. The most apparent ommission is the lack of research devoted to generating minimally informative descriptions: the maxim of Quantity. Attending to this maxim will become increasingly important as the quality and coverage of object, attribute, and scene detectors increases. It would be undesirable to develop models that describe every detected object in an image because that would be likely to violate the maxim of Quantity (Spain and Perona, 2010). Similarly, if it is possible to associate an accurate attribute with each object in the image, it will be important to be sparing in the application of those attributes: is it relevant to describe “furry” sheep when there are no sheared sheep in an image? How should image description models be evaluated with respect to the maxims of the Cooperative Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram human judgements of correctness Hodosh et al. (2013) for discussion of framing image description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-based measures against human judgements). The semantic correctness judgements task typically present a variant of “Rate the relevance of the description for this image”, which only evaluates the description vis- `a-vis the maxim of Relevance. One exception is the study of Mitchell et al. (2012), in which judgements about the ordering of noun phrases (the maxim of Manner) were also collected. The importance of being able to evaluate according to multiple maxims becomes clearer as computer vision becomes more accurate. It seems intuitive that a model that describes and relates every object in the image could be characterised as generating Relevant and Quality descriptions, but not necessarily descriptions of work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: creativecommons.org/licenses/by/4.0/ discussion primarily applies to image and not to image See (Hodosh et al., 2013) and (Panofsky, 1939) for a discussion of the differences between descriptions and captions. 109 of the 25th International Conference on Computational pages Dublin, Ireland, August 23-29 2014. Category Maxim Attention in the literature Be as informative as required ??? not be more informative than ??? required Quality Do not say what you believe is false All models exploit some kind of corpus data to construct descriptions that are maximally probable (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. Do not say that for which you lack evidence No models try to generate irrelevant descriptions. relevant Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief ??? Mitchell et al. (2012) and Elliott and Keller (2013) Be orderly explicitly try to predict the best ordering of objects in the final description. Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we are unaware of any models that implicitly or explicitly claim to address this type of maxim. adequate Quantity. It is not clear that current human judgements capture this distinction, yet the goldstandard crowdsourced descriptions almost certainly do conform to the maxim of sufficient Quantity. A further important consideration is how to obtain human judgements for multiple maxims without making the studies prohibitively expensive. Using Grice’s maxims to think about image description from the perspective of enabling effective communication helps us reconsider the state of the art of automatic image description and directions for future research. In particular, we identified the open problems of determining the minimum and most relevant aspects of an image, and the challenges of conducting human evaluations along alternative dimensions to semantic correctness.</abstract>
<note confidence="0.892240444444445">Acknowledgments S. Frank, D. Frassinelli, and the anonymous reviewers provided valuable feedback on this paper. The research is funded by ERC Starting Grant SYNPROC No. 203427. References Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, Yejin Hal Daum´e III, Alex Berg, and Tamara Berg. 2012. Detecting visual text. In of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language pages 762–772, Montr´eal, Canada. Elliott and Frank Keller. 2013. Image Description using Visual Dependency Representations. In Proof the 2013 Conference on Empirical Methods in Natural Language pages 1292–1302, Seattle, Washington, U.S.A. 110 Desmond Elliott and Frank Keller. 2014. Comparing Automatic Evaluation Measures for Image Description. of the 52nd Annual Meeting of the Association for Computational pages 452–457, Baltimore, Maryland, U.S.A. Paul Grice. 1975. Logic and Conversation. In P. Cole and J. L. Morgan, editors, and Semantics 3: pages 41–58. Academic Press, Inc. Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data, and Evaluation Metrics. of Artificial Intelligence 47:853–899. Hovy. 1987. Generating natural language under pragmatic constraints. of 11(6):689–719. Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. Baby talk: Understanding and generating simple image descriptions. In IEEE Conference on Vision and Pattern pages 1601–1608, Colorado Springs, Colorado, U.S.A. Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective of Natural Image Descriptions. In of the 50th Annual Meeting of the Association for pages 359–368, Jeju Island, South Korea. Dieu Thu Le, Jasper Uijlings, and Raffaella Bernardi. 2013. Exploiting language models for visual recognition. of the 2013 Conference on Empirical Methods in Natural Language pages 769–779, Seattle, Washington, U.S.A. Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image using web-scale n-grams. In of the Fifteenth Conference on Computational Natural pages 220–228, Portland, Oregon, U.S.A. McKeown, J Robin, and K Kukich. 1995. Generating concise natural language summaries. &amp; 31(5):703–733. Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Alyssa Mensch, Alex Berg, Tamara Berg, and Hal Daum. 2012. Midge : Generating Image Descriptions From Computer Vision Detections. In of the 13th Conference of the European Chapter of the Association for Computational pages 747–756, Avignon, France. Panofsky. 1939. in Oxford University Press. Spain and Pietro Perona. 2010. Measuring and Predicting Object Importance. Journal of 91(1):59–76. Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-Guided Sentence Generation Natural Images. In of the Conference on Empirical Methods in Natural Language pages 444–454, Edinburgh, Scotland, UK. 111</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Margaret Mitchell</author>
<author>Karl Stratos</author>
<author>Kota Yamaguchi</author>
<author>Yejin Choi</author>
<author>Hal Daum´e Alex Berg</author>
<author>Tamara Berg</author>
</authors>
<title>Detecting visual text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>762--772</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="4991" citStr="Dodge et al. (2012)" startWordPosition="761" endWordPosition="764">y Maxim Attention in the literature Be as informative as required ??? Quantity Do not be more informative than ??? required Quality Do not say what you All models exploit some kind of corpus data to believe is false construct descriptions that are maximally probable Do not say that for which (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., you lack evidence 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. No models try to generate irrelevant descriptions. Relevance Be relevant Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief ??? Mitchell et al. (2012) and Elliott and Keller (2013) Be orderly explicitly try to predict the best ordering of objects in the final description. Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we are unaware of any models that implicitly or explicitly claim to address this t</context>
</contexts>
<marker>Dodge, Goyal, Han, Mensch, Mitchell, Stratos, Yamaguchi, Choi, Berg, Berg, 2012</marker>
<rawString>Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, Yejin Choi, Hal Daum´e III, Alex Berg, and Tamara Berg. 2012. Detecting visual text. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762–772, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Image Description using Visual Dependency Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1292--1302</pages>
<location>Seattle, Washington, U.S.A.</location>
<contexts>
<context position="5319" citStr="Elliott and Keller (2013)" startWordPosition="810" endWordPosition="813">, 2011; Kuznetsova et al., you lack evidence 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. No models try to generate irrelevant descriptions. Relevance Be relevant Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief ??? Mitchell et al. (2012) and Elliott and Keller (2013) Be orderly explicitly try to predict the best ordering of objects in the final description. Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we are unaware of any models that implicitly or explicitly claim to address this type of maxim. adequate Quantity. It is not clear that current human judgements capture this distinction, yet the goldstandard crowdsourced descriptions almost certainly do conform to the maxim of sufficient Quantity. A further important consideration is how to obtain human judgements for multiple maxims without making the stud</context>
</contexts>
<marker>Elliott, Keller, 2013</marker>
<rawString>Desmond Elliott and Frank Keller. 2013. Image Description using Visual Dependency Representations. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292–1302, Seattle, Washington, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Comparing Automatic Evaluation Measures for Image Description.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>452--457</pages>
<location>Baltimore, Maryland, U.S.A.</location>
<contexts>
<context position="3130" citStr="Elliott and Keller (2014)" startWordPosition="480" endWordPosition="483">, 2010). Similarly, if it is possible to associate an accurate attribute with each object in the image, it will be important to be sparing in the application of those attributes: is it relevant to describe “furry” sheep when there are no sheared sheep in an image? How should image description models be evaluated with respect to the maxims of the Cooperative Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram BLEU and human judgements of semantic correctness (see Hodosh et al. (2013) for discussion of framing image description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-based measures against human judgements). The semantic correctness judgements task typically present a variant of “Rate the relevance of the description for this image”, which only evaluates the description vis`a-vis the maxim of Relevance. One exception is the study of Mitchell et al. (2012), in which judgements about the ordering of noun phrases (the maxim of Manner) were also collected. The importance of being able to evaluate according to multiple maxims becomes clearer as computer vision becomes more accurate. It seems intuitive that a model that descri</context>
</contexts>
<marker>Elliott, Keller, 2014</marker>
<rawString>Desmond Elliott and Frank Keller. 2014. Comparing Automatic Evaluation Measures for Image Description. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 452–457, Baltimore, Maryland, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics 3: Speech Arts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and J. L. Morgan, editors,</editor>
<publisher>Academic Press, Inc.</publisher>
<contexts>
<context position="653" citStr="Grice, 1975" startWordPosition="98" endWordPosition="99">iptions Desmond Elliott Institute of Language, Communication, and Computation School of Informatics University of Edinburgh d.elliott@ed.ac.uk What does it mean to produce a good description of an image? Is a description good because it correctly identifies all of the objects in the image, because it describes the interesting attributes of the objects, or because it is short, yet informative? Grice’s Cooperative Principle, stated as “Make your contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged” (Grice, 1975), alongside other ideas of pragmatics in communication, have proven useful in thinking about language generation (Hovy, 1987; McKeown et al., 1995). The Cooperative Principle provides one possible framework for thinking about the generation and evaluation of image descriptions.1 The immediate question is whether automatic image description is within the scope of the Cooperative Principle. Consider the task of searching for images using natural language, where the purpose of the exchange is for the user to quickly and accurately find images that match their information needs. In this scenario, </context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. Paul Grice. 1975. Logic and Conversation. In P. Cole and J. L. Morgan, editors, Syntax and Semantics 3: Speech Arts, pages 41–58. Academic Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research, 47:853–899. E Hovy.</journal>
<volume>11</volume>
<issue>6</issue>
<contexts>
<context position="3037" citStr="Hodosh et al. (2013)" startWordPosition="465" endWordPosition="468">an image because that would be likely to violate the maxim of Quantity (Spain and Perona, 2010). Similarly, if it is possible to associate an accurate attribute with each object in the image, it will be important to be sparing in the application of those attributes: is it relevant to describe “furry” sheep when there are no sheared sheep in an image? How should image description models be evaluated with respect to the maxims of the Cooperative Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram BLEU and human judgements of semantic correctness (see Hodosh et al. (2013) for discussion of framing image description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-based measures against human judgements). The semantic correctness judgements task typically present a variant of “Rate the relevance of the description for this image”, which only evaluates the description vis`a-vis the maxim of Relevance. One exception is the study of Mitchell et al. (2012), in which judgements about the ordering of noun phrases (the maxim of Manner) were also collected. The importance of being able to evaluate according to multiple maxims becomes </context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence Research, 47:853–899. E Hovy. 1987. Generating natural language under pragmatic constraints. Journal of Pragmatics, 11(6):689–719.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In 2011 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1601--1608</pages>
<location>Colorado Springs, Colorado, U.S.A.</location>
<contexts>
<context position="5187" citStr="Kulkarni et al. (2011)" startWordPosition="790" endWordPosition="793">ta to believe is false construct descriptions that are maximally probable Do not say that for which (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., you lack evidence 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. No models try to generate irrelevant descriptions. Relevance Be relevant Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief ??? Mitchell et al. (2012) and Elliott and Keller (2013) Be orderly explicitly try to predict the best ordering of objects in the final description. Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we are unaware of any models that implicitly or explicitly claim to address this type of maxim. adequate Quantity. It is not clear that current human judgements capture this distinction, yet the goldstandard crowdsourced descriptions almost certainly do conform to the maxim of </context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In 2011 IEEE Conference on Computer Vision and Pattern Recognition, pages 1601–1608, Colorado Springs, Colorado, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective Generation of Natural Image Descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>359--368</pages>
<location>Jeju Island, South</location>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective Generation of Natural Image Descriptions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359–368, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dieu Thu Le</author>
<author>Jasper Uijlings</author>
<author>Raffaella Bernardi</author>
</authors>
<title>Exploiting language models for visual recognition.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>769--779</pages>
<location>Seattle, Washington, U.S.A.</location>
<contexts>
<context position="4761" citStr="Le et al., 2013" startWordPosition="728" endWordPosition="731">anofsky, 1939) for a discussion of the differences between descriptions and captions. 109 Proceedings of the 25th International Conference on Computational Linguistics, pages 109–111, Dublin, Ireland, August 23-29 2014. Category Maxim Attention in the literature Be as informative as required ??? Quantity Do not be more informative than ??? required Quality Do not say what you All models exploit some kind of corpus data to believe is false construct descriptions that are maximally probable Do not say that for which (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., you lack evidence 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. No models try to generate irrelevant descriptions. Relevance Be relevant Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief ??? Mitchell et al. (2012) and Elliott and Keller (2013) Be orderly explicitly try to predict the </context>
</contexts>
<marker>Le, Uijlings, Bernardi, 2013</marker>
<rawString>Dieu Thu Le, Jasper Uijlings, and Raffaella Bernardi. 2013. Exploiting language models for visual recognition. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769–779, Seattle, Washington, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale n-grams.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>220--228</pages>
<location>Portland, Oregon, U.S.A.</location>
<contexts>
<context position="4700" citStr="Li et al., 2011" startWordPosition="717" endWordPosition="720"> and not to image captions. See (Hodosh et al., 2013) and (Panofsky, 1939) for a discussion of the differences between descriptions and captions. 109 Proceedings of the 25th International Conference on Computational Linguistics, pages 109–111, Dublin, Ireland, August 23-29 2014. Category Maxim Attention in the literature Be as informative as required ??? Quantity Do not be more informative than ??? required Quality Do not say what you All models exploit some kind of corpus data to believe is false construct descriptions that are maximally probable Do not say that for which (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., you lack evidence 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. No models try to generate irrelevant descriptions. Relevance Be relevant Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief ??? Mitchell et al. (2012) and Elliot</context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale n-grams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220–228, Portland, Oregon, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>J Robin</author>
<author>K Kukich</author>
</authors>
<title>Generating concise natural language summaries.</title>
<date>1995</date>
<journal>Information Processing &amp; Management,</journal>
<volume>31</volume>
<issue>5</issue>
<contexts>
<context position="800" citStr="McKeown et al., 1995" startWordPosition="117" endWordPosition="120">c.uk What does it mean to produce a good description of an image? Is a description good because it correctly identifies all of the objects in the image, because it describes the interesting attributes of the objects, or because it is short, yet informative? Grice’s Cooperative Principle, stated as “Make your contribution such as is required, at the stage at which it occurs, by the accepted purpose or direction of the talk exchange in which you are engaged” (Grice, 1975), alongside other ideas of pragmatics in communication, have proven useful in thinking about language generation (Hovy, 1987; McKeown et al., 1995). The Cooperative Principle provides one possible framework for thinking about the generation and evaluation of image descriptions.1 The immediate question is whether automatic image description is within the scope of the Cooperative Principle. Consider the task of searching for images using natural language, where the purpose of the exchange is for the user to quickly and accurately find images that match their information needs. In this scenario, the user formulates a complete sentence query to express their needs, e.g. A sheepdog chasing sheep in afield, and initiates an exchange with the s</context>
</contexts>
<marker>McKeown, Robin, Kukich, 1995</marker>
<rawString>K McKeown, J Robin, and K Kukich. 1995. Generating concise natural language summaries. Information Processing &amp; Management, 31(5):703–733.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Kota Yamaguchi</author>
<author>Karl Stratos</author>
<author>Alyssa Mensch</author>
<author>Alex Berg</author>
<author>Tamara Berg</author>
<author>Hal Daum</author>
</authors>
<title>Midge : Generating Image Descriptions From Computer Vision Detections.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>747--756</pages>
<location>Avignon, France.</location>
<contexts>
<context position="3459" citStr="Mitchell et al. (2012)" startWordPosition="530" endWordPosition="533">e maxims of the Cooperative Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram BLEU and human judgements of semantic correctness (see Hodosh et al. (2013) for discussion of framing image description as a ranking task, and Elliott and Keller (2014) for a correlation analysis of text-based measures against human judgements). The semantic correctness judgements task typically present a variant of “Rate the relevance of the description for this image”, which only evaluates the description vis`a-vis the maxim of Relevance. One exception is the study of Mitchell et al. (2012), in which judgements about the ordering of noun phrases (the maxim of Manner) were also collected. The importance of being able to evaluate according to multiple maxims becomes clearer as computer vision becomes more accurate. It seems intuitive that a model that describes and relates every object in the image could be characterised as generating Relevant and Quality descriptions, but not necessarily descriptions of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1This discussion primarily appl</context>
<context position="5289" citStr="Mitchell et al. (2012)" startWordPosition="805" endWordPosition="808">ang et al., 2011; Li et al., 2011; Kuznetsova et al., you lack evidence 2012; Le et al., 2013). These approaches typically use language modelling to construct hypotheses based on the available evidence, but may eventually be false. No models try to generate irrelevant descriptions. Relevance Be relevant Dodge et al. (2012) explored the separation between what can be seen/not seen in an image/caption pair. Avoid obscure expressions No model has been deliberately obscure. Manner Avoid ambiguity Kulkarni et al. (2011) introduced visual attributes to describe and distinguish objects. Be brief ??? Mitchell et al. (2012) and Elliott and Keller (2013) Be orderly explicitly try to predict the best ordering of objects in the final description. Table 1: An overview of Grice’s maxims and the relevant image description models. ??? means that we are unaware of any models that implicitly or explicitly claim to address this type of maxim. adequate Quantity. It is not clear that current human judgements capture this distinction, yet the goldstandard crowdsourced descriptions almost certainly do conform to the maxim of sufficient Quantity. A further important consideration is how to obtain human judgements for multiple </context>
</contexts>
<marker>Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Mensch, Berg, Berg, Daum, 2012</marker>
<rawString>Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Alyssa Mensch, Alex Berg, Tamara Berg, and Hal Daum. 2012. Midge : Generating Image Descriptions From Computer Vision Detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747–756, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Panofsky</author>
</authors>
<title>Studies in Iconology.</title>
<date>1939</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="4159" citStr="Panofsky, 1939" startWordPosition="632" endWordPosition="634">lso collected. The importance of being able to evaluate according to multiple maxims becomes clearer as computer vision becomes more accurate. It seems intuitive that a model that describes and relates every object in the image could be characterised as generating Relevant and Quality descriptions, but not necessarily descriptions of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1This discussion primarily applies to image descriptions, and not to image captions. See (Hodosh et al., 2013) and (Panofsky, 1939) for a discussion of the differences between descriptions and captions. 109 Proceedings of the 25th International Conference on Computational Linguistics, pages 109–111, Dublin, Ireland, August 23-29 2014. Category Maxim Attention in the literature Be as informative as required ??? Quantity Do not be more informative than ??? required Quality Do not say what you All models exploit some kind of corpus data to believe is false construct descriptions that are maximally probable Do not say that for which (Yang et al., 2011; Li et al., 2011; Kuznetsova et al., you lack evidence 2012; Le et al., 201</context>
</contexts>
<marker>Panofsky, 1939</marker>
<rawString>Erwin Panofsky. 1939. Studies in Iconology. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Merrielle Spain</author>
<author>Pietro Perona</author>
</authors>
<title>Measuring and Predicting Object Importance.</title>
<date>2010</date>
<journal>International Journal of Computer Vision,</journal>
<volume>91</volume>
<issue>1</issue>
<contexts>
<context position="2512" citStr="Spain and Perona, 2010" startWordPosition="379" endWordPosition="382">lopment and evaluation of automatic image description models. An overview of the image description literature from the perspective of Grice’s maxims can be found in Table 1. The most apparent ommission is the lack of research devoted to generating minimally informative descriptions: the maxim of Quantity. Attending to this maxim will become increasingly important as the quality and coverage of object, attribute, and scene detectors increases. It would be undesirable to develop models that describe every detected object in an image because that would be likely to violate the maxim of Quantity (Spain and Perona, 2010). Similarly, if it is possible to associate an accurate attribute with each object in the image, it will be important to be sparing in the application of those attributes: is it relevant to describe “furry” sheep when there are no sheared sheep in an image? How should image description models be evaluated with respect to the maxims of the Cooperative Principle? So far model evaulation has focused on automatic text-based measures, such as Unigram BLEU and human judgements of semantic correctness (see Hodosh et al. (2013) for discussion of framing image description as a ranking task, and Elliott</context>
</contexts>
<marker>Spain, Perona, 2010</marker>
<rawString>Merrielle Spain and Pietro Perona. 2010. Measuring and Predicting Object Importance. International Journal of Computer Vision, 91(1):59–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>444--454</pages>
<location>Edinburgh, Scotland, UK.</location>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-Guided Sentence Generation of Natural Images. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 444–454, Edinburgh, Scotland, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
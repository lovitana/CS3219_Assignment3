<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006935">
<title confidence="0.977495">
Fact Checking: Task definition and dataset construction
</title>
<author confidence="0.996014">
Andreas Vlachos
</author>
<affiliation confidence="0.861046">
Dept. of Computer Science
University College London
London, United Kingdom
</affiliation>
<email confidence="0.99599">
a.vlachos@cs.ucl.ac.uk
</email>
<author confidence="0.997565">
Sebastian Riedel
</author>
<affiliation confidence="0.861103666666667">
Dept. of Computer Science
University College London
London, United Kingdom
</affiliation>
<email confidence="0.997461">
s.riedel@ucl.ac.uk
</email>
<sectionHeader confidence="0.993891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99964652631579">
In this paper we introduce the task of fact
checking, i.e. the assessment of the truth-
fulness of a claim. The task is commonly
performed manually by journalists verify-
ing the claims made by public figures. Fur-
thermore, ordinary citizens need to assess
the truthfulness of the increasing volume
of statements they consume. Thus, de-
veloping fact checking systems is likely
to be of use to various members of soci-
ety. We first define the task and detail the
construction of a publicly available dataset
using statements fact-checked by journal-
ists available online. Then, we discuss
baseline approaches for the task and the
challenges that need to be addressed. Fi-
nally, we discuss how fact checking relates
to mainstream natural language processing
tasks and can stimulate further research.
</bodyText>
<sectionHeader confidence="0.988646" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999578538461539">
Fact checking is the task of assessing the truth-
fulness of claims made by public figures such
as politicians, pundits, etc. It is commonly per-
formed by journalists employed by news organisa-
tions in the process of news article creation. More
recently, institutes and websites dedicated to this
cause have emerged such as Full Fact1 and Politi-
Fact2 respectively. Figure 1 shows two examples
of fact checked statements, together with the ver-
dicts offered by the journalists.
Fact-checking is a time-consuming process. In
assessing the first claim in Figure 1 a journalist
would need to consult a variety of sources to find
</bodyText>
<footnote confidence="0.999941">
1http://fullfact.org
2http://politifact.com
</footnote>
<bodyText confidence="0.9998898">
the average “full-time earnings” for criminal bar-
risters. Fact checking websites commonly provide
the detailed analysis (not shown in the figure) per-
formed to support the verdict.
Automating the process of fact checking has re-
cently been discussed in the context of computa-
tional journalism (Cohen et al., 2011; Flew et al.,
2012). Inspired by the recent progress in natural
language processing, databases and information
retrieval, the vision is to provide journalists with
tools that would allow them to perform this task
automatically, or even render the articles “live” by
updating them with most current data. This au-
tomation is further enabled by the increasing on-
line availability of datasets, survey results, and re-
ports in machine readable formats by various insti-
tutions, e.g. EUROSTAT releases detailed statis-
tics for all European economies.3
Furthermore, ordinary citizens need to fact
check the information provided to them. This need
is intensified with the proliferation of social media
such as Twitter, since the dissemination of news
and information commonly circumvents the tra-
ditional news channels (Petrovic, 2013). In addi-
tion, the rise of citizen journalism (Goode, 2009)
suggests that often citizens become the sources
of information. Since the information provided
by them is not edited or curated, automated fact
checking would assist in avoiding the spreading
false information.
In this paper we define the task of fact-checking.
We then detail the construction of a dataset using
fact-checked statements available online. Finally,
we describe the challenges it poses and its relation
to current research in natural language processing.
</bodyText>
<footnote confidence="0.992335">
3http://epp.eurostat.ec.europa.eu/
portal/page/portal/eurostat/home
</footnote>
<page confidence="0.984418">
18
</page>
<note confidence="0.4352745">
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18–22,
Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.897304" genericHeader="introduction">
2 Task definition
</sectionHeader>
<bodyText confidence="0.980258150537635">
We define fact-checking to be the assignment of a
truth value to a claim made in a particular con-
text. Thus it is natural to consider it as a bi-
nary classification task. However, it is often the
case that the statements are not completely true
or false. For example, the verdict for the third
claim in Figure 1 is MOSTLYTRUE because some
of the sources dispute it, while in the fourth exam-
ple the statistics can be manipulated to support or
disprove the claim as desired. Therefore it is bet-
ter to consider fact-checking as an ordinal classifi-
cation task (Frank and Hall, 2001), thus allowing
systems to capture the nuances of the task.
The verdict by itself, even if graded, needs to be
supported by an analysis (e.g., what is the systems
interpretation of the statement). However, given
the difficulty of carving out exactly what the cor-
rect analysis for a statement might be, we restrict
the task to be a prediction problem so that we can
evaluate performance automatically.
Context can be crucial in fact-checking. For ex-
ample, knowing that the fourth claim of Figure 1
is made by a UK politician is necessary in order
to assess it using data about this country. Fur-
thermore, time is also important since the vari-
ous comparisons usually refer to time-frames an-
chored at the time a claim is made.
The task is rather challenging. While some
claims such as the one about Crimea can be fact-
checked by extracting relations from WikiPedia,
the verdict often hinges on interpreting relatively
fine points, e.g. the last claim refers to a partic-
ular definition of income. Journalists also check
multiple sources in producing their verdicts, as in
the case of the third claim. Interestingly, they also
consider multiple interpretations of the data; e.g.
in the last claim is assessed as HALFTRUE since
different but reasonable interpretations of the same
data lead to different conclusions.
We consider all of the aspects mentioned (time,
speaker, multiple sources and interpretations) as
part of the task of fact checking. However, we
want to restrict the task to statements that can be
fact-checked objectively, which is not always true
for the statements assessed by journalists. There-
fore, we do not consider statements such as “New
Labour promised social improvement but deliv-
ered a collapse in social mobility” to be part to
the task since there are no universal definitions of
“social improvement” and “social mobility”.4
4http://blogs.channel4.com/factcheck/
factcheck-social-mobility-collapsed/
Claim (by Minister Shailesh Vara)
“The average criminal bar barrister working full-
time is earning some £84,000.”
Verdict: FALSE (by Channel 4 Fact Check)
The figures the Ministry of Justice have stressed
this week seem decidedly dodgy. Even if you do
want to use the figures, once you take away the
many overheads self-employed advocates have to
pay you are left with a middling sum of money.
Claim (by U.S. Rep. Mike Rogers)
“Crimea was part of Russia until 1954, when it
was given to the Soviet Republic of the Ukraine.”
Verdict: TRUE (by Politifact)
Rogers said Crimea belonged to Russia until
1954, when Khrushchev gave the land to
Ukraine, then a Soviet republic.
Claim (by President Barack Obama)
“For the first time in over a decade, business
leaders around the world have declared that
China is no longer the world’s No. 1 place to
invest; America is.”
Verdict: MOSTLYTRUE (by Politifact)
The president is accurate by citing one particular
study, and that study did ask business leaders
what they thought about investing in the United
States. A broader look at other rankings doesn’t
make the United States seem like such a power-
house, even if it does still best China in some lists.
Claim (by Chancellor George Osborne)
“Real household disposable income is rising.”
Verdict: HALFTRUE (by Channel 4 Fact Check)
RHDI did grow in latest period we know about
(the second quarter of 2013), making Mr Osborne
arguably right to say that it is rising as we speak.
But over the last two quarters we know about,
income was down 0.1 per cent. If you want to
compare the latest four quarters of data with the
previous four, there was a fall in household
income, making the chancellor wrong. But if
you compare the latest full year of results, 2012,
with 2011, income is up and he’s right again.
</bodyText>
<figureCaption confidence="0.999191">
Figure 1: Fact-checked statements.
</figureCaption>
<page confidence="0.997277">
19
</page>
<sectionHeader confidence="0.990702" genericHeader="method">
3 Dataset construction
</sectionHeader>
<bodyText confidence="0.999987714285714">
In order to construct a dataset to develop and eval-
uate approaches to fact checking, we first surveyed
popular fact checking websites. We decided to
consider statements from two of them, the fact
checking blog of Channel 45 and the Truth-O-
Meter from PolitiFact.6 Both websites have large
archives of fact-checked statements (more than
1,000 statements each), they cover a wide range of
prevalent issues of U.K. and U.S. public life, and
they provide detailed verdicts with fine-grained la-
bels such as MOSTLYFALSE and HALFTRUE.
We examined recent fact-checks from each
website at the time of writing. For each state-
ment, apart from the statement itself, we recorded
the date it was made, the speaker, the label of
the verdict and the URL. As the two websites
use different labelling schemes, we aligned the la-
bels of the verdicts to a five-point scale: TRUE,
MOSTLYTRUE, HALFTRUE, MOSTLYFALSE and
FALSE. The speakers included, apart from pub-
lic figures, associations such as the American Bev-
erage Association, activists, even viral FaceBook
posts submitted by the public.
We then decided which of the statements should
be considered for the task proposed. As discussed
in the previous section we want to avoid state-
ments that cannot be assessed objectively. Follow-
ing this, we deemed unsuitable statements:
</bodyText>
<listItem confidence="0.935689166666667">
• assessing causal relations, e.g. whether a
statistic should be attributed to a particular law
• concerning the future, e.g. speculations involv-
ing oil prices
• not concerning facts, e.g. whether a politician
is supporting certain policies
</listItem>
<bodyText confidence="0.999821545454546">
For the statements that were considered suit-
able, we also collected the sources used by the
journalists in the analysis provided for the verdict.
Common sources include tables with statistics and
reports from governments, think tanks and other
organisations, available online. Automatic identi-
fication of the sources needed to fact check a state-
ment is an important stage in the process, which is
potentially useful in its own right in the context
of assisting journalists in a semi-automated fact-
checking approach Cohen et al. (2011). Some-
</bodyText>
<footnote confidence="0.88675475">
16444
5http://blogs.channel4.com/factcheck/
6http://www.politifact.com/
truth-o-meter/statements/
</footnote>
<bodyText confidence="0.9999066">
times the verdicts relied on data that were not
available online such personal communications;
statements whose verdict relied on such data were
also deemed unsuitable for the task.
As mentioned earlier, the verdicts on the web-
sites are accompanied by lengthy analyses. While
such analyses could be useful annotation for in-
termediate stages of the task — e.g. we could use
it as supervision to learn how to combine the in-
formation extracted from the various sources into
a verdict — we noticed that the language used in
them is indicative of the verdict.7 Thus we decided
not to include them in the dataset, as it would en-
able tackling part of the task as sentiment analy-
sis. Out of the 221 fact-checked statements exam-
ined, we judged 106 as suitable. The dataset col-
lected including our suitability judgements is pub-
licly available8 and we are working on extending
it so that it can support the development and the
automatic evaluation of fact checking approaches.
</bodyText>
<sectionHeader confidence="0.993063" genericHeader="method">
4 Baseline approaches
</sectionHeader>
<bodyText confidence="0.976468884615385">
As discussed in Section 2, we consider fact check-
ing as an ordinal classification task. Thus, in the-
ory it would be possible to tackle it as a supervised
classification task using algorithms that learn from
statements annotated with the verdict labels. How-
ever this is unlikely to be successful, since state-
ments such as the ones verified by journalists do
not contain the world knowledge and the temporal
and spatial context needed for this purpose.
A different approach would be to match state-
ments to ones already fact-checked by journalists
and return the label in a K-nearest neighbour fash-
ion.9 Thus the task is reduced to assessing the se-
mantic similarity between statements, which was
explored in a recent shared task (Agirre et al.,
2013). An obvious shortcoming of this approach
is that it cannot be applied to new claims that have
not been fact-checked, thus it can only be used to
detect repetitions and paraphrases of false claims.
A possible mechanism to extend the coverage of
such an approach to novel statements is to assume
that some large text collection is the source of all
true statements. For example, Wikipedia is likely
7E.g. part of the analysis of the first claim in Figure 1
reads: “the full-time figure has the handy effect of stripping
out the very lowest earners and bumping up the average”.
</bodyText>
<footnote confidence="0.9974332">
8https://sites.google.com/site/
andreasvlachos/resources
9The Truth-Teller by Washington Post (http://
truthteller.washingtonpost.com/) follows this
approach.
</footnote>
<page confidence="0.996536">
20
</page>
<bodyText confidence="0.99712025">
to contain a statement that would match the sec-
ond claim in Figure 1. However, it would still be
unable to tackle the other claims mentioned, since
they require calculations based on the data.
</bodyText>
<sectionHeader confidence="0.996449" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999984185185185">
The main drawback of the baseline approaches
mentioned (aside from their potential coverage) is
the lack of interpretability of their verdicts, also re-
ferred to as algorithmic accountability (Diakopou-
los, 2014). While it is possible for a natural lan-
guage processing expert to inspect aspects of the
prediction such as feature weights, this tends to
become harder as the approaches become more so-
phisticated. Ultimately, the user of a fact checking
system would trust a verdict only if it is accom-
panied by an analysis similar to the one provided
by the journalists. This desideratum is present in
other tasks such as the recently proposed science
test question answering (Clark et al., 2013).
Cohen et al. (2011) propose that fact checking
is about asking the right questions. These ques-
tions might be database queries, requests for in-
formation to be extracted from textual resources,
etc. For example, in checking the last claim in Fig-
ure 1 a critical reader would like to know what are
the possible interpretations of “real household dis-
posable income” and what the calculations might
be for other reasonable time spans.
The manual fact checking process suggests an
approach that is more likely to give an inter-
pretable analysis and would decompose the task
into the following stages:
</bodyText>
<listItem confidence="0.9987575">
1. extract statements to be fact-checked
2. construct appropriate questions
3. obtain the answers from relevant sources
4. reach a verdict using these answers
</listItem>
<bodyText confidence="0.999984023255814">
The stages of this architecture can be mapped
to tasks well-explored in the natural language pro-
cessing community. Statement extraction could
be tackled as a sentence classification problem,
following approaches similar to those proposed
for speculation detection (Farkas et al., 2010) and
veridicality assessment (de Marneffe et al., 2012).
Furthermore, obtaining answers to questions from
databases is a task typically addressed in the con-
text of semantic parsing research, while obtaining
such answers from textual sources is usually con-
sidered in the context of information extraction.
Finally, the compilation of the answers into a ver-
dict could be considered as a form of logic-based
textual entailment (Bos and Markert, 2005).
However, the fact-checking stages described in-
clude a novel task, namely question construc-
tion for a given statement. This task is likely
to rely on semantic parsing of the statement fol-
lowed by restructuring of the logical form gener-
ated. Since question construction is a rather un-
common task, it is likely to require human supervi-
sion, which could possibly be obtained via crowd-
sourcing. Furthermore, the open-domain nature of
fact checking places greater demands on the estab-
lished tasks of information extraction and seman-
tic parsing. Thus, fact-checking is likely to stim-
ulate research in these tasks on methods that do
not require domain-specific supervision (Riedel et
al., 2013) and are able to adapt to new information
requests (Kwiatkowski et al., 2013).
Fact-checking is related to the tasks of textual
entailment (Dagan et al., 2006) and machine com-
prehension (Richardson et al., 2013), with the dif-
ference that the text which should be used to pre-
dict the entailment of the hypothesis or the correct
answer respectively is not provided in the input.
Instead, systems need to locate the sources needed
to predict the verdict label as part of the task. Fur-
thermore, by defining the task in the context of
real-world journalism we are able to obtain labeled
statements at no annotation cost, apart from the as-
sessment of their suitability for the task.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999597">
In this paper we introduced the task of fact check-
ing and detailed the construction of a dataset us-
ing statements fact-checked by journalists avail-
able online. In addition, we discussed baseline ap-
proaches that could be applied to perform the task
and the challenges that need to be addressed.
Apart from being a challenging testbed to stim-
ulate progress in natural language processing, re-
search in fact checking is likely to inhibit the in-
tentional or unintentional dissemination of false
information. Even an approach that would return
the sources related to a statement could be very
helpful to journalists as well as other critical read-
ers in a semi-automated fact checking approach.
</bodyText>
<sectionHeader confidence="0.998306" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9887045">
The authors would like to thank the members of
the Machine Reading lab for useful discussions
</bodyText>
<page confidence="0.996969">
21
</page>
<bodyText confidence="0.889103">
and their help in compiling the dataset.
</bodyText>
<sectionHeader confidence="0.986686" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999767797297298">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity. In Proceedings of
the Second Joint Conference on Lexical and Compu-
tational Semantics and the Shared Task: Semantic
Textual Similarity, pages 32–43, Atlanta, GA.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of the 2005 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2005),
pages 628–635.
Peter Clark, Philip Harrison, and Niranjan Balasubra-
manian. 2013. A study of the knowledge base re-
quirements for passing an elementary science test.
In Proceedings of the 2013 Workshop on Automated
Knowledge Base Construction, pages 37–42.
Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu.
2011. Computational journalism: A call to arms to
database researchers. In Proceedings of the Confer-
ence on Innovative Data Systems Research, volume
2011, pages 148–151.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Proceedings of the First International
Conference on Machine Learning Challenges: Eval-
uating Predictive Uncertainty Visual Object Classi-
fication, and Recognizing Textual Entailment, pages
177–190.
Marie-Catherine de Marneffe, Christopher D. Man-
ning, and Christopher Potts. 2012. Did it happen?
the pragmatic complexity of veridicality assessment.
Computational Linguistics, 38(2):301–333, June.
Nick Diakopoulos. 2014. Algorithmic accountabil-
ity reporting: On the investigation of black boxes.
Technical report, Tow Center for Digital Journalism.
Richard Farkas, Veronika Vincze, Gyorgy Mora, Janos
Csirik, and Gyorgy Szarvas. 2010. The CoNLL
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the CoNLL 2010 Shared Task.
Terry Flew, Anna Daniel, and Christina L. Spurgeon.
2012. The promise of computational journalism.
In Proceedings of the Australian and New Zealand
Communication Association Conference, pages 1–
19.
Eibe Frank and Mark Hall. 2001. A simple approach
to ordinal classification. In Proceedings of the 12th
European Conference on Machine Learning, pages
145–156.
Luke Goode. 2009. Social news, citizen journalism
and democracy. New Media &amp; Society, 11(8):1287–
1305.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1545–1556, Seattle,
WA.
Sasa Petrovic. 2013. Real-time event detection in mas-
sive streams. Ph.D. thesis, School of Informatics,
University of Edinburgh.
Matthew Richardson, Christopher J.C. Burges, and
Erin Renshaw. 2013. MCTest: A challenge dataset
for the open-domain machine comprehension of
text. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 193–203, Seattle, WA.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin,
and Andrew McCallum. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, GA.
</reference>
<page confidence="0.999016">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998582">Fact Checking: Task definition and dataset construction</title>
<author confidence="0.990986">Andreas</author>
<affiliation confidence="0.884849666666667">Dept. of Computer University College London, United</affiliation>
<email confidence="0.966952">a.vlachos@cs.ucl.ac.uk</email>
<author confidence="0.999559">Sebastian Riedel</author>
<affiliation confidence="0.999792">Dept. of Computer Science University College London</affiliation>
<address confidence="0.780478">London, United Kingdom</address>
<email confidence="0.998439">s.riedel@ucl.ac.uk</email>
<abstract confidence="0.996082216">In this paper we introduce the task of fact checking, i.e. the assessment of the truthfulness of a claim. The task is commonly performed manually by journalists verifying the claims made by public figures. Furthermore, ordinary citizens need to assess the truthfulness of the increasing volume of statements they consume. Thus, developing fact checking systems is likely to be of use to various members of society. We first define the task and detail the construction of a publicly available dataset using statements fact-checked by journalists available online. Then, we discuss baseline approaches for the task and the challenges that need to be addressed. Finally, we discuss how fact checking relates to mainstream natural language processing tasks and can stimulate further research. 1 Motivation Fact checking is the task of assessing the truthfulness of claims made by public figures such as politicians, pundits, etc. It is commonly performed by journalists employed by news organisations in the process of news article creation. More recently, institutes and websites dedicated to this have emerged such as Full and Politirespectively. Figure 1 shows two examples of fact checked statements, together with the verdicts offered by the journalists. Fact-checking is a time-consuming process. In assessing the first claim in Figure 1 a journalist would need to consult a variety of sources to find the average “full-time earnings” for criminal barristers. Fact checking websites commonly provide the detailed analysis (not shown in the figure) performed to support the verdict. Automating the process of fact checking has recently been discussed in the context of computational journalism (Cohen et al., 2011; Flew et al., 2012). Inspired by the recent progress in natural language processing, databases and information retrieval, the vision is to provide journalists with tools that would allow them to perform this task automatically, or even render the articles “live” by updating them with most current data. This automation is further enabled by the increasing online availability of datasets, survey results, and reports in machine readable formats by various institutions, e.g. EUROSTAT releases detailed statisfor all European Furthermore, ordinary citizens need to fact check the information provided to them. This need is intensified with the proliferation of social media such as Twitter, since the dissemination of news and information commonly circumvents the traditional news channels (Petrovic, 2013). In addition, the rise of citizen journalism (Goode, 2009) suggests that often citizens become the sources of information. Since the information provided by them is not edited or curated, automated fact checking would assist in avoiding the spreading false information. In this paper we define the task of fact-checking. We then detail the construction of a dataset using fact-checked statements available online. Finally, we describe the challenges it poses and its relation to current research in natural language processing. portal/page/portal/eurostat/home 18 of the ACL 2014 Workshop on Language Technologies and Computational Social pages Maryland, USA, June 26, 2014. Association for Computational Linguistics 2 Task definition We define fact-checking to be the assignment of a truth value to a claim made in a particular context. Thus it is natural to consider it as a binary classification task. However, it is often the case that the statements are not completely true or false. For example, the verdict for the third in Figure 1 is some of the sources dispute it, while in the fourth example the statistics can be manipulated to support or disprove the claim as desired. Therefore it is better to consider fact-checking as an ordinal classification task (Frank and Hall, 2001), thus allowing systems to capture the nuances of the task. The verdict by itself, even if graded, needs to be supported by an analysis (e.g., what is the systems interpretation of the statement). However, given the difficulty of carving out exactly what the correct analysis for a statement might be, we restrict the task to be a prediction problem so that we can evaluate performance automatically. Context can be crucial in fact-checking. For example, knowing that the fourth claim of Figure 1 is made by a UK politician is necessary in order to assess it using data about this country. Furthermore, time is also important since the various comparisons usually refer to time-frames anchored at the time a claim is made. The task is rather challenging. While some claims such as the one about Crimea can be factchecked by extracting relations from WikiPedia, the verdict often hinges on interpreting relatively fine points, e.g. the last claim refers to a particular definition of income. Journalists also check multiple sources in producing their verdicts, as in the case of the third claim. Interestingly, they also consider multiple interpretations of the data; e.g. the last claim is assessed as different but reasonable interpretations of the same data lead to different conclusions. We consider all of the aspects mentioned (time, speaker, multiple sources and interpretations) as part of the task of fact checking. However, we want to restrict the task to statements that can be fact-checked objectively, which is not always true for the statements assessed by journalists. Therefore, we do not consider statements such as “New Labour promised social improvement but delivered a collapse in social mobility” to be part to the task since there are no universal definitions of improvement” and “social factcheck-social-mobility-collapsed/</abstract>
<note confidence="0.914591647058823">Minister Shailesh Vara) “The average criminal bar barrister working fullis earning some Channel 4 Fact Check) The figures the Ministry of Justice have stressed this week seem decidedly dodgy. Even if you do want to use the figures, once you take away the many overheads self-employed advocates have to pay you are left with a middling sum of money. U.S. Rep. Mike Rogers) “Crimea was part of Russia until 1954, when it was given to the Soviet Republic of the Ukraine.” Politifact) Rogers said Crimea belonged to Russia until 1954, when Khrushchev gave the land to Ukraine, then a Soviet republic. President Barack Obama) “For the first time in over a decade, business leaders around the world have declared that China is no longer the world’s No. 1 place to invest; America is.” Politifact) The president is accurate by citing one particular study, and that study did ask business leaders what they thought about investing in the United States. A broader look at other rankings doesn’t make the United States seem like such a power-house, even if it does still best China in some lists. Chancellor George Osborne) “Real household disposable income is rising.” Channel 4 Fact Check) RHDI did grow in latest period we know about (the second quarter of 2013), making Mr Osborne arguably right to say that it is rising as we speak. But over the last two quarters we know about, income was down 0.1 per cent. If you want to compare the latest four quarters of data with the previous four, there was a fall in household income, making the chancellor wrong. But if you compare the latest full year of results, 2012, with 2011, income is up and he’s right again. Figure 1: Fact-checked statements.</note>
<date confidence="0.401687">19</date>
<abstract confidence="0.994951303030303">3 Dataset construction In order to construct a dataset to develop and evaluate approaches to fact checking, we first surveyed popular fact checking websites. We decided to consider statements from two of them, the fact blog of Channel and the Truth-Ofrom Both websites have large archives of fact-checked statements (more than 1,000 statements each), they cover a wide range of prevalent issues of U.K. and U.S. public life, and they provide detailed verdicts with fine-grained lasuch as We examined recent fact-checks from each website at the time of writing. For each statement, apart from the statement itself, we recorded the date it was made, the speaker, the label of the verdict and the URL. As the two websites use different labelling schemes, we aligned the laof the verdicts to a five-point scale: The speakers included, apart from public figures, associations such as the American Beverage Association, activists, even viral FaceBook posts submitted by the public. We then decided which of the statements should be considered for the task proposed. As discussed in the previous section we want to avoid statements that cannot be assessed objectively. Following this, we deemed unsuitable statements: • assessing causal relations, e.g. whether a statistic should be attributed to a particular law • concerning the future, e.g. speculations involving oil prices • not concerning facts, e.g. whether a politician is supporting certain policies For the statements that were considered suitable, we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online. Automatic identification of the sources needed to fact check a statement is an important stage in the process, which is potentially useful in its own right in the context of assisting journalists in a semi-automated factapproach Cohen et al. (2011). Some- 16444 truth-o-meter/statements/ times the verdicts relied on data that were not available online such personal communications; statements whose verdict relied on such data were also deemed unsuitable for the task. As mentioned earlier, the verdicts on the websites are accompanied by lengthy analyses. While such analyses could be useful annotation for intermediate stages of the task — e.g. we could use it as supervision to learn how to combine the information extracted from the various sources into a verdict — we noticed that the language used in is indicative of the Thus we decided not to include them in the dataset, as it would enable tackling part of the task as sentiment analysis. Out of the 221 fact-checked statements examined, we judged 106 as suitable. The dataset collected including our suitability judgements is puband we are working on extending it so that it can support the development and the automatic evaluation of fact checking approaches. 4 Baseline approaches As discussed in Section 2, we consider fact checking as an ordinal classification task. Thus, in theory it would be possible to tackle it as a supervised classification task using algorithms that learn from statements annotated with the verdict labels. However this is unlikely to be successful, since statements such as the ones verified by journalists do not contain the world knowledge and the temporal and spatial context needed for this purpose. A different approach would be to match statements to ones already fact-checked by journalists and return the label in a K-nearest neighbour fash- Thus the task is reduced to assessing the semantic similarity between statements, which was explored in a recent shared task (Agirre et al., 2013). An obvious shortcoming of this approach is that it cannot be applied to new claims that have not been fact-checked, thus it can only be used to detect repetitions and paraphrases of false claims. A possible mechanism to extend the coverage of such an approach to novel statements is to assume that some large text collection is the source of all true statements. For example, Wikipedia is likely part of the analysis of the first claim in Figure 1 reads: “the full-time figure has the handy effect of stripping out the very lowest earners and bumping up the average”. andreasvlachos/resources Truth-Teller by Washington Post follows this approach. 20 to contain a statement that would match the second claim in Figure 1. However, it would still be unable to tackle the other claims mentioned, since they require calculations based on the data. 5 Discussion The main drawback of the baseline approaches mentioned (aside from their potential coverage) is the lack of interpretability of their verdicts, also referred to as algorithmic accountability (Diakopoulos, 2014). While it is possible for a natural language processing expert to inspect aspects of the prediction such as feature weights, this tends to become harder as the approaches become more sophisticated. Ultimately, the user of a fact checking system would trust a verdict only if it is accompanied by an analysis similar to the one provided by the journalists. This desideratum is present in other tasks such as the recently proposed science test question answering (Clark et al., 2013). Cohen et al. (2011) propose that fact checking is about asking the right questions. These questions might be database queries, requests for information to be extracted from textual resources, etc. For example, in checking the last claim in Figure 1 a critical reader would like to know what are the possible interpretations of “real household disposable income” and what the calculations might be for other reasonable time spans. The manual fact checking process suggests an approach that is more likely to give an interpretable analysis and would decompose the task into the following stages: 1. extract statements to be fact-checked 2. construct appropriate questions 3. obtain the answers from relevant sources 4. reach a verdict using these answers The stages of this architecture can be mapped to tasks well-explored in the natural language processing community. Statement extraction could be tackled as a sentence classification problem, following approaches similar to those proposed for speculation detection (Farkas et al., 2010) and veridicality assessment (de Marneffe et al., 2012). Furthermore, obtaining answers to questions from databases is a task typically addressed in the context of semantic parsing research, while obtaining such answers from textual sources is usually considered in the context of information extraction. Finally, the compilation of the answers into a verdict could be considered as a form of logic-based textual entailment (Bos and Markert, 2005). However, the fact-checking stages described include a novel task, namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no annotation cost, apart from the assessment of their suitability for the task. 6 Conclusions In this paper we introduced the task of fact checking and detailed the construction of a dataset using statements fact-checked by journalists available online. In addition, we discussed baseline approaches that could be applied to perform the task and the challenges that need to be addressed. Apart from being a challenging testbed to stimulate progress in natural language processing, research in fact checking is likely to inhibit the intentional or unintentional dissemination of false information. Even an approach that would return the sources related to a statement could be very helpful to journalists as well as other critical readers in a semi-automated fact checking approach. Acknowledgments The authors would like to thank the members of the Machine Reading lab for useful discussions 21 and their help in compiling the dataset.</abstract>
<note confidence="0.896344461538461">References Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez- Agirre, and Weiwei Guo. 2013. *sem 2013 shared Semantic textual similarity. In of the Second Joint Conference on Lexical and Computational Semantics and the Shared Task: Semantic pages 32–43, Atlanta, GA. Johan Bos and Katja Markert. 2005. Recognising texentailment with logical inference. In Proceedings of the 2005 Conference on Empirical Methods Natural Language Processing (EMNLP pages 628–635. Peter Clark, Philip Harrison, and Niranjan Balasubramanian. 2013. A study of the knowledge base requirements for passing an elementary science test. of the 2013 Workshop on Automated Base pages 37–42. Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu. 2011. Computational journalism: A call to arms to researchers. In of the Conferon Innovative Data Systems volume 2011, pages 148–151. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment In of the First International Conference on Machine Learning Challenges: Eval-</note>
<title confidence="0.841022">uating Predictive Uncertainty Visual Object Classiand Recognizing Textual pages</title>
<abstract confidence="0.912849142857143">177–190. Marie-Catherine de Marneffe, Christopher D. Manning, and Christopher Potts. 2012. Did it happen? the pragmatic complexity of veridicality assessment. 38(2):301–333, June. Nick Diakopoulos. 2014. Algorithmic accountability reporting: On the investigation of black boxes.</abstract>
<note confidence="0.9033621">Technical report, Tow Center for Digital Journalism. Richard Farkas, Veronika Vincze, Gyorgy Mora, Janos Csirik, and Gyorgy Szarvas. 2010. The CoNLL 2010 Shared Task: Learning to Detect Hedges and Scope in Natural Language Text. In Proceedof the CoNLL 2010 Shared Terry Flew, Anna Daniel, and Christina L. Spurgeon. 2012. The promise of computational journalism. of the Australian and New Zealand Association pages 1–</note>
<abstract confidence="0.7043325">19. Eibe Frank and Mark Hall. 2001. A simple approach ordinal classification. In of the 12th Conference on Machine pages 145–156. Luke Goode. 2009. Social news, citizen journalism democracy. Media &amp; 11(8):1287– 1305. Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with ontology matching. In of the 2013 Conference on Empirical Methods in Natu-</abstract>
<address confidence="0.6043215">Language pages 1545–1556, Seattle, WA.</address>
<author confidence="0.328944">event detection in mas-</author>
<affiliation confidence="0.881268">Ph.D. thesis, School of Informatics, University of Edinburgh.</affiliation>
<address confidence="0.668018">Matthew Richardson, Christopher J.C. Burges, and</address>
<note confidence="0.6641906">Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of In of the 2013 Conference on Em- Methods in Natural Language pages 193–203, Seattle, WA.</note>
<author confidence="0.57604">Relation extraction</author>
<note confidence="0.6310675">with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computa- Linguistics: Human Language Atlanta, GA. 22</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>32--43</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="11979" citStr="Agirre et al., 2013" startWordPosition="1913" endWordPosition="1916"> tackle it as a supervised classification task using algorithms that learn from statements annotated with the verdict labels. However this is unlikely to be successful, since statements such as the ones verified by journalists do not contain the world knowledge and the temporal and spatial context needed for this purpose. A different approach would be to match statements to ones already fact-checked by journalists and return the label in a K-nearest neighbour fashion.9 Thus the task is reduced to assessing the semantic similarity between statements, which was explored in a recent shared task (Agirre et al., 2013). An obvious shortcoming of this approach is that it cannot be applied to new claims that have not been fact-checked, thus it can only be used to detect repetitions and paraphrases of false claims. A possible mechanism to extend the coverage of such an approach to novel statements is to assume that some large text collection is the source of all true statements. For example, Wikipedia is likely 7E.g. part of the analysis of the first claim in Figure 1 reads: “the full-time figure has the handy effect of stripping out the very lowest earners and bumping up the average”. 8https://sites.google.co</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics and the Shared Task: Semantic Textual Similarity, pages 32–43, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with logical inference.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>628--635</pages>
<contexts>
<context position="15102" citStr="Bos and Markert, 2005" startWordPosition="2408" endWordPosition="2411">ity. Statement extraction could be tackled as a sentence classification problem, following approaches similar to those proposed for speculation detection (Farkas et al., 2010) and veridicality assessment (de Marneffe et al., 2012). Furthermore, obtaining answers to questions from databases is a task typically addressed in the context of semantic parsing research, while obtaining such answers from textual sources is usually considered in the context of information extraction. Finally, the compilation of the answers into a verdict could be considered as a form of logic-based textual entailment (Bos and Markert, 2005). However, the fact-checking stages described include a novel task, namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in thes</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Johan Bos and Katja Markert. 2005. Recognising textual entailment with logical inference. In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP 2005), pages 628–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Philip Harrison</author>
<author>Niranjan Balasubramanian</author>
</authors>
<title>A study of the knowledge base requirements for passing an elementary science test.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="13615" citStr="Clark et al., 2013" startWordPosition="2175" endWordPosition="2178">potential coverage) is the lack of interpretability of their verdicts, also referred to as algorithmic accountability (Diakopoulos, 2014). While it is possible for a natural language processing expert to inspect aspects of the prediction such as feature weights, this tends to become harder as the approaches become more sophisticated. Ultimately, the user of a fact checking system would trust a verdict only if it is accompanied by an analysis similar to the one provided by the journalists. This desideratum is present in other tasks such as the recently proposed science test question answering (Clark et al., 2013). Cohen et al. (2011) propose that fact checking is about asking the right questions. These questions might be database queries, requests for information to be extracted from textual resources, etc. For example, in checking the last claim in Figure 1 a critical reader would like to know what are the possible interpretations of “real household disposable income” and what the calculations might be for other reasonable time spans. The manual fact checking process suggests an approach that is more likely to give an interpretable analysis and would decompose the task into the following stages: 1. e</context>
</contexts>
<marker>Clark, Harrison, Balasubramanian, 2013</marker>
<rawString>Peter Clark, Philip Harrison, and Niranjan Balasubramanian. 2013. A study of the knowledge base requirements for passing an elementary science test. In Proceedings of the 2013 Workshop on Automated Knowledge Base Construction, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Cohen</author>
<author>Chengkai Li</author>
<author>Jun Yang</author>
<author>Cong Yu</author>
</authors>
<title>Computational journalism: A call to arms to database researchers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Innovative Data Systems Research,</booktitle>
<volume>volume</volume>
<pages>148--151</pages>
<contexts>
<context position="2066" citStr="Cohen et al., 2011" startWordPosition="313" endWordPosition="316">ely. Figure 1 shows two examples of fact checked statements, together with the verdicts offered by the journalists. Fact-checking is a time-consuming process. In assessing the first claim in Figure 1 a journalist would need to consult a variety of sources to find 1http://fullfact.org 2http://politifact.com the average “full-time earnings” for criminal barristers. Fact checking websites commonly provide the detailed analysis (not shown in the figure) performed to support the verdict. Automating the process of fact checking has recently been discussed in the context of computational journalism (Cohen et al., 2011; Flew et al., 2012). Inspired by the recent progress in natural language processing, databases and information retrieval, the vision is to provide journalists with tools that would allow them to perform this task automatically, or even render the articles “live” by updating them with most current data. This automation is further enabled by the increasing online availability of datasets, survey results, and reports in machine readable formats by various institutions, e.g. EUROSTAT releases detailed statistics for all European economies.3 Furthermore, ordinary citizens need to fact check the in</context>
<context position="10140" citStr="Cohen et al. (2011)" startWordPosition="1614" endWordPosition="1617">ces • not concerning facts, e.g. whether a politician is supporting certain policies For the statements that were considered suitable, we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online. Automatic identification of the sources needed to fact check a statement is an important stage in the process, which is potentially useful in its own right in the context of assisting journalists in a semi-automated factchecking approach Cohen et al. (2011). Some16444 5http://blogs.channel4.com/factcheck/ 6http://www.politifact.com/ truth-o-meter/statements/ times the verdicts relied on data that were not available online such personal communications; statements whose verdict relied on such data were also deemed unsuitable for the task. As mentioned earlier, the verdicts on the websites are accompanied by lengthy analyses. While such analyses could be useful annotation for intermediate stages of the task — e.g. we could use it as supervision to learn how to combine the information extracted from the various sources into a verdict — we noticed th</context>
<context position="13636" citStr="Cohen et al. (2011)" startWordPosition="2179" endWordPosition="2182">s the lack of interpretability of their verdicts, also referred to as algorithmic accountability (Diakopoulos, 2014). While it is possible for a natural language processing expert to inspect aspects of the prediction such as feature weights, this tends to become harder as the approaches become more sophisticated. Ultimately, the user of a fact checking system would trust a verdict only if it is accompanied by an analysis similar to the one provided by the journalists. This desideratum is present in other tasks such as the recently proposed science test question answering (Clark et al., 2013). Cohen et al. (2011) propose that fact checking is about asking the right questions. These questions might be database queries, requests for information to be extracted from textual resources, etc. For example, in checking the last claim in Figure 1 a critical reader would like to know what are the possible interpretations of “real household disposable income” and what the calculations might be for other reasonable time spans. The manual fact checking process suggests an approach that is more likely to give an interpretable analysis and would decompose the task into the following stages: 1. extract statements to </context>
</contexts>
<marker>Cohen, Li, Yang, Yu, 2011</marker>
<rawString>Sarah Cohen, Chengkai Li, Jun Yang, and Cong Yu. 2011. Computational journalism: A call to arms to database researchers. In Proceedings of the Conference on Innovative Data Systems Research, volume 2011, pages 148–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment,</booktitle>
<pages>177--190</pages>
<contexts>
<context position="15949" citStr="Dagan et al., 2006" startWordPosition="2544" endWordPosition="2547">rm generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no annotation cost, apart from the assessment of their suitability for the task. 6 Conclusions In this paper we introduced the task of fact checking and detailed t</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
<author>Christopher Potts</author>
</authors>
<title>Did it happen? the pragmatic complexity of veridicality assessment.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>2</issue>
<marker>de Marneffe, Manning, Potts, 2012</marker>
<rawString>Marie-Catherine de Marneffe, Christopher D. Manning, and Christopher Potts. 2012. Did it happen? the pragmatic complexity of veridicality assessment. Computational Linguistics, 38(2):301–333, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Diakopoulos</author>
</authors>
<title>Algorithmic accountability reporting: On the investigation of black boxes.</title>
<date>2014</date>
<tech>Technical report,</tech>
<institution>Tow Center for Digital Journalism.</institution>
<contexts>
<context position="13133" citStr="Diakopoulos, 2014" startWordPosition="2094" endWordPosition="2096">st earners and bumping up the average”. 8https://sites.google.com/site/ andreasvlachos/resources 9The Truth-Teller by Washington Post (http:// truthteller.washingtonpost.com/) follows this approach. 20 to contain a statement that would match the second claim in Figure 1. However, it would still be unable to tackle the other claims mentioned, since they require calculations based on the data. 5 Discussion The main drawback of the baseline approaches mentioned (aside from their potential coverage) is the lack of interpretability of their verdicts, also referred to as algorithmic accountability (Diakopoulos, 2014). While it is possible for a natural language processing expert to inspect aspects of the prediction such as feature weights, this tends to become harder as the approaches become more sophisticated. Ultimately, the user of a fact checking system would trust a verdict only if it is accompanied by an analysis similar to the one provided by the journalists. This desideratum is present in other tasks such as the recently proposed science test question answering (Clark et al., 2013). Cohen et al. (2011) propose that fact checking is about asking the right questions. These questions might be databas</context>
</contexts>
<marker>Diakopoulos, 2014</marker>
<rawString>Nick Diakopoulos. 2014. Algorithmic accountability reporting: On the investigation of black boxes. Technical report, Tow Center for Digital Journalism.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Farkas</author>
<author>Veronika Vincze</author>
<author>Gyorgy Mora</author>
<author>Janos Csirik</author>
<author>Gyorgy Szarvas</author>
</authors>
<title>The CoNLL</title>
<date>2010</date>
<booktitle>In Proceedings of the CoNLL</booktitle>
<note>Shared Task.</note>
<contexts>
<context position="14655" citStr="Farkas et al., 2010" startWordPosition="2339" endWordPosition="2342">time spans. The manual fact checking process suggests an approach that is more likely to give an interpretable analysis and would decompose the task into the following stages: 1. extract statements to be fact-checked 2. construct appropriate questions 3. obtain the answers from relevant sources 4. reach a verdict using these answers The stages of this architecture can be mapped to tasks well-explored in the natural language processing community. Statement extraction could be tackled as a sentence classification problem, following approaches similar to those proposed for speculation detection (Farkas et al., 2010) and veridicality assessment (de Marneffe et al., 2012). Furthermore, obtaining answers to questions from databases is a task typically addressed in the context of semantic parsing research, while obtaining such answers from textual sources is usually considered in the context of information extraction. Finally, the compilation of the answers into a verdict could be considered as a form of logic-based textual entailment (Bos and Markert, 2005). However, the fact-checking stages described include a novel task, namely question construction for a given statement. This task is likely to rely on se</context>
</contexts>
<marker>Farkas, Vincze, Mora, Csirik, Szarvas, 2010</marker>
<rawString>Richard Farkas, Veronika Vincze, Gyorgy Mora, Janos Csirik, and Gyorgy Szarvas. 2010. The CoNLL 2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the CoNLL 2010 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Flew</author>
<author>Anna Daniel</author>
<author>Christina L Spurgeon</author>
</authors>
<title>The promise of computational journalism.</title>
<date>2012</date>
<booktitle>In Proceedings of the Australian and New Zealand Communication Association Conference,</booktitle>
<pages>1--19</pages>
<contexts>
<context position="2086" citStr="Flew et al., 2012" startWordPosition="317" endWordPosition="320">two examples of fact checked statements, together with the verdicts offered by the journalists. Fact-checking is a time-consuming process. In assessing the first claim in Figure 1 a journalist would need to consult a variety of sources to find 1http://fullfact.org 2http://politifact.com the average “full-time earnings” for criminal barristers. Fact checking websites commonly provide the detailed analysis (not shown in the figure) performed to support the verdict. Automating the process of fact checking has recently been discussed in the context of computational journalism (Cohen et al., 2011; Flew et al., 2012). Inspired by the recent progress in natural language processing, databases and information retrieval, the vision is to provide journalists with tools that would allow them to perform this task automatically, or even render the articles “live” by updating them with most current data. This automation is further enabled by the increasing online availability of datasets, survey results, and reports in machine readable formats by various institutions, e.g. EUROSTAT releases detailed statistics for all European economies.3 Furthermore, ordinary citizens need to fact check the information provided t</context>
</contexts>
<marker>Flew, Daniel, Spurgeon, 2012</marker>
<rawString>Terry Flew, Anna Daniel, and Christina L. Spurgeon. 2012. The promise of computational journalism. In Proceedings of the Australian and New Zealand Communication Association Conference, pages 1– 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eibe Frank</author>
<author>Mark Hall</author>
</authors>
<title>A simple approach to ordinal classification.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>145--156</pages>
<contexts>
<context position="4284" citStr="Frank and Hall, 2001" startWordPosition="655" endWordPosition="658">Computational Linguistics 2 Task definition We define fact-checking to be the assignment of a truth value to a claim made in a particular context. Thus it is natural to consider it as a binary classification task. However, it is often the case that the statements are not completely true or false. For example, the verdict for the third claim in Figure 1 is MOSTLYTRUE because some of the sources dispute it, while in the fourth example the statistics can be manipulated to support or disprove the claim as desired. Therefore it is better to consider fact-checking as an ordinal classification task (Frank and Hall, 2001), thus allowing systems to capture the nuances of the task. The verdict by itself, even if graded, needs to be supported by an analysis (e.g., what is the systems interpretation of the statement). However, given the difficulty of carving out exactly what the correct analysis for a statement might be, we restrict the task to be a prediction problem so that we can evaluate performance automatically. Context can be crucial in fact-checking. For example, knowing that the fourth claim of Figure 1 is made by a UK politician is necessary in order to assess it using data about this country. Furthermor</context>
</contexts>
<marker>Frank, Hall, 2001</marker>
<rawString>Eibe Frank and Mark Hall. 2001. A simple approach to ordinal classification. In Proceedings of the 12th European Conference on Machine Learning, pages 145–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Goode</author>
</authors>
<title>Social news, citizen journalism and democracy.</title>
<date>2009</date>
<journal>New Media &amp; Society,</journal>
<volume>11</volume>
<issue>8</issue>
<pages>1305</pages>
<contexts>
<context position="2949" citStr="Goode, 2009" startWordPosition="450" endWordPosition="451">ting them with most current data. This automation is further enabled by the increasing online availability of datasets, survey results, and reports in machine readable formats by various institutions, e.g. EUROSTAT releases detailed statistics for all European economies.3 Furthermore, ordinary citizens need to fact check the information provided to them. This need is intensified with the proliferation of social media such as Twitter, since the dissemination of news and information commonly circumvents the traditional news channels (Petrovic, 2013). In addition, the rise of citizen journalism (Goode, 2009) suggests that often citizens become the sources of information. Since the information provided by them is not edited or curated, automated fact checking would assist in avoiding the spreading false information. In this paper we define the task of fact-checking. We then detail the construction of a dataset using fact-checked statements available online. Finally, we describe the challenges it poses and its relation to current research in natural language processing. 3http://epp.eurostat.ec.europa.eu/ portal/page/portal/eurostat/home 18 Proceedings of the ACL 2014 Workshop on Language Technologi</context>
</contexts>
<marker>Goode, 2009</marker>
<rawString>Luke Goode. 2009. Social news, citizen journalism and democracy. New Media &amp; Society, 11(8):1287– 1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1545--1556</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="15867" citStr="Kwiatkowski et al., 2013" startWordPosition="2531" endWordPosition="2534">to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no annotation cost, apart from the assessment of their suitability for the task. </context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545–1556, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
</authors>
<title>Real-time event detection in massive streams.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="2890" citStr="Petrovic, 2013" startWordPosition="440" endWordPosition="441">task automatically, or even render the articles “live” by updating them with most current data. This automation is further enabled by the increasing online availability of datasets, survey results, and reports in machine readable formats by various institutions, e.g. EUROSTAT releases detailed statistics for all European economies.3 Furthermore, ordinary citizens need to fact check the information provided to them. This need is intensified with the proliferation of social media such as Twitter, since the dissemination of news and information commonly circumvents the traditional news channels (Petrovic, 2013). In addition, the rise of citizen journalism (Goode, 2009) suggests that often citizens become the sources of information. Since the information provided by them is not edited or curated, automated fact checking would assist in avoiding the spreading false information. In this paper we define the task of fact-checking. We then detail the construction of a dataset using fact-checked statements available online. Finally, we describe the challenges it poses and its relation to current research in natural language processing. 3http://epp.eurostat.ec.europa.eu/ portal/page/portal/eurostat/home 18 </context>
</contexts>
<marker>Petrovic, 2013</marker>
<rawString>Sasa Petrovic. 2013. Real-time event detection in massive streams. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Christopher J C Burges</author>
<author>Erin Renshaw</author>
</authors>
<title>MCTest: A challenge dataset for the open-domain machine comprehension of text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--203</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="16001" citStr="Richardson et al., 2013" startWordPosition="2552" endWordPosition="2555">rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no annotation cost, apart from the assessment of their suitability for the task. 6 Conclusions In this paper we introduced the task of fact checking and detailed the construction of a dataset using statements fact-c</context>
</contexts>
<marker>Richardson, Burges, Renshaw, 2013</marker>
<rawString>Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193–203, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Benjamin M Marlin</author>
<author>Andrew McCallum</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="15790" citStr="Riedel et al., 2013" startWordPosition="2518" endWordPosition="2521">namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no a</context>
</contexts>
<marker>Riedel, Yao, Marlin, McCallum, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and Andrew McCallum. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, GA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
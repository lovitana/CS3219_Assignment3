<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000200">
<title confidence="0.993487">
Measuring Language Closeness by Modeling Regularity
</title>
<author confidence="0.993325">
Javad Nouri and Roman Yangarber
</author>
<affiliation confidence="0.999429">
Department of Computer Science
University of Helsinki, Finland
</affiliation>
<email confidence="0.973151">
first.last@cs.helsinki.fi
</email>
<sectionHeader confidence="0.997133" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995792">
This paper addresses the problems of mea-
suring similarity between languages—
where the term language covers any of the
senses denoted by language, dialect or lin-
guistic variety, as defined by any theory.
We argue that to devise an effective way to
measure the similarity between languages
one should build a probabilistic model that
tries to capture as much regular correspon-
dence between the languages as possible.
This approach yields two benefits. First,
given a set of language data, for any two
models, this gives a way of objectively
determining which model is better, i.e.,
which model is more likely to be accurate
and informative. Second, given a model,
for any two languages we can determine,
in a principled way, how close they are.
The better models will be better at judg-
ing similarity. We present experiments on
data from three language families to sup-
port these ideas. In particular, our results
demonstrate the arbitrary nature of terms
such as language vs. dialect, when applied
to related languages.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99217014">
In the context of building and applying NLP tools
to similar languages, language varieties, or di-
alects,1 we are interested in principled ways of
capturing the notion of language closeness.
Starting from scratch to develop resources and
tools for languages that are close to each other
is expensive; the hope is that the cost can be re-
duced by making use of pre-existing resources and
tools for related languages, which are richer in re-
sources.
1We use the term language to mean any of: language,
dialect, or linguistic variety, according to any definition.
In the context of this workshop, we assume that
we deal with some method, “Method X,” that is
applied to two (or more) related languages. For
example, Method X may involve adapting/porting
a linguistic resource from one language to another;
or may be trying to translate between the lan-
guages; etc. We also assume that the success of
Method X directly depends in some way on how
similar—or close—the languages are: that is, the
similarity between the languages is expected to be
a good predictor of how successful the application
of the method will be. Thus, in such a setting, it is
worthwhile to devote some effort to devising good
ways of measuring similarity between languages.
This is the main position of this paper.
We survey some of the approaches to measur-
ing inter-language similarity in Section 2. We as-
sume that we are dealing with languages that are
related genetically (i.e., etymologically). Related
languages may be (dis)similar on many levels; in
this paper, we focus on similarity on the lexical
level. This is admittedly a potential limitation,
since, e.g., for Method X, similarity on the level of
syntactic structure may be more relevant than sim-
ilarity on the lexical level. However, as is done in
other work, we use lexical similarity as a “general”
indicator of relatedness between the languages.2
Most of the surveyed methods begin with align-
ment at the level of individual phonetic segments
(phones), which is seen as an essential phase in
the process of evaluating similarity. Alignment
procedures are applied to the input data, which
are sets of words which are judged to be similar
(cognate)—drawn from the related languages.
Once an alignment is obtained using some
method, the natural question arises: how effective
is the particular output alignment?
Once the data is aligned (and, hopefully, aligned
</bodyText>
<footnote confidence="0.992907666666667">
2This is a well-studied subject in linguistics, with gen-
eral consensus that the lexical level has stronger resistance to
change than other levels.
</footnote>
<page confidence="0.931519">
56
</page>
<note confidence="0.653624">
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 56–65,
October 29, 2014, Doha, Qatar. (c 2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999940489795919">
well), it becomes possible to devise measures for
computing distances between the aligned words.
One of the simplest of such measures is the Leven-
shtein edit distance (LED), which is a crude count
of edit operations needed to transform one word
into one another. Averaging across LEDs between
individual word pairs gives an estimate of the dis-
tance between the languages. The question then
arises: how accurate is the obtained distance?
LED has obvious limitations. LED charges an
edit operation for substituting similar as well as
dissimilar phones—regardless of how regular (and
hence, probable) a given substitution is. Con-
versely, LED charges nothing for substituting a
phone x in language A for the same phone in lan-
guage B, even if x in A regularly (e.g., always!)
corresponds to y in B. More sophisticated variants
of LED are then proposed, which try to take into
account some aspects of the natural alignment set-
ting (such as assigning different weights to dif-
ferent edit operations, e.g., by saying that it is
cheaper to transform t into d than t into w).
Thus, in pursuit of effective similarity mea-
sures, we are faced with a sequence of steps:
procedures for aligning data produce alignments;
from the individual word-level alignments we de-
rive distance measures; averaging distances across
all words we obtain similarity measures between
languages; we then require methods for compar-
ing and validating the resulting language distance
measures. At various phases, these steps involve
subjectivity—typically in the form of gold stan-
dards. We discuss the kinds of subjectivity en-
countered with this approach in detail in Sec-
tion 2.1.
As an alternative approach, we advocate view-
ing closeness between languages in terms of regu-
larity in the data: if two languages are very close,
it means that either the differences between them
are very few, or—if they are many—then they
are very regular.3 As the number of differences
grows and their nature becomes less regular, the
languages grow more distant. The goal then is to
build probabilistic models that capture regularity
in the data; to do this, we need to devise algorithms
to discover as much regularity as possible.
This approach yields several advantages. First,
a model assigns a probability to observed data.
This has deep implications for this task, since it
</bodyText>
<footnote confidence="0.852723">
3In the former case, the differences form a short list; in the
latter, the rules describing the differences form a short list.
</footnote>
<bodyText confidence="0.999482448979592">
allows us to quantify uncertainty in a principled
fashion, rather than commit to ad-hoc decisions
and prior assumptions. We will show that prob-
abilistic modeling requires us to make fewer sub-
jective judgements. Second, the probabilities that
the models assign to data allow us to build natu-
ral distance measures. A pair of languages whose
data have a higher probability under a given model
are closer than a pair with a lower probability, in
a well-defined sense. This also allows us to de-
fine distance between individual word pairs. The
smarter the model—i.e., the more regularity it cap-
tures in the data—the more we will be able to
trust in the distance measures based on the model.
Third—and equally important for this problem
setting—this offers a principled way of comparing
methods: if model X assigns higher probability to
real data than model Y, then model X is better, and
can be trusted more. The key point here is that
we can then compare models without any “ground
truth” or gold-standard, pre-annotated data.
One way to see this is by using the model to
predict unobserved data. We can withhold one
word pair (wA, wB) from languages A and B be-
fore building the model (so the model does not
see the true correspondence); once the model is
built, show it wA, and ask what is the correspond-
ing word in B. Theoretically, this is simple: the
best guess for ˆwB is simply the one that maxi-
mizes the probability of the pair pm(wA, ˆwB) un-
der the model, over all possible strings ˆwB in B.4
Measuring the distance between wB and ˆwB tells
how good M is at predicting unseen data. Now, if
model M1 consistently predicts better than M2, it
is very difficult to argue that M1 is in any sense the
worse model; and it is able to predict better only
because it has succeeded in learning more about
the data and the regularities in it.
Thus we can compare different models for mea-
suring linguistic similarity. And this can be done
in a principled fashion—if the distances are based
on probabilistic models.
The paper is organized as follows. We continue
with a discussion of related work. In Section 3
we present one particular approach to modeling,
based on information-theoretic principles. In Sec-
tion 4 we show some applications of these models
to several linguistic data sets, from three different
language families. We conclude with plans for fu-
</bodyText>
<footnote confidence="0.9978685">
4In practice, this can be done efficiently, using heuristics
to constrain the search over all strings ˆWB in B.
</footnote>
<page confidence="0.998901">
57
</page>
<bodyText confidence="0.89882">
ture work, in Section 5.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99996575">
In this section we survey related work on similar-
ity measures between languages, and contrast the
principles on which this work relies against the
principles which we advocate.
</bodyText>
<subsectionHeader confidence="0.992702">
2.1 Subjectivity
</subsectionHeader>
<bodyText confidence="0.993769698113208">
Typically, alignment-based approaches use several
kinds of inputs that have a subjective nature.
One such input is the data itself, which is to
be aligned. For a pair of closely related di-
alects, deciding which words to align may ap-
pear “self-evident.” However, as we take di-
alects/languages that are progressively more dis-
tant, such judgements become progressively less
self-evident; therefore, in all cases, we should
keep in mind that the input data itself is a source of
subjectivity in measuring similarity based on data
that is comprised of lists of related words.
Another source of subjectivity in some of the
related work is gold-standard alignments, which
accompany the input data. Again, for very close
languages, the “correct” alignment may appear to
be obvious. However, we must recognize that this
necessarily involves subjective judgements from
the creators of the gold-standard alignment.
Further, many alignment methods pre-suppose
one-to-one correspondence between phones. On
one hand, this is due to limitations of the meth-
ods themselves (there exist methods for aligning
phones in other than one-to-one fashion); on an-
other hand, it violates accepted linguistic under-
standing that phones do not need to correspond
in a one-to-one fashion among close languages.
Another potential source of subjectivity comes in
the form of prior assumptions or restrictions on
permissible alignments.5 Another common as-
sumption is insistence on consonant-to-consonant
and vowel-to-vowel alignments. More relaxed as-
sumptions may come in the form of prior proba-
bilities of phone alignments. Although these may
appear “natural” in some sense, it is important to
keep in mind that they are ad hoc, and reflect a
subjective judgement which may not be correct.
After alignment and computation of language
distance, the question arises: which of the dis-
tance measures is more accurate? Again, one way
5One-to-one alignment is actually one such restriction.
to answer this question is to resort to gold stan-
dards. For example, this can be done via phylo-
genetic clustering; if method A says language l1
is closer to l2 than to l3, and method B says the
opposite (that l1 is closer to l3), and if we “know”
the latter to be true—from a gold standard—then
we can prefer method B. Further, if we have a
gold-standard tree for the group of languages, we
can apply tree-distance measures6 to check how
the trees generated by a given method differ from
the gold-standard. The method that deviates least
from the gold standard is then considered best.
</bodyText>
<subsectionHeader confidence="0.998912">
2.2 Levenshtein-based algorithms
</subsectionHeader>
<bodyText confidence="0.975837675675676">
The Levenshtein algorithm is a dynamic program-
ming approach for aligning a word pair (A, B) us-
ing a least expensive set of insertion, deletion and
substitution operations required for transforming
A into B. While the original Levenshtein edit dis-
tance is based on these three operations without
any restrictions, later algorithms adapt this method
by additional edit operations or restrictions.
Wieling et al. (2009) compare several align-
ment algorithms applied to dialect pronunciation
data. These algorithms include several adaptations
of the Levenshtein algorithm and the Pair Hid-
den Markov Model. They evaluate the algorithms
by comparing the resulting pairwise alignments to
alignments generated from a set of manually cor-
rected multiple alignments. Standard Levenshtein
edit distance is used for comparing the output of
each algorithm to the gold standard alignment, to
determine which algorithm is preferred.
All alignment algorithms based on Levenshtein
distance evaluated by Wieling et al. (2009) restrict
aligning vowels with consonants.
VC-sensitive Levenshtein algorithm: uses the
standard Levenshtein algorithm, prohibits aligning
vowels with consonants, and assigns unit cost for
all edit operations. The only sense in which it cap-
tures regularities is the assumption that the same
symbol in two languages represents same sound,
which results in assigning a cost of 0 to aligning
a symbol to itself. It also prevents the algorithm
from finding vowel-to-consonant correspondences
(found in some languages), such as u–v, u–l, etc.
Levenshtein algorithm with Swap: adds an edit
operation to enable the algorithm to capture phe-
nomena such as metathesis, via a transposition:
6Tree-distance measures are developed in the context of
work on phylogenetic trees in biological/genetic applications.
</bodyText>
<page confidence="0.993178">
58
</page>
<bodyText confidence="0.999418633333333">
aligning ab in A to ba in B costs a single edit oper-
ation. This algorithm also forbids aligning vowels
to consonants, except in a swap.
Levenshtein algorithm with generated segment
distances based on phonetic features: The above
algorithms assign unit cost for all edit opera-
tions, regardless of how the segments are related.
Heeringa (2004) uses a variant where the distances
are obtained from differences between phonetic
features of the segment pairs. The authors observe
that this is subjective because one could choose
from different possible feature sets.
Levenshtein algorithm with generated segment
distances based on acoustic features: To avoid
subjectivity of feature selection, Heeringa (2004)
experiments with assigning different costs to dif-
ferent segment pairs based on how phonetically
close they are; segment distances are calculated
by comparing spectrograms of recorded pronun-
ciations. These algorithms do not attempt to dis-
cover regularity in data, since they only consider
the word pair at a time, using no information about
the rest of the data.
Levenshtein algorithm with distances based on
PMI: Wieling et al. (2009) use Point-wise Mutual
Information (PMI) as the basis for segment dis-
tances. They assign different costs to segments,
and use the entire dataset for each alignment. PMI
for outcomes x and y of random variables X and
Y is defined as:
</bodyText>
<equation confidence="0.988914666666667">
p(x, y)
pmi(x, y) = log2 (1)
p(x)p(y)
</equation>
<bodyText confidence="0.991615">
PMI is calculated using estimated probabilities of
the events. Since greater PMI shows higher ten-
dency of x and y to co-occur, it is reversed and
normalized to obtain a dissimilarity measure to
be used as segment distance. Details about this
method are in (Wieling and Nerbonne, 2011).
</bodyText>
<subsectionHeader confidence="0.999001">
2.3 Other distance measures
</subsectionHeader>
<bodyText confidence="0.999790702702703">
Ellison and Kirby (2006) present a distance mea-
sure based on comparing intra-language lexica
only, arguing that there is no well-founded com-
mon language-independent phonetic space to be
used for comparing word forms across languages.
Instead, they focus on inferring the distances by
comparing how meanings in language A are likely
to be confused for each other, and comparing it to
the confusion probabilities in language B.
Given a lexicon containing mappings from a set
of meanings M to a set of forms F, confusion
probability P(m1|m2; L) for each pair of mean-
ings (m1, m2) in L is the probability of confus-
ing m1 for m2. This probability is formulated
based on an adaptation of neighborhood activation
model, and depends on the edit distance between
the corresponding forms in the lexicon. Following
this approach, they construct a confusion probabil-
ity matrix for each language, which can be viewed
as a probability distribution. Inter-language dis-
tances are then calculated as the distance between
the corresponding distributions, using symmetric
Kullback-Liebler distance and Rao distance. The
inferred distances are used to construct a phylo-
genetic tree of the Indo-European languages. The
approach is evaluated by comparing the resulting
taxonomy to a gold-standard tree, which is re-
ported to be a good fit.
As with other presented methods, although
this method can be seen as measuring distances
between languages, there remain two problems.
First, they do not reflect the genetic differences
and similarities—and regularities—between the
languages in a transparent, easily interpretable
way. Second, they offer no direct way to com-
pare competing approaches, except indirectly, and
using (subjective) gold-standards.
</bodyText>
<sectionHeader confidence="0.9904255" genericHeader="method">
3 Methods for measuring language
closeness
</sectionHeader>
<bodyText confidence="0.999981933333333">
We now discuss an approach which follows the
proposal outlined in Section 1, and allows us to
build probabilistic models for measuring closeness
between languages. Other approaches that rely on
probabilistic modeling would serve equally well.
A comprehensive survey of methods for measur-
ing language closeness may be found in (Wiel-
ing and Nerbonne, 2015). Work that is proba-
bilistically oriented, similarly to our proposed ap-
proaches, includes (Bouchard-Cˆot´e et al., 2007;
Kondrak, 2004) and others. We next review two
types of models (some of which are described
elsewhere), which are based on information-
theoretic principles. We discuss how these models
suit the proposed approach, in the next section.
</bodyText>
<subsectionHeader confidence="0.998547">
3.1 1-1 symbol model
</subsectionHeader>
<bodyText confidence="0.9994526">
We begin with our “basic” model, described
in (Wettig and Yangarber, 2011; Wettig et
al., 2011), which makes several simplifying
assumptions—which the subsequent, more ad-
vanced models relax (Wettig et al., 2012; Wettig
</bodyText>
<page confidence="0.994729">
59
</page>
<bodyText confidence="0.999654035714286">
et al., 2013).7 The basic model is based on align-
ment, similarly to much of the related work men-
tioned above: for every word pair in our data set—
the “corpus”—it builds a complete alignment for
all symbols (Wettig et al., 2011). The basic model
considers pairwise alignments only, i.e., two lan-
guages at a time; we call them the source and
the target languages. Later models relax this re-
striction by using N-dimensional alignment, with
N &gt; 2 languages aligned simultaneously. The
basic model allows only 1-1 symbol alignments:
one source symbol8 may correspond to one tar-
get symbol—or to the empty symbol E (which we
mark as “:”). More advanced models align sub-
strings of more than one symbol to each other. The
basic model also ignores context, whereas in re-
ality symbol correspondences are heavily condi-
tioned on their context. Finally, the basic model
treats the symbols as atoms, whereas more ad-
vanced models treat the symbols as vectors of dis-
tinctive features.
We distinguish between the raw, observed data
and complete data—i.e., complete with the align-
ment; the hidden data is where the insertions and
deletions occur. For example, if we ask what is
the “correct” alignment between Finnish vuosi and
Khanty al (cognate words from these two Uralic
languages, both meaning “year”):
</bodyText>
<equation confidence="0.840696">
v u o . s i v u o s i
             ||
. a . l . . . . a l .
</equation>
<bodyText confidence="0.999873">
are two possible alignments, among many oth-
ers. From among all alignments, we seek the best
alignment: one that is globally optimal, i.e., one
that is consistent with as many regular sound cor-
respondences as possible. This leads to a chicken-
and-egg problem: on one hand, if we had the best
alignment for the data, we could simply read off
a set of rules, by observing which source sym-
bol corresponds frequently to which target sym-
bol. On the other hand, if we had a complete
set of rules, we could construct the best align-
ment, by using dynamic programming (`a la one of
the above mentioned methods, since the costs of
all possible edit operations are determined by the
rules). Since at the start we have neither, the rules
and the alignment are bootstrapped in tandem.
</bodyText>
<footnote confidence="0.99573625">
7The models can be downloaded from ety-
mon.cs.helsinki.fi
8In this paper, we equate symbols with sounds: we assume
our data to be given in phonetic transcription.
</footnote>
<bodyText confidence="0.992064958333333">
Following the Minimum Description Length
(MDL) principle, the best alignment is the one that
can be encoded (i.e., written down) in the shortest
space. That is, we aim to code the complete data—
for all word pairs in the given language pair—as
compactly as possible. To find the optimal align-
ment, we need A. an objective function—a way to
measure the quality of any given alignment—and
B. a search algorithm, to sift through all possible
alignments for one that optimizes the objective.
We can use various methods to code the com-
plete data. Essentially, they all amount to measur-
ing how many bits it costs to “transmit” the com-
plete set of alignment “events”, where each align-
ment event e is a pair of aligned symbols (σ : τ)
e = (σ : τ) E E U I.,#} xTUI.,#}
drawn from the source alphabet E and the target
alphabet T, respectively.9 One possible coding
scheme is “prequential” coding, or the Bayesian
marginal likelihood, see, e.g., (Kontkanen et al.,
1996), used in (Wettig et al., 2011); another is nor-
malized maximum likelihood (NML) code, (Ris-
sanen, 1996), used in (Wettig et al., 2012).
Prequential coding gives the total code length
</bodyText>
<equation confidence="0.9991144">
ELbase(D) = − log c(e)!
e∈E
+ logIeEE
E c(e) + K − 1J ! − log(K − 1)! (2)
E
</equation>
<bodyText confidence="0.9999730625">
for data D. Here, c(e) denotes the event count,
and K is the total number of event types.
To find the optimal alignments, the algorithm
starts with aligning word pairs randomly, and then
iteratively searching for the best alignment given
rest of the data for each word pair at a time. To do
this, we first exclude the current alignment from
our complete data. The best alignment in the re-
aligning process is found using a Dynamic Pro-
gramming matrix, with source word symbols in
the rows and target word symbols as the columns.
Each possible alignment of the word pair corre-
sponds to a path from top-left cell of the matrix
to the bottom-right cell. Each cell V (σi, τj) holds
the cost of aligning sub-string σ1..σi with τ1..τj,
and is computed as:
</bodyText>
<equation confidence="0.999352666666667">
V (σi, Tj−1) +L(.: Tj)
V (σi,τj) = min V (σi−1, Tj) +L(σi :.) (3)
V (σi−1, Tj−1) +L(σi : Tj)
</equation>
<bodyText confidence="0.96899775">
9Note, that the alphabets need not be the same, or even
have any symbols in common. We add a special end-of-word
symbol, always aligned to itself: (# : #). Empty alignments
(.:.) are not allowed.
</bodyText>
<page confidence="0.996529">
60
</page>
<bodyText confidence="0.999912">
where L(e) is the cost of coding event e. The cost
of aligning the full word pair, is then found in the
bottom-right cell, and the corresponding path is
chosen as the new alignment, which is registered
back into the complete data.
We should mention that due to vulnerability of
the algorithm to local optima, we use simulated
annealing with (50) random restarts.
</bodyText>
<subsectionHeader confidence="0.997338">
3.2 Context model
</subsectionHeader>
<bodyText confidence="0.999900652173913">
Context model is described in detail in (Wettig et
al., 2013). We use a modified version of this model
to achieve faster run-time.
One limitation of the basic model described
above is that it uses no information about the con-
text of the sounds, thus ignoring the fact that lin-
guistic sound change is regular and highly depends
on context. The 1-1 model also treats symbols of
the words as atoms, ignoring how two sounds are
phonetically close. The context model, addresses
both of these issues.
Each sound is represented as a vector of distinc-
tive phonetic features. Since we are using MDL as
the basis of the model here, we need to code (i.e.,
transmit) the data. This can be done by coding one
feature at a time on each level.
To code a feature F on a level L, we construct a
decision tree. First, we collect all instances of the
sounds in the data of the corresponding level that
have the current feature, and then build a count
matrix based on how many instances take each
value. Here is an example of such a matrix for
feature V (vertical articulation of a vowel).
</bodyText>
<table confidence="0.745082">
V Close Mid-close Mid-open Open
10 25 33 24
</table>
<bodyText confidence="0.999350361111111">
This shows that there are 10 close vowels, 25
mid-close vowels, etc.
This serves as the root node of the tree. The tree
can then query features of the sounds in the current
context by choosing from a set of candidate con-
texts. Each candidate is a triplet (L, P, F), repre-
senting Level, Position, and Feature respectively.
L can be either source or target, since we are
dealing with a pair of language varieties at a time.
P is the position of the sound that is being queried
relative to current sound, and F is the feature be-
ing queried. Examples of a Position are previ-
ous vowel, previous position, itself, etc. The tree
expands depending on the possible responses to
the query, resulting in child nodes with their own
count matrix. The idea here is to make the matri-
ces in the child nodes as sparse as possible in order
to code them with fewer bits.
This process continues until the tree cannot be
expanded any more. Finally the data in each leaf
node is coded using prequential coding as before
with the same cost explained in Equation 2.
Code length for the complete data consists of
cost of encoding the trees and the cost of encoding
the data given the trees. The search algorithm re-
mains the same as the 1-1 algorithm, but uses the
constructed trees to calculate the cost of events.
This method spends much time rebuilding the
trees on each iteration; its run-time is very high.
In the modified version used in this paper, the trees
are not allowed to expand initially, when the model
has just started and everything is random due to
simulated annealing. Once the simulated anneal-
ing phase is complete, the trees are expanded fully
normally. Our experiments show that this results
in trees that are equally good as the original ones.
</bodyText>
<subsectionHeader confidence="0.98887">
3.3 Normalized Compression Distance
</subsectionHeader>
<bodyText confidence="0.996193583333333">
The cost of coding the data for a language pair un-
der a model reflects the amount of regularity the
model discovered, and thus is a means of measur-
ing the distance between these languages. How-
ever the cost also depends on the size of the data
for the language pair; thus, a way of normalizing
the cost is needed to make them comparable across
language pairs. We use “Normalized Compres-
sion Distance” (NCD), described in (Cilibrasi and
Vitanyi, 2005) to achieve this.
Given a model that can compress a language
pair (a, b) with cost C(a, b), NCD of (a, b) is:
</bodyText>
<equation confidence="0.994348333333333">
C(a, b) − min (C(a), C(b))
NCD(a, b) = (4)
max (C(a), C(b))
</equation>
<bodyText confidence="0.999214333333333">
Since NCD of different pairs are comparable un-
der the same model, it can be used as a distance
measure between language varieties.
</bodyText>
<subsectionHeader confidence="0.999167">
3.4 Prediction of unobserved data
</subsectionHeader>
<bodyText confidence="0.9998917">
The models mentioned above are also able to
predict unobserved data as described in Sec-
tion 1 (Wettig et al., 2013).
For the basic 1-1 model, since no informa-
tion about the context is used, prediction sim-
ply means looking for the most probable symbol
in target language for each symbol of wA. For
the context model, a more sophisticated dynamic-
programming heuristic is needed to predict the un-
seen word, (Hiltunen, 2012). The predicted word
</bodyText>
<page confidence="0.998935">
61
</page>
<figureCaption confidence="0.999989">
Figure 1: Model comparison: MDL costs. Figure 2: Model comparison: NFED.
</figureCaption>
<bodyText confidence="0.988785375">
ˆWB is then compared to the real corresponding
word WB to measure how well the model per-
formed on the task.
Feature-wise Levenshtein edit distance is used
for this comparison. The edit distances for all
word pairs are normalized, resulting in Normal-
ized Feature-wise Edit Distance (NFED) which
can serve as a measure of model quality.
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999658">
To illustrate the principles discussed above, we
experiment with the two principal model types de-
scribed above—the baseline 1-1 model and the
context-sensitive model, using data from three dif-
ferent language families.
</bodyText>
<subsectionHeader confidence="0.995695">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999979411764706">
We use data from the StarLing data bases,
(Starostin, 2005), for the Turkic and Uralic lan-
guage families, and for the Slavic branch of the
Indo-European family. For dozens of language
families, StarLing has rich data sets (going be-
yond Swadesh-style lists, as in some other lexical
data collections built for judging language and di-
alect distances). The databases are under constant
development, and have different quality. Some
datasets, (most notably the IE data) are drawn
from multiple sources, which use different nota-
tion, transcription, etc., and are not yet unified.
The data we chose for use is particularly clean.
For the Turkic family, StarLing at present con-
tains 2017 cognate sets; we use 19 (of the total 27)
languages, which have a substantial amount of at-
tested word-forms in the data collection.
</bodyText>
<subsectionHeader confidence="0.998906">
4.2 Model comparison
</subsectionHeader>
<bodyText confidence="0.999982064516129">
We first demonstrate how the “best” model can be
chosen from among several models, in a principled
way. This is feasible if we work with probabilis-
tic models—models that assign probabilities to the
observed data. If the model is also able to perform
prediction (of unseen data), then we can measure
the model’s predictive power and select the best
model using predictive power as the criterion. We
will show that in the case of the two probabilistic
models presented above, these two criteria yield
the same result.
We ran the baseline 1-1 model and the context
model against the entire Turkic dataset, i.e., the
19 × 18 language pairs,10 (with 50 restarts for
each pair, a total of 17100 runs). For each lan-
guage pair, we select the best out of 50 runs for
each model, according to the cost it assigns to this
language pair. Figure 1 shows the costs obtained
by the best run: each point denotes a language
pair; X-coordinate is the cost according to the 1-
1 model, Y-coordinate is the cost of the context
model. The Figure shows that all 19×18 points lie
below the diagonal (x=y), i.e., for every language
pair, the context model finds a code with lower
cost—as is expected, since the context model is
“smarter,” uses more information from the data,
and hence finds more regularity in it.
Next, for each language pair, we take the run
that found the lowest cost, and use it to impute
unseen data, as explained in Section 3—yielding
NFED, the distance from the imputed string to the
</bodyText>
<footnote confidence="0.697095833333333">
10Turkic languages in tables and figures are:
azb:Azerbaijani, bas:Bashkir, blk:Balkar, chv:Chuvash,
hak:Khakas, jak:Yakut, kaz:Kazakh, krg:Kyrgyz, nog:Nogaj,
qum:Qumyk, shr:Shor, sjg:Sary Uyghur, tat:Tatar,
tof:Tofalar, trk:Turkish, trm:Turkmen, tuv:Tuva, uig:Uyghur,
uzb:Uzbek.
</footnote>
<page confidence="0.996096">
62
</page>
<table confidence="0.9991446">
ru ukr cz slk pl usrb lsrb bulg scr
ru 0 .41 .41 .39 .41 .51 .53 .48 .40
ukr .41 0 .48 .46 .51 .49 .50 .48 .47
cz .40 .48 0 .29 .38 .45 .52 .50 .39
slk .38 .45 .29 0 .38 .41 .44 .45 .38
pl .43 .51 .39 .41 0 .48 .50 .52 .45
usrb .50 .48 .44 .40 .46 0 .29 .49 .48
lsrb .52 .51 .49 .44 .47 .30 0 .52 .50
bulg .46 .47 .48 .45 .51 .47 .49 0 .41
scr .40 .47 .38 .38 .43 .49 .51 .44 0
</table>
<tableCaption confidence="0.753012">
Table 1: NCDs for 9 Slavic languages, StarLing
database: context model
</tableCaption>
<bodyText confidence="0.997389925925926">
actual, correct string in the target language. This
again yields 19×18 points, shown in Figure 2; this
time the X and Y values lie between 0 and 1, since
NFED is normalized. (In the figure, the points are
linked with line segments as follows: for any pair
(a,b) the point (a,b) is joined by a line to the point
(b,a). This is done for easier identification, since
the point (a,b) displays the legend symbol for only
language a.) Overall, many more points lie below
the diagonal, (approximately 10% of the points are
above). The context model performs better, and it
would therefore be a safer/wiser choice, if we wish
to measure language closeness; which agrees with
the result obtained using raw compression costs.
The key point here is that this compari-
son method can accommodate any probabilistic
model: for any new candidate model we check—
over the same datasets—what probability values
does the model assign to each data point. Probabil-
ities and (compression) costs are interchangeable:
information theory tells us that for a data set D and
model M, the probability P of data D under model
M and the cost (code length) L of D under M are
related by: LM(D) = − log PM(D). If the new
model assigns higher probability (or lower cost) to
observed data, it is preferable—obviating the need
for gold-standards, or subjective judgements.
</bodyText>
<subsectionHeader confidence="0.999647">
4.3 Language closeness
</subsectionHeader>
<bodyText confidence="0.789313090909091">
We next explore various datasets using the context
model—the better model we have available.
Uralic: We begin with Uralic data from Star-
Ling.11 The Uralic database contains data from
more than one variant of many languages: we ex-
tracted data for the top two dialects—in terms of
counts of available word-forms—for Komi, Ud-
11We use data from the Finno-Ugric sub-family. The
language codes are: est:Estonian, fin:Finnish, khn:Khanty,
kom:Komi, man:Mansi, mar:Mari, mrd:Mordva, saa:Saami,
udm:Udmurt.
</bodyText>
<table confidence="0.998639117647059">
Language pair NCD
kom s kom p .18
kom p kom s .19
udm s udm g .20
udm g udm s .21
mar b mar kb .28
mar kb mar b .28
mrd m mrd e .29
mrd e mrd m .29
est fin .32
fin est .32
man p man so .34
khn v khn dn .35
khn dn khn v .36
man so man p .36
saa n saa l .37
saa l saa n .37
</table>
<tableCaption confidence="0.929766">
Table 2: Comparison of Uralic dialect/language
pairs, sorted by NCD: context model.
</tableCaption>
<bodyText confidence="0.94955571875">
murt, Mari, Mordva, Mansi, Khanty and Saami.
Table 2 shows the normalized compression dis-
tances for each of the pairs; the NCD costs for
Finnish and Estonian are given for comparison.
It is striking that the pairs that score below
Finnish/Estonian are all “true” dialects, whereas
those that score above are not. E.g., the Mansi
variants Pelym and Sosva, (Honti, 1998), and
Demjanka and Vakh Khanty, (Abondolo, 1998),
are mutually unintelligible. The same is true for
North and Lule Saami.
Turkic: We compute NCDs for the Turkic lan-
guages under the context model. Some of the Tur-
kic languages are known to form a much tighter
dialect continuum, (Johanson, 1998), which is ev-
ident from the NCDs in Table 3. E.g., Tofa is most-
closely related to the Tuvan language and forms a
dialect continuum with it, (Johanson, 1998). Turk-
ish and Azerbaijani closely resemble each other
and are mutually intelligible. In the table we high-
light language pairs with NCD ≤ 0.30.
Slavic: We analyzed data from StarLing for 9
Slavic languages.12 The NCDs are shown in Ta-
ble 1. Of all pairs, the normalized compression
costs for (cz, slk) and (lsrb, usrb) fall below the
.30 mark, and indeed these pairs have high mutual
intelligibility, unlike all other pairs.
When the data from Table 1 are fed into
the NeighborJoining algorithm, (Saitou and Nei,
1987), it draws the phylogeny in Figure 3, which
clearly separates the languages into the 3 ac-
cepted branches of Slavic: East (ru, ukr), South
</bodyText>
<footnote confidence="0.899408333333333">
12The Slavic languages from StarLing: bulg:Bulgarian,
cz:Czech, pl:Polish, ru:Russian, slk:Slovak, scr:Serbo-
Croatian, ukr:Ukrainian, lsrb/usrb:Lower and Upper Sorbian.
</footnote>
<page confidence="0.996442">
63
</page>
<table confidence="0.8839895">
SCR
BULG
RU
UKR
SLK
CZ
PL
USRB
LSRB
0 0.05 0.1 0.15 0.2 0.25
</table>
<figureCaption confidence="0.992972">
Figure 3: NeighborJoining tree for Slavic lan-
guages in Table 1.
</figureCaption>
<bodyText confidence="0.996771">
(scr, bulg) and West (pl, cz, slk, u/lsrb). The
phylogeny also supports later separation (rela-
tive time depth &gt; 0.05) of the pairs with higher
mutual intelligibility—Upper/Lower Sorbian, and
Czech/Slovak.13
</bodyText>
<sectionHeader confidence="0.996914" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9999594">
We have presented a case for using probabilistic
modeling when we need reliable quantitative mea-
sures of language closeness. Such needs arise, for
example, when one attempts to develop methods
whose success directly depends on how close the
languages in question are. We attempt to demon-
strate two main points. One is that using proba-
bilistic models provides a principled and natural
way of comparing models—to determine which
candidate model we can trust more when mea-
suring how close the languages are. It also lets
us compare models without having to build gold-
standard datasets; this is important, since gold-
standards are subjective, not always reliable, and
expensive to produce. We are really interested in
regularity, and the proof of the model’s quality is
in its ability to assign high probability to observed
and unobserved data.
The second main point of the paper is show-
ing how probabilistic models can be employed to
measure language closeness. Our best-performing
model seems to provide reasonable judgements
of closeness when applied to languages/linguistic
variants from very different language families. For
all of Uralic, Turkic and Slavic data, those that fell
</bodyText>
<footnote confidence="0.550447333333333">
13We should note that the NCDs produce excellent phylo-
genies also for the Turkic and Uralic data; not included here
due to space constraints.
</footnote>
<bodyText confidence="0.9999838">
below the 0.30 mark on the NCD axis are known
to have higher mutual intelligibility, while those
that are above the mark have lower or no mutual
intelligibility. Of course, we do not claim that 0.30
is a magic number; for a different model the line
of demarcation may fall elsewhere entirely. How-
ever, it shows that the model (which we selected
on the basis of its superiority according to our se-
lection criteria) is quite consistent in predicting the
degree of mutual intelligibility, overall.
Incidentally, these experiments demonstrate, in
a principled fashion, the well-known arbitrary na-
ture of the terms language vs. dialect—this dis-
tinction is simply not supported by real linguis-
tic data. More importantly, probabilistic methods
require us to make fewer subjective judgements,
with no ad hoc priors or gold-standards, which in
many cases are difficult to obtain and justify—and
rather rely on the observed data as the ultimate and
sufficient truth.
</bodyText>
<sectionHeader confidence="0.999084" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9857022">
This research was supported in part by: the
FinUgRevita Project of the Academy of Finland,
and by the National Centre of Excellence “Al-
gorithmic Data Analysis (ALGODAN)” of the
Academy of Finland.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999800260869565">
Daniel Abondolo. 1998. Khanty. In Daniel Abon-
dolo, editor, The Uralic Languages, pages 358–386.
Routledge.
Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Grif-
fiths, and Dan Klein. 2007. A probabilistic ap-
proach to diachronic phonology. In Proceedings of
the Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL:2007),
pages 887–896, Prague, Czech Republic.
Rudi Cilibrasi and Paul M.B. Vitanyi. 2005. Cluster-
ing by compression. IEEE Transactions on Infor-
mation Theory, 51(4):1523–1545.
T. Mark Ellison and Simon Kirby. 2006. Measuring
language divergence by intra-lexical comparison. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics (COLING/ACL-2006), pages 273–280, Syd-
ney, Australia.
Wilbert Heeringa. 2004. Measuring Dialect Pronunci-
ation Differences using Levenshtein Distance. Ph.D.
thesis, Rijksuniversiteit Groningen.
</reference>
<page confidence="0.99865">
64
</page>
<table confidence="0.59085915">
azb bas blk chv hak jak kaz krg nog qum shr sjg tat tof trk trm tuv uig uzb
azb 0 .40 .35 .60 .48 .59 .38 .39 .35 .34 .43 .41 .38 .45 .27 .33 .46 .40 .37
bas .39 0 .28 .58 .45 .59 .27 .31 .24 .26 .40 .41 .21 .46 .39 .38 .45 .36 .34
blk .35 .30 0 .57 .42 .57 .26 .28 .22 .19 .36 .40 .27 .42 .34 .36 .42 .35 .30
chv .59 .59 .56 0 .62 .67 .56 .58 .55 .53 .60 .57 .56 .60 .56 .60 .62 .61 .58
hak .47 .44 .41 .63 0 .58 .41 .40 .37 .40 .27 .43 .43 .39 .46 .50 .40 .46 .46
jak .57 .57 .57 .70 .58 0 .56 .57 .55 .54 .55 .54 .57 .51 .58 .57 .56 .58 .57
kaz .38 .28 .27 .57 .42 .57 0 .24 .16 .24 .38 .39 .29 .44 .37 .39 .41 .36 .33
krg .38 .31 .27 .60 .40 .57 .23 0 .21 .26 .35 .40 .32 .41 .36 .39 .40 .35 .33
nog .35 .25 .22 .57 .39 .55 .15 .22 0 .19 .36 .38 .26 .43 .33 .35 .41 .35 .31
qum .34 .27 .19 .57 .41 .55 .23 .26 .19 0 .35 .37 .26 .41 .33 .35 .41 .33 .31
shr .43 .40 .36 .63 .28 .55 .38 .36 .35 .34 0 .40 .40 .36 .43 .44 .38 .42 .42
sjg .43 .42 .41 .58 .45 .55 .40 .41 .39 .38 .40 0 .42 .44 .43 .43 .43 .41 .41
tat .36 .22 .27 .60 .44 .59 .28 .32 .26 .26 .40 .41 0 .45 .38 .38 .45 .36 .33
tof .47 .45 .42 .61 .39 .50 .42 .42 .42 .41 .36 .42 .45 0 .48 .46 .24 .44 .43
trk .28 .40 .35 .58 .48 .59 .37 .36 .33 .34 .43 .42 .39 .47 0 .34 .46 .40 .38
trm .32 .40 .36 .62 .51 .59 .39 .40 .36 .35 .44 .43 .39 .46 .34 0 .49 .41 .36
tuv .46 .46 .41 .63 .40 .56 .41 .40 .41 .41 .38 .42 .45 .23 .45 .48 0 .45 .46
uig .40 .39 .34 .60 .49 .58 .36 .36 .36 .33 .43 .40 .38 .45 .41 .42 .46 0 .33
uzb .37 .36 .31 .60 .48 .58 .34 .34 .32 .32 .43 .41 .34 .44 .38 .36 .47 .33 0
</table>
<tableCaption confidence="0.996813">
Table 3: Normalized compression distances for 19 Turkic languages (StarLing database): context model
</tableCaption>
<reference confidence="0.998785892307692">
Suvi Hiltunen. 2012. Minimum description length
modeling of etymological data. Master’s thesis,
University of Helsinki.
L´aszl´o Honti. 1998. Ob’ Ugrian. In Daniel Abon-
dolo, editor, The Uralic Languages, pages 327–357.
Routledge.
Lars Johanson. 1998. The history of Turkic. In
Lars Johanson &amp; ´Eva ´Agnes Csat´o, editor, The Tur-
kic Languages, pages 81–125. London, New York:
Routledge. Classification of Turkic languages (at
Turkiclanguages.com).
Grzegorz Kondrak. 2004. Combining evidence in
cognate identification. In Proceedings of the Sev-
enteenth Canadian Conference on Artificial Intelli-
gence (Canadian AI 2004), pages 44–59, London,
Ontario. Lecture Notes in Computer Science 3060,
Springer-Verlag.
Petri Kontkanen, Petri Myllym¨aki, and Henry Tirri.
1996. Constructing Bayesian finite mixture models
by the EM algorithm. Technical Report NC-TR-97-
003, ESPRIT NeuroCOLT: Working Group on Neu-
ral and Computational Learning.
Jorma Rissanen. 1996. Fisher information and
stochastic complexity. IEEE Transactions on Infor-
mation Theory, 42(1):40–47.
Naruya Saitou and Masatoshi Nei. 1987. The
neighbor-joining method: a new method for recon-
structing phylogenetic trees. Molecular biology and
evolution, 4(4):406–425.
Sergei A. Starostin. 2005. Tower of Babel: StarLing
etymological databases. http://newstar.rinet.ru/.
Hannes Wettig and Roman Yangarber. 2011. Proba-
bilistic models for alignment of etymological data.
In Proceedings of NoDaLiDa: the 18th Nordic Con-
ference on Computational Linguistics, Riga, Latvia.
Hannes Wettig, Suvi Hiltunen, and Roman Yangarber.
2011. MDL-based Models for Alignment of Et-
ymological Data. In Proceedings of RANLP: the
8th Conference on Recent Advances in Natural Lan-
guage Processing, Hissar, Bulgaria.
Hannes Wettig, Kirill Reshetnikov, and Roman Yan-
garber. 2012. Using context and phonetic features
in models of etymological sound change. In Proc.
EACL Workshop on Visualization of Linguistic Pat-
terns and Uncovering Language History from Mul-
tilingual Resources, pages 37–44, Avignon, France.
Hannes Wettig, Javad Nouri, Kirill Reshetnikov, and
Roman Yangarber. 2013. Information-theoretic
modeling of etymological sound change. In Lars
Borin and Anju Saxena, editors, Approaches to mea-
suring linguistic differences, volume 265 of Trends
in Linguistics, pages 507–531. de Gruyter Mouton.
Martijn Wieling and John Nerbonne. 2011. Measur-
ing linguistic variation commensurably. In Dialec-
tologia Special Issue II: Production, Perception and
Attitude, pages 141–162.
Martijn Wieling and John Nerbonne. 2015. Advances
in dialectometry. In Annual Review of Linguistics,
volume 1. To appear.
Martijn Wieling, Jelena Proki´c, and John Nerbonne.
2009. Evaluating the pairwise string alignment of
pronunciations. In Proceedings of the EACL 2009
Workshop on Language Technology and Resources
for Cultural Heritage, Social Sciences, Humanities,
and Education, pages 26–34, Athens, Greece.
</reference>
<page confidence="0.999605">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959208">
<title confidence="0.999931">Measuring Language Closeness by Modeling Regularity</title>
<author confidence="0.997923">Nouri</author>
<affiliation confidence="0.9999565">Department of Computer University of Helsinki,</affiliation>
<email confidence="0.996329">first.last@cs.helsinki.fi</email>
<abstract confidence="0.998626769230769">This paper addresses the problems of measuring similarity between languages— the term any of the denoted by linas defined by any theory. We argue that to devise an effective way to measure the similarity between languages one should build a probabilistic model that tries to capture as much regular correspondence between the languages as possible. This approach yields two benefits. First, given a set of language data, for any two models, this gives a way of objectively determining which model is better, i.e., which model is more likely to be accurate and informative. Second, given a model, for any two languages we can determine, in a principled way, how close they are. The better models will be better at judging similarity. We present experiments on data from three language families to support these ideas. In particular, our results demonstrate the arbitrary nature of terms as when applied to related languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Abondolo</author>
</authors>
<date>1998</date>
<booktitle>The Uralic Languages,</booktitle>
<pages>358--386</pages>
<editor>Khanty. In Daniel Abondolo, editor,</editor>
<publisher>Routledge.</publisher>
<contexts>
<context position="33263" citStr="Abondolo, 1998" startWordPosition="5602" endWordPosition="5603">32 fin est .32 man p man so .34 khn v khn dn .35 khn dn khn v .36 man so man p .36 saa n saa l .37 saa l saa n .37 Table 2: Comparison of Uralic dialect/language pairs, sorted by NCD: context model. murt, Mari, Mordva, Mansi, Khanty and Saami. Table 2 shows the normalized compression distances for each of the pairs; the NCD costs for Finnish and Estonian are given for comparison. It is striking that the pairs that score below Finnish/Estonian are all “true” dialects, whereas those that score above are not. E.g., the Mansi variants Pelym and Sosva, (Honti, 1998), and Demjanka and Vakh Khanty, (Abondolo, 1998), are mutually unintelligible. The same is true for North and Lule Saami. Turkic: We compute NCDs for the Turkic languages under the context model. Some of the Turkic languages are known to form a much tighter dialect continuum, (Johanson, 1998), which is evident from the NCDs in Table 3. E.g., Tofa is mostclosely related to the Tuvan language and forms a dialect continuum with it, (Johanson, 1998). Turkish and Azerbaijani closely resemble each other and are mutually intelligible. In the table we highlight language pairs with NCD ≤ 0.30. Slavic: We analyzed data from StarLing for 9 Slavic lang</context>
</contexts>
<marker>Abondolo, 1998</marker>
<rawString>Daniel Abondolo. 1998. Khanty. In Daniel Abondolo, editor, The Uralic Languages, pages 358–386. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Percy Liang</author>
<author>Thomas Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>A probabilistic approach to diachronic phonology.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL:2007),</booktitle>
<pages>887--896</pages>
<location>Prague, Czech Republic.</location>
<marker>Bouchard-Cˆot´e, Liang, Griffiths, Klein, 2007</marker>
<rawString>Alexandre Bouchard-Cˆot´e, Percy Liang, Thomas Griffiths, and Dan Klein. 2007. A probabilistic approach to diachronic phonology. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL:2007), pages 887–896, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi Cilibrasi</author>
<author>Paul M B Vitanyi</author>
</authors>
<title>Clustering by compression.</title>
<date>2005</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>51</volume>
<issue>4</issue>
<contexts>
<context position="26155" citStr="Cilibrasi and Vitanyi, 2005" startWordPosition="4356" endWordPosition="4359">s complete, the trees are expanded fully normally. Our experiments show that this results in trees that are equally good as the original ones. 3.3 Normalized Compression Distance The cost of coding the data for a language pair under a model reflects the amount of regularity the model discovered, and thus is a means of measuring the distance between these languages. However the cost also depends on the size of the data for the language pair; thus, a way of normalizing the cost is needed to make them comparable across language pairs. We use “Normalized Compression Distance” (NCD), described in (Cilibrasi and Vitanyi, 2005) to achieve this. Given a model that can compress a language pair (a, b) with cost C(a, b), NCD of (a, b) is: C(a, b) − min (C(a), C(b)) NCD(a, b) = (4) max (C(a), C(b)) Since NCD of different pairs are comparable under the same model, it can be used as a distance measure between language varieties. 3.4 Prediction of unobserved data The models mentioned above are also able to predict unobserved data as described in Section 1 (Wettig et al., 2013). For the basic 1-1 model, since no information about the context is used, prediction simply means looking for the most probable symbol in target lang</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2005</marker>
<rawString>Rudi Cilibrasi and Paul M.B. Vitanyi. 2005. Clustering by compression. IEEE Transactions on Information Theory, 51(4):1523–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
<author>Simon Kirby</author>
</authors>
<title>Measuring language divergence by intra-lexical comparison.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-2006),</booktitle>
<pages>273--280</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="15212" citStr="Ellison and Kirby (2006)" startWordPosition="2460" endWordPosition="2463">g et al. (2009) use Point-wise Mutual Information (PMI) as the basis for segment distances. They assign different costs to segments, and use the entire dataset for each alignment. PMI for outcomes x and y of random variables X and Y is defined as: p(x, y) pmi(x, y) = log2 (1) p(x)p(y) PMI is calculated using estimated probabilities of the events. Since greater PMI shows higher tendency of x and y to co-occur, it is reversed and normalized to obtain a dissimilarity measure to be used as segment distance. Details about this method are in (Wieling and Nerbonne, 2011). 2.3 Other distance measures Ellison and Kirby (2006) present a distance measure based on comparing intra-language lexica only, arguing that there is no well-founded common language-independent phonetic space to be used for comparing word forms across languages. Instead, they focus on inferring the distances by comparing how meanings in language A are likely to be confused for each other, and comparing it to the confusion probabilities in language B. Given a lexicon containing mappings from a set of meanings M to a set of forms F, confusion probability P(m1|m2; L) for each pair of meanings (m1, m2) in L is the probability of confusing m1 for m2.</context>
</contexts>
<marker>Ellison, Kirby, 2006</marker>
<rawString>T. Mark Ellison and Simon Kirby. 2006. Measuring language divergence by intra-lexical comparison. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-2006), pages 273–280, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilbert Heeringa</author>
</authors>
<title>Measuring Dialect Pronunciation Differences using Levenshtein Distance.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Rijksuniversiteit Groningen.</institution>
<contexts>
<context position="13805" citStr="Heeringa (2004)" startWordPosition="2237" endWordPosition="2238">u–l, etc. Levenshtein algorithm with Swap: adds an edit operation to enable the algorithm to capture phenomena such as metathesis, via a transposition: 6Tree-distance measures are developed in the context of work on phylogenetic trees in biological/genetic applications. 58 aligning ab in A to ba in B costs a single edit operation. This algorithm also forbids aligning vowels to consonants, except in a swap. Levenshtein algorithm with generated segment distances based on phonetic features: The above algorithms assign unit cost for all edit operations, regardless of how the segments are related. Heeringa (2004) uses a variant where the distances are obtained from differences between phonetic features of the segment pairs. The authors observe that this is subjective because one could choose from different possible feature sets. Levenshtein algorithm with generated segment distances based on acoustic features: To avoid subjectivity of feature selection, Heeringa (2004) experiments with assigning different costs to different segment pairs based on how phonetically close they are; segment distances are calculated by comparing spectrograms of recorded pronunciations. These algorithms do not attempt to di</context>
</contexts>
<marker>Heeringa, 2004</marker>
<rawString>Wilbert Heeringa. 2004. Measuring Dialect Pronunciation Differences using Levenshtein Distance. Ph.D. thesis, Rijksuniversiteit Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suvi Hiltunen</author>
</authors>
<title>Minimum description length modeling of etymological data. Master’s thesis,</title>
<date>2012</date>
<institution>University of Helsinki.</institution>
<contexts>
<context position="26910" citStr="Hiltunen, 2012" startWordPosition="4495" endWordPosition="4496">(a, b) = (4) max (C(a), C(b)) Since NCD of different pairs are comparable under the same model, it can be used as a distance measure between language varieties. 3.4 Prediction of unobserved data The models mentioned above are also able to predict unobserved data as described in Section 1 (Wettig et al., 2013). For the basic 1-1 model, since no information about the context is used, prediction simply means looking for the most probable symbol in target language for each symbol of wA. For the context model, a more sophisticated dynamicprogramming heuristic is needed to predict the unseen word, (Hiltunen, 2012). The predicted word 61 Figure 1: Model comparison: MDL costs. Figure 2: Model comparison: NFED. ˆWB is then compared to the real corresponding word WB to measure how well the model performed on the task. Feature-wise Levenshtein edit distance is used for this comparison. The edit distances for all word pairs are normalized, resulting in Normalized Feature-wise Edit Distance (NFED) which can serve as a measure of model quality. 4 Experiments To illustrate the principles discussed above, we experiment with the two principal model types described above—the baseline 1-1 model and the context-sens</context>
</contexts>
<marker>Hiltunen, 2012</marker>
<rawString>Suvi Hiltunen. 2012. Minimum description length modeling of etymological data. Master’s thesis, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´aszl´o Honti</author>
</authors>
<title>Ob’ Ugrian.</title>
<date>1998</date>
<booktitle>The Uralic Languages,</booktitle>
<pages>327--357</pages>
<editor>In Daniel Abondolo, editor,</editor>
<publisher>Routledge.</publisher>
<contexts>
<context position="33215" citStr="Honti, 1998" startWordPosition="5595" endWordPosition="5596">.28 mrd m mrd e .29 mrd e mrd m .29 est fin .32 fin est .32 man p man so .34 khn v khn dn .35 khn dn khn v .36 man so man p .36 saa n saa l .37 saa l saa n .37 Table 2: Comparison of Uralic dialect/language pairs, sorted by NCD: context model. murt, Mari, Mordva, Mansi, Khanty and Saami. Table 2 shows the normalized compression distances for each of the pairs; the NCD costs for Finnish and Estonian are given for comparison. It is striking that the pairs that score below Finnish/Estonian are all “true” dialects, whereas those that score above are not. E.g., the Mansi variants Pelym and Sosva, (Honti, 1998), and Demjanka and Vakh Khanty, (Abondolo, 1998), are mutually unintelligible. The same is true for North and Lule Saami. Turkic: We compute NCDs for the Turkic languages under the context model. Some of the Turkic languages are known to form a much tighter dialect continuum, (Johanson, 1998), which is evident from the NCDs in Table 3. E.g., Tofa is mostclosely related to the Tuvan language and forms a dialect continuum with it, (Johanson, 1998). Turkish and Azerbaijani closely resemble each other and are mutually intelligible. In the table we highlight language pairs with NCD ≤ 0.30. Slavic: </context>
</contexts>
<marker>Honti, 1998</marker>
<rawString>L´aszl´o Honti. 1998. Ob’ Ugrian. In Daniel Abondolo, editor, The Uralic Languages, pages 327–357. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Johanson</author>
</authors>
<title>The history of Turkic.</title>
<date>1998</date>
<booktitle>The Turkic Languages,</booktitle>
<pages>81--125</pages>
<editor>In Lars Johanson &amp; ´Eva ´Agnes Csat´o, editor,</editor>
<location>London, New York:</location>
<contexts>
<context position="33508" citStr="Johanson, 1998" startWordPosition="5644" endWordPosition="5645">able 2 shows the normalized compression distances for each of the pairs; the NCD costs for Finnish and Estonian are given for comparison. It is striking that the pairs that score below Finnish/Estonian are all “true” dialects, whereas those that score above are not. E.g., the Mansi variants Pelym and Sosva, (Honti, 1998), and Demjanka and Vakh Khanty, (Abondolo, 1998), are mutually unintelligible. The same is true for North and Lule Saami. Turkic: We compute NCDs for the Turkic languages under the context model. Some of the Turkic languages are known to form a much tighter dialect continuum, (Johanson, 1998), which is evident from the NCDs in Table 3. E.g., Tofa is mostclosely related to the Tuvan language and forms a dialect continuum with it, (Johanson, 1998). Turkish and Azerbaijani closely resemble each other and are mutually intelligible. In the table we highlight language pairs with NCD ≤ 0.30. Slavic: We analyzed data from StarLing for 9 Slavic languages.12 The NCDs are shown in Table 1. Of all pairs, the normalized compression costs for (cz, slk) and (lsrb, usrb) fall below the .30 mark, and indeed these pairs have high mutual intelligibility, unlike all other pairs. When the data from Ta</context>
</contexts>
<marker>Johanson, 1998</marker>
<rawString>Lars Johanson. 1998. The history of Turkic. In Lars Johanson &amp; ´Eva ´Agnes Csat´o, editor, The Turkic Languages, pages 81–125. London, New York: Routledge. Classification of Turkic languages (at Turkiclanguages.com).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Combining evidence in cognate identification.</title>
<date>2004</date>
<journal>Lecture Notes in Computer Science</journal>
<booktitle>In Proceedings of the Seventeenth Canadian Conference on Artificial Intelligence (Canadian AI 2004),</booktitle>
<volume>3060</volume>
<pages>44--59</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, Ontario.</location>
<contexts>
<context position="17447" citStr="Kondrak, 2004" startWordPosition="2804" endWordPosition="2805">e competing approaches, except indirectly, and using (subjective) gold-standards. 3 Methods for measuring language closeness We now discuss an approach which follows the proposal outlined in Section 1, and allows us to build probabilistic models for measuring closeness between languages. Other approaches that rely on probabilistic modeling would serve equally well. A comprehensive survey of methods for measuring language closeness may be found in (Wieling and Nerbonne, 2015). Work that is probabilistically oriented, similarly to our proposed approaches, includes (Bouchard-Cˆot´e et al., 2007; Kondrak, 2004) and others. We next review two types of models (some of which are described elsewhere), which are based on informationtheoretic principles. We discuss how these models suit the proposed approach, in the next section. 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et al., 2013).7 The basic model is based on alignment, similarly to much of the related work mentioned above: for every word pair in our da</context>
</contexts>
<marker>Kondrak, 2004</marker>
<rawString>Grzegorz Kondrak. 2004. Combining evidence in cognate identification. In Proceedings of the Seventeenth Canadian Conference on Artificial Intelligence (Canadian AI 2004), pages 44–59, London, Ontario. Lecture Notes in Computer Science 3060, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petri Kontkanen</author>
<author>Petri Myllym¨aki</author>
<author>Henry Tirri</author>
</authors>
<title>Constructing Bayesian finite mixture models by the EM algorithm.</title>
<date>1996</date>
<booktitle>Working Group on Neural and Computational Learning.</booktitle>
<tech>Technical Report NC-TR-97-003, ESPRIT NeuroCOLT:</tech>
<marker>Kontkanen, Myllym¨aki, Tirri, 1996</marker>
<rawString>Petri Kontkanen, Petri Myllym¨aki, and Henry Tirri. 1996. Constructing Bayesian finite mixture models by the EM algorithm. Technical Report NC-TR-97-003, ESPRIT NeuroCOLT: Working Group on Neural and Computational Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Fisher information and stochastic complexity.</title>
<date>1996</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="21238" citStr="Rissanen, 1996" startWordPosition="3465" endWordPosition="3467">nts for one that optimizes the objective. We can use various methods to code the complete data. Essentially, they all amount to measuring how many bits it costs to “transmit” the complete set of alignment “events”, where each alignment event e is a pair of aligned symbols (σ : τ) e = (σ : τ) E E U I.,#} xTUI.,#} drawn from the source alphabet E and the target alphabet T, respectively.9 One possible coding scheme is “prequential” coding, or the Bayesian marginal likelihood, see, e.g., (Kontkanen et al., 1996), used in (Wettig et al., 2011); another is normalized maximum likelihood (NML) code, (Rissanen, 1996), used in (Wettig et al., 2012). Prequential coding gives the total code length ELbase(D) = − log c(e)! e∈E + logIeEE E c(e) + K − 1J ! − log(K − 1)! (2) E for data D. Here, c(e) denotes the event count, and K is the total number of event types. To find the optimal alignments, the algorithm starts with aligning word pairs randomly, and then iteratively searching for the best alignment given rest of the data for each word pair at a time. To do this, we first exclude the current alignment from our complete data. The best alignment in the realigning process is found using a Dynamic Programming ma</context>
</contexts>
<marker>Rissanen, 1996</marker>
<rawString>Jorma Rissanen. 1996. Fisher information and stochastic complexity. IEEE Transactions on Information Theory, 42(1):40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naruya Saitou</author>
<author>Masatoshi Nei</author>
</authors>
<title>The neighbor-joining method: a new method for reconstructing phylogenetic trees. Molecular biology and evolution,</title>
<date>1987</date>
<pages>4--4</pages>
<contexts>
<context position="34180" citStr="Saitou and Nei, 1987" startWordPosition="5759" endWordPosition="5762">Tofa is mostclosely related to the Tuvan language and forms a dialect continuum with it, (Johanson, 1998). Turkish and Azerbaijani closely resemble each other and are mutually intelligible. In the table we highlight language pairs with NCD ≤ 0.30. Slavic: We analyzed data from StarLing for 9 Slavic languages.12 The NCDs are shown in Table 1. Of all pairs, the normalized compression costs for (cz, slk) and (lsrb, usrb) fall below the .30 mark, and indeed these pairs have high mutual intelligibility, unlike all other pairs. When the data from Table 1 are fed into the NeighborJoining algorithm, (Saitou and Nei, 1987), it draws the phylogeny in Figure 3, which clearly separates the languages into the 3 accepted branches of Slavic: East (ru, ukr), South 12The Slavic languages from StarLing: bulg:Bulgarian, cz:Czech, pl:Polish, ru:Russian, slk:Slovak, scr:SerboCroatian, ukr:Ukrainian, lsrb/usrb:Lower and Upper Sorbian. 63 SCR BULG RU UKR SLK CZ PL USRB LSRB 0 0.05 0.1 0.15 0.2 0.25 Figure 3: NeighborJoining tree for Slavic languages in Table 1. (scr, bulg) and West (pl, cz, slk, u/lsrb). The phylogeny also supports later separation (relative time depth &gt; 0.05) of the pairs with higher mutual intelligibility—</context>
</contexts>
<marker>Saitou, Nei, 1987</marker>
<rawString>Naruya Saitou and Masatoshi Nei. 1987. The neighbor-joining method: a new method for reconstructing phylogenetic trees. Molecular biology and evolution, 4(4):406–425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei A Starostin</author>
</authors>
<date>2005</date>
<note>Tower of Babel: StarLing etymological databases. http://newstar.rinet.ru/.</note>
<contexts>
<context position="27642" citStr="Starostin, 2005" startWordPosition="4612" endWordPosition="4613">d to the real corresponding word WB to measure how well the model performed on the task. Feature-wise Levenshtein edit distance is used for this comparison. The edit distances for all word pairs are normalized, resulting in Normalized Feature-wise Edit Distance (NFED) which can serve as a measure of model quality. 4 Experiments To illustrate the principles discussed above, we experiment with the two principal model types described above—the baseline 1-1 model and the context-sensitive model, using data from three different language families. 4.1 Data We use data from the StarLing data bases, (Starostin, 2005), for the Turkic and Uralic language families, and for the Slavic branch of the Indo-European family. For dozens of language families, StarLing has rich data sets (going beyond Swadesh-style lists, as in some other lexical data collections built for judging language and dialect distances). The databases are under constant development, and have different quality. Some datasets, (most notably the IE data) are drawn from multiple sources, which use different notation, transcription, etc., and are not yet unified. The data we chose for use is particularly clean. For the Turkic family, StarLing at </context>
</contexts>
<marker>Starostin, 2005</marker>
<rawString>Sergei A. Starostin. 2005. Tower of Babel: StarLing etymological databases. http://newstar.rinet.ru/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannes Wettig</author>
<author>Roman Yangarber</author>
</authors>
<title>Probabilistic models for alignment of etymological data.</title>
<date>2011</date>
<booktitle>In Proceedings of NoDaLiDa: the 18th Nordic Conference on Computational Linguistics,</booktitle>
<location>Riga, Latvia.</location>
<contexts>
<context position="17759" citStr="Wettig and Yangarber, 2011" startWordPosition="2853" endWordPosition="2856">aches that rely on probabilistic modeling would serve equally well. A comprehensive survey of methods for measuring language closeness may be found in (Wieling and Nerbonne, 2015). Work that is probabilistically oriented, similarly to our proposed approaches, includes (Bouchard-Cˆot´e et al., 2007; Kondrak, 2004) and others. We next review two types of models (some of which are described elsewhere), which are based on informationtheoretic principles. We discuss how these models suit the proposed approach, in the next section. 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et al., 2013).7 The basic model is based on alignment, similarly to much of the related work mentioned above: for every word pair in our data set— the “corpus”—it builds a complete alignment for all symbols (Wettig et al., 2011). The basic model considers pairwise alignments only, i.e., two languages at a time; we call them the source and the target languages. Later models relax this restriction by using N-dimensional alignment, with N &gt; 2 languag</context>
</contexts>
<marker>Wettig, Yangarber, 2011</marker>
<rawString>Hannes Wettig and Roman Yangarber. 2011. Probabilistic models for alignment of etymological data. In Proceedings of NoDaLiDa: the 18th Nordic Conference on Computational Linguistics, Riga, Latvia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannes Wettig</author>
<author>Suvi Hiltunen</author>
<author>Roman Yangarber</author>
</authors>
<title>MDL-based Models for Alignment of Etymological Data.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP: the 8th Conference on Recent Advances in Natural Language Processing,</booktitle>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="17781" citStr="Wettig et al., 2011" startWordPosition="2857" endWordPosition="2860">stic modeling would serve equally well. A comprehensive survey of methods for measuring language closeness may be found in (Wieling and Nerbonne, 2015). Work that is probabilistically oriented, similarly to our proposed approaches, includes (Bouchard-Cˆot´e et al., 2007; Kondrak, 2004) and others. We next review two types of models (some of which are described elsewhere), which are based on informationtheoretic principles. We discuss how these models suit the proposed approach, in the next section. 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et al., 2013).7 The basic model is based on alignment, similarly to much of the related work mentioned above: for every word pair in our data set— the “corpus”—it builds a complete alignment for all symbols (Wettig et al., 2011). The basic model considers pairwise alignments only, i.e., two languages at a time; we call them the source and the target languages. Later models relax this restriction by using N-dimensional alignment, with N &gt; 2 languages aligned simultaneou</context>
<context position="21167" citStr="Wettig et al., 2011" startWordPosition="3453" endWordPosition="3456">en alignment—and B. a search algorithm, to sift through all possible alignments for one that optimizes the objective. We can use various methods to code the complete data. Essentially, they all amount to measuring how many bits it costs to “transmit” the complete set of alignment “events”, where each alignment event e is a pair of aligned symbols (σ : τ) e = (σ : τ) E E U I.,#} xTUI.,#} drawn from the source alphabet E and the target alphabet T, respectively.9 One possible coding scheme is “prequential” coding, or the Bayesian marginal likelihood, see, e.g., (Kontkanen et al., 1996), used in (Wettig et al., 2011); another is normalized maximum likelihood (NML) code, (Rissanen, 1996), used in (Wettig et al., 2012). Prequential coding gives the total code length ELbase(D) = − log c(e)! e∈E + logIeEE E c(e) + K − 1J ! − log(K − 1)! (2) E for data D. Here, c(e) denotes the event count, and K is the total number of event types. To find the optimal alignments, the algorithm starts with aligning word pairs randomly, and then iteratively searching for the best alignment given rest of the data for each word pair at a time. To do this, we first exclude the current alignment from our complete data. The best alig</context>
</contexts>
<marker>Wettig, Hiltunen, Yangarber, 2011</marker>
<rawString>Hannes Wettig, Suvi Hiltunen, and Roman Yangarber. 2011. MDL-based Models for Alignment of Etymological Data. In Proceedings of RANLP: the 8th Conference on Recent Advances in Natural Language Processing, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannes Wettig</author>
<author>Kirill Reshetnikov</author>
<author>Roman Yangarber</author>
</authors>
<title>Using context and phonetic features in models of etymological sound change.</title>
<date>2012</date>
<booktitle>In Proc. EACL Workshop on Visualization of Linguistic Patterns and Uncovering Language History from Multilingual Resources,</booktitle>
<pages>37--44</pages>
<location>Avignon, France.</location>
<contexts>
<context position="17896" citStr="Wettig et al., 2012" startWordPosition="2873" endWordPosition="2876">und in (Wieling and Nerbonne, 2015). Work that is probabilistically oriented, similarly to our proposed approaches, includes (Bouchard-Cˆot´e et al., 2007; Kondrak, 2004) and others. We next review two types of models (some of which are described elsewhere), which are based on informationtheoretic principles. We discuss how these models suit the proposed approach, in the next section. 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et al., 2013).7 The basic model is based on alignment, similarly to much of the related work mentioned above: for every word pair in our data set— the “corpus”—it builds a complete alignment for all symbols (Wettig et al., 2011). The basic model considers pairwise alignments only, i.e., two languages at a time; we call them the source and the target languages. Later models relax this restriction by using N-dimensional alignment, with N &gt; 2 languages aligned simultaneously. The basic model allows only 1-1 symbol alignments: one source symbol8 may correspond to one target symbol—or t</context>
<context position="21269" citStr="Wettig et al., 2012" startWordPosition="3470" endWordPosition="3473"> the objective. We can use various methods to code the complete data. Essentially, they all amount to measuring how many bits it costs to “transmit” the complete set of alignment “events”, where each alignment event e is a pair of aligned symbols (σ : τ) e = (σ : τ) E E U I.,#} xTUI.,#} drawn from the source alphabet E and the target alphabet T, respectively.9 One possible coding scheme is “prequential” coding, or the Bayesian marginal likelihood, see, e.g., (Kontkanen et al., 1996), used in (Wettig et al., 2011); another is normalized maximum likelihood (NML) code, (Rissanen, 1996), used in (Wettig et al., 2012). Prequential coding gives the total code length ELbase(D) = − log c(e)! e∈E + logIeEE E c(e) + K − 1J ! − log(K − 1)! (2) E for data D. Here, c(e) denotes the event count, and K is the total number of event types. To find the optimal alignments, the algorithm starts with aligning word pairs randomly, and then iteratively searching for the best alignment given rest of the data for each word pair at a time. To do this, we first exclude the current alignment from our complete data. The best alignment in the realigning process is found using a Dynamic Programming matrix, with source word symbols </context>
</contexts>
<marker>Wettig, Reshetnikov, Yangarber, 2012</marker>
<rawString>Hannes Wettig, Kirill Reshetnikov, and Roman Yangarber. 2012. Using context and phonetic features in models of etymological sound change. In Proc. EACL Workshop on Visualization of Linguistic Patterns and Uncovering Language History from Multilingual Resources, pages 37–44, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hannes Wettig</author>
<author>Javad Nouri</author>
<author>Kirill Reshetnikov</author>
<author>Roman Yangarber</author>
</authors>
<title>Information-theoretic modeling of etymological sound change.</title>
<date>2013</date>
<booktitle>In Lars Borin and Anju Saxena, editors, Approaches to measuring linguistic differences, volume 265 of Trends in Linguistics,</booktitle>
<pages>507--531</pages>
<note>de Gruyter Mouton.</note>
<contexts>
<context position="22876" citStr="Wettig et al., 2013" startWordPosition="3766" endWordPosition="3769">bets need not be the same, or even have any symbols in common. We add a special end-of-word symbol, always aligned to itself: (# : #). Empty alignments (.:.) are not allowed. 60 where L(e) is the cost of coding event e. The cost of aligning the full word pair, is then found in the bottom-right cell, and the corresponding path is chosen as the new alignment, which is registered back into the complete data. We should mention that due to vulnerability of the algorithm to local optima, we use simulated annealing with (50) random restarts. 3.2 Context model Context model is described in detail in (Wettig et al., 2013). We use a modified version of this model to achieve faster run-time. One limitation of the basic model described above is that it uses no information about the context of the sounds, thus ignoring the fact that linguistic sound change is regular and highly depends on context. The 1-1 model also treats symbols of the words as atoms, ignoring how two sounds are phonetically close. The context model, addresses both of these issues. Each sound is represented as a vector of distinctive phonetic features. Since we are using MDL as the basis of the model here, we need to code (i.e., transmit) the da</context>
<context position="26605" citStr="Wettig et al., 2013" startWordPosition="4441" endWordPosition="4444">f normalizing the cost is needed to make them comparable across language pairs. We use “Normalized Compression Distance” (NCD), described in (Cilibrasi and Vitanyi, 2005) to achieve this. Given a model that can compress a language pair (a, b) with cost C(a, b), NCD of (a, b) is: C(a, b) − min (C(a), C(b)) NCD(a, b) = (4) max (C(a), C(b)) Since NCD of different pairs are comparable under the same model, it can be used as a distance measure between language varieties. 3.4 Prediction of unobserved data The models mentioned above are also able to predict unobserved data as described in Section 1 (Wettig et al., 2013). For the basic 1-1 model, since no information about the context is used, prediction simply means looking for the most probable symbol in target language for each symbol of wA. For the context model, a more sophisticated dynamicprogramming heuristic is needed to predict the unseen word, (Hiltunen, 2012). The predicted word 61 Figure 1: Model comparison: MDL costs. Figure 2: Model comparison: NFED. ˆWB is then compared to the real corresponding word WB to measure how well the model performed on the task. Feature-wise Levenshtein edit distance is used for this comparison. The edit distances for</context>
</contexts>
<marker>Wettig, Nouri, Reshetnikov, Yangarber, 2013</marker>
<rawString>Hannes Wettig, Javad Nouri, Kirill Reshetnikov, and Roman Yangarber. 2013. Information-theoretic modeling of etymological sound change. In Lars Borin and Anju Saxena, editors, Approaches to measuring linguistic differences, volume 265 of Trends in Linguistics, pages 507–531. de Gruyter Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Wieling</author>
<author>John Nerbonne</author>
</authors>
<title>Measuring linguistic variation commensurably.</title>
<date>2011</date>
<booktitle>In Dialectologia Special Issue II: Production, Perception and Attitude,</booktitle>
<pages>141--162</pages>
<contexts>
<context position="15158" citStr="Wieling and Nerbonne, 2011" startWordPosition="2452" endWordPosition="2455">Levenshtein algorithm with distances based on PMI: Wieling et al. (2009) use Point-wise Mutual Information (PMI) as the basis for segment distances. They assign different costs to segments, and use the entire dataset for each alignment. PMI for outcomes x and y of random variables X and Y is defined as: p(x, y) pmi(x, y) = log2 (1) p(x)p(y) PMI is calculated using estimated probabilities of the events. Since greater PMI shows higher tendency of x and y to co-occur, it is reversed and normalized to obtain a dissimilarity measure to be used as segment distance. Details about this method are in (Wieling and Nerbonne, 2011). 2.3 Other distance measures Ellison and Kirby (2006) present a distance measure based on comparing intra-language lexica only, arguing that there is no well-founded common language-independent phonetic space to be used for comparing word forms across languages. Instead, they focus on inferring the distances by comparing how meanings in language A are likely to be confused for each other, and comparing it to the confusion probabilities in language B. Given a lexicon containing mappings from a set of meanings M to a set of forms F, confusion probability P(m1|m2; L) for each pair of meanings (m</context>
</contexts>
<marker>Wieling, Nerbonne, 2011</marker>
<rawString>Martijn Wieling and John Nerbonne. 2011. Measuring linguistic variation commensurably. In Dialectologia Special Issue II: Production, Perception and Attitude, pages 141–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Wieling</author>
<author>John Nerbonne</author>
</authors>
<title>Advances in dialectometry.</title>
<date>2015</date>
<booktitle>In Annual Review of Linguistics,</booktitle>
<volume>1</volume>
<note>To appear.</note>
<contexts>
<context position="17312" citStr="Wieling and Nerbonne, 2015" startWordPosition="2782" endWordPosition="2786">ences and similarities—and regularities—between the languages in a transparent, easily interpretable way. Second, they offer no direct way to compare competing approaches, except indirectly, and using (subjective) gold-standards. 3 Methods for measuring language closeness We now discuss an approach which follows the proposal outlined in Section 1, and allows us to build probabilistic models for measuring closeness between languages. Other approaches that rely on probabilistic modeling would serve equally well. A comprehensive survey of methods for measuring language closeness may be found in (Wieling and Nerbonne, 2015). Work that is probabilistically oriented, similarly to our proposed approaches, includes (Bouchard-Cˆot´e et al., 2007; Kondrak, 2004) and others. We next review two types of models (some of which are described elsewhere), which are based on informationtheoretic principles. We discuss how these models suit the proposed approach, in the next section. 3.1 1-1 symbol model We begin with our “basic” model, described in (Wettig and Yangarber, 2011; Wettig et al., 2011), which makes several simplifying assumptions—which the subsequent, more advanced models relax (Wettig et al., 2012; Wettig 59 et a</context>
</contexts>
<marker>Wieling, Nerbonne, 2015</marker>
<rawString>Martijn Wieling and John Nerbonne. 2015. Advances in dialectometry. In Annual Review of Linguistics, volume 1. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Wieling</author>
<author>Jelena Proki´c</author>
<author>John Nerbonne</author>
</authors>
<title>Evaluating the pairwise string alignment of pronunciations.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education,</booktitle>
<pages>26--34</pages>
<location>Athens, Greece.</location>
<marker>Wieling, Proki´c, Nerbonne, 2009</marker>
<rawString>Martijn Wieling, Jelena Proki´c, and John Nerbonne. 2009. Evaluating the pairwise string alignment of pronunciations. In Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education, pages 26–34, Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
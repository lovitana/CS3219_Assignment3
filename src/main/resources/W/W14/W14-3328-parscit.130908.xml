<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006772">
<title confidence="0.997035">
Domain Adaptation for Medical Text Translation Using Web Re-
sources
</title>
<author confidence="0.999049">
Yi Lu, Longyue Wang, Derek F. Wong, Lidia S. Chao, Yiming Wang, Francisco Oliveira
</author>
<affiliation confidence="0.992879666666667">
Natural Language Processing &amp; Portuguese-Chinese Machine Translation Laboratory,
Department of Computer and Information Science,
University of Macau, Macau, China
</affiliation>
<email confidence="0.931079">
takamachi660@gmail.com, vincentwang0229@hotmail.com,
derekfw@umac.mo, lidiasc@umac.mo, wang2008499@gmail.com,
olifran@umac.mo
</email>
<sectionHeader confidence="0.996515" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999488">
This paper describes adapting statistical
machine translation (SMT) systems to
medical domain using in-domain and
general-domain data as well as web-
crawled in-domain resources. In order to
complement the limited in-domain corpo-
ra, we apply domain focused web-
crawling approaches to acquire in-
domain monolingual data and bilingual
lexicon from the Internet. The collected
data is used for adapting the language
model and translation model to boost the
overall translation quality. Besides, we
propose an alternative filtering approach
to clean the crawled data and to further
optimize the domain-specific SMT sys-
tem. We attend the medical summary
sentence unconstrained translation task of
the Ninth Workshop on Statistical Ma-
chine Translation (WMT2014). Our sys-
tems achieve the second best BLEU
scores for Czech-English, fourth for
French-English, English-French language
pairs and the third best results for re-
minding pairs.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908913043478">
In this paper, we report the experiments carried
out by the NLP2CT Laboratory at University of
Macau for WMT2014 medical sentence transla-
tion task on six language pairs: Czech-English
(cs-en), French-English (fr-en), German-English
(de-en) and the reverse direction pairs (i.e., en-cs,
en-fr and en-de).
As data in specific domain are usually rela-
tively scarce, the use of web resources to com-
plement the training resources provides an effec-
tive way to enhance the SMT systems (Resnik
and smith, 2003; Esplà-Gomis and Forcada, 2010;
Pecina et al., 2011; Pecina et al., 2012; Pecina et
al., 2014). In our experiments, we not only use
all available training data provided by the
WMT2014 standard translation task1 (general-
domain data) and medical translation task2 (in-
domain data), but also acquire addition in-
domain bilingual translations (i.e. dictionary) and
monolingual data from online sources.
First of all, we collect the medical terminolo-
gies from the web. This tiny but significant par-
allel data are helpful to reduce the out-of-
vocabulary words (OOVs) in translation models.
In addition, the use of larger language models
during decoding is aided by more efficient stor-
age and inference (Heafield, 2011). Thus, we
crawl more in-domain monolingual data from the
Internet based on domain focused web-crawling
approach. In order to detect and remove out-
domain data from the crawled data, we not only
explore text-to-topic classifier, but also propose
an alternative filtering approach combined the
existing one (text-to-topic classifier) with per-
plexity. After carefully pre-processing all the
available training data, we apply language model
adaptation and translation model adaptation us-
ing various kinds of training corpora. Experi-
mental results show that the presented approach-
es are helpful to further boost the baseline system.
The reminder of this paper is organized as fol-
lows. In Section 2, we detail the workflow of
web resources acquisition. Section 3 describes
the pre-processing steps for the corpora. Section
5 presents the baseline system. Section 6 reports
the experimental results and discussions. Finally,
</bodyText>
<footnote confidence="0.9969">
1 http://www.statmt.org/wmt14/translation-task.html.
2 http://www.statmt.org/wmt14/medical-task/.
</footnote>
<page confidence="0.973624">
233
</page>
<bodyText confidence="0.67081625">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 233–238,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
the submitted systems and the official results are
reported in Section 7.
</bodyText>
<sectionHeader confidence="0.88481" genericHeader="method">
2 Domain Focused Web-Crawling
</sectionHeader>
<bodyText confidence="0.999783">
In this section, we introduce our domain focused
web-crawling approaches on acquisition of in-
domain translation terminologies and monolin-
gual sentences.
</bodyText>
<subsectionHeader confidence="0.996692">
2.1 Bilingual Dictionary
</subsectionHeader>
<bodyText confidence="0.998863428571429">
Terminology is a system of words used to name
things in a particular discipline. The in-domain
vocabulary size directly affects the performance
of domain-specific SMT systems. Small size of
in-domain vocabulary may result in serious
OOVs problem in a translation system. Therefore,
we crawl medical terminologies from some
online sources such as dict.cc3, where the vocab-
ularies are divided into different subjects. We
obtain the related bilingual entries in medicine
subject by using Scala build-in XML parser and
XPath. After cleaning, we collected 28,600,
37,407, and 37,600 entries in total for cs-en, de-
en, and fr-en respectively.
</bodyText>
<subsectionHeader confidence="0.999436">
2.2 Monolingual Data
</subsectionHeader>
<bodyText confidence="0.990039230769231">
The workflow for acquiring in-domain resources
consists of a number of steps such as domain
identification, text normalization, language iden-
tification, noise filtering, and post-processing as
well as parallel sentence identification.
Firstly we use an open-source crawler, Com-
bine4, to crawl webpages from the Internet. In
order to classify these webpages as relevant to
the medical domain, we use a list of triplets
&lt;term, relevance weight, topic class&gt; as the
basic entries to define the topic. Term is a word
or phrase. We select terms for each language
from the following sources:
</bodyText>
<listItem confidence="0.992559125">
• The Wikipedia title corpus, a WMT2014 of-
ficial data set consisting of titles of medical
articles.
• The dict.cc dictionary, as is described in Sec-
tion 2.1.
• The DrugBank corpus, which is a WMT2014
official data set on bioinformatics and
cheminformatics.
</listItem>
<bodyText confidence="0.99975575">
For the parallel data, i.e. Wikipedia and dict.cc
dictionary, we separate the source and target text
into individual text and use either side of them
for constructing the term list for different lan-
</bodyText>
<footnote confidence="0.9676555">
3 http://www.dict.cc/.
4 http://combine.it.lth.se/.
</footnote>
<bodyText confidence="0.995384">
guages. Regarding the DrugBank corpus, we di-
rectly extract the terms from the “name” field.
The vocabulary size of collected text for each
language is shown in Table 1.
</bodyText>
<table confidence="0.9959846">
EI CS DE FR
Wikipedia Titles 12,684 3,404 10,396 8,436
dict.cc 29,294 16,564 29,963 22,513
DrugBank 2,788
Total 44,766 19,968 40,359 30,949
</table>
<tableCaption confidence="0.999883">
Table 1: Size of terms used for topic definition.
</tableCaption>
<bodyText confidence="0.999033375">
Relevance weight is the score for each occur-
rence of the term, which is assigned by its length,
i.e., number of tokens. The topic class indicates
the topics. In this study, we are interested in
medical domain, the topic class is always marked
with “MED” in our topic definition.
The topic relevance of each document is cal-
culated5 as follows:
</bodyText>
<equation confidence="0.9921215">
∑ ∑
(1)
</equation>
<bodyText confidence="0.999823689655172">
where is the amount of terms in the topic defi-
nition; is the weight of term ; is the
weight of term at location . is the number of
occurrences of term at position. In implemen-
tation, we use the default values for setting and
parameters. Another input required by the crawl-
er is a list of seed URLs, which are web sites that
related to medical topic. We limit the crawler
from getting the pages within the http domain
guided by the seed links. We acquired the list
from the Open Directory Project6, which is a re-
pository maintained by volunteer editors. Totally,
we collected 12,849 URLs from the medicine
category.
Text normalization is to convert the text of
each HTML page into UTF-8 encoding accord-
ing to the content_charset of the header. In addi-
tion, HTML pages often consist of a number of
irrelevant contents such as the navigation links,
advertisements disclaimers, etc., which may neg-
atively affect the performance of SMT system.
Therefore, we use the Boilerpipe tool
(Kohlschütter et al., 2010) to filter these noisy
data and preserve the useful content that is
marked by the tag, &lt;canonicalDocument&gt;. The
resulting text is saved in an XML file, which will
be further processed by the subsequent tasks. For
language identification, we use the language-
detection7 toolkit to determine the possible lan-
</bodyText>
<footnote confidence="0.974272">
5 http://combine.it.lth.se/documentation/DocMain/node6.html.
6 http://www.dmoz.org/Health/Medicine/.
7 https://code.google.com/p/language-detection/.
</footnote>
<page confidence="0.996989">
234
</page>
<bodyText confidence="0.9990055">
guage of the text, and discard the articles which
are in the right language we are interested.
</bodyText>
<subsectionHeader confidence="0.999237">
2.3 Data Filtering
</subsectionHeader>
<bodyText confidence="0.99999184">
The web-crawled documents (described in Sec-
tion 2.2) may consist a number of out-domain
data, which would harm the domain-specific lan-
guage and translation models. We explore and
propose two filtering approaches for this task.
The first one is to filter the documents based on
their relative score, Eq. (1). We rank all the doc-
uments according to their relative scores and se-
lect top K percentage of entire collection for fur-
ther processing.
Second, we use a combination method, which
takes both the perplexity and relative score into
account for the selection. Perplexity-based data
selection has shown to be a powerful mean on
SMT domain adaptation (Wang et al., 2013;
Wang et al., 2014; Toral, 2013; Rubino et al.,
2013; Duh et al., 2013). The combination method
is carried out as follows: we first retrieve the
documents based on their relative scores. The
documents are then split into sentences, and
ranked according to their perplexity using Eq. (2)
(Stolcke et al., 2002). The used language model
is trained on the official in-domain data. Finally,
top I percentage of ranked sentences are consid-
ered as additional relevant in-domain data.
</bodyText>
<equation confidence="0.8500635">
(T)
(s)
</equation>
<bodyText confidence="0.9998225">
where s is a input sentence or document, P(T) is
the probability of n -gram segments estimated
from the training set. Word is the number of
tokens of an input string.
</bodyText>
<sectionHeader confidence="0.952596" genericHeader="method">
3 Pre-processing
</sectionHeader>
<bodyText confidence="0.999863733333333">
Both official training data and web-crawled re-
sources are processed using the Moses scripts8,
this includes the text tokenization, truecasing and
length cleaning. For trusecasing, we use both the
target side of parallel corpora and monolingual
data to train the trucase models. We consider the
target system is intended for summary translation,
the sentences tend to be short in length. We re-
move sentence pairs which are more than 80
words at length in either sides of the parallel text.
In addition to these general data filtering steps,
we introduce some extra steps to pre-process the
training data. The first step is to remove the du-
plicate sentences. In data-driven methods, the
more frequent a term occurs, the higher probabil-
</bodyText>
<footnote confidence="0.394945">
8 http://www.statmt.org/moses/?n=Moses.Baseline.
</footnote>
<bodyText confidence="0.999327888888889">
ity it biases. Duplicate data may lead to unpre-
dicted behavior during the decoding. Therefore,
we keep only the distinct sentences in monolin-
gual corpus. By taking into account multiple
translations in parallel corpus, we remove the
duplicate sentence pairs. We also use a biomedi-
cal sentence splitter9 (Rune et al., 2007) to split
sentences in monolingual corpora. The statistics
of the data are provided in Table 2.
</bodyText>
<sectionHeader confidence="0.991827" genericHeader="method">
4 Baseline System
</sectionHeader>
<bodyText confidence="0.999988333333333">
We built our baseline system on an optimized
level. It is trained on all official in-domain train-
ing corpora and a portion of general-domain data.
We apply the Moore-Lewis method (Moore and
Lewis, 2010) and modified Moore-Lewis method
(Axelrod et al., 2011) for selecting in-domain
data from the general-domain monolingual and
parallel corpora, respectively. The top M per-
centages of ranked sentences are selected as a
pseudo in-domain data to train an additional LM
and TM. For LM, we linearly interpolate the ad-
ditional LM with in-domain LM. For TM, the
additional model is log-linearly interpolated with
the in-domain model using the multi-decoding
method described in (Koehn and Schroeder,
2007). Finally, LM adaptation and TM adapta-
tion are combined to further improve the transla-
tion quality of baseline system.
</bodyText>
<sectionHeader confidence="0.996936" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999794944444445">
The official medical summary development sets
(dev) are used for tuning and evaluating the
comparative systems. The official medical sum-
mary test sets (test) are only used in our final
submitted systems.
The experiments were carried out with the
Moses 1.010 (Koehn et al., 2007). The translation
and the re-ordering model utilizes the “grow-
diag-final” symmetrized word-to-word align-
ments created with MGIZA++11 (Och and Ney,
2003; Gao and Vogel, 2008) and the training
scripts from Moses. A 5-gram LM was trained
using the SRILM toolkit12 (Stolcke et al., 2002),
exploiting improved modified Kneser-Ney
smoothing, and quantizing both probabilities and
back-off weights. For the log-linear model train-
ing, we take the minimum-error-rate training
(MERT) method as described in (Och, 2003).
</bodyText>
<footnote confidence="0.974113">
9 http://www.nactem.ac.uk/y-matsu/geniass/.
10 http://www.statmt.org/moses/.
11 http://www.kyloo.net/software/doku.php/mgiza:overview.
12 http://www.speech.sri.com/projects/srilm/.
</footnote>
<figure confidence="0.352343">
(2)
</figure>
<page confidence="0.959376">
235
</page>
<table confidence="0.999971677419355">
Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Docs
In-domain cs/en 1,770,421 9,373,482/ 134,998/ 5.29/
Parallel Data 10,605,222 156,402 5.99
de/en 3,894,099 52,211,730/ 1,146,262/ 13.41/
58,544,608 487,850 15.03
fr/en 4,579,533 77,866,237/ 495,856/ 17.00/
68,429,649 556,587 14.94
General- cs/en 12,426,374 180,349,215/ 1,614,023/ 14.51/
domain 183,841,805 1,661,830 14.79
Parallel Data
de/en 4,421,961 106,001,775/ 1,912,953/ 23.97/
112,294,414 919,046 25.39
fr/en 36,342,530 1,131,027,766/ 3,149,336/ 31.12/
953,644,980 3,324,481 26.24
In-domain cs 106,548 1,779,677 150,672 16.70
Mono. Data
fr 1,424,539 53,839,928 644,484 37.79
de 2,222,502 53,840,304 1,415,202 24.23
en 7,802,610 199430649 1,709,594 25.56
General- cs 33,408,340 567,174,266 3,431,946 16.98
domain
Mono. Data
fr 30,850,165 780,965,861 2,142,470 25.31
de 84,633,641 1,548,187,668 10,726,992 18.29
en 85,254,788 2,033,096,800 4,488,816 23.85
Web-crawled en 8,448,566 280,211,580 3,047,758 33.16 26 1,601
In-domain
Mono. Data
cs 44,198 1,280,326 137,179 28.96 4 388
de 473,171 14,087,687 728,652 29.77 17 968
fr 852,036 35,339,445 718,141 41.47 10 683
</table>
<tableCaption confidence="0.999465">
Table 2: Statistics summary of corpora after pre-processing.
</tableCaption>
<bodyText confidence="0.99997725">
In the following sub-sections, we describe the
results of baseline systems, which are trained on
the official corpora. We also present the en-
hanced systems that make use of the web-
crawled bilingual dictionary and monolingual
data as the additional training resources. Two
variants of enhanced system are constructed
based on different filtering criteria.
</bodyText>
<subsectionHeader confidence="0.991615">
5.1 Baseline System
</subsectionHeader>
<bodyText confidence="0.997462285714286">
The baseline systems is constructed based on the
combination of TM adaptation and LM adapta-
tion, where the corresponding selection thresh-
olds (M) are manually tuned. Table 3 shows the
BLEU scores of baseline systems as well as the
threshold values of M for general-domain mono-
lingual corpora and parallel corpora selection,
respectively.
By looking into the results, we find that en-cs
system performs poorly, because of the limited
in-domain parallel and monolingual corpora
(shown in Table 2). While the fr-en and en-fr
systems achieve the best scores, due the availa-
bility of the high volume training data. We ex-
periment with different values of M={0, 25, 50,
75, 100} that indicates the percentages of sen-
tences out of the general corpus used for con-
structing the LM adaptation and TM adaptation.
After tuning the parameter M, we find that
BLEU scores of different systems peak at differ-
ent values of M. LM adaptation can achieve the
best translation results for cs-en, en-fr and de-en
pairs when M=25, en-cs and en-de pairs when
M=50, and fr-en pair when M=75. While TM
adaptation yields the best scores for en-fr and en-
de pairs at M=25 and cs-en and fr-en pairs at
M=50, de-en pair when M=75 and en-cs pair at
M=100.
</bodyText>
<table confidence="0.99833075">
Lang. Pair BLEU Mono. Parallel
(M%) (M%)
en-cs 17.57 50% 100%
cs-en 31.29 25% 50%
en-fr 38.36 25% 25%
fr-en 44.36 75% 50%
en-de 18.01 50% 25%
de-en 32.50 25% 75%
</table>
<tableCaption confidence="0.966848">
Table 3: BLEU scores of baseline systems for
different language pairs.
</tableCaption>
<subsectionHeader confidence="0.999381">
5.2 Based on Relevance Score Filtering
</subsectionHeader>
<bodyText confidence="0.999858666666667">
As described in Section 2.3, we use the relevance
score to filter out the non-in-domain documents.
Once again, we evaluate different values of
</bodyText>
<page confidence="0.995096">
236
</page>
<bodyText confidence="0.999368375">
K={0, 25, 50, 75, 100} that represents the per-
centages of crawled documents we used for
training the LMs. In Table 4, we show the abso-
lute BLEU scores of the evaluated systems, listed
with the optimized thresholds, and the relative
improvements (Δ%) in compared to the baseline
system. The size of additional training data (for
LM) is displayed at the last column.
</bodyText>
<table confidence="0.9987005">
Lang. Docs BLEU a Sent.
Pair ( K%) (%)
en-cs 50 17.59 0.11 31,065
en-de 75 18.52 2.83 435,547
en-fr 50 39.08 1.88 743,735
cs-en 75 32.22 2.97 7,943,931
de-en 25 33.50 3.08 4,951,189
fr-en 100 45.45 2.46 8,448,566
</table>
<tableCaption confidence="0.9938905">
Table 4: Evaluation results for systems that
trained on relevance-score-filtered documents.
</tableCaption>
<bodyText confidence="0.999777333333333">
The relevance score filtering approach yields
an improvement of 3.08% of BLEU score for de-
en pair that is the best result among the language
pairs. On the other hand, en-cs pair obtains a
marginal gain. The reason is very obvious that
the training data is very insufficient. Empirical
results of all language pairs expect fr-en indicate
that data filtering is the necessity to improve the
system performance.
</bodyText>
<subsectionHeader confidence="0.997781">
5.3 Based on Moore-Lewis Filtering
</subsectionHeader>
<bodyText confidence="0.999960363636364">
In this approach, we need to determine the values
of two parameters, top K documents and top N
sentences, where K={100, 75, 50} and N={75,
50, 25}, N &lt; K. When K=100, it is a conven-
tional perplexity-based data selection method, i.e.
no document will be filtered. Table 5 shows the
combination of different K and N that gives the
best translation score for each language pair. We
provide the absolute BLEU for each system, to-
gether with relative improvements (A%) that
compared to the baseline system.
</bodyText>
<table confidence="0.998257">
Lang. Docs Target BLEU a (%)
Pair (K%) Size (N%)
en-cs 50 25 17.69 0.68
en-de 100 50 18.03 0.11
en-fr 100 50 38.73 0.96
cs-en 100 25 32.20 2.91
de-en 100 25 33.10 1.85
fr-en 100 25 45.22 1.94
</table>
<tableCaption confidence="0.9971395">
Table 5: Evaluation results for systems that
trained on combination filtering approach.
</tableCaption>
<bodyText confidence="0.999412857142857">
In this shared task, we have a quality and
quantity in-domain monolingual training data for
English. All the systems that take English as the
target translation always outperform the other
reverse pairs. Besides, we found the systems
based on the perplexity data selection method
tend to achieve a better scores in BLEU.
</bodyText>
<sectionHeader confidence="0.845975" genericHeader="conclusions">
6 Official Results and Conclusions
</sectionHeader>
<bodyText confidence="0.995347137931035">
We described our study on developing uncon-
strained systems in the medical translation task
of 2014 Workshop on Statistical Machine Trans-
lation. In this work, we adopt the web crawling
strategy for acquiring the in-domain monolingual
data. In detection the domain data, we exploited
Moore-Lewis data selection method to filter the
collected data in addition to the build-in scoring
model provided by the crawler toolkit. However,
after investigation, we found that the two meth-
ods are very competitive to each other.
The systems we submitted to the shared task
were built using the language models and trans-
lation models that yield the best results in the
individual testing. The official test set is convert-
ed into the recased and detokenized SGML for-
mat. Table 9 presents the official results of our
submissions for every language pair.
Lang. BLEU of Combined Official
Pair systems BLEU
en-cs 23.16 (+5.59) 22.10
cs-en 36.8 (+5.51) 37.40
en-fr 40.34 (+1.98) 40.80
fr-en 45.79 (+1.43) 43.80
en-de 19.36 (+1.35) 18.80
de-en 34.17 (+1.67) 32.70
Table 6: BLEU scores of the submitted systems
for the medical translation task in six language
pairs.
</bodyText>
<sectionHeader confidence="0.996976" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999563">
The authors are grateful to the Science and
Technology Development Fund of Macau and
the Research Committee of the University of
Macau for the funding support for their research,
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS.
</bodyText>
<sectionHeader confidence="0.996995" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9515235">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of EMILP, pages 355-
362.
</reference>
<page confidence="0.984012">
237
</page>
<reference confidence="0.999791267326733">
K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Ad-
aptation data selection using neural language mod-
els: Experiments in machine translation. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics, pages, 678–683.
M. Esplà-Gomis and M. L. Forcada. 2010. Combining
Content-Based and URL-Based Heuristics toHar-
vest Aligned Bitexts from Multilingual Sites with
Bitextor. The Prague Bulletin of Mathemathical
Lingustics, 93:77–86.
Qin Gao and Stephan Vogel. 2008. Parallel Imple-
mentations of Word Alignment Tool. Software En-
gineering, Testing, and Quality Assurance for Nat-
ural Language Processing, pp. 49-57.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187-197.
Papineni, Kishore, Salim Roukos, ToddWard, and-
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In 40th Annu-
al Meeting on Association for Computational Lin-
guistics, ACL ’02, pages 311–318, Philadelphia,
USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran et al.
2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of ACL, pages
177-180.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the 2nd ACL Work-
shop on Statistical Machine Translation, pages
224-227.
Christian Kohlschütter, Peter Fankhauser, and Wolf-
gang Nejdl. 2010. Boilerplate detection using shal-
low text features. In Proceedings of the 3rd ACM
International Conference on Web Search and Data
Mining, pages 441-450.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of ACL: Short Papers, pages 220-224.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. Proceedings of
ACL, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29:19-51.
P. Pecina, A. Toral, A. Way, V. Papavassiliou, P.
Prokopidis, and M. Giagkou. 2011. Towards Using
WebCrawled Data for Domain Adaptation in Sta-
tistical Machine Translation. In Proceedings of the
15th Annual Conference of the European Associta-
tion for Machine Translation, pages 297-304.
P. Pecina, A. Toral, V. Papavassiliou, P. Prokopidis, J.
van Genabith, and R. I. C. Athena. 2012. Domain
adaptation of statistical machine translation using
web-crawled resources: a case study. In Proceed-
ings of the 16th Annual Conference of the Europe-
an Association for Machine Translation, pp. 145-
152.
P. Pecina, O. Dušek, L. Goeuriot, J. Haji6, J. Hla-
vá6ová, G. J. Jones, and Z. Urešová. 2014. Adapta-
tion of machine translation for multilingual infor-
mation retrieval in the medical domain. Artificial
intelligence in medicine, pages 1-25.
Philip Resnik and Noah A. Smith. 2003. The Web as
a parallel corpus. Computational Linguistics,
29:349–380
Raphael Rubino, Antonio Toral, Santiago Cortés
Vaíllo, Jun Xie, Xiaofeng Wu, Stephen Doherty,
and Qun Liu. 2013. The CNGL-DCU-Prompsit
translation systems for WMT13. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 213-218.
Sætre Rune, Kazuhiro Yoshida, Akane Yakushiji,
Yusuke Miyao, Yuichiro Matsubayashi and Tomo-
ko Ohta. 2007. AKANE System: Protein-Protein
Interaction Pairs in BioCreAtIvE2 Challenge, PPI-
IPS subtask. In Proceedings of the Second BioCre-
ative Challenge Evaluation Workshop, pp. 209-212.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. Proceedings of the Inter-
national Conference on Spoken Language Pro-
cessing, pp. 901-904.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information
and perplexity. In ACL Workshop on Hybrid Ma-
chine Approaches to Translation.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu,
and Junwen Xing. 2014. A Systematic Com-
parison of Data Selection Criteria for SMT Domain
Adaptation. The Scientific World Journal, vol.
2014, Article ID 745485, 10 pages.
Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu,
Junwen Xing. 2013. iCPE: A Hybrid Data Selec-
tion Model for SMT Domain Adaptation. Chinese
Computational Linguistics and Natural Language
Processing Based on Naturally Annotated Big Da-
ta. Springer Berlin Heidelberg. pages, 280-290
</reference>
<page confidence="0.997062">
238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.513732">
<title confidence="0.971082">Adaptation for Medical Text Translation Using Web Resources</title>
<author confidence="0.997671">Yi Lu</author>
<author confidence="0.997671">Longyue Wang</author>
<author confidence="0.997671">Derek F Wong</author>
<author confidence="0.997671">Lidia S Chao</author>
<author confidence="0.997671">Yiming Wang</author>
<author confidence="0.997671">Francisco</author>
<affiliation confidence="0.970405">Natural Language Processing &amp; Portuguese-Chinese Machine Translation Department of Computer and Information University of Macau, Macau, China</affiliation>
<email confidence="0.845094666666667">takamachi660@gmail.com,derekfw@umac.mo,lidiasc@umac.mo,olifran@umac.mo</email>
<abstract confidence="0.998929423076923">This paper describes adapting statistical machine translation (SMT) systems to medical domain using in-domain and general-domain data as well as webcrawled in-domain resources. In order to complement the limited in-domain corpora, we apply domain focused webcrawling approaches to acquire indomain monolingual data and bilingual lexicon from the Internet. The collected data is used for adapting the language model and translation model to boost the overall translation quality. Besides, we propose an alternative filtering approach to clean the crawled data and to further optimize the domain-specific SMT system. We attend the medical summary sentence unconstrained translation task of the Ninth Workshop on Statistical Machine Translation (WMT2014). Our systems achieve the second best BLEU scores for Czech-English, fourth for French-English, English-French language pairs and the third best results for reminding pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of EMILP,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="10929" citStr="Axelrod et al., 2011" startWordPosition="1685" endWordPosition="1688">e decoding. Therefore, we keep only the distinct sentences in monolingual corpus. By taking into account multiple translations in parallel corpus, we remove the duplicate sentence pairs. We also use a biomedical sentence splitter9 (Rune et al., 2007) to split sentences in monolingual corpora. The statistics of the data are provided in Table 2. 4 Baseline System We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. 5 Experiments and Results The official </context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of EMILP, pages 355-362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Duh</author>
<author>G Neubig</author>
<author>K Sudoh</author>
<author>H Tsukada</author>
</authors>
<title>Adaptation data selection using neural language models: Experiments in machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>678--683</pages>
<contexts>
<context position="8855" citStr="Duh et al., 2013" startWordPosition="1351" endWordPosition="1354">ecific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top I percentage of ranked sentences are considered as additional relevant in-domain data. (T) (s) where s is a input sentence or document, P(T) is the probability of n -gram segments estimated from the training set. Word is the number of tokens of an input string. 3 Pre-processing B</context>
</contexts>
<marker>Duh, Neubig, Sudoh, Tsukada, 2013</marker>
<rawString>K. Duh, G. Neubig, K. Sudoh, H. Tsukada. 2013. Adaptation data selection using neural language models: Experiments in machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages, 678–683.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Esplà-Gomis</author>
<author>M L Forcada</author>
</authors>
<title>Combining Content-Based and URL-Based Heuristics toHarvest Aligned Bitexts from Multilingual Sites with Bitextor. The Prague Bulletin of Mathemathical Lingustics,</title>
<date>2010</date>
<pages>93--77</pages>
<contexts>
<context position="1919" citStr="Esplà-Gomis and Forcada, 2010" startWordPosition="268" endWordPosition="271">ish, English-French language pairs and the third best results for reminding pairs. 1 Introduction In this paper, we report the experiments carried out by the NLP2CT Laboratory at University of Macau for WMT2014 medical sentence translation task on six language pairs: Czech-English (cs-en), French-English (fr-en), German-English (de-en) and the reverse direction pairs (i.e., en-cs, en-fr and en-de). As data in specific domain are usually relatively scarce, the use of web resources to complement the training resources provides an effective way to enhance the SMT systems (Resnik and smith, 2003; Esplà-Gomis and Forcada, 2010; Pecina et al., 2011; Pecina et al., 2012; Pecina et al., 2014). In our experiments, we not only use all available training data provided by the WMT2014 standard translation task1 (generaldomain data) and medical translation task2 (indomain data), but also acquire addition indomain bilingual translations (i.e. dictionary) and monolingual data from online sources. First of all, we collect the medical terminologies from the web. This tiny but significant parallel data are helpful to reduce the out-ofvocabulary words (OOVs) in translation models. In addition, the use of larger language models du</context>
</contexts>
<marker>Esplà-Gomis, Forcada, 2010</marker>
<rawString>M. Esplà-Gomis and M. L. Forcada. 2010. Combining Content-Based and URL-Based Heuristics toHarvest Aligned Bitexts from Multilingual Sites with Bitextor. The Prague Bulletin of Mathemathical Lingustics, 93:77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<date>2008</date>
<booktitle>Parallel Implementations of Word Alignment Tool. Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="11967" citStr="Gao and Vogel, 2008" startWordPosition="1846" endWordPosition="1849">ehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. 5 Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. 10 http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview. 12 http://www.speech.sri.com/projects/srilm/. (2) 235 Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Docs In-domain cs/en 1,7</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pp. 49-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<contexts>
<context position="2598" citStr="Heafield, 2011" startWordPosition="378" endWordPosition="379">. In our experiments, we not only use all available training data provided by the WMT2014 standard translation task1 (generaldomain data) and medical translation task2 (indomain data), but also acquire addition indomain bilingual translations (i.e. dictionary) and monolingual data from online sources. First of all, we collect the medical terminologies from the web. This tiny but significant parallel data are helpful to reduce the out-ofvocabulary words (OOVs) in translation models. In addition, the use of larger language models during decoding is aided by more efficient storage and inference (Heafield, 2011). Thus, we crawl more in-domain monolingual data from the Internet based on domain focused web-crawling approach. In order to detect and remove outdomain data from the crawled data, we not only explore text-to-topic classifier, but also propose an alternative filtering approach combined the existing one (text-to-topic classifier) with perplexity. After carefully pre-processing all the available training data, we apply language model adaptation and translation model adaptation using various kinds of training corpora. Experimental results show that the presented approaches are helpful to further</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>andWei-Jing Zhu ToddWard</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA.</location>
<marker>Papineni, Roukos, ToddWard, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, ToddWard, andWei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="11794" citStr="Koehn et al., 2007" startWordPosition="1821" endWordPosition="1824">ate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. 5 Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. 10 http://www.statmt.org/moses/. 11 http://www</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL, pages 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2nd ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<contexts>
<context position="11371" citStr="Koehn and Schroeder, 2007" startWordPosition="1753" endWordPosition="1756">official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. 5 Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the 2nd ACL Workshop on Statistical Machine Translation, pages 224-227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Kohlschütter</author>
<author>Peter Fankhauser</author>
<author>Wolfgang Nejdl</author>
</authors>
<title>Boilerplate detection using shallow text features.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>441--450</pages>
<contexts>
<context position="7541" citStr="Kohlschütter et al., 2010" startWordPosition="1150" endWordPosition="1153">tting the pages within the http domain guided by the seed links. We acquired the list from the Open Directory Project6, which is a repository maintained by volunteer editors. Totally, we collected 12,849 URLs from the medicine category. Text normalization is to convert the text of each HTML page into UTF-8 encoding according to the content_charset of the header. In addition, HTML pages often consist of a number of irrelevant contents such as the navigation links, advertisements disclaimers, etc., which may negatively affect the performance of SMT system. Therefore, we use the Boilerpipe tool (Kohlschütter et al., 2010) to filter these noisy data and preserve the useful content that is marked by the tag, &lt;canonicalDocument&gt;. The resulting text is saved in an XML file, which will be further processed by the subsequent tasks. For language identification, we use the languagedetection7 toolkit to determine the possible lan5 http://combine.it.lth.se/documentation/DocMain/node6.html. 6 http://www.dmoz.org/Health/Medicine/. 7 https://code.google.com/p/language-detection/. 234 guage of the text, and discard the articles which are in the right language we are interested. 2.3 Data Filtering The web-crawled documents (</context>
</contexts>
<marker>Kohlschütter, Fankhauser, Nejdl, 2010</marker>
<rawString>Christian Kohlschütter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text features. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining, pages 441-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL: Short Papers,</booktitle>
<pages>220--224</pages>
<contexts>
<context position="10874" citStr="Moore and Lewis, 2010" startWordPosition="1677" endWordPosition="1680">uplicate data may lead to unpredicted behavior during the decoding. Therefore, we keep only the distinct sentences in monolingual corpus. By taking into account multiple translations in parallel corpus, we remove the duplicate sentence pairs. We also use a biomedical sentence splitter9 (Rune et al., 2007) to split sentences in monolingual corpora. The statistics of the data are provided in Table 2. 4 Baseline System We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of b</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of ACL: Short Papers, pages 220-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="12302" citStr="Och, 2003" startWordPosition="1897" endWordPosition="1898">ur final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. 10 http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview. 12 http://www.speech.sri.com/projects/srilm/. (2) 235 Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Docs In-domain cs/en 1,770,421 9,373,482/ 134,998/ 5.29/ Parallel Data 10,605,222 156,402 5.99 de/en 3,894,099 52,211,730/ 1,146,262/ 13.41/ 58,544,608 487,850 15.03 fr/en 4,579,533 77,866,237/ 495,856/ 17.00/ 68,429,649 556,587 14.94 General- cs/en 12,426,374 180,349,215/ 1,614,023/ 14.51/ domain 183,841,805 1,661,830 14.79 Parallel Data de/en 4,421,961 10</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. Proceedings of ACL, pp. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="11945" citStr="Och and Ney, 2003" startWordPosition="1842" endWordPosition="1845">od described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. 5 Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. 10 http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview. 12 http://www.speech.sri.com/projects/srilm/. (2) 235 Data Set Lang. Sent. Words Vocab. Ave. Len. Sites Do</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
<author>A Toral</author>
<author>A Way</author>
<author>V Papavassiliou</author>
<author>P Prokopidis</author>
<author>M Giagkou</author>
</authors>
<title>Towards Using WebCrawled Data for Domain Adaptation in Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Annual Conference of the European Associtation for Machine Translation,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="1940" citStr="Pecina et al., 2011" startWordPosition="272" endWordPosition="275">irs and the third best results for reminding pairs. 1 Introduction In this paper, we report the experiments carried out by the NLP2CT Laboratory at University of Macau for WMT2014 medical sentence translation task on six language pairs: Czech-English (cs-en), French-English (fr-en), German-English (de-en) and the reverse direction pairs (i.e., en-cs, en-fr and en-de). As data in specific domain are usually relatively scarce, the use of web resources to complement the training resources provides an effective way to enhance the SMT systems (Resnik and smith, 2003; Esplà-Gomis and Forcada, 2010; Pecina et al., 2011; Pecina et al., 2012; Pecina et al., 2014). In our experiments, we not only use all available training data provided by the WMT2014 standard translation task1 (generaldomain data) and medical translation task2 (indomain data), but also acquire addition indomain bilingual translations (i.e. dictionary) and monolingual data from online sources. First of all, we collect the medical terminologies from the web. This tiny but significant parallel data are helpful to reduce the out-ofvocabulary words (OOVs) in translation models. In addition, the use of larger language models during decoding is aide</context>
</contexts>
<marker>Pecina, Toral, Way, Papavassiliou, Prokopidis, Giagkou, 2011</marker>
<rawString>P. Pecina, A. Toral, A. Way, V. Papavassiliou, P. Prokopidis, and M. Giagkou. 2011. Towards Using WebCrawled Data for Domain Adaptation in Statistical Machine Translation. In Proceedings of the 15th Annual Conference of the European Associtation for Machine Translation, pages 297-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
<author>A Toral</author>
<author>V Papavassiliou</author>
<author>P Prokopidis</author>
<author>J van Genabith</author>
<author>R I C Athena</author>
</authors>
<title>Domain adaptation of statistical machine translation using web-crawled resources: a case study.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Annual Conference of the European Association for Machine Translation,</booktitle>
<pages>145--152</pages>
<marker>Pecina, Toral, Papavassiliou, Prokopidis, van Genabith, Athena, 2012</marker>
<rawString>P. Pecina, A. Toral, V. Papavassiliou, P. Prokopidis, J. van Genabith, and R. I. C. Athena. 2012. Domain adaptation of statistical machine translation using web-crawled resources: a case study. In Proceedings of the 16th Annual Conference of the European Association for Machine Translation, pp. 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
<author>O Dušek</author>
<author>L Goeuriot</author>
<author>J Haji6</author>
<author>J Hlavá6ová</author>
<author>G J Jones</author>
<author>Z Urešová</author>
</authors>
<title>Adaptation of machine translation for multilingual information retrieval in the medical domain. Artificial intelligence in medicine,</title>
<date>2014</date>
<pages>1--25</pages>
<marker>Pecina, Dušek, Goeuriot, Haji6, Hlavá6ová, Jones, Urešová, 2014</marker>
<rawString>P. Pecina, O. Dušek, L. Goeuriot, J. Haji6, J. Hlavá6ová, G. J. Jones, and Z. Urešová. 2014. Adaptation of machine translation for multilingual information retrieval in the medical domain. Artificial intelligence in medicine, pages 1-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The Web as a parallel corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--349</pages>
<marker>Resnik, Smith, 2003</marker>
<rawString>Philip Resnik and Noah A. Smith. 2003. The Web as a parallel corpus. Computational Linguistics, 29:349–380</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
</authors>
<title>Antonio Toral, Santiago Cortés Vaíllo,</title>
<date></date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>213--218</pages>
<marker>Rubino, </marker>
<rawString>Raphael Rubino, Antonio Toral, Santiago Cortés Vaíllo, Jun Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu. 2013. The CNGL-DCU-Prompsit translation systems for WMT13. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 213-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sætre Rune</author>
</authors>
<title>Kazuhiro Yoshida, Akane Yakushiji, Yusuke Miyao, Yuichiro Matsubayashi and Tomoko Ohta.</title>
<date>2007</date>
<booktitle>AKANE System: Protein-Protein Interaction Pairs in BioCreAtIvE2 Challenge, PPIIPS subtask. In Proceedings of the Second BioCreative Challenge Evaluation Workshop,</booktitle>
<pages>209--212</pages>
<marker>Rune, 2007</marker>
<rawString>Sætre Rune, Kazuhiro Yoshida, Akane Yakushiji, Yusuke Miyao, Yuichiro Matsubayashi and Tomoko Ohta. 2007. AKANE System: Protein-Protein Interaction Pairs in BioCreAtIvE2 Challenge, PPIIPS subtask. In Proceedings of the Second BioCreative Challenge Evaluation Workshop, pp. 209-212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM-an extensible language modeling toolkit. Proceedings of the International Conference on Spoken Language Processing, pp. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
</authors>
<title>Hybrid selection of language model training data using linguistic information and perplexity.</title>
<date>2013</date>
<booktitle>In ACL Workshop on Hybrid Machine Approaches to Translation.</booktitle>
<contexts>
<context position="8815" citStr="Toral, 2013" startWordPosition="1345" endWordPosition="1346">ta, which would harm the domain-specific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top I percentage of ranked sentences are considered as additional relevant in-domain data. (T) (s) where s is a input sentence or document, P(T) is the probability of n -gram segments estimated from the training set. Word is the number of token</context>
</contexts>
<marker>Toral, 2013</marker>
<rawString>Antonio Toral. 2013. Hybrid selection of language model training data using linguistic information and perplexity. In ACL Workshop on Hybrid Machine Approaches to Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longyue Wang</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Yi Lu</author>
<author>Junwen Xing</author>
</authors>
<title>A Systematic Comparison of Data Selection Criteria for SMT Domain Adaptation.</title>
<date>2014</date>
<journal>The Scientific World Journal,</journal>
<volume>vol.</volume>
<pages>pages.</pages>
<contexts>
<context position="8802" citStr="Wang et al., 2014" startWordPosition="1341" endWordPosition="1344">er of out-domain data, which would harm the domain-specific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top I percentage of ranked sentences are considered as additional relevant in-domain data. (T) (s) where s is a input sentence or document, P(T) is the probability of n -gram segments estimated from the training set. Word is the nu</context>
</contexts>
<marker>Wang, Wong, Chao, Lu, Xing, 2014</marker>
<rawString>Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, and Junwen Xing. 2014. A Systematic Comparison of Data Selection Criteria for SMT Domain Adaptation. The Scientific World Journal, vol. 2014, Article ID 745485, 10 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longyue Wang</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Yi Lu</author>
<author>Junwen Xing</author>
</authors>
<title>iCPE: A Hybrid Data Selection Model for SMT Domain Adaptation. Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data.</title>
<date>2013</date>
<pages>280--290</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="8783" citStr="Wang et al., 2013" startWordPosition="1337" endWordPosition="1340"> may consist a number of out-domain data, which would harm the domain-specific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top I percentage of ranked sentences are considered as additional relevant in-domain data. (T) (s) where s is a input sentence or document, P(T) is the probability of n -gram segments estimated from the training </context>
</contexts>
<marker>Wang, Wong, Chao, Lu, Xing, 2013</marker>
<rawString>Longyue Wang, Derek F. Wong, Lidia S. Chao, Yi Lu, Junwen Xing. 2013. iCPE: A Hybrid Data Selection Model for SMT Domain Adaptation. Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. Springer Berlin Heidelberg. pages, 280-290</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
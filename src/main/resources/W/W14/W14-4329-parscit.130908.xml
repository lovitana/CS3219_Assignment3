<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.9646845">
Learning to Re-rank for Interactive Problem Resolution and Query
Refinement
</title>
<author confidence="0.972952">
Rashmi Gangadharaiah Balakrishnan Narayanaswamy and Charles Elkan
</author>
<affiliation confidence="0.997476">
IBM Research, Department of CSE,
India Research Lab, University of California, San Diego
</affiliation>
<address confidence="0.608354">
Bangalore, KA, India La Jolla, CA, USA
</address>
<email confidence="0.996176">
rashgang@in.ibm.com {muralib, elkan}@cs.ucsd.edu
</email>
<sectionHeader confidence="0.997352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998036">
We study the design of an information re-
trieval (IR) system that assists customer
service agents while they interact with
end-users. The type of IR needed is
difficult because of the large lexical gap
between problems as described by cus-
tomers, and solutions. We describe an
approach that bridges this lexical gap by
learning semantic relatedness using tensor
representations. Queries that are short and
vague, which are common in practice, re-
sult in a large number of documents be-
ing retrieved, and a high cognitive load
for customer service agents. We show
how to reduce this burden by providing
suggestions that are selected based on the
learned measures of semantic relatedness.
Experiments show that the approach offers
substantial benefit compared to the use of
standard lexical similarity.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924272727273">
Information retrieval systems help businesses and
individuals make decisions by automatically ex-
tracting actionable intelligence from large (un-
structured) data (Musen et al., 2006; Antonio
Palma-dos Reis, 1999). This paper focuses on the
application of retrieval systems in a contact cen-
ters where the system assists agents while they are
helping customers with problem resolution.
Currently, most contact center information re-
trieval use (web based) front-ends to search en-
gines indexed with knowledge sources (Holland,
2005). Agents enter queries to retrieve documents
related to the customer’s problem. These sources
are often incomplete as it is unlikely that all pos-
sible customer problems can be identified before
product release. This is particularly true for re-
cently released and frequently updated products.
One approach, which we build on here, is to mine
problems and resolutions from online discussion
forums Yahoo! Answers1 Ubuntu Forums2 and
Apple Support Communities3. While these often
provide useful solutions within hours or days of
a problem surfacing, they are semantically noisy
(Gangadharaiah and Narayanaswamy, 2013).
Most contact centers and agents are evaluated
based on the number of calls they handle over a
period (Pinedo et al., 2000). As a result, queries
entered by agents into the search engine are usu-
ally underspecified. This, together with noise in
the database, results in a large number of docu-
ments being retrieved as relevant documents. This
in turn, increases the cognitive load on agents, and
reduces the effectiveness of the search system and
the efficiency of the contact center. Our first task
in this paper is to automatically make candidate
suggestions that reduce the search space of rel-
evant documents in a contact center application.
The agent/user then interacts with the system by
selecting one of the suggestions. This is used to
expand the original query and the process can be
repeated. We show that even one round of inter-
action, with a small set of suggestions, can lead to
high quality solutions to user problems.
In query expansion, the classical approach is to
automatically find suggestions either in the form
of words, phrases or similar queries (Kelly et al.,
2009; Feuer et al., 2007; Leung et al., 2008).
These can be obtained either from query logs or
based on their representativeness of the initial re-
trieved documents (Guo et al., 2008; Baeza-yates
et al., 2004). The suggestions are then ranked ei-
ther based on their frequencies or based on their
similarity to the original query (Kelly et al., 2009;
Leung et al., 2008). For example, if suggestions
and queries are represented as term vectors (e.g.
</bodyText>
<footnote confidence="0.999959">
1http://answers.yahoo.com/
2http://ubuntuforums.org/
3https://discussions.apple.com/
</footnote>
<page confidence="0.860683">
218
</page>
<note confidence="0.749073">
Proceedings of the SIGDIAL 2014 Conference, pages 218–227,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999749523809524">
term frequency-inverse document frequency or tf-
idf) their similarity may be determined using simi-
larity measures such as cosine similarity or inverse
of euclidean distance (Salton and McGill, 1983).
However, in question-answering and problem-
resolution domains, and in contrast to traditional
Information Retrieval, most often the query and
the suggestions do not have many overlapping
words. This leads to low similarity scores, even
when the suggestion is highly relevant. Consider
the representative example in Table 1, taken from
our crawled dataset. Although the suggestions,
“does not support file transfer”, “connection not
stable”, “pairing failed” are highly relevant for the
problem of “Bluetooth not working”, their lexi-
cal similarity score is zero. The second task that
this paper addresses is how to bridge this lexical
chasm between the query and the suggestions. For
this, we learn a measure of semantic-relatedness
between the query and the suggestions rather than
defining closeness based on lexical similarity.
</bodyText>
<tableCaption confidence="0.644885">
Query Bluetooth not working .
Suggestions devices not discovered,
bluetooth greyed out,
bluetooth device did not respond,
does not support file transfer,
connection not stable,
pairing failed
Table 1: Suggestions for the Query or customer’s
problem, “Bluetooth not working”.
</tableCaption>
<bodyText confidence="0.970308">
The primary contributions of this paper are that:
</bodyText>
<listItem confidence="0.973935117647059">
• We show how tensor methods can be used
to learn measures of question-answer or
problem-resolution similarity. In addition,
we show that these learned measures can
be used directly with well studied classifica-
tion techniques like Support Vector Machines
(SVMs) and Logistic Classifiers to classify
whether suggestions are relevant. This results
in substantially improved performance over
using conventional similarity metrics.
• We show that along with the learned similar-
ity metric, a data dependent Information Gain
(which incorporates knowledge about the set
of documents in the database) can be used as
a feature to further boost accuracy.
• We demonstrate the efficacy of our approach
on a complete end-to-end problem-resolution
</listItem>
<bodyText confidence="0.901982333333333">
system, which includes crawled data from
online forums and gold standard user inter-
action annotations.
</bodyText>
<sectionHeader confidence="0.864577" genericHeader="method">
2 System outline
</sectionHeader>
<bodyText confidence="0.999984878048781">
As discussed in the Introduction, online discus-
sion forums form a rich source of problems and
their corresponding resolutions. Thread initiators
or users of a product facing problems with their
product post in these forums. Other users post
possible solutions to the problem. At the same
time, there is noise due to unstructured content,
off-topic replies and other factors. Our interac-
tion system has two phases, as shown in Figure
1. The offline phase attempts to reduce noise in
the database, while the online phase assists users
deal with the cognitive overload caused by a large
set of retrieved documents. In our paper, threads
form the documents indexed by the system.
The goals of the offline phase are two-fold.
First, to reduce the aforementioned noise in the
database, we succinctly represent each document
(i.e., a thread in online discussion forums) by its
signature, which is composed of units extracted
from the first post of the underlying thread that
best describe the problem discussed in the thread.
Second, the system makes use of click-through
data, where users clicked on relevant suggestions
for their queries to build a relevancy model. As
mentioned before, the primary challenge is to
build a model that can identify units that are se-
mantically similar to a given query.
In the online phase, the agent who acts as the
mediator between the user and the Search Engine
enters the user’s/customer’s query to retrieve rele-
vant documents. From these retrieved documents,
the system then obtains candidate suggestions and
ranks these suggestions using the relevancy model
built in the offline phase to further better under-
stand the query and thereby reduce the space of
documents retrieved. The user then selects the
suggestion that is most relevant to his query. The
retrieved documents are then filtered displaying
only those documents that contain the selected
suggestion in their signatures. The process con-
tinues until the user quits.
</bodyText>
<subsectionHeader confidence="0.999096">
2.1 Signatures of documents
</subsectionHeader>
<bodyText confidence="0.99992275">
In the offline phase, every document (correspond-
ing to a thread in online discussion forums) is
represented by units that best describe a problem.
We adopt the approach suggested in (Gangadhara-
</bodyText>
<page confidence="0.998242">
219
</page>
<bodyText confidence="0.999873842105263">
iah and Narayanaswamy, 2013) to automatically
generate these signatures from each discussion
thread. We assume that the first post describes
the user’s problem, something we have found to
be true in practice. From the dependency parse
trees of the first posts, we extract three types of
units (i) phrases (e.g., sync server), (ii) attribute-
values (e.g., iOS, 4) and (iii) action-attribute tuples
(e.g., sync server: failed). Phrases form good base
problem descriptors. Attribute-value pairs provide
configurational contexts to the problem. Action-
attribute tuples, as suggested in (Gangadharaiah
and Narayanaswamy, 2013), capture segments of
the first post that indicate user wanting to perform
an action (“I cannot hear notifications on blue-
tooth”) or the problems caused by a user’s action
(“working great before I updated”). These make
them particularly valuable features for problem-
resolution and question-answering.
</bodyText>
<subsectionHeader confidence="0.998682">
2.2 Representation of Queries and
Suggestions
</subsectionHeader>
<bodyText confidence="0.99998362962963">
Queries are represented as term vectors using the
term frequency-inverse document frequency (tf-
idf) representation forming the query space. The
term frequency is defined as the frequency with
which word appears in the query and the inverse
document frequency for a word is defined as the
frequency of queries in which the word appeared.
Similarly, units are represented as tf-idf term vec-
tors from the suggestion space. Term frequency in
the unit space is defined as the number of times
a word appears in the unit and its inverse docu-
ment frequency is defined in terms of the number
of units in which the word appeared. Since the
vocabulary used in the queries and documents are
different, the representations for queries and units
belong to different spaces of different dimensions.
For every query-unit pair, we learn a measure
of similarity as explained in Section 4. Addi-
tionally, we use similarity features based on co-
sine similarity between the query and the unit un-
der consideration. We also consider an additional
feature based on information gain (Gangadhara-
iah and Narayanaswamy, 2013). In particular, if
S represents the set all retrieved documents, S1 is
a subset of S (S1 ⊆ S) containing a unit uniti and
S2 is a subset of S that does not contain uniti,
information gain with uniti is,
</bodyText>
<equation confidence="0.993804">
Gain(S, uniti) = E(S) − |S1 ||S|E(S1) − |S2 ||S|E(S2) (1)
E(S) = � −p(dock)log2p(dock). (2)
k=1,...|S|
</equation>
<bodyText confidence="0.99998">
The probability for each document is based on its
rank in the retrieved of results,
</bodyText>
<equation confidence="0.995356666666667">
1
rank(docj) (3)
Ek=1 ,... |S |rank(dock)
</equation>
<bodyText confidence="0.9999968">
We crawled posts and threads from online forums
for the products of interest, as detailed in Sec-
tion 5.1, and these form the documents. We used
trial interactions and retrievals to collect the click-
though data, which we used as labeled data for
similarity metric learning. In particular, labels in-
dicate which candidate units were selected as rel-
evant suggestions by a human annotator. We now
explain our training (offline) and testing (online)
phases that use this data in more detail.
</bodyText>
<subsectionHeader confidence="0.994725">
2.3 Training
</subsectionHeader>
<bodyText confidence="0.99999635483871">
The labeled (click-through) data for training the
relevance model is collected as follows. Anno-
tators were given pairs of queries. Each pair is
composed of an underspecified query and a spe-
cific query (Section 5.1 provides more informa-
tion on the creation of these queries). An un-
derspecified query is a query that reflects what a
user/agent typically enters into the system, and the
corresponding specific query is full-specified ver-
sion of the underspecified query. Annotators were
first asked to query the search engine with each
underspecified query. We use the Lemur search
engine (Strohman et al., 2004). From the resulting
set of retrieved documents, the system uses the in-
formation gain criteria (as given in (1) below) to
rank and display to the annotators the candidate
suggestions (i.e., the units that appear in the signa-
tures of the retrieved documents). Thus, our sys-
tem is bootstrapped using the information gain cri-
terion. The annotators then selects the candidate
suggestion that is most relevant to the correspond-
ing specific query. The interaction with the system
continues until the annotators quit.
We then provide a class label for each unit based
on the collected click-through information. In par-
ticular, if a unit s E S(x) was clicked by a user for
his query x, from the list S we provide a + la-
bel to indicate that the unit is relevant suggestion
for the query. Similarly, for all other units that are
never clicked by users for x are labeled as −. This
forms the training data for the system. Details on
</bodyText>
<equation confidence="0.965356">
p(docj) =
</equation>
<page confidence="0.984789">
220
</page>
<figureCaption confidence="0.772838">
Figure 1: Outline of our interactive query refine-
ment system for problem resolution
the feature extraction and how the model is created
is given in Section 3.
</figureCaption>
<subsectionHeader confidence="0.998096">
2.4 Testing
</subsectionHeader>
<bodyText confidence="0.999992636363636">
In the online phase, the search engine retrieves
documents for the user’s query x&apos;. Signatures for
the retrieved documents form the initial space of
candidate units. As done in training, for every pair
of x&apos; and unit the label is predicted using the model
built in the training phase. Units that are predicted
as + are shown to the user. When a user clicks
on his most relevant suggestion, the retrieved re-
sults are filtered to show only those documents that
contain the suggestion (i.e., in its signature). This
process continues until the user quits.
</bodyText>
<sectionHeader confidence="0.99481" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999815928571429">
We consider underspecified queries x E Rxd and
units y E Ryd. Given an underspecified query x
we pass it through a search engine, resulting in a
list of results S(x).
As explained in Section 2.3, our training data
consists of labels r(x, y) E +1,−1 for each
under-specified query, y E S(x). r(x, y) = +1
if the unit is labeled a relevant suggestion and
r(x, y) = −1 if it is not labeled relevant. Units
are relevant or not based on the final query, and
not just y, a distinction we expand upon below.
At each time step, our system proposes a list
Z(x) of possible query refinement suggestions z
to the user. The user can select one or none of
these suggestions. If the user selects z, only those
documents that contain the suggestion (i.e., in its
signature) are shown to the user, resulting in a fil-
tered set of results, S(x + z).
This process can be repeated until a stopping
criterion is reached. Stopping criterion include the
size of the returned list is smaller than some num-
ber |S(x + z) |&lt; N, in which case all remain-
ing documents are returned. Special cases include
when only one document is returned N = 1. We
will design query suggestions so that |S(x+z) |&gt;
0. Another criterion we use is to return all remain-
ing documents after a certain maximum number of
interactions or until the user quits.
</bodyText>
<sectionHeader confidence="0.977119" genericHeader="method">
4 Our Approach
</sectionHeader>
<bodyText confidence="0.998181588235294">
We specify our algorithm using a tensor notation.
We do this since tensors appear to subsume most
of the methods applied in practice, where different
algorithms use slightly different costs, losses and
constraints. These ideas are strongly motivated by,
but generalize to some extent, suggestions for this
problem presented in (Elkan, 2010).
For our purposes, we consider tensors as multi-
dimensional arrays, with the number of dimen-
sions defined as the order of the tensor. An M
order tensor X E RI1×I2...IM. As such tensors
subsume vectors (1st order tensors) and matrices
(2nd order tensors). The vectorization of a ten-
sor of order M is obtained by stacking elements
from the M dimensions into a vector of length
I1 x I2 x ... x IM in the natural way.
The inner product of two tensors is defined as
</bodyText>
<equation confidence="0.895221">
xi1wi1xi2wi2 ... xiM wiM
(4)
</equation>
<bodyText confidence="0.993104117647059">
Analogous to the definition for vectors, the
(Kharti-Rao) outer product A = X ® W of two
tensors X and W has Aij = XiWj where i and j
run over all elements of X and W. Thus, if X is
of order MX and W of order MW, A is of order
MA = MX + MW.
The particular tensor we are interested in is a
2-D tensor (matrix) X which is the outer product
of query and unit pairs (Feats). In particular, for a
query x and unit y, Xi,j = xiyj.
Given this representation, standard classifica-
tion and regression methods from the machine
learning literature can often be extended to deal
with tensors. In our work we consider two clas-
sifiers that have been successful in many applica-
tions, logistic regression and support vector ma-
chines (SVMs) (Bishop, 2006).
</bodyText>
<figure confidence="0.894017333333334">
Forum Discussion
Threads
User clicks on
(units, query)
query
Search Engine
Learn Relevance
Model
Unit Extraction
results
Interaction Module
Finds suggestions
Candidate
Suggestions
Suggestion units
for first posts
Offline
Online
(X, W) = �I1 I2 . . . IM
i1 � iM
i2
</figure>
<page confidence="0.990581">
221
</page>
<bodyText confidence="0.999539">
In the case of logistic regression, the conditional
probability of a reward signal r(X) = r(x, y) is,
</bodyText>
<equation confidence="0.9930225">
1
p(r(X) = +1) = 1 + exp(−(X,W) + b) (5)
</equation>
<bodyText confidence="0.9983865">
The parameters W and b can be obtained by min-
imizing the log loss Greg on the training data D
</bodyText>
<equation confidence="0.998862666666667">
Greg(W, b) = (6)
� log(1 + exp(−r(X)(X,W) + b)
(X,r(X))ED
</equation>
<bodyText confidence="0.744039">
For SVMs with the hinge loss we select param-
eters to minimize Ghinge,
</bodyText>
<equation confidence="0.997941666666667">
Ghinge(W, b) = ||X||2F + (7)
A � max[0,1 − (r(X)(X, W) + b)]
(X,r(X))ED
</equation>
<bodyText confidence="0.99924025">
where ||X||F is the Frobenius norm of tensor X.
Given the number of parameters in our system
(W, b) to limit overfitting, we have to regularize
these parameters. We use regularizers of the form
</bodyText>
<equation confidence="0.995121">
Q(W,b) = AW||W||F (8)
</equation>
<bodyText confidence="0.989771982456141">
such regularizes have been successful in many
large scale machine learning tasks including
learning of high dimensional graphical models
(Ravikumar et al., 2010) and link prediction
(Menon and Elkan, 2011).
Thus, the final optimization problem we are
faced with is of the form
min G(W, b) + Q(W, b) (9)
W,b
where G is Greg or Ghinge as appropriate. Other
losses, classifiers and regularizers may be used.
The advantage of tensors over their vectorized
counterparts, that may be lost in the notation, is
that they do not lose the information that the dif-
ferent dimensions can (and in our case do) lie in
different spaces. In particular, in our case we use
different features to represent queries and units (as
discussed in Section 2.2) which are not of the same
length, and as a result trivially do not lie in the
same space.
Tensor methods also allow us to regularize the
components of queries and units separately in dif-
ferent ways. This can be done for example by,
i) forcing W = Q1Q2, where Q1 and Q2 are
constrained to be of fixed rank s ii) using trace or
Frobenius norms on Q1 and Q2 for separate regu-
larization as proxies for the rank iii) using different
sparsity promoting norms on the rows of Q1 and
Q2 iv) weighing these penalties differently for the
two matrices in the final loss function. Note that
by analogy to the vector case, we directly obtain
generalization error guarantees for our methods.
We also discuss the advantage of the tensor
representation above over a natural representation
X = [x; y] i.e. X is the column vector obtained
by stacking the query and unit representations.
Note that in this representation, for logistic regres-
sion, while a change in the query x can change
the probability for a unit P(r(X) = 1) it can-
not change the relative probability of two different
units. Thus, the ordering of all unit remains the
same for all queries. This flaw has been pointed
out in the literature in (Vert and Jacob, 2008) and
(Bai et al., 2009), but was brought to our attention
by (Elkan, 2010).
Finally, we note that by normalizing the query
and unit vectors (x and y), and selecting W = I
(the identity matrix) we can recover the cosine
similarity metric (Elkan, 2010). Thus, our rep-
resentation is atleast as accurate and we show
that learning the diagonal and off-diagonal com-
ponents of W can substantially improve accuracy.
Additionally, for every (query,unit) we also
compute information gain (IG) as given in (1), and
the lexical similarity (Sim) in terms of cosine sim-
ilarity between the query and the unit as additional
features in the feature vectors.
</bodyText>
<sectionHeader confidence="0.999746" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999972333333333">
To evaluate our system, we built and simulated
a contact center information retrieval system for
iPhone problem resolution.
</bodyText>
<subsectionHeader confidence="0.999241">
5.1 Description of the Dataset
</subsectionHeader>
<bodyText confidence="0.999941416666667">
We collected data by crawling forum discussion
threads from the Apple Discussion Forum, created
during the period 2007-2011, resulting in about
147,000 discussion threads. The underspecified
queries and specific queries were created as fol-
lows. Discussion threads were first clustered treat-
ing each discussion thread as a data point using a
tf-idf representation. The thread nearest the cen-
troid of the 60 largest clusters were marked as the
‘most common’ problems.
The first post is used as a proxy for the problem
description. An annotator was asked to then create
</bodyText>
<page confidence="0.98804">
222
</page>
<figure confidence="0.998856222222222">
0.06
0.05
0.02
0.01
00 500 1000 1500 2000 2500 3000 3500 4000
Sim
Feats−IG−Sim
Feats+IG+Sim
Number of training query−suggestion pairs
</figure>
<figureCaption confidence="0.755929">
Figure 2: Performance with Logistic Regression
using different features and various sizes of Train-
ing and Test sets. Feats-IG-Sim does not use co-
sine similarity (Sim) and information gain (IG).
Feats+IG+Sim considers Sim and IG.
set. Figure 2 shows error rate obtained with logis-
tic regression (a similar trend was observed with
SVMs) on various sizes of the training data and
test data. The plot shows that the model (Feats-
IG-Sim and Feats+IG+Sim) performs significantly
better at predicting the relevancy of units for un-
derspecified queries when compared to just us-
ing cosine similarity (Sim) as a feature. Feats-
IG-Sim does not make use of cosine similarity
as a feature or the information gain feature while
Feats+IG+Sim uses both these features for train-
ing the relevancy model and for predicting the rel-
evancy of units. As expected the performance of
the classifier improves as the size of the training
data is increased.
</figureCaption>
<subsubsectionHeader confidence="0.597636">
5.2.2 Evaluating the Interaction Engine
</subsubsectionHeader>
<bodyText confidence="0.997995529411765">
We evaluate a complete system with both the user
(the agent) and the search engine in the loop. We
measure the value of the interactions by an analy-
sis of which results ‘rise to the top’. Users were
given a specific query and its underspecified query
along with the results obtained when the under-
specified query was input to the search engine.
They were presented with suggestions that were
predicted + for the underspecified query using
SVMs. The user was asked to select the most ap-
propriate suggestion that made the underspecified
query more specific. This process continues until
the user quits either because he is satisfied with the
retrieved results or does not obtain relevant sug-
gestions from the system. For example, for the
underspecified query in Table 2, one of the pre-
dicted suggestions was, “server:stopped respond-
</bodyText>
<figure confidence="0.989159555555556">
0.04
Error rate
0.03
Underspecified query “Safari not working”
1. safari:crashes
2. safari:cannot find:server
3. server:stopped responding
4. phone:freezes
5. update:failed
</figure>
<tableCaption confidence="0.879819">
Table 2: Specific Queries generated with the un-
derspecified Query, ”Safari not working”.
</tableCaption>
<bodyText confidence="0.989507777777778">
a short query (underspecified) from the first post
of each of the 60 selected threads. These queries
were given to the Lemur search engine (Strohman
et al., 2004) to retrieve the 50 most similar threads
from an index built on the entire set of 147,000
threads. The annotator manually analyzed the first
posts of the retrieved threads to create contexts,
resulting in a total 200 specific queries.
We give an example to illustrate the data cre-
ation in Table 2. From an under-specified query
“Safari not working”, the annotator found 5 spe-
cific queries. Two other annotators, were given
these specific queries with the search engine’s
results from the corresponding under-specified
query. They were asked to choose the most rel-
evant results for the specific queries. The intersec-
tion of the choices of the annotators formed our
‘gold standard’ of relevant documents.
</bodyText>
<subsectionHeader confidence="0.835654">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999985">
We simulated a contact center retrieval systems (as
in Figure 1) to evaluate the approach proposed in
this paper. To evaluate the generality of our ap-
proach we conduct experiments with both SVMs
and Logistic Regression. Due to lack of space we
illustrate each result for only one kind of classifier.
</bodyText>
<subsectionHeader confidence="0.970617">
5.2.1 Evaluating the Relevance Model
</subsectionHeader>
<bodyText confidence="0.999991066666667">
To measure the performance of the relevance
model for predicting the class labels or for finding
the most relevant units towards making the user’s
underspecified query more specific, we performed
the following experiment. 4000 random query-
unit pairs were picked from the training data, col-
lected as explained in Section 2. Since most units
are not relevant for a query, 90% of the pairs be-
longed to the − class. On average, every spe-
cific query gave rise to 2.4 suggestions. Hence,
predicting − for all pairs still achieves an error
rate of 10%. This data was then split into vary-
ing sizes of training and test sets. The relevancy
model was then built on the training half and the
classifiers were used to predict labels on the test
</bodyText>
<page confidence="0.997684">
223
</page>
<figure confidence="0.954765">
Success at
Size of retrieved list
</figure>
<figureCaption confidence="0.969641333333333">
Figure 3: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of MAP at N.
</figureCaption>
<figure confidence="0.49141">
Size of retrieved list
</figure>
<figureCaption confidence="0.984925">
Figure 4: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of Success at N.
</figureCaption>
<figure confidence="0.989585833333333">
1 2 3 4 5 6 7 8 9 10
2.5
2
1.5
1
0.5
0
Baseline
Feats−IG−Sim
Feats+IG+Sim
1 2 3 4 5 6 7 8 9 10
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
Baseline
Feats−IG−Sim
Feats+IG+Sim
Mean Average Precision
</figure>
<bodyText confidence="0.9985095">
ing”. If the user finds the suggestion relevant, he
clicks on it. The selected suggestion then reduces
the number of retrieved results. We then measured
the relevance of the reduced result, with respect
to the gold standard for that specific query, using
metrics used in IR - MRR, Mean Average Preci-
sion (MAP) and Success at rank N.
Figures 3, 4 and Table 3 evaluate the results ob-
tained with the interaction engine using Feats-IG-
Sim and Feats+IG+Sim. We compared the per-
formance of our algorithms with a Baseline that
does not perform any interaction and is evaluated
based on the retrieved results obtained with the un-
derspecified queries. The models for each of the
systems were trained using query-suggestion pairs
collected from 100 specific queries (data collected
as explained in Section 2). The remaining 100 spe-
cific queries were used for testing. We see that the
suggestions predicted by the classifiers using the
relevancy model indeed improves the performance
of the baseline. Also, adding the IG and Sim fea-
ture further boosts the performance of the system.
</bodyText>
<table confidence="0.99131425">
Systems MRR
Baseline 0.4218
Feats-IG-Sim 0.9449
Feats+IG+Sim 0.9968
</table>
<tableCaption confidence="0.998557">
Table 3: Comparison of the proposed approach
</tableCaption>
<bodyText confidence="0.724573">
with respect to the Baseline that does not involve
interaction in terms of MRR.
</bodyText>
<subsectionHeader confidence="0.633658">
5.3 Related Work
</subsectionHeader>
<bodyText confidence="0.999980394736842">
Learning affinities between queries an documents
is a well studied area. (Liu, 2009) provides an ex-
cellent survey of these approaches. In these meth-
ods, there is a fixed feature function Φ(x, y) de-
fined between any query-document pair. These
features are then used, along with labeled train-
ing data, to learn the parameters of a model that
can then be used to predict the relevance r(x, y)
of a new query-document pair. The output of the
model can also be used to re-rank the results of a
search engine. In contrast to this class of methods,
we define and parameterize the Φ function and
jointly optimize the parameters of the feature map-
ping and the machine learning re-ranking model.
Latent tensor methods for regression and clas-
sification have recently become popular in the im-
age and signal processing domain. Most of these
methods solve an optimization problem similar to
our own (9), but add additional constraints limit-
ing the rank of the learned matrix W either ex-
plicitly or implicit by defining W = Q1QT2 , and
defining Q1 ∈ Rd.×d and Q2 ∈ Rdy×d. This ap-
proach is used for example in (Pirsiavash et al.,
2009) and more recently in (Tan et al., 2013) (Guo
et al., 2012). While this reduces the number of pa-
rameters to be learned from dxdy to d(dx + dy) it
makes the problem non-convex and introduces an
additional parameter d that must be selected.
This approach of restricting the rank was re-
cently suggested for information retrieval in (Wu
et al., 2013). They look at a regression problem,
using click-through rates as the reward function
r(x, y). In addition, (Wu et al., 2013) does not
use an initial search engine and hence must learn
an affinity function between all query-document
pairs. In contrast to this, we learn a classification
function that discriminates between the true and
false positive documents that are deemed similar
</bodyText>
<page confidence="0.995519">
224
</page>
<bodyText confidence="0.999985016949153">
by the search engine. This has three beneficial ef-
fects : (i) it reduces the amount of labeled training
data required and the imbalance between the posi-
tive and negative classes which can make learning
difficult (He and Garcia, 2009) and (ii) allows us
to build on the strengths of fast and strong existing
search engines increasing accuracy and decreas-
ing retrieval time and (iii) allows the learnt model
to focus learning on the query-document pairs that
are most problematic for the search engine.
Bilinear forms of tensor models without the
rank restriction have recently been studied for link
prediction (Menon and Elkan, 2011) and image
processing (Kobayashi and Otsu, 2012). Since
the applications are different, there is no prelimi-
nary search engine which retrieves results, making
them ranking methods and ours a re-ranking ap-
proach. Related work in text IR includes (Beefer-
man and Berger, 2000), where two queries are
considered semantically similar if their clicks lead
to the same page. However, the probability that
different queries lead to common clicks of the
same URLs is very small, again increasing the
training data required. Approaches in the past
have also proposed techniques to automatically
find suggestions either in the form of words,
phrases (Kelly et al., 2009; Feuer et al., 2007;
Baeza-yates et al., 2004) or similar queries (Leung
et al., 2008) from query logs (Guo et al., 2008;
Baeza-yates et al., 2004) or based on their prob-
ability of representing the initial retrieved doc-
uments (Kelly et al., 2009; Feuer et al., 2007).
These suggestions are then ranked either based on
their frequencies or based on their closeness to the
query. Closeness is defined in terms of lexical sim-
ilarity to the query. However, most often the query
and the suggestions do not have any co-occurring
words leading to low similarity scores, even when
the suggestion is relevant.
(Gangadharaiah and Narayanaswamy, 2013)
use information gain to rank candidate sugges-
tions. However, the relevancy of the suggestions
highly depends on the relevancy of the initial re-
trieved documents. Our work here addresses the
question of how to bridge this lexical chasm be-
tween the query and the suggestions. For this, we
use semantic-relatedness between the query and
the suggestions as a measure of closeness rather
than defining closeness based on lexical similar-
ity. A related approach to handle this lexical gap
by applying alignment techniques from Statistical
Machine translation (Brown et al., 1993), in par-
ticular by building translation models for infor-
mation retrieval (Berger and Lafferty, 1999; Rie-
zler et al., 2007). These approaches require train-
ing data in the form of question-answer pairs, are
again limited to words or phrases and are not in-
tended for understanding the user’s problem better
through interaction, which is our focus.
</bodyText>
<sectionHeader confidence="0.988461" genericHeader="evaluation">
6 Conclusions, Discussions and Future
Work
</sectionHeader>
<bodyText confidence="0.99997365">
We studied the problem of designing Information
Retrieval systems for interactive problem resolu-
tion. We developed a system for bridging the
large lexical gap between short, incomplete prob-
lem queries and documents in a database of reso-
lutions. We showed that tensor representations are
a useful tool to learn measures of semantic relat-
edness, beyond the cosine similarity metric. Our
results show that with interaction, suggestions can
be effective in pruning large sets of retrieved doc-
uments. We showed that our approach offers sub-
stantial improvement over systems that only use
lexical similarities for retrieval and re-ranking, in
an end-to-end problem-resolution domain.
In addition to the classification losses consid-
ered in this paper, we can also use another loss
term based on ideas from recommender systems,
in particular (Menon and Elkan, 2011). Consider
the matrix T with all training queries as rows and
all units as the columns. If we view the query
refinement problem as a matrix completion prob-
lem, it is natural to assume that this matrix has low
rank, so that T can be written as T = UΛVT,
where Λ is a diagonal matrix and parameter of our
optimization. These can then be incorporated into
the training process by appropriate changes to the
cost and regularization terms.
Another benefit of the tensor representation is
that it can easily be extended to incorporate other
meta-information that may be available. For ex-
ample, if context sensitive features, like the iden-
tity of the agent, are available these can be incor-
porated as another dimension in the tensor. While
optimization over these higher dimensional ten-
sors may be more computationally complex, the
problems are still convex and can be solved ef-
ficiently. This is a direction of future research
we are pursuing. Finally, exploring the power of
information gain type features in larger database
systems is of interest.
</bodyText>
<page confidence="0.997798">
225
</page>
<sectionHeader confidence="0.996182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999800878504673">
Fatemeh Zahedi Antonio Palma-dos Reis. 1999. De-
signing personalized intelligent financial decision
support systems.
Ricardo Baeza-yates, Carlos Hurtado, and Marcelo
Mendoza. 2004. Query recommendation us-
ing query logs in search engines. In In Interna-
tional Workshop on Clustering Information over the
Web (ClustWeb, in conjunction with EDBT), Creete,
pages 588–596. Springer.
Bing Bai, Jason Weston, David Grangier, Ronan Col-
lobert, Kunihiko Sadamasa, Yanjun Qi, Corinna
Cortes, and Mehryar Mohri. 2009. Polynomial se-
mantic indexing. In NIPS, pages 64–72.
Doug Beeferman and Adam Berger. 2000. Agglomer-
ative clustering of a search engine query log. In Pro-
ceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ’00, pages 407–416, New York, NY, USA.
ACM.
Adam Berger and John Lafferty. 1999. Information
retrieval as statistical translation. In Proceedings of
the 22Nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ’99, pages 222–229, New York,
NY, USA. ACM.
Christopher M Bishop. 2006. Pattern recognition and
machine learning.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263–
311, June.
Charles Elkan. 2010. Learning affinity with biliear
models. Unpublished Notes.
Alan Feuer, Stefan Savev, and Javed A. Aslam. 2007.
Evaluation of phrasal query suggestions. In Pro-
ceedings of the Sixteenth ACM Conference on Con-
ference on Information and Knowledge Manage-
ment, CIKM ’07, pages 841–848, New York, NY,
USA. ACM.
Rashmi Gangadharaiah and Balakrishnan
Narayanaswamy. 2013. Natural language query
refinement for problem resolution from crowd-
sourced semi-structured data. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 243–251, Nagoya,
Japan, October. Asian Federation of Natural
Language Processing.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Sung-Hyon Myaeng, Douglas W. Oard,
Fabrizio Sebastiani, Tat-Seng Chua, and Mun-Kew
Leong, editors, SIGIR, pages 379–386. ACM.
Weiwei Guo, Irene Kotsia, and Ioannis Patras. 2012.
Tensor learning for regression. Image Processing,
IEEE Transactions on, 21(2):816–827.
Haibo He and Edwardo A Garcia. 2009. Learning
from imbalanced data. Knowledge and Data Engi-
neering, IEEE Transactions on, 21(9):1263–1284.
Alexander Holland. 2005. Modeling uncertainty in
decision support systems for customer call center.
In Computational Intelligence, Theory and Applica-
tions, pages 763–770. Springer.
Diane Kelly, Karl Gyllstrom, and Earl W. Bailey. 2009.
A comparison of query and term suggestion fea-
tures for interactive searching. In Proceedings of the
32Nd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
SIGIR ’09, pages 371–378, New York, NY, USA.
ACM.
Takumi Kobayashi and Nobuyuki Otsu. 2012. Effi-
cient optimization for low-rank integrated bilinear
classifiers. In Computer Vision–ECCV 2012, pages
474–487. Springer.
Kenneth Wai-Ting Leung, Wilfred Ng, and Dik Lun
Lee. 2008. Personalized concept-based clustering
of search engine queries. IEEE Trans. on Knowl.
and Data Eng., 20(11):1505–1518, November.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225–331.
Aditya Krishna Menon and Charles Elkan. 2011. Link
prediction via matrix factorization. In Machine
Learning and Knowledge Discovery in Databases,
pages 437–452. Springer.
Mark A Musen, Yuval Shahar, and Edward H Short-
liffe. 2006. Clinical decision-support systems.
Michael Pinedo, Sridhar Seshadri, and J George Shan-
thikumar. 2000. Call centers in financial services:
strategies, technologies, and operations. In Cre-
ating Value in Financial Services, pages 357–388.
Springer.
Hamed Pirsiavash, Deva Ramanan, and Charless
Fowlkes. 2009. Bilinear classifiers for visual recog-
nition. In NIPS, pages 1482–1490.
Pradeep Ravikumar, Martin J Wainwright, and John D
Lafferty. 2010. High-dimensional ising model se-
lection using 1-regularized logistic regression. The
Annals of Statistics, 38(3):1287–1319.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 464–471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
</reference>
<page confidence="0.980971">
226
</page>
<reference confidence="0.99862152631579">
Gerard Salton and Michael J McGill. 1983. Introduc-
tion to modern information retrieval.
T. Strohman, D. Metzler, H. Turtle, and W. B. Croft.
2004. Indri: A language model-based search engine
for complex queries. Proceedings of the Interna-
tional Conference on Intelligence Analysis.
Xu Tan, Yin Zhang, Siliang Tang, Jian Shao, Fei Wu,
and Yueting Zhuang. 2013. Logistic tensor re-
gression for classification. In Intelligent Science
and Intelligent Data Engineering, pages 573–581.
Springer.
Jean-Philippe Vert and Laurent Jacob. 2008. Machine
learning for in silico virtual screening and chemical
genomics: new strategies. Combinatorial chemistry
&amp; high throughput screening, 11(8):677.
Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learn-
ing bilinear model for matching queries and docu-
ments. The Journal of Machine Learning Research,
14(1):2519–2548.
</reference>
<page confidence="0.997901">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.971192">
<title confidence="0.9893525">Learning to Re-rank for Interactive Problem Resolution and Query Refinement</title>
<author confidence="0.999513">Rashmi Gangadharaiah Balakrishnan Narayanaswamy</author>
<author confidence="0.999513">Charles Elkan</author>
<affiliation confidence="0.9998285">IBM Research, Department of CSE, India Research Lab, University of California, San Diego</affiliation>
<address confidence="0.999103">Bangalore, KA, India La Jolla, CA, USA</address>
<abstract confidence="0.999641714285714">We study the design of an information retrieval (IR) system that assists customer service agents while they interact with end-users. The type of IR needed is difficult because of the large lexical gap between problems as described by customers, and solutions. We describe an approach that bridges this lexical gap by learning semantic relatedness using tensor representations. Queries that are short and vague, which are common in practice, result in a large number of documents being retrieved, and a high cognitive load for customer service agents. We show how to reduce this burden by providing suggestions that are selected based on the learned measures of semantic relatedness. Experiments show that the approach offers substantial benefit compared to the use of standard lexical similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fatemeh Zahedi</author>
</authors>
<title>Antonio Palma-dos Reis.</title>
<date>1999</date>
<marker>Zahedi, 1999</marker>
<rawString>Fatemeh Zahedi Antonio Palma-dos Reis. 1999. Designing personalized intelligent financial decision support systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo Baeza-yates</author>
<author>Carlos Hurtado</author>
<author>Marcelo Mendoza</author>
</authors>
<title>Query recommendation using query logs in search engines.</title>
<date>2004</date>
<booktitle>In In International Workshop on Clustering Information over the Web (ClustWeb, in conjunction with EDBT), Creete,</booktitle>
<pages>588--596</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3571" citStr="Baeza-yates et al., 2004" startWordPosition="551" endWordPosition="554">teracts with the system by selecting one of the suggestions. This is used to expand the original query and the process can be repeated. We show that even one round of interaction, with a small set of suggestions, can lead to high quality solutions to user problems. In query expansion, the classical approach is to automatically find suggestions either in the form of words, phrases or similar queries (Kelly et al., 2009; Feuer et al., 2007; Leung et al., 2008). These can be obtained either from query logs or based on their representativeness of the initial retrieved documents (Guo et al., 2008; Baeza-yates et al., 2004). The suggestions are then ranked either based on their frequencies or based on their similarity to the original query (Kelly et al., 2009; Leung et al., 2008). For example, if suggestions and queries are represented as term vectors (e.g. 1http://answers.yahoo.com/ 2http://ubuntuforums.org/ 3https://discussions.apple.com/ 218 Proceedings of the SIGDIAL 2014 Conference, pages 218–227, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics term frequency-inverse document frequency or tfidf) their similarity may be determined using similarity measures such as cosi</context>
<context position="29983" citStr="Baeza-yates et al., 2004" startWordPosition="4964" endWordPosition="4967">fferent, there is no preliminary search engine which retrieves results, making them ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions either in the form of words, phrases (Kelly et al., 2009; Feuer et al., 2007; Baeza-yates et al., 2004) or similar queries (Leung et al., 2008) from query logs (Guo et al., 2008; Baeza-yates et al., 2004) or based on their probability of representing the initial retrieved documents (Kelly et al., 2009; Feuer et al., 2007). These suggestions are then ranked either based on their frequencies or based on their closeness to the query. Closeness is defined in terms of lexical similarity to the query. However, most often the query and the suggestions do not have any co-occurring words leading to low similarity scores, even when the suggestion is relevant. (Gangadharaiah and Narayanaswamy, 2013) use i</context>
</contexts>
<marker>Baeza-yates, Hurtado, Mendoza, 2004</marker>
<rawString>Ricardo Baeza-yates, Carlos Hurtado, and Marcelo Mendoza. 2004. Query recommendation using query logs in search engines. In In International Workshop on Clustering Information over the Web (ClustWeb, in conjunction with EDBT), Creete, pages 588–596. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Bai</author>
<author>Jason Weston</author>
<author>David Grangier</author>
</authors>
<title>Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Corinna Cortes, and Mehryar Mohri.</title>
<date>2009</date>
<booktitle>In NIPS,</booktitle>
<pages>64--72</pages>
<contexts>
<context position="19489" citStr="Bai et al., 2009" startWordPosition="3214" endWordPosition="3217">ly obtain generalization error guarantees for our methods. We also discuss the advantage of the tensor representation above over a natural representation X = [x; y] i.e. X is the column vector obtained by stacking the query and unit representations. Note that in this representation, for logistic regression, while a change in the query x can change the probability for a unit P(r(X) = 1) it cannot change the relative probability of two different units. Thus, the ordering of all unit remains the same for all queries. This flaw has been pointed out in the literature in (Vert and Jacob, 2008) and (Bai et al., 2009), but was brought to our attention by (Elkan, 2010). Finally, we note that by normalizing the query and unit vectors (x and y), and selecting W = I (the identity matrix) we can recover the cosine similarity metric (Elkan, 2010). Thus, our representation is atleast as accurate and we show that learning the diagonal and off-diagonal components of W can substantially improve accuracy. Additionally, for every (query,unit) we also compute information gain (IG) as given in (1), and the lexical similarity (Sim) in terms of cosine similarity between the query and the unit as additional features in the</context>
</contexts>
<marker>Bai, Weston, Grangier, 2009</marker>
<rawString>Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Corinna Cortes, and Mehryar Mohri. 2009. Polynomial semantic indexing. In NIPS, pages 64–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
</authors>
<title>Agglomerative clustering of a search engine query log.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’00,</booktitle>
<pages>407--416</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="29551" citStr="Beeferman and Berger, 2000" startWordPosition="4894" endWordPosition="4898">s of fast and strong existing search engines increasing accuracy and decreasing retrieval time and (iii) allows the learnt model to focus learning on the query-document pairs that are most problematic for the search engine. Bilinear forms of tensor models without the rank restriction have recently been studied for link prediction (Menon and Elkan, 2011) and image processing (Kobayashi and Otsu, 2012). Since the applications are different, there is no preliminary search engine which retrieves results, making them ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions either in the form of words, phrases (Kelly et al., 2009; Feuer et al., 2007; Baeza-yates et al., 2004) or similar queries (Leung et al., 2008) from query logs (Guo et al., 2008; Baeza-yates et al., 2004) or based on their probability of representing the initial retrieve</context>
</contexts>
<marker>Beeferman, Berger, 2000</marker>
<rawString>Doug Beeferman and Adam Berger. 2000. Agglomerative clustering of a search engine query log. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’00, pages 407–416, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Information retrieval as statistical translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’99,</booktitle>
<pages>222--229</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="31249" citStr="Berger and Lafferty, 1999" startWordPosition="5167" endWordPosition="5170">ions. However, the relevancy of the suggestions highly depends on the relevancy of the initial retrieved documents. Our work here addresses the question of how to bridge this lexical chasm between the query and the suggestions. For this, we use semantic-relatedness between the query and the suggestions as a measure of closeness rather than defining closeness based on lexical similarity. A related approach to handle this lexical gap by applying alignment techniques from Statistical Machine translation (Brown et al., 1993), in particular by building translation models for information retrieval (Berger and Lafferty, 1999; Riezler et al., 2007). These approaches require training data in the form of question-answer pairs, are again limited to words or phrases and are not intended for understanding the user’s problem better through interaction, which is our focus. 6 Conclusions, Discussions and Future Work We studied the problem of designing Information Retrieval systems for interactive problem resolution. We developed a system for bridging the large lexical gap between short, incomplete problem queries and documents in a database of resolutions. We showed that tensor representations are a useful tool to learn m</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’99, pages 222–229, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern recognition and machine learning.</title>
<date>2006</date>
<contexts>
<context position="16596" citStr="Bishop, 2006" startWordPosition="2698" endWordPosition="2699">e i and j run over all elements of X and W. Thus, if X is of order MX and W of order MW, A is of order MA = MX + MW. The particular tensor we are interested in is a 2-D tensor (matrix) X which is the outer product of query and unit pairs (Feats). In particular, for a query x and unit y, Xi,j = xiyj. Given this representation, standard classification and regression methods from the machine learning literature can often be extended to deal with tensors. In our work we consider two classifiers that have been successful in many applications, logistic regression and support vector machines (SVMs) (Bishop, 2006). Forum Discussion Threads User clicks on (units, query) query Search Engine Learn Relevance Model Unit Extraction results Interaction Module Finds suggestions Candidate Suggestions Suggestion units for first posts Offline Online (X, W) = �I1 I2 . . . IM i1 � iM i2 221 In the case of logistic regression, the conditional probability of a reward signal r(X) = r(x, y) is, 1 p(r(X) = +1) = 1 + exp(−(X,W) + b) (5) The parameters W and b can be obtained by minimizing the log loss Greg on the training data D Greg(W, b) = (6) � log(1 + exp(−r(X)(X,W) + b) (X,r(X))ED For SVMs with the hinge loss we sel</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M Bishop. 2006. Pattern recognition and machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="31150" citStr="Brown et al., 1993" startWordPosition="5152" endWordPosition="5155">evant. (Gangadharaiah and Narayanaswamy, 2013) use information gain to rank candidate suggestions. However, the relevancy of the suggestions highly depends on the relevancy of the initial retrieved documents. Our work here addresses the question of how to bridge this lexical chasm between the query and the suggestions. For this, we use semantic-relatedness between the query and the suggestions as a measure of closeness rather than defining closeness based on lexical similarity. A related approach to handle this lexical gap by applying alignment techniques from Statistical Machine translation (Brown et al., 1993), in particular by building translation models for information retrieval (Berger and Lafferty, 1999; Riezler et al., 2007). These approaches require training data in the form of question-answer pairs, are again limited to words or phrases and are not intended for understanding the user’s problem better through interaction, which is our focus. 6 Conclusions, Discussions and Future Work We studied the problem of designing Information Retrieval systems for interactive problem resolution. We developed a system for bridging the large lexical gap between short, incomplete problem queries and documen</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Comput. Linguist., 19(2):263– 311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Elkan</author>
</authors>
<title>Learning affinity with biliear models. Unpublished Notes.</title>
<date>2010</date>
<contexts>
<context position="15369" citStr="Elkan, 2010" startWordPosition="2465" endWordPosition="2466">ed. Special cases include when only one document is returned N = 1. We will design query suggestions so that |S(x+z) |&gt; 0. Another criterion we use is to return all remaining documents after a certain maximum number of interactions or until the user quits. 4 Our Approach We specify our algorithm using a tensor notation. We do this since tensors appear to subsume most of the methods applied in practice, where different algorithms use slightly different costs, losses and constraints. These ideas are strongly motivated by, but generalize to some extent, suggestions for this problem presented in (Elkan, 2010). For our purposes, we consider tensors as multidimensional arrays, with the number of dimensions defined as the order of the tensor. An M order tensor X E RI1×I2...IM. As such tensors subsume vectors (1st order tensors) and matrices (2nd order tensors). The vectorization of a tensor of order M is obtained by stacking elements from the M dimensions into a vector of length I1 x I2 x ... x IM in the natural way. The inner product of two tensors is defined as xi1wi1xi2wi2 ... xiM wiM (4) Analogous to the definition for vectors, the (Kharti-Rao) outer product A = X ® W of two tensors X and W has A</context>
<context position="19540" citStr="Elkan, 2010" startWordPosition="3225" endWordPosition="3226">s. We also discuss the advantage of the tensor representation above over a natural representation X = [x; y] i.e. X is the column vector obtained by stacking the query and unit representations. Note that in this representation, for logistic regression, while a change in the query x can change the probability for a unit P(r(X) = 1) it cannot change the relative probability of two different units. Thus, the ordering of all unit remains the same for all queries. This flaw has been pointed out in the literature in (Vert and Jacob, 2008) and (Bai et al., 2009), but was brought to our attention by (Elkan, 2010). Finally, we note that by normalizing the query and unit vectors (x and y), and selecting W = I (the identity matrix) we can recover the cosine similarity metric (Elkan, 2010). Thus, our representation is atleast as accurate and we show that learning the diagonal and off-diagonal components of W can substantially improve accuracy. Additionally, for every (query,unit) we also compute information gain (IG) as given in (1), and the lexical similarity (Sim) in terms of cosine similarity between the query and the unit as additional features in the feature vectors. 5 Results and Discussion To evalu</context>
</contexts>
<marker>Elkan, 2010</marker>
<rawString>Charles Elkan. 2010. Learning affinity with biliear models. Unpublished Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Feuer</author>
<author>Stefan Savev</author>
<author>Javed A Aslam</author>
</authors>
<title>Evaluation of phrasal query suggestions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM ’07,</booktitle>
<pages>841--848</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3387" citStr="Feuer et al., 2007" startWordPosition="520" endWordPosition="523">irst task in this paper is to automatically make candidate suggestions that reduce the search space of relevant documents in a contact center application. The agent/user then interacts with the system by selecting one of the suggestions. This is used to expand the original query and the process can be repeated. We show that even one round of interaction, with a small set of suggestions, can lead to high quality solutions to user problems. In query expansion, the classical approach is to automatically find suggestions either in the form of words, phrases or similar queries (Kelly et al., 2009; Feuer et al., 2007; Leung et al., 2008). These can be obtained either from query logs or based on their representativeness of the initial retrieved documents (Guo et al., 2008; Baeza-yates et al., 2004). The suggestions are then ranked either based on their frequencies or based on their similarity to the original query (Kelly et al., 2009; Leung et al., 2008). For example, if suggestions and queries are represented as term vectors (e.g. 1http://answers.yahoo.com/ 2http://ubuntuforums.org/ 3https://discussions.apple.com/ 218 Proceedings of the SIGDIAL 2014 Conference, pages 218–227, Philadelphia, U.S.A., 18-20 J</context>
<context position="29956" citStr="Feuer et al., 2007" startWordPosition="4960" endWordPosition="4963"> applications are different, there is no preliminary search engine which retrieves results, making them ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions either in the form of words, phrases (Kelly et al., 2009; Feuer et al., 2007; Baeza-yates et al., 2004) or similar queries (Leung et al., 2008) from query logs (Guo et al., 2008; Baeza-yates et al., 2004) or based on their probability of representing the initial retrieved documents (Kelly et al., 2009; Feuer et al., 2007). These suggestions are then ranked either based on their frequencies or based on their closeness to the query. Closeness is defined in terms of lexical similarity to the query. However, most often the query and the suggestions do not have any co-occurring words leading to low similarity scores, even when the suggestion is relevant. (Gangadharaiah and</context>
</contexts>
<marker>Feuer, Savev, Aslam, 2007</marker>
<rawString>Alan Feuer, Stefan Savev, and Javed A. Aslam. 2007. Evaluation of phrasal query suggestions. In Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM ’07, pages 841–848, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Gangadharaiah</author>
<author>Balakrishnan Narayanaswamy</author>
</authors>
<title>Natural language query refinement for problem resolution from crowdsourced semi-structured data.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>243--251</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="2280" citStr="Gangadharaiah and Narayanaswamy, 2013" startWordPosition="333" endWordPosition="336"> sources (Holland, 2005). Agents enter queries to retrieve documents related to the customer’s problem. These sources are often incomplete as it is unlikely that all possible customer problems can be identified before product release. This is particularly true for recently released and frequently updated products. One approach, which we build on here, is to mine problems and resolutions from online discussion forums Yahoo! Answers1 Ubuntu Forums2 and Apple Support Communities3. While these often provide useful solutions within hours or days of a problem surfacing, they are semantically noisy (Gangadharaiah and Narayanaswamy, 2013). Most contact centers and agents are evaluated based on the number of calls they handle over a period (Pinedo et al., 2000). As a result, queries entered by agents into the search engine are usually underspecified. This, together with noise in the database, results in a large number of documents being retrieved as relevant documents. This in turn, increases the cognitive load on agents, and reduces the effectiveness of the search system and the efficiency of the contact center. Our first task in this paper is to automatically make candidate suggestions that reduce the search space of relevant</context>
<context position="9056" citStr="Gangadharaiah and Narayanaswamy, 2013" startWordPosition="1383" endWordPosition="1386"> in (Gangadhara219 iah and Narayanaswamy, 2013) to automatically generate these signatures from each discussion thread. We assume that the first post describes the user’s problem, something we have found to be true in practice. From the dependency parse trees of the first posts, we extract three types of units (i) phrases (e.g., sync server), (ii) attributevalues (e.g., iOS, 4) and (iii) action-attribute tuples (e.g., sync server: failed). Phrases form good base problem descriptors. Attribute-value pairs provide configurational contexts to the problem. Actionattribute tuples, as suggested in (Gangadharaiah and Narayanaswamy, 2013), capture segments of the first post that indicate user wanting to perform an action (“I cannot hear notifications on bluetooth”) or the problems caused by a user’s action (“working great before I updated”). These make them particularly valuable features for problemresolution and question-answering. 2.2 Representation of Queries and Suggestions Queries are represented as term vectors using the term frequency-inverse document frequency (tfidf) representation forming the query space. The term frequency is defined as the frequency with which word appears in the query and the inverse document freq</context>
<context position="10498" citStr="Gangadharaiah and Narayanaswamy, 2013" startWordPosition="1613" endWordPosition="1617">efined as the number of times a word appears in the unit and its inverse document frequency is defined in terms of the number of units in which the word appeared. Since the vocabulary used in the queries and documents are different, the representations for queries and units belong to different spaces of different dimensions. For every query-unit pair, we learn a measure of similarity as explained in Section 4. Additionally, we use similarity features based on cosine similarity between the query and the unit under consideration. We also consider an additional feature based on information gain (Gangadharaiah and Narayanaswamy, 2013). In particular, if S represents the set all retrieved documents, S1 is a subset of S (S1 ⊆ S) containing a unit uniti and S2 is a subset of S that does not contain uniti, information gain with uniti is, Gain(S, uniti) = E(S) − |S1 ||S|E(S1) − |S2 ||S|E(S2) (1) E(S) = � −p(dock)log2p(dock). (2) k=1,...|S| The probability for each document is based on its rank in the retrieved of results, 1 rank(docj) (3) Ek=1 ,... |S |rank(dock) We crawled posts and threads from online forums for the products of interest, as detailed in Section 5.1, and these form the documents. We used trial interactions and </context>
<context position="30577" citStr="Gangadharaiah and Narayanaswamy, 2013" startWordPosition="5062" endWordPosition="5065">euer et al., 2007; Baeza-yates et al., 2004) or similar queries (Leung et al., 2008) from query logs (Guo et al., 2008; Baeza-yates et al., 2004) or based on their probability of representing the initial retrieved documents (Kelly et al., 2009; Feuer et al., 2007). These suggestions are then ranked either based on their frequencies or based on their closeness to the query. Closeness is defined in terms of lexical similarity to the query. However, most often the query and the suggestions do not have any co-occurring words leading to low similarity scores, even when the suggestion is relevant. (Gangadharaiah and Narayanaswamy, 2013) use information gain to rank candidate suggestions. However, the relevancy of the suggestions highly depends on the relevancy of the initial retrieved documents. Our work here addresses the question of how to bridge this lexical chasm between the query and the suggestions. For this, we use semantic-relatedness between the query and the suggestions as a measure of closeness rather than defining closeness based on lexical similarity. A related approach to handle this lexical gap by applying alignment techniques from Statistical Machine translation (Brown et al., 1993), in particular by building</context>
</contexts>
<marker>Gangadharaiah, Narayanaswamy, 2013</marker>
<rawString>Rashmi Gangadharaiah and Balakrishnan Narayanaswamy. 2013. Natural language query refinement for problem resolution from crowdsourced semi-structured data. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 243–251, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiafeng Guo</author>
<author>Gu Xu</author>
<author>Hang Li</author>
<author>Xueqi Cheng</author>
</authors>
<title>A unified and discriminative model for query refinement.</title>
<date>2008</date>
<booktitle>In Sung-Hyon Myaeng,</booktitle>
<pages>379--386</pages>
<editor>W. Oard, Fabrizio Sebastiani, Tat-Seng Chua, and Mun-Kew Leong, editors, SIGIR,</editor>
<publisher>ACM.</publisher>
<location>Douglas</location>
<contexts>
<context position="3544" citStr="Guo et al., 2008" startWordPosition="547" endWordPosition="550">agent/user then interacts with the system by selecting one of the suggestions. This is used to expand the original query and the process can be repeated. We show that even one round of interaction, with a small set of suggestions, can lead to high quality solutions to user problems. In query expansion, the classical approach is to automatically find suggestions either in the form of words, phrases or similar queries (Kelly et al., 2009; Feuer et al., 2007; Leung et al., 2008). These can be obtained either from query logs or based on their representativeness of the initial retrieved documents (Guo et al., 2008; Baeza-yates et al., 2004). The suggestions are then ranked either based on their frequencies or based on their similarity to the original query (Kelly et al., 2009; Leung et al., 2008). For example, if suggestions and queries are represented as term vectors (e.g. 1http://answers.yahoo.com/ 2http://ubuntuforums.org/ 3https://discussions.apple.com/ 218 Proceedings of the SIGDIAL 2014 Conference, pages 218–227, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics term frequency-inverse document frequency or tfidf) their similarity may be determined using simil</context>
<context position="30057" citStr="Guo et al., 2008" startWordPosition="4978" endWordPosition="4981"> ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions either in the form of words, phrases (Kelly et al., 2009; Feuer et al., 2007; Baeza-yates et al., 2004) or similar queries (Leung et al., 2008) from query logs (Guo et al., 2008; Baeza-yates et al., 2004) or based on their probability of representing the initial retrieved documents (Kelly et al., 2009; Feuer et al., 2007). These suggestions are then ranked either based on their frequencies or based on their closeness to the query. Closeness is defined in terms of lexical similarity to the query. However, most often the query and the suggestions do not have any co-occurring words leading to low similarity scores, even when the suggestion is relevant. (Gangadharaiah and Narayanaswamy, 2013) use information gain to rank candidate suggestions. However, the relevancy of t</context>
</contexts>
<marker>Guo, Xu, Li, Cheng, 2008</marker>
<rawString>Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008. A unified and discriminative model for query refinement. In Sung-Hyon Myaeng, Douglas W. Oard, Fabrizio Sebastiani, Tat-Seng Chua, and Mun-Kew Leong, editors, SIGIR, pages 379–386. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Irene Kotsia</author>
<author>Ioannis Patras</author>
</authors>
<title>Tensor learning for regression. Image Processing,</title>
<date>2012</date>
<journal>IEEE Transactions on,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="27971" citStr="Guo et al., 2012" startWordPosition="4636" endWordPosition="4639">eterize the Φ function and jointly optimize the parameters of the feature mapping and the machine learning re-ranking model. Latent tensor methods for regression and classification have recently become popular in the image and signal processing domain. Most of these methods solve an optimization problem similar to our own (9), but add additional constraints limiting the rank of the learned matrix W either explicitly or implicit by defining W = Q1QT2 , and defining Q1 ∈ Rd.×d and Q2 ∈ Rdy×d. This approach is used for example in (Pirsiavash et al., 2009) and more recently in (Tan et al., 2013) (Guo et al., 2012). While this reduces the number of parameters to be learned from dxdy to d(dx + dy) it makes the problem non-convex and introduces an additional parameter d that must be selected. This approach of restricting the rank was recently suggested for information retrieval in (Wu et al., 2013). They look at a regression problem, using click-through rates as the reward function r(x, y). In addition, (Wu et al., 2013) does not use an initial search engine and hence must learn an affinity function between all query-document pairs. In contrast to this, we learn a classification function that discriminate</context>
</contexts>
<marker>Guo, Kotsia, Patras, 2012</marker>
<rawString>Weiwei Guo, Irene Kotsia, and Ioannis Patras. 2012. Tensor learning for regression. Image Processing, IEEE Transactions on, 21(2):816–827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haibo He</author>
<author>Edwardo A Garcia</author>
</authors>
<title>Learning from imbalanced data.</title>
<date>2009</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>21</volume>
<issue>9</issue>
<contexts>
<context position="28880" citStr="He and Garcia, 2009" startWordPosition="4788" endWordPosition="4791">They look at a regression problem, using click-through rates as the reward function r(x, y). In addition, (Wu et al., 2013) does not use an initial search engine and hence must learn an affinity function between all query-document pairs. In contrast to this, we learn a classification function that discriminates between the true and false positive documents that are deemed similar 224 by the search engine. This has three beneficial effects : (i) it reduces the amount of labeled training data required and the imbalance between the positive and negative classes which can make learning difficult (He and Garcia, 2009) and (ii) allows us to build on the strengths of fast and strong existing search engines increasing accuracy and decreasing retrieval time and (iii) allows the learnt model to focus learning on the query-document pairs that are most problematic for the search engine. Bilinear forms of tensor models without the rank restriction have recently been studied for link prediction (Menon and Elkan, 2011) and image processing (Kobayashi and Otsu, 2012). Since the applications are different, there is no preliminary search engine which retrieves results, making them ranking methods and ours a re-ranking </context>
</contexts>
<marker>He, Garcia, 2009</marker>
<rawString>Haibo He and Edwardo A Garcia. 2009. Learning from imbalanced data. Knowledge and Data Engineering, IEEE Transactions on, 21(9):1263–1284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Holland</author>
</authors>
<title>Modeling uncertainty in decision support systems for customer call center.</title>
<date>2005</date>
<booktitle>In Computational Intelligence, Theory and Applications,</booktitle>
<pages>763--770</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1666" citStr="Holland, 2005" startWordPosition="244" endWordPosition="245"> substantial benefit compared to the use of standard lexical similarity. 1 Introduction Information retrieval systems help businesses and individuals make decisions by automatically extracting actionable intelligence from large (unstructured) data (Musen et al., 2006; Antonio Palma-dos Reis, 1999). This paper focuses on the application of retrieval systems in a contact centers where the system assists agents while they are helping customers with problem resolution. Currently, most contact center information retrieval use (web based) front-ends to search engines indexed with knowledge sources (Holland, 2005). Agents enter queries to retrieve documents related to the customer’s problem. These sources are often incomplete as it is unlikely that all possible customer problems can be identified before product release. This is particularly true for recently released and frequently updated products. One approach, which we build on here, is to mine problems and resolutions from online discussion forums Yahoo! Answers1 Ubuntu Forums2 and Apple Support Communities3. While these often provide useful solutions within hours or days of a problem surfacing, they are semantically noisy (Gangadharaiah and Naraya</context>
</contexts>
<marker>Holland, 2005</marker>
<rawString>Alexander Holland. 2005. Modeling uncertainty in decision support systems for customer call center. In Computational Intelligence, Theory and Applications, pages 763–770. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Kelly</author>
<author>Karl Gyllstrom</author>
<author>Earl W Bailey</author>
</authors>
<title>A comparison of query and term suggestion features for interactive searching.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09,</booktitle>
<pages>371--378</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3367" citStr="Kelly et al., 2009" startWordPosition="516" endWordPosition="519">ontact center. Our first task in this paper is to automatically make candidate suggestions that reduce the search space of relevant documents in a contact center application. The agent/user then interacts with the system by selecting one of the suggestions. This is used to expand the original query and the process can be repeated. We show that even one round of interaction, with a small set of suggestions, can lead to high quality solutions to user problems. In query expansion, the classical approach is to automatically find suggestions either in the form of words, phrases or similar queries (Kelly et al., 2009; Feuer et al., 2007; Leung et al., 2008). These can be obtained either from query logs or based on their representativeness of the initial retrieved documents (Guo et al., 2008; Baeza-yates et al., 2004). The suggestions are then ranked either based on their frequencies or based on their similarity to the original query (Kelly et al., 2009; Leung et al., 2008). For example, if suggestions and queries are represented as term vectors (e.g. 1http://answers.yahoo.com/ 2http://ubuntuforums.org/ 3https://discussions.apple.com/ 218 Proceedings of the SIGDIAL 2014 Conference, pages 218–227, Philadelp</context>
<context position="29936" citStr="Kelly et al., 2009" startWordPosition="4956" endWordPosition="4959">su, 2012). Since the applications are different, there is no preliminary search engine which retrieves results, making them ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions either in the form of words, phrases (Kelly et al., 2009; Feuer et al., 2007; Baeza-yates et al., 2004) or similar queries (Leung et al., 2008) from query logs (Guo et al., 2008; Baeza-yates et al., 2004) or based on their probability of representing the initial retrieved documents (Kelly et al., 2009; Feuer et al., 2007). These suggestions are then ranked either based on their frequencies or based on their closeness to the query. Closeness is defined in terms of lexical similarity to the query. However, most often the query and the suggestions do not have any co-occurring words leading to low similarity scores, even when the suggestion is relevant</context>
</contexts>
<marker>Kelly, Gyllstrom, Bailey, 2009</marker>
<rawString>Diane Kelly, Karl Gyllstrom, and Earl W. Bailey. 2009. A comparison of query and term suggestion features for interactive searching. In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’09, pages 371–378, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takumi Kobayashi</author>
<author>Nobuyuki Otsu</author>
</authors>
<title>Efficient optimization for low-rank integrated bilinear classifiers.</title>
<date>2012</date>
<booktitle>In Computer Vision–ECCV 2012,</booktitle>
<pages>474--487</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="29327" citStr="Kobayashi and Otsu, 2012" startWordPosition="4859" endWordPosition="4862"> : (i) it reduces the amount of labeled training data required and the imbalance between the positive and negative classes which can make learning difficult (He and Garcia, 2009) and (ii) allows us to build on the strengths of fast and strong existing search engines increasing accuracy and decreasing retrieval time and (iii) allows the learnt model to focus learning on the query-document pairs that are most problematic for the search engine. Bilinear forms of tensor models without the rank restriction have recently been studied for link prediction (Menon and Elkan, 2011) and image processing (Kobayashi and Otsu, 2012). Since the applications are different, there is no preliminary search engine which retrieves results, making them ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions either in the form of words, phrases (Kelly et </context>
</contexts>
<marker>Kobayashi, Otsu, 2012</marker>
<rawString>Takumi Kobayashi and Nobuyuki Otsu. 2012. Efficient optimization for low-rank integrated bilinear classifiers. In Computer Vision–ECCV 2012, pages 474–487. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Wai-Ting Leung</author>
<author>Wilfred Ng</author>
<author>Dik Lun Lee</author>
</authors>
<title>Personalized concept-based clustering of search engine queries.</title>
<date>2008</date>
<journal>IEEE Trans. on Knowl. and Data Eng.,</journal>
<volume>20</volume>
<issue>11</issue>
<contexts>
<context position="3408" citStr="Leung et al., 2008" startWordPosition="524" endWordPosition="527">per is to automatically make candidate suggestions that reduce the search space of relevant documents in a contact center application. The agent/user then interacts with the system by selecting one of the suggestions. This is used to expand the original query and the process can be repeated. We show that even one round of interaction, with a small set of suggestions, can lead to high quality solutions to user problems. In query expansion, the classical approach is to automatically find suggestions either in the form of words, phrases or similar queries (Kelly et al., 2009; Feuer et al., 2007; Leung et al., 2008). These can be obtained either from query logs or based on their representativeness of the initial retrieved documents (Guo et al., 2008; Baeza-yates et al., 2004). The suggestions are then ranked either based on their frequencies or based on their similarity to the original query (Kelly et al., 2009; Leung et al., 2008). For example, if suggestions and queries are represented as term vectors (e.g. 1http://answers.yahoo.com/ 2http://ubuntuforums.org/ 3https://discussions.apple.com/ 218 Proceedings of the SIGDIAL 2014 Conference, pages 218–227, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Asso</context>
<context position="30023" citStr="Leung et al., 2008" startWordPosition="4971" endWordPosition="4974"> which retrieves results, making them ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions either in the form of words, phrases (Kelly et al., 2009; Feuer et al., 2007; Baeza-yates et al., 2004) or similar queries (Leung et al., 2008) from query logs (Guo et al., 2008; Baeza-yates et al., 2004) or based on their probability of representing the initial retrieved documents (Kelly et al., 2009; Feuer et al., 2007). These suggestions are then ranked either based on their frequencies or based on their closeness to the query. Closeness is defined in terms of lexical similarity to the query. However, most often the query and the suggestions do not have any co-occurring words leading to low similarity scores, even when the suggestion is relevant. (Gangadharaiah and Narayanaswamy, 2013) use information gain to rank candidate sugges</context>
</contexts>
<marker>Leung, Ng, Lee, 2008</marker>
<rawString>Kenneth Wai-Ting Leung, Wilfred Ng, and Dik Lun Lee. 2008. Personalized concept-based clustering of search engine queries. IEEE Trans. on Knowl. and Data Eng., 20(11):1505–1518, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
</authors>
<title>Learning to rank for information retrieval.</title>
<date>2009</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="26877" citStr="Liu, 2009" startWordPosition="4441" endWordPosition="4442">(data collected as explained in Section 2). The remaining 100 specific queries were used for testing. We see that the suggestions predicted by the classifiers using the relevancy model indeed improves the performance of the baseline. Also, adding the IG and Sim feature further boosts the performance of the system. Systems MRR Baseline 0.4218 Feats-IG-Sim 0.9449 Feats+IG+Sim 0.9968 Table 3: Comparison of the proposed approach with respect to the Baseline that does not involve interaction in terms of MRR. 5.3 Related Work Learning affinities between queries an documents is a well studied area. (Liu, 2009) provides an excellent survey of these approaches. In these methods, there is a fixed feature function Φ(x, y) defined between any query-document pair. These features are then used, along with labeled training data, to learn the parameters of a model that can then be used to predict the relevance r(x, y) of a new query-document pair. The output of the model can also be used to re-rank the results of a search engine. In contrast to this class of methods, we define and parameterize the Φ function and jointly optimize the parameters of the feature mapping and the machine learning re-ranking model</context>
</contexts>
<marker>Liu, 2009</marker>
<rawString>Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Krishna Menon</author>
<author>Charles Elkan</author>
</authors>
<title>Link prediction via matrix factorization.</title>
<date>2011</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>437--452</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17724" citStr="Menon and Elkan, 2011" startWordPosition="2896" endWordPosition="2899"> Greg(W, b) = (6) � log(1 + exp(−r(X)(X,W) + b) (X,r(X))ED For SVMs with the hinge loss we select parameters to minimize Ghinge, Ghinge(W, b) = ||X||2F + (7) A � max[0,1 − (r(X)(X, W) + b)] (X,r(X))ED where ||X||F is the Frobenius norm of tensor X. Given the number of parameters in our system (W, b) to limit overfitting, we have to regularize these parameters. We use regularizers of the form Q(W,b) = AW||W||F (8) such regularizes have been successful in many large scale machine learning tasks including learning of high dimensional graphical models (Ravikumar et al., 2010) and link prediction (Menon and Elkan, 2011). Thus, the final optimization problem we are faced with is of the form min G(W, b) + Q(W, b) (9) W,b where G is Greg or Ghinge as appropriate. Other losses, classifiers and regularizers may be used. The advantage of tensors over their vectorized counterparts, that may be lost in the notation, is that they do not lose the information that the different dimensions can (and in our case do) lie in different spaces. In particular, in our case we use different features to represent queries and units (as discussed in Section 2.2) which are not of the same length, and as a result trivially do not lie</context>
<context position="29279" citStr="Menon and Elkan, 2011" startWordPosition="4852" endWordPosition="4855">rch engine. This has three beneficial effects : (i) it reduces the amount of labeled training data required and the imbalance between the positive and negative classes which can make learning difficult (He and Garcia, 2009) and (ii) allows us to build on the strengths of fast and strong existing search engines increasing accuracy and decreasing retrieval time and (iii) allows the learnt model to focus learning on the query-document pairs that are most problematic for the search engine. Bilinear forms of tensor models without the rank restriction have recently been studied for link prediction (Menon and Elkan, 2011) and image processing (Kobayashi and Otsu, 2012). Since the applications are different, there is no preliminary search engine which retrieves results, making them ranking methods and ours a re-ranking approach. Related work in text IR includes (Beeferman and Berger, 2000), where two queries are considered semantically similar if their clicks lead to the same page. However, the probability that different queries lead to common clicks of the same URLs is very small, again increasing the training data required. Approaches in the past have also proposed techniques to automatically find suggestions</context>
<context position="32394" citStr="Menon and Elkan, 2011" startWordPosition="5346" endWordPosition="5349">solutions. We showed that tensor representations are a useful tool to learn measures of semantic relatedness, beyond the cosine similarity metric. Our results show that with interaction, suggestions can be effective in pruning large sets of retrieved documents. We showed that our approach offers substantial improvement over systems that only use lexical similarities for retrieval and re-ranking, in an end-to-end problem-resolution domain. In addition to the classification losses considered in this paper, we can also use another loss term based on ideas from recommender systems, in particular (Menon and Elkan, 2011). Consider the matrix T with all training queries as rows and all units as the columns. If we view the query refinement problem as a matrix completion problem, it is natural to assume that this matrix has low rank, so that T can be written as T = UΛVT, where Λ is a diagonal matrix and parameter of our optimization. These can then be incorporated into the training process by appropriate changes to the cost and regularization terms. Another benefit of the tensor representation is that it can easily be extended to incorporate other meta-information that may be available. For example, if context s</context>
</contexts>
<marker>Menon, Elkan, 2011</marker>
<rawString>Aditya Krishna Menon and Charles Elkan. 2011. Link prediction via matrix factorization. In Machine Learning and Knowledge Discovery in Databases, pages 437–452. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Musen</author>
<author>Yuval Shahar</author>
<author>Edward H Shortliffe</author>
</authors>
<date>2006</date>
<note>Clinical decision-support systems.</note>
<contexts>
<context position="1319" citStr="Musen et al., 2006" startWordPosition="190" endWordPosition="193">ns. Queries that are short and vague, which are common in practice, result in a large number of documents being retrieved, and a high cognitive load for customer service agents. We show how to reduce this burden by providing suggestions that are selected based on the learned measures of semantic relatedness. Experiments show that the approach offers substantial benefit compared to the use of standard lexical similarity. 1 Introduction Information retrieval systems help businesses and individuals make decisions by automatically extracting actionable intelligence from large (unstructured) data (Musen et al., 2006; Antonio Palma-dos Reis, 1999). This paper focuses on the application of retrieval systems in a contact centers where the system assists agents while they are helping customers with problem resolution. Currently, most contact center information retrieval use (web based) front-ends to search engines indexed with knowledge sources (Holland, 2005). Agents enter queries to retrieve documents related to the customer’s problem. These sources are often incomplete as it is unlikely that all possible customer problems can be identified before product release. This is particularly true for recently rel</context>
</contexts>
<marker>Musen, Shahar, Shortliffe, 2006</marker>
<rawString>Mark A Musen, Yuval Shahar, and Edward H Shortliffe. 2006. Clinical decision-support systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pinedo</author>
<author>Sridhar Seshadri</author>
<author>J George Shanthikumar</author>
</authors>
<title>Call centers in financial services: strategies, technologies, and operations.</title>
<date>2000</date>
<booktitle>In Creating Value in Financial Services,</booktitle>
<pages>357--388</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2404" citStr="Pinedo et al., 2000" startWordPosition="355" endWordPosition="358">it is unlikely that all possible customer problems can be identified before product release. This is particularly true for recently released and frequently updated products. One approach, which we build on here, is to mine problems and resolutions from online discussion forums Yahoo! Answers1 Ubuntu Forums2 and Apple Support Communities3. While these often provide useful solutions within hours or days of a problem surfacing, they are semantically noisy (Gangadharaiah and Narayanaswamy, 2013). Most contact centers and agents are evaluated based on the number of calls they handle over a period (Pinedo et al., 2000). As a result, queries entered by agents into the search engine are usually underspecified. This, together with noise in the database, results in a large number of documents being retrieved as relevant documents. This in turn, increases the cognitive load on agents, and reduces the effectiveness of the search system and the efficiency of the contact center. Our first task in this paper is to automatically make candidate suggestions that reduce the search space of relevant documents in a contact center application. The agent/user then interacts with the system by selecting one of the suggestion</context>
</contexts>
<marker>Pinedo, Seshadri, Shanthikumar, 2000</marker>
<rawString>Michael Pinedo, Sridhar Seshadri, and J George Shanthikumar. 2000. Call centers in financial services: strategies, technologies, and operations. In Creating Value in Financial Services, pages 357–388. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamed Pirsiavash</author>
<author>Deva Ramanan</author>
<author>Charless Fowlkes</author>
</authors>
<title>Bilinear classifiers for visual recognition. In</title>
<date>2009</date>
<booktitle>NIPS,</booktitle>
<pages>1482--1490</pages>
<contexts>
<context position="27912" citStr="Pirsiavash et al., 2009" startWordPosition="4624" endWordPosition="4627"> engine. In contrast to this class of methods, we define and parameterize the Φ function and jointly optimize the parameters of the feature mapping and the machine learning re-ranking model. Latent tensor methods for regression and classification have recently become popular in the image and signal processing domain. Most of these methods solve an optimization problem similar to our own (9), but add additional constraints limiting the rank of the learned matrix W either explicitly or implicit by defining W = Q1QT2 , and defining Q1 ∈ Rd.×d and Q2 ∈ Rdy×d. This approach is used for example in (Pirsiavash et al., 2009) and more recently in (Tan et al., 2013) (Guo et al., 2012). While this reduces the number of parameters to be learned from dxdy to d(dx + dy) it makes the problem non-convex and introduces an additional parameter d that must be selected. This approach of restricting the rank was recently suggested for information retrieval in (Wu et al., 2013). They look at a regression problem, using click-through rates as the reward function r(x, y). In addition, (Wu et al., 2013) does not use an initial search engine and hence must learn an affinity function between all query-document pairs. In contrast to</context>
</contexts>
<marker>Pirsiavash, Ramanan, Fowlkes, 2009</marker>
<rawString>Hamed Pirsiavash, Deva Ramanan, and Charless Fowlkes. 2009. Bilinear classifiers for visual recognition. In NIPS, pages 1482–1490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pradeep Ravikumar</author>
<author>Martin J Wainwright</author>
<author>John D Lafferty</author>
</authors>
<title>High-dimensional ising model selection using 1-regularized logistic regression.</title>
<date>2010</date>
<journal>The Annals of Statistics,</journal>
<volume>38</volume>
<issue>3</issue>
<contexts>
<context position="17680" citStr="Ravikumar et al., 2010" startWordPosition="2889" endWordPosition="2892">zing the log loss Greg on the training data D Greg(W, b) = (6) � log(1 + exp(−r(X)(X,W) + b) (X,r(X))ED For SVMs with the hinge loss we select parameters to minimize Ghinge, Ghinge(W, b) = ||X||2F + (7) A � max[0,1 − (r(X)(X, W) + b)] (X,r(X))ED where ||X||F is the Frobenius norm of tensor X. Given the number of parameters in our system (W, b) to limit overfitting, we have to regularize these parameters. We use regularizers of the form Q(W,b) = AW||W||F (8) such regularizes have been successful in many large scale machine learning tasks including learning of high dimensional graphical models (Ravikumar et al., 2010) and link prediction (Menon and Elkan, 2011). Thus, the final optimization problem we are faced with is of the form min G(W, b) + Q(W, b) (9) W,b where G is Greg or Ghinge as appropriate. Other losses, classifiers and regularizers may be used. The advantage of tensors over their vectorized counterparts, that may be lost in the notation, is that they do not lose the information that the different dimensions can (and in our case do) lie in different spaces. In particular, in our case we use different features to represent queries and units (as discussed in Section 2.2) which are not of the same </context>
</contexts>
<marker>Ravikumar, Wainwright, Lafferty, 2010</marker>
<rawString>Pradeep Ravikumar, Martin J Wainwright, and John D Lafferty. 2010. High-dimensional ising model selection using 1-regularized logistic regression. The Annals of Statistics, 38(3):1287–1319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical Machine Translation for Query Expansion in Answer Retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>464--471</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="31272" citStr="Riezler et al., 2007" startWordPosition="5171" endWordPosition="5175">y of the suggestions highly depends on the relevancy of the initial retrieved documents. Our work here addresses the question of how to bridge this lexical chasm between the query and the suggestions. For this, we use semantic-relatedness between the query and the suggestions as a measure of closeness rather than defining closeness based on lexical similarity. A related approach to handle this lexical gap by applying alignment techniques from Statistical Machine translation (Brown et al., 1993), in particular by building translation models for information retrieval (Berger and Lafferty, 1999; Riezler et al., 2007). These approaches require training data in the form of question-answer pairs, are again limited to words or phrases and are not intended for understanding the user’s problem better through interaction, which is our focus. 6 Conclusions, Discussions and Future Work We studied the problem of designing Information Retrieval systems for interactive problem resolution. We developed a system for bridging the large lexical gap between short, incomplete problem queries and documents in a database of resolutions. We showed that tensor representations are a useful tool to learn measures of semantic rel</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical Machine Translation for Query Expansion in Answer Retrieval. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<contexts>
<context position="4243" citStr="Salton and McGill, 1983" startWordPosition="642" endWordPosition="645">n their frequencies or based on their similarity to the original query (Kelly et al., 2009; Leung et al., 2008). For example, if suggestions and queries are represented as term vectors (e.g. 1http://answers.yahoo.com/ 2http://ubuntuforums.org/ 3https://discussions.apple.com/ 218 Proceedings of the SIGDIAL 2014 Conference, pages 218–227, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics term frequency-inverse document frequency or tfidf) their similarity may be determined using similarity measures such as cosine similarity or inverse of euclidean distance (Salton and McGill, 1983). However, in question-answering and problemresolution domains, and in contrast to traditional Information Retrieval, most often the query and the suggestions do not have many overlapping words. This leads to low similarity scores, even when the suggestion is highly relevant. Consider the representative example in Table 1, taken from our crawled dataset. Although the suggestions, “does not support file transfer”, “connection not stable”, “pairing failed” are highly relevant for the problem of “Bluetooth not working”, their lexical similarity score is zero. The second task that this paper addre</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J McGill. 1983. Introduction to modern information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strohman</author>
<author>D Metzler</author>
<author>H Turtle</author>
<author>W B Croft</author>
</authors>
<title>Indri: A language model-based search engine for complex queries.</title>
<date>2004</date>
<booktitle>Proceedings of the International Conference on Intelligence Analysis.</booktitle>
<contexts>
<context position="12040" citStr="Strohman et al., 2004" startWordPosition="1874" endWordPosition="1877">2.3 Training The labeled (click-through) data for training the relevance model is collected as follows. Annotators were given pairs of queries. Each pair is composed of an underspecified query and a specific query (Section 5.1 provides more information on the creation of these queries). An underspecified query is a query that reflects what a user/agent typically enters into the system, and the corresponding specific query is full-specified version of the underspecified query. Annotators were first asked to query the search engine with each underspecified query. We use the Lemur search engine (Strohman et al., 2004). From the resulting set of retrieved documents, the system uses the information gain criteria (as given in (1) below) to rank and display to the annotators the candidate suggestions (i.e., the units that appear in the signatures of the retrieved documents). Thus, our system is bootstrapped using the information gain criterion. The annotators then selects the candidate suggestion that is most relevant to the corresponding specific query. The interaction with the system continues until the annotators quit. We then provide a class label for each unit based on the collected click-through informat</context>
<context position="23213" citStr="Strohman et al., 2004" startWordPosition="3814" endWordPosition="3817">ith the retrieved results or does not obtain relevant suggestions from the system. For example, for the underspecified query in Table 2, one of the predicted suggestions was, “server:stopped respond0.04 Error rate 0.03 Underspecified query “Safari not working” 1. safari:crashes 2. safari:cannot find:server 3. server:stopped responding 4. phone:freezes 5. update:failed Table 2: Specific Queries generated with the underspecified Query, ”Safari not working”. a short query (underspecified) from the first post of each of the 60 selected threads. These queries were given to the Lemur search engine (Strohman et al., 2004) to retrieve the 50 most similar threads from an index built on the entire set of 147,000 threads. The annotator manually analyzed the first posts of the retrieved threads to create contexts, resulting in a total 200 specific queries. We give an example to illustrate the data creation in Table 2. From an under-specified query “Safari not working”, the annotator found 5 specific queries. Two other annotators, were given these specific queries with the search engine’s results from the corresponding under-specified query. They were asked to choose the most relevant results for the specific querie</context>
</contexts>
<marker>Strohman, Metzler, Turtle, Croft, 2004</marker>
<rawString>T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. 2004. Indri: A language model-based search engine for complex queries. Proceedings of the International Conference on Intelligence Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Tan</author>
<author>Yin Zhang</author>
<author>Siliang Tang</author>
<author>Jian Shao</author>
<author>Fei Wu</author>
<author>Yueting Zhuang</author>
</authors>
<title>Logistic tensor regression for classification.</title>
<date>2013</date>
<booktitle>In Intelligent Science and Intelligent Data Engineering,</booktitle>
<pages>573--581</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="27952" citStr="Tan et al., 2013" startWordPosition="4632" endWordPosition="4635">we define and parameterize the Φ function and jointly optimize the parameters of the feature mapping and the machine learning re-ranking model. Latent tensor methods for regression and classification have recently become popular in the image and signal processing domain. Most of these methods solve an optimization problem similar to our own (9), but add additional constraints limiting the rank of the learned matrix W either explicitly or implicit by defining W = Q1QT2 , and defining Q1 ∈ Rd.×d and Q2 ∈ Rdy×d. This approach is used for example in (Pirsiavash et al., 2009) and more recently in (Tan et al., 2013) (Guo et al., 2012). While this reduces the number of parameters to be learned from dxdy to d(dx + dy) it makes the problem non-convex and introduces an additional parameter d that must be selected. This approach of restricting the rank was recently suggested for information retrieval in (Wu et al., 2013). They look at a regression problem, using click-through rates as the reward function r(x, y). In addition, (Wu et al., 2013) does not use an initial search engine and hence must learn an affinity function between all query-document pairs. In contrast to this, we learn a classification functio</context>
</contexts>
<marker>Tan, Zhang, Tang, Shao, Wu, Zhuang, 2013</marker>
<rawString>Xu Tan, Yin Zhang, Siliang Tang, Jian Shao, Fei Wu, and Yueting Zhuang. 2013. Logistic tensor regression for classification. In Intelligent Science and Intelligent Data Engineering, pages 573–581. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Philippe Vert</author>
<author>Laurent Jacob</author>
</authors>
<title>Machine learning for in silico virtual screening and chemical genomics: new strategies.</title>
<date>2008</date>
<booktitle>Combinatorial chemistry &amp; high throughput screening,</booktitle>
<pages>11--8</pages>
<contexts>
<context position="19466" citStr="Vert and Jacob, 2008" startWordPosition="3209" endWordPosition="3212"> the vector case, we directly obtain generalization error guarantees for our methods. We also discuss the advantage of the tensor representation above over a natural representation X = [x; y] i.e. X is the column vector obtained by stacking the query and unit representations. Note that in this representation, for logistic regression, while a change in the query x can change the probability for a unit P(r(X) = 1) it cannot change the relative probability of two different units. Thus, the ordering of all unit remains the same for all queries. This flaw has been pointed out in the literature in (Vert and Jacob, 2008) and (Bai et al., 2009), but was brought to our attention by (Elkan, 2010). Finally, we note that by normalizing the query and unit vectors (x and y), and selecting W = I (the identity matrix) we can recover the cosine similarity metric (Elkan, 2010). Thus, our representation is atleast as accurate and we show that learning the diagonal and off-diagonal components of W can substantially improve accuracy. Additionally, for every (query,unit) we also compute information gain (IG) as given in (1), and the lexical similarity (Sim) in terms of cosine similarity between the query and the unit as add</context>
</contexts>
<marker>Vert, Jacob, 2008</marker>
<rawString>Jean-Philippe Vert and Laurent Jacob. 2008. Machine learning for in silico virtual screening and chemical genomics: new strategies. Combinatorial chemistry &amp; high throughput screening, 11(8):677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
</authors>
<title>Learning bilinear model for matching queries and documents.</title>
<date>2013</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="28258" citStr="Wu et al., 2013" startWordPosition="4686" endWordPosition="4689">mization problem similar to our own (9), but add additional constraints limiting the rank of the learned matrix W either explicitly or implicit by defining W = Q1QT2 , and defining Q1 ∈ Rd.×d and Q2 ∈ Rdy×d. This approach is used for example in (Pirsiavash et al., 2009) and more recently in (Tan et al., 2013) (Guo et al., 2012). While this reduces the number of parameters to be learned from dxdy to d(dx + dy) it makes the problem non-convex and introduces an additional parameter d that must be selected. This approach of restricting the rank was recently suggested for information retrieval in (Wu et al., 2013). They look at a regression problem, using click-through rates as the reward function r(x, y). In addition, (Wu et al., 2013) does not use an initial search engine and hence must learn an affinity function between all query-document pairs. In contrast to this, we learn a classification function that discriminates between the true and false positive documents that are deemed similar 224 by the search engine. This has three beneficial effects : (i) it reduces the amount of labeled training data required and the imbalance between the positive and negative classes which can make learning difficult</context>
</contexts>
<marker>Wu, Lu, Li, 2013</marker>
<rawString>Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learning bilinear model for matching queries and documents. The Journal of Machine Learning Research, 14(1):2519–2548.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
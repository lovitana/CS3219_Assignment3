<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001649">
<title confidence="0.998476">
A System for Predicting ICD-10-PCS Codes
from Electronic Health Records
</title>
<author confidence="0.854333">
Michael Subotin
</author>
<affiliation confidence="0.49691">
3M Health Information Systems
</affiliation>
<address confidence="0.695026">
Silver Spring, MD
</address>
<email confidence="0.99714">
msubotin@mmm.com
</email>
<sectionHeader confidence="0.993853" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999364">
Medical coding is a process of classify-
ing health records according to standard
code sets representing procedures and di-
agnoses. It is an integral part of health
care in the U.S., and the high costs it
incurs have prompted adoption of natu-
ral language processing techniques for au-
tomatic generation of these codes from
the clinical narrative contained in elec-
tronic health records. The need for effec-
tive auto-coding methods becomes even
greater with the impending adoption of
ICD-10, a code inventory of greater com-
plexity than the currently used code sets.
This paper presents a system that predicts
ICD-10 procedure codes from the clinical
narrative using several levels of abstrac-
tion. First, partial hierarchical classifica-
tion is used to identify potentially rele-
vant concepts and codes. Then, for each
of these concepts we estimate the confi-
dence that it appears in a procedure code
for that document. Finally, confidence val-
ues for the candidate codes are estimated
using features derived from concept confi-
dence scores. The concept models can be
trained on data with ICD-9 codes to sup-
plement sparse ICD-10 training resources.
Evaluation on held-out data shows promis-
ing results.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999808">
In many countries reimbursement rules for health
care services stipulate that the patient encounter
must be assigned codes representing diagnoses
that were made for and procedures that were per-
formed on the patient. These codes may be as-
signed by general health care personnel or by spe-
cially trained medical coders. The billing codes
</bodyText>
<note confidence="0.975198666666667">
Anthony R. Davis
3M Health Information Systems
Silver Spring, MD
</note>
<email confidence="0.914979">
adavis4@mmm.com
</email>
<bodyText confidence="0.998593538461538">
used in the U.S. include International Statisti-
cal Classification of Diseases and Related Health
Problems (ICD) codes, whose version 9 is cur-
rently in use and whose version 10 was scheduled
for adoption in October 20141, as well as Current
Procedural Terminology (CPT) codes. The same
codes are also used for research, internal book-
keeping, and other purposes.
Assigning codes to clinical documentation of-
ten requires extensive technical training and in-
volves substantial labor costs. This, together with
increasing prominence of electronic health records
(EHRs), has prompted development and adoption
of NLP algorithms that support the coding work-
flow by automatically inferring appropriate codes
from the clinical narrative and other information
contained in the EHR (Chute et al., 1994; Heinze
et al., 2001; Resnik et al., 2006; Pakhomov et al.,
2006; Benson, 2006). The need for effective auto-
coding methods becomes especially acute with the
introduction of ICD-10 and the associated increase
of training and labor costs for manual coding.
The novelty and complexity of ICD-10 presents
unprecedented challenges for developers of rule-
based auto-coding software. Thus, while ICD-9
contains 3882 codes for procedures, the number
of codes defined by the ICD-10 Procedure Cod-
ing System (PCS) is greater than 70,000. Further-
more, the organization of ICD-10-PCS is funda-
mentally different from ICD-9, which means that
the investment of time and money that had gone
into writing auto-coding rules for ICD-9 proce-
dure codes cannot be easily leveraged in the tran-
sition to ICD-10.
In turn, statistical auto-coding methods are con-
strained by the scarcity of available training data
with manually assigned ICD-10 codes. While this
problem will be attenuated over the years as ICD-
10-coded data are accumulated, the health care
</bodyText>
<footnote confidence="0.9956155">
1The deadline was delayed by at least a year while this
paper was in review.
</footnote>
<page confidence="0.979516">
59
</page>
<note confidence="0.7882975">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 59–67,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999841288888889">
industry needs effective technology for ICD-10
computer-assisted coding in advance of the imple-
mentation deadline. Thus, for developers of statis-
tical auto-coding algorithms two desiderata come
to the fore: these algorithms should take advantage
of all available training data, including documents
supplied only with ICD-9 codes, and they should
possess high capacity for statistical generalization
in order to maximize the benefits of training mate-
rial with ICD-10 codes.
The auto-coding system described here seeks
to meet both these requirements. Rather than
predicting codes directly from the clinical narra-
tive, a set of classifiers is first applied to identify
coding-related concepts that appear in the EHR.
We use General Equivalence Mappings (GEMs)
between ICD-9 and ICD-10 codes (CMS, 2014)
to train these models not only on data with human-
assigned ICD-10 codes, but also on ICD-9-coded
data. We then use the predicted concepts to de-
rive features for a model that estimates probabil-
ity of ICD-10 codes. Besides the intermediate ab-
straction to concepts, the code confidence model
itself is also designed so as to counteract sparsity
of the training data. Rather than train a separate
classifier for each code, we use a single model
whose features can generalize beyond individual
codes. Partial hierarchical classification is used for
greater run-time efficiency. To our knowledge, this
is the first research publication describing an auto-
coding system for ICD-10-PCS. It is currently de-
ployed, in tandem with other auto-coding mod-
ules, to support computer-assisted coding in the
3MTM360 EncompassTMSystem.
The rest of the paper is organized as follows.
Section 2 reviews the overall organization of ICD-
10-PCS. Section 4.1 outlines the run-time process-
ing flow of the system to show how its components
fit together. Section 4.2 describes the concept con-
fidence models, including the hierarchical classi-
fication components. Section 4.3 discusses how
data with manually assigned ICD-9 codes is used
to train some of the concept confidence models.
Section 4.4 describes the code confidence model.
Finally, Section 5 reports experimental results.
</bodyText>
<sectionHeader confidence="0.902037" genericHeader="method">
2 ICD-10 Procedure Coding System
</sectionHeader>
<bodyText confidence="0.999482592592593">
ICD-10-PCS is a set of codes for medical proce-
dures, developed by 3M Health Information Sys-
tems under contract to the Center for Medicare and
Medicaid Services of the U.S. government. ICD-
10-PCS has been designed systematically; each
code consists of seven characters, and the charac-
ter in each of these positions signifies one partic-
ular aspect of the code. The first character des-
ignates the “section” of ICD-10-PCS: 0 for Med-
ical and Surgical, 1 for Obstetrics, 2 for Place-
ment, and so on. Within each section, the seven
components, or axes of classification, are intended
to have a consistent meaning; for example in the
Medical and Surgical section, the second charac-
ter designates the body system involved, the third
the root operation, and so on (see Table 1 for a
list). All procedures in this section are thus clas-
sified along these axes. For instance, in a code
such as 0DBJ3ZZ, the D in the second position in-
dicates that the body system involved is the gas-
trointestinal system, B in the third position always
indicates that the root operation is an excision of a
body part, the J in the fourth position indicates that
the appendix is the body part involved, and the 3 in
the fifth position indicates that the approach is per-
cutaneous. The value Z in the last two axes means
than neither a device nor a qualifier are specified.
</bodyText>
<footnote confidence="0.979966">
Character Meaning
1st Section
2nd Body System
3rd Root Operation
4th Body Part
5th Approach
6th Device
7th Qualifier
</footnote>
<tableCaption confidence="0.7284625">
Table 1: Character Specification of the Medical
and Surgical Section of ICD-10-PCS
</tableCaption>
<bodyText confidence="0.9995206875">
Several consequences of the compositional
structure of ICD-10-PCS are especially relevant
for statistical auto-coding methods.
On the one hand, it defines over 70,000 codes,
many of which are logically possible, but very rare
in practice. Thus, attempts to predict the codes as
unitary entities are bound to suffer from data spar-
sity problems even with a large training corpus.
Furthermore, some of the axis values are formu-
lated in ways that are different from how the cor-
responding concepts would normally be expressed
in a clinical narrative. For example, ICD-10-PCS
uses multiple axes (root opreration, body part, and,
in a sense, the first two axes as well) to encode
what many traditional procedure terms (such as
those ending in -tomy and -plasty) express by a
</bodyText>
<page confidence="0.996961">
60
</page>
<bodyText confidence="0.999990909090909">
single word, while the device axis uses generic
categories where a clinical narrative would refer
only to specific brand names. This drastically lim-
its how much can be accomplished by matching
code descriptions or indexes derived from them
against the text of EHRs.
On the other hand, the systematic conceptual
structure of PCS codes and of the codeset as a
whole can be exploited to compensate for data
sparsity and idiosyncracies of axis definitions by
introducing abstraction into the model.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999989865384615">
There exists a large literature on automatic clas-
sification of clinical text (Stanfill et al., 2010). A
sizeable portion of it is devoted to detecting cate-
gories corresponding to billing codes, but most of
these studies are limited to one or a handful of cat-
egories. This is in part because the use of patient
records is subject to strict regulation. Thus, the
corpus used for most auto-coding research up to
date consists of about two thousand documents an-
notated with 45 ICD-9 codes (Pestian et al., 2007).
It was used in a shared task at the 2007 BioNLP
workshop and gave rise to papers studying a va-
riety of rule-based and statistical methods, which
are too numerous to list here.
We limit our attention to a smaller set of re-
search publications describing identification of an
entire set of billing codes, or a significant por-
tion thereof, which better reflects the role of auto-
coding in real-life applications. Mayo Clinic was
among the earliest adopters of auto-coding (Chute
et al., 1994), where it was deployed to assign
codes from a customized and greatly expanded
version of ICD-8, consisting of almost 30K diag-
nostic codes. A recently reported version of their
system (Pakhomov et al., 2006) leverages a com-
bination of example-based techniques and Naive
Bayes classification over a database of over 20M
EHRs. The phrases representing the diagnoses
have to be itemized as a list beforehand. In an-
other pioneering study, Larkey &amp; Croft (1995) in-
vestigated k-Nearest Neighbor, Naive Bayes, and
relevance feedback on a set of 12K discharge
summaries, predicting ICD-9 codes. Heinze et
al (2000) and Ribeiro-Neto et al (2001) describe
systems centered on symbolic computation. Jiang
et al (2006) discuss confidence assessment for
ICD-9 and CPT codes, performed separately from
code generation. Medori &amp; Fairon (2010) com-
bine information extraction with a Naive Bayes
classifier, working with a corpus of about 20K dis-
charge summaries in French. In a recent paper,
Perotte et al (2014) study standard and hierarchi-
cal classification using support vector machines on
a corpus of about 20K EHRs with ICD-9 codes.
We are not aware of any previous publications
on auto-coding for ICD-10-PCS, and the results
of these studies cannot be directly compared with
those reported below due to the unique nature of
this code set. Our original contributions also in-
clude explicit modeling of concepts and the ca-
pability to assign previously unobserved codes
within a machine learning framework.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="method">
4 Methods
</sectionHeader>
<subsectionHeader confidence="0.999727">
4.1 Run-time processing flow
</subsectionHeader>
<bodyText confidence="0.9997435">
We first describe the basic run-time processing
flow of the system, shown in Figure 1.
</bodyText>
<figureCaption confidence="0.99831">
Figure 1: Run-time processing flow
</figureCaption>
<bodyText confidence="0.997987">
In a naive approach, one could generate all
codes from the ICD-10-PCS inventory for each
EHR2 and estimate their probability in turn, but
this would be too computationally expensive. In-
stead, the hypothesis space is restricted by two-
2We use the term EHR generically in this paper. The sys-
tem can be applied at the level of individual clinical docu-
ments or entire patient encounters, whichever is appropriate
for the given application.
</bodyText>
<page confidence="0.995995">
61
</page>
<bodyText confidence="0.999978678571429">
level hierarchical classification with beam search.
First, a set of classifiers estimates the confidence
of all PCS sections (one-character prefixes of the
codes), one per section. The sections whose con-
fidence exceeds a threshold are used to generate
candidate body systems (two-character code pre-
fixes), whose confidence is estimated by another
set of classifiers. Then, body systems whose con-
fidence exceeds a threshold are used to generate
a set of candidate codes and the set of concepts
expressed by these codes. The probability of ob-
serving each of the candidate concepts in the EHR
is estimated by a separate classifier. Finally, these
concept confidence scores are used to derive fea-
tures for a model that estimates the probability of
observing each of the candidate codes, and the
highest-scoring codes are chosen according to a
thresholding decision rule.
The choice of two hierarchical layers is partially
determined by the amount of training data with
ICD-10 codes available for this study, since many
three-character code prefixes are too infrequent to
train reliable classifiers. Given more training data,
additional hierarchical classification layers could
be used, which would trade a higher risk of recall
errors against greater processing speed. The same
trade-off can be negotiated by adjusting the beam
search threshold.
</bodyText>
<subsectionHeader confidence="0.996474">
4.2 Concept confidence models
</subsectionHeader>
<bodyText confidence="0.999970484375">
Estimation of concept confidence – including the
confidence of code prefixes in the two hierarchi-
cal classification layers – is performed by a set of
classifiers, one per concept, which are trained on
EHRs supplied with ICD-10 and ICD-9 procedure
codes.
The basis for training the concept models is
provided by a mapping between codes and con-
cepts expressed by the codes. For example, the
code 0GB24ZZ (Excision of Left Adrenal Gland,
Percutaneous Endoscopic Approach) expresses,
among other concepts, the concept adrenal gland
and the more specific concept left adrenal gland.
It also expresses the concept of adrenalectomy
(surgical removal of one or both of the adrenal
glands), which corresponds to the regular expres-
sion 0G[BT][234]..Z over ICD-10-PCS codes.
We used the code-to-concept mapping described
in Mills (2013), supplemented by some additional
categories that do not correspond to traditional
clinical concepts. For example, our set of concepts
included entries for the categories of no device
and no qualifer, which are widely used in ICD-10-
PCS. We also added entries that specified the de-
vice axis or the qualifier axis together with the first
three axes, where they were absent in the original
concept map, reasoning that the language used to
express the choice of the device or qualifier can be
specific to particular procedures and body parts.
For data with ICD-10-PCS codes, the logic used
to generate training instances is straightforward.
Whenever a manually assigned code expresses a
given concept, a positive training instance for the
corresponding classifier is generated. Negative
training instances are sub-sampled from the con-
cepts generated by hierarchical classification lay-
ers for that EHR. As can be seen from this logic,
the precise question that the concept models seek
to answer is as follows: given that this particular
concept has been generated by the upstream hier-
archical layers, how likely is it that it will be ex-
pressed by one of the ICD-10 procedure codes as-
signed to that EHR?
In estimating concept confidence we do not at-
tempt to localize where in the clinical narrative
the given concept is expressed. Our baseline
feature set is simply a bag of tokens. We also
experimented with other feature types, including
frequency-based weighting schemes for token fea-
ture values and features based on string matches of
Unified Medical Language System (UMLS) con-
cept dictionaries. For the concepts of left and right
we define an additional feature type, indicating
whether the token left or right appears more fre-
quently in the EHR. While still rudimentary, this
feature type is more apt to infer laterality than a
bag of tokens.
A number of statistical methods can be used
to estimate concept confidence. We use the
Mallet (McCallum, 2002) implementation offl-
regularized logistic regression, which has shown
good performance for NLP tasks in terms of ac-
curacy as well as scalability at training and run-
time (Gao et al., 2007).
</bodyText>
<subsectionHeader confidence="0.995553">
4.3 Training on ICD-9 data
</subsectionHeader>
<bodyText confidence="0.999696166666666">
In training concept confidence models on data
with ICD-9 codes we make use of the General
Equivalence Mappings (GEMs), a publicly avail-
able resource establishing relationships between
ICD-9 and ICD-10 codes (CMS, 2014). Most cor-
respondences between ICD-9 and ICD-10 proce-
</bodyText>
<page confidence="0.996322">
62
</page>
<bodyText confidence="0.99943906060606">
dure codes are one-to-many, although other map-
ping patterns are also found. Furthermore, a code
in one set can correspond to a combination of
codes from the other set. For example, the ICD-
9 code for combined heart-lung transplantation
maps to a set of pairs of ICD-10 codes, the first
code in the pair representing one of three possible
types of heart transplantation, and the other rep-
resenting one of three possible types of bilateral
lung transplantation.
A complete description of the rules underlying
GEMs and our logic for processing them is beyond
the scope of this paper, and we limit our discussion
to the principles underlying our approach. We first
distribute a unit probability mass over the ICD-
10 codes or code combinations mapped to each
ICD-9 code, using logic that reflects the struc-
ture of GEMs and distributing probability mass
uniformly among comparable alternatives. From
these probabilities we compute a cumulative prob-
ability mass for each concept appearing in the
ICD-10 codes. For example, if an ICD-9 code
maps to four ICD-10 codes over which we dis-
tribute a uniform probability distibution, and a
given concept appears in two of them, we assign
the probability of 0.5 to that concept. For a given
EHR, we assign to each concept the highest prob-
ability it receives from any of the codes observed
for the EHR. Finally, we use the resulting concept
probabilities to weight positive training instances.
Negative instances still have unit weights, since
they correspond to concepts that can be unequivo-
cably ruled out based on the GEMs.
</bodyText>
<subsectionHeader confidence="0.993578">
4.4 Code confidence model
</subsectionHeader>
<bodyText confidence="0.999839373134329">
The code confidence model produces a confidence
score for candidate codes generated by the hierar-
chical classification layers, using features derived
from the output of the code confidence models
described above. The code confidence model is
trained on data with ICD-10 codes. Whenever a
candidate code matches a code assigned by hu-
man annotators, a positive training instance is gen-
erated. Otherwise, a negative instance is gener-
ated, with sub-sampling. We report experiments
using logistic regression with i1 and E2 regulariza-
tion (Gao et al., 2007).
The definition of features used in the model re-
quires careful attention, because it is in the form of
the feature space that the proposed model differs
from a standard one-vs-all approach. To elucidate
the contrast we may start with a form of the feature
space that would correspond to one-vs-all classi-
fication. This can be achieved by specifying the
identity of a particular code in all feature names.
Then, the objective function for logistic regression
would decompose into independent learning sub-
problems, one for each code, producing a collec-
tion of one-vs-all classifiers. There are clear draw-
backs to this approach. If all parameters are re-
stricted to a specific code, the training data would
be fragmented along the same lines. Thus, even
if features derived from concepts may seem to en-
able generalization, in reality they would in each
case be estimated only from training instances cor-
responding to a single code, causing unnecessary
data sparsity.
This shortcoming can be overcome in logistic
regression simply by introducing generalized fea-
tures, without changing the rest of the model (Sub-
otin, 2011). Thus, in deriving features from scores
of concept confidence models we include only
those concepts which are expressed by the given
code, but we do not specify the identity of the code
in the feature names. In this way the weights for
these features are estimated at once from training
instances for all codes in which these concepts ap-
pear. We combine these generalized features with
the code-bound features described earlier. The lat-
ter should help us learn more specific predictors
for particular procedures, when such predictors
exist in the feature space.
While the scores of concept confidence mod-
els provide the basis for the feature space of the
code confidence model, there are multiple ways in
which features can be derived from these scores.
The simplest way is to take concept identity (op-
tionally specified by code identity) as the fea-
ture name and the confidence score as the feature
value. We supplement these features with features
based on score quantization. That is, we thresh-
old each concept confidence score at several points
and define binary features indicating whether the
score exceeds each of the thresholds. For both
these feature types, we generate separate features
for predictions of concept models trained on ICD-
9 data and concept models trained on ICD-10 data
in order to allow the code confidence model to
learn how useful predictions of concept confidence
models are, depending on the type of their training
data.
Both the concept confidence models and the
</bodyText>
<page confidence="0.998849">
63
</page>
<bodyText confidence="0.999698">
code confidence model can be trained on data with
ICD-10 codes. We are thus faced with the ques-
tion of how best to use this limited resource. The
simplest approach would be to train both types of
models on all available training data, but there is a
concern that predictions of the concept models on
their own training data would not reflect their out-
of-sample performance, and this would mislead
the code confidence model into relying on them
too much. An alternative approach, often called
stacked generalization (Wolpert, 1992), would be
to generate training data for the code confidence
model by running concept confidence models on
out-of-sample data. We compare the performance
of these approaches below.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="method">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.7709">
5.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999917148148148">
We evaluated the proposed model using a cor-
pus of 28,536 EHRs (individual clinical records),
compiled to represent a wide variety of clinical
contexts and supplied with ICD-10-PCS codes by
trained medical coders. The corpus was annotated
under the auspices of 3M Health Information Sys-
tems for the express purpose of developing auto-
coding technology for ICD-10. There was a total
of 51,082 PCS codes and 5,650 unique PCS codes
in the corpus, only 76 of which appeared in more
than 100 EHRs, and 2,609 of which appeared just
once. Multiple coders worked on some of the doc-
uments, but they were allowed to collaborate, pro-
ducing what was effectively a single set of codes
for each EHR. We held out about a thousand EHRs
for development testing and evaluation, each, us-
ing the rest for training. The same corpus, as well
as 175,798 outpatient surgery EHRs with ICD-9
procedure codes submitted for billing by a health
provider were also used to train hierarchical and
concept confidence models.
We evaluated auto-coding performance by a
modified version of mean reciprocal rank (MRR).
MRR is a common evaluation metric for systems
with ranked outputs. For a set of Q correct out-
puts with ranks ranki among all outputs, standard
MRR is computed as:
</bodyText>
<sectionHeader confidence="0.352856" genericHeader="method">
MRR =
</sectionHeader>
<bodyText confidence="0.998876181818181">
For example, a MRR value of 0.25 means that
that the correct answer has rank 4 on average. This
metric is designed for tasks where only one of the
outputs can be correct. When applied directly to
tasks where more than one output can be correct,
MRR unfairly penalizes cases with multiple cor-
rect outputs, increasing the rank of some correct
outputs on account of other, higher-ranked outputs
that are also correct. We modify MRR for our task
by ignoring correct outputs in the rank computa-
tions. In other words, the rank of a correct output
is computed as the number of higher-ranked incor-
rect outputs, plus one. This metric has the advan-
tage of summarizing the accuracy of an auto-coder
without reference to a particular choice of thresh-
old, which may be determined by business rules or
research considerations, as would be the case for
precision and recall.
One advantage of regularized logistic regres-
sion is that the value of 1 is often a near-optimal
setting for the regularization trade-off parameter.
This can save considerable computation time that
would be required for tuning this parameter for
each experimental condition. We have previously
observed that the value of 1 consistently produced
near-optimal results for the E1 regularizer in con-
cept confidence models and for the E2 regularizer
in the code confidence models, and we have used
this setting for all the experiments reported here.
For the code confidence model with E1-regularized
logistic regression we saw a slight improvement
with weaker regularization, and we report the best
result we obtained for this model below.
</bodyText>
<subsectionHeader confidence="0.893528">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999256833333333">
The results are shown in Table 2. The top MMR
score of 0.572 corresponds to a micro-averaged F-
score of 0.485 (0.490 precision, 0.480 recall) when
the threshold is chosen to obtain approximately
equal values for recall and precision3. The best
result was obtained when:
</bodyText>
<listItem confidence="0.99395075">
• the concept models used bag-of-tokens fea-
tures (with the additional laterality features
described in Section 4.2);
• both concept models trained on ICD-9 data
and those trained on ICD-10 data were used;
• the code confidence model was trained on
data with predictions of concept models
trained on all of ICD-10 data (i.e., no
</listItem>
<footnote confidence="0.976605">
3To put these numbers into perspective, note that the aver-
age accuracy of trained medical coders for ICD-10 has been
estimated to be 63% (HIMSS/WEDI, 2013).
</footnote>
<figure confidence="0.8398316">
1
1
Q
ranki
� Q
</figure>
<page confidence="0.747374">
i=1
64
</page>
<bodyText confidence="0.921903">
data splitting for stacked generalization was
used);
</bodyText>
<listItem confidence="0.7278415">
• the code confidence model used all of the fea-
ture types described in Section 4.4;
• the code confidence model used logistic re-
gression with E2 regularization.
</listItem>
<bodyText confidence="0.751813">
We examine the impact of all these choices on
system performance in turn.
</bodyText>
<table confidence="0.999482642857143">
Model MRR
All data, all features, E2 reg. 0.572
Concept model training:
Trained on ICD-10 only 0.558
Trained on ICD-9 only 0.341
Code model features:
One-vs-all 0.519
No code-bound features 0.553
No quantization features 0.560
Stacked generalization:
half &amp; half data split 0.501
5-fold cross-validation 0.539
Code model algorithm:
E1 regularization 0.528
</table>
<tableCaption confidence="0.996734">
Table 2: Evaluation results. Each row after the
</tableCaption>
<bodyText confidence="0.960440666666667">
first correponds to varying one aspect of the model
shown in the first row. See Section 5.3 for details
of the experimental conditions.
</bodyText>
<subsectionHeader confidence="0.985325">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999986492753624">
Despite its apparent primitive nature, the bag-of-
token feature space for the concept confidence
models has turned out to provide a remarkably
strong baseline. Our experiments with frequency-
based weighting schemes for the feature values
and with features derived from text matches from
the UMLS concept dictionaries did not yield sub-
stantial improvements in the results. Thus, the use
of UMLS-based features, obtained using Apache
ConceptMapper, yielded a relative improvement
of 0.6% (i.e., 0.003 in absolute terms), but at the
cost of nearly doubling run-time processing time.
Nonetheless, we remain optimistic that more so-
phisticated features can benefit performance of the
concept models while maintaining their scalabil-
ity.
As can be seen from the table, both concept
models trained on ICD-9 data and those trained on
ICD-10 data contributed to the overall effective-
ness of the system. However, the contribution of
the latter is markedly stronger. This suggests that
further research is needed in finding the best ways
of exploiting ICD-9-coded data for ICD-10 auto-
coding. Given that data with ICD-9 codes is likely
to be more readily available than ICD-10 training
data in the foreseeable future, this line of investi-
gation holds potential for significant gains in auto-
coding performance.
For the choice of features used in the code con-
fidence model, the most prominent contribution is
made by the feature that generalize beyond spe-
cific codes, as discussed in Section 4.4. Adding
these features yields a 10% relative improvement
over the set of features equivalent to a one-vs-
all model. In fact, using the generalized features
alone (see the row marked “no code-bound fea-
tures” in Table 2) gives a score only 0.02 lower
than the best result. As would be expected, gener-
alized features are particularly important for codes
with limited training data. Thus, if we restrict
our attention to codes with fewer than 25 training
instances (which account for 95% of the unique
codes in our ICD-10 training data), we find that
generalized features yielded a 25% relative im-
provement over the one-vs-all model (0.247 to
0.309). In contrast, for codes with over 100 train-
ing instances (which account for 1% of the unique
codes, but 36% of the total code volume in our
corpus) the relative improvement from generalized
features is less than 4% (0.843 to 0.876). These
numbers afford two further observations. First,
the model can be improved dramatically by adding
a few dozen EHRs per code to the training cor-
pus. Secondly, there is still much room for re-
search in mitigating the effects of data sparsity
and improving prediction accuracy for less com-
mon codes. Elsewhere in Table 2 we see that
quantization-based features contribute a modest
predictive value.
Perhaps the most surprising result of the series
came from investigating the options for using the
available ICD-10 training data, which act as train-
ing material both for concept confidence models
and the code confidence model. The danger of
training both type of models on the same corpus
is intuitively apparent. If the training instances
for the code model are generated by concept mod-
els whose training data included the same EHRs,
the accuracy of these concept predictions may not
</bodyText>
<page confidence="0.998727">
65
</page>
<bodyText confidence="0.999987027777778">
reflect out-of-sample performance of the concept
models, causing the code model to rely on them
excessively.
The simplest implementation of Wolpert’s
stacked generalization proposal, which is intended
to guard against this risk, is to use one part of the
corpus to train one predictive layer and use its pre-
dictions on the another part of the corpus to train
the other layer. The result in Table 2 (see the
row marked “half &amp; half data split”) shows that
the resulting increase in sparsity of the training
data for both models leads to a major degradation
of the system’s performance, even though at run-
time concept models trained on all available data
are used. We also investigated a cross-validation
version of stacked generalization designed to mit-
igate against this fragmentation of training data.
We trained a separate set of concept models on the
training portion of each cross-validation fold, and
ran them on the held-out portion. The training set
for the code confidence model was then obtained
by combining these held-out portions. At run-
time, concept models trained on all of the avail-
able data were used. However, as intuitively com-
pelling as the arguments motivating this procedure
may be, the results were not competitive with the
baseline approach of using all available training
data for all the models.
Finally, we found that an E2 regularizer per-
formed clearly better than an E1 regularizer for the
code confidence model, even though we set the E2
trade-off constant to 1 and tuned the E1 trade-off
constant on the development test set. This is in
contrast to concept confidence models, where we
observed slightly better results with E1 regulariza-
tion than with E2 regularization.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999998333333333">
We have described a system for predicting ICD-
10-PCS codes from the clinical narrative con-
tained in EHRs. The proposed approach seeks to
mitigate the sparsity of training data with manu-
ally assigned ICD-10-PCS codes in three ways:
through an intermediate abstraction to clinical
concepts, through the use of data with ICD-9
codes to train concept confidence models, and
through the use of a code confidence model
whose parameters can generalize beyond individ-
ual codes. Our experiments show promising re-
sults and point out directions for further research.
</bodyText>
<sectionHeader confidence="0.996585" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.925973625">
We would like to thank Ron Mills for provid-
ing the crosswalk between ICD-10-PCS codes and
clinical concepts; Guoli Wang, Michael Nossal,
Kavita Ganesan, Joel Bradley, Edward Johnson,
Lyle Schofield, Michael Connor, Jean Stoner and
Roxana Safari for helpful discussions relating to
this work; and the anonymous reviewers for their
constructive criticism.
</bodyText>
<sectionHeader confidence="0.991625" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99324825">
Sean Benson. 2006. Computer-assisted Coding Soft-
ware Improves Documentation, Coding, Compli-
ance, and Revenue. Perspectives in Health Infor-
mation Management, CAC Proceedings, Fall 2006.
Centers for Medicare &amp; Medicaid Services. 2014.
General Equivalence Mappings. Documentation
for Technical Users. Electronically published at
cms.gov.
Chute CG, Yang Y, Buntrock J. 1994. An evalua-
tion of computer assisted clinical classification algo-
rithms. Proc Annu Symp Comput Appl Med Care.,
1994:162–6.
Jianfeng Gao, Galen Andrew, Mark Johnson, Kristina
Toutanova. 2007. A Comparative Study of Param-
eter Estimation Methods for Statistical Natural Lan-
guage Processing. ACL 2007.
Daniel T. Heinze, Mark L. Morsch, Ronald E. Shef-
fer, Jr., Michelle A. Jimmink, Mark A. Jennings,
William C. Morris, and Amy E. W. Morsch. 2000.
LifeCodeTM– A Natural Language Processing Sys-
tem for Medical Coding and Data Mining. AAAI
Proceedings.
Daniel T. Heinze, Mark Morsch, Ronald Sheffer,
Michelle Jimmink, Mark Jennings, William Mor-
ris, and Amy Morsch. 2001. LifeCode: A De-
ployed Application for Automated Medical Coding.
AI Magazine, Vol 22, No 2.
HIMSS/WEDI. 2013. ICD-10 National Pilot Pro-
gram Outcomes Report. Electronically published at
himss.org.
Yuankai Jiang, Michael Nossal, and Philip Resnik.
2006. How Does the System Know It’s Right?
Automated Confidence Assessment for Compliant
Coding. Perspectives in Health Information Man-
agement, Computer Assisted Coding Conference
Proceedings, Fall 2006.
Leah Larkey and W. Bruce Croft. 1995. Automatic As-
signment of ICD9 Codes To Discharge Summaries.
Technical report, Center for Intelligent Information
Retrieval at University of Massachusetts.
</reference>
<page confidence="0.879737">
66
</page>
<reference confidence="0.999676276595745">
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu
Medori, Julia and Fairon, C´edrick. 2010. Machine
Learning and Features Selection for Semi-automatic
ICD-9-CM Encoding. Proceedings of the NAACL
HLT 2010 Second Louhi Workshop on Text and Data
Mining of Health Documents, 2010: 84–89.
Ronald E. Mills. 2013. Methods using multi-
dimensional representations of medical codes. US
Patent Application US20130006653.
S.V. Pakhomov, J.D. Buntrock, and C.G. Chute. 2006.
Automating the assignment of diagnosis codes to
patient encounters using example-based and ma-
chine learning techniques. JAm Med Inform Assoc,
13(5):516–25.
Adler Perotte, Rimma Pivovarov, Karthik Natarajan,
Nicole Weiskopf, Frank Wood, No´emie Elhadad .
2014. Diagnosis code assignment: models and eval-
uation metrics. JAm Med Inform Assoc, 21(2):231–
7.
Pestian, JP, Brew C, Matykiewicz P, Hovermale DJ,
Johnson N, Bretonnel Cohen K, and Duch W. 2007.
A shared task involving multi-label classification
of clinical free text. Proceedings ACL: BioNLP,
2007:97–104.
Philip Resnik, Michael Niv, Michael Nossal, Gregory
Schnitzer, Jean Stoner, Andrew Kapit, and Richard
Toren. 2006. Using intrinsic and extrinsic metrics
to evaluate accuracy and facilitation in computer-
assisted coding.. Perspectives in Health Informa-
tion Management, Computer Assisted Coding Con-
ference Proceedings, Fall 2006.
Berthier Ribeiro-Neto, Alberto H.F. Laender and Lu-
ciano R.S. de Lima. 2001. An experimental study
in automatically categorizing medical documents.
Journal of the American Society for Information Sci-
ence and Technology, 52(5): 391–401.
Mary H. Stanfill, Margaret Williams, Susan H. Fenton,
Robert A. Jenders, and William R. Hersh. 2010.
A systematic literature review of automated clinical
coding and classification systems. JAm Med Inform
Assoc., 17(6): 646–651.
Michael Subotin. 2011. An exponential translation
model for target language morphology. ACL 2011.
David H. Wolpert. 1992. Stacked Generalization.
Neural Networks, 5:241–259.
</reference>
<page confidence="0.999507">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.857845">
<title confidence="0.981007">A System for Predicting ICD-10-PCS from Electronic Health Records</title>
<author confidence="0.986177">Michael</author>
<affiliation confidence="0.95354">3M Health Information Silver Spring,</affiliation>
<email confidence="0.999858">msubotin@mmm.com</email>
<abstract confidence="0.998812580645161">Medical coding is a process of classifying health records according to standard code sets representing procedures and diagnoses. It is an integral part of health care in the U.S., and the high costs it incurs have prompted adoption of natural language processing techniques for automatic generation of these codes from the clinical narrative contained in electronic health records. The need for effective auto-coding methods becomes even greater with the impending adoption of ICD-10, a code inventory of greater complexity than the currently used code sets. This paper presents a system that predicts ICD-10 procedure codes from the clinical narrative using several levels of abstraction. First, partial hierarchical classification is used to identify potentially relevant concepts and codes. Then, for each of these concepts we estimate the confidence that it appears in a procedure code for that document. Finally, confidence values for the candidate codes are estimated using features derived from concept confidence scores. The concept models can be trained on data with ICD-9 codes to supplement sparse ICD-10 training resources. Evaluation on held-out data shows promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sean Benson</author>
</authors>
<date>2006</date>
<booktitle>Computer-assisted Coding Software Improves Documentation, Coding, Compliance, and Revenue. Perspectives in Health Information Management, CAC Proceedings,</booktitle>
<location>Fall</location>
<contexts>
<context position="2650" citStr="Benson, 2006" startWordPosition="411" endWordPosition="412">inology (CPT) codes. The same codes are also used for research, internal bookkeeping, and other purposes. Assigning codes to clinical documentation often requires extensive technical training and involves substantial labor costs. This, together with increasing prominence of electronic health records (EHRs), has prompted development and adoption of NLP algorithms that support the coding workflow by automatically inferring appropriate codes from the clinical narrative and other information contained in the EHR (Chute et al., 1994; Heinze et al., 2001; Resnik et al., 2006; Pakhomov et al., 2006; Benson, 2006). The need for effective autocoding methods becomes especially acute with the introduction of ICD-10 and the associated increase of training and labor costs for manual coding. The novelty and complexity of ICD-10 presents unprecedented challenges for developers of rulebased auto-coding software. Thus, while ICD-9 contains 3882 codes for procedures, the number of codes defined by the ICD-10 Procedure Coding System (PCS) is greater than 70,000. Furthermore, the organization of ICD-10-PCS is fundamentally different from ICD-9, which means that the investment of time and money that had gone into w</context>
</contexts>
<marker>Benson, 2006</marker>
<rawString>Sean Benson. 2006. Computer-assisted Coding Software Improves Documentation, Coding, Compliance, and Revenue. Perspectives in Health Information Management, CAC Proceedings, Fall 2006.</rawString>
</citation>
<citation valid="true">
<title>Medicare &amp; Medicaid Services.</title>
<date>2014</date>
<institution>Centers for</institution>
<contexts>
<context position="10869" citStr="(2014)" startWordPosition="1742" endWordPosition="1742">list beforehand. In another pioneering study, Larkey &amp; Croft (1995) investigated k-Nearest Neighbor, Naive Bayes, and relevance feedback on a set of 12K discharge summaries, predicting ICD-9 codes. Heinze et al (2000) and Ribeiro-Neto et al (2001) describe systems centered on symbolic computation. Jiang et al (2006) discuss confidence assessment for ICD-9 and CPT codes, performed separately from code generation. Medori &amp; Fairon (2010) combine information extraction with a Naive Bayes classifier, working with a corpus of about 20K discharge summaries in French. In a recent paper, Perotte et al (2014) study standard and hierarchical classification using support vector machines on a corpus of about 20K EHRs with ICD-9 codes. We are not aware of any previous publications on auto-coding for ICD-10-PCS, and the results of these studies cannot be directly compared with those reported below due to the unique nature of this code set. Our original contributions also include explicit modeling of concepts and the capability to assign previously unobserved codes within a machine learning framework. 4 Methods 4.1 Run-time processing flow We first describe the basic run-time processing flow of the syst</context>
</contexts>
<marker>2014</marker>
<rawString>Centers for Medicare &amp; Medicaid Services. 2014. General Equivalence Mappings. Documentation for Technical Users. Electronically published at cms.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chute CG</author>
<author>Y Yang</author>
<author>J Buntrock</author>
</authors>
<title>An evaluation of computer assisted clinical classification algorithms. Proc Annu Symp Comput Appl Med Care.,</title>
<date>1994</date>
<marker>CG, Yang, Buntrock, 1994</marker>
<rawString>Chute CG, Yang Y, Buntrock J. 1994. An evaluation of computer assisted clinical classification algorithms. Proc Annu Symp Comput Appl Med Care., 1994:162–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Galen Andrew</author>
<author>Mark Johnson</author>
<author>Kristina Toutanova</author>
</authors>
<title>A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing.</title>
<date>2007</date>
<publisher>ACL</publisher>
<contexts>
<context position="16303" citStr="Gao et al., 2007" startWordPosition="2607" endWordPosition="2610">matches of Unified Medical Language System (UMLS) concept dictionaries. For the concepts of left and right we define an additional feature type, indicating whether the token left or right appears more frequently in the EHR. While still rudimentary, this feature type is more apt to infer laterality than a bag of tokens. A number of statistical methods can be used to estimate concept confidence. We use the Mallet (McCallum, 2002) implementation offlregularized logistic regression, which has shown good performance for NLP tasks in terms of accuracy as well as scalability at training and runtime (Gao et al., 2007). 4.3 Training on ICD-9 data In training concept confidence models on data with ICD-9 codes we make use of the General Equivalence Mappings (GEMs), a publicly available resource establishing relationships between ICD-9 and ICD-10 codes (CMS, 2014). Most correspondences between ICD-9 and ICD-10 proce62 dure codes are one-to-many, although other mapping patterns are also found. Furthermore, a code in one set can correspond to a combination of codes from the other set. For example, the ICD9 code for combined heart-lung transplantation maps to a set of pairs of ICD-10 codes, the first code in the </context>
<context position="18741" citStr="Gao et al., 2007" startWordPosition="3004" endWordPosition="3007">quivocably ruled out based on the GEMs. 4.4 Code confidence model The code confidence model produces a confidence score for candidate codes generated by the hierarchical classification layers, using features derived from the output of the code confidence models described above. The code confidence model is trained on data with ICD-10 codes. Whenever a candidate code matches a code assigned by human annotators, a positive training instance is generated. Otherwise, a negative instance is generated, with sub-sampling. We report experiments using logistic regression with i1 and E2 regularization (Gao et al., 2007). The definition of features used in the model requires careful attention, because it is in the form of the feature space that the proposed model differs from a standard one-vs-all approach. To elucidate the contrast we may start with a form of the feature space that would correspond to one-vs-all classification. This can be achieved by specifying the identity of a particular code in all feature names. Then, the objective function for logistic regression would decompose into independent learning subproblems, one for each code, producing a collection of one-vs-all classifiers. There are clear d</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Jianfeng Gao, Galen Andrew, Mark Johnson, Kristina Toutanova. 2007. A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing. ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel T Heinze</author>
<author>Mark L Morsch</author>
<author>Ronald E Sheffer</author>
<author>Michelle A Jimmink</author>
<author>Mark A Jennings</author>
<author>William C Morris</author>
<author>Amy E W Morsch</author>
</authors>
<title>LifeCodeTM– A Natural Language Processing System for Medical Coding and Data Mining.</title>
<date>2000</date>
<publisher>AAAI Proceedings.</publisher>
<contexts>
<context position="10480" citStr="Heinze et al (2000)" startWordPosition="1678" endWordPosition="1681">et al., 1994), where it was deployed to assign codes from a customized and greatly expanded version of ICD-8, consisting of almost 30K diagnostic codes. A recently reported version of their system (Pakhomov et al., 2006) leverages a combination of example-based techniques and Naive Bayes classification over a database of over 20M EHRs. The phrases representing the diagnoses have to be itemized as a list beforehand. In another pioneering study, Larkey &amp; Croft (1995) investigated k-Nearest Neighbor, Naive Bayes, and relevance feedback on a set of 12K discharge summaries, predicting ICD-9 codes. Heinze et al (2000) and Ribeiro-Neto et al (2001) describe systems centered on symbolic computation. Jiang et al (2006) discuss confidence assessment for ICD-9 and CPT codes, performed separately from code generation. Medori &amp; Fairon (2010) combine information extraction with a Naive Bayes classifier, working with a corpus of about 20K discharge summaries in French. In a recent paper, Perotte et al (2014) study standard and hierarchical classification using support vector machines on a corpus of about 20K EHRs with ICD-9 codes. We are not aware of any previous publications on auto-coding for ICD-10-PCS, and the </context>
</contexts>
<marker>Heinze, Morsch, Sheffer, Jimmink, Jennings, Morris, Morsch, 2000</marker>
<rawString>Daniel T. Heinze, Mark L. Morsch, Ronald E. Sheffer, Jr., Michelle A. Jimmink, Mark A. Jennings, William C. Morris, and Amy E. W. Morsch. 2000. LifeCodeTM– A Natural Language Processing System for Medical Coding and Data Mining. AAAI Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel T Heinze</author>
<author>Mark Morsch</author>
<author>Ronald Sheffer</author>
<author>Michelle Jimmink</author>
<author>Mark Jennings</author>
<author>William Morris</author>
<author>Amy Morsch</author>
</authors>
<title>LifeCode: A Deployed Application for Automated Medical Coding.</title>
<date>2001</date>
<journal>AI Magazine, Vol</journal>
<volume>22</volume>
<contexts>
<context position="2591" citStr="Heinze et al., 2001" startWordPosition="399" endWordPosition="402">for adoption in October 20141, as well as Current Procedural Terminology (CPT) codes. The same codes are also used for research, internal bookkeeping, and other purposes. Assigning codes to clinical documentation often requires extensive technical training and involves substantial labor costs. This, together with increasing prominence of electronic health records (EHRs), has prompted development and adoption of NLP algorithms that support the coding workflow by automatically inferring appropriate codes from the clinical narrative and other information contained in the EHR (Chute et al., 1994; Heinze et al., 2001; Resnik et al., 2006; Pakhomov et al., 2006; Benson, 2006). The need for effective autocoding methods becomes especially acute with the introduction of ICD-10 and the associated increase of training and labor costs for manual coding. The novelty and complexity of ICD-10 presents unprecedented challenges for developers of rulebased auto-coding software. Thus, while ICD-9 contains 3882 codes for procedures, the number of codes defined by the ICD-10 Procedure Coding System (PCS) is greater than 70,000. Furthermore, the organization of ICD-10-PCS is fundamentally different from ICD-9, which means</context>
</contexts>
<marker>Heinze, Morsch, Sheffer, Jimmink, Jennings, Morris, Morsch, 2001</marker>
<rawString>Daniel T. Heinze, Mark Morsch, Ronald Sheffer, Michelle Jimmink, Mark Jennings, William Morris, and Amy Morsch. 2001. LifeCode: A Deployed Application for Automated Medical Coding. AI Magazine, Vol 22, No 2.</rawString>
</citation>
<citation valid="false">
<title>ICD-10 National Pilot Program Outcomes Report. Electronically published at himss.org.</title>
<marker></marker>
<rawString>HIMSS/WEDI. 2013. ICD-10 National Pilot Program Outcomes Report. Electronically published at himss.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuankai Jiang</author>
<author>Michael Nossal</author>
<author>Philip Resnik</author>
</authors>
<title>How Does the System Know It’s Right? Automated Confidence Assessment for Compliant Coding. Perspectives</title>
<date>2006</date>
<booktitle>in Health Information Management, Computer Assisted Coding Conference Proceedings,</booktitle>
<location>Fall</location>
<contexts>
<context position="10580" citStr="Jiang et al (2006)" startWordPosition="1693" endWordPosition="1696">f ICD-8, consisting of almost 30K diagnostic codes. A recently reported version of their system (Pakhomov et al., 2006) leverages a combination of example-based techniques and Naive Bayes classification over a database of over 20M EHRs. The phrases representing the diagnoses have to be itemized as a list beforehand. In another pioneering study, Larkey &amp; Croft (1995) investigated k-Nearest Neighbor, Naive Bayes, and relevance feedback on a set of 12K discharge summaries, predicting ICD-9 codes. Heinze et al (2000) and Ribeiro-Neto et al (2001) describe systems centered on symbolic computation. Jiang et al (2006) discuss confidence assessment for ICD-9 and CPT codes, performed separately from code generation. Medori &amp; Fairon (2010) combine information extraction with a Naive Bayes classifier, working with a corpus of about 20K discharge summaries in French. In a recent paper, Perotte et al (2014) study standard and hierarchical classification using support vector machines on a corpus of about 20K EHRs with ICD-9 codes. We are not aware of any previous publications on auto-coding for ICD-10-PCS, and the results of these studies cannot be directly compared with those reported below due to the unique nat</context>
</contexts>
<marker>Jiang, Nossal, Resnik, 2006</marker>
<rawString>Yuankai Jiang, Michael Nossal, and Philip Resnik. 2006. How Does the System Know It’s Right? Automated Confidence Assessment for Compliant Coding. Perspectives in Health Information Management, Computer Assisted Coding Conference Proceedings, Fall 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah Larkey</author>
<author>W Bruce Croft</author>
</authors>
<title>Automatic Assignment of ICD9 Codes To Discharge Summaries.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Center for Intelligent Information Retrieval at University of Massachusetts.</institution>
<contexts>
<context position="10330" citStr="Larkey &amp; Croft (1995)" startWordPosition="1655" endWordPosition="1658">tion thereof, which better reflects the role of autocoding in real-life applications. Mayo Clinic was among the earliest adopters of auto-coding (Chute et al., 1994), where it was deployed to assign codes from a customized and greatly expanded version of ICD-8, consisting of almost 30K diagnostic codes. A recently reported version of their system (Pakhomov et al., 2006) leverages a combination of example-based techniques and Naive Bayes classification over a database of over 20M EHRs. The phrases representing the diagnoses have to be itemized as a list beforehand. In another pioneering study, Larkey &amp; Croft (1995) investigated k-Nearest Neighbor, Naive Bayes, and relevance feedback on a set of 12K discharge summaries, predicting ICD-9 codes. Heinze et al (2000) and Ribeiro-Neto et al (2001) describe systems centered on symbolic computation. Jiang et al (2006) discuss confidence assessment for ICD-9 and CPT codes, performed separately from code generation. Medori &amp; Fairon (2010) combine information extraction with a Naive Bayes classifier, working with a corpus of about 20K discharge summaries in French. In a recent paper, Perotte et al (2014) study standard and hierarchical classification using support</context>
</contexts>
<marker>Larkey, Croft, 1995</marker>
<rawString>Leah Larkey and W. Bruce Croft. 1995. Automatic Assignment of ICD9 Codes To Discharge Summaries. Technical report, Center for Intelligent Information Retrieval at University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu</note>
<contexts>
<context position="16117" citStr="McCallum, 2002" startWordPosition="2578" endWordPosition="2579">feature set is simply a bag of tokens. We also experimented with other feature types, including frequency-based weighting schemes for token feature values and features based on string matches of Unified Medical Language System (UMLS) concept dictionaries. For the concepts of left and right we define an additional feature type, indicating whether the token left or right appears more frequently in the EHR. While still rudimentary, this feature type is more apt to infer laterality than a bag of tokens. A number of statistical methods can be used to estimate concept confidence. We use the Mallet (McCallum, 2002) implementation offlregularized logistic regression, which has shown good performance for NLP tasks in terms of accuracy as well as scalability at training and runtime (Gao et al., 2007). 4.3 Training on ICD-9 data In training concept confidence models on data with ICD-9 codes we make use of the General Equivalence Mappings (GEMs), a publicly available resource establishing relationships between ICD-9 and ICD-10 codes (CMS, 2014). Most correspondences between ICD-9 and ICD-10 proce62 dure codes are one-to-many, although other mapping patterns are also found. Furthermore, a code in one set can </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Medori</author>
<author>C´edrick Fairon</author>
</authors>
<title>Machine Learning and Features Selection for Semi-automatic ICD-9-CM Encoding.</title>
<date>2010</date>
<booktitle>Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents,</booktitle>
<pages>84--89</pages>
<contexts>
<context position="10701" citStr="Medori &amp; Fairon (2010)" startWordPosition="1710" endWordPosition="1713">6) leverages a combination of example-based techniques and Naive Bayes classification over a database of over 20M EHRs. The phrases representing the diagnoses have to be itemized as a list beforehand. In another pioneering study, Larkey &amp; Croft (1995) investigated k-Nearest Neighbor, Naive Bayes, and relevance feedback on a set of 12K discharge summaries, predicting ICD-9 codes. Heinze et al (2000) and Ribeiro-Neto et al (2001) describe systems centered on symbolic computation. Jiang et al (2006) discuss confidence assessment for ICD-9 and CPT codes, performed separately from code generation. Medori &amp; Fairon (2010) combine information extraction with a Naive Bayes classifier, working with a corpus of about 20K discharge summaries in French. In a recent paper, Perotte et al (2014) study standard and hierarchical classification using support vector machines on a corpus of about 20K EHRs with ICD-9 codes. We are not aware of any previous publications on auto-coding for ICD-10-PCS, and the results of these studies cannot be directly compared with those reported below due to the unique nature of this code set. Our original contributions also include explicit modeling of concepts and the capability to assign </context>
</contexts>
<marker>Medori, Fairon, 2010</marker>
<rawString>Medori, Julia and Fairon, C´edrick. 2010. Machine Learning and Features Selection for Semi-automatic ICD-9-CM Encoding. Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, 2010: 84–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald E Mills</author>
</authors>
<title>Methods using multidimensional representations of medical codes. US Patent Application US20130006653.</title>
<date>2013</date>
<contexts>
<context position="14162" citStr="Mills (2013)" startWordPosition="2258" endWordPosition="2259">d with ICD-10 and ICD-9 procedure codes. The basis for training the concept models is provided by a mapping between codes and concepts expressed by the codes. For example, the code 0GB24ZZ (Excision of Left Adrenal Gland, Percutaneous Endoscopic Approach) expresses, among other concepts, the concept adrenal gland and the more specific concept left adrenal gland. It also expresses the concept of adrenalectomy (surgical removal of one or both of the adrenal glands), which corresponds to the regular expression 0G[BT][234]..Z over ICD-10-PCS codes. We used the code-to-concept mapping described in Mills (2013), supplemented by some additional categories that do not correspond to traditional clinical concepts. For example, our set of concepts included entries for the categories of no device and no qualifer, which are widely used in ICD-10- PCS. We also added entries that specified the device axis or the qualifier axis together with the first three axes, where they were absent in the original concept map, reasoning that the language used to express the choice of the device or qualifier can be specific to particular procedures and body parts. For data with ICD-10-PCS codes, the logic used to generate </context>
</contexts>
<marker>Mills, 2013</marker>
<rawString>Ronald E. Mills. 2013. Methods using multidimensional representations of medical codes. US Patent Application US20130006653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V Pakhomov</author>
<author>J D Buntrock</author>
<author>C G Chute</author>
</authors>
<title>Automating the assignment of diagnosis codes to patient encounters using example-based and machine learning techniques. JAm Med Inform Assoc,</title>
<date>2006</date>
<contexts>
<context position="2635" citStr="Pakhomov et al., 2006" startWordPosition="407" endWordPosition="410">Current Procedural Terminology (CPT) codes. The same codes are also used for research, internal bookkeeping, and other purposes. Assigning codes to clinical documentation often requires extensive technical training and involves substantial labor costs. This, together with increasing prominence of electronic health records (EHRs), has prompted development and adoption of NLP algorithms that support the coding workflow by automatically inferring appropriate codes from the clinical narrative and other information contained in the EHR (Chute et al., 1994; Heinze et al., 2001; Resnik et al., 2006; Pakhomov et al., 2006; Benson, 2006). The need for effective autocoding methods becomes especially acute with the introduction of ICD-10 and the associated increase of training and labor costs for manual coding. The novelty and complexity of ICD-10 presents unprecedented challenges for developers of rulebased auto-coding software. Thus, while ICD-9 contains 3882 codes for procedures, the number of codes defined by the ICD-10 Procedure Coding System (PCS) is greater than 70,000. Furthermore, the organization of ICD-10-PCS is fundamentally different from ICD-9, which means that the investment of time and money that </context>
<context position="10081" citStr="Pakhomov et al., 2006" startWordPosition="1615" endWordPosition="1618">o papers studying a variety of rule-based and statistical methods, which are too numerous to list here. We limit our attention to a smaller set of research publications describing identification of an entire set of billing codes, or a significant portion thereof, which better reflects the role of autocoding in real-life applications. Mayo Clinic was among the earliest adopters of auto-coding (Chute et al., 1994), where it was deployed to assign codes from a customized and greatly expanded version of ICD-8, consisting of almost 30K diagnostic codes. A recently reported version of their system (Pakhomov et al., 2006) leverages a combination of example-based techniques and Naive Bayes classification over a database of over 20M EHRs. The phrases representing the diagnoses have to be itemized as a list beforehand. In another pioneering study, Larkey &amp; Croft (1995) investigated k-Nearest Neighbor, Naive Bayes, and relevance feedback on a set of 12K discharge summaries, predicting ICD-9 codes. Heinze et al (2000) and Ribeiro-Neto et al (2001) describe systems centered on symbolic computation. Jiang et al (2006) discuss confidence assessment for ICD-9 and CPT codes, performed separately from code generation. Me</context>
</contexts>
<marker>Pakhomov, Buntrock, Chute, 2006</marker>
<rawString>S.V. Pakhomov, J.D. Buntrock, and C.G. Chute. 2006. Automating the assignment of diagnosis codes to patient encounters using example-based and machine learning techniques. JAm Med Inform Assoc, 13(5):516–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adler Perotte</author>
<author>Rimma Pivovarov</author>
<author>Karthik Natarajan</author>
<author>Nicole Weiskopf</author>
<author>Frank Wood</author>
</authors>
<title>No´emie Elhadad .</title>
<date>2014</date>
<volume>21</volume>
<issue>2</issue>
<pages>7</pages>
<contexts>
<context position="10869" citStr="Perotte et al (2014)" startWordPosition="1739" endWordPosition="1742">itemized as a list beforehand. In another pioneering study, Larkey &amp; Croft (1995) investigated k-Nearest Neighbor, Naive Bayes, and relevance feedback on a set of 12K discharge summaries, predicting ICD-9 codes. Heinze et al (2000) and Ribeiro-Neto et al (2001) describe systems centered on symbolic computation. Jiang et al (2006) discuss confidence assessment for ICD-9 and CPT codes, performed separately from code generation. Medori &amp; Fairon (2010) combine information extraction with a Naive Bayes classifier, working with a corpus of about 20K discharge summaries in French. In a recent paper, Perotte et al (2014) study standard and hierarchical classification using support vector machines on a corpus of about 20K EHRs with ICD-9 codes. We are not aware of any previous publications on auto-coding for ICD-10-PCS, and the results of these studies cannot be directly compared with those reported below due to the unique nature of this code set. Our original contributions also include explicit modeling of concepts and the capability to assign previously unobserved codes within a machine learning framework. 4 Methods 4.1 Run-time processing flow We first describe the basic run-time processing flow of the syst</context>
</contexts>
<marker>Perotte, Pivovarov, Natarajan, Weiskopf, Wood, 2014</marker>
<rawString>Adler Perotte, Rimma Pivovarov, Karthik Natarajan, Nicole Weiskopf, Frank Wood, No´emie Elhadad . 2014. Diagnosis code assignment: models and evaluation metrics. JAm Med Inform Assoc, 21(2):231– 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JP Pestian</author>
<author>C Brew</author>
<author>P Matykiewicz</author>
<author>Hovermale DJ</author>
<author>N Johnson</author>
<author>Bretonnel Cohen K</author>
<author>W Duch</author>
</authors>
<title>A shared task involving multi-label classification of clinical free text.</title>
<date>2007</date>
<booktitle>Proceedings ACL: BioNLP,</booktitle>
<pages>2007--97</pages>
<contexts>
<context position="9385" citStr="Pestian et al., 2007" startWordPosition="1497" endWordPosition="1500">or data sparsity and idiosyncracies of axis definitions by introducing abstraction into the model. 3 Related work There exists a large literature on automatic classification of clinical text (Stanfill et al., 2010). A sizeable portion of it is devoted to detecting categories corresponding to billing codes, but most of these studies are limited to one or a handful of categories. This is in part because the use of patient records is subject to strict regulation. Thus, the corpus used for most auto-coding research up to date consists of about two thousand documents annotated with 45 ICD-9 codes (Pestian et al., 2007). It was used in a shared task at the 2007 BioNLP workshop and gave rise to papers studying a variety of rule-based and statistical methods, which are too numerous to list here. We limit our attention to a smaller set of research publications describing identification of an entire set of billing codes, or a significant portion thereof, which better reflects the role of autocoding in real-life applications. Mayo Clinic was among the earliest adopters of auto-coding (Chute et al., 1994), where it was deployed to assign codes from a customized and greatly expanded version of ICD-8, consisting of </context>
</contexts>
<marker>Pestian, Brew, Matykiewicz, DJ, Johnson, K, Duch, 2007</marker>
<rawString>Pestian, JP, Brew C, Matykiewicz P, Hovermale DJ, Johnson N, Bretonnel Cohen K, and Duch W. 2007. A shared task involving multi-label classification of clinical free text. Proceedings ACL: BioNLP, 2007:97–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Michael Niv</author>
<author>Michael Nossal</author>
<author>Gregory Schnitzer</author>
<author>Jean Stoner</author>
<author>Andrew Kapit</author>
<author>Richard Toren</author>
</authors>
<title>Using intrinsic and extrinsic metrics to evaluate accuracy and facilitation in computerassisted coding..</title>
<date>2006</date>
<booktitle>Perspectives in Health Information Management, Computer Assisted Coding Conference Proceedings,</booktitle>
<location>Fall</location>
<contexts>
<context position="2612" citStr="Resnik et al., 2006" startWordPosition="403" endWordPosition="406">er 20141, as well as Current Procedural Terminology (CPT) codes. The same codes are also used for research, internal bookkeeping, and other purposes. Assigning codes to clinical documentation often requires extensive technical training and involves substantial labor costs. This, together with increasing prominence of electronic health records (EHRs), has prompted development and adoption of NLP algorithms that support the coding workflow by automatically inferring appropriate codes from the clinical narrative and other information contained in the EHR (Chute et al., 1994; Heinze et al., 2001; Resnik et al., 2006; Pakhomov et al., 2006; Benson, 2006). The need for effective autocoding methods becomes especially acute with the introduction of ICD-10 and the associated increase of training and labor costs for manual coding. The novelty and complexity of ICD-10 presents unprecedented challenges for developers of rulebased auto-coding software. Thus, while ICD-9 contains 3882 codes for procedures, the number of codes defined by the ICD-10 Procedure Coding System (PCS) is greater than 70,000. Furthermore, the organization of ICD-10-PCS is fundamentally different from ICD-9, which means that the investment </context>
</contexts>
<marker>Resnik, Niv, Nossal, Schnitzer, Stoner, Kapit, Toren, 2006</marker>
<rawString>Philip Resnik, Michael Niv, Michael Nossal, Gregory Schnitzer, Jean Stoner, Andrew Kapit, and Richard Toren. 2006. Using intrinsic and extrinsic metrics to evaluate accuracy and facilitation in computerassisted coding.. Perspectives in Health Information Management, Computer Assisted Coding Conference Proceedings, Fall 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berthier Ribeiro-Neto</author>
<author>Alberto H F Laender</author>
<author>Luciano R S de Lima</author>
</authors>
<title>An experimental study in automatically categorizing medical documents.</title>
<date>2001</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>52</volume>
<issue>5</issue>
<pages>391--401</pages>
<marker>Ribeiro-Neto, Laender, de Lima, 2001</marker>
<rawString>Berthier Ribeiro-Neto, Alberto H.F. Laender and Luciano R.S. de Lima. 2001. An experimental study in automatically categorizing medical documents. Journal of the American Society for Information Science and Technology, 52(5): 391–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary H Stanfill</author>
<author>Margaret Williams</author>
<author>Susan H Fenton</author>
<author>Robert A Jenders</author>
<author>William R Hersh</author>
</authors>
<title>A systematic literature review of automated clinical coding and classification systems.</title>
<date>2010</date>
<journal>JAm Med Inform Assoc.,</journal>
<volume>17</volume>
<issue>6</issue>
<pages>646--651</pages>
<contexts>
<context position="8978" citStr="Stanfill et al., 2010" startWordPosition="1425" endWordPosition="1428">ess by a 60 single word, while the device axis uses generic categories where a clinical narrative would refer only to specific brand names. This drastically limits how much can be accomplished by matching code descriptions or indexes derived from them against the text of EHRs. On the other hand, the systematic conceptual structure of PCS codes and of the codeset as a whole can be exploited to compensate for data sparsity and idiosyncracies of axis definitions by introducing abstraction into the model. 3 Related work There exists a large literature on automatic classification of clinical text (Stanfill et al., 2010). A sizeable portion of it is devoted to detecting categories corresponding to billing codes, but most of these studies are limited to one or a handful of categories. This is in part because the use of patient records is subject to strict regulation. Thus, the corpus used for most auto-coding research up to date consists of about two thousand documents annotated with 45 ICD-9 codes (Pestian et al., 2007). It was used in a shared task at the 2007 BioNLP workshop and gave rise to papers studying a variety of rule-based and statistical methods, which are too numerous to list here. We limit our at</context>
</contexts>
<marker>Stanfill, Williams, Fenton, Jenders, Hersh, 2010</marker>
<rawString>Mary H. Stanfill, Margaret Williams, Susan H. Fenton, Robert A. Jenders, and William R. Hersh. 2010. A systematic literature review of automated clinical coding and classification systems. JAm Med Inform Assoc., 17(6): 646–651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Subotin</author>
</authors>
<title>An exponential translation model for target language morphology.</title>
<date>2011</date>
<journal>ACL</journal>
<contexts>
<context position="19859" citStr="Subotin, 2011" startWordPosition="3187" endWordPosition="3189">problems, one for each code, producing a collection of one-vs-all classifiers. There are clear drawbacks to this approach. If all parameters are restricted to a specific code, the training data would be fragmented along the same lines. Thus, even if features derived from concepts may seem to enable generalization, in reality they would in each case be estimated only from training instances corresponding to a single code, causing unnecessary data sparsity. This shortcoming can be overcome in logistic regression simply by introducing generalized features, without changing the rest of the model (Subotin, 2011). Thus, in deriving features from scores of concept confidence models we include only those concepts which are expressed by the given code, but we do not specify the identity of the code in the feature names. In this way the weights for these features are estimated at once from training instances for all codes in which these concepts appear. We combine these generalized features with the code-bound features described earlier. The latter should help us learn more specific predictors for particular procedures, when such predictors exist in the feature space. While the scores of concept confidenc</context>
</contexts>
<marker>Subotin, 2011</marker>
<rawString>Michael Subotin. 2011. An exponential translation model for target language morphology. ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked Generalization. Neural Networks,</title>
<date>1992</date>
<pages>5--241</pages>
<contexts>
<context position="21885" citStr="Wolpert, 1992" startWordPosition="3524" endWordPosition="3525">g on the type of their training data. Both the concept confidence models and the 63 code confidence model can be trained on data with ICD-10 codes. We are thus faced with the question of how best to use this limited resource. The simplest approach would be to train both types of models on all available training data, but there is a concern that predictions of the concept models on their own training data would not reflect their outof-sample performance, and this would mislead the code confidence model into relying on them too much. An alternative approach, often called stacked generalization (Wolpert, 1992), would be to generate training data for the code confidence model by running concept confidence models on out-of-sample data. We compare the performance of these approaches below. 5 Evaluation 5.1 Methodology We evaluated the proposed model using a corpus of 28,536 EHRs (individual clinical records), compiled to represent a wide variety of clinical contexts and supplied with ICD-10-PCS codes by trained medical coders. The corpus was annotated under the auspices of 3M Health Information Systems for the express purpose of developing autocoding technology for ICD-10. There was a total of 51,082 </context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked Generalization. Neural Networks, 5:241–259.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
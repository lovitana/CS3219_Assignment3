<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.996668">
Abu-MaTran at WMT 2014 Translation Task:
Two-step Data Selection and RBMT-Style Synthetic Rules
</title>
<author confidence="0.917444">
Raphael Rubino?, Antonio Toral†, Victor M. S´anchez-Cartagena?‡,
Jorge Ferr´andez-Tordera?, Sergio Ortiz-Rojas?, Gema Ramirez-S´anchez?,
Felipe S´anchez-Martinez‡, Andy Way†
</author>
<affiliation confidence="0.752177">
? Prompsit Language Engineering, S.L., Elche, Spain
</affiliation>
<email confidence="0.938397">
{rrubino,vmsanchez,jferrandez,sortiz,gramirez}@prompsit.com
</email>
<address confidence="0.534991">
† NCLT, School of Computing, Dublin City University, Ireland
</address>
<email confidence="0.941514">
{atoral,away}@computing.dcu.ie
</email>
<author confidence="0.359566">
‡ Dep. Llenguatges i Sistemes Inform`atics, Universitat d’Alacant, Spain
</author>
<email confidence="0.920812">
fsanchez@dlsi.ua.es
</email>
<sectionHeader confidence="0.990624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954111111111">
This paper presents the machine trans-
lation systems submitted by the Abu-
MaTran project to the WMT 2014 trans-
lation task. The language pair concerned
is English–French with a focus on French
as the target language. The French to En-
glish translation direction is also consid-
ered, based on the word alignment com-
puted in the other direction. Large lan-
guage and translation models are built us-
ing all the datasets provided by the shared
task organisers, as well as the monolin-
gual data from LDC. To build the trans-
lation models, we apply a two-step data
selection method based on bilingual cross-
entropy difference and vocabulary satura-
tion, considering each parallel corpus in-
dividually. Synthetic translation rules are
extracted from the development sets and
used to train another translation model.
We then interpolate the translation mod-
els, minimising the perplexity on the de-
velopment sets, to obtain our final SMT
system. Our submission for the English to
French translation task was ranked second
amongst nine teams and a total of twenty
submissions.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986534883721">
This paper presents the systems submitted by the
Abu-MaTran project (runs named DCU-Prompsit-
UA) to the WMT 2014 translation task for the
English–French language pair. Phrase-based sta-
tistical machine translation (SMT) systems were
submitted, considering the two translation direc-
tions, with the focus on the English to French di-
rection. Language models (LMs) and translation
models (TMs) are trained using all the data pro-
vided by the shared task organisers, as well as
the Gigaword monolingual corpora distributed by
LDC.
To train the LMs, monolingual corpora and the
target side of the parallel corpora are first used
individually to train models. Then the individ-
ual models are interpolated according to perplex-
ity minimisation on the development sets.
To train the TMs, first a baseline is built us-
ing the News Commentary parallel corpus. Sec-
ond, each remaining parallel corpus is processed
individually using bilingual cross-entropy differ-
ence (Axelrod et al., 2011) in order to sepa-
rate pseudo in-domain and out-of-domain sen-
tence pairs, and filtering the pseudo out-of-
domain instances with the vocabulary saturation
approach (Lewis and Eetemadi, 2013). Third,
synthetic translation rules are automatically ex-
tracted from the development set and used to train
another translation model following a novel ap-
proach (S´anchez-Cartagena et al., 2014). Finally,
we interpolate the four translation models (base-
line, in-domain, filtered out-of-domain and rules)
by minimising the perplexity obtained on the de-
velopment sets and investigate the best tuning and
decoding parameters.
The reminder of this paper is organised as fol-
lows: the datasets and tools used in our experi-
ments are described in Section 2. Then, details
about the LMs and TMs are given in Section 3 and
Section 4 respectively. Finally, we evaluate the
performance of the final SMT system according to
different tuning and decoding parameters in Sec-
tion 5 before presenting conclusions in Section 6.
</bodyText>
<page confidence="0.977765">
171
</page>
<affiliation confidence="0.36295">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171–177,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<sectionHeader confidence="0.865035" genericHeader="introduction">
2 Datasets and Tools
</sectionHeader>
<bodyText confidence="0.999971434782609">
We use all the monolingual and parallel datasets
in English and French provided by the shared task
organisers, as well as the LDC Gigaword for the
same languages1. For each language, a true-case
model is trained using all the data, using the train-
truecaser.perl script included in the MOSES tool-
kit (Koehn et al., 2007).
Punctuation marks of all the monolingual and
parallel corpora are then normalised using the
script normalize-punctuation.perl provided by the
organisers, before being tokenised and true-cased
using the scripts distributed with the MOSES tool-
kit. The same pre-processing steps are applied to
the development and test sets. As development
sets, we used all the test sets from previous years
of WMT, from 2008 to 2013 (newstest2008-2013).
Finally, the training parallel corpora are cleaned
using the script clean-corpus-n.perl, keeping the
sentences longer than 1 word, shorter than 80
words, and with a length ratio between sentence
pairs lower than 4.2 The statistics about the cor-
pora used in our experiments after pre-processing
are presented in Table 1.
For training LMs we use KENLM (Heafield et
al., 2013) and the SRILM tool-kit (Stolcke et al.,
2011). For training TMs, we use MOSES (Koehn
et al., 2007) version 2.1 with MGIZA++ (Och and
Ney, 2003; Gao and Vogel, 2008). These tools are
used with default parameters for our experiments
except when explicitly said.
The decoder used to generate translations is
MOSES using features weights optimised with
MERT (Och, 2003). As our approach relies on
training individual TMs, one for each parallel cor-
pus, our final TM is obtained by linearly interpo-
lating the individual ones. The interpolation of
TMs is performed using the script tmcombine.py,
minimising the cross-entropy between the TM
and the concatenated development sets from 2008
to 2012 (noted newstest2008-2012), as described
in Sennrich (2012). Finally, we make use of the
findings from WMT 2013 brought by the win-
ning team (Durrani et al., 2013) and decide to use
the Operation Sequence Model (OSM), based on
minimal translation units and Markov chains over
sequences of operations, implemented in MOSES
</bodyText>
<footnote confidence="0.99784725">
1LDC2011T07 English Gigaword Fifth Edition,
LDC2011T10 French Gigaword Third Edition
2This ratio was empirically chosen based on words fertil-
ity between English and French.
</footnote>
<table confidence="0.999893024390244">
Corpus Sentences (k) Words (M)
Monolingual Data – English
Europarl v7 2,218.2 59.9
News Commentary v8 304.2 7.4
News Shuffled 2007 3,782.5 90.2
News Shuffled 2008 12,954.5 308.1
News Shuffled 2009 14,680.0 347.0
News Shuffled 2010 6,797.2 157.8
News Shuffled 2011 15,437.7 358.1
News Shuffled 2012 14,869.7 345.5
News Shuffled 2013 21,688.4 495.2
LDC afp 7,184.9 869.5
LDC apw 8,829.4 1,426.7
LDC cna 618.4 45.7
LDC ltw 986.9 321.1
LDC nyt 5,327.7 1,723.9
LDC wpb 108.8 20.8
LDC xin 5,121.9 423.7
Monolingual Data – French
Europarl v7 2,190.6 63.5
News Commentary v8 227.0 6.5
News Shuffled 2007 119.0 2.7
News Shuffled 2008 4,718.8 110.3
News Shuffled 2009 4,366.7 105.3
News Shuffled 2010 1,846.5 44.8
News Shuffled 2011 6,030.1 146.1
News Shuffled 2012 4,114.4 100.8
News Shuffled 2013 9,256.3 220.2
LDC afp 6,793.5 784.5
LDC apw 2,525.1 271.3
Parallel Data
109 Corpus 21,327.1 549.0 (EN)
642.5 (FR)
Common Crawl 3,168.5 76.0 (EN)
82.7 (FR)
Europarl v7 1,965.5 52.5 (EN)
56.7 (FR)
News Commentary v9 181.3 4.5 (EN)
5.3 (FR)
UN 12,354.7 313.4 (EN)
356.5 (FR)
</table>
<tableCaption confidence="0.703699666666667">
Table 1: Data statistics after pre-processing of the
monolingual and parallel corpora used in our ex-
periments.
</tableCaption>
<bodyText confidence="0.995564">
and introduced by Durrani et al. (2011).
</bodyText>
<sectionHeader confidence="0.99416" genericHeader="method">
3 Language Models
</sectionHeader>
<bodyText confidence="0.999986916666667">
The LMs are trained in the same way for both
languages. First, each monolingual and parallel
corpus is considered individually (except the par-
allel version of Europarl and News Commentary)
and used to train a 5-gram LM with the modified
Kneser-Ney smoothing method. We then interpo-
late the individual LMs using the script compute-
best-mix available with the SRILM tool-kit (Stol-
cke et al., 2011), based on their perplexity scores
on the concatenation of the development sets from
2008 to 2012 (the 2013 version is held-out for the
tuning of the TMs).
</bodyText>
<page confidence="0.989252">
172
</page>
<bodyText confidence="0.9676275">
The final LM for French contains all the word
sequences from 1 to 5-grams contained in the
training corpora without any pruning. However,
with the computing resources at our disposal, the
English LMs could not be interpolated without
pruning non-frequent n-grams. Thus, n-grams
with n ∈ [3; 5] with a frequency lower than 2 were
removed. Details about the final LMs are given in
</bodyText>
<tableCaption confidence="0.98122">
Table 2.
</tableCaption>
<table confidence="0.966148666666667">
1-gram 2-gram 3-gram 4-gram 5-gram
English 13.4 198.6 381.2 776.3 1,068.7
French 6.0 75.5 353.2 850.8 1,354.0
</table>
<tableCaption confidence="0.9799615">
Table 2: Statistics, in millions of n-grams, of the
interpolated LMs.
</tableCaption>
<sectionHeader confidence="0.991727" genericHeader="method">
4 Translation Models
</sectionHeader>
<bodyText confidence="0.99993675">
In this Section, we describe the TMs trained for
the shared task. First, we present the two-step data
selection process which aims to (i) separate in and
out-of-domain parallel sentences and (ii) reduce
the total amount of out-of-domain data. Second,
a novel approach for the automatic extraction of
translation rules and their use to enrich the phrase
table is detailed.
</bodyText>
<subsectionHeader confidence="0.984085">
4.1 Parallel Data Filtering and Vocabulary
Saturation
</subsectionHeader>
<bodyText confidence="0.999988">
Amongst the parallel corpora provided by the
shared task organisers, only News Commentary
can be considered as in-domain regarding the de-
velopment and test sets. We use this training
corpus to build our baseline SMT system. The
other parallel corpora are individually filtered us-
ing bilingual cross-entropy difference (Moore and
Lewis, 2010; Axelrod et al., 2011). This data
filtering method relies on four LMs, two in the
source and two in the target language, which
aim to model particular features of in and out-of-
domain sentences.
We build the in-domain LMs using the source
and target sides of the News Commentary paral-
lel corpus. Out-of-domain LMs are trained on a
vocabulary-constrained subset of each remaining
parallel corpus individually using the SRILM tool-
kit, which leads to eight models (four in the source
language and four in the target language).3
</bodyText>
<footnote confidence="0.750848">
3The subsets contain the same number of sentences and
the same vocabulary as News Commentary.
</footnote>
<bodyText confidence="0.999680333333333">
Then, for each out-of-domain parallel corpus,
we compute the bilingual cross-entropy difference
of each sentence pair as:
</bodyText>
<equation confidence="0.988863">
[Hi.(Ssrc) − Hout(Ssrc)] + [Hi.(Strg) − Hout(Strg)] (1)
</equation>
<bodyText confidence="0.99335905">
where Ssrc and Strg are the source and the tar-
get sides of a sentence pair, Hin and Hout are
the cross-entropies of the in and out-of-domain
LMs given a sentence pair. The sentence pairs are
then ranked and the lowest-scoring ones are taken
to train the pseudo in-domain TMs. However,
the cross-entropy difference threshold required to
split a corpus in two parts (pseudo in and out-of-
domain) is usually set empirically by testing sev-
eral subset sizes of the top-ranked sentence pairs.
This method is costly in our setup as it would lead
to training and evaluating multiple SMT systems
for each of the pseudo in-domain parallel corpora.
In order to save time and computing power,
we consider only pseudo in-domain sentence pairs
those with a bilingual cross-entropy difference be-
low 0, i.e. those deemed more similar to the
in-domain LMs than to the out-of-domain LMs
(Hin &lt; Hout). A sample of the distribution of
scores for the out-of-domain corpora is shown in
</bodyText>
<figureCaption confidence="0.917984">
Figure 1. The resulting pseudo in-domain corpora
are used to train individual TMs, as detailed in Ta-
ble 3.
</figureCaption>
<figure confidence="0.9784895">
0 2k 4k 6k 8k 10k
Sentence Pairs
</figure>
<figureCaption confidence="0.958984333333333">
Figure 1: Sample of ranked sentence-pairs (10k)
of each of the out-of-domain parallel corpora with
bilingual cross-entropy difference
</figureCaption>
<bodyText confidence="0.999917">
The results obtained using the pseudo in-
domain data show BLEU (Papineni et al., 2002)
scores superior or equal to the baseline score.
Only the Europarl subset is slightly lower than
the baseline, while the subset taken from the 109
corpus reaches the highest BLEU compared to the
other systems (30.29). This is mainly due to the
</bodyText>
<figure confidence="0.997492230769231">
Bilingual Cross-Entropy Difference
10
-2
-4
4
2
6
0
8
Common Crawl
Europarl
10^9
UN
</figure>
<page confidence="0.995093">
173
</page>
<bodyText confidence="0.999767333333333">
size of this subset which is ten times larger than
the one taken from Europarl. The last row of Ta-
ble 3 shows the BLEU score obtained after interpo-
lating the four pseudo in-domain translation mod-
els. This system outperforms the best pseudo in-
domain one by 0.5 absolute points.
</bodyText>
<table confidence="0.982873">
Corpus Sentences (k) BLEUde„
Baseline 181.3 27.76
Common Crawl 208.3 27.73
Europarl 142.0 27.63
109 Corpus 1,442.4 30.29
UN 642.4 28.91
Interpolation - 30.78
</table>
<tableCaption confidence="0.99639">
Table 3: Number of sentence pairs and BLEU
</tableCaption>
<bodyText confidence="0.993296764705883">
scores reported by MERT on English–French new-
stest2013 for the pseudo in-domain corpora ob-
tained by filtering the out-of-domain corpora with
bilingual cross-entropy difference. The interpola-
tion of pseudo in-domain models is evaluated in
the last row.
After evaluating the pseudo in-domain parallel
data, the remaining sentence pairs for each cor-
pora are considered out-of-domain according to
our filtering approach. However, they may still
contain useful information, thus we make use of
these corpora by building individual TMs for each
corpus (in a similar way we built the pseudo in-
domain models). The total amount of remaining
data (more than 33 million sentence pairs) makes
the training process costly in terms of time and
computing power. In order to reduce these costs,
sentence pairs with a bilingual cross-entropy dif-
ference higher than 10 were filtered out, as we no-
ticed that most of the sentences above this thresh-
old contain noise (non-alphanumeric characters,
foreign languages, etc.).
We also limit the size of the remaining data by
applying the vocabulary saturation method (Lewis
and Eetemadi, 2013). For the out-of-domain sub-
set of each corpus, we traverse the sentence pairs
in the order they are ranked by perplexity differ-
ence and filter out those sentence pairs for which
we have seen already each 1-gram at least 10
times. Each out-of-domain subset from each par-
allel corpus is then used to train a TM before inter-
polating them to create the pseudo out-of-domain
TM. The results reported by MERT obtained on
the newstest2013 development set are detailed in
</bodyText>
<tableCaption confidence="0.5047675">
Table 4.
Mainly due to the sizes of the pseudo out-of-
</tableCaption>
<table confidence="0.999520857142857">
Corpus Sentences (k) BLEUde„
Baseline 181.3 27.76
Common Crawl 1,598.7 29.84
Europarl 461.9 28.87
109 Corpus 5,153.0 30.50
UN 1,707.3 29.03
Interpolation - 31.37
</table>
<tableCaption confidence="0.986012">
Table 4: Number of sentence pairs and BLEU
</tableCaption>
<bodyText confidence="0.98860004">
scores reported by MERT on English–French
newstest2013 for the pseudo out-of-domain cor-
pora obtained by filtering the out-of-domain cor-
pora with bilingual cross-entropy difference, keep-
ing sentence pairs below an entropy score of 10
and applying vocabulary saturation. The interpo-
lation of pseudo out-of-domain models is evalu-
ated in the last row.
domain subsets, the reported BLEU scores are
higher than the baseline for the four individual
SMT systems and the interpolated one. This latter
system outperforms the baseline by 3.61 absolute
points. Compared to the results obtained with the
pseudo in-domain data, we observe a slight im-
provement of the BLEU scores using the pseudo
out-of-domain data. However, despite the com-
paratively larger sizes of the latter datasets, the
BLEU scores reached are not that higher. For in-
stance with the 109 corpus, the pseudo in and out-
of-domain subsets contain 1.4 and 5.1 million sen-
tence pairs respectively, and the two systems reach
30.3 and 30.5 BLEU. These scores indicate that
the pseudo in-domain SMT systems are more ef-
ficient on the English–French newstest2013 devel-
opment set.
</bodyText>
<subsectionHeader confidence="0.999123">
4.2 Extraction of Translation Rules
</subsectionHeader>
<bodyText confidence="0.999769466666667">
A synthetic phrase-table based on shallow-transfer
MT rules and dictionaries is built as follows. First,
a set of shallow-transfer rules is inferred from the
concatenation of the newstest2008-2012 develop-
ment corpora exactly in the same way as in the
UA-Prompsit submission to this translation shared
task (S´anchez-Cartagena et al., 2014). In sum-
mary, rules are obtained from a set of bilingual
phrases extracted from the parallel corpus after
its morphological analysis and part-of-speech dis-
ambiguation with the tools in the Apertium rule-
based MT platform (Forcada et al., 2011).
The extraction algorithm commonly used in
phrase-based SMT is followed with some added
heuristics which ensure that the bilingual phrases
</bodyText>
<page confidence="0.996532">
174
</page>
<bodyText confidence="0.999762037037037">
extracted are compatible with the bilingual dic-
tionary. Then, many different rules are generated
from each bilingual phrase; each of them encodes
a different degree of generalisation over the partic-
ular example it has been extracted from. Finally,
the minimum set of rules which correctly repro-
duces all the bilingual phrases is found based on
integer linear programming search (Garfinkel and
Nemhauser, 1972).
Once the rules have been inferred, the phrase
table is built from them and the original rule-
based MT dictionaries, following the method
by S´anchez-Cartagena et al. (2011), which was
one of winning systems4 (together with two on-
line SMT systems) in the pairwise manual evalu-
ation of the WMT11 English–Spanish translation
task (Callison-Burch et al., 2011). This phrase-
table is then interpolated with the baseline TM and
the results are presented in Table 5. A slight im-
provement over the baseline is observed, which
motivates the use of synthetic rules in our final MT
system. This small improvement may be related
to the small coverage of the Apertium dictionar-
ies: the English–French bilingual dictionary has a
low number of entries compared to more mature
language pairs in Apertium which have around 20
times more bilingual entries.
</bodyText>
<subsectionHeader confidence="0.388547333333333">
System BLEUdev
Baseline 27.76
Baseline+Rules 28.06
</subsectionHeader>
<bodyText confidence="0.727442">
Table 5: BLEU scores reported by MERT on
English–French newstest2013 for the baseline
SMT system standalone and with automatically
extracted translation rules.
</bodyText>
<sectionHeader confidence="0.875727" genericHeader="evaluation">
5 Tuning and Decoding
</sectionHeader>
<bodyText confidence="0.999904818181818">
We present in this Section a short selection of our
experiments, amongst 15+ different configura-
tions, conducted on the interpolation of TMs, tun-
ing and decoding parameters. We first interpolate
the four TMs: the baseline, the pseudo in and out-
of-domain, and the translation rules, minimising
the perplexity obtained on the concatenated de-
velopment sets from 2008 to 2012 (newstest2008-
2012). We investigate the use of OSM trained on
pseudo in-domain data only or using all the paral-
lel data available. Finally, we make variations of
</bodyText>
<footnote confidence="0.5915815">
4No other system was found statistically significantly bet-
ter using the sign test at p ≤ 0.1.
</footnote>
<bodyText confidence="0.9959430625">
the number of n-bests used by MERT.
Results obtained on the development set new-
stest2013 are reported in Table 6. These scores
show that adding OSM to the interpolated trans-
lation models slightly degrades BLEU. However,
by increasing the number of n-bests considered by
MERT to 200-bests, the SMT system with OSM
outperforms the systems evaluated previously in
our experiments. Adding the synthetic translation
rules degrades BLEU (as indicated by the last row
in the Table), thus we decide to submit two sys-
tems to the shared task: one without and one with
synthetic rules. By submitting a system without
synthetic rules, we also ensure that our SMT sys-
tem is constrained according to the shared task
guidelines.
</bodyText>
<table confidence="0.998246">
System BLEUdev
Baseline 27.76
+ pseudo in + pseudo out 31.93
+ OSM 31.90
+ MERT 200-best 32.21
+ Rules 32.10
</table>
<tableCaption confidence="0.9064935">
Table 6: BLEU scores reported by MERT on
English–French newstest2013 development set.
</tableCaption>
<bodyText confidence="0.999697076923077">
As MERT is not suitable when a large number
of features are used (our system uses 19 fetures),
we switch to the Margin Infused Relaxed Algo-
rithm (MIRA) for our submitted systems (Watan-
abe et al., 2007). The development set used is
newstest2012, as we aim to select the best decod-
ing parameters according to the scores obtained
when decoding the newstest2013 corpus, after de-
truecasing and de-tokenising using the scripts dis-
tributed with MOSES. This setup allowed us to
compare our results with the participants of the
translation shared task last year. We pick the de-
coding parameters leading to the best results in
terms of BLEU and decode the official test set of
WMT14 newstest2014. The results are reported in
Table 7. Results on newstest2013 show that the de-
coding parameters investigation leads to an over-
all improvement of 0.1 BLEU absolute. The re-
sults on newstest2014 show that adding synthetic
rules did not help improving BLEU and degraded
slightly TER (Snover et al., 2006) scores.
In addition to our English→French submission,
we submitted a French→English translation. Our
French→English MT system is built on the align-
ments obtained from the English→French direc-
tion. The training processes between the two sys-
</bodyText>
<page confidence="0.990048">
175
</page>
<figure confidence="0.9785616">
System
newstest2013
Best tuning
cube-pruning (pop-limit 10000)
increased table-limit (100)
monotonic reordering
Best decoding
newstest2014
Best decoding
Best decoding + Rules
</figure>
<tableCaption confidence="0.931103">
Table 7: Case sensitive results obtained with
</tableCaption>
<bodyText confidence="0.972375666666667">
our final English–French SMT system on new-
stest2013 when experimenting with different de-
coding parameters. The best parameters are kept
to translate the WMT14 test set (newstest2014)
and official results are reported in the last two
rows.
tems are identical, except for the synthetic rules
which are not extracted for the French→English
direction. Tuning and decoding parameters for
this latter translation direction are the best ones
obtained in our previous experiments on this
shared task. The case-sensitive scores obtained
for French→English on newstest2014 are 35.0
BLEU13A and 53.1 TER, which ranks us at the
fifth position for this translation direction.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99994664">
We have presented the MT systems developed by
the Abu-MaTran project for the WMT14 trans-
lation shared task. We focused on the French–
English language pair and particularly on the
English→French direction. We have used a two-
step data selection process based on bilingual
cross-entropy difference and vocabulary satura-
tion, as well as a novel approach for the extraction
of synthetic translation rules and their use to en-
rich the phrase table. For the LMs and the TMs,
we rely on training individual models per corpus
before interpolating them by minimising perplex-
ity according to the development set. Finally, we
made use of the findings of WMT13 by including
an OSM model.
Our English→French translation system was
ranked second amongst nine teams and a total of
twenty submissions, while our French→English
submission was ranked fifth. As future work,
we plan to investigate the effect of adding to the
phrase table synthetic translation rules based on
larger dictionaries. We also would like to study the
link between OSM and the different decoding pa-
rameters implemented in MOSES, as we observed
inconsistent results in our experiments.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995236">
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran).
</bodyText>
<sectionHeader confidence="0.998508" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998251">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain Adaptation Via Pseudo In-domain
Data Selection. In Proceedings of EMNLP, pages
355–362.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceed-
ings of WMT, pages 22–64.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A Joint Sequence Translation Model with In-
tegrated Reordering. In Proceedings of ACL/HLT,
pages 1045–1054.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh’s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of WMT, pages 112–119.
Mikel L Forcada, Mireia Ginestf-Rosell, Jacob Nord-
falk, Jim O’Regan, Sergio Ortiz-Rojas, Juan An-
tonio P´erez-Ortiz, Felipe S´anchez-Martfnez, Gema
Ramfrez-S´anchez, and Francis M Tyers. 2011.
Apertium: A Free/Open-source Platform for Rule-
based Machine Translation. Machine Translation,
25(2):127–144.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49–57.
Robert S Garfinkel and George L Nemhauser. 1972.
Integer Programming, volume 4. Wiley New York.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Mod-
ified Kneser-Ney Language Model Estimation. In
Proceedings ofACL, pages 690–696.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings ofACL, Interactive Poster and Demonstra-
tion Sessions, pages 177–180.
</reference>
<figure confidence="0.97833125">
BLEU13A TER
31.02 60.77
31.04 60.71
31.06 60.77
31.07 60.69
31.14 60.66
34.90 54.70
34.90 54.80
</figure>
<page confidence="0.985806">
176
</page>
<reference confidence="0.99981080952381">
William D. Lewis and Sauleh Eetemadi. 2013. Dra-
matically Reducing Training Data Size Through Vo-
cabulary Saturation. In Proceedings of WMT, pages
281–291.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In Pro-
ceedings of ACL, pages 220–224.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19–51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
ofACL, volume 1, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, pages 311–318.
Vfctor M. S´anchez-Cartagena, Felipe S´anchez-
Martfnez, and Juan Antonio P´erez-Ortiz. 2011. In-
tegrating Shallow-transfer Rules into Phrase-based
Statistical Machine Translation. In Proceedings of
MT Summit XIII, pages 562–569.
Vfctor M. S´anchez-Cartagena, Juan Antonio P´erez-
Ortiz, and Felipe S´anchez-Martfnez. 2014. The
UA-Prompsit Hybrid Machine Translation System
for the 2014 Workshop on Statistical Machine
Translation. In Proceedings of WMT.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statisti-
cal Machine Translation. In Proceedings of EACL,
pages 539–549.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of AMTA, pages 223–231.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at Sixteen: Update and Out-
look. In Proceedings of ASRU.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online Large-margin Train-
ing for Statistical Machine Translation. In Proceed-
ings of EMNLP.
</reference>
<page confidence="0.997725">
177
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.466874">
<title confidence="0.991502">Abu-MaTran at WMT 2014 Translation Two-step Data Selection and RBMT-Style Synthetic Rules</title>
<author confidence="0.982903333333333">Antonio Victor M Sergio Gema Andy</author>
<affiliation confidence="0.846677666666667">Language Engineering, S.L., Elche, School of Computing, Dublin City University, Llenguatges i Sistemes Inform`atics, Universitat d’Alacant,</affiliation>
<email confidence="0.970138">fsanchez@dlsi.ua.es</email>
<abstract confidence="0.998591892857143">This paper presents the machine translation systems submitted by the Abu- MaTran project to the WMT 2014 translation task. The language pair concerned is English–French with a focus on French as the target language. The French to English translation direction is also considered, based on the word alignment computed in the other direction. Large language and translation models are built using all the datasets provided by the shared task organisers, as well as the monolingual data from LDC. To build the translation models, we apply a two-step data selection method based on bilingual crossentropy difference and vocabulary saturation, considering each parallel corpus individually. Synthetic translation rules are extracted from the development sets and used to train another translation model. We then interpolate the translation models, minimising the perplexity on the development sets, to obtain our final SMT system. Our submission for the English to French translation task was ranked second amongst nine teams and a total of twenty submissions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain Adaptation Via Pseudo In-domain Data Selection.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="2616" citStr="Axelrod et al., 2011" startWordPosition="381" endWordPosition="384">ls (LMs) and translation models (TMs) are trained using all the data provided by the shared task organisers, as well as the Gigaword monolingual corpora distributed by LDC. To train the LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding pa</context>
<context position="9290" citStr="Axelrod et al., 2011" startWordPosition="1443" endWordPosition="1446">parallel sentences and (ii) reduce the total amount of out-of-domain data. Second, a novel approach for the automatic extraction of translation rules and their use to enrich the phrase table is detailed. 4.1 Parallel Data Filtering and Vocabulary Saturation Amongst the parallel corpora provided by the shared task organisers, only News Commentary can be considered as in-domain regarding the development and test sets. We use this training corpus to build our baseline SMT system. The other parallel corpora are individually filtered using bilingual cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011). This data filtering method relies on four LMs, two in the source and two in the target language, which aim to model particular features of in and out-ofdomain sentences. We build the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 3The subsets contain the same number of sentences and the same vocabulary as News Commentary. Then</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain Adaptation Via Pseudo In-domain Data Selection. In Proceedings of EMNLP, pages 355–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>22--64</pages>
<contexts>
<context position="16689" citStr="Callison-Burch et al., 2011" startWordPosition="2643" endWordPosition="2646">encodes a different degree of generalisation over the particular example it has been extracted from. Finally, the minimum set of rules which correctly reproduces all the bilingual phrases is found based on integer linear programming search (Garfinkel and Nemhauser, 1972). Once the rules have been inferred, the phrase table is built from them and the original rulebased MT dictionaries, following the method by S´anchez-Cartagena et al. (2011), which was one of winning systems4 (together with two online SMT systems) in the pairwise manual evaluation of the WMT11 English–Spanish translation task (Callison-Burch et al., 2011). This phrasetable is then interpolated with the baseline TM and the results are presented in Table 5. A slight improvement over the baseline is observed, which motivates the use of synthetic rules in our final MT system. This small improvement may be related to the small coverage of the Apertium dictionaries: the English–French bilingual dictionary has a low number of entries compared to more mature language pairs in Apertium which have around 20 times more bilingual entries. System BLEUdev Baseline 27.76 Baseline+Rules 28.06 Table 5: BLEU scores reported by MERT on English–French newstest201</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of WMT, pages 22–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A Joint Sequence Translation Model with Integrated Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>1045--1054</pages>
<contexts>
<context position="7338" citStr="Durrani et al. (2011)" startWordPosition="1127" endWordPosition="1130"> 2007 119.0 2.7 News Shuffled 2008 4,718.8 110.3 News Shuffled 2009 4,366.7 105.3 News Shuffled 2010 1,846.5 44.8 News Shuffled 2011 6,030.1 146.1 News Shuffled 2012 4,114.4 100.8 News Shuffled 2013 9,256.3 220.2 LDC afp 6,793.5 784.5 LDC apw 2,525.1 271.3 Parallel Data 109 Corpus 21,327.1 549.0 (EN) 642.5 (FR) Common Crawl 3,168.5 76.0 (EN) 82.7 (FR) Europarl v7 1,965.5 52.5 (EN) 56.7 (FR) News Commentary v9 181.3 4.5 (EN) 5.3 (FR) UN 12,354.7 313.4 (EN) 356.5 (FR) Table 1: Data statistics after pre-processing of the monolingual and parallel corpora used in our experiments. and introduced by Durrani et al. (2011). 3 Language Models The LMs are trained in the same way for both languages. First, each monolingual and parallel corpus is considered individually (except the parallel version of Europarl and News Commentary) and used to train a 5-gram LM with the modified Kneser-Ney smoothing method. We then interpolate the individual LMs using the script computebest-mix available with the SRILM tool-kit (Stolcke et al., 2011), based on their perplexity scores on the concatenation of the development sets from 2008 to 2012 (the 2013 version is held-out for the tuning of the TMs). 172 The final LM for French co</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A Joint Sequence Translation Model with Integrated Reordering. In Proceedings of ACL/HLT, pages 1045–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Machine Translation Systems for European Language Pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>112--119</pages>
<contexts>
<context position="5795" citStr="Durrani et al., 2013" startWordPosition="881" endWordPosition="884">s except when explicitly said. The decoder used to generate translations is MOSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in MOSES 1LDC2011T07 English Gigaword Fifth Edition, LDC2011T10 French Gigaword Third Edition 2This ratio was empirically chosen based on words fertility between English and French. Corpus Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 59.9 News Commentary v8 304.2 7.4 News Shuffled 2007 3,782.5 90.2 News Shuffled 2008 12,954.5 308.1 News Shuffled 2009 14,680.0 347.0 News Shuffled 2010 6,797.2 157.8 News Shuffled 2011 15,4</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn. 2013. Edinburgh’s Machine Translation Systems for European Language Pairs. In Proceedings of WMT, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikel L Forcada</author>
<author>Mireia Ginestf-Rosell</author>
<author>Jacob Nordfalk</author>
<author>Jim O’Regan</author>
<author>Sergio Ortiz-Rojas</author>
<author>Juan Antonio P´erez-Ortiz</author>
<author>Felipe S´anchez-Martfnez</author>
<author>Gema Ramfrez-S´anchez</author>
<author>Francis M Tyers</author>
</authors>
<title>Apertium: A Free/Open-source Platform for Rulebased Machine Translation.</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>2</issue>
<marker>Forcada, Ginestf-Rosell, Nordfalk, O’Regan, Ortiz-Rojas, P´erez-Ortiz, S´anchez-Martfnez, Ramfrez-S´anchez, Tyers, 2011</marker>
<rawString>Mikel L Forcada, Mireia Ginestf-Rosell, Jacob Nordfalk, Jim O’Regan, Sergio Ortiz-Rojas, Juan Antonio P´erez-Ortiz, Felipe S´anchez-Martfnez, Gema Ramfrez-S´anchez, and Francis M Tyers. 2011. Apertium: A Free/Open-source Platform for Rulebased Machine Translation. Machine Translation, 25(2):127–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel Implementations of Word Alignment Tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="5109" citStr="Gao and Vogel, 2008" startWordPosition="773" endWordPosition="776"> the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use KENLM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use MOSES (Koehn et al., 2007) version 2.1 with MGIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is MOSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we mak</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel Implementations of Word Alignment Tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert S Garfinkel</author>
<author>George L Nemhauser</author>
</authors>
<date>1972</date>
<booktitle>Integer Programming,</booktitle>
<volume>4</volume>
<publisher>Wiley</publisher>
<location>New York.</location>
<contexts>
<context position="16332" citStr="Garfinkel and Nemhauser, 1972" startWordPosition="2586" endWordPosition="2589">n with the tools in the Apertium rulebased MT platform (Forcada et al., 2011). The extraction algorithm commonly used in phrase-based SMT is followed with some added heuristics which ensure that the bilingual phrases 174 extracted are compatible with the bilingual dictionary. Then, many different rules are generated from each bilingual phrase; each of them encodes a different degree of generalisation over the particular example it has been extracted from. Finally, the minimum set of rules which correctly reproduces all the bilingual phrases is found based on integer linear programming search (Garfinkel and Nemhauser, 1972). Once the rules have been inferred, the phrase table is built from them and the original rulebased MT dictionaries, following the method by S´anchez-Cartagena et al. (2011), which was one of winning systems4 (together with two online SMT systems) in the pairwise manual evaluation of the WMT11 English–Spanish translation task (Callison-Burch et al., 2011). This phrasetable is then interpolated with the baseline TM and the results are presented in Table 5. A slight improvement over the baseline is observed, which motivates the use of synthetic rules in our final MT system. This small improvemen</context>
</contexts>
<marker>Garfinkel, Nemhauser, 1972</marker>
<rawString>Robert S Garfinkel and George L Nemhauser. 1972. Integer Programming, volume 4. Wiley New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable Modified Kneser-Ney Language Model Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>690--696</pages>
<contexts>
<context position="4944" citStr="Heafield et al., 2013" startWordPosition="743" endWordPosition="746">ased using the scripts distributed with the MOSES toolkit. The same pre-processing steps are applied to the development and test sets. As development sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use KENLM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use MOSES (Koehn et al., 2007) version 2.1 with MGIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is MOSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable Modified Kneser-Ney Language Model Estimation. In Proceedings ofACL, pages 690–696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL, Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="4133" citStr="Koehn et al., 2007" startWordPosition="618" endWordPosition="621">different tuning and decoding parameters in Section 5 before presenting conclusions in Section 6. 171 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 171–177, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2 Datasets and Tools We use all the monolingual and parallel datasets in English and French provided by the shared task organisers, as well as the LDC Gigaword for the same languages1. For each language, a true-case model is trained using all the data, using the traintruecaser.perl script included in the MOSES toolkit (Koehn et al., 2007). Punctuation marks of all the monolingual and parallel corpora are then normalised using the script normalize-punctuation.perl provided by the organisers, before being tokenised and true-cased using the scripts distributed with the MOSES toolkit. The same pre-processing steps are applied to the development and test sets. As development sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings ofACL, Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William D Lewis</author>
<author>Sauleh Eetemadi</author>
</authors>
<title>Dramatically Reducing Training Data Size Through Vocabulary Saturation.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>281--291</pages>
<contexts>
<context position="2803" citStr="Lewis and Eetemadi, 2013" startWordPosition="409" endWordPosition="412">he LMs, monolingual corpora and the target side of the parallel corpora are first used individually to train models. Then the individual models are interpolated according to perplexity minimisation on the development sets. To train the TMs, first a baseline is built using the News Commentary parallel corpus. Second, each remaining parallel corpus is processed individually using bilingual cross-entropy difference (Axelrod et al., 2011) in order to separate pseudo in-domain and out-of-domain sentence pairs, and filtering the pseudo out-ofdomain instances with the vocabulary saturation approach (Lewis and Eetemadi, 2013). Third, synthetic translation rules are automatically extracted from the development set and used to train another translation model following a novel approach (S´anchez-Cartagena et al., 2014). Finally, we interpolate the four translation models (baseline, in-domain, filtered out-of-domain and rules) by minimising the perplexity obtained on the development sets and investigate the best tuning and decoding parameters. The reminder of this paper is organised as follows: the datasets and tools used in our experiments are described in Section 2. Then, details about the LMs and TMs are given in S</context>
<context position="13314" citStr="Lewis and Eetemadi, 2013" startWordPosition="2106" endWordPosition="2109">e corpora by building individual TMs for each corpus (in a similar way we built the pseudo indomain models). The total amount of remaining data (more than 33 million sentence pairs) makes the training process costly in terms of time and computing power. In order to reduce these costs, sentence pairs with a bilingual cross-entropy difference higher than 10 were filtered out, as we noticed that most of the sentences above this threshold contain noise (non-alphanumeric characters, foreign languages, etc.). We also limit the size of the remaining data by applying the vocabulary saturation method (Lewis and Eetemadi, 2013). For the out-of-domain subset of each corpus, we traverse the sentence pairs in the order they are ranked by perplexity difference and filter out those sentence pairs for which we have seen already each 1-gram at least 10 times. Each out-of-domain subset from each parallel corpus is then used to train a TM before interpolating them to create the pseudo out-of-domain TM. The results reported by MERT obtained on the newstest2013 development set are detailed in Table 4. Mainly due to the sizes of the pseudo out-ofCorpus Sentences (k) BLEUde„ Baseline 181.3 27.76 Common Crawl 1,598.7 29.84 Europa</context>
</contexts>
<marker>Lewis, Eetemadi, 2013</marker>
<rawString>William D. Lewis and Sauleh Eetemadi. 2013. Dramatically Reducing Training Data Size Through Vocabulary Saturation. In Proceedings of WMT, pages 281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent Selection of Language Model Training Data.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>220--224</pages>
<contexts>
<context position="9267" citStr="Moore and Lewis, 2010" startWordPosition="1439" endWordPosition="1442">e in and out-of-domain parallel sentences and (ii) reduce the total amount of out-of-domain data. Second, a novel approach for the automatic extraction of translation rules and their use to enrich the phrase table is detailed. 4.1 Parallel Data Filtering and Vocabulary Saturation Amongst the parallel corpora provided by the shared task organisers, only News Commentary can be considered as in-domain regarding the development and test sets. We use this training corpus to build our baseline SMT system. The other parallel corpora are individually filtered using bilingual cross-entropy difference (Moore and Lewis, 2010; Axelrod et al., 2011). This data filtering method relies on four LMs, two in the source and two in the target language, which aim to model particular features of in and out-ofdomain sentences. We build the in-domain LMs using the source and target sides of the News Commentary parallel corpus. Out-of-domain LMs are trained on a vocabulary-constrained subset of each remaining parallel corpus individually using the SRILM toolkit, which leads to eight models (four in the source language and four in the target language).3 3The subsets contain the same number of sentences and the same vocabulary a</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent Selection of Language Model Training Data. In Proceedings of ACL, pages 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics,</title>
<date>2003</date>
<pages>29--19</pages>
<contexts>
<context position="5087" citStr="Och and Ney, 2003" startWordPosition="769" endWordPosition="772">t sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use KENLM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use MOSES (Koehn et al., 2007) version 2.1 with MGIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is MOSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<volume>1</volume>
<pages>160--167</pages>
<contexts>
<context position="5310" citStr="Och, 2003" startWordPosition="805" endWordPosition="806">, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use KENLM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use MOSES (Koehn et al., 2007) version 2.1 with MGIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is MOSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over se</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings ofACL, volume 1, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="11386" citStr="Papineni et al., 2002" startWordPosition="1790" endWordPosition="1793">udo in-domain sentence pairs those with a bilingual cross-entropy difference below 0, i.e. those deemed more similar to the in-domain LMs than to the out-of-domain LMs (Hin &lt; Hout). A sample of the distribution of scores for the out-of-domain corpora is shown in Figure 1. The resulting pseudo in-domain corpora are used to train individual TMs, as detailed in Table 3. 0 2k 4k 6k 8k 10k Sentence Pairs Figure 1: Sample of ranked sentence-pairs (10k) of each of the out-of-domain parallel corpora with bilingual cross-entropy difference The results obtained using the pseudo indomain data show BLEU (Papineni et al., 2002) scores superior or equal to the baseline score. Only the Europarl subset is slightly lower than the baseline, while the subset taken from the 109 corpus reaches the highest BLEU compared to the other systems (30.29). This is mainly due to the Bilingual Cross-Entropy Difference 10 -2 -4 4 2 6 0 8 Common Crawl Europarl 10^9 UN 173 size of this subset which is ten times larger than the one taken from Europarl. The last row of Table 3 shows the BLEU score obtained after interpolating the four pseudo in-domain translation models. This system outperforms the best pseudo indomain one by 0.5 absolute</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vfctor M S´anchez-Cartagena</author>
<author>Felipe S´anchezMartfnez</author>
<author>Juan Antonio P´erez-Ortiz</author>
</authors>
<title>Integrating Shallow-transfer Rules into Phrase-based Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of MT Summit XIII,</booktitle>
<pages>562--569</pages>
<marker>S´anchez-Cartagena, S´anchezMartfnez, P´erez-Ortiz, 2011</marker>
<rawString>Vfctor M. S´anchez-Cartagena, Felipe S´anchezMartfnez, and Juan Antonio P´erez-Ortiz. 2011. Integrating Shallow-transfer Rules into Phrase-based Statistical Machine Translation. In Proceedings of MT Summit XIII, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vfctor M S´anchez-Cartagena</author>
<author>Juan Antonio P´erezOrtiz</author>
<author>Felipe S´anchez-Martfnez</author>
</authors>
<date>2014</date>
<booktitle>The UA-Prompsit Hybrid Machine Translation System for the 2014 Workshop on Statistical Machine Translation. In Proceedings of WMT.</booktitle>
<marker>S´anchez-Cartagena, P´erezOrtiz, S´anchez-Martfnez, 2014</marker>
<rawString>Vfctor M. S´anchez-Cartagena, Juan Antonio P´erezOrtiz, and Felipe S´anchez-Martfnez. 2014. The UA-Prompsit Hybrid Machine Translation System for the 2014 Workshop on Statistical Machine Translation. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>539--549</pages>
<contexts>
<context position="5692" citStr="Sennrich (2012)" startWordPosition="863" endWordPosition="864"> Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is MOSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the concatenated development sets from 2008 to 2012 (noted newstest2008-2012), as described in Sennrich (2012). Finally, we make use of the findings from WMT 2013 brought by the winning team (Durrani et al., 2013) and decide to use the Operation Sequence Model (OSM), based on minimal translation units and Markov chains over sequences of operations, implemented in MOSES 1LDC2011T07 English Gigaword Fifth Edition, LDC2011T10 French Gigaword Third Edition 2This ratio was empirically chosen based on words fertility between English and French. Corpus Sentences (k) Words (M) Monolingual Data – English Europarl v7 2,218.2 59.9 News Commentary v8 304.2 7.4 News Shuffled 2007 3,782.5 90.2 News Shuffled 2008 12</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012. Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation. In Proceedings of EACL, pages 539–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="19928" citStr="Snover et al., 2006" startWordPosition="3176" endWordPosition="3179">after detruecasing and de-tokenising using the scripts distributed with MOSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of BLEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation leads to an overall improvement of 0.1 BLEU absolute. The results on newstest2014 show that adding synthetic rules did not help improving BLEU and degraded slightly TER (Snover et al., 2006) scores. In addition to our English→French submission, we submitted a French→English translation. Our French→English MT system is built on the alignments obtained from the English→French direction. The training processes between the two sys175 System newstest2013 Best tuning cube-pruning (pop-limit 10000) increased table-limit (100) monotonic reordering Best decoding newstest2014 Best decoding Best decoding + Rules Table 7: Case sensitive results obtained with our final English–French SMT system on newstest2013 when experimenting with different decoding parameters. The best parameters are kept</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of AMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>SRILM at Sixteen: Update and Outlook.</title>
<date>2011</date>
<booktitle>In Proceedings of ASRU.</booktitle>
<contexts>
<context position="4990" citStr="Stolcke et al., 2011" startWordPosition="751" endWordPosition="754">ES toolkit. The same pre-processing steps are applied to the development and test sets. As development sets, we used all the test sets from previous years of WMT, from 2008 to 2013 (newstest2008-2013). Finally, the training parallel corpora are cleaned using the script clean-corpus-n.perl, keeping the sentences longer than 1 word, shorter than 80 words, and with a length ratio between sentence pairs lower than 4.2 The statistics about the corpora used in our experiments after pre-processing are presented in Table 1. For training LMs we use KENLM (Heafield et al., 2013) and the SRILM tool-kit (Stolcke et al., 2011). For training TMs, we use MOSES (Koehn et al., 2007) version 2.1 with MGIZA++ (Och and Ney, 2003; Gao and Vogel, 2008). These tools are used with default parameters for our experiments except when explicitly said. The decoder used to generate translations is MOSES using features weights optimised with MERT (Och, 2003). As our approach relies on training individual TMs, one for each parallel corpus, our final TM is obtained by linearly interpolating the individual ones. The interpolation of TMs is performed using the script tmcombine.py, minimising the cross-entropy between the TM and the conc</context>
<context position="7752" citStr="Stolcke et al., 2011" startWordPosition="1194" endWordPosition="1198">81.3 4.5 (EN) 5.3 (FR) UN 12,354.7 313.4 (EN) 356.5 (FR) Table 1: Data statistics after pre-processing of the monolingual and parallel corpora used in our experiments. and introduced by Durrani et al. (2011). 3 Language Models The LMs are trained in the same way for both languages. First, each monolingual and parallel corpus is considered individually (except the parallel version of Europarl and News Commentary) and used to train a 5-gram LM with the modified Kneser-Ney smoothing method. We then interpolate the individual LMs using the script computebest-mix available with the SRILM tool-kit (Stolcke et al., 2011), based on their perplexity scores on the concatenation of the development sets from 2008 to 2012 (the 2013 version is held-out for the tuning of the TMs). 172 The final LM for French contains all the word sequences from 1 to 5-grams contained in the training corpora without any pruning. However, with the computing resources at our disposal, the English LMs could not be interpolated without pruning non-frequent n-grams. Thus, n-grams with n ∈ [3; 5] with a frequency lower than 2 were removed. Details about the final LMs are given in Table 2. 1-gram 2-gram 3-gram 4-gram 5-gram English 13.4 198.</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. SRILM at Sixteen: Update and Outlook. In Proceedings of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online Large-margin Training for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="19143" citStr="Watanabe et al., 2007" startWordPosition="3045" endWordPosition="3049">de to submit two systems to the shared task: one without and one with synthetic rules. By submitting a system without synthetic rules, we also ensure that our SMT system is constrained according to the shared task guidelines. System BLEUdev Baseline 27.76 + pseudo in + pseudo out 31.93 + OSM 31.90 + MERT 200-best 32.21 + Rules 32.10 Table 6: BLEU scores reported by MERT on English–French newstest2013 development set. As MERT is not suitable when a large number of features are used (our system uses 19 fetures), we switch to the Margin Infused Relaxed Algorithm (MIRA) for our submitted systems (Watanabe et al., 2007). The development set used is newstest2012, as we aim to select the best decoding parameters according to the scores obtained when decoding the newstest2013 corpus, after detruecasing and de-tokenising using the scripts distributed with MOSES. This setup allowed us to compare our results with the participants of the translation shared task last year. We pick the decoding parameters leading to the best results in terms of BLEU and decode the official test set of WMT14 newstest2014. The results are reported in Table 7. Results on newstest2013 show that the decoding parameters investigation leads</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online Large-margin Training for Statistical Machine Translation. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
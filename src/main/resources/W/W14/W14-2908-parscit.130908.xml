<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003234">
<title confidence="0.986396">
Is the Stanford Dependency Representation Semantic?
</title>
<author confidence="0.994539">
Rachel Rudinger1 and Benjamin Van Durme1,2
</author>
<affiliation confidence="0.937486666666667">
Center for Language and Speech Processing1
Human Language Technology Center of Excellence2
Johns Hopkins University
</affiliation>
<email confidence="0.995188">
rudinger@jhu.edu, vandurme@cs.jhu.edu
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873368421053">
The Stanford Dependencies are a deep
syntactic representation that are widely
used for semantic tasks, like Recognizing
Textual Entailment. But do they capture
all of the semantic information a meaning
representation ought to convey? This pa-
per explores this question by investigating
the feasibility of mapping Stanford depen-
dency parses to Hobbsian Logical Form,
a practical, event-theoretic semantic rep-
resentation, using only a set of determin-
istic rules. Although we find that such a
mapping is possible in a large number of
cases, we also find cases for which such a
mapping seems to require information be-
yond what the Stanford Dependencies en-
code. These cases shed light on the kinds
of semantic information that are and are
not present in the Stanford Dependencies.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999492352941176">
The Stanford dependency parser (De Marneffe et
al., 2006) provides “deep” syntactic analysis of
natural language by layering a set of hand-written
post-processing rules on top of Stanford’s sta-
tistical constituency parser (Klein and Manning,
2003). Stanford dependency parses are commonly
used as a semantic representation in natural lan-
guage understanding and inference systems.1 For
example, they have been used as a basic meaning
representation for the Recognizing Textual Entail-
ment task proposed by Dagan et al. (2005), such as
by Haghighi et al. (2005) or MacCartney (2009)
and in other inference systems (Chambers et al.,
2007; MacCartney, 2009).
Because of their popular use as a semantic rep-
resentation, it is important to ask whether the Stan-
ford Dependencies do, in fact, encode the kind of
</bodyText>
<footnote confidence="0.9717805">
1Statement presented by Chris Manning at the
*SEM 2013 Panel on Language Understanding
http://nlpers.blogspot.com/2013/07/the-sem-2013-panel-
on-language.html.
</footnote>
<bodyText confidence="0.999979483870968">
information that ought to be present in a versa-
tile semantic form. This paper explores this ques-
tion by attempting to map the Stanford Depen-
dencies into Hobbsian Logical Form (henceforth,
HLF), a neo-Davidsonian semantic representation
designed for practical use (Hobbs, 1985). Our ap-
proach is to layer a set of hand-written rules on
top of the Stanford Dependencies to further trans-
form the representation into HLFs. This approach
is a natural extension of the Stanford Dependen-
cies which are, themselves, derived from manually
engineered post-processing routines.
The aim of this paper is neither to demonstrate
the semantic completeness of the Stanford Depen-
dencies, nor to exhaustively enumerate their se-
mantic deficiencies. Indeed, to do so would be to
presuppose HLF as an entirely complete seman-
tic representation, or, a perfect semantic standard
against which to compare the Stanford Dependen-
cies. We make no such claim. Rather, our intent is
to provide a qualitative discussion of the Stanford
Dependencies as a semantic resource through the
lens of this HLF mapping task. It is only necessary
that HLF capture some subset of important seman-
tic phenomena to make this exercise meaningful.
Our results indicate that in a number of cases,
it is, in fact, possible to directly derive HLFs from
Stanford dependency parses. At the same time,
however, we also find difficult-to-map phenomena
that reveal inherent limitations of the dependen-
cies as a meaning representation.
</bodyText>
<sectionHeader confidence="0.982059" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9979905">
This section provides a brief overview of the HLF
and Stanford dependency formalisms.
</bodyText>
<subsectionHeader confidence="0.995677">
2.1 Hobbsian Logical Form
</subsectionHeader>
<bodyText confidence="0.99905625">
The key insight of event-theoretic semantic repre-
sentations is the reification of events (Davidson,
1967), or, treating events as entities in the world.
As a logical, first-order representation, Hobbsian
</bodyText>
<page confidence="0.981687">
54
</page>
<note confidence="0.505798">
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 54–58,
</note>
<page confidence="0.337634">
Baltimore, Maryland, USA, June 22-27, 2014. c�2014 Association for Computational Linguistics
</page>
<bodyText confidence="0.99872025">
Logical Form (Hobbs, 1985) employs this ap-
proach by allowing for the reification of any pred-
icate into an event variable. Specifically, for any
predicate p(x1, · · · , xn), there is a corresponding
predicate, p0(E, x1, · · · , xn), where E refers to
the predicate (or event) p(x1, · · · , xn). The reified
predicates are related to their non-reified forms
with the following axiom schema:
</bodyText>
<equation confidence="0.9799455">
(∀x1 ··· xn)p(x1 ··· xn) ↔ (∃e)Exist(e) ∧
p0(e, x1 ··· xn)
</equation>
<bodyText confidence="0.95700375">
In HLF, “A boy runs” would be represented as:
(∃e, x)Exist(e) ∧ run0(e, x) ∧ boy(x)
and the sentence “A boy wants to build a boat
quickly” (Hobbs, 1985) would be represented as:
</bodyText>
<equation confidence="0.5805065">
(∃e1, e2, e3, x, y)Exist(e1) ∧ want0(e1, x, e2) ∧
quick0(e2, e3)∧build0(e3, x, y)∧boy(x)∧boat(y)
</equation>
<subsectionHeader confidence="0.999206">
2.2 Stanford Dependencies
</subsectionHeader>
<bodyText confidence="0.982371857142857">
A Stanford dependency parse is a set of triples
consisting of two tokens (a governor and a depen-
dent), and a labeled syntactic or semantic relation
between the two tokens. Parses can be rendered
as labeled, directed graphs, as in Figure 1. Note
that this paper assumes the collapsed version of
the Stanford Dependencies.2
</bodyText>
<figureCaption confidence="0.961543">
Figure 1: Dependency parse of “A boy wants to
build a boat quickly.”
</figureCaption>
<sectionHeader confidence="0.870534" genericHeader="method">
3 Mapping to HLF
</sectionHeader>
<bodyText confidence="0.996158142857143">
We describe in this section our deterministic algo-
rithm for mapping Stanford dependency parses to
HLF. The algorithm proceeds in four stages: event
2The collapsed version is more convenient for our pur-
poses, but using the uncollapsed version would not signifi-
cantly affect our results.
extraction, argument identification, predicate-
argument assignment, and formula construction.
We demonstrate these steps on the above example
sentence “A boy wants to build a boat quickly.”3
The rule-based algorithm operates on the sen-
tence level and is purely a function of the depen-
dency parse or other trivially extractible informa-
tion, such as capitalization.
</bodyText>
<subsectionHeader confidence="0.999391">
3.1 Event Extraction
</subsectionHeader>
<bodyText confidence="0.999818875">
The first step is to identify the set of event predi-
cates that will appear in the final HLF and assign
an event variable to each. Most predicates are gen-
erated by a single token in the sentence (e.g., the
main verb). For each token t in the sentence, an
event (ei, pt) (where ei is the event variable and pt
is the predicate) is added to the set of events if any
of the following conditions are met:
</bodyText>
<listItem confidence="0.9891155">
1. t is the dependent of the relation root,
ccomp, xcomp, advcl, advmod, or
partmod.
2. t is the governor of the relation nsubj, dobj,
ccomp, xcomp, xsubj, advcl, nsubjpass,
or agent.
</listItem>
<bodyText confidence="0.972249428571429">
Furthermore, an event (ei, pr) is added for any
triple (rel, gov, dep) where rel is prefixed with
“prep ” (e.g., prep to, prep from, prep by, etc.).
Applying this step to our example sentence “A
boy wants to build a boat quickly.” yields the fol-
lowing set:
(e1, wants), (e2, quickly), (e3, build)
</bodyText>
<subsectionHeader confidence="0.997436">
3.2 Argument Identification
</subsectionHeader>
<bodyText confidence="0.999753166666667">
Next, the set of entities that will serve as predicate
arguments are identified. Crucially, this set will
include some event variables generated in the pre-
vious step. For each token, t, an argument (xi, t)
is added to the set of arguments if one of the fol-
lowing conditions is met:
</bodyText>
<listItem confidence="0.9947498">
1. t is the dependent of the relation nsubj,
xsubj, dobj, ccomp, xcomp, nsubjpass,
agent, or iobj.
2. t is the governor of the relation advcl,
advmod, or partmod.
</listItem>
<footnote confidence="0.973855">
3Hobbs (1985) uses the example sentence “A boy wanted
to build a boat quickly.”
</footnote>
<page confidence="0.998092">
55
</page>
<bodyText confidence="0.954496666666667">
Applying this step to our example sentence, we
get the following argument set:
(x1, boat), (x2, build), (x3, boy)
After running this stage on our example sen-
tence, the predicate-argument assignments are as
follows:
</bodyText>
<equation confidence="0.840842">
wants(x3, e2), build(x3, x1), quickly(e3)
</equation>
<bodyText confidence="0.999761333333333">
Notice that the token build has generated both
an event predicate and an argument. This is be-
cause in our final HLF, build will be both an event
predicate that takes the arguments boy and boat,
as well as an argument to the intensional predicate
want.
</bodyText>
<subsectionHeader confidence="0.994903">
3.3 Predicate-Argument Assignment
</subsectionHeader>
<bodyText confidence="0.953678090909091">
In this stage, arguments are assigned to each pred-
icate. pt.argz denotes the ith argument of pred-
icate pt and arg(t) denotes the argument associ-
ated with token t. For example, arg(boy) = x2
and arg(quickly) = e3. We also say that if the
token t1 governs t2 by some relation, e.g. nsubj,
then t1 nsubj-governs t2, or t2 nsubj-depends on
t1. Note that argz refers to any slot past arg2. Ar-
guments are assigned as follows.
For each predicate pt (corresponding to token
t):
</bodyText>
<listItem confidence="0.979369111111111">
1. If there is a token t&apos; such that t nsubj-,
xsubj-, or agent-governs t&apos;, then pt.arg1 =
arg(t&apos;).
2. If there is a token t&apos; such that t dobj-governs
t&apos;, then pt.arg2 = arg(t&apos;).
3. If there is a token t&apos; such that t nsubjpass-
governs t&apos;, then pt.argz = arg(t&apos;).
4. If there is a token t&apos; such that t partmod-
depends on t&apos;, then pt.arg2 = arg(t&apos;).
5. If there is a token t&apos; such that t iobj-governs
t&apos;, then pt.argz = arg(t&apos;).
6. If there is a token t&apos; such that t ccomp- or
xcomp-governs t&apos;, then pt.argz = arg(t&apos;)
(a) UNLESS there is a token t&apos;&apos; such that
t&apos; advmod-governs t&apos;&apos;, in which case
pt.argz = arg(t&apos;&apos;).
7. If there is a token t&apos; such that t advmod- or
advcl-depends on t&apos;, then pt.argz = arg(t&apos;).
</listItem>
<bodyText confidence="0.953284">
And for each pr generated from relation
(rel, gov, dep) (i.e. all of the “prep ” relations):
</bodyText>
<listItem confidence="0.9279125">
1. pr.arg1 = arg(gov)
2. pr.argz = arg(dep)
</listItem>
<bodyText confidence="0.8670305">
Each predicate can be directly replaced with its
reified forms (i.e., p&apos;):
</bodyText>
<equation confidence="0.987125">
wants&apos;(e1, x3, e2),build&apos;(e3, x3, x1),
quickly&apos;(e2, e3)
</equation>
<bodyText confidence="0.999982375">
Two kinds of non-eventive predicates still need
to be formed. First, every entity (xz, t) that is
neither a reified event nor a proper noun, e.g.,
(x3, boy), generates a predicate of the form t(xz).
Second, we generate Hobbs’s Exist predicate,
which identifies which event actually occurs in the
“real world.” This is simply the event generated
by the dependent of the root relation.
</bodyText>
<subsectionHeader confidence="0.842625">
3.4 Formula Construction
</subsectionHeader>
<bodyText confidence="0.962114166666667">
In this stage, the final HLF is pieced together. We
join all of the predicates formed above with the
and conjunction, and existentially quantify over
every variable found therein. For our example sen-
tence, the resulting HLF is:
A boy wants to build a boat quickly.
</bodyText>
<equation confidence="0.960759">
(∃e1, e2, e3, x1, x3)[Exist(e1) ∧ boat(x1) ∧
boy(x3) ∧ wants&apos;(e1, x3, e2) ∧ build&apos;(e3, x3, x1)
∧ quickly&apos;(e2, e3)]
</equation>
<sectionHeader confidence="0.888613" genericHeader="method">
4 Analysis of Results
</sectionHeader>
<bodyText confidence="0.999803">
This section discusses semantic phenomena that
our mapping does and does not capture, providing
a lens for assessing the usefulness of the Stanford
Dependencies as a semantic resource.
</bodyText>
<subsectionHeader confidence="0.980829">
4.1 Successes
</subsectionHeader>
<bodyText confidence="0.999659692307692">
Formulas 1-7 are correct HLFs that our mapping
rules successfully generate. They illustrate the di-
versity of semantic information that is easily re-
coverable from Stanford dependency parses.
Formulas 1-2 show successful parses in sim-
ple transitive sentences with active/passive alter-
nations, and Formula 3 demonstrates success in
parsing ditransitives. Also easily recovered from
the dependency structures are semantic parses of
sentences with adverbs (Formula 4) and reporting
verbs (Formula 5). Lest it appear that these phe-
nomena may only be handled in isolation, Equa-
tions 6-7 show successful parses for sentences
</bodyText>
<page confidence="0.992409">
56
</page>
<bodyText confidence="0.994838">
with arbitrary combinations of the above phenom-
ena.
</bodyText>
<equation confidence="0.990206038461539">
A boy builds a boat.
(∃e1, x1,x2)[Exist(e1) ∧ boy(x2) ∧ boat(x1)
∧ builds0(e1, x2, x1)] (1)
A boat was built by a boy.
(∃e1, x1, x2)[Exist(e1) ∧ boat(x2) ∧ boy(x1)
∧ built0(e1, x1, x2)] (2)
John gave Mary a boat.
(∃e1, x1)[Exist(e1) ∧ boat(x1)
∧ gave0(e1, John, x1, Mary)] (3)
John built a boat quickly.
OR John quickly built a boat.
(∃e1, e2, x1)[Exist(e1) ∧ boat(x1) ∧
quickly(e2, e1) ∧ built0(e1, John, x1)] (4)
John told Mary that a boy built a boat.
(∃e1, e2, x1, x4)[Exist(e1)∧boy(x1)∧boat(x4)∧
built0(e2, x1, x4) ∧ told0(e1, John, Mary, e2)]
John told Mary that Sue told Joe (5)
that Adam loves Eve.
(∃e1, e2, e3)[Exist(e1)∧told0(e2, Sue, Joe, e3)∧
loves0(e3, Adam, Eve) ∧
told0(e1, John, Mary, e2)] (6)
John was told by Mary that Sue wants
Joe to build a boat quickly.
(∃e1, e2, e3, e4, x7)[Exist(e1) ∧ boat(x7) ∧
build0(e2, Joe, x7)∧told0(e1, Mary, John, e4)∧
wants0(e4, Sue, e3) ∧ quickly0(e3, e2)] (7)
</equation>
<subsectionHeader confidence="0.985763">
4.2 Limitations
</subsectionHeader>
<bodyText confidence="0.99999152631579">
Though our mapping rules enable us to directly ex-
tract deep semantic information directly from the
Stanford dependency parses in the above cases,
there are a number of difficulties with this ap-
proach that shed light on inherent limitations of
the Stanford Dependencies as a semantic resource.
A major such limitation arises in cases of event
nominalizations. Because dependency parses are
syntax-based, their structures do not distinguish
between eventive noun phrases like “the bombing
of the city” and non-eventive ones like “the mother
of the child”; such a distinction, however, would
be found in the corresponding HLFs.
Certain syntactic alternations also prove prob-
lematic. For example, the dependency structure
does not recognize that “window” takes the same
semantic role in the sentences “John broke the mir-
ror.” and “The mirror broke.” The use of addi-
tional semantic resources, like PropBank (Palmer
et al., 2005), would be necessary to determine this.
Prepositional phrases present another problem
for our mapping task, as the Stanford dependen-
cies will typically not distinguish between PPs
indicating arguments and adjuncts. For exam-
ple, “Mary stuffed envelopes with coupons” and
“Mary stuffed envelopes with John” have identical
dependency structures, yet “coupons” and “John”
are (hopefully for John) taking on different seman-
tic roles. This is, in fact, a prime example of how
Stanford dependency parses may resolve syntactic
ambiguity without resolving semantic ambiguity.
Of course, one might manage more HLF cov-
erage by adding more rules to our system, but the
limitations discussed here are fundamental. If two
sentences have different semantic interpretations
but identical dependency structures, then there can
be no deterministic mapping rule (based on depen-
dency structure alone) that yields this distinction.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999991272727273">
We have presented here our attempt to map the
Stanford Dependencies to HLF via a second layer
of hand-written rules. That our mapping rules,
which are purely a function of dependency struc-
ture, succeed in producing correct HLFs in some
cases is good evidence that the Stanford Depen-
dencies do contain some practical level of seman-
tic information. Nevertheless, we were also able to
quickly identify aspects of meaning that the Stan-
ford Dependencies did not capture.
Our argument does not require that HLF be an
optimal representation, only that it capture worth-
while aspects of semantics and that it not be read-
ily derived from the Stanford representation. This
is enough to conclude that the Stanford Dependen-
cies are not complete as a meaning representation.
While not surprising (as they are intended as a
syntactic representation), we hope this short study
will help further discussion on what the commu-
nity wants or needs in a meaning representation:
what gaps are acceptable, if any, and whether a
more “complete” representation is needed.
</bodyText>
<sectionHeader confidence="0.998817" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.85127275">
This material is partially based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
</bodyText>
<page confidence="0.998078">
57
</page>
<sectionHeader confidence="0.995876" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999924926829268">
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh,
and Christopher D Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing, pages 165–170. Associa-
tion for Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entail-
ment.
Donald Davidson. 1967. The logical form of action
sentences. In The Logic of Decision and Action,
pages 81–120. Univ. of Pittsburgh Press.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449–454.
Aria D Haghighi, Andrew Y Ng, and Christopher D
Manning. 2005. Robust textual inference via graph
matching. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 387–394.
Association for Computational Linguistics.
Jerry R Hobbs. 1985. Ontological promiscuity. In
Proceedings of the 23rd annual meeting on Associ-
ation for Computational Linguistics, pages 60–69.
Association for Computational Linguistics.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Bill MacCartney. 2009. Natural language inference.
Ph.D. thesis, Stanford University.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
</reference>
<page confidence="0.99926">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417601">
<title confidence="0.794772">Is the Stanford Dependency Representation Semantic? and Van for Language and Speech Language Technology Center of</title>
<author confidence="0.836086">Johns Hopkins</author>
<email confidence="0.999921">rudinger@jhu.edu,vandurme@cs.jhu.edu</email>
<abstract confidence="0.9990494">The Stanford Dependencies are a deep syntactic representation that are widely used for semantic tasks, like Recognizing Textual Entailment. But do they capture all of the semantic information a meaning representation ought to convey? This paper explores this question by investigating the feasibility of mapping Stanford dependency parses to Hobbsian Logical Form, a practical, event-theoretic semantic representation, using only a set of deterministic rules. Although we find that such a mapping is possible in a large number of cases, we also find cases for which such a mapping seems to require information beyond what the Stanford Dependencies encode. These cases shed light on the kinds of semantic information that are and are not present in the Stanford Dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Cer</author>
<author>Trond Grenager</author>
<author>David Hall</author>
<author>Chloe Kiddon</author>
<author>Bill MacCartney</author>
<author>MarieCatherine de Marneffe</author>
<author>Daniel Ramage</author>
<author>Eric Yeh</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning alignments and leveraging natural logic.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>165--170</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, de Marneffe, Ramage, Yeh, Manning, 2007</marker>
<rawString>Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill MacCartney, MarieCatherine de Marneffe, Daniel Ramage, Eric Yeh, and Christopher D Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 165–170. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1571" citStr="Dagan et al. (2005)" startWordPosition="230" endWordPosition="233"> semantic information that are and are not present in the Stanford Dependencies. 1 Introduction The Stanford dependency parser (De Marneffe et al., 2006) provides “deep” syntactic analysis of natural language by layering a set of hand-written post-processing rules on top of Stanford’s statistical constituency parser (Klein and Manning, 2003). Stanford dependency parses are commonly used as a semantic representation in natural language understanding and inference systems.1 For example, they have been used as a basic meaning representation for the Recognizing Textual Entailment task proposed by Dagan et al. (2005), such as by Haghighi et al. (2005) or MacCartney (2009) and in other inference systems (Chambers et al., 2007; MacCartney, 2009). Because of their popular use as a semantic representation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of 1Statement presented by Chris Manning at the *SEM 2013 Panel on Language Understanding http://nlpers.blogspot.com/2013/07/the-sem-2013-panelon-language.html. information that ought to be present in a versatile semantic form. This paper explores this question by attempting to map the Stanford Dependencies into Hobbsian L</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>The logical form of action sentences.</title>
<date>1967</date>
<booktitle>In The Logic of Decision and Action,</booktitle>
<pages>81--120</pages>
<publisher>Univ. of Pittsburgh Press.</publisher>
<contexts>
<context position="3715" citStr="Davidson, 1967" startWordPosition="563" endWordPosition="564"> necessary that HLF capture some subset of important semantic phenomena to make this exercise meaningful. Our results indicate that in a number of cases, it is, in fact, possible to directly derive HLFs from Stanford dependency parses. At the same time, however, we also find difficult-to-map phenomena that reveal inherent limitations of the dependencies as a meaning representation. 2 Background This section provides a brief overview of the HLF and Stanford dependency formalisms. 2.1 Hobbsian Logical Form The key insight of event-theoretic semantic representations is the reification of events (Davidson, 1967), or, treating events as entities in the world. As a logical, first-order representation, Hobbsian 54 Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 54–58, Baltimore, Maryland, USA, June 22-27, 2014. c�2014 Association for Computational Linguistics Logical Form (Hobbs, 1985) employs this approach by allowing for the reification of any predicate into an event variable. Specifically, for any predicate p(x1, · · · , xn), there is a corresponding predicate, p0(E, x1, · · · , xn), where E refers to the predicate (or event) p(x1, · · · , xn).</context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Donald Davidson. 1967. The logical form of action sentences. In The Logic of Decision and Action, pages 81–120. Univ. of Pittsburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria D Haghighi</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust textual inference via graph matching.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>387--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1606" citStr="Haghighi et al. (2005)" startWordPosition="237" endWordPosition="240">nd are not present in the Stanford Dependencies. 1 Introduction The Stanford dependency parser (De Marneffe et al., 2006) provides “deep” syntactic analysis of natural language by layering a set of hand-written post-processing rules on top of Stanford’s statistical constituency parser (Klein and Manning, 2003). Stanford dependency parses are commonly used as a semantic representation in natural language understanding and inference systems.1 For example, they have been used as a basic meaning representation for the Recognizing Textual Entailment task proposed by Dagan et al. (2005), such as by Haghighi et al. (2005) or MacCartney (2009) and in other inference systems (Chambers et al., 2007; MacCartney, 2009). Because of their popular use as a semantic representation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of 1Statement presented by Chris Manning at the *SEM 2013 Panel on Language Understanding http://nlpers.blogspot.com/2013/07/the-sem-2013-panelon-language.html. information that ought to be present in a versatile semantic form. This paper explores this question by attempting to map the Stanford Dependencies into Hobbsian Logical Form (henceforth, HLF), a ne</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria D Haghighi, Andrew Y Ng, and Christopher D Manning. 2005. Robust textual inference via graph matching. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 387–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Ontological promiscuity.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>60--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2284" citStr="Hobbs, 1985" startWordPosition="337" endWordPosition="338">al., 2007; MacCartney, 2009). Because of their popular use as a semantic representation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of 1Statement presented by Chris Manning at the *SEM 2013 Panel on Language Understanding http://nlpers.blogspot.com/2013/07/the-sem-2013-panelon-language.html. information that ought to be present in a versatile semantic form. This paper explores this question by attempting to map the Stanford Dependencies into Hobbsian Logical Form (henceforth, HLF), a neo-Davidsonian semantic representation designed for practical use (Hobbs, 1985). Our approach is to layer a set of hand-written rules on top of the Stanford Dependencies to further transform the representation into HLFs. This approach is a natural extension of the Stanford Dependencies which are, themselves, derived from manually engineered post-processing routines. The aim of this paper is neither to demonstrate the semantic completeness of the Stanford Dependencies, nor to exhaustively enumerate their semantic deficiencies. Indeed, to do so would be to presuppose HLF as an entirely complete semantic representation, or, a perfect semantic standard against which to compa</context>
<context position="4048" citStr="Hobbs, 1985" startWordPosition="607" endWordPosition="608">of the dependencies as a meaning representation. 2 Background This section provides a brief overview of the HLF and Stanford dependency formalisms. 2.1 Hobbsian Logical Form The key insight of event-theoretic semantic representations is the reification of events (Davidson, 1967), or, treating events as entities in the world. As a logical, first-order representation, Hobbsian 54 Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 54–58, Baltimore, Maryland, USA, June 22-27, 2014. c�2014 Association for Computational Linguistics Logical Form (Hobbs, 1985) employs this approach by allowing for the reification of any predicate into an event variable. Specifically, for any predicate p(x1, · · · , xn), there is a corresponding predicate, p0(E, x1, · · · , xn), where E refers to the predicate (or event) p(x1, · · · , xn). The reified predicates are related to their non-reified forms with the following axiom schema: (∀x1 ··· xn)p(x1 ··· xn) ↔ (∃e)Exist(e) ∧ p0(e, x1 ··· xn) In HLF, “A boy runs” would be represented as: (∃e, x)Exist(e) ∧ run0(e, x) ∧ boy(x) and the sentence “A boy wants to build a boat quickly” (Hobbs, 1985) would be represented as: </context>
<context position="7216" citStr="Hobbs (1985)" startWordPosition="1159" endWordPosition="1160">our example sentence “A boy wants to build a boat quickly.” yields the following set: (e1, wants), (e2, quickly), (e3, build) 3.2 Argument Identification Next, the set of entities that will serve as predicate arguments are identified. Crucially, this set will include some event variables generated in the previous step. For each token, t, an argument (xi, t) is added to the set of arguments if one of the following conditions is met: 1. t is the dependent of the relation nsubj, xsubj, dobj, ccomp, xcomp, nsubjpass, agent, or iobj. 2. t is the governor of the relation advcl, advmod, or partmod. 3Hobbs (1985) uses the example sentence “A boy wanted to build a boat quickly.” 55 Applying this step to our example sentence, we get the following argument set: (x1, boat), (x2, build), (x3, boy) After running this stage on our example sentence, the predicate-argument assignments are as follows: wants(x3, e2), build(x3, x1), quickly(e3) Notice that the token build has generated both an event predicate and an argument. This is because in our final HLF, build will be both an event predicate that takes the arguments boy and boat, as well as an argument to the intensional predicate want. 3.3 Predicate-Argumen</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Jerry R Hobbs. 1985. Ontological promiscuity. In Proceedings of the 23rd annual meeting on Association for Computational Linguistics, pages 60–69. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1295" citStr="Klein and Manning, 2003" startWordPosition="188" endWordPosition="191">tion, using only a set of deterministic rules. Although we find that such a mapping is possible in a large number of cases, we also find cases for which such a mapping seems to require information beyond what the Stanford Dependencies encode. These cases shed light on the kinds of semantic information that are and are not present in the Stanford Dependencies. 1 Introduction The Stanford dependency parser (De Marneffe et al., 2006) provides “deep” syntactic analysis of natural language by layering a set of hand-written post-processing rules on top of Stanford’s statistical constituency parser (Klein and Manning, 2003). Stanford dependency parses are commonly used as a semantic representation in natural language understanding and inference systems.1 For example, they have been used as a basic meaning representation for the Recognizing Textual Entailment task proposed by Dagan et al. (2005), such as by Haghighi et al. (2005) or MacCartney (2009) and in other inference systems (Chambers et al., 2007; MacCartney, 2009). Because of their popular use as a semantic representation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of 1Statement presented by Chris Manning at the </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
</authors>
<title>Natural language inference.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1627" citStr="MacCartney (2009)" startWordPosition="242" endWordPosition="243">Stanford Dependencies. 1 Introduction The Stanford dependency parser (De Marneffe et al., 2006) provides “deep” syntactic analysis of natural language by layering a set of hand-written post-processing rules on top of Stanford’s statistical constituency parser (Klein and Manning, 2003). Stanford dependency parses are commonly used as a semantic representation in natural language understanding and inference systems.1 For example, they have been used as a basic meaning representation for the Recognizing Textual Entailment task proposed by Dagan et al. (2005), such as by Haghighi et al. (2005) or MacCartney (2009) and in other inference systems (Chambers et al., 2007; MacCartney, 2009). Because of their popular use as a semantic representation, it is important to ask whether the Stanford Dependencies do, in fact, encode the kind of 1Statement presented by Chris Manning at the *SEM 2013 Panel on Language Understanding http://nlpers.blogspot.com/2013/07/the-sem-2013-panelon-language.html. information that ought to be present in a versatile semantic form. This paper explores this question by attempting to map the Stanford Dependencies into Hobbsian Logical Form (henceforth, HLF), a neo-Davidsonian semanti</context>
</contexts>
<marker>MacCartney, 2009</marker>
<rawString>Bill MacCartney. 2009. Natural language inference. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="12807" citStr="Palmer et al., 2005" startWordPosition="2099" endWordPosition="2102">ch limitation arises in cases of event nominalizations. Because dependency parses are syntax-based, their structures do not distinguish between eventive noun phrases like “the bombing of the city” and non-eventive ones like “the mother of the child”; such a distinction, however, would be found in the corresponding HLFs. Certain syntactic alternations also prove problematic. For example, the dependency structure does not recognize that “window” takes the same semantic role in the sentences “John broke the mirror.” and “The mirror broke.” The use of additional semantic resources, like PropBank (Palmer et al., 2005), would be necessary to determine this. Prepositional phrases present another problem for our mapping task, as the Stanford dependencies will typically not distinguish between PPs indicating arguments and adjuncts. For example, “Mary stuffed envelopes with coupons” and “Mary stuffed envelopes with John” have identical dependency structures, yet “coupons” and “John” are (hopefully for John) taking on different semantic roles. This is, in fact, a prime example of how Stanford dependency parses may resolve syntactic ambiguity without resolving semantic ambiguity. Of course, one might manage more </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
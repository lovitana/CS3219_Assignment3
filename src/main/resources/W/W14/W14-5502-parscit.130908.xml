<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.9980185">
Konkanverter - A Finite State Transducer based Statistical
Machine Transliteration Engine for Konkani Language
</title>
<author confidence="0.995257">
Vinodh Rajan
</author>
<affiliation confidence="0.9947415">
School of Computer Science
Universitv of St Andrews
</affiliation>
<address confidence="0.863919">
Scotland, UK
</address>
<email confidence="0.910596">
vrs30st—andrews.ac.uk
</email>
<sectionHeader confidence="0.991924" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999582428571429">
We have developed a finite state transducer based transliteration engine called Konkan-
verter that performs statistical machine transliteration between three different scripts
used to write the Konkani language. The statistical machine transliteration svstem con-
sists of cascading finite state transducers combining both rule-based and statistical ap-
proaches. Based on the limited availabilitv of parallel corpora, this cascading approach is
found to perform significantlv better than a pure rule-based approach or pure statistical
approach.
</bodyText>
<sectionHeader confidence="0.999053" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999905714285714">
Konkani is an Indian language spoken bv approximatelv 2.5 million people (Gov. of India,
2001), mainlv in the Indian state of Goa. It also has a substantial amount of linguistic minoritv
population living in neighboring states of Karnataka and Kerala. In Goa, Konkani uses the
Devanagari script and the Roman script (locallv known as Romi). In Karnataka and Kerala,
the dominant regional scripts, Kannada and Malavalam, are being used to write the language.
Muslim sections of the Konkani population are also known to use a Perso-Arabic based alphabet.
This polvgraphic scenario is unique to Konkani in contemporarv Indian linguistic milieu.
Among these different orthographies, Devanagari, Kannada and the Roman script are the
mainstream orthographic svstems. For all practical purposes, Konkani can therefore be consid-
ered to possess svnchronic trigraphia. There are several important features that differentiate
these orthographies. Consonants of Indic scripts carrv an inherent schwa, which is unmarked,
while other vowel combinations with a consonant are represented as combining signs. However,
absence of schwa in a consonant is marked bv explicit orthographic consonantal clusters or using
a special sign called a Virdma. All orthographies other than Devanagari universallv show ex-
plicit schwa deletion. Devanagari and Kannada distinguish vowel lengths, but their distribution
and representation are verv idiosvncratic to each orthographv. In contrast to the Indic scripts,
Romi does not differentiate vowel length at all. Most importantlv, the Romi orthographv does
not distinguish schwa from vowel o. Both are represented using the same grapheme o. Several
Indic graphemic combinations are also rendered as vowel digraphs or trigraphs in Romi. As a
result, manv Indic sequences are merged in Romi orthographv. Table 1 lists some sample words
in various orthographies.
Svnchronic trigraphia is a major issue of political contention inside the communitv, each group
favoring the usage of a particular script as the official orthographv. Different orthographic
communities exist in isolation with minimal interaction and with its own literarv tradition,
as verv few people are fluent in multiple orthographies. A statistical machine transliteration
engine with reasonable accuracv would greatlv enable cohesion and interaction among the greater
linguistic communitv. Facilitating the usage of multiple scripts would also encourage more
linguistic diversitv among the communitv.
</bodyText>
<footnote confidence="0.698266">
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added bv the organizers. License details: http://creativecommons.org/licenses/bv/4.
0/
</footnote>
<page confidence="0.97993">
11
</page>
<note confidence="0.9801615">
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 11–19,
Dublin, Ireland, August 23-29 2014.
</note>
<table confidence="0.993486833333333">
Devanagari Kannada Romi
XygZpIbr devanagari dez-&apos;aule devnagari devnagri
OQG} jhatako 4L32s jhatko zhottko
qXg4Ly divamce so � dimvce dinvche
XygpXayZ devadavena z3mdaba-&apos; devadaven devadoien
jøygaĘap sattevavalva ;d4��u�� sattevavlva sat&apos;tevoilea
</table>
<tableCaption confidence="0.999086">
Table 1: Sample Konkani words in various orthographies
</tableCaption>
<sectionHeader confidence="0.999129" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997425318181818">
Machine transliteration frequentlv occurs within machine translation when either named entities
or out of vocabularv (OOV) words are encountered. Machine transliteration is also useful for
cross-language information retrieval (CLIR). Consequentlv, a significant amount of work has
been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998). Machine transliteration
can generallv be classified as rule-based or statistical depending on the approach.
Rule-based transliteration is tvpicallv performed through hand-crafted rules and is usu-
allv graphemic in nature. Within Indic transliteration, there have been several attempts on
rule-based approaches. Malik et al (2008) implemented a Hindi-Urdu transliteration svstem
with finite-state transducers using a universal intermediate transcription (UIT). It was based
on the graphemic equivalence between Perso-Arabic script and Devanagari script. Similarlv,
Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic
correspondence between Meetei Mavek and Bengali script.
On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar
from statistical machine translation, including character alignments and subsequent training on
the aligned data. Jia et al (2009) also developed a noisv channel model for the English-Chinese
language pair using Moses, an SMT tool. Malik et al (2013) evaluated 28 different kinds of statis-
tical models for Hindi-Urdu machine transliteration using GIZA++ and Moses. Similarlv, Chin-
nakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil
and English-Kannada, focusing on fine-tuning the character sequence model (CSM). Singh (2012)
evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei
Mavek transliteration. A hvbrid approach combining FSM based techniques with a statistical
word language model with better performance was proposed bv Malik et al (2009).
</bodyText>
<sectionHeader confidence="0.99803" genericHeader="method">
3 Initial Attempts
</sectionHeader>
<bodyText confidence="0.9999639375">
Konkani, being a minor language did not have anv parallel corpora that could have been har-
nessed for statistical machine transliteration. The World Konkani Center, Mangalore had at-
tempted to manuallv mine transliteration rules bv studving the three orthographies and ana-
lvzing the differences. Thev also developed schwa deletion and schwa insertion rules for the
orthographies, modeled on Hindi schwa deletion rules. For the initial machine transliteration
svstem, we refined and improved upon these rules and implemented a rule-based transliteration
svstem. In the absence of corpora, we iterated the rule-based svstem in an ardent attempt to
improve accuracv. However, the performance of the transliteration engine was not verv satis-
factorv and could not be improved bevond a certain limit. The performance of this rule-based
engine is discussed in section 5.
Even though significant effort had to be spent towards the creation of a parallel corpus, it was
finallv decided to incorporate statistical models to improve accuracv. A monolingual untagged
text corpus was obtained for Konkani in Devanagari script. In liaison with linguistic experts, we
manuallv constructed a substantiallv sized corpus, despite several practical difficulties. Table 2
lists the word count of parallel word lists in each orthographic pair. This corpus will be released
into the public domain, after some revisions and proof-checking. Currentlv it is available on
</bodyText>
<page confidence="0.996183">
12
</page>
<table confidence="0.9995825">
Orthographv Word Count
Devanagari - Kannada 23 187
Devanagari - Romi 38 550
Kannada - Romi 14 396
</table>
<tableCaption confidence="0.998947">
Table 2: Word count of parallel corpora
</tableCaption>
<bodyText confidence="0.9414148">
request.
We initiallv contemplated using an interlingua-like approach bv choosing Devanagari as the
intermediate script, thus reducing the need for a dedicated Kannada-Romi transliteration module
and parallel corpus. However, we were skeptical of the error propagation that might occur if the
conversion to the intermediate script itself is not verv accurate.
</bodyText>
<sectionHeader confidence="0.98104" genericHeader="method">
4 Architecture
</sectionHeader>
<bodyText confidence="0.999971125">
The implementation of the framework has been done entirelv using OpenFst (Allauzen et al.,
2007). Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile
those rules into finite-state transducers compatible with Openfst. Thrax was also used to define
finite state acceptors. We found Thrax to be particularlv robust and flexible in generating
various FSTs. Character alignments were performed using Phonetisaurus (Novak et al., 2011).
N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also
generates the models as FSTs.
Below, we describe the detailed architecture for each transliteration pair.
</bodyText>
<subsectionHeader confidence="0.999118">
4.1 Devanagari to Kannada
</subsectionHeader>
<bodyText confidence="0.9998170625">
Input text was first converted into an intermediate romanized encoding, where schwa is explicitlv
denoted. This also results in converting the script from svllabic to alphabetic form. This
eases defining rules to a considerable extent, since independent vowels and dependent vowel
signs need not be dealt with separatelv. We have used a customized encoding which onlv uses
monographs and hence maintains a direct grapheme-to-grapheme correspondence with Indic
scripts. However, to increase readabilitv standardized Indic romanization scheme ISO 15919 has
been used instead for presentation in this paper.
Before proceeding with schwa deletion rules, let us define the necessarv sets. Let V stand for
the class of vowels, C for the class of consonants, Cnj be a list of consonants that cannot occur
as a second element of a triconsonantal cluster and D for the dependent signs Chandrabindu,
Anusvara and Visarga. The morphemic boundarv is denoted bv M. The beginning of string and
end of string is denoted bv si and sf respectivelv. £ denotes a null character with Σ denoting
the entire alphabet.
Let 8 be an orthographic svllable, while 8−a is the set of svllables that do not contain schwa.
8nj is the set of svllables that do not start with Cnj, while 8nj−a is the set that excludes the
svllables with schwa. Using regular expressions, these are defined as:
</bodyText>
<equation confidence="0.751139">
8 &lt;-- C * VD? , 8−a &lt;-- C * (V − a)D? , 8nj &lt;-- Cnj?CVD? , 8nj−a &lt;-- Cnj?C(V − a)D?
</equation>
<bodyText confidence="0.999658125">
The initial word boundarv Bi and final word boundarv Bf are: Bi &lt;-- M|si and Bf &lt;-- M|sf.
The general schwa deletion rules have been given below as context dependent rewrite rules.
These rules are expressed in the form φ -4 ψ/λ��ρ. Here, φ is replaced bv ψ whenever it is
preceded bv λ and succeeded bv ρ, λ and ρ being the left and right contexts respectivelv. The
rules below are listed with a corresponding sample case. These rules were compiled as finite-state
transducers (Narasimhan et al., 2004)
We have produced a list of possible suffixes and prefixes from the collected corpus. Let Pre
denote the set of prefixes and 8uf the set of prefixes. We then mark the morphemic boundarv
</bodyText>
<page confidence="0.909717">
13
</page>
<equation confidence="0.980568071428571">
M as:
Bm = E → M/Pre��Suf
The schwa deletion rules can be effectivelv summarized as follows.
Wf = a → E/(BiS + C+)��Bf  |asata → asat
W3 = a → E/(BiSC)��(S_aBf)  |amvado → amvdo
W3vy = a → E/(BiS(y|v)) (SBf)  |pavasa → pavs
W4 = a → E/(BiSC)��(SnjSBf)  |adakati → adkati
W4y = a → E/(BiSSy)��(SnjBf)  |ulavatam → ulavtam
W43 = a → E/(BiSS_aC)��(SnjBf)  |vicarata → vicarta
WP2 = a → E/(BiSC)��(SnjSS + Bf)  |vacatakuca → vactakuc
WPl = a → E/(BiSS(y|l|v))��(SnjSS + Bf)  |vajavatalo → vajavtalo
WP3 = a → E/(BiS_aS)��(SnjSS + Bf)  |juvanapana → juvanpan
Wtri = a → E/(SC)��Snj  |govamtalva → govamtlva
Wgrm = a → E/(BiSSS + (w|y))ND?)  |govamtale → govamtle
</equation>
<bodyText confidence="0.934120857142857">
Let Br denote the removal of morphemic boundaries. The overall schwa deletion can be con-
structed bv composing all of the above transducers:
Wd = Bm ◦ W3 ◦ W3vy ◦ W4y ◦ W43 ◦ WP2 ◦ WPl ◦ WP3 ◦ Wtri ◦ Wgrm ◦ Wf ◦ Br
Devanagari has two additional vowels � e and 3T o, and two special conjunct characters &apos;:4 rv
and &apos;:9 rh. Thev were directlv mapped to the Kannada characters a e, 2&amp;quot; o, o:bF rv and ZdF rh
respectivelv. Let this mapping be Rvc.
In Kannada, i and u are used at word endings, where a and ii are found in Devanagari.
</bodyText>
<equation confidence="0.991932">
Ri = i → i/(C + ��D?Bf  |sat-1 → sati
Ru = u� → u/(C + ��D?Bf  |vastu → vastu
</equation>
<bodyText confidence="0.8701935">
The Devanagari sequence vamk corresponds to mvk in Kannada, let this be Rvmk The overall
rule-based transduction is:
</bodyText>
<equation confidence="0.808286">
Rkn = Wd ◦ Rvc ◦ Ri ◦ Ru ◦ Rvmk
</equation>
<bodyText confidence="0.997692">
The precedence for the rule-based compositions were decided based on emperical observations.
The input word I is composed with the overall rule-based transducer. The second (output)
projection of this transducer is used for later operations.
</bodyText>
<equation confidence="0.727831">
Rknp = π2(Rkn ◦ I)
</equation>
<bodyText confidence="0.926196">
When either a or ii appears before a final schwa-consonant, with or without a preceding r or
m, it can retain its length or become short. Also, e and o, which usuallv transform into short
vowels a and o, are retained in case of loan words. Additional arcs were added to the transducer
Rknp to reflect this. The new transducer Rknm contains multiple paths for a given input.
We then created a lexical acceptor ALkn from the Kannada words in the corpus. For a given
lattice, this removes all non-lexical entries.
Tknl = Rknm ◦ ALkn
</bodyText>
<page confidence="0.936247">
14
</page>
<figure confidence="0.9953845">
&lt;s&gt;:&lt;s&gt; s:s a:a r:r a:� p:p a:a 1:1 ✓:i &lt;\s&gt;:&lt;\s&gt;
0 1 2 3 4 5 6 7 8 9 10
</figure>
<figureCaption confidence="0.98038">
Figure 1: Transducer Rkn for the Devanagari input jb\er sarapala
Figure 2: Weighted Transducer Tdv2kn for the Devanagari input V}St4G todiimka. Weights indicate
negative log n-gram probabilities
</figureCaption>
<bodyText confidence="0.87710375">
In case none of the outputs are present in the lexicon, or if multiple paths are lexicallv valid
(in case of several standard variants), we proceed to choose the best path bv utilizing n-gram
probabilities. An n-gram word model Nkn was generated based on the list of Kannada words in
the corpus.
</bodyText>
<equation confidence="0.89026575">
If Tknl is null then,
Tdv2kn = Rknm ◦ Nkn
Else,
Tdv2kn = Tknl ◦ Nkn
</equation>
<bodyText confidence="0.999364">
Tdv2kn is the final transducer that transliterates Devanagari to Kannada. The best path was
chosen from the lattice as the most probable transliteration.
</bodyText>
<subsectionHeader confidence="0.98866">
4.2 Kannada to Devanagari
</subsectionHeader>
<bodyText confidence="0.999850705882353">
Kannada to Devanagari transliteration is more complex than Devanagari to Kannada. Since
Kannada orthographv shows explicit schwa deletion, schwa needs to be inserted in this case.
First, we inverted the schwa deletion transducer Wd, effectivelv making it perform schwa
insertion. However, reversing the schwa deletion results in multiple alternate paths, all of which
are theoreticallv viable. In order to prune the lattice, a cluster acceptor ACdv was created. ACdv
rejects all paths that contain non-standard consonantal clusters for the Devanagari orthographv.
This list of non-standard clusters was manuallv created bv analvzing the Devanagari corpus.
For example, the word Ď6y±f ciktun could hvpotheticallv have resulted from schwa deletion of
çLGQtZ cikatuna or çLûQtZ ciktuna (ignoring additional hvpotheses for word-internal u) . However,
the cluster ûQ kt is a non-standard ligature and is highlv unlikelv to appear in Devanagari
orthographv. The acceptor would prune anv path that would result in the cluster kt. Similarlv,
the word TWy¶ vastu could have resulted from gjVt vasatu or gĞVt vastu. But ĞV st being a
standard consonantal cluster, both are equallv plausible. In this case, a lexicon lookup or n-
gram model is necessarv to choose the most probable output.
Similarlv, inverting Rkn also results in multiple alternate paths for i and u among others.
Since Devanagari onlv uses long a and o, let Reo be the transducer that replaces the short
vowels a and o with the corresponding long vowels.
</bodyText>
<figure confidence="0.98740147368421">
&lt;\s&gt;:&lt;\s&gt;/0.09499
17/-2.1753
k:k/0.45317
14
m:m/1.3945
11
u:u/3.4444
7
&lt;s&gt;:&lt;s&gt;/2.1133e-05 t:t/3.1532
0 1 2
✏:✏/9.4242
3
0:0/3.1051
5
⌫:⌫/5.6025
8
m:m/2.8013
12
k:k/2.1108
15
&lt;\s&gt;:&lt;\s&gt;/0.28196
18/-3.2288
o:o/2.7203
m:m/2.8013
4
0:0/2.7583
6
⌫:⌫/5.354
10
u:u/1.4725
9
m:m/-0.52867
13
k:k/-0.26705
16
&lt;\s&gt;:&lt;\s&gt;/0.09499
19/-2.1753
15
</figure>
<figureCaption confidence="0.999979">
Figure 3: Transducer Rdv for the Kannada input Meb±ŉ ciktun
Figure 4: Transducer Rdv for the Kannada input TWjvastu
</figureCaption>
<bodyText confidence="0.895122">
The final rule-based transducer Rdv is,
</bodyText>
<equation confidence="0.956064333333333">
Rdv = R−1
kn ◦ (W−1
d ◦ ACdv) ◦ Reo
</equation>
<bodyText confidence="0.9683715">
In case of non-standard input such as words which alreadv have a schwa in a position where
none is possible, the composition fails. In this case we have a rule-based Schwa insertion trans-
ducer Wir that inserts an additional arc that inserts schwa to non-standard consonantal clusters.
In this case,
</bodyText>
<equation confidence="0.966872166666667">
Rdv = R−1
kn ◦ Wir ◦ Reo
Similar to Tdv2kn, the Kannada to Devanagari transducer Tkn2dv is formed with a Devanagari
lexical acceptor and if needed, a corresponding Devanagari n-gram word model.
Tkn2dv = (π2(Rdv ◦ I) ◦ ALdv) ◦ Ndv or
Tkn2dv = π2(Rdv ◦ I) ◦ Ndv
</equation>
<subsectionHeader confidence="0.991194">
4.3 Kannada to R.omi
</subsectionHeader>
<bodyText confidence="0.9578556">
As in previous transliterations, we romanized the input. Since Romi and Kannada share different
graphemic sets, we proceed with performing a character alignment with a Kannada-Romi par-
allel wordlist. Romanizing the input verv marginallv improves the character alignment process,
since both input and output are then alphabetic scripts (as compared to svllabic to alphabetic
alignment). We initiallv experimented with tools such as GIZA++, but found Phonetisaurus
produced better alignments compared to other tools as it uses manv-to-manv alignments devel-
oped specificallv for grapheme to phoneme svstems (Jiampojamarn et al., 2007).
A sample alignment sequence from Phonetisaurus is given below:
melillem I mellil&apos;lem → m}m e}e 1}lel i}i l}l1&apos; l}l e}e m}m
gadvemtlvan I gaddientlean → g}g a}a d}dld v}i e}e m}n t}t l}l v}e a}a n}n
bharatasarkva I bharotasarkea → bh}blh a}a r}r a}o t}t a}a s}s a}a r}r k}k v}e a}a
bhiveli I bhieli → bh}blh ilv}i e}e l}l i}i
Where } denotes individual character alignment, I between characters indicates grapheme
chunks, and d}dld implies that the source grapheme ed» is mapped to the target graphemic
chunk edd».
</bodyText>
<figure confidence="0.997769740740741">
&lt;s&gt;:&lt;s&gt; c:c i:i k:k a:a t:t
0 1 2 3 4 5 6
✓:✓
u:u n:n 9
8
n:n
a:a &lt;\s&gt;:&lt;\s&gt;
10 11
7
u:u
8
&lt;\s&gt;:&lt;\s&gt;
t:t
6
⇣:⇣
&lt;s&gt;:&lt;s&gt; v:v a:a s:s
0 1 2 3 4
a:a 9
⇣:⇣
5
t:t
7
&lt;\s&gt;:&lt;\s&gt;
&lt;\s&gt;:&lt;\s&gt;
11
u:u
10
</figure>
<page confidence="0.712793">
16
</page>
<figureCaption confidence="0.998958">
Figure 5: 5 best paths of Lp for Kannada input 26Àf aikv
</figureCaption>
<bodyText confidence="0.957975">
This alignment lattice was then used to create a joint sequence n-gram model (Galescu and
Allen, 2002) Nknrm. This is then composed with the input word I, whose output projection
we use. We also modifv the resulting transducer bv removing edges with grapheme chunks and
replacing it with succeeding edges with the individual graphemes of the chunk. This is necessarv
for later operations.
Some graphemic sequences such as geminate vowel graphemes aa, ii etc. do not occur in the
orthographv. We construct a transducer Arm, which accepts onlv paths with standard graphemic
sequences. Thus effectivelv creating a pruned lattice Lp.
Lp = 72(I o Nknrm) o Arm
We created a Romi lexical acceptor ALrm which is composed with Lp. If all the paths are
non-lexical, we select the cheapest path from Lp.
</bodyText>
<subsectionHeader confidence="0.737219">
Tkn2rm = Lp o ALrm or Tknr2rm = shortest(Lp)
4.4 Romi to Kannada
</subsectionHeader>
<bodyText confidence="0.9999126">
We produced a similar set of transducers as in Kannada -4 Romi. The lattice pruner here rejects
non-standard Indic forms like digraphic vowel sequences. We generated a new joint sequence n-
gram model bv swapping the original training data and retraining it. We initiallv attempted to
invert Nknrm, to avoid re-training, but the accuracv was found to be 18% lower than a retrained
model.
</bodyText>
<equation confidence="0.771527">
Lp = 72(I o Nrmkn) o Aind
Tkn2rm = Lp o ALkn or Tkn2rm = shortest(Lp)
</equation>
<subsectionHeader confidence="0.997043">
4.5 Devanagari to Romi
</subsectionHeader>
<bodyText confidence="0.999851833333333">
We performed an initial schwa deletion on the input using Wd. We found that schwa-deleted
input improves the joint sequence n-gram model. Schwa deletion being a grammatical process,
removing one of the underlving uncertainties effectivelv improves the performance. As a result
of this, the joint sequence n-gram model performs better with preprocessed input.
Following Schwa deletion, a similar process to that described in section 4.3 is performed, to
generate the transducer Tdv2rm.
</bodyText>
<equation confidence="0.422273">
Lp = 72((Wd o I) o Ndvrm) o Arm
Tdv2rm = Lp o ALrm or Tdv2rm = shortest(Lp)
</equation>
<subsectionHeader confidence="0.996435">
4.6 Romi to Devanagari
</subsectionHeader>
<bodyText confidence="0.99992475">
We performed a rule-based schwa insertion on the input here. However, we did not see anv
substantial improvement in the performance in terms of accuracv, as compared to the raw input.
However, we retained the preprocessing, to take advantage of even the marginal improvement.
Also, the preprocessing model could be improved in the future. We performed a similar set of
</bodyText>
<figure confidence="0.991020617647059">
&lt;s&gt;:&lt;s&gt;
12
o:o/3.7373
13
i:i/8.297
&lt;s&gt;:&lt;s&gt;
10
a:a/8.8837
11
i:i/3.8662
0
&lt;s&gt;:&lt;s&gt;
&lt;s&gt;:&lt;s&gt;
8
o:o/5.4715
n:n/5.4715 i:i/4.0928
19 9
14
i:i/4.5017 15
k:k/2.8467
i:i/6.6737
16
o:o/1.5448 17 &lt;/s&gt;:&lt;/s&gt;/-2.3985
&lt;s&gt;:&lt;s&gt;
6
e:e/10.943
7
k:k/2.8467 4
i:i/6.6737 5
&lt;/s&gt;:&lt;/s&gt;/2.7255
18
o:o/3.7373 i:i/8.297
1 3
2
</figure>
<page confidence="0.987334">
17
</page>
<table confidence="0.999786">
Script Pair Rules-bases Svstem Statistical Svstem Cascading Svstem
Devanagari - Kannada 83.9% 84.59% 90.383%
Kannada - Devanagari 79.49% 90.16% 96.66%
Devanagari - Romi 74.88% 78.02% 95.39%
Romi - Devanagari 54.02% 74.04% 83.41%
Kannada - Romi 81.29% 87.63% 96.12%
Romi - Kannada 68.01% 82.21% 97.87%
</table>
<tableCaption confidence="0.9066665">
Table 3: Accuracv of three different svstems
transductions as in section 4.4.
</tableCaption>
<equation confidence="0.438932">
Lp = π2((Wir o I) o Nrmdv) o Aind
Trm2dv = Lp o ALdv or Trm2dv = shortest(Lp)
</equation>
<sectionHeader confidence="0.992774" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999983947368421">
For the procedures involving statistical methods, we split the corpus with 90% being used for
training and the remaining 10% for testing. The accuracv results for the orthographies are
reported in table 3. For the rule-based svstem, the initial svstem developed was used for the
evaluation. For the pure statistical approach, we used Phonetisaurus&apos;s in-built g2p svstem. The
cascading svstem was that discussed in this paper.
As expected, the initial rule-based svstem has the least accurate performance. Although it is
theoreticallv possible to mine all rules that can applv to a svstem, in practice the rule-mining
process is highlv inefficient and user-dependent. The statistical svstem performs better than the
rule-based svstem. The mediocre performance of the statistical svstem can be mainlv attributed
to the limited corpora that we possess. Konkani being an inflectional language, the effective
number of unique words in the corpus is considerablv lower than the absolute word count. The
hvbrid svstem performs best when compared to others.
Of all the transliteration pairs, Romi to Devanagari appears to have the lowest accuracv of
all the three svstems. Compared to other transliteration pairs, Romi to Devanagari is the most
complex svstem as it involves both schwa insertion and disambiguation of confounded graphemic
sequences. The poor performance of the hvbrid svstem can be attributed to the fact that we
had used a rule-based schwa insertion as a part of preprocessing. While this svstem worked well
for an Indic svstem such as Kannada, it turned out to be not verv efficient for Romi, where
consonantal sequences are rendered as vowel digraphs or trigraphs.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991571428571">
We have developed a finite state transducer based transliteration engine called Konkanverter
which performs statistical machine transliteration between three different scripts used to write
the Konkani language. We have explained the detailed architecture of this statistical machine
transliteration svstem, which consists of cascading finite state transducers combining both rule-
based and statistical approaches. The transliteration engine was evaluated and its performance
was reported. This cascading approach is found to perform significantlv better than a pure
rule-based approach or pure statistical approach.
</bodyText>
<sectionHeader confidence="0.997964" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.845847">
This work was supported bv the World Institute of Konkani Language (WIKL) part of the
World Konkani Centre, Mangalore, India. The author would also personallv like to thank
Mr Gurudath Bantwalkar, Asst Director of WIKL for all his linguistic expertise regarding the
Konkani language.
</bodyText>
<page confidence="0.998924">
18
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876555555555">
Cvril Allauzen, Michael Rilev, Johan Schalkwvk, Wojciech Skut, and Mehrvar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer librarv. In Proceedings of the Ninth International
Conference on Implementation and Application of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11-23. Springer. http://www.openfst.org.
Mansur Arbabi, Scott M Fischthal, Vincent C Cheng, and Elizabeth Bart. 1994. Algorithms for Arabic
name transliteration. IBM Journal of research and Development, 38(2):183-194.
Manoj Kumar Chinnakotla and Om P Damani. 2009. Experiences with English-Hindi, English-Tamil
and English-Kannada transliteration tasks at news 2009. In Proceedings of the 2009 Named Entities
Workshop: Shared Task on Transliteration, pages 44-47. Association for Computational Linguistics.
Lucian Galescu and James F Allen. 2002. Pronunciation of proper names with a joint n-gram model for
bi-directional grapheme-to-phoneme conversion. In INTERSPEECH.
Ministrv of Home affairs Gov. of India. 2001. Abstract of speakers&apos; strength of languages
and mother tongues - 2001. http://www.censusindia.gov.in/Census_Data_2001/Census_Data_
Online/Language/Statementi.aspx. Accessed: 2014-05-30.
Yuxiang Jia, Danqing Zhu, and Shiwen Yu. 2009. A noisv channel model for grapheme-based machine
transliteration. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,
pages 88-91. Association for Computational Linguistics.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applving manv-to-manv alignments
and hidden markov models to letter-to-phoneme conversion. In HLT-NAACL, volume 7, pages 372-379.
N Kishorjit. 2011. Manipuri transliteration from Bengali script to Meitei Mavek: A rule based approach,
c. singh et al.(eds.): Icisil 2011, ccis vol. 139, part 2.
Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics,
24(4):599-612.
M. G. Abbas Malik, Christian Boitet, and Pushpak Bhattacharvva. 2008. Hindi Urdu machine translit-
eration using finite-state transducers. In Proceedings of the 22nd International Conference on Com-
putational Linguistics-Volume 1, pages 537-544. Association for Computational Linguistics.
M. G. Abbas Malik, Laurent Besacier, Christian Boitet, and Pushpak Bhattacharvva. 2009. A hvbrid
model for Urdu Hindi transliteration. In Proceedings of the 2009 Named Entities Workshop: Shared
Task on Transliteration, pages 177-185. Association for Computational Linguistics.
M. G. Abbas Malik, Christian Boitet, Laurent Besacier, and Pushpak Bhattcharvva. 2013. Urdu Hindi
machine transliteration using SMT. In the Proceedings of the 4th Workshop on South and Southeast
Asian Natural Language Processing, International Joint Conference on Natural Language Processing,
pages 43--57.
Bhuvana Narasimhan, Richard Sproat, and George Kiraz. 2004. Schwa-deletion in Hindi text-to-speech
svnthesis. International Journal of Speech Technologv, 7(4):319-333.
Josef Novak, Dong Yang, Nobuaki Minematsu, and Keikichi Hirose. 2011. Initial evaluations of an open
source WFST-based phoneticizer. The Universitv of Tokvo, Tokvo Institute of Technologv.
Brian Roark, Richard Sproat, Cvril Allauzen, Michael Rilev, Jeffrev Sorensen, and Terrv Tai. 2012. The
OpenGrm open-source finite-state grammar software libraries. In Proceedings of the ACL 2012 Svstem
Demonstrations, pages 61-66, Jeju Island, Korea, Julv. Association for Computational Linguistics.
Thoudam Doren Singh. 2012. Bidirectional bengali script and meetei mavek transliteration of web based
manipuri news corpus. In the Proceedings of the 3rd Workshop on South and Southeast Asian Natural
Language Processing (SANLP) of COLING, pages 181-189.
Terrv Tai, Wojciech Skut, and Richard Sproat. 2011. Thrax: An open source grammar compiler built
on openfst. ASRU.
</reference>
<page confidence="0.999332">
19
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.487643">
<title confidence="0.989984">Konkanverter - A Finite State Transducer based Machine Transliteration Engine for Konkani Language</title>
<author confidence="0.938297">Vinodh</author>
<affiliation confidence="0.855343">School of Computer Universitv of St Scotland,</affiliation>
<email confidence="0.993467">vrs30st—andrews.ac.uk</email>
<abstract confidence="0.99250125">have developed a finite state transducer based transliteration engine called Konkanperforms statistical machine transliteration between three different scripts used to write the Konkani language. The statistical machine transliteration svstem consists of cascading finite state transducers combining both rule-based and statistical approaches. Based on the limited availabilitv of parallel corpora, this cascading approach is found to perform significantlv better than a pure rule-based approach or pure statistical approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cvril Allauzen</author>
<author>Michael Rilev</author>
<author>Johan Schalkwvk</author>
<author>Wojciech Skut</author>
<author>Mehrvar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer librarv.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<publisher>Springer. http://www.openfst.org.</publisher>
<contexts>
<context position="8044" citStr="Allauzen et al., 2007" startWordPosition="1138" endWordPosition="1141">ilable on 12 Orthographv Word Count Devanagari - Kannada 23 187 Devanagari - Romi 38 550 Kannada - Romi 14 396 Table 2: Word count of parallel corpora request. We initiallv contemplated using an interlingua-like approach bv choosing Devanagari as the intermediate script, thus reducing the need for a dedicated Kannada-Romi transliteration module and parallel corpus. However, we were skeptical of the error propagation that might occur if the conversion to the intermediate script itself is not verv accurate. 4 Architecture The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007). Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst. Thrax was also used to define finite state acceptors. We found Thrax to be particularlv robust and flexible in generating various FSTs. Character alignments were performed using Phonetisaurus (Novak et al., 2011). N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs. Below, we describe the detailed architecture for each transliteration pair. 4.1 Devanagari to Kannada Input </context>
</contexts>
<marker>Allauzen, Rilev, Schalkwvk, Skut, Mohri, 2007</marker>
<rawString>Cvril Allauzen, Michael Rilev, Johan Schalkwvk, Wojciech Skut, and Mehrvar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer librarv. In Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA 2007), volume 4783 of Lecture Notes in Computer Science, pages 11-23. Springer. http://www.openfst.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mansur Arbabi</author>
<author>Scott M Fischthal</author>
<author>Vincent C Cheng</author>
<author>Elizabeth Bart</author>
</authors>
<title>Algorithms for Arabic name transliteration.</title>
<date>1994</date>
<journal>IBM Journal of research and Development,</journal>
<pages>38--2</pages>
<contexts>
<context position="4295" citStr="Arbabi et al., 1994" startWordPosition="597" endWordPosition="600">XygZpIbr devanagari dez-&apos;aule devnagari devnagri OQG} jhatako 4L32s jhatko zhottko qXg4Ly divamce so � dimvce dinvche XygpXayZ devadavena z3mdaba-&apos; devadaven devadoien jøygaĘap sattevavalva ;d4��u�� sattevavlva sat&apos;tevoilea Table 1: Sample Konkani words in various orthographies 2 Related Work Machine transliteration frequentlv occurs within machine translation when either named entities or out of vocabularv (OOV) words are encountered. Machine transliteration is also useful for cross-language information retrieval (CLIR). Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998). Machine transliteration can generallv be classified as rule-based or statistical depending on the approach. Rule-based transliteration is tvpicallv performed through hand-crafted rules and is usuallv graphemic in nature. Within Indic transliteration, there have been several attempts on rule-based approaches. Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT). It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script. Similarlv, Kishorjit</context>
</contexts>
<marker>Arbabi, Fischthal, Cheng, Bart, 1994</marker>
<rawString>Mansur Arbabi, Scott M Fischthal, Vincent C Cheng, and Elizabeth Bart. 1994. Algorithms for Arabic name transliteration. IBM Journal of research and Development, 38(2):183-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manoj Kumar Chinnakotla</author>
<author>Om P Damani</author>
</authors>
<title>Experiences with English-Hindi, English-Tamil and English-Kannada transliteration tasks at news</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,</booktitle>
<pages>44--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Chinnakotla, Damani, 2009</marker>
<rawString>Manoj Kumar Chinnakotla and Om P Damani. 2009. Experiences with English-Hindi, English-Tamil and English-Kannada transliteration tasks at news 2009. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, pages 44-47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Galescu</author>
<author>James F Allen</author>
</authors>
<title>Pronunciation of proper names with a joint n-gram model for bi-directional grapheme-to-phoneme conversion.</title>
<date>2002</date>
<booktitle>In INTERSPEECH. Ministrv of Home affairs Gov. of India.</booktitle>
<pages>2014--05</pages>
<contexts>
<context position="17888" citStr="Galescu and Allen, 2002" startWordPosition="2823" endWordPosition="2826"> r}r a}o t}t a}a s}s a}a r}r k}k v}e a}a bhiveli I bhieli → bh}blh ilv}i e}e l}l i}i Where } denotes individual character alignment, I between characters indicates grapheme chunks, and d}dld implies that the source grapheme ed» is mapped to the target graphemic chunk edd». &lt;s&gt;:&lt;s&gt; c:c i:i k:k a:a t:t 0 1 2 3 4 5 6 ✓:✓ u:u n:n 9 8 n:n a:a &lt;\s&gt;:&lt;\s&gt; 10 11 7 u:u 8 &lt;\s&gt;:&lt;\s&gt; t:t 6 ⇣:⇣ &lt;s&gt;:&lt;s&gt; v:v a:a s:s 0 1 2 3 4 a:a 9 ⇣:⇣ 5 t:t 7 &lt;\s&gt;:&lt;\s&gt; &lt;\s&gt;:&lt;\s&gt; 11 u:u 10 16 Figure 5: 5 best paths of Lp for Kannada input 26Àf aikv This alignment lattice was then used to create a joint sequence n-gram model (Galescu and Allen, 2002) Nknrm. This is then composed with the input word I, whose output projection we use. We also modifv the resulting transducer bv removing edges with grapheme chunks and replacing it with succeeding edges with the individual graphemes of the chunk. This is necessarv for later operations. Some graphemic sequences such as geminate vowel graphemes aa, ii etc. do not occur in the orthographv. We construct a transducer Arm, which accepts onlv paths with standard graphemic sequences. Thus effectivelv creating a pruned lattice Lp. Lp = 72(I o Nknrm) o Arm We created a Romi lexical acceptor ALrm which i</context>
</contexts>
<marker>Galescu, Allen, 2002</marker>
<rawString>Lucian Galescu and James F Allen. 2002. Pronunciation of proper names with a joint n-gram model for bi-directional grapheme-to-phoneme conversion. In INTERSPEECH. Ministrv of Home affairs Gov. of India. 2001. Abstract of speakers&apos; strength of languages and mother tongues - 2001. http://www.censusindia.gov.in/Census_Data_2001/Census_Data_ Online/Language/Statementi.aspx. Accessed: 2014-05-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuxiang Jia</author>
<author>Danqing Zhu</author>
<author>Shiwen Yu</author>
</authors>
<title>A noisv channel model for grapheme-based machine transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,</booktitle>
<pages>88--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5263" citStr="Jia et al (2009)" startWordPosition="722" endWordPosition="725">008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT). It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script. Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script. On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data. Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool. Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses. Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM). Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration. A hvbrid approach combining FSM based tech</context>
</contexts>
<marker>Jia, Zhu, Yu, 2009</marker>
<rawString>Yuxiang Jia, Danqing Zhu, and Shiwen Yu. 2009. A noisv channel model for grapheme-based machine transliteration. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, pages 88-91. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Grzegorz Kondrak</author>
<author>Tarek Sherif</author>
</authors>
<title>Applving manv-to-manv alignments and hidden markov models to letter-to-phoneme conversion.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<volume>7</volume>
<pages>372--379</pages>
<contexts>
<context position="17022" citStr="Jiampojamarn et al., 2007" startWordPosition="2654" endWordPosition="2657"> previous transliterations, we romanized the input. Since Romi and Kannada share different graphemic sets, we proceed with performing a character alignment with a Kannada-Romi parallel wordlist. Romanizing the input verv marginallv improves the character alignment process, since both input and output are then alphabetic scripts (as compared to svllabic to alphabetic alignment). We initiallv experimented with tools such as GIZA++, but found Phonetisaurus produced better alignments compared to other tools as it uses manv-to-manv alignments developed specificallv for grapheme to phoneme svstems (Jiampojamarn et al., 2007). A sample alignment sequence from Phonetisaurus is given below: melillem I mellil&apos;lem → m}m e}e 1}lel i}i l}l1&apos; l}l e}e m}m gadvemtlvan I gaddientlean → g}g a}a d}dld v}i e}e m}n t}t l}l v}e a}a n}n bharatasarkva I bharotasarkea → bh}blh a}a r}r a}o t}t a}a s}s a}a r}r k}k v}e a}a bhiveli I bhieli → bh}blh ilv}i e}e l}l i}i Where } denotes individual character alignment, I between characters indicates grapheme chunks, and d}dld implies that the source grapheme ed» is mapped to the target graphemic chunk edd». &lt;s&gt;:&lt;s&gt; c:c i:i k:k a:a t:t 0 1 2 3 4 5 6 ✓:✓ u:u n:n 9 8 n:n a:a &lt;\s&gt;:&lt;\s&gt; 10 11 7 </context>
</contexts>
<marker>Jiampojamarn, Kondrak, Sherif, 2007</marker>
<rawString>Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek Sherif. 2007. Applving manv-to-manv alignments and hidden markov models to letter-to-phoneme conversion. In HLT-NAACL, volume 7, pages 372-379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kishorjit</author>
</authors>
<title>Manipuri transliteration from Bengali script to Meitei Mavek: A rule based approach, c. singh et al.(eds.):</title>
<date>2011</date>
<booktitle>Icisil 2011, ccis</booktitle>
<volume>139</volume>
<pages>2</pages>
<contexts>
<context position="4902" citStr="Kishorjit (2011)" startWordPosition="677" endWordPosition="678">al., 1994; Knight and Graehl, 1998). Machine transliteration can generallv be classified as rule-based or statistical depending on the approach. Rule-based transliteration is tvpicallv performed through hand-crafted rules and is usuallv graphemic in nature. Within Indic transliteration, there have been several attempts on rule-based approaches. Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT). It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script. Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script. On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data. Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool. Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses. Simi</context>
</contexts>
<marker>Kishorjit, 2011</marker>
<rawString>N Kishorjit. 2011. Manipuri transliteration from Bengali script to Meitei Mavek: A rule based approach, c. singh et al.(eds.): Icisil 2011, ccis vol. 139, part 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="4321" citStr="Knight and Graehl, 1998" startWordPosition="601" endWordPosition="604">ez-&apos;aule devnagari devnagri OQG} jhatako 4L32s jhatko zhottko qXg4Ly divamce so � dimvce dinvche XygpXayZ devadavena z3mdaba-&apos; devadaven devadoien jøygaĘap sattevavalva ;d4��u�� sattevavlva sat&apos;tevoilea Table 1: Sample Konkani words in various orthographies 2 Related Work Machine transliteration frequentlv occurs within machine translation when either named entities or out of vocabularv (OOV) words are encountered. Machine transliteration is also useful for cross-language information retrieval (CLIR). Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998). Machine transliteration can generallv be classified as rule-based or statistical depending on the approach. Rule-based transliteration is tvpicallv performed through hand-crafted rules and is usuallv graphemic in nature. Within Indic transliteration, there have been several attempts on rule-based approaches. Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT). It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script. Similarlv, Kishorjit (2011) developed a rule-b</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599-612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Abbas Malik</author>
<author>Christian Boitet</author>
<author>Pushpak Bhattacharvva</author>
</authors>
<title>Hindi Urdu machine transliteration using finite-state transducers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>537--544</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4651" citStr="Malik et al (2008)" startWordPosition="645" endWordPosition="648">ation when either named entities or out of vocabularv (OOV) words are encountered. Machine transliteration is also useful for cross-language information retrieval (CLIR). Consequentlv, a significant amount of work has been done in this field (Arbabi et al., 1994; Knight and Graehl, 1998). Machine transliteration can generallv be classified as rule-based or statistical depending on the approach. Rule-based transliteration is tvpicallv performed through hand-crafted rules and is usuallv graphemic in nature. Within Indic transliteration, there have been several attempts on rule-based approaches. Malik et al (2008) implemented a Hindi-Urdu transliteration svstem with finite-state transducers using a universal intermediate transcription (UIT). It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script. Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script. On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data. Jia </context>
</contexts>
<marker>Malik, Boitet, Bhattacharvva, 2008</marker>
<rawString>M. G. Abbas Malik, Christian Boitet, and Pushpak Bhattacharvva. 2008. Hindi Urdu machine transliteration using finite-state transducers. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 537-544. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Abbas Malik</author>
<author>Laurent Besacier</author>
<author>Christian Boitet</author>
<author>Pushpak Bhattacharvva</author>
</authors>
<title>A hvbrid model for Urdu Hindi transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration,</booktitle>
<pages>177--185</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5967" citStr="Malik et al (2009)" startWordPosition="824" endWordPosition="827">s, an SMT tool. Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses. Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM). Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration. A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009). 3 Initial Attempts Konkani, being a minor language did not have anv parallel corpora that could have been harnessed for statistical machine transliteration. The World Konkani Center, Mangalore had attempted to manuallv mine transliteration rules bv studving the three orthographies and analvzing the differences. Thev also developed schwa deletion and schwa insertion rules for the orthographies, modeled on Hindi schwa deletion rules. For the initial machine transliteration svstem, we refined and improved upon these rules and implemented a rule-based transliteration svstem. In the absence of co</context>
</contexts>
<marker>Malik, Besacier, Boitet, Bhattacharvva, 2009</marker>
<rawString>M. G. Abbas Malik, Laurent Besacier, Christian Boitet, and Pushpak Bhattacharvva. 2009. A hvbrid model for Urdu Hindi transliteration. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, pages 177-185. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Abbas Malik</author>
<author>Christian Boitet</author>
<author>Laurent Besacier</author>
<author>Pushpak Bhattcharvva</author>
</authors>
<title>Urdu Hindi machine transliteration using SMT.</title>
<date>2013</date>
<booktitle>In the Proceedings of the 4th Workshop on South and Southeast Asian Natural Language Processing, International Joint Conference on Natural Language Processing,</booktitle>
<pages>43--57</pages>
<contexts>
<context position="5383" citStr="Malik et al (2013)" startWordPosition="742" endWordPosition="745">scription (UIT). It was based on the graphemic equivalence between Perso-Arabic script and Devanagari script. Similarlv, Kishorjit (2011) developed a rule-based transliteration svstem also based on direct graphemic correspondence between Meetei Mavek and Bengali script. On the other hand, statistical machine transliteration svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data. Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool. Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses. Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM). Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration. A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009). 3 Initial Atte</context>
</contexts>
<marker>Malik, Boitet, Besacier, Bhattcharvva, 2013</marker>
<rawString>M. G. Abbas Malik, Christian Boitet, Laurent Besacier, and Pushpak Bhattcharvva. 2013. Urdu Hindi machine transliteration using SMT. In the Proceedings of the 4th Workshop on South and Southeast Asian Natural Language Processing, International Joint Conference on Natural Language Processing, pages 43--57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bhuvana Narasimhan</author>
<author>Richard Sproat</author>
<author>George Kiraz</author>
</authors>
<title>Schwa-deletion in Hindi text-to-speech svnthesis.</title>
<date>2004</date>
<journal>International Journal of Speech Technologv,</journal>
<pages>7--4</pages>
<contexts>
<context position="10584" citStr="Narasimhan et al., 2004" startWordPosition="1559" endWordPosition="1562">ar expressions, these are defined as: 8 &lt;-- C * VD? , 8−a &lt;-- C * (V − a)D? , 8nj &lt;-- Cnj?CVD? , 8nj−a &lt;-- Cnj?C(V − a)D? The initial word boundarv Bi and final word boundarv Bf are: Bi &lt;-- M|si and Bf &lt;-- M|sf. The general schwa deletion rules have been given below as context dependent rewrite rules. These rules are expressed in the form φ -4 ψ/λ��ρ. Here, φ is replaced bv ψ whenever it is preceded bv λ and succeeded bv ρ, λ and ρ being the left and right contexts respectivelv. The rules below are listed with a corresponding sample case. These rules were compiled as finite-state transducers (Narasimhan et al., 2004) We have produced a list of possible suffixes and prefixes from the collected corpus. Let Pre denote the set of prefixes and 8uf the set of prefixes. We then mark the morphemic boundarv 13 M as: Bm = E → M/Pre��Suf The schwa deletion rules can be effectivelv summarized as follows. Wf = a → E/(BiS + C+)��Bf |asata → asat W3 = a → E/(BiSC)��(S_aBf) |amvado → amvdo W3vy = a → E/(BiS(y|v)) (SBf) |pavasa → pavs W4 = a → E/(BiSC)��(SnjSBf) |adakati → adkati W4y = a → E/(BiSSy)��(SnjBf) |ulavatam → ulavtam W43 = a → E/(BiSS_aC)��(SnjBf) |vicarata → vicarta WP2 = a → E/(BiSC)��(SnjSS + Bf) |vacatakuca</context>
</contexts>
<marker>Narasimhan, Sproat, Kiraz, 2004</marker>
<rawString>Bhuvana Narasimhan, Richard Sproat, and George Kiraz. 2004. Schwa-deletion in Hindi text-to-speech svnthesis. International Journal of Speech Technologv, 7(4):319-333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Novak</author>
<author>Dong Yang</author>
<author>Nobuaki Minematsu</author>
<author>Keikichi Hirose</author>
</authors>
<title>Initial evaluations of an open source WFST-based phoneticizer. The Universitv of Tokvo, Tokvo Institute of Technologv.</title>
<date>2011</date>
<contexts>
<context position="8413" citStr="Novak et al., 2011" startWordPosition="1192" endWordPosition="1195">ver, we were skeptical of the error propagation that might occur if the conversion to the intermediate script itself is not verv accurate. 4 Architecture The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007). Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst. Thrax was also used to define finite state acceptors. We found Thrax to be particularlv robust and flexible in generating various FSTs. Character alignments were performed using Phonetisaurus (Novak et al., 2011). N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs. Below, we describe the detailed architecture for each transliteration pair. 4.1 Devanagari to Kannada Input text was first converted into an intermediate romanized encoding, where schwa is explicitlv denoted. This also results in converting the script from svllabic to alphabetic form. This eases defining rules to a considerable extent, since independent vowels and dependent vowel signs need not be dealt with separatelv. We have used a customized encoding which onlv uses mo</context>
</contexts>
<marker>Novak, Yang, Minematsu, Hirose, 2011</marker>
<rawString>Josef Novak, Dong Yang, Nobuaki Minematsu, and Keikichi Hirose. 2011. Initial evaluations of an open source WFST-based phoneticizer. The Universitv of Tokvo, Tokvo Institute of Technologv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Richard Sproat</author>
<author>Cvril Allauzen</author>
<author>Michael Rilev</author>
<author>Jeffrev Sorensen</author>
<author>Terrv Tai</author>
</authors>
<title>The OpenGrm open-source finite-state grammar software libraries.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 Svstem Demonstrations,</booktitle>
<pages>61--66</pages>
<institution>Jeju Island, Korea, Julv. Association for Computational Linguistics.</institution>
<contexts>
<context position="8493" citStr="Roark et al., 2012" startWordPosition="1205" endWordPosition="1208">on to the intermediate script itself is not verv accurate. 4 Architecture The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007). Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst. Thrax was also used to define finite state acceptors. We found Thrax to be particularlv robust and flexible in generating various FSTs. Character alignments were performed using Phonetisaurus (Novak et al., 2011). N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs. Below, we describe the detailed architecture for each transliteration pair. 4.1 Devanagari to Kannada Input text was first converted into an intermediate romanized encoding, where schwa is explicitlv denoted. This also results in converting the script from svllabic to alphabetic form. This eases defining rules to a considerable extent, since independent vowels and dependent vowel signs need not be dealt with separatelv. We have used a customized encoding which onlv uses monographs and hence maintains a direct grapheme-to-grapheme correspondence with I</context>
</contexts>
<marker>Roark, Sproat, Allauzen, Rilev, Sorensen, Tai, 2012</marker>
<rawString>Brian Roark, Richard Sproat, Cvril Allauzen, Michael Rilev, Jeffrev Sorensen, and Terrv Tai. 2012. The OpenGrm open-source finite-state grammar software libraries. In Proceedings of the ACL 2012 Svstem Demonstrations, pages 61-66, Jeju Island, Korea, Julv. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thoudam Doren Singh</author>
</authors>
<title>Bidirectional bengali script and meetei mavek transliteration of web based manipuri news corpus.</title>
<date>2012</date>
<booktitle>In the Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP) of COLING,</booktitle>
<pages>181--189</pages>
<contexts>
<context position="5703" citStr="Singh (2012)" startWordPosition="789" endWordPosition="790">svstems tvpicallv use procedures familiar from statistical machine translation, including character alignments and subsequent training on the aligned data. Jia et al (2009) also developed a noisv channel model for the English-Chinese language pair using Moses, an SMT tool. Malik et al (2013) evaluated 28 different kinds of statistical models for Hindi-Urdu machine transliteration using GIZA++ and Moses. Similarlv, Chinnakotla et al (2009) used the same tools for three language pairs - English-Hindi, English-Tamil and English-Kannada, focusing on fine-tuning the character sequence model (CSM). Singh (2012) evaluated both rule-based and statistical methods for bidirectional Bengali script and Meetei Mavek transliteration. A hvbrid approach combining FSM based techniques with a statistical word language model with better performance was proposed bv Malik et al (2009). 3 Initial Attempts Konkani, being a minor language did not have anv parallel corpora that could have been harnessed for statistical machine transliteration. The World Konkani Center, Mangalore had attempted to manuallv mine transliteration rules bv studving the three orthographies and analvzing the differences. Thev also developed s</context>
</contexts>
<marker>Singh, 2012</marker>
<rawString>Thoudam Doren Singh. 2012. Bidirectional bengali script and meetei mavek transliteration of web based manipuri news corpus. In the Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing (SANLP) of COLING, pages 181-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terrv Tai</author>
<author>Wojciech Skut</author>
<author>Richard Sproat</author>
</authors>
<title>Thrax: An open source grammar compiler built on openfst.</title>
<date>2011</date>
<publisher>ASRU.</publisher>
<contexts>
<context position="8070" citStr="Tai et al., 2011" startWordPosition="1143" endWordPosition="1146">ount Devanagari - Kannada 23 187 Devanagari - Romi 38 550 Kannada - Romi 14 396 Table 2: Word count of parallel corpora request. We initiallv contemplated using an interlingua-like approach bv choosing Devanagari as the intermediate script, thus reducing the need for a dedicated Kannada-Romi transliteration module and parallel corpus. However, we were skeptical of the error propagation that might occur if the conversion to the intermediate script itself is not verv accurate. 4 Architecture The implementation of the framework has been done entirelv using OpenFst (Allauzen et al., 2007). Thrax (Tai et al., 2011) was used to define context-dependent rewrite rules and compile those rules into finite-state transducers compatible with Openfst. Thrax was also used to define finite state acceptors. We found Thrax to be particularlv robust and flexible in generating various FSTs. Character alignments were performed using Phonetisaurus (Novak et al., 2011). N-gram models were created with the OpenGrm Ngram librarv (Roark et al., 2012), which also generates the models as FSTs. Below, we describe the detailed architecture for each transliteration pair. 4.1 Devanagari to Kannada Input text was first converted i</context>
</contexts>
<marker>Tai, Skut, Sproat, 2011</marker>
<rawString>Terrv Tai, Wojciech Skut, and Richard Sproat. 2011. Thrax: An open source grammar compiler built on openfst. ASRU.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
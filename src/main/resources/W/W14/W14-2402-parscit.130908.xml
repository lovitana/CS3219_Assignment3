<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.106098">
<title confidence="0.988119">
Semantic Parsing using Distributional Semantics and Probabilistic Logic
</title>
<author confidence="0.997566">
Islam Beltagy§ Katrin Erk† Raymond Mooney§
</author>
<affiliation confidence="0.998808333333333">
§Department of Computer Science
†Department of Linguistics
The University of Texas at Austin
</affiliation>
<address confidence="0.755536">
Austin, Texas 78712
</address>
<email confidence="0.995855">
§{beltagy,mooney}@cs.utexas.edu
†katrin.erk@mail.utexas.edu
</email>
<sectionHeader confidence="0.993864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856705882353">
We propose a new approach to semantic
parsing that is not constrained by a fixed
formal ontology and purely logical infer-
ence. Instead, we use distributional se-
mantics to generate only the relevant part
of an on-the-fly ontology. Sentences and
the on-the-fly ontology are represented in
probabilistic logic. For inference, we
use probabilistic logic frameworks like
Markov Logic Networks (MLN) and Prob-
abilistic Soft Logic (PSL). This seman-
tic parsing approach is evaluated on two
tasks, Textual Entitlement (RTE) and Tex-
tual Similarity (STS), both accomplished
using inference in probabilistic logic. Ex-
periments show the potential of the ap-
proach.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998744375">
Semantic Parsing is probably best defined as the
task of representing the meaning of a natural lan-
guage sentence in some formal knowledge repre-
sentation language that supports automated infer-
ence. A semantic parser is best defined as having
three parts, a formal language, an ontology, and an
inference mechanism. Both the formal language
(e.g. first-order logic) and the ontology define the
formal knowledge representation. The formal lan-
guage uses predicate symbols from the ontology,
and the ontology provides them with meanings by
defining the relations between them.1. A formal
expression by itself without an ontology is insuf-
ficient for semantic interpretation; we call it un-
interpreted logical form. An uninterpreted logical
form is not enough as a knowledge representation
</bodyText>
<footnote confidence="0.94636925">
1For conciseness, here we use the term “ontology” to refer
to a set of predicates as well as a knowledge base (KB) of
axioms that defines a complex set of relationships between
them
</footnote>
<bodyText confidence="0.999871682926829">
because the predicate symbols do not have mean-
ing in themselves, they get this meaning from the
ontology. Inference is what takes a problem repre-
sented in the formal knowledge representation and
the ontology and performs the target task (e.g. tex-
tual entailment, question answering, etc.).
Prior work in standard semantic parsing uses a
pre-defined set of predicates in a fixed ontology.
However, it is difficult to construct formal ontolo-
gies of properties and relations that have broad
coverage, and very difficult to do semantic parsing
based on such an ontology. Consequently, current
semantic parsers are mostly restricted to fairly lim-
ited domains, such as querying a specific database
(Kwiatkowski et al., 2013; Berant et al., 2013).
We propose a semantic parser that is not re-
stricted to a predefined ontology. Instead, we
use distributional semantics to generate the needed
part of an on-the-fly ontology. Distributional se-
mantics is a statistical technique that represents
the meaning of words and phrases as distributions
over context words (Turney and Pantel, 2010; Lan-
dauer and Dumais, 1997). Distributional infor-
mation can be used to predict semantic relations
like synonymy and hyponymy between words and
phrases of interest (Lenci and Benotto, 2012;
Kotlerman et al., 2010). The collection of pre-
dicted semantic relations is the “on-the-fly ontol-
ogy” our semantic parser uses. A distributional
semantics is relatively easy to build from a large
corpus of raw text, and provides the wide cover-
age that formal ontologies lack.
The formal language we would like to use in the
semantic parser is first-order logic. However, dis-
tributional information is graded in nature, so the
on-the-fly ontology and its predicted semantic re-
lations are also graded. This means, that standard
first-order logic is insufficient because it is binary
by nature. Probabilistic logic solves this problem
because it accepts weighted first order logic for-
mulas. For example, in probabilistic logic, the
</bodyText>
<page confidence="0.986665">
7
</page>
<bodyText confidence="0.927313107142857">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7–11,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
synonymy relation between “man” and “guy” is
represented by: bx. man(x) &lt;---&gt; guy(x)  |w1 and
the hyponymy relation between “car” and “vehi-
cle” is: bx. car(x) =&gt; vehicle(x)  |w2 where w1
and w1 are some certainty measure estimated from
the distributional semantics.
For inference, we use probabilistic logic
frameworks like Markov Logic Networks
(MLN) (Richardson and Domingos, 2006) and
Probabilistic Soft Logic (PSL) (Kimmig et al.,
2012). They are Statistical Relational Learning
(SRL) techniques (Getoor and Taskar, 2007) that
combine logical and statistical knowledge in one
uniform framework, and provide a mechanism for
coherent probabilistic inference. We implemented
this semantic parser (Beltagy et al., 2013; Beltagy
et al., 2014) and used it to perform two tasks
that require deep semantic analysis, Recognizing
Textual Entailment (RTE), and Semantic Textual
Similarity (STS).
The rest of the paper is organized as follows:
section 2 presents background material, section
3 explains the three components of the semantic
parser, section 4 shows how this semantic parser
can be used for RTE and STS tasks, section 5
presents the evaluation and 6 concludes.
</bodyText>
<sectionHeader confidence="0.993724" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999142">
2.1 Logical Semantics
</subsectionHeader>
<bodyText confidence="0.999445588235294">
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle “graded” aspects of meaning in language
because they are binary by nature. Also, the logi-
cal predicates and relations do not have semantics
by themselves without an accompanying ontology,
which we want to replace in our semantic parser
with distributional semantics.
To map a sentence to logical form, we use Boxer
(Bos, 2008), a tool for wide-coverage semantic
analysis that produces uninterpreted logical forms
using Discourse Representation Structures (Kamp
and Reyle, 1993). It builds on the C&amp;C CCG
parser (Clark and Curran, 2004).
</bodyText>
<subsectionHeader confidence="0.997969">
2.2 Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.999962391304348">
Distributional models use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Turney and Pantel,
2010; Mitchell and Lapata, 2010), based on the
observation that semantically similar words occur
in similar contexts (Landauer and Dumais, 1997;
Lund and Burgess, 1996). So words can be rep-
resented as vectors in high dimensional spaces
generated from the contexts in which they occur.
Distributional models capture the graded nature
of meaning, but do not adequately capture log-
ical structure (Grefenstette, 2013). It is possi-
ble to compute vector representations for larger
phrases compositionally from their parts (Lan-
dauer and Dumais, 1997; Mitchell and Lapata,
2008; Mitchell and Lapata, 2010; Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011). Distributional similarity is usually a mix-
ture of semantic relations, but particular asymmet-
ric similarity measures can, to a certain extent,
predict hypernymy and lexical entailment distri-
butionally (Lenci and Benotto, 2012; Kotlerman
et al., 2010).
</bodyText>
<subsectionHeader confidence="0.998769">
2.3 Markov Logic Network
</subsectionHeader>
<bodyText confidence="0.999805571428571">
Markov Logic Network (MLN) (Richardson and
Domingos, 2006) is a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
MLNs define a probability distribution over possi-
ble worlds, where a world’s probability increases
exponentially with the total weight of the logical
clauses that it satisfies. A variety of inference
methods for MLNs have been developed, however,
their computational complexity is a fundamental
issue.
</bodyText>
<subsectionHeader confidence="0.997907">
2.4 Probabilistic Soft Logic
</subsectionHeader>
<bodyText confidence="0.999694071428571">
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms have soft, continuous
truth values in the interval [0, 1] rather than bi-
nary truth values as used in MLNs and most other
probabilistic logics. Given a set of weighted in-
ference rules, and with the help of Lukasiewicz’s
relaxation of the logical operators, PSL builds a
graphical model defining a probability distribution
</bodyText>
<page confidence="0.989539">
8
</page>
<bodyText confidence="0.998643375">
over the continuous space of values of the random
variables in the model. Then, PSL’s MPE infer-
ence (Most Probable Explanation) finds the over-
all interpretation with the maximum probability
given a set of evidence. It turns out that this op-
timization problem is second-order cone program
(SOCP) (Kimmig et al., 2012) and can be solved
efficiently in polynomial time.
</bodyText>
<subsectionHeader confidence="0.995486">
2.5 Recognizing Textual Entailment
</subsectionHeader>
<bodyText confidence="0.99793025">
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or not related
(Neutral) to another, the hypothesis.
</bodyText>
<subsectionHeader confidence="0.986977">
2.6 Semantic Textual Similarity
</subsectionHeader>
<bodyText confidence="0.999586857142857">
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system’s output and
gold standard scores.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.9998725">
A semantic parser is three components, a formal
language, an ontology, and an inference mecha-
nism. This section explains the details of these
components in our semantic parser. It also points
out the future work related to each part of the sys-
tem.
</bodyText>
<subsectionHeader confidence="0.984666">
3.1 Formal Language: first-order logic
</subsectionHeader>
<bodyText confidence="0.999960666666667">
Natural sentences are mapped to logical form us-
ing Boxer (Bos, 2008), which maps the input
sentences into a lexically-based logical form, in
which the predicates are words in the sentence.
For example, the sentence “A man is driving a car”
in logical form is:
</bodyText>
<equation confidence="0.957683">
∃x, y, z. man(x) ∧ agent(y, x) ∧ drive(y) ∧
patient(y, z) ∧ car(z)
</equation>
<bodyText confidence="0.999836857142857">
We call Boxer’s output alone an uninterpreted
logical form because predicates do not have mean-
ing by themselves. They still need to be connected
with an ontology.
Future work: While Boxer has wide coverage,
additional linguistic phenomena like generalized
quantifiers need to be handled.
</bodyText>
<subsectionHeader confidence="0.975103">
3.2 Ontology: on-the-fly ontology
</subsectionHeader>
<bodyText confidence="0.999711466666667">
Distributional information is used to generate the
needed part of an on-the-fly ontology for the given
input sentences. It is encoded in the form of
weighted inference rules describing the seman-
tic relations connecting words and phrases in the
input sentences. For example, for sentences “A
man is driving a car”, and “A guy is driving a
vehicle”, we would like to generate rules like
∀x. man(x) ⇔ guy(x)  |w1 indicating that “man”
and “guy” are synonyms with some certainty w1,
and ∀x. car(x) ⇒ vehicle(x)  |w2 indicating that
“car” is a hyponym of “vehicle” with some cer-
tainty w2. Other semantic relations can also be
easily encoded as inference rules like antonyms
∀x. tall(x) ⇔ ¬short(x)  |w, contextonymy rela-
tion ∀x. hospital(x) ⇒ ∃y. doctor(y)  |w. For
now, we generate inference rules only as syn-
onyms (Beltagy et al., 2013), but we are experi-
menting with more types of semantic relations.
In (Beltagy et al., 2013), we generate infer-
ence rules between all pairs of words and phrases.
Given two input sentences T and H, for all pairs
(a, b), where a and b are words or phrases of T
and H respectively, generate an inference rule:
a → b  |w, where the rule’s weight w =
sim(−→a , →−b ), and sim is the cosine of the angle
between vectors →−a and →−b . Note that this simi-
larity measure cannot yet distinguish relations like
synonymy and hypernymy. Phrases are defined in
terms of Boxer’s output to be more than one unary
atom sharing the same variable like “a little kid”
which in logic is little(k) ∧ kid(k), or two unary
atoms connected by a relation like “a man is driv-
ing” which in logic is man(m) ∧ agent(d, m) ∧
drive(d). We used vector addition (Mitchell and
Lapata, 2010) to calculate vectors for phrases.
Future Work: This can be extended in many
directions. We are currently experimenting with
asymmetric similarity functions to distinguish se-
mantic relations. We would also like to use longer
phrases and other compositionality techniques as
in (Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011). Also more inference rules can
be added from paraphrases collections like PPDB
(Ganitkevitch et al., 2013).
</bodyText>
<subsectionHeader confidence="0.968188">
3.3 Inference: probabilistic logical inference
</subsectionHeader>
<bodyText confidence="0.999886">
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
</bodyText>
<page confidence="0.995296">
9
</page>
<bodyText confidence="0.999892818181818">
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E, RB).
Probabilistic logic frameworks define a proba-
bility distribution over all possible worlds. The
number of constants in a world depends on the
number of the discourse entities in the Boxer out-
put, plus additional constants introduced to han-
dle quantification. Mostly, all constants are com-
bined with all literals, except for rudimentary type
checking.
</bodyText>
<sectionHeader confidence="0.997214" genericHeader="method">
4 Tasks
</sectionHeader>
<bodyText confidence="0.999344">
This section explains how we perform the RTE
and STS tasks using our semantic parser.
</bodyText>
<subsectionHeader confidence="0.986409">
4.1 Task 1: RTE using MLNs
</subsectionHeader>
<bodyText confidence="0.99996896">
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE’s classification prob-
lem for the relation between T and H, and given
the rule base RB generated as in 3.2, can be
split into two inference tasks. The first is find-
ing if T entails H, Pr(H|T, RB). The second
is finding if the negation of the text ¬T entails H,
Pr(H|¬T, RB). In case Pr(H|T, RB) is high,
while Pr(H|¬T, RB) is low, this indicates En-
tails. In case it is the other way around, this indi-
cates Contradicts. If both values are close to each
other, this means T does not affect probability of
H and that is an indication of Neutral. We train a
classifier to map the two values to the final classi-
fication decision.
Future Work: One general problem with
MLNs is its computational overhead especially
for the type of inference problems we have. The
other problem is that MLNs, as with most other
probabilistic logics, make the Domain Closure
Assumption (Richardson and Domingos, 2006)
which means that quantifiers sometimes behave in
an undesired way.
</bodyText>
<subsectionHeader confidence="0.992468">
4.2 Task 2: STS using PSL
</subsectionHeader>
<bodyText confidence="0.999741142857143">
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach to compute similarity between struc-
tured objects. PSL does not work “out of the
box” for STS, because Lukasiewicz’s equation for
the conjunction is very restrictive. We addressed
this problem (Beltagy et al., 2014) by replacing
</bodyText>
<table confidence="0.99714225">
SICK-RTE SICK-STS
dist 0.60 0.65
logic 0.71 0.68
logic+dist 0.73 0.70
</table>
<tableCaption confidence="0.999878">
Table 1: RTE accuracy and STS Correlation
</tableCaption>
<bodyText confidence="0.999900769230769">
Lukasiewicz’s equation for the conjunction with
an averaging equation, then change the optimiza-
tion problem and the grounding technique accord-
ingly.
For each STS pair of sentences 51, 52, we run
PSL twice, once where E = 51, Q = 52 and
another where E = 52, Q = 51, and output the
two scores. The final similarity score is produced
from a regressor trained to map the two PSL scores
to the overall similarity score.
Future Work: Use a weighted average where
different weights are learned for different parts of
the sentence.
</bodyText>
<sectionHeader confidence="0.994926" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999877285714286">
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014. The initial data
release for the competition consists of 5,000 pairs
of sentences which are annotated for both RTE and
STS. For this evaluation, we performed 10-fold
cross validation on this initial data.
Table 1 shows results comparing our full
approach (logic+dist) to two baselines, a
distributional-only baseline (dist) that uses vector
addition, and a probabilistic logic-only baseline
(logic) which is our semantic parser without distri-
butional inference rules. The integrated approach
(logic+dist) out-performs both baselines.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989416666667">
We presented an approach to semantic parsing that
has a wide-coverage for words and relations, and
does not require a fixed formal ontology. An
on-the-fly ontology of semantic relations between
predicates is derived from distributional informa-
tion and encoded in the form of soft inference rules
in probabilistic logic. We evaluated this approach
on two task, RTE and STS, using two probabilistic
logics, MLNs and PSL respectively. The semantic
parser can be extended in different direction, es-
pecially in predicting more complex semantic re-
lations, and enhancing the inference mechanisms.
</bodyText>
<page confidence="0.998038">
10
</page>
<sectionHeader confidence="0.998314" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995939375">
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999598787234043">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
L. Getoor and B. Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato’s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373–398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107–136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
</reference>
<page confidence="0.999477">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.130996">
<title confidence="0.90842">Semantic Parsing using Distributional Semantics and Probabilistic Logic</title>
<degree confidence="0.3833995">of Computer of The University of Texas at Austin, Texas</degree>
<abstract confidence="0.9800145">We propose a new approach to semantic parsing that is not constrained by a fixed formal ontology and purely logical inference. Instead, we use distributional semantics to generate only the relevant part of an on-the-fly ontology. Sentences and the on-the-fly ontology are represented in probabilistic logic. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) and Probabilistic Soft Logic (PSL). This semantic parsing approach is evaluated on two tasks, Textual Entitlement (RTE) and Textual Similarity (STS), both accomplished using inference in probabilistic logic. Experiments show the potential of the approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of Semantic Evaluation (SemEval-12).</booktitle>
<contexts>
<context position="9288" citStr="Agirre et al., 2012" startWordPosition="1411" endWordPosition="1414">erall interpretation with the maximum probability given a set of evidence. It turns out that this optimization problem is second-order cone program (SOCP) (Kimmig et al., 2012) and can be solved efficiently in polynomial time. 2.5 Recognizing Textual Entailment Recognizing Textual Entailment (RTE) is the task of determining whether one natural language text, the premise, Entails, Contradicts, or not related (Neutral) to another, the hypothesis. 2.6 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 Approach A semantic parser is three components, a formal language, an ontology, and an inference mechanism. This section explains the details of these components in our semantic parser. It also points out the future work related to each part of the system. 3.1 Formal Language: first-order logic Natural sentences are mapped to logical form using Boxer (Bos, 2008), which maps the input sentences into a lexically-based lo</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of Semantic Evaluation (SemEval-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-10).</booktitle>
<contexts>
<context position="6861" citStr="Baroni and Zamparelli, 2010" startWordPosition="1039" endWordPosition="1042">, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). 2.3 Markov Logic Network Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening th</context>
<context position="12398" citStr="Baroni and Zamparelli, 2010" startWordPosition="1940" endWordPosition="1943">re defined in terms of Boxer’s output to be more than one unary atom sharing the same variable like “a little kid” which in logic is little(k) ∧ kid(k), or two unary atoms connected by a relation like “a man is driving” which in logic is man(m) ∧ agent(d, m) ∧ drive(d). We used vector addition (Mitchell and Lapata, 2010) to calculate vectors for phrases. Future Work: This can be extended in many directions. We are currently experimenting with asymmetric similarity functions to distinguish semantic relations. We would also like to use longer phrases and other compositionality techniques as in (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Also more inference rules can be added from paraphrases collections like PPDB (Ganitkevitch et al., 2013). 3.3 Inference: probabilistic logical inference The last component is probabilistic logical inference. Given the logical form of the input sentences, and the weighted inference rules, we use them to build a probabilistic logic program whose solution is the answer to the target task. A probabilistic logic program consists of the evidence set 9 E, the set of weighted first order logical expressions (rule base RB), and a query Q. Inference is the process o</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Montague meets Markov: Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM-13).</booktitle>
<contexts>
<context position="4796" citStr="Beltagy et al., 2013" startWordPosition="725" endWordPosition="728"> hyponymy relation between “car” and “vehicle” is: bx. car(x) =&gt; vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle man</context>
<context position="11240" citStr="Beltagy et al., 2013" startWordPosition="1734" endWordPosition="1737">connecting words and phrases in the input sentences. For example, for sentences “A man is driving a car”, and “A guy is driving a vehicle”, we would like to generate rules like ∀x. man(x) ⇔ guy(x) |w1 indicating that “man” and “guy” are synonyms with some certainty w1, and ∀x. car(x) ⇒ vehicle(x) |w2 indicating that “car” is a hyponym of “vehicle” with some certainty w2. Other semantic relations can also be easily encoded as inference rules like antonyms ∀x. tall(x) ⇔ ¬short(x) |w, contextonymy relation ∀x. hospital(x) ⇒ ∃y. doctor(y) |w. For now, we generate inference rules only as synonyms (Beltagy et al., 2013), but we are experimenting with more types of semantic relations. In (Beltagy et al., 2013), we generate inference rules between all pairs of words and phrases. Given two input sentences T and H, for all pairs (a, b), where a and b are words or phrases of T and H respectively, generate an inference rule: a → b |w, where the rule’s weight w = sim(−→a , →−b ), and sim is the cosine of the angle between vectors →−a and →−b . Note that this similarity measure cannot yet distinguish relations like synonymy and hypernymy. Phrases are defined in terms of Boxer’s output to be more than one unary atom </context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. Montague meets Markov: Deep semantics with probabilistic logical form. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Probabilistic soft logic for semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL-14).</booktitle>
<contexts>
<context position="4819" citStr="Beltagy et al., 2014" startWordPosition="729" endWordPosition="732">ween “car” and “vehicle” is: bx. car(x) =&gt; vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phen</context>
<context position="14973" citStr="Beltagy et al., 2014" startWordPosition="2381" endWordPosition="2384">al overhead especially for the type of inference problems we have. The other problem is that MLNs, as with most other probabilistic logics, make the Domain Closure Assumption (Richardson and Domingos, 2006) which means that quantifiers sometimes behave in an undesired way. 4.2 Task 2: STS using PSL PSL is the probabilistic logic we use for the STS task since it has been shown to be an effective approach to compute similarity between structured objects. PSL does not work “out of the box” for STS, because Lukasiewicz’s equation for the conjunction is very restrictive. We addressed this problem (Beltagy et al., 2014) by replacing SICK-RTE SICK-STS dist 0.60 0.65 logic 0.71 0.68 logic+dist 0.73 0.70 Table 1: RTE accuracy and STS Correlation Lukasiewicz’s equation for the conjunction with an averaging equation, then change the optimization problem and the grounding technique accordingly. For each STS pair of sentences 51, 52, we run PSL twice, once where E = 51, Q = 52 and another where E = 52, Q = 51, and output the two scores. The final similarity score is produced from a regressor trained to map the two PSL scores to the overall similarity score. Future Work: Use a weighted average where different weight</context>
</contexts>
<marker>Beltagy, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Katrin Erk, and Raymond Mooney. 2014. Probabilistic soft logic for semantic textual similarity. In Proceedings of Association for Computational Linguistics (ACL-14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</booktitle>
<contexts>
<context position="2665" citStr="Berant et al., 2013" startWordPosition="401" endWordPosition="404">what takes a problem represented in the formal knowledge representation and the ontology and performs the target task (e.g. textual entailment, question answering, etc.). Prior work in standard semantic parsing uses a pre-defined set of predicates in a fixed ontology. However, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations i</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer.</title>
<date>2008</date>
<booktitle>In Proceedings of Semantics in Text Processing (STEP-08).</booktitle>
<contexts>
<context position="5849" citStr="Bos, 2008" startWordPosition="891" endWordPosition="892">udes. 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). 2.2 Distributional Semantics Distributional models use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in h</context>
<context position="9830" citStr="Bos, 2008" startWordPosition="1500" endWordPosition="1501"> of a pair of sentences on a scale from 1 to 5 (Agirre et al., 2012). Gold standard scores are averaged over multiple human annotations and systems are evaluated using the Pearson correlation between a system’s output and gold standard scores. 3 Approach A semantic parser is three components, a formal language, an ontology, and an inference mechanism. This section explains the details of these components in our semantic parser. It also points out the future work related to each part of the system. 3.1 Formal Language: first-order logic Natural sentences are mapped to logical form using Boxer (Bos, 2008), which maps the input sentences into a lexically-based logical form, in which the predicates are words in the sentence. For example, the sentence “A man is driving a car” in logical form is: ∃x, y, z. man(x) ∧ agent(y, x) ∧ drive(y) ∧ patient(y, z) ∧ car(z) We call Boxer’s output alone an uninterpreted logical form because predicates do not have meaning by themselves. They still need to be connected with an ontology. Future work: While Boxer has wide coverage, additional linguistic phenomena like generalized quantifiers need to be handled. 3.2 Ontology: on-the-fly ontology Distributional info</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with Boxer. In Proceedings of Semantics in Text Processing (STEP-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL-04).</booktitle>
<contexts>
<context position="6058" citStr="Clark and Curran, 2004" startWordPosition="919" endWordPosition="922">lational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). 2.2 Distributional Semantics Distributional models use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is p</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proceedings of Association for Computational Linguistics (ACL-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13).</booktitle>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-13).</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>L. Getoor and B. Taskar, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>2007</marker>
<rawString>L. Getoor and B. Taskar, editors. 2007. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-11).</booktitle>
<contexts>
<context position="6896" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1043" endWordPosition="1046">2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). 2.3 Markov Logic Network Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constra</context>
<context position="12433" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="1944" endWordPosition="1947">s output to be more than one unary atom sharing the same variable like “a little kid” which in logic is little(k) ∧ kid(k), or two unary atoms connected by a relation like “a man is driving” which in logic is man(m) ∧ agent(d, m) ∧ drive(d). We used vector addition (Mitchell and Lapata, 2010) to calculate vectors for phrases. Future Work: This can be extended in many directions. We are currently experimenting with asymmetric similarity functions to distinguish semantic relations. We would also like to use longer phrases and other compositionality techniques as in (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Also more inference rules can be added from paraphrases collections like PPDB (Ganitkevitch et al., 2013). 3.3 Inference: probabilistic logical inference The last component is probabilistic logical inference. Given the logical form of the input sentences, and the weighted inference rules, we use them to build a probabilistic logic program whose solution is the answer to the target task. A probabilistic logic program consists of the evidence set 9 E, the set of weighted first order logical expressions (rule base RB), and a query Q. Inference is the process of calculating Pr(Q|E, RB). Probabil</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
</authors>
<title>Towards a formal distributional semantics: Simulating logical calculi with tensors.</title>
<date>2013</date>
<booktitle>In Proceedings of Second Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="6649" citStr="Grefenstette, 2013" startWordPosition="1009" endWordPosition="1010">ser (Clark and Curran, 2004). 2.2 Distributional Semantics Distributional models use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). 2.3 Markov Logic Network Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework f</context>
</contexts>
<marker>Grefenstette, 2013</marker>
<rawString>Edward Grefenstette. 2013. Towards a formal distributional semantics: Simulating logical calculi with tensors. In Proceedings of Second Joint Conference on Lexical and Computational Semantics (*SEM 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<date>1993</date>
<booktitle>From Discourse to Logic.</booktitle>
<publisher>Kluwer.</publisher>
<contexts>
<context position="5379" citStr="Kamp and Reyle, 1993" startWordPosition="816" endWordPosition="819"> semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Kimmig</author>
<author>Stephen H Bach</author>
<author>Matthias Broecheler</author>
<author>Bert Huang</author>
<author>Lise Getoor</author>
</authors>
<title>A short introduction to Probabilistic Soft Logic.</title>
<date>2012</date>
<booktitle>In Proceedings of NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12).</booktitle>
<contexts>
<context position="4518" citStr="Kimmig et al., 2012" startWordPosition="687" endWordPosition="690">babilistic logic, the 7 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7–11, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics synonymy relation between “man” and “guy” is represented by: bx. man(x) &lt;---&gt; guy(x) |w1 and the hyponymy relation between “car” and “vehicle” is: bx. car(x) =&gt; vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, </context>
<context position="7948" citStr="Kimmig et al., 2012" startWordPosition="1197" endWordPosition="1201">pactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. MLNs define a probability distribution over possible worlds, where a world’s probability increases exponentially with the total weight of the logical clauses that it satisfies. A variety of inference methods for MLNs have been developed, however, their computational complexity is a fundamental issue. 2.4 Probabilistic Soft Logic Probabilistic Soft Logic (PSL) is another recently proposed framework for probabilistic logic (Kimmig et al., 2012). It uses logical representations to compactly define large graphical models with continuous variables, and includes methods for performing efficient probabilistic inference for the resulting models. A key distinguishing feature of PSL is that ground atoms have soft, continuous truth values in the interval [0, 1] rather than binary truth values as used in MLNs and most other probabilistic logics. Given a set of weighted inference rules, and with the help of Lukasiewicz’s relaxation of the logical operators, PSL builds a graphical model defining a probability distribution 8 over the continuous </context>
</contexts>
<marker>Kimmig, Bach, Broecheler, Huang, Getoor, 2012</marker>
<rawString>Angelika Kimmig, Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor. 2012. A short introduction to Probabilistic Soft Logic. In Proceedings of NIPS Workshop on Probabilistic Programming: Foundations and Applications (NIPS Workshop-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference. Natural Language Engineering.</title>
<date>2010</date>
<contexts>
<context position="3215" citStr="Kotlerman et al., 2010" startWordPosition="486" endWordPosition="489">rying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations is the “on-the-fly ontology” our semantic parser uses. A distributional semantics is relatively easy to build from a large corpus of raw text, and provides the wide coverage that formal ontologies lack. The formal language we would like to use in the semantic parser is first-order logic. However, distributional information is graded in nature, so the on-the-fly ontology and its predicted semantic relations are also graded. This means, that standard first-order logic is insufficient because it is binary by nature. Probabilistic logic solves this </context>
<context position="7147" citStr="Kotlerman et al., 2010" startWordPosition="1079" endWordPosition="1082">tributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). 2.3 Markov Logic Network Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. MLNs define a probability distribution over possible worlds, where a world’s probability increases exponentially with the total weight of the logical clauses that it satisfies. A variety of inference methods for MLNs have been developed, however</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</booktitle>
<contexts>
<context position="2643" citStr="Kwiatkowski et al., 2013" startWordPosition="397" endWordPosition="400">he ontology. Inference is what takes a problem represented in the formal knowledge representation and the ontology and performs the target task (e.g. textual entailment, question answering, etc.). Prior work in standard semantic parsing uses a pre-defined set of predicates in a fixed ontology. However, it is difficult to construct formal ontologies of properties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicte</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review.</journal>
<contexts>
<context position="3030" citStr="Landauer and Dumais, 1997" startWordPosition="457" endWordPosition="461">ave broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations is the “on-the-fly ontology” our semantic parser uses. A distributional semantics is relatively easy to build from a large corpus of raw text, and provides the wide coverage that formal ontologies lack. The formal language we would like to use in the semantic parser is first-order logic. However, distributional information is graded in nature, so the on-the-fly on</context>
<context position="6379" citStr="Landauer and Dumais, 1997" startWordPosition="965" endWordPosition="968">rser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). 2.2 Distributional Semantics Distributional models use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particu</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the first Joint Conference on Lexical and Computational Semantics (*SEM-12).</booktitle>
<contexts>
<context position="3190" citStr="Lenci and Benotto, 2012" startWordPosition="482" endWordPosition="485">ited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations is the “on-the-fly ontology” our semantic parser uses. A distributional semantics is relatively easy to build from a large corpus of raw text, and provides the wide coverage that formal ontologies lack. The formal language we would like to use in the semantic parser is first-order logic. However, distributional information is graded in nature, so the on-the-fly ontology and its predicted semantic relations are also graded. This means, that standard first-order logic is insufficient because it is binary by nature. Probabi</context>
<context position="7122" citStr="Lenci and Benotto, 2012" startWordPosition="1075" endWordPosition="1078"> in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). 2.3 Markov Logic Network Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. MLNs define a probability distribution over possible worlds, where a world’s probability increases exponentially with the total weight of the logical clauses that it satisfies. A variety of inference methods for MLNs hav</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of the first Joint Conference on Lexical and Computational Semantics (*SEM-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers.</title>
<date>1996</date>
<contexts>
<context position="6404" citStr="Lund and Burgess, 1996" startWordPosition="969" endWordPosition="972">mantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). 2.2 Distributional Semantics Distributional models use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, and Computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of Association for Computational Linguistics (ACL08).</booktitle>
<contexts>
<context position="6805" citStr="Mitchell and Lapata, 2008" startWordPosition="1031" endWordPosition="1034">tic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). 2.3 Markov Logic Network Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of Association for Computational Linguistics (ACL08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Journal of Cognitive Science.</journal>
<contexts>
<context position="6268" citStr="Mitchell and Lapata, 2010" startWordPosition="949" endWordPosition="952">o not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). 2.2 Distributional Semantics Distributional models use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefe</context>
<context position="12093" citStr="Mitchell and Lapata, 2010" startWordPosition="1895" endWordPosition="1898">nd b are words or phrases of T and H respectively, generate an inference rule: a → b |w, where the rule’s weight w = sim(−→a , →−b ), and sim is the cosine of the angle between vectors →−a and →−b . Note that this similarity measure cannot yet distinguish relations like synonymy and hypernymy. Phrases are defined in terms of Boxer’s output to be more than one unary atom sharing the same variable like “a little kid” which in logic is little(k) ∧ kid(k), or two unary atoms connected by a relation like “a man is driving” which in logic is man(m) ∧ agent(d, m) ∧ drive(d). We used vector addition (Mitchell and Lapata, 2010) to calculate vectors for phrases. Future Work: This can be extended in many directions. We are currently experimenting with asymmetric similarity functions to distinguish semantic relations. We would also like to use longer phrases and other compositionality techniques as in (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Also more inference rules can be added from paraphrases collections like PPDB (Ganitkevitch et al., 2013). 3.3 Inference: probabilistic logical inference The last component is probabilistic logical inference. Given the logical form of the input sentences, an</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Journal of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<date>1970</date>
<booktitle>Universal grammar. Theoria,</booktitle>
<pages>36--373</pages>
<contexts>
<context position="5356" citStr="Montague, 1970" startWordPosition="814" endWordPosition="815">implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section 3 explains the three components of the semantic parser, section 4 shows how this semantic parser can be used for RTE and STS tasks, section 5 presents the evaluation and 6 concludes. 2 Background 2.1 Logical Semantics Logic-based representations of meaning have a long tradition (Montague, 1970; Kamp and Reyle, 1993). They handle many complex semantic phenomena such as relational propositions, logical operators, and quantifiers; however, they can not handle “graded” aspects of meaning in language because they are binary by nature. Also, the logical predicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Repr</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Richard Montague. 1970. Universal grammar. Theoria, 36:373–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Markov logic networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="4461" citStr="Richardson and Domingos, 2006" startWordPosition="678" endWordPosition="681">it accepts weighted first order logic formulas. For example, in probabilistic logic, the 7 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7–11, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics synonymy relation between “man” and “guy” is represented by: bx. man(x) &lt;---&gt; guy(x) |w1 and the hyponymy relation between “car” and “vehicle” is: bx. car(x) =&gt; vehicle(x) |w2 where w1 and w1 are some certainty measure estimated from the distributional semantics. For inference, we use probabilistic logic frameworks like Markov Logic Networks (MLN) (Richardson and Domingos, 2006) and Probabilistic Soft Logic (PSL) (Kimmig et al., 2012). They are Statistical Relational Learning (SRL) techniques (Getoor and Taskar, 2007) that combine logical and statistical knowledge in one uniform framework, and provide a mechanism for coherent probabilistic inference. We implemented this semantic parser (Beltagy et al., 2013; Beltagy et al., 2014) and used it to perform two tasks that require deep semantic analysis, Recognizing Textual Entailment (RTE), and Semantic Textual Similarity (STS). The rest of the paper is organized as follows: section 2 presents background material, section</context>
<context position="7232" citStr="Richardson and Domingos, 2006" startWordPosition="1091" endWordPosition="1094"> capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). Distributional similarity is usually a mixture of semantic relations, but particular asymmetric similarity measures can, to a certain extent, predict hypernymy and lexical entailment distributionally (Lenci and Benotto, 2012; Kotlerman et al., 2010). 2.3 Markov Logic Network Markov Logic Network (MLN) (Richardson and Domingos, 2006) is a framework for probabilistic logic that employ weighted formulas in firstorder logic to compactly encode complex undirected probabilistic graphical models (i.e., Markov networks). Weighting the rules is a way of softening them compared to hard logical constraints. MLNs define a probability distribution over possible worlds, where a world’s probability increases exponentially with the total weight of the logical clauses that it satisfies. A variety of inference methods for MLNs have been developed, however, their computational complexity is a fundamental issue. 2.4 Probabilistic Soft Logic</context>
<context position="14558" citStr="Richardson and Domingos, 2006" startWordPosition="2310" endWordPosition="2313">r(H|¬T, RB). In case Pr(H|T, RB) is high, while Pr(H|¬T, RB) is low, this indicates Entails. In case it is the other way around, this indicates Contradicts. If both values are close to each other, this means T does not affect probability of H and that is an indication of Neutral. We train a classifier to map the two values to the final classification decision. Future Work: One general problem with MLNs is its computational overhead especially for the type of inference problems we have. The other problem is that MLNs, as with most other probabilistic logics, make the Domain Closure Assumption (Richardson and Domingos, 2006) which means that quantifiers sometimes behave in an undesired way. 4.2 Task 2: STS using PSL PSL is the probabilistic logic we use for the STS task since it has been shown to be an effective approach to compute similarity between structured objects. PSL does not work “out of the box” for STS, because Lukasiewicz’s equation for the conjunction is very restrictive. We addressed this problem (Beltagy et al., 2014) by replacing SICK-RTE SICK-STS dist 0.60 0.65 logic 0.71 0.68 logic+dist 0.73 0.70 Table 1: RTE accuracy and STS Correlation Lukasiewicz’s equation for the conjunction with an averagin</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research</journal>
<contexts>
<context position="3002" citStr="Turney and Pantel, 2010" startWordPosition="453" endWordPosition="456">ties and relations that have broad coverage, and very difficult to do semantic parsing based on such an ontology. Consequently, current semantic parsers are mostly restricted to fairly limited domains, such as querying a specific database (Kwiatkowski et al., 2013; Berant et al., 2013). We propose a semantic parser that is not restricted to a predefined ontology. Instead, we use distributional semantics to generate the needed part of an on-the-fly ontology. Distributional semantics is a statistical technique that represents the meaning of words and phrases as distributions over context words (Turney and Pantel, 2010; Landauer and Dumais, 1997). Distributional information can be used to predict semantic relations like synonymy and hyponymy between words and phrases of interest (Lenci and Benotto, 2012; Kotlerman et al., 2010). The collection of predicted semantic relations is the “on-the-fly ontology” our semantic parser uses. A distributional semantics is relatively easy to build from a large corpus of raw text, and provides the wide coverage that formal ontologies lack. The formal language we would like to use in the semantic parser is first-order logic. However, distributional information is graded in </context>
<context position="6240" citStr="Turney and Pantel, 2010" startWordPosition="945" endWordPosition="948">redicates and relations do not have semantics by themselves without an accompanying ontology, which we want to replace in our semantic parser with distributional semantics. To map a sentence to logical form, we use Boxer (Bos, 2008), a tool for wide-coverage semantic analysis that produces uninterpreted logical forms using Discourse Representation Structures (Kamp and Reyle, 1993). It builds on the C&amp;C CCG parser (Clark and Curran, 2004). 2.2 Distributional Semantics Distributional models use statistics on contextual data from large corpora to predict semantic similarity of words and phrases (Turney and Pantel, 2010; Mitchell and Lapata, 2010), based on the observation that semantically similar words occur in similar contexts (Landauer and Dumais, 1997; Lund and Burgess, 1996). So words can be represented as vectors in high dimensional spaces generated from the contexts in which they occur. Distributional models capture the graded nature of meaning, but do not adequately capture logical structure (Grefenstette, 2013). It is possible to compute vector representations for larger phrases compositionally from their parts (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Baroni</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research (JAIR-10).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
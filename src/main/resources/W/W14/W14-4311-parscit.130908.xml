<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.927861">
Alex: Bootstrapping a Spoken Dialogue System for a New Domain by Real
Users∗
</title>
<author confidence="0.990173">
Ond-rej Dušek, Ond-rej Plátek, Lukáš Žilka, and Filip Jur-cí-cek
</author>
<affiliation confidence="0.980767">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.804929">
Malostranské námˇestí 25, CZ-11800 Prague, Czech Republic
</address>
<email confidence="0.994807">
{odusek,oplatek,zilka,jurcicek}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994555">
When deploying a spoken dialogue sys-
tem in a new domain, one faces a situation
where little to no data is available to train
domain-specific statistical models. We de-
scribe our experience with bootstrapping
a dialogue system for public transit and
weather information in real-word deploy-
ment under public use. We proceeded in-
crementally, starting from a minimal sys-
tem put on a toll-free telephone number to
collect speech data. We were able to incor-
porate statistical modules trained on col-
lected data – in-domain speech recogni-
tion language models and spoken language
understanding – while simultaneously ex-
tending the domain, making use of auto-
matically generated semantic annotation.
Our approach shows that a successful sys-
tem can be built with minimal effort and
no in-domain data at hand.
</bodyText>
<sectionHeader confidence="0.998966" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992201">
The Alex Public Transit Information System is an
experimental Czech spoken dialogue system pro-
viding information about all kinds of public tran-
sit in the Czech Republic, publicly available at a
toll-free 800 telephone number.1 It was launched
for public use as soon as a first minimal working
version was developed, using no in-domain speech
data. We chose an incremental approach to sys-
tem development in order to collect call data and
use them to bootstrap statistical modules. Nearly
</bodyText>
<footnote confidence="0.921555666666667">
∗This work was funded by the Ministry of Education,
Youth and Sports of the Czech Republic under the grant
agreement LK11221 and core research funding, SVV project
260 104, and grants GAUK 2058214 and 2076214 of Charles
University in Prague. It used language resources stored and
distributed by the LINDAT/CLARIN project of the Min-
istry of Education, Youth and Sports of the Czech Republic
(project LM2010013).
1Call 800-899-998 from the Czech Republic.
</footnote>
<bodyText confidence="0.999758375">
a year after launch, we have collected over 1,300
calls from the general public, which enabled us
to train and deploy an in-domain language model
for Automatic Speech Recognition (ASR) and a
statistical Spoken Language Understanding (SLU)
module. The domain supported by the system has
extended from transit information in one city to ca.
5,000 towns and cities in the whole country, plus
weather and time information. This shows that a
even a very basic system is useful in collecting in-
domain data and that the incremental approach is
viable.
Spoken dialogue systems have been a topic of
research for the past several decades, and many
experimental systems were developed and tested
with users (Walker et al., 2001; Gaši´c et al., 2013;
Janarthanam et al., 2013). However, few experi-
mental systems became available to general public
use. Let’s Go (Raux et al., 2005; Raux et al., 2006)
is a notable example in the public transportation
domain. Using interaction with users from the
public to bootstrap data-driven methods and im-
prove the system is also not a common practice.
Both Let’s Go and the GOOG-411 business finder
system (Bacchiani et al., 2008) collected speech
data, but applied data-driven methods only to im-
prove statistical ASR. We use the call data for sta-
tistical SLU as well and plan to further introduce
statistical modules for dialogue management and
natural language generation.
Our spoken dialogue system framework is
freely available on GitHub2 and designed for easy
adaptation to new domains and languages. An En-
glish version of our system is in preparation.
We first present the overall structure of the Alex
SDS framework and then describe the minimal
system that has been put to public use, as well as
our incremental extensions. Finally, we provide
an evaluation of our system based on the recorded
calls.
</bodyText>
<footnote confidence="0.973268">
2http://github.com/UFAL-DSG/alex
</footnote>
<page confidence="0.988412">
79
</page>
<bodyText confidence="0.4068445">
Proceedings of the SIGDIAL 2014 Conference, pages 79–83,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.731349" genericHeader="method">
2 Overall Alex SDS System Structure
</sectionHeader>
<bodyText confidence="0.999942909090909">
The basic architecture of Alex is modular and con-
sists of the traditional SDS components: automatic
speech recognizer (ASR), spoken language under-
standing (SLU), dialogue manager (DM), natural
language generator (NLG), and a text-to-speech
(TTS) module.
We designed the system to allow for easy re-
placement of the individual components: There is
a defined interface for each of them. As the in-
terfaces are domain-independent, changing the do-
main is facilitated as well by this approach.
</bodyText>
<sectionHeader confidence="0.983533" genericHeader="method">
3 Baseline Transit Information System
</sectionHeader>
<bodyText confidence="0.999944">
We decided to create a minimal working system
that would not require any in-domain data and
open it to general public to collect call data as soon
as possible. We believe that this is a viable al-
ternative to Wizard-of-Oz experiments (Rieser and
Lemon, 2008), allowing for incremental develop-
ment and producing data that correspond to real
usage scenarios (see Section 4).
</bodyText>
<subsectionHeader confidence="0.9132555">
3.1 Baseline Implementation of the
Components
</subsectionHeader>
<bodyText confidence="0.997990666666667">
Having no in-domain data available, we resorted
to very basic implementations using hand-written
rules or external services:
</bodyText>
<listItem confidence="0.9988965">
• ASR used a neural network based voice activity
detector trained on small out-of-domain data.
Recordings classified as speech were fed to the
the web-based Google ASR service.
• SLU was handcrafted for our domain using sim-
ple keyword-spotting rules.
• In DM, the dialogue tracker held only one value
per dialogue slot, and the dialogue policy was
handcrafted for the basic tasks in our domain.
• NLG is a simple template-based module.
• We use a web-based Czech TTS service pro-
vided to us by SpeechTech.3
</listItem>
<subsectionHeader confidence="0.998544">
3.2 Baseline Domain
</subsectionHeader>
<bodyText confidence="0.999819666666667">
At baseline, our domain only consisted of a very
basic public transport information for the city of
Prague. Our ontology contained ca. 2,500 public
transit stops. The system was able to present the
next connection between two stops requested by
the user, repeat the information, or return several
</bodyText>
<figure confidence="0.773412">
3http://www.speechtech.cz/
Training set portion
20
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</figure>
<figureCaption confidence="0.94484">
Figure 1: ASR word error rate depending on the
size of in-domain language model training data
</figureCaption>
<bodyText confidence="0.97708575">
The full training set amounts to 9,495 utterances (30,126 to-
kens). The test set contains 1,187 utterances (4,392 tokens).
following connections. Connection search was
based on Google Directions API.4
</bodyText>
<subsectionHeader confidence="0.7697885">
4 Collecting Data and Extending the
System in Real Usage
</subsectionHeader>
<bodyText confidence="0.999967">
We launched our system at a public toll-free 800
number and advertised the service at our univer-
sity, among friends, and via Facebook. We also
cooperate with the Czech Blind United associa-
tion,5 promoting our system among its members
and receiving comments about its use. We adver-
tised our extensions and improvements using the
same channels.
We record and collect all calls to the system,
including our own testing calls, to obtain training
data and build statistical models into our system.
</bodyText>
<subsectionHeader confidence="0.9945275">
4.1 Speech Recognition: Building In-Domain
Models
</subsectionHeader>
<bodyText confidence="0.99994375">
The Google on-line ASR service, while reach-
ing state-of-the-art performance in some tasks
(Morbini et al., 2013), showed very high word er-
ror rate in our specific domain (see Figure 1). We
replaced it with the Kaldi ASR engine (Povey et
al., 2011) trained on general-domain Czech acous-
tic data (Korvas et al., 2014) with an in-domain
class-based language model built using collected
call data and lists of all available cities and stops.
We describe our modifications to Kaldi for on-
line decoding in Plátek and Jurˇcíˇcek (2014). A
performance comparison of Google ASR with
</bodyText>
<footnote confidence="0.999209">
4https://developers.google.com/maps/
documentation/directions/
5http://www.sons.cz
</footnote>
<figure confidence="0.982018153846154">
Word error rate (%)
Google ASR
Kaldi ASR
50
45
40
35
30
25
80
Training set portion
64
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</figure>
<figureCaption confidence="0.992249">
Figure 2: SLU performance (F-measure on dia-
logue act items) depending on training data size
</figureCaption>
<bodyText confidence="0.970107857142857">
The same data sets as in Figure 1 are used, with semantic
annotations from handcrafted SLU running on manual tran-
scriptions.
Kaldi trained on our data is shown in Figure 1.
One can see that the in-domain language model
brings a substantial improvement, even with very
small data sizes.
</bodyText>
<subsectionHeader confidence="0.991997">
4.2 Spoken Language Understanding
</subsectionHeader>
<bodyText confidence="0.999949">
To increase system robustness, we built a statisti-
cal SLU based on a set of logistic regression clas-
sifiers and word n-gram features (Jurˇcíˇcek et al.,
2014). We train it on the output of our handcrafted
SLU applied to manual transcriptions. We chose
this approach over obtaining manual semantic an-
notation due to two main reasons:
</bodyText>
<listItem confidence="0.988775">
1. Obtaining semantic annotation for Czech data is
relatively slow and complicated; using crowd-
sourcing is not a possibility due to lack of
speakers of Czech on the platforms.
2. As we intended to gradually extend our domain,
semantic annotation changed over time as well.
</listItem>
<bodyText confidence="0.999389333333333">
This approach still allows the statistical SLU to
improve on a handcrafted one by compensating
for errors made by the ASR. Figure 2 shows that
the performance of the statistical SLU module in-
creases with more training data and with the in-
domain ASR models.
</bodyText>
<subsectionHeader confidence="0.99808">
4.3 Dialogue Manager
</subsectionHeader>
<bodyText confidence="0.999928538461539">
We have replaced the initial simplistic dialogue
state tracker (see Section 3.1) by the probabilis-
tic discriminative tracker of Žilka et al. (2013),
which achieves near state-of-the-art performance
while remaining completely parameter-free. This
property allowed us to employ the tracker without
any training data; our gradual domain extensions
also required no further adjustments.
The dialogue policy is handcrafted, though it
takes advantage of uncertainty estimated by the
belief tracker. Its main logic is similar to that of
Jurˇcíˇcek et al. (2012). First, it implements a set of
domain-independent actions, such as:
</bodyText>
<listItem confidence="0.999734666666667">
• dialogue opening, closing, and restart,
• implicit confirmation of changed slots with high
probability of the most probable value,
• explicit confirmation for slots with a lower
probability of the most probable value,
• a choice among two similarly probable values.
</listItem>
<bodyText confidence="0.553425">
Second, domain-specific actions are imple-
mented for the domain(s) described in Section 4.4.
</bodyText>
<subsectionHeader confidence="0.994396">
4.4 Extending the Domain
</subsectionHeader>
<bodyText confidence="0.9999645">
We have expanded our public transit information
domain with the following tasks:
</bodyText>
<listItem confidence="0.777411555555556">
• The user may specify departure or arrival time
in absolute or relative terms (“in ten minutes”,
“tomorrow morning”, “at 6 pm.”, “at 8:35” etc.).
• The user may request more details about the
connection: number of transfers, journey dura-
tion, departure and arrival time.
• The user may travel not only among public
transport stops within one city, but also among
multiple cities or towns.
</listItem>
<bodyText confidence="0.99944975">
The expansion to multiple cities has lead to an
ontology improvement: The system is able to find
the corresponding city in the database based on a
stop name, and can use a default stop for a given
city. We initially supported three Czech major
cities covered by the Google Directions service,
then extended the coverage to the whole country
(ca. 44,000 stops in 5,000 cities and towns) using
Czech national public transport database provided
by CHAPS.6
We now also include weather information for all
Czech cities in the system. The user may ask for
weather at the given time or on the whole day. We
use OpenWeatherMap as our data source.7
Furthermore, the user may ask about the current
time at any point in the dialogue.
</bodyText>
<sectionHeader confidence="0.98646" genericHeader="method">
5 System Evaluation from Recorded
Calls
</sectionHeader>
<bodyText confidence="0.9996455">
We have used the recorded call data for an eval-
uation of our system. Figure 3 presents the num-
</bodyText>
<footnote confidence="0.988898">
6http://www.idos.cz
7http://openweathermap.org/
</footnote>
<figure confidence="0.96082875">
80
78
76
74
72
70
68
66
Dialogue act items
F-measure (%)
SLU trained on Google ASR
SLU trained on Kaldi ASR
81
110
90
A
80
70
60
I
50
40
30
20
C
10
0
Jun &apos;13 Jul Aug Sep Oct Nov Dec Jan &apos;14 Feb Mar Apr May
</figure>
<figureCaption confidence="0.999965">
Figure 3: Number of calls per week
</figureCaption>
<bodyText confidence="0.941631666666667">
The dashed line shows all recorded calls, including those
made by the authors. The full line shows calls from the public
only.
</bodyText>
<construct confidence="0.478329333333333">
Spikes: A – initial testing, B – first advertising, C – system
partially offline due to a bug, D – testing statistical SLU mod-
ule, E – larger advertising with Czech Blind United, F – test-
ing domain enhancements, G – no advertising and limited
system performance, H – deploying Kaldi ASR and nation-
wide coverage, I – no further advertising.
</construct>
<figure confidence="0.969531333333333">
1
0.9
0.8
0.7
0.6
0.5
0.4
% Rather positive answer
Jun &apos;13 Jul Aug Sep Oct Nov Dec Jan &apos;14 Feb Mar Apr May
</figure>
<figureCaption confidence="0.999951">
Figure 4: System success rates by month
</figureCaption>
<bodyText confidence="0.9931148">
Percentage of calls where the system provided information
(or apology for not having one) and percentage of rather pos-
itive responses to the final question, both shown with standard
error bars.
ber of calls to our system per week and reflects
the testing and advertising phases, as well as some
of our extensions and improvements described in
Section 4. A steeper usage increase is visible in
recent weeks after the introduction of Kaldi ASR
engine and nationwide coverage (see Sections 4.1
and 4.4). The number of calls and unique users
(caller phone numbers) grows steadily; so far,
more than 300 users from the public have made
over 1,300 calls to the system (cf. Figure 5 and
Table 1 in the appendix).8
</bodyText>
<figureCaption confidence="0.9224355">
Figure 4 (and Table 1 in the appendix) give a de-
tailed view of the success of our system. Informa-
</figureCaption>
<footnote confidence="0.954526">
8We only count calls with at least one valid user utterance,
disregarding calls where users hang up immediately.
</footnote>
<bodyText confidence="0.999527294117647">
tion is provided in the vast majority of calls. Upon
manual inspection of call transcripts, we discov-
ered that about half of the cases where no infor-
mation is provided can be attributed to the system
failing to react properly; the rest is off-topic calls
or users hanging up too early.
We have also introduced a “final question“ as
an additional success metric. After the user says
good-bye, the system asks them if they received
the information they were looking for. By looking
at the transcriptions of responses to this question,
we recognize a majority of them as rather positive
(“Yes”, “Nearly” etc.); the proportion of positive
reactions seems to remain stable. However, the fi-
nal question is not an accurate measure as most
users seem to hang up directly after receiving in-
formation from the system.
</bodyText>
<sectionHeader confidence="0.98877" genericHeader="method">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.9999151">
We use an iterative approach to build a complex
dialogue system within the public transit informa-
tion domain. The system is publicly available on a
toll-free phone number. Our extensible dialogue
system framework as well as the system imple-
mentation for our domain can be downloaded from
GitHub under the Apache 2.0 license.
We have shown that even very limited work-
ing version can be used to collect calls from
the public, gathering training data for statistical
system components. Our experiments with the
Kaldi speech recognizer show that already a small
amount of in-domain data for the language model
brings a substantial improvement. Generating au-
tomatic semantic annotation from recording tran-
scripts allows us to maintain a statistical spoken
language understanding unit with changing do-
main and growing data.
The analysis of our call logs shows that our sys-
tem is able to provide information in the vast ma-
jority of cases. Success rating provided by the
users themselves is mostly positive, yet the con-
clusiveness of this metric is limited as users tend
to hang up directly after receiving information.
In future, we plan to add an English version
of the system and further expand the domain, al-
lowing more specific connection options. As we
gather more training data, we plan to introduce sta-
tistical modules into the remaining system compo-
nents.
</bodyText>
<figure confidence="0.997387466666667">
0.3
0.2
0.1
0
100
B
D
E
Total calls
(incl.testing)
Public user calls
F
G
N
% Informed or apologized
</figure>
<page confidence="0.939103">
82
</page>
<sectionHeader confidence="0.56513" genericHeader="method">
A System Evaluation Data
</sectionHeader>
<bodyText confidence="0.941239">
In the following, we include additional data from
call logs evaluation presented in Section 5.
</bodyText>
<note confidence="0.926951">
Jun &apos;13 Jul Aug Sep Oct Nov Dec Jan &apos;14 Feb Mar Apr May
</note>
<figureCaption confidence="0.992595">
Figure 5: Cumulative number of calls and unique
callers from the public by weeks
</figureCaption>
<bodyText confidence="0.853712">
The growth rates of the number of unique users and the total
number of calls both correspond to the testing and advertising
periods shown in Figure 3.
</bodyText>
<table confidence="0.971555090909091">
Total calls 1,359
Unique users (caller phone numbers) 304
System informed (or apologized) 1,124
System informed about directions 990
System informed about weather 88
System informed about current time 41
Apologized for not having information 223
System asked the final question 229
Final question answered by the user 199
Rather positive user’s answer 146
Rather negative user’s answer 23
</table>
<tableCaption confidence="0.999608">
Table 1: Detailed call statistics
</tableCaption>
<bodyText confidence="0.980664">
Total absolute numbers of calls from general public users
over the period of nearly one year are shown.
</bodyText>
<sectionHeader confidence="0.998569" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999484883333333">
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schus-
ter, and B. Strope. 2008. Deploying GOOG-411:
early lessons in data, measurement, and testing. In
Proceedings of ICASSP, page 5260–5263. IEEE.
M. Ga&amp;quot;si´c, C. Breslin, M. Henderson, D. Kim,
M. Szummer, B. Thomson, P. Tsiakoulis, and
S. Young. 2013. On-line policy optimisation of
bayesian spoken dialogue systems via human inter-
action. In Proceedings of ICASSP, page 8367–8371.
IEEE.
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas,
A. Dickinson, X. Liu, W. Mackaness, and B. Web-
ber. 2013. Evaluating a city exploration dialogue
system combining question-answering and pedes-
trian navigation. In Proceedings ofACL.
F. Jurˇcíˇcek, B. Thomson, and S. Young. 2012. Rein-
forcement learning for parameter estimation in sta-
tistical spoken dialogue systems. Computer Speech
&amp; Language, 26(3):168–192.
F. Jurˇcíˇcek, O. Du&amp;quot;sek, and O. Plátek. 2014. A factored
discriminative spoken language understanding for
spoken dialogue systems. In Proceedings of TSD.
To appear.
M. Korvas, O. Plátek, O. Du&amp;quot;sek, L. Žilka, and F. Ju-
rˇcíˇcek. 2014. Free English and Czech telephone
speech corpus shared under the CC-BY-SA 3.0 li-
cense. In Proceedings of LREC, Reykjavík.
F. Morbini, K. Audhkhasi, K. Sagae, R. Artstein,
D. Can, P. Georgiou, S. Narayanan, A. Leuski, and
D. Traum. 2013. Which ASR should i choose for
my dialogue system? In Proceedings of SIGDIAL,
page 394–403.
O. Plátek and F. Jurˇcíˇcek. 2014. Free on-line speech
recogniser based on kaldi ASR toolkit producing
word posterior lattices. In Proceedings of SIGDIAL.
D. Povey, A. Ghoshal, G. Boulianne, L. Burget,
O. Glembek, N. Goel, M. Hannemann, P. Motlicek,
Y. Qian, P. Schwarz, et al. 2011. The Kaldi speech
recognition toolkit. In Proceedings of ASRU, page
1–4, Hawaii.
A. Raux, B. Langner, D. Bohus, Alan W. Black, and
M. Eskenazi. 2005. Let’s go public! taking a spo-
ken dialog system to the real world. In Proceedings
of Interspeech.
A. Raux, D. Bohus, B. Langner, Alan W. Black, and
M. Eskenazi. 2006. Doing research on a deployed
spoken dialogue system: one year of Let’s Go! ex-
perience. In Proceedings of Interspeech.
V. Rieser and O. Lemon. 2008. Learning effective
multimodal dialogue strategies from Wizard-of-Oz
data: Bootstrapping and evaluation. In Proceedings
ofACL, page 638–646.
M. A. Walker, R. Passonneau, and J. E. Boland. 2001.
Quantitative and qualitative evaluation of DARPA
communicator spoken dialogue systems. In Pro-
ceedings of ACL, page 515–522.
L. Žilka, D. Marek, M. Korvas, and F. Jurˇcíˇcek. 2013.
Comparison of bayesian discriminative and genera-
tive models for dialogue state tracking. In Proceed-
ings of SIGDIAL, page 452–456, Metz, France.
</reference>
<figure confidence="0.99399425">
1350
1200
1050
450
900
750
600
300
150
0
Total calls
Unique callers
</figure>
<page confidence="0.982239">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.264376">
<author confidence="0.36409">Alex Bootstrapping a Spoken Dialogue System for a New Domain by Real</author>
<affiliation confidence="0.811188">Dušek, Plátek, Lukáš Žilka, Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.944915">Malostranské námˇestí 25, CZ-11800 Prague, Czech</address>
<email confidence="0.960058">odusek@ufal.mff.cuni.cz</email>
<email confidence="0.960058">oplatek@ufal.mff.cuni.cz</email>
<email confidence="0.960058">zilka@ufal.mff.cuni.cz</email>
<email confidence="0.960058">jurcicek@ufal.mff.cuni.cz</email>
<abstract confidence="0.995599571428571">When deploying a spoken dialogue system in a new domain, one faces a situation where little to no data is available to train domain-specific statistical models. We describe our experience with bootstrapping a dialogue system for public transit and weather information in real-word deployment under public use. We proceeded incrementally, starting from a minimal system put on a toll-free telephone number to collect speech data. We were able to incorporate statistical modules trained on collected data – in-domain speech recognition language models and spoken language understanding – while simultaneously extending the domain, making use of automatically generated semantic annotation. Our approach shows that a successful system can be built with minimal effort and no in-domain data at hand.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bacchiani</author>
<author>F Beaufays</author>
<author>J Schalkwyk</author>
<author>M Schuster</author>
<author>B Strope</author>
</authors>
<title>Deploying GOOG-411: early lessons in data, measurement, and testing.</title>
<date>2008</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>5260--5263</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3275" citStr="Bacchiani et al., 2008" startWordPosition="515" endWordPosition="518"> is viable. Spoken dialogue systems have been a topic of research for the past several decades, and many experimental systems were developed and tested with users (Walker et al., 2001; Gaši´c et al., 2013; Janarthanam et al., 2013). However, few experimental systems became available to general public use. Let’s Go (Raux et al., 2005; Raux et al., 2006) is a notable example in the public transportation domain. Using interaction with users from the public to bootstrap data-driven methods and improve the system is also not a common practice. Both Let’s Go and the GOOG-411 business finder system (Bacchiani et al., 2008) collected speech data, but applied data-driven methods only to improve statistical ASR. We use the call data for statistical SLU as well and plan to further introduce statistical modules for dialogue management and natural language generation. Our spoken dialogue system framework is freely available on GitHub2 and designed for easy adaptation to new domains and languages. An English version of our system is in preparation. We first present the overall structure of the Alex SDS framework and then describe the minimal system that has been put to public use, as well as our incremental extensions</context>
</contexts>
<marker>Bacchiani, Beaufays, Schalkwyk, Schuster, Strope, 2008</marker>
<rawString>M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster, and B. Strope. 2008. Deploying GOOG-411: early lessons in data, measurement, and testing. In Proceedings of ICASSP, page 5260–5263. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gasi´c</author>
<author>C Breslin</author>
<author>M Henderson</author>
<author>D Kim</author>
<author>M Szummer</author>
<author>B Thomson</author>
<author>P Tsiakoulis</author>
<author>S Young</author>
</authors>
<title>On-line policy optimisation of bayesian spoken dialogue systems via human interaction.</title>
<date>2013</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<pages>8367--8371</pages>
<publisher>IEEE.</publisher>
<marker>Gasi´c, Breslin, Henderson, Kim, Szummer, Thomson, Tsiakoulis, Young, 2013</marker>
<rawString>M. Ga&amp;quot;si´c, C. Breslin, M. Henderson, D. Kim, M. Szummer, B. Thomson, P. Tsiakoulis, and S. Young. 2013. On-line policy optimisation of bayesian spoken dialogue systems via human interaction. In Proceedings of ICASSP, page 8367–8371. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
<author>P Bartie</author>
<author>T Dalmas</author>
<author>A Dickinson</author>
<author>X Liu</author>
<author>W Mackaness</author>
<author>B Webber</author>
</authors>
<title>Evaluating a city exploration dialogue system combining question-answering and pedestrian navigation.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="2883" citStr="Janarthanam et al., 2013" startWordPosition="450" endWordPosition="453"> model for Automatic Speech Recognition (ASR) and a statistical Spoken Language Understanding (SLU) module. The domain supported by the system has extended from transit information in one city to ca. 5,000 towns and cities in the whole country, plus weather and time information. This shows that a even a very basic system is useful in collecting indomain data and that the incremental approach is viable. Spoken dialogue systems have been a topic of research for the past several decades, and many experimental systems were developed and tested with users (Walker et al., 2001; Gaši´c et al., 2013; Janarthanam et al., 2013). However, few experimental systems became available to general public use. Let’s Go (Raux et al., 2005; Raux et al., 2006) is a notable example in the public transportation domain. Using interaction with users from the public to bootstrap data-driven methods and improve the system is also not a common practice. Both Let’s Go and the GOOG-411 business finder system (Bacchiani et al., 2008) collected speech data, but applied data-driven methods only to improve statistical ASR. We use the call data for statistical SLU as well and plan to further introduce statistical modules for dialogue managem</context>
</contexts>
<marker>Janarthanam, Lemon, Bartie, Dalmas, Dickinson, Liu, Mackaness, Webber, 2013</marker>
<rawString>S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dickinson, X. Liu, W. Mackaness, and B. Webber. 2013. Evaluating a city exploration dialogue system combining question-answering and pedestrian navigation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jurˇcíˇcek</author>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<title>Reinforcement learning for parameter estimation in statistical spoken dialogue systems.</title>
<date>2012</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Jurˇcíˇcek, Thomson, Young, 2012</marker>
<rawString>F. Jurˇcíˇcek, B. Thomson, and S. Young. 2012. Reinforcement learning for parameter estimation in statistical spoken dialogue systems. Computer Speech &amp; Language, 26(3):168–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jurˇcíˇcek</author>
<author>O Dusek</author>
<author>O Plátek</author>
</authors>
<title>A factored discriminative spoken language understanding for spoken dialogue systems.</title>
<date>2014</date>
<booktitle>In Proceedings of TSD.</booktitle>
<note>To appear.</note>
<marker>Jurˇcíˇcek, Dusek, Plátek, 2014</marker>
<rawString>F. Jurˇcíˇcek, O. Du&amp;quot;sek, and O. Plátek. 2014. A factored discriminative spoken language understanding for spoken dialogue systems. In Proceedings of TSD. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Korvas</author>
<author>O Plátek</author>
<author>O Dusek</author>
<author>L Žilka</author>
<author>F Jurˇcíˇcek</author>
</authors>
<title>Free English and Czech telephone speech corpus shared under the CC-BY-SA 3.0 license.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Reykjavík.</location>
<marker>Korvas, Plátek, Dusek, Žilka, Jurˇcíˇcek, 2014</marker>
<rawString>M. Korvas, O. Plátek, O. Du&amp;quot;sek, L. Žilka, and F. Jurˇcíˇcek. 2014. Free English and Czech telephone speech corpus shared under the CC-BY-SA 3.0 license. In Proceedings of LREC, Reykjavík.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morbini</author>
<author>K Audhkhasi</author>
<author>K Sagae</author>
<author>R Artstein</author>
<author>D Can</author>
<author>P Georgiou</author>
<author>S Narayanan</author>
<author>A Leuski</author>
<author>D Traum</author>
</authors>
<title>Which ASR should i choose for my dialogue system?</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>394--403</pages>
<contexts>
<context position="7159" citStr="Morbini et al., 2013" startWordPosition="1131" endWordPosition="1134"> toll-free 800 number and advertised the service at our university, among friends, and via Facebook. We also cooperate with the Czech Blind United association,5 promoting our system among its members and receiving comments about its use. We advertised our extensions and improvements using the same channels. We record and collect all calls to the system, including our own testing calls, to obtain training data and build statistical models into our system. 4.1 Speech Recognition: Building In-Domain Models The Google on-line ASR service, while reaching state-of-the-art performance in some tasks (Morbini et al., 2013), showed very high word error rate in our specific domain (see Figure 1). We replaced it with the Kaldi ASR engine (Povey et al., 2011) trained on general-domain Czech acoustic data (Korvas et al., 2014) with an in-domain class-based language model built using collected call data and lists of all available cities and stops. We describe our modifications to Kaldi for online decoding in Plátek and Jurˇcíˇcek (2014). A performance comparison of Google ASR with 4https://developers.google.com/maps/ documentation/directions/ 5http://www.sons.cz Word error rate (%) Google ASR Kaldi ASR 50 45 40 35 30</context>
</contexts>
<marker>Morbini, Audhkhasi, Sagae, Artstein, Can, Georgiou, Narayanan, Leuski, Traum, 2013</marker>
<rawString>F. Morbini, K. Audhkhasi, K. Sagae, R. Artstein, D. Can, P. Georgiou, S. Narayanan, A. Leuski, and D. Traum. 2013. Which ASR should i choose for my dialogue system? In Proceedings of SIGDIAL, page 394–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Plátek</author>
<author>F Jurˇcíˇcek</author>
</authors>
<title>Free on-line speech recogniser based on kaldi ASR toolkit producing word posterior lattices.</title>
<date>2014</date>
<booktitle>In Proceedings of SIGDIAL.</booktitle>
<marker>Plátek, Jurˇcíˇcek, 2014</marker>
<rawString>O. Plátek and F. Jurˇcíˇcek. 2014. Free on-line speech recogniser based on kaldi ASR toolkit producing word posterior lattices. In Proceedings of SIGDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Povey</author>
<author>A Ghoshal</author>
<author>G Boulianne</author>
<author>L Burget</author>
<author>O Glembek</author>
<author>N Goel</author>
<author>M Hannemann</author>
<author>P Motlicek</author>
<author>Y Qian</author>
<author>P Schwarz</author>
</authors>
<title>The Kaldi speech recognition toolkit.</title>
<date>2011</date>
<booktitle>In Proceedings of ASRU,</booktitle>
<pages>1--4</pages>
<location>Hawaii.</location>
<contexts>
<context position="7294" citStr="Povey et al., 2011" startWordPosition="1157" endWordPosition="1160">nd United association,5 promoting our system among its members and receiving comments about its use. We advertised our extensions and improvements using the same channels. We record and collect all calls to the system, including our own testing calls, to obtain training data and build statistical models into our system. 4.1 Speech Recognition: Building In-Domain Models The Google on-line ASR service, while reaching state-of-the-art performance in some tasks (Morbini et al., 2013), showed very high word error rate in our specific domain (see Figure 1). We replaced it with the Kaldi ASR engine (Povey et al., 2011) trained on general-domain Czech acoustic data (Korvas et al., 2014) with an in-domain class-based language model built using collected call data and lists of all available cities and stops. We describe our modifications to Kaldi for online decoding in Plátek and Jurˇcíˇcek (2014). A performance comparison of Google ASR with 4https://developers.google.com/maps/ documentation/directions/ 5http://www.sons.cz Word error rate (%) Google ASR Kaldi ASR 50 45 40 35 30 25 80 Training set portion 64 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Figure 2: SLU performance (F-measure on dialogue act items) depe</context>
</contexts>
<marker>Povey, Ghoshal, Boulianne, Burget, Glembek, Goel, Hannemann, Motlicek, Qian, Schwarz, 2011</marker>
<rawString>D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, et al. 2011. The Kaldi speech recognition toolkit. In Proceedings of ASRU, page 1–4, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>B Langner</author>
<author>D Bohus</author>
<author>Alan W Black</author>
<author>M Eskenazi</author>
</authors>
<title>Let’s go public! taking a spoken dialog system to the real world.</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="2986" citStr="Raux et al., 2005" startWordPosition="467" endWordPosition="470"> domain supported by the system has extended from transit information in one city to ca. 5,000 towns and cities in the whole country, plus weather and time information. This shows that a even a very basic system is useful in collecting indomain data and that the incremental approach is viable. Spoken dialogue systems have been a topic of research for the past several decades, and many experimental systems were developed and tested with users (Walker et al., 2001; Gaši´c et al., 2013; Janarthanam et al., 2013). However, few experimental systems became available to general public use. Let’s Go (Raux et al., 2005; Raux et al., 2006) is a notable example in the public transportation domain. Using interaction with users from the public to bootstrap data-driven methods and improve the system is also not a common practice. Both Let’s Go and the GOOG-411 business finder system (Bacchiani et al., 2008) collected speech data, but applied data-driven methods only to improve statistical ASR. We use the call data for statistical SLU as well and plan to further introduce statistical modules for dialogue management and natural language generation. Our spoken dialogue system framework is freely available on GitHub</context>
</contexts>
<marker>Raux, Langner, Bohus, Black, Eskenazi, 2005</marker>
<rawString>A. Raux, B. Langner, D. Bohus, Alan W. Black, and M. Eskenazi. 2005. Let’s go public! taking a spoken dialog system to the real world. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>D Bohus</author>
<author>B Langner</author>
<author>Alan W Black</author>
<author>M Eskenazi</author>
</authors>
<title>Doing research on a deployed spoken dialogue system: one year of Let’s Go! experience.</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="3006" citStr="Raux et al., 2006" startWordPosition="471" endWordPosition="474">y the system has extended from transit information in one city to ca. 5,000 towns and cities in the whole country, plus weather and time information. This shows that a even a very basic system is useful in collecting indomain data and that the incremental approach is viable. Spoken dialogue systems have been a topic of research for the past several decades, and many experimental systems were developed and tested with users (Walker et al., 2001; Gaši´c et al., 2013; Janarthanam et al., 2013). However, few experimental systems became available to general public use. Let’s Go (Raux et al., 2005; Raux et al., 2006) is a notable example in the public transportation domain. Using interaction with users from the public to bootstrap data-driven methods and improve the system is also not a common practice. Both Let’s Go and the GOOG-411 business finder system (Bacchiani et al., 2008) collected speech data, but applied data-driven methods only to improve statistical ASR. We use the call data for statistical SLU as well and plan to further introduce statistical modules for dialogue management and natural language generation. Our spoken dialogue system framework is freely available on GitHub2 and designed for e</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>A. Raux, D. Bohus, B. Langner, Alan W. Black, and M. Eskenazi. 2006. Doing research on a deployed spoken dialogue system: one year of Let’s Go! experience. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Learning effective multimodal dialogue strategies from Wizard-of-Oz data: Bootstrapping and evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>638--646</pages>
<contexts>
<context position="4953" citStr="Rieser and Lemon, 2008" startWordPosition="778" endWordPosition="781">), dialogue manager (DM), natural language generator (NLG), and a text-to-speech (TTS) module. We designed the system to allow for easy replacement of the individual components: There is a defined interface for each of them. As the interfaces are domain-independent, changing the domain is facilitated as well by this approach. 3 Baseline Transit Information System We decided to create a minimal working system that would not require any in-domain data and open it to general public to collect call data as soon as possible. We believe that this is a viable alternative to Wizard-of-Oz experiments (Rieser and Lemon, 2008), allowing for incremental development and producing data that correspond to real usage scenarios (see Section 4). 3.1 Baseline Implementation of the Components Having no in-domain data available, we resorted to very basic implementations using hand-written rules or external services: • ASR used a neural network based voice activity detector trained on small out-of-domain data. Recordings classified as speech were fed to the the web-based Google ASR service. • SLU was handcrafted for our domain using simple keyword-spotting rules. • In DM, the dialogue tracker held only one value per dialogue </context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>V. Rieser and O. Lemon. 2008. Learning effective multimodal dialogue strategies from Wizard-of-Oz data: Bootstrapping and evaluation. In Proceedings ofACL, page 638–646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Walker</author>
<author>R Passonneau</author>
<author>J E Boland</author>
</authors>
<title>Quantitative and qualitative evaluation of DARPA communicator spoken dialogue systems.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>515--522</pages>
<contexts>
<context position="2835" citStr="Walker et al., 2001" startWordPosition="442" endWordPosition="445"> to train and deploy an in-domain language model for Automatic Speech Recognition (ASR) and a statistical Spoken Language Understanding (SLU) module. The domain supported by the system has extended from transit information in one city to ca. 5,000 towns and cities in the whole country, plus weather and time information. This shows that a even a very basic system is useful in collecting indomain data and that the incremental approach is viable. Spoken dialogue systems have been a topic of research for the past several decades, and many experimental systems were developed and tested with users (Walker et al., 2001; Gaši´c et al., 2013; Janarthanam et al., 2013). However, few experimental systems became available to general public use. Let’s Go (Raux et al., 2005; Raux et al., 2006) is a notable example in the public transportation domain. Using interaction with users from the public to bootstrap data-driven methods and improve the system is also not a common practice. Both Let’s Go and the GOOG-411 business finder system (Bacchiani et al., 2008) collected speech data, but applied data-driven methods only to improve statistical ASR. We use the call data for statistical SLU as well and plan to further in</context>
</contexts>
<marker>Walker, Passonneau, Boland, 2001</marker>
<rawString>M. A. Walker, R. Passonneau, and J. E. Boland. 2001. Quantitative and qualitative evaluation of DARPA communicator spoken dialogue systems. In Proceedings of ACL, page 515–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Žilka</author>
<author>D Marek</author>
<author>M Korvas</author>
<author>F Jurˇcíˇcek</author>
</authors>
<title>Comparison of bayesian discriminative and generative models for dialogue state tracking.</title>
<date>2013</date>
<booktitle>In Proceedings of SIGDIAL,</booktitle>
<pages>452--456</pages>
<location>Metz, France.</location>
<marker>Žilka, Marek, Korvas, Jurˇcíˇcek, 2013</marker>
<rawString>L. Žilka, D. Marek, M. Korvas, and F. Jurˇcíˇcek. 2013. Comparison of bayesian discriminative and generative models for dialogue state tracking. In Proceedings of SIGDIAL, page 452–456, Metz, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
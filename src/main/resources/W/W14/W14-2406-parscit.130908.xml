<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001166">
<title confidence="0.997112">
Combining Formal and Distributional Models of Temporal and
Intensional Semantics
</title>
<author confidence="0.991485">
Mike Lewis
</author>
<affiliation confidence="0.998257">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.987915">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.999466">
mike.lewis@ed.ac.uk
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99979145">
We outline a vision for computational se-
mantics in which formal compositional
semantics is combined with a powerful,
structured lexical semantics derived from
distributional statistics. We consider how
existing work (Lewis and Steedman, 2013)
could be extended with a much richer
lexical semantics using recent techniques
for modelling processes (Scaria et al.,
2013)—for example, learning that visit-
ing events start with arriving and end with
leaving. We show how to closely inte-
grate this information with theories of for-
mal semantics, allowing complex compo-
sitional inferences such as is visiting→has
arrived in but will leave, which requires
interpreting both the function and content
words. This will allow machine reading
systems to understand not just what has
happened, but when.
</bodyText>
<sectionHeader confidence="0.965572" genericHeader="keywords">
1 Combined Distributional and Logical
Semantics
</sectionHeader>
<bodyText confidence="0.999012764705882">
Distributional semantics aims to induce the mean-
ing of language from unlabelled text. Traditional
approaches to distributional semantics have repre-
sented semantics in vector spaces (Baroni et al.,
2013). Words are assigned vectors based on col-
locations in large corpora, and then these vectors
a composed into vectors representing longer utter-
ances. However, so far there is relatively limited
empirical evidence that composed vectors provide
useful representations for whole sentences, and it
is unclear how to represent logical operators (such
as universal quantifiers) in a vector space. While
future breakthroughs may overcome these limita-
tions, there are already well developed solutions in
the formal semantics literature using logical rep-
resentations. On the other hand, standard for-
mal semantic approaches such as Bos (2008) have
</bodyText>
<author confidence="0.809157">
Mark Steedman
</author>
<affiliation confidence="0.996875">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.959922">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.974919">
steedman@inf.ed.ac.uk
</email>
<bodyText confidence="0.999938777777778">
found that hand-built ontologies such as Word-
Net (Miller, 1995) provide an insufficient model
of lexical semantics, leading to low recall on appli-
cations. The complementary strengths and weak-
nesses of formal and distributional semantics mo-
tivate combining them into a single model.
In Lewis and Steedman (2013), we proposed
a solution to these problems which uses CCG
(Steedman, 2012) as a model of formal semantics,
making it straightforward to build wide-coverage
logical forms. Hand built representations are
added for a small number of function words such
as negatives and quantifiers—but the lexical se-
mantics is represented by first clustering predi-
cates (based on their usage in large corpora), and
then using the cluster-identifiers as symbols in the
logical form. For example, the induced CCG lexi-
con might contain entries such as the following1:
</bodyText>
<equation confidence="0.95715125">
write (S\NP)/NP
: AyAxAe.rel43(x, y, e)
author N/PPof
: AyAxAe.rel43(x, y, e)
</equation>
<bodyText confidence="0.92352">
Equivalent sentences like Shakespeare wrote
Macbeth and Shakespeare is the author of
Macbeth can then both be mapped to a
rel43(shakespeare, macbeth) logical form, us-
ing derivations such as:
Shakespeare wrote Macbeth
</bodyText>
<equation confidence="0.93977975">
S\NP
AxAe.rel43(x, macbeth, e)
S
Ae.rel43(shakespeare, macbeth, e)
</equation>
<bodyText confidence="0.998939428571429">
This approach interacts seamlessly with stan-
dard formal semantics—for example modelling
negation by mapping Francis Bacon didn’t write
Macbeth to ¬rel43(francis bacon, macbeth).
Their method has shown good performance on a
dataset of multi-sentence textual inference prob-
lems involving quantifiers, by using first-order the-
</bodyText>
<equation confidence="0.81726525">
1The e variables are Davidsonian event variables.
NP (S\NP)/NP NP
shakespeare AyAxAe.rel43(x, y, e) macbeth
&gt;
</equation>
<page confidence="0.967946">
28
</page>
<bodyText confidence="0.905774">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 28–32,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
orem proving. Ambiguity is handled by a proba-
bilistic model, based on the types of the nouns.
Beltagy et al. (2013) use an alternative approach
with similar goals, in which every word instance
expresses a unique semantic primitive, but is con-
nected to the meanings of other word instances us-
ing distributionally-derived probabilistic inference
rules. This approach risks requiring very large
number of inference rules, which may make infer-
ence inefficient. Our approach avoid this problem
by attempting to fully represent lexical semantics
in the lexicon.
</bodyText>
<sectionHeader confidence="0.990787" genericHeader="method">
2 Proposal
</sectionHeader>
<bodyText confidence="0.9977346">
We propose how our previous model could be ex-
tended to make more sophisticated inferences. We
will demonstrate how many interesting problems
in semantics could be solved with a system based
on three components:
</bodyText>
<listItem confidence="0.943789388888889">
• A CCG syntactic parse for modelling com-
position. Using CCG allows us to handle in-
teresting forms of composition, such as co-
ordination, extraction, questions, right node
raising, etc. CCG also has both a developed
theory of operator semantics and a transpar-
ent interface to the underlying predicate ar-
gument structure.
• A small hand built lexicon for words
with complex semantics—such as negatives,
quantifiers, modals, and implicative verbs.
• A rich model of lexical semantics de-
rived from distributionally-induced entail-
ment graphs (Berant et al., 2011), extended
with subcategories of entailment relations in
a similar way to Scaria et al. (2013). We show
how such graphs can be converted into a CCG
lexicon.
</listItem>
<subsectionHeader confidence="0.98323">
2.1 Directional Inference
</subsectionHeader>
<bodyText confidence="0.999971210526316">
A major limitation of our previous model is
that it uses a flat clustering to model the
meaning of content words. This method en-
ables them to model synonymy relations be-
tween words, but not relations where the en-
tailment only holds in one direction—for ex-
ample, conquers—*invades, but not vice-versa.
This problem can be addressed using the en-
tailment graph framework introduced by Berant
et al. (2011), which learns globally consistent
graphs over predicates in which directed edges
indicate entailment relations. Exactly the same
methods can be used to build entailment graphs
over the predicates derived from a CCG parse:
The graph can then be converted to a CCG lexi-
con by making the semantics of a word be the con-
junction of all the relation identifiers it implies in
the graph. For example, the above graph is equiv-
alent to the following lexicon:
</bodyText>
<equation confidence="0.957412333333333">
attack (S\NP)/NP
: AxAyAe.rel1(x, y, e)
bomb (S\NP)/NP
: AxAyAe.rel1(x, y, e)nrel4(x, y, e)
invade (S\NP)/NP
: AxAyAe.rel1(x, y, e)nrel2(x, y, e)
conquer (S\NP)/NP
: AxAyAe.rel1(x, y, e) n
rel2(x, y, e) n rel3(x, y, e)
</equation>
<bodyText confidence="0.998708">
This lexicon supports the correct infer-
ences, such as conquers—*attacks and didn’t
invade—*didn’t conquer.
</bodyText>
<subsectionHeader confidence="0.996109">
2.2 Temporal Semantics
</subsectionHeader>
<bodyText confidence="0.999915666666667">
One case where combining formal and distribu-
tional semantics may be particularly helpful is in
giving a detailed model of temporal semantics. A
rich understanding of time would allow us to un-
derstand when events took place, or when states
were true. Most existing work ignores tense, and
would treat the expressions used to be president
and is president either as equivalent or completely
unrelated. Failing to model tense would lead to in-
correct inferences when answering questions such
as Who is the president of the USA?
Another motivation for considering a detailed
model of temporal semantics is that understanding
the time of events should improve the quality of
the distributional clustering. It has recently been
shown that such information is extremely useful
for learning equivalences between predicates, by
determining which sentences describe the same
</bodyText>
<figure confidence="0.950098111111111">
2
invadearg0,arg1
invasionposs,of
4
bombarg0,arg1
1 attackarg0,arg1
conquerarg0,arg1
annexarg0,arg1
3
</figure>
<page confidence="0.994684">
29
</page>
<bodyText confidence="0.999930346153846">
events using date-stamped text and simple tense
heuristics (Zhang and Weld, 2013). Such meth-
ods escape common problems with traditional ap-
proaches to distributional similarity, such as con-
flating causes with effects, and may prove very
useful for building entailment graphs.
Temporal information is conveyed by both by
auxiliary verbs such as will or used to, and in
the semantics of content words. For example, the
statement John is visiting Baltimore licences en-
tailments such as John has arrived in Baltimore
and John will leave Baltimore, which can only be
understood through both knowledge of tense and
lexical semantic relations.
The requisite information about lexical seman-
tics could be represented by labelling edges in the
entailment graphs, along the lines of Scaria et al.
(2013). Instead of edges simply representing en-
tailment, they should represent different kinds of
lexical relations, such as precondition or conse-
quence. Building such graphs requires training
classifiers that predict fine-grained semantic rela-
tions between predicates, and defining transitivity
properties of the relations (e.g. a precondition of a
precondition is a precondition). For example, the
system might learn the following graph:
</bodyText>
<figure confidence="0.9069555">
1 visitarg0,arg1
initiated by terminated by
3
leavearg0,arg1
exitarg0,arg1
departarg0,from
</figure>
<bodyText confidence="0.998770333333333">
By defining a simple mapping between edge la-
bels and logical forms, this graph can be converted
to CCG lexical entries such as:
</bodyText>
<equation confidence="0.820041">
visit (S\NP)/NP
: AyAxAe.rel1(x, y, e) n
le&apos;[rel2(x, y, e&apos;) n before(e, e&apos;)] n
le&apos;&apos;[rel3(x, y, e&apos;&apos;) n after(e, e&apos;&apos;)]
arrive (S\NP)/PPin
: AyAxAe.rel2(x, y, e)
leave (S\NP)/NP
: AyAxAe.rel3(x, y, e)
</equation>
<bodyText confidence="0.999266">
These lexical entries could be complemented
with hand-built interpretations for a small set of
common auxiliary verbs:
</bodyText>
<equation confidence="0.991870666666667">
has (S\NP)/(Sb\NP)
: ApAxAe.before(r, e) n p(x, e)
will (S\NP)/(Sb\NP)
: ApAxAe.after(r, e) n p(x, e)
is (S\NP)/(Sng\NP)
: ApAxAe.during(r, e) n p(x, e)
used (S\NP)/(Sto\NP)
: ApAxAe.before(r, e) n p(x, e) n
¬le&apos;[during(r) n p(x, e&apos;)]
</equation>
<bodyText confidence="0.9947669375">
Here, r is the reference time (e.g. the time that
the news article was written). It is easy to verify
that such a lexicon supports inferences such as is
visiting→will leave, has visited→has arrived in,
or used to be president→is not president.
The model described here only discusses tense,
not aspect—so does not distinguish between John
arrived in Baltimore and John has arrived in Bal-
timore (the latter says that the consequences of his
arrival still hold—i.e. that he is still in Baltimore).
Going further, we could implement the much more
detailed proposal of Moens and Steedman (1988).
Building this model would require distinguishing
states from events—for example, the semantics of
arrive, visit and leave could all be expressed in
terms of the times that an is in state holds.
</bodyText>
<subsectionHeader confidence="0.997187">
2.3 Intensional Semantics
</subsectionHeader>
<bodyText confidence="0.999969333333333">
Similar work could be done by subcatego-
rizing edges in the graph with other lexi-
cal relations. For example, we could ex-
tend the graph with goal relations between
words, such as between set out for and ar-
rive in, search and find, or invade and conquer:
</bodyText>
<equation confidence="0.817094">
1
set outarg0,for
headarg0,to
</equation>
<bodyText confidence="0.981789428571429">
The corresponding lexicon contains entries such
as:
set out (S\NP)/PPfor
: AyAxAe.rel1(x, y, e) n
ole&apos;[goal(e, e&apos;) n rel2(x, y, e&apos;)]
The modal logic o operator is used to mark that
the goal event is a hypothetical proposition, that
is not asserted to be true in the real world—so
Columbus set out for India7→Columbus reached
India. The same mechanism allows us to handle
Montague (1973)’s example that John seeks a uni-
corn does not imply the existence of a unicorn.
Just as temporal information can be expressed
by auxiliary verbs, relations such as goals can
</bodyText>
<page confidence="0.518455">
2
</page>
<figure confidence="0.969749875">
arrivearg0,in
reacharg0,arg1
2
arrivearg0,in
reacharg0,arg1
goal
30
Columbus failed to reach India
Sdcl/(Sdcl\NP) (Sdcl\NP)/(Sto\NP) (Sto\NP)/(Sb\NP) Sb\NP
Ap.p(Columbus) ApAxAe. 0 3e&apos;[p(x, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[p(x, e&apos;&apos;)] ApAxAe.p(x, e) AxAe.rel2(x, India, e)
Sto\NP
AxAe.rel2(x, India, e)
Sdcl\NP
AxAe. 0 3e&apos;[rel2(x, India, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[rel2(x, India, e&apos;&apos;)]
Sdcl
Ae. 0 3e&apos;[rel2(Columbus, India, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[rel2(Columbus, India, e&apos;&apos;)]
</figure>
<figureCaption confidence="0.996632">
Figure 1: Output from our system for the sentence Columbus failed to reach India
</figureCaption>
<figure confidence="0.939544">
&lt;
&gt;
&gt;
&gt;
</figure>
<bodyText confidence="0.953679708333333">
be expressed using implicative verbs like try or
fail. As the semantics of implicative verbs is of-
ten complex (Karttunen, 1971), we propose hand-
coding their lexical entries:
try (S\NP)/(Sto\NP)
: λpλxλe.oIe0[goal(e, e0)∧p(x, e0)]
fail (S\NP)/(Sto\NP)
: λpλxλe.oIe0[goal(e, e0)∧p(x, e0)]∧
-,Ie00[goal(e, e00) ∧ p(x, e00)]
The o operator is used to assert that the comple-
ment of try is a hypothetical proposition (so try to
reach74reach). Our semantics for fail is the same
as that for try, except that it asserts that the goal
event did not occur in the real world.
These lexical entries allow us to make complex
compositional inferences, for example Columbus
failed to reach India now entails Columbus set
out for India, Columbus tried to reach India and
Columbus didn’t arrive in India.
Again, we expect that the improved model of
formal semantics should increase the quality of
the entailment graphs, by allowing us to only clus-
ter predicates based on their real-world arguments
(ignoring hypothetical events).
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.9999555">
We have argued that several promising recent
threads of research in semantics can be combined
into a single model. The model we have described
would enable wide-coverage mapping of open-
domain text onto rich logical forms that model
complex aspects of semantics such as negation,
quantification, modality and tense—whilst also
using a robust distributional model of lexical se-
mantics that captures the structure of events. Con-
sidering these interwined issues would allow com-
plex compositional inferences which are far be-
yond the current state of the art, and would give
a more powerful model for natural language un-
derstanding.
</bodyText>
<sectionHeader confidence="0.996398" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9996298">
We thank Omri Abend, Michael Roth and the
anonymous reviewers for their helpful comments.
This work was funded by ERC Advanced Fellow-
ship 249520 GRAMPLUS and IP EC-FP7-270273
Xperience.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975715625">
M. Baroni, R. Bernardi, and R. Zamparelli. 2013.
Frege in space: A program for compositional dis-
tributional semantics. Linguistic Issues in Language
Technologies.
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets markov: Deep semantics with
probabilistic logical form. pages 11–21, June.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ’11, pages
610–619. Association for Computational Linguis-
tics.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
L. Karttunen. 1971. The Logic of English Predi-
cate Complement Constructions. Linguistics Club
Bloomington, Ind: IU Linguistics Club. Indiana
University Linguistics Club.
Mike Lewis and Mark Steedman. 2013. Combined
Distributional and Logical Semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.
G.A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.
</reference>
<page confidence="0.997673">
31
</page>
<reference confidence="0.9992875625">
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15–28.
Richard Montague. 1973. The proper treatment of
quantification in ordinary english. In Approaches to
natural language, pages 221–242. Springer.
Aju Thalappillil Scaria, Jonathan Berant, Mengqiu
Wang, Peter Clark, Justin Lewis, Brittany Harding,
and Christopher D. Manning. 2013. Learning bi-
ological processes with global constraints. In Pro-
ceedings of EMNLP.
Mark Steedman. 2012. Taking Scope: The Natural
Semantics of Quantifiers. MIT Press.
Congle Zhang and Daniel S Weld. 2013. Harvest-
ing parallel news streams to generate paraphrases of
event relations.
</reference>
<page confidence="0.999294">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.996509">Combining Formal and Distributional Models of Temporal Intensional Semantics</title>
<author confidence="0.996581">Mike</author>
<affiliation confidence="0.843558">School of University of Edinburgh, EH8 9AB,</affiliation>
<email confidence="0.997548">mike.lewis@ed.ac.uk</email>
<abstract confidence="0.9970281">We outline a vision for computational semantics in which formal compositional semantics is combined with a powerful, structured lexical semantics derived from distributional statistics. We consider how existing work (Lewis and Steedman, 2013) could be extended with a much richer lexical semantics using recent techniques for modelling processes (Scaria et al., example, learning that visitstart with end with We show how to closely integrate this information with theories of formal semantics, allowing complex compoinferences such as in but will which requires interpreting both the function and content words. This will allow machine reading to understand not just but 1 Combined Distributional and Logical Semantics Distributional semantics aims to induce the meaning of language from unlabelled text. Traditional approaches to distributional semantics have represented semantics in vector spaces (Baroni et al., 2013). Words are assigned vectors based on collocations in large corpora, and then these vectors a composed into vectors representing longer utterances. However, so far there is relatively limited empirical evidence that composed vectors provide useful representations for whole sentences, and it is unclear how to represent logical operators (such as universal quantifiers) in a vector space. While future breakthroughs may overcome these limitations, there are already well developed solutions in the formal semantics literature using logical representations. On the other hand, standard formal semantic approaches such as Bos (2008) have</abstract>
<author confidence="0.98196">Mark</author>
<affiliation confidence="0.9986445">School of University of</affiliation>
<address confidence="0.513429">Edinburgh, EH8 9AB,</address>
<email confidence="0.639415">steedman@inf.ed.ac.uk</email>
<abstract confidence="0.990863652380952">found that hand-built ontologies such as Word- Net (Miller, 1995) provide an insufficient model of lexical semantics, leading to low recall on applications. The complementary strengths and weaknesses of formal and distributional semantics motivate combining them into a single model. In Lewis and Steedman (2013), we proposed a solution to these problems which uses CCG (Steedman, 2012) as a model of formal semantics, making it straightforward to build wide-coverage logical forms. Hand built representations are added for a small number of function words such as negatives and quantifiers—but the lexical semantics is represented by first clustering predicates (based on their usage in large corpora), and then using the cluster-identifiers as symbols in the logical form. For example, the induced CCG leximight contain entries such as the y, y, sentences like wrote is the author of then both be mapped to a form, using derivations such as: Shakespeare wrote Macbeth macbeth, S macbeth, This approach interacts seamlessly with standard formal semantics—for example modelling by mapping Bacon didn’t write bacon, Their method has shown good performance on a dataset of multi-sentence textual inference probinvolving quantifiers, by using first-order theare Davidsonian event variables. NP y, &gt; 28 of the ACL 2014 Workshop on Semantic pages Maryland USA, June 26 2014. Association for Computational Linguistics orem proving. Ambiguity is handled by a probabilistic model, based on the types of the nouns. Beltagy et al. (2013) use an alternative approach with similar goals, in which every word instance expresses a unique semantic primitive, but is connected to the meanings of other word instances using distributionally-derived probabilistic inference rules. This approach risks requiring very large number of inference rules, which may make inference inefficient. Our approach avoid this problem by attempting to fully represent lexical semantics in the lexicon. 2 Proposal We propose how our previous model could be extended to make more sophisticated inferences. We will demonstrate how many interesting problems in semantics could be solved with a system based on three components: • A CCG syntactic parse for modelling composition. Using CCG allows us to handle interesting forms of composition, such as coordination, extraction, questions, right node raising, etc. CCG also has both a developed theory of operator semantics and a transparent interface to the underlying predicate argument structure. • A small hand built lexicon for words with complex semantics—such as negatives, quantifiers, modals, and implicative verbs. • A rich model of lexical semantics derived from distributionally-induced entailment graphs (Berant et al., 2011), extended with subcategories of entailment relations in a similar way to Scaria et al. (2013). We show how such graphs can be converted into a CCG lexicon. 2.1 Directional Inference A major limitation of our previous model is that it uses a flat clustering to model the meaning of content words. This method enables them to model synonymy relations between words, but not relations where the entailment only holds in one direction—for exbut not vice-versa. problem can be addressed using the engraph introduced by Berant et al. (2011), which learns globally consistent graphs over predicates in which directed edges indicate entailment relations. Exactly the same methods can be used to build entailment graphs over the predicates derived from a CCG parse: The graph can then be converted to a CCG lexicon by making the semantics of a word be the conjunction of all the relation identifiers it implies in the graph. For example, the above graph is equivalent to the following lexicon: y, y, y, y, y, y, y, y, This lexicon supports the correct infersuch as 2.2 Temporal Semantics One case where combining formal and distributional semantics may be particularly helpful is in giving a detailed model of temporal semantics. A rich understanding of time would allow us to untook place, or when states were true. Most existing work ignores tense, and treat the expressions to be president president as equivalent or completely unrelated. Failing to model tense would lead to incorrect inferences when answering questions such is the president of the USA? Another motivation for considering a detailed model of temporal semantics is that understanding the time of events should improve the quality of the distributional clustering. It has recently been shown that such information is extremely useful for learning equivalences between predicates, by determining which sentences describe the same 2 4 3 29 events using date-stamped text and simple tense heuristics (Zhang and Weld, 2013). Such methods escape common problems with traditional approaches to distributional similarity, such as conflating causes with effects, and may prove very useful for building entailment graphs. Temporal information is conveyed by both by verbs such as and in the semantics of content words. For example, the is visiting Baltimore ensuch as has arrived in Baltimore will leave which can only be understood through both knowledge of tense and lexical semantic relations. The requisite information about lexical semantics could be represented by labelling edges in the entailment graphs, along the lines of Scaria et al. (2013). Instead of edges simply representing entailment, they should represent different kinds of relations, such as conse- Building such graphs requires training classifiers that predict fine-grained semantic relations between predicates, and defining transitivity properties of the relations (e.g. a precondition of a precondition is a precondition). For example, the system might learn the following graph: initiated by terminated by 3 By defining a simple mapping between edge labels and logical forms, this graph can be converted to CCG lexical entries such as: y, y, y, y, y, These lexical entries could be complemented with hand-built interpretations for a small set of common auxiliary verbs: the reference time (e.g. the time that the news article was written). It is easy to verify such a lexicon supports inferences such as arrived to be not The model described here only discusses tense, aspect—so does not distinguish between in Baltimore has arrived in Ballatter says that the consequences of his arrival still hold—i.e. that he is still in Baltimore). Going further, we could implement the much more detailed proposal of Moens and Steedman (1988). Building this model would require distinguishing example, the semantics of all be expressed in of the times that an in holds. 2.3 Intensional Semantics Similar work could be done by subcategoedges in the graph with other lexirelations. For example, we could the graph with between such as between out for aror 1 The corresponding lexicon contains entries such as: out y, y, modal logic is used to mark that the goal event is a hypothetical proposition, that is not asserted to be true in the real world—so set out for reached The same mechanism allows us to handle (1973)’s example that seeks a uninot imply the existence of a unicorn. Just as temporal information can be expressed by auxiliary verbs, relations such as goals can 2 2 goal 30</abstract>
<note confidence="0.721912833333333">Columbus failed to reach India ApAxAe. 0 n e)] n ApAxAe.p(x, e) AxAe.rel2(x, India, e) AxAe.rel2(x, India, e) 0 India, n e)] n India, 0 India, n e)] n India, 1: Output from our system for the sentence failed to reach India</note>
<abstract confidence="0.985513731707317">lt; &gt; &gt; &gt; expressed using implicative verbs like As the semantics of implicative verbs is often complex (Karttunen, 1971), we propose handcoding their lexical entries: is used to assert that the compleof a hypothetical proposition (so to Our semantics for the same that for except that it asserts that the goal event did not occur in the real world. These lexical entries allow us to make complex inferences, for example to reach India entails set for tried to reach India didn’t arrive in Again, we expect that the improved model of formal semantics should increase the quality of the entailment graphs, by allowing us to only cluster predicates based on their real-world arguments (ignoring hypothetical events). 3 Conclusion We have argued that several promising recent threads of research in semantics can be combined into a single model. The model we have described would enable wide-coverage mapping of opendomain text onto rich logical forms that model complex aspects of semantics such as negation, quantification, modality and tense—whilst also using a robust distributional model of lexical semantics that captures the structure of events. Considering these interwined issues would allow complex compositional inferences which are far beyond the current state of the art, and would give a more powerful model for natural language understanding. Acknowledgements We thank Omri Abend, Michael Roth and the anonymous reviewers for their helpful comments.</abstract>
<note confidence="0.9721326">This work was funded by ERC Advanced Fellowship 249520 GRAMPLUS and IP EC-FP7-270273 Xperience. References M. Baroni, R. Bernardi, and R. Zamparelli. 2013.</note>
<title confidence="0.802531">Frege in space: A program for compositional dissemantics. Issues in Language</title>
<author confidence="0.961997">Islam Beltagy</author>
<author confidence="0.961997">Cuong Chau</author>
<author confidence="0.961997">Gemma Boleda</author>
<author confidence="0.961997">Dan Gar-</author>
<abstract confidence="0.791625692307692">rette, Katrin Erk, and Raymond Mooney. 2013. Montague meets markov: Deep semantics with probabilistic logical form. pages 11–21, June. Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Technologies - Volume HLT ’11, pages 610–619. Association for Computational Linguistics. Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Deleditors, in Text Processing. STEP</abstract>
<note confidence="0.809898">Conference Research in Computational Semantics, pages 277–286. College Publications. Karttunen. 1971. Logic of English Predi-</note>
<affiliation confidence="0.764184666666667">Complement Linguistics Club Bloomington, Ind: IU Linguistics Club. Indiana University Linguistics Club.</affiliation>
<note confidence="0.791717875">Mike Lewis and Mark Steedman. 2013. Combined and Logical Semantics. the Association for Computational 1:179–192. G.A. Miller. 1995. Wordnet: a lexical database for of the 38(11):39– 41. 31</note>
<author confidence="0.574715">Temporal on-</author>
<abstract confidence="0.7975704">and temporal reference. lin- 14(2):15–28. Richard Montague. 1973. The proper treatment of in ordinary english. In to pages 221–242. Springer.</abstract>
<title confidence="0.306423">Aju Thalappillil Scaria, Jonathan Berant, Mengqiu</title>
<author confidence="0.908759">Peter Clark Wang</author>
<author confidence="0.908759">Justin Lewis</author>
<author confidence="0.908759">Brittany Harding</author>
<abstract confidence="0.773819625">and Christopher D. Manning. 2013. Learning biprocesses with global constraints. In Proof Steedman. 2012. Scope: The Natural of MIT Press. Congle Zhang and Daniel S Weld. 2013. Harvesting parallel news streams to generate paraphrases of event relations.</abstract>
<intro confidence="0.902719">32</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Bernardi</author>
<author>R Zamparelli</author>
</authors>
<title>Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technologies.</title>
<date>2013</date>
<contexts>
<context position="1228" citStr="Baroni et al., 2013" startWordPosition="174" endWordPosition="177">ith arriving and end with leaving. We show how to closely integrate this information with theories of formal semantics, allowing complex compositional inferences such as is visiting→has arrived in but will leave, which requires interpreting both the function and content words. This will allow machine reading systems to understand not just what has happened, but when. 1 Combined Distributional and Logical Semantics Distributional semantics aims to induce the meaning of language from unlabelled text. Traditional approaches to distributional semantics have represented semantics in vector spaces (Baroni et al., 2013). Words are assigned vectors based on collocations in large corpora, and then these vectors a composed into vectors representing longer utterances. However, so far there is relatively limited empirical evidence that composed vectors provide useful representations for whole sentences, and it is unclear how to represent logical operators (such as universal quantifiers) in a vector space. While future breakthroughs may overcome these limitations, there are already well developed solutions in the formal semantics literature using logical representations. On the other hand, standard formal semantic</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2013</marker>
<rawString>M. Baroni, R. Bernardi, and R. Zamparelli. 2013. Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Cuong Chau</author>
<author>Gemma Boleda</author>
<author>Dan Garrette</author>
<author>Katrin Erk</author>
<author>Raymond Mooney</author>
</authors>
<title>Montague meets markov: Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<pages>11--21</pages>
<contexts>
<context position="3897" citStr="Beltagy et al. (2013)" startWordPosition="572" endWordPosition="575">egation by mapping Francis Bacon didn’t write Macbeth to ¬rel43(francis bacon, macbeth). Their method has shown good performance on a dataset of multi-sentence textual inference problems involving quantifiers, by using first-order the1The e variables are Davidsonian event variables. NP (S\NP)/NP NP shakespeare AyAxAe.rel43(x, y, e) macbeth &gt; 28 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 28–32, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics orem proving. Ambiguity is handled by a probabilistic model, based on the types of the nouns. Beltagy et al. (2013) use an alternative approach with similar goals, in which every word instance expresses a unique semantic primitive, but is connected to the meanings of other word instances using distributionally-derived probabilistic inference rules. This approach risks requiring very large number of inference rules, which may make inference inefficient. Our approach avoid this problem by attempting to fully represent lexical semantics in the lexicon. 2 Proposal We propose how our previous model could be extended to make more sophisticated inferences. We will demonstrate how many interesting problems in sema</context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Garrette, Katrin Erk, and Raymond Mooney. 2013. Montague meets markov: Deep semantics with probabilistic logical form. pages 11–21, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>610--619</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5118" citStr="Berant et al., 2011" startWordPosition="761" endWordPosition="764">tics could be solved with a system based on three components: • A CCG syntactic parse for modelling composition. Using CCG allows us to handle interesting forms of composition, such as coordination, extraction, questions, right node raising, etc. CCG also has both a developed theory of operator semantics and a transparent interface to the underlying predicate argument structure. • A small hand built lexicon for words with complex semantics—such as negatives, quantifiers, modals, and implicative verbs. • A rich model of lexical semantics derived from distributionally-induced entailment graphs (Berant et al., 2011), extended with subcategories of entailment relations in a similar way to Scaria et al. (2013). We show how such graphs can be converted into a CCG lexicon. 2.1 Directional Inference A major limitation of our previous model is that it uses a flat clustering to model the meaning of content words. This method enables them to model synonymy relations between words, but not relations where the entailment only holds in one direction—for example, conquers—*invades, but not vice-versa. This problem can be addressed using the entailment graph framework introduced by Berant et al. (2011), which learns </context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 610–619. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics,</booktitle>
<pages>277--286</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="1858" citStr="Bos (2008)" startWordPosition="270" endWordPosition="271"> vectors based on collocations in large corpora, and then these vectors a composed into vectors representing longer utterances. However, so far there is relatively limited empirical evidence that composed vectors provide useful representations for whole sentences, and it is unclear how to represent logical operators (such as universal quantifiers) in a vector space. While future breakthroughs may overcome these limitations, there are already well developed solutions in the formal semantics literature using logical representations. On the other hand, standard formal semantic approaches such as Bos (2008) have Mark Steedman School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK steedman@inf.ed.ac.uk found that hand-built ontologies such as WordNet (Miller, 1995) provide an insufficient model of lexical semantics, leading to low recall on applications. The complementary strengths and weaknesses of formal and distributional semantics motivate combining them into a single model. In Lewis and Steedman (2013), we proposed a solution to these problems which uses CCG (Steedman, 2012) as a model of formal semantics, making it straightforward to build wide-coverage logical forms. Hand bui</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>The Logic of English Predicate Complement Constructions.</title>
<date>1971</date>
<institution>Linguistics Club Bloomington, Ind: IU Linguistics Club. Indiana University Linguistics Club.</institution>
<contexts>
<context position="11811" citStr="Karttunen, 1971" startWordPosition="1822" endWordPosition="1823">umbus failed to reach India Sdcl/(Sdcl\NP) (Sdcl\NP)/(Sto\NP) (Sto\NP)/(Sb\NP) Sb\NP Ap.p(Columbus) ApAxAe. 0 3e&apos;[p(x, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[p(x, e&apos;&apos;)] ApAxAe.p(x, e) AxAe.rel2(x, India, e) Sto\NP AxAe.rel2(x, India, e) Sdcl\NP AxAe. 0 3e&apos;[rel2(x, India, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[rel2(x, India, e&apos;&apos;)] Sdcl Ae. 0 3e&apos;[rel2(Columbus, India, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[rel2(Columbus, India, e&apos;&apos;)] Figure 1: Output from our system for the sentence Columbus failed to reach India &lt; &gt; &gt; &gt; be expressed using implicative verbs like try or fail. As the semantics of implicative verbs is often complex (Karttunen, 1971), we propose handcoding their lexical entries: try (S\NP)/(Sto\NP) : λpλxλe.oIe0[goal(e, e0)∧p(x, e0)] fail (S\NP)/(Sto\NP) : λpλxλe.oIe0[goal(e, e0)∧p(x, e0)]∧ -,Ie00[goal(e, e00) ∧ p(x, e00)] The o operator is used to assert that the complement of try is a hypothetical proposition (so try to reach74reach). Our semantics for fail is the same as that for try, except that it asserts that the goal event did not occur in the real world. These lexical entries allow us to make complex compositional inferences, for example Columbus failed to reach India now entails Columbus set out for India, Columb</context>
</contexts>
<marker>Karttunen, 1971</marker>
<rawString>L. Karttunen. 1971. The Logic of English Predicate Complement Constructions. Linguistics Club Bloomington, Ind: IU Linguistics Club. Indiana University Linguistics Club.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<date>2013</date>
<booktitle>Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics,</booktitle>
<pages>1--179</pages>
<contexts>
<context position="2279" citStr="Lewis and Steedman (2013)" startWordPosition="330" endWordPosition="333">me these limitations, there are already well developed solutions in the formal semantics literature using logical representations. On the other hand, standard formal semantic approaches such as Bos (2008) have Mark Steedman School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK steedman@inf.ed.ac.uk found that hand-built ontologies such as WordNet (Miller, 1995) provide an insufficient model of lexical semantics, leading to low recall on applications. The complementary strengths and weaknesses of formal and distributional semantics motivate combining them into a single model. In Lewis and Steedman (2013), we proposed a solution to these problems which uses CCG (Steedman, 2012) as a model of formal semantics, making it straightforward to build wide-coverage logical forms. Hand built representations are added for a small number of function words such as negatives and quantifiers—but the lexical semantics is represented by first clustering predicates (based on their usage in large corpora), and then using the cluster-identifiers as symbols in the logical form. For example, the induced CCG lexicon might contain entries such as the following1: write (S\NP)/NP : AyAxAe.rel43(x, y, e) author N/PPof </context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined Distributional and Logical Semantics. Transactions of the Association for Computational Linguistics, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>41</pages>
<contexts>
<context position="2032" citStr="Miller, 1995" startWordPosition="294" endWordPosition="295"> empirical evidence that composed vectors provide useful representations for whole sentences, and it is unclear how to represent logical operators (such as universal quantifiers) in a vector space. While future breakthroughs may overcome these limitations, there are already well developed solutions in the formal semantics literature using logical representations. On the other hand, standard formal semantic approaches such as Bos (2008) have Mark Steedman School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK steedman@inf.ed.ac.uk found that hand-built ontologies such as WordNet (Miller, 1995) provide an insufficient model of lexical semantics, leading to low recall on applications. The complementary strengths and weaknesses of formal and distributional semantics motivate combining them into a single model. In Lewis and Steedman (2013), we proposed a solution to these problems which uses CCG (Steedman, 2012) as a model of formal semantics, making it straightforward to build wide-coverage logical forms. Hand built representations are added for a small number of function words such as negatives and quantifiers—but the lexical semantics is represented by first clustering predicates (b</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39– 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Moens</author>
<author>Mark Steedman</author>
</authors>
<title>Temporal ontology and temporal reference.</title>
<date>1988</date>
<journal>Computational linguistics,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="10058" citStr="Moens and Steedman (1988)" startWordPosition="1537" endWordPosition="1540">x, e) n ¬le&apos;[during(r) n p(x, e&apos;)] Here, r is the reference time (e.g. the time that the news article was written). It is easy to verify that such a lexicon supports inferences such as is visiting→will leave, has visited→has arrived in, or used to be president→is not president. The model described here only discusses tense, not aspect—so does not distinguish between John arrived in Baltimore and John has arrived in Baltimore (the latter says that the consequences of his arrival still hold—i.e. that he is still in Baltimore). Going further, we could implement the much more detailed proposal of Moens and Steedman (1988). Building this model would require distinguishing states from events—for example, the semantics of arrive, visit and leave could all be expressed in terms of the times that an is in state holds. 2.3 Intensional Semantics Similar work could be done by subcategorizing edges in the graph with other lexical relations. For example, we could extend the graph with goal relations between words, such as between set out for and arrive in, search and find, or invade and conquer: 1 set outarg0,for headarg0,to The corresponding lexicon contains entries such as: set out (S\NP)/PPfor : AyAxAe.rel1(x, y, e) </context>
</contexts>
<marker>Moens, Steedman, 1988</marker>
<rawString>Marc Moens and Mark Steedman. 1988. Temporal ontology and temporal reference. Computational linguistics, 14(2):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary english. In Approaches to natural language,</title>
<date>1973</date>
<pages>221--242</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10948" citStr="Montague (1973)" startWordPosition="1690" endWordPosition="1691">n the graph with other lexical relations. For example, we could extend the graph with goal relations between words, such as between set out for and arrive in, search and find, or invade and conquer: 1 set outarg0,for headarg0,to The corresponding lexicon contains entries such as: set out (S\NP)/PPfor : AyAxAe.rel1(x, y, e) n ole&apos;[goal(e, e&apos;) n rel2(x, y, e&apos;)] The modal logic o operator is used to mark that the goal event is a hypothetical proposition, that is not asserted to be true in the real world—so Columbus set out for India7→Columbus reached India. The same mechanism allows us to handle Montague (1973)’s example that John seeks a unicorn does not imply the existence of a unicorn. Just as temporal information can be expressed by auxiliary verbs, relations such as goals can 2 arrivearg0,in reacharg0,arg1 2 arrivearg0,in reacharg0,arg1 goal 30 Columbus failed to reach India Sdcl/(Sdcl\NP) (Sdcl\NP)/(Sto\NP) (Sto\NP)/(Sb\NP) Sb\NP Ap.p(Columbus) ApAxAe. 0 3e&apos;[p(x, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[p(x, e&apos;&apos;)] ApAxAe.p(x, e) AxAe.rel2(x, India, e) Sto\NP AxAe.rel2(x, India, e) Sdcl\NP AxAe. 0 3e&apos;[rel2(x, India, e&apos;) n goal(e&apos;, e)] n -3e&apos;&apos;[rel2(x, India, e&apos;&apos;)] Sdcl Ae. 0 3e&apos;[rel2(Columbus, India, e&apos;) n go</context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Richard Montague. 1973. The proper treatment of quantification in ordinary english. In Approaches to natural language, pages 221–242. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aju Thalappillil Scaria</author>
<author>Jonathan Berant</author>
<author>Mengqiu Wang</author>
<author>Peter Clark</author>
<author>Justin Lewis</author>
<author>Brittany Harding</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning biological processes with global constraints.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5212" citStr="Scaria et al. (2013)" startWordPosition="776" endWordPosition="779">lling composition. Using CCG allows us to handle interesting forms of composition, such as coordination, extraction, questions, right node raising, etc. CCG also has both a developed theory of operator semantics and a transparent interface to the underlying predicate argument structure. • A small hand built lexicon for words with complex semantics—such as negatives, quantifiers, modals, and implicative verbs. • A rich model of lexical semantics derived from distributionally-induced entailment graphs (Berant et al., 2011), extended with subcategories of entailment relations in a similar way to Scaria et al. (2013). We show how such graphs can be converted into a CCG lexicon. 2.1 Directional Inference A major limitation of our previous model is that it uses a flat clustering to model the meaning of content words. This method enables them to model synonymy relations between words, but not relations where the entailment only holds in one direction—for example, conquers—*invades, but not vice-versa. This problem can be addressed using the entailment graph framework introduced by Berant et al. (2011), which learns globally consistent graphs over predicates in which directed edges indicate entailment relatio</context>
<context position="8263" citStr="Scaria et al. (2013)" startWordPosition="1260" endWordPosition="1263">rity, such as conflating causes with effects, and may prove very useful for building entailment graphs. Temporal information is conveyed by both by auxiliary verbs such as will or used to, and in the semantics of content words. For example, the statement John is visiting Baltimore licences entailments such as John has arrived in Baltimore and John will leave Baltimore, which can only be understood through both knowledge of tense and lexical semantic relations. The requisite information about lexical semantics could be represented by labelling edges in the entailment graphs, along the lines of Scaria et al. (2013). Instead of edges simply representing entailment, they should represent different kinds of lexical relations, such as precondition or consequence. Building such graphs requires training classifiers that predict fine-grained semantic relations between predicates, and defining transitivity properties of the relations (e.g. a precondition of a precondition is a precondition). For example, the system might learn the following graph: 1 visitarg0,arg1 initiated by terminated by 3 leavearg0,arg1 exitarg0,arg1 departarg0,from By defining a simple mapping between edge labels and logical forms, this gr</context>
</contexts>
<marker>Scaria, Berant, Wang, Clark, Lewis, Harding, Manning, 2013</marker>
<rawString>Aju Thalappillil Scaria, Jonathan Berant, Mengqiu Wang, Peter Clark, Justin Lewis, Brittany Harding, and Christopher D. Manning. 2013. Learning biological processes with global constraints. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Taking Scope: The Natural Semantics of Quantifiers.</title>
<date>2012</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2353" citStr="Steedman, 2012" startWordPosition="344" endWordPosition="345">tics literature using logical representations. On the other hand, standard formal semantic approaches such as Bos (2008) have Mark Steedman School of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK steedman@inf.ed.ac.uk found that hand-built ontologies such as WordNet (Miller, 1995) provide an insufficient model of lexical semantics, leading to low recall on applications. The complementary strengths and weaknesses of formal and distributional semantics motivate combining them into a single model. In Lewis and Steedman (2013), we proposed a solution to these problems which uses CCG (Steedman, 2012) as a model of formal semantics, making it straightforward to build wide-coverage logical forms. Hand built representations are added for a small number of function words such as negatives and quantifiers—but the lexical semantics is represented by first clustering predicates (based on their usage in large corpora), and then using the cluster-identifiers as symbols in the logical form. For example, the induced CCG lexicon might contain entries such as the following1: write (S\NP)/NP : AyAxAe.rel43(x, y, e) author N/PPof : AyAxAe.rel43(x, y, e) Equivalent sentences like Shakespeare wrote Macbet</context>
</contexts>
<marker>Steedman, 2012</marker>
<rawString>Mark Steedman. 2012. Taking Scope: The Natural Semantics of Quantifiers. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Harvesting parallel news streams to generate paraphrases of event relations.</title>
<date>2013</date>
<contexts>
<context position="7553" citStr="Zhang and Weld, 2013" startWordPosition="1147" endWordPosition="1150">lead to incorrect inferences when answering questions such as Who is the president of the USA? Another motivation for considering a detailed model of temporal semantics is that understanding the time of events should improve the quality of the distributional clustering. It has recently been shown that such information is extremely useful for learning equivalences between predicates, by determining which sentences describe the same 2 invadearg0,arg1 invasionposs,of 4 bombarg0,arg1 1 attackarg0,arg1 conquerarg0,arg1 annexarg0,arg1 3 29 events using date-stamped text and simple tense heuristics (Zhang and Weld, 2013). Such methods escape common problems with traditional approaches to distributional similarity, such as conflating causes with effects, and may prove very useful for building entailment graphs. Temporal information is conveyed by both by auxiliary verbs such as will or used to, and in the semantics of content words. For example, the statement John is visiting Baltimore licences entailments such as John has arrived in Baltimore and John will leave Baltimore, which can only be understood through both knowledge of tense and lexical semantic relations. The requisite information about lexical seman</context>
</contexts>
<marker>Zhang, Weld, 2013</marker>
<rawString>Congle Zhang and Daniel S Weld. 2013. Harvesting parallel news streams to generate paraphrases of event relations.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.997947">
From Visualisation to Hypothesis Construction
for Second Language Acquisition
</title>
<author confidence="0.997566">
Shervin Malmasi
</author>
<affiliation confidence="0.995525">
Centre for Language Technology
Macquarie University
</affiliation>
<address confidence="0.707399">
Sydney, NSW, Australia
</address>
<email confidence="0.997842">
shervin.malmasi@mq.edu.au
</email>
<author confidence="0.995669">
Mark Dras
</author>
<affiliation confidence="0.99547">
Centre for Language Technology
Macquarie University
</affiliation>
<address confidence="0.707796">
Sydney, NSW, Australia
</address>
<email confidence="0.998701">
mark.dras@mq.edu.au
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969529411765">
One research goal in Second Language Acqui-
sition (SLA) is to formulate and test hypothe-
ses about errors and the environments in which
they are made, a process which often involves
substantial effort; large amounts of data and
computational visualisation techniques promise
help here. In this paper we have defined a new
task for finding contexts for errors that vary
with the native language of the speaker that are
potentially useful for SLA research. We pro-
pose four models for approaching this task, and
find that one based only on error-feature co-
occurrence and another based on determining
maximum weight cliques in a feature associ-
ation graph discover strongly distinguishing
contexts, with an apparent trade-off between
false positives and very specific contexts.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935903225807">
SLA researchers are interested in a wide variety of as-
pects of humans learning a new language (L2) different
from their native one (L1): cognitive issues and devel-
opmental sequences for learners Pienemann (2005), so-
ciocultural factors (Lantolf, 2001), and so on. One long-
standing question, dating back to at least Lado (1957),
is expressed by Ortega (2009) in the following way:
“What is the role played by first language in L2 develop-
ment, vis-`a-vis the role of other universal development
forces?”
An example of SLA research that looks at this ques-
tion is the study of Di´ez-Bedmar and Papp (2008), com-
paring Chinese and Spanish learners of English with
respect to the English article system (a, an, the) using
corpora of essays by native and non-native speakers
of English (Granger, 2011). Drawing on the 175 non-
native texts, they take a particular theoretical analysis
(the so-called Bickerton semantic wheel), use the simple
Wordsmith tools designed to extract data for lexicogra-
phers to identify errors in a semi-automatic way, and
evaluate whether Chinese and Spanish L1 speakers do
behave differently via hypothesis testing (ANOVA, chi-
square and z-tests, in their case). They conclude that
Chinese and Spanish do have characteristic differences,
with patterns of zero article and definite article use dif-
fering according to semantic context. Such studies are
typically carried out on relatively small datasets, and
use fairly elementary tools. Sources such as Ellis (2008)
and Ortega (2009) give good overviews of such studies
and of SLA research in general.
A goal of this paper is to investigate a particular way
in which Natural Language Processing (NLP) can use-
fully contribute to SLA. In terms of existing work, the
subfield of Native Language Identification (NLI) has
been quite active recently, which looks at predicting
the L1 of writers writing in a common L2 within a
classification task framework; see for example the re-
cent NLI shared task with 29 entrants (Tetreault et al.,
2013).1 From within linguistics, there has been much
interest in how data-driven approaches can contribute to
SLA. Granger (2011) discusses a body of work based
on the the methodology of carrying out corpus-based
approaches to SLA with a focus on NLP tools; Jarvis
and Crossley (2012) in an edited collection present re-
cent work by linguists who extend the corpus-based
setup by using a text classification approach, looking at
what feature selection might say for SLA. From within
NLP, Swanson and Charniak (2013) and Swanson and
Charniak (2014) take a data-driven approach to SLA
investigations much in the spirit of this work.
One particular approach to finding aspects of texts
characteristic of their L1s that has motivated the present
work is described in Yannakoudakis et al. (2012), the
goal of which is to develop visualisation tools for SLA
researchers. They present graphs of the relationships
between errors and their contexts, such that SLA re-
searchers can navigate through the graphs to find con-
texts for particular errors that can lead to hypotheses
like that of Di´ez-Bedmar and Papp (2008) above. In this
paper, we look at approaches to finding such hypothesis
candidates automatically in the context of L1–L2 inter-
action by analysing the graphs used in the visualisations
</bodyText>
<footnote confidence="0.997372">
1http://sites.google.com/site/
nlisharedtask2013/
</footnote>
<page confidence="0.983474">
56
</page>
<note confidence="0.784538333333333">
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 56–64,
October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
and exists() a binary function returning true if the input
feature occurs in sk.
of Yannakoudakis et al. (2012). Specifically, we do the
following:
</note>
<listItem confidence="0.988009636363636">
• We propose a new task that is more directly ori-
ented to SLA research than NLI has been for the
most part, with the goal of identifying error-related
contexts that are characteristic of L1s.
• We evaluate a number of models for finding such
contexts, ranging from a simple baseline to treat-
ing the problem as a graph-theoretic maximum
weighted clique one.
• We examine the results of some of the models to
see how the task and the models might contribute
to SLA research.
</listItem>
<bodyText confidence="0.999911666666667">
Because we draw heavily on the work of Yan-
nakoudakis et al. (2012), we first review relevant aspects
of that work in §2; we then present our task definition
and experimental setup in §3; we give results along with
a discussion in §4; we follow with some more detail on
related work in §5; and we conclude in §6.
</bodyText>
<sectionHeader confidence="0.955161" genericHeader="method">
2 Developing Hypotheses: A
Visualisation Tool
</sectionHeader>
<bodyText confidence="0.99996475">
The context of the Yannakoudakis et al. (2012) work
is automated grading of English as a Second or Other
Language (ESOL) exam scripts, as described in Briscoe
et al. (2010). The automated grading takes a classifi-
cation approach, using a binary discriminative learner,
with useful features including lexical and part-of-speech
(PoS) n-grams.
The publicly available dataset on which the work was
carried out consists of texts from the First Certificate in
English (FCE) exam, aimed at upper-intermediate stu-
dents of English across various L1s, and was presented
in Yannakoudakis et al. (2011). This FCE corpus2 con-
sists of a subset of 1244 texts of the Cambridge Learner
Corpus,3 and is manually annotated with errors and their
corrections, as well as a classification according to an
error typology, as in Figure 1.
Yannakoudakis et al. (2012) present their English
Profile (EP) visualiser as a way to “visually analyse as
well as perform a linguistic interpretation of discrimi-
native features that characterise learner English”, using
the features of this essay classification task. They de-
fine a measure of co-occurrence of features, among
themselves and with errors, as a core part of their
analysis. Given the set of all sentences in the corpus
S = {s1, s2, ... , s|S|} and the set of all features F =
{f1, f2, ... , f|F|}, a feature fi E F is associated with
a feature fj E F (i =� j,1 &lt; i, j &lt; M) according to
the score given in Equation (1), for sk E S,1 &lt; k &lt; N
</bodyText>
<footnote confidence="0.99225375">
2http://ilexir.co.uk/applications/
ep-visualiser/
3http://www.cup.cam.ac.uk/gb/elt/
catalogue/subject/custom/item364603/
</footnote>
<equation confidence="0.9976785">
scoreff(fj, fi) = Ek=1 |S |exists(fj, fi, sk) (1)
EIk 11 exists(fi, sk)
</equation>
<bodyText confidence="0.7706085">
They mention an analogous measure for feature-error
co-occurrence; we assume given the set of all errors
</bodyText>
<equation confidence="0.9485155">
E = {e1, e2, ... , e|E|} that this is defined as follows:
score a
E|S |exists(fj, ei, sk) /
ef(fj, z) 2
— 1 l )
E |SI k=1 exists(ei, sk)
</equation>
<bodyText confidence="0.999635842105263">
A graph is defined with features and errors as vertices;
an edge between features (resp. features and errors) is
established if scoreff() (resp. scoreef) is within some
user-defined range. This graph of feature–feature (resp.
feature–error) relationships is then presented visually.
The paper then presents a case study of how the EP vi-
sualiser can be used to assist SLA researchers. The case
study starts by noting that RG_JJ_NN1 is the 18th most
discriminative negative feature from the essay classi-
fier; then, further inspecting the graph of discriminative
features, that it’s linked to JJ_NN1_II and VBZ_RG.
Then, looking at feature-error relations, it investigates
an association with error MD (missing determiner), and
presents some examples that match the features (e.g.
Unix is very powerful system but there is one thing
against it), along with a discussion of relationships to
various L1s. It is this process of finding interesting fea-
tures and linking them to particular errors and L1s that
we present an approach to automating in this paper.
</bodyText>
<sectionHeader confidence="0.993934" genericHeader="method">
3 Task Definition &amp; Experimental Setup
</sectionHeader>
<bodyText confidence="0.9999252">
At a general level, our goal is to find which kinds of
constructions (in a loose sense) centred around errors
are particularly characteristic of various L1s.
The specific task we define for this paper, then, is
to select a set of features (in the terminology of Yan-
nakoudakis et al. (2012))—which we refer to as the
ERROR CONTEXT—that, when combined with the er-
ror, show a strong association with L1, in a manner
we describe below. So, for example, this may involve
finding that an MD error in the context of RG_JJ_NN1,
JJ_NN1_II and VBZ_RG shows a strong association
with L1. We investigate a number of models for this
selection process: the task then is the identification of
which models produce poor error contexts (which will
not rank highly in hypothesis testing) and which pro-
duce good ones (potentially worth considering by an
SLA researcher). Below we discuss the data we use,
the measure of association for an error and its context,
the set of errors chosen, and the models for selecting
context.
</bodyText>
<subsectionHeader confidence="0.995811">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.997286">
The corpus we use for evaluating the models for our task
is derived from the FCE corpus of Yannakoudakis et al.
</bodyText>
<page confidence="0.998704">
57
</page>
<table confidence="0.97503975">
Verb Agreement &lt;p&gt;Some people &lt;ns type=&amp;quot;AGV&amp;quot;&gt;&lt;i&gt;says&lt;/i&gt;&lt;c&gt;say&lt;/c&gt;&lt;/ns&gt; ...&lt;/p&gt;
Incorrect Verb &lt;p&gt;The day I &lt;ns type=&amp;quot;IV&amp;quot;&gt;&lt;i&gt;shaked&lt;/i&gt;&lt;c&gt;shook&lt;/c&gt;&lt;/ns&gt; their
Inflection hands,...&lt;/p&gt;
Missing Determiner &lt;p&gt;I am &lt;ns type=&amp;quot;MD&amp;quot;&gt;&lt;c&gt;a&lt;/c&gt;&lt;/ns&gt; really good singer.&lt;/p&gt;
</table>
<figureCaption confidence="0.566318">
Figure 1: FCE corpus examples. Error types indicated by &lt;ns type&gt;...&lt;/ns&gt;; errors indicated by &lt;i&gt;...&lt;/i&gt;;
corrections indicated by &lt;c&gt;...&lt;/c&gt;.
</figureCaption>
<table confidence="0.999443555555556">
language size
Chinese CHI 66
French FRE 146
German GER 69
Italian ITA 76
Japanese JAP 81
Korean KOR 86
Spanish SPA 200
Turkish TUR 75
</table>
<tableCaption confidence="0.999926">
Table 1: FCESUB, broken down by language
</tableCaption>
<bodyText confidence="0.9929468">
(2012). The full FCE corpus consists of 1244 scripts
over 16 languages; script counts range from 2 (Dutch)
to 200 (Spanish).
The features used by Yannakoudakis et al. (2012)
were derived from their essay classification task. As we
are interested in associations with L1, we instead use
features from a system submitted to the NLI shared task
(Anonymous, 2013), which was applied to a dataset of
Test of English as a Foreign Language (TOEFL) scripts:
the task and its designated corpus are described in the
task overview paper (Tetreault et al., 2013). In this work
we use a system trained on the TOEFL11 corpus con-
sisting of texts written in English from speakers of 11
different L1s, with 1100 essays per L1 and balanced
across topic. We only use PoS n-grams (n = 1, 2,3) as
features in this work. Note that we use the terminology
of Yannakoudakis et al. (2012) here: what had their
origin as features in the essay classification task are still
referred to as features in the visualisation tool, although
the task carried out there is not a classification one. Sim-
ilarly, we refer to our PoS n-grams as features, although
we are not classifying errors using these features and
so are not carrying out feature selection for the typical
purpose of optimising classification performance.
For this, as did Yannakoudakis et al. (2012), we use
the RASP parser (Briscoe et al., 2006) for tagging; the
tags are consequently from the CLAWS2 tagset,4 which
are more fine-grained in terms of linguistic analysis than
the more frequently used Penn Treebank tags.
For our task, we then used the subset of the FCE cor-
pus where the languages overlapped with the TOEFL11
corpus: we refer to this as FCESUB. This gives 799
scripts over 8 languages, distributed as in Table 1; a
positive byproduct is that the L1s are more similar in
size than the full FCE corpus.
</bodyText>
<footnote confidence="0.966326">
4http://ucrel.lancs.ac.uk/claws2tags.
html
</footnote>
<table confidence="0.968889">
language mean
CHI 0.885790
FRE 0.460894
GER 0.366587
ITA 0.581401
JAP 1.058159
KOR 1.067211
SPA 0.472253
TUR 1.014129
F-stat 18.031
sig. &lt;0.001
</table>
<tableCaption confidence="0.959772">
Table 2: ANOVA results giving mean score (number
</tableCaption>
<bodyText confidence="0.789912">
of sentences with MD error per 10 sentences) for each
language, the ANOVA F-statistic, and significance value
</bodyText>
<subsectionHeader confidence="0.999833">
3.2 Association Measure
</subsectionHeader>
<bodyText confidence="0.999871035714286">
We noted in §1 that SLA studies such as Di´ez-Bedmar
and Papp (2008) use standard hypothesis testing tech-
niques. We take this as a starting point. We could, for
example, evaluate whether a particular raw error (that
is, without a feature context) is strongly associated with
L1s by using a single factor ANOVA test.5 The indepen-
dent variable would be the L1. The dependent variable
could be one of a number of alternatives; we choose the
number of sentences with a particular error per 10 sen-
tences.6 To illustrate, we give the ANOVA results from
FCESUB for the MD error in Table 2. The ANOVA
calculation is based on an F-statistic which compares
variance between treatments against variance within
treatments; this is compared against critical values for
the F-statistic to determine statistical significance. The
expected value of the F-statistic under the null hypoth-
esis is 1, with values above 1 increasingly inconsistent
with the null hypothesis. The data in Table 2 shows
that the MD error does vary significantly with L1; a
post-hoc Tukey HSD test lets us identify which specific
languages exhibit this difference and shows that, for
example (and as can be observed in the means), German
L1 speakers are significantly different from Korean L1
speakers in the occurrence of MD errors.
For our task we are not interested in significance per
se. Rather, we are interested in whether we can find oc-
currences of errors plus contexts that are more strongly
associated with, or that vary across, L1s, e.g. that an
</bodyText>
<footnote confidence="0.99159025">
5See, e.g., Jackson (2009).
6We note that the texts differ significantly in length by L1,
so it would not be suitable to normalise as occurrences per
document.
</footnote>
<page confidence="0.99643">
58
</page>
<table confidence="0.999938">
type name F-stat p-val N
DJ Wrong Derived 3.27 .002 332
Adjective
DN Wrong Derived 0.70 .671 294
Noun
MD Missing Determiner 18.03 .000 1702
MT Missing Preposition 2.81 .007 985
UD Unnecessary Determiner 1.20 .301 807
UT Unnecessary Preposition 0.26 .968 689
UV Unnecessary Verb 0.78 .606 317
</table>
<tableCaption confidence="0.995437">
Table 3: Error types chosen for evaluation, including F-
</tableCaption>
<bodyText confidence="0.965023037037037">
statistic, ANOVA p-value and corpus count of sentences
containing error.
MD error in the context of RG_JJ_NN1, JJ_NN1_II
and VBZ_RG is more strongly associated with L1s; and
we are also interested in which of our proposed methods
for identifying an error’s feature context does this best.
For this purpose, then, we use just the F-statistic from
the ANOVA test, this time with the dependent variable
as the ratio of occurrences of error plus error context
per 10 sentences: a higher F-statistic shows a stronger
association with L1s.7
We also consider the x2-statistic from Pearson’s chi-
squared test, noting that it is also used in SLA hypothe-
sis testing and that it was additionally found by Swanson
and Charniak (2013) to be good at distinguishing inter-
esting features in their related task (see §5 for more
detail). The F-statistic and x2-statistic are closely re-
lated: a random variate of the F-distribution is the ratio
of two chi-squared variates scaled by their degrees of
freedom. A difference is that x2 compares observed
versus expected counts rather than proportions: to take
account of the differing text lengths, our observed fre-
quency is the number of sentences with error and error
context per L1; our expected frequency is the total num-
ber of sentences with that error and error context scaled
according to the proportion of sentences labelled with
that L1 relative to the corpus as a whole.
</bodyText>
<subsectionHeader confidence="0.998116">
3.3 Errors Chosen
</subsectionHeader>
<bodyText confidence="0.992176157894737">
From the 74 error types in the FCE corpus, we select a
subset to evaluate our models. In addition to the MD er-
ror used in the case study of Yannakoudakis et al. (2012),
we choose a subset which has a range of F-statistic val-
ues as described above: some show very similar patterns
across L1s (i.e. with low F-statistic), such as DN Wrong
Derived Noun (e.g. hot vs heat); others do vary signif-
icantly with L1, such as DJ Wrong Derived Adjective
(e.g. reasonally vs reasonable). Having errors with
a range of F-statistic values lets us evaluate whether
finding good error contexts works only for strongly L1-
associated errors, weakly L1-associated errors, or across
7As we are only using the F-statistic to evaluate ranks, we
do not need a multiple comparison adjustment such as the
Bonferroni correction: this would only apply for comparisons
to a significance threshold, and in any case the Bonferroni is
monotonic and does not affect rankings.
the spectrum. Our subset is in Table 3, along with their
F-statistic, ANOVA p-value and counts in FCESUB.
</bodyText>
<subsectionHeader confidence="0.985751">
3.4 Models
</subsectionHeader>
<bodyText confidence="0.998968382978723">
We propose four models for choosing error contexts.
These models rank error contexts; we evaluate the
ranked error contexts by F-statistic and x2-statistic val-
ues (§3.2).
ERRORCOOCC In this model we rank features by
error-feature co-occurrence scores given by Equation
(2). The L1 is not taken into account, so this will just
return common features which may be equally strongly
associated with errors across all L1s. We look at results
for when k = 1..3 features are chosen. For k = 2, 3,
we add the individual error-feature scores together for
the ranking.8 It may be the case that interesting results
could be obtained for k &gt; 3, but we only look at the
k = 1..3 in this preliminary work to see if there are
any discernible trends suggesting that larger values of k
could help.
L1ASSOC Here we use features that are strongly as-
sociated with the L1s from the TOEFL11 corpus and
NLI shared task. Specifically, we rank features by their
Information Gain with respect to L1s as in the process of
feature selection from the shared task.9 The relationship
between errors and features (in the form of error-feature
co-occurrence scores) is not taken into account here.
Again, we look at results for when k = 1..3 features
are chosen, and for k = 2, 3, we add the individual
error-feature scores together for the ranking.
MAXWEIGHTCLIQUE Both of the preceding mod-
els look only at one factor that might be relevant: error-
feature scores (finding features that are related to the
errors) and a measure of the association of features with
L1s; but there is no link between them, and interaction
of features is not taken into account. In Yannakoudakis
et al. (2012), the visualiser provides to the SLA re-
searcher a graph showing the relatedness of features,
based on Equation (1), and the SLA researcher com-
bines this with error-feature scores to find interesting
candidate error contexts; we create a similar graph and
aim to imitate the process by incorporating error-feature
scores as follows.
We define a weighted undirected graph G = (V, A)
such that V is the set of features used in the above
models (i.e. PoS n-grams from ERRORCOOCC); A is
defined such that (vi, vj) E A for vertices vi, vj E V
if 0.8 &lt; scoreff(vi, vj) &lt; 1.0 where scoreff() is as
defined as in Equation (1).10 Given our set of errors
E defined at Equation (2) above, the weight of a ver-
tex vi is defined as scoreef(vi,ej) for some ej E E.
</bodyText>
<footnote confidence="0.891787">
8For k = 2 the combinations were made from the top 100
features from k = 1, and for k = 3 from the top 50.
9We recalculated this over the subset of eight languages
used in this paper.
10We choose this threshold value as it is the one used in the
graph definition of Yannakoudakis et al. (2012).
</footnote>
<page confidence="0.990876">
59
</page>
<table confidence="0.9979296">
model r
ERRORCOOCC 0.95
L1ASSOC 0.97
MAXWEIGHTCLIQUE 0.95
MAXWEIGHTCLIQUE-L1 0.92
</table>
<tableCaption confidence="0.99439">
Table 4: Average correlation coefficient r between F-
</tableCaption>
<bodyText confidence="0.972498066666666">
statistic and X2-statistic for each model
Given this graph, it is possible to characterise the find-
ing of related features with strong aggregate associations
with errors as an instance of the MAXIMUM WEIGHT
CLIQUE PROBLEM (Bomze et al., 1999). As the name
suggests, this finds a clique of maximum weight, here
the strongest aggregate feature–error association. While
this is an NP-hard problem, there are quite efficient algo-
rithms for solving it; we use one proposed by ¨Osterg˚ard
(1999).11
MAXWEIGHTCLIQUE-L1 We also look at a vari-
ant of MAXWEIGHTCLIQUE where we construct the
graphs based only on relationships among features for
a particular L1. That is, there will be eight weighted
graphs per error of interest.
</bodyText>
<sectionHeader confidence="0.999697" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.986385">
4.1 Overall Results
</subsectionHeader>
<bodyText confidence="0.94134584375">
We only present the F-statistic results here; the X2-
statistic showed very similar patterns. The average
correlation between the two for each model shows the
strong similarity (Table 4).
For the F-statistic results, presented in Table 5, we
report the highest F-statistic in the N-best list (N =
1, 5, 20, 50) for each model. For models ERRORCOOCC
and L1ASSOC we report the highest F-statistic for each
value of k (k = 1, 2, 3). The number of occurrences
of the error context with the highest F-statistic is given
in parentheses after the F-statistic; the highest value
for each N is in bold. For MAXWEIGHTCLIQUE-L1,
we also note the language of the graph from which the
highest score was derived.
We note by comparing Table 5 with Table 3 that for
each error type except for MD, it is possible to find
an error context that is more strongly associated with
L1s than is the raw error type alone. For MD this is
not surprising, as its frequency of occurrence is very
strongly linked to the L1, as noted in Table 2 and §3.2.12
(For the error type MT also, no model produces an error
context more strongly associated with the L1 for the
single best choice where N = 1, but does for larger
values of N.)
11Code for the used wclique is available at http://tcs.
legacy.ics.tkk.fi/˜pat/wclique.html.
12The fact that determiner errors are very widely studied in
terms of analysing cross-linguistic influence suggests a broad
consensus that they vary strongly with L1. In addition to Di´ez-
Bedmar and Papp (2008), a sample of other studies includes
Parrish (1987), Young (1996) and Ionin and Montrul (2010).
With respect to the individual models, the simple ER-
RORCOOCC scores highly, giving the best result about
half the time, and the best results can occur for any
of k = 1, 2, 3. The number of instances returned for
each error plus error context is larger than for the other
models as well, which is not surprising as the model
aims to find contexts strongly associated with the errors
rather than with L1s. However, these are then likely to
be features that are fairly common across L1s; we look
at some examples in §4.2.
L1ASSOC performs fairly poorly on our evaluation
measure, although in many cases it does find an error
context more strongly associated with the L1 than just
the raw error type. Counts are also lower. Also, for this
model, k = 2,3 are always worse than k = 1: bringing
in a second context feature reduces the number of oc-
currences to such an extent that the F-statistic can drop
dramatically. This is probably in part an artefact of the
size of the FCE corpus (and particularly our FCESUB
subcorpus): these features derived from the TOEFL11
corpus just do not occur sufficiently often in our evalua-
tion corpus (and in fact there are often large numbers of
zero occurrences for k = 2, 3).
MAXWEIGHTCLIQUE also performs fairly poorly.
However, in many cases it also finds an error context
more strongly associated with L1 than the raw error type
alone (DN, MT, UD, UT, UV), even if not always for
N = 1, and it has intermediate counts of occurrences.
MAXWEIGHTCLIQUE-L1 gives the best results in
the other half of the cases where ERRORCOOCC does
not. The error contexts that it finds, however, are very
specific, often to a single language (as might be expected
by its definition) with very small numbers of counts.
</bodyText>
<subsectionHeader confidence="0.99521">
4.2 Some Examples
</subsectionHeader>
<bodyText confidence="0.999849086956522">
We look at some examples in Figure 2, to illustrate both
interesting error contexts found and areas where the
models do a poor job. In these sample sentences, only
errors of interest are retained and highlighted.
The DJ error with context { JJ, NN1 } illustrates the
top result found under the ERRORCOOCC model for
N = 20. In the first sentence the model seems to find a
useful pattern: the adjective that is at the centre of the
error occurs in the context of a singular noun. On the
other hand, the second sentence illustrates a problem:
because the range of the context is the whole sentence,
frequent features such as NN1 will occur a lot in other
parts of the sentence that have no apparent relation to
the actual error. The ERRORCOOCC model is thus likely
to be picking up false positives by virtue of the relatively
high frequencies of its error contexts.
The UV error with context { TO_VV0_II, NNL1,
II, NN2, VV0_II } illustrates the top result found
under the MAXWEIGHTCLIQUE-L1 model for N =
5. This is very specific, and its three instances only
appear in Turkish. But all three are similar errors from
different documents, so it appears likely to be a genuine
pattern, although the NN2 seems only to have a tenuous
</bodyText>
<page confidence="0.997652">
60
</page>
<tableCaption confidence="0.932354">
Table 5: Results for the chosen error types under the four proposed models. All error types and models report the best F-statistic for the selected error context and frequency within
</tableCaption>
<figure confidence="0.916521021276596">
the top N (N = 1, 5, 20, 50). ERRORCOOCC and L1ASSOC give the best score for the set of k features (k = 1, 2, 3). MAXWEIGHTCLIQUE-L1 also notes the language graph
with the best result.
MAXWEIGHTCLIQUE-L1
5.83(198) [KOR]
5.83(198) [KOR]
6.47(110) [KOR]
4.05(91) [KOR]
1.54(20) [GER]
4.27(10) [SPA]
2.48(20) [CHI]
3.08(2) [GER]
4.61(3) [GER]
3.06(2) [GER]
4.10(3) [TUR]
4.10(3) [TUR]
4.10(3) [TUR]
4.09(3) [TUR]
4.09(3) [TUR]
3.24(2) [CHI]
3.24(2) [CHI]
3.24(2) [CHI]
3.24(2) [CHI]
4.47(3) [CHI]
4.47(3) [CHI]
3.54(9) [CHI]
4.63(3) [CHI]
2.53(2) [JAP]
3.50(5) [ITA]
3.84(3) [ITA]
3.93(3) [ITA]
4.06(3) [ITA]
MAXWEIGHTCLIQUE
3.07(297)
5.83(198)
5.83(198)
5.83(198)
0.99(15)
1.74(41)
2.34(24)
2.48(18)
1.26(63)
1.26(63)
1.76(30)
3.41(18)
1.70(61)
2.14(64)
2.79(44)
</figure>
<equation confidence="0.961181942857143">
4.54(74)
0.64(47)
1.45(62)
1.90(29)
2.85(66)
0.81(35)
1.01(51)
2.58(45)
2.58(45)
1.49(28)
1.49(28)
2.38(15)
2.38(15)
8.36(831) / 5.83(198) / 5.83(198)
8.20(268) / 5.83(198) / 2.60(36)
2.58(107) / 1.59(31) / 1.59(31)
5.83(198) / 5.83(198) / 0.54(2)
8.20(268) / 5.83(198) / 1.93(3)
2.54(101) / 1.85(79) / 1.55(13)
4.44(295) / 3.11(25) / 3.11(25)
4.44(295) / 3.86(33) / 3.11(25)
1.85(79) / 1.85(79) / 1.55(13)
1.45(62) / 1.45(62) / 0.73(10)
1.59(31) / 1.59(31) / 0.81(6)
2.19(12) / 1.59(31) / 0.81(6)
2.53(70) / 1.59(31) / 1.36(1)
1.09(40) / 1.09(40) / 0.70(7)
1.59(26) / 1.45(62) / 1.36(1)
3.41(51) / 1.45(62) / 1.36(1)
1.01(51) / 1.01(51) / 0.43(1)
1.06(15) / 1.06(15) / 1.29(2)
2.04(26) / 1.36(1) / 1.36(1)
3.41(51) / 1.54(4) / 1.54(4)
2.28(23) / 1.36(1) / 1.36(1)
2.91(51) / 1.53(6) / 1.36(1)
2.91(51) / 1.53(6) / 1.36(1)
1.25(5) / 1.36(1) / 1.36(1)
3.89(4) / 2.75(2) / 2.75(2)
2.29(8) / 1.29(2) / 1.29(2)
3.22(8) / 1.52(1) / 1.52(1)
3.22(8) / 1.52(1) / 1.52(1)
L1ASSOC
14.28(1310) / 12.18(769) / 6.75(582)
14.28(1319) / 9.09(985) / 6.38(753)
14.41(850) / 12.18(769) / 6.82(593)
14.41(850) / 12.18(769) / 7.99(483)
2.78(274) / 3.19(227) / 2.95(158)
3.60(268) / 3.19(227) / 3.02(148)
0.77(268) / 1.63(185) / 1.73(119)
1.80(191) / 2.29(153) / 2.54(142)
3.34(794) / 3.00(666) / 3.02(485)
3.34(794) / 3.46(478) / 3.37(378)
4.44(295) / 3.64(375) / 4.60(294)
4.50(277) / 5.21(247) / 4.72(215)
0.69(679) / 1.05(475) / 2.08(334)
1.70(405) / 1.17(452) / 2.08(334)
2.08(223) / 2.11(360) / 2.32(276)
3.27(112) / 3.01(188) / 2.33(198)
0.14(548) / 0.45(414) / 1.12(259)
0.82(368) / 1.16(321) / 1.58(249)
1.51(351) / 1.77(275) / 1.89(225)
2.25(112) / 2.66(201) / 3.18(178)
0.88(260) / 0.97(186) / 1.18(119)
2.22(175) / 2.21(162) / 1.68(109)
3.72(194) / 3.33(163) / 4.02(93)
3.72(194) / 3.39(114) / 4.02(93)
2.34(86) / 2.69(144) / 2.95(113)
2.86(61) / 3.16(120) / 2.95(113)
2.25(125) / 2.82(127) / 3.13(96)
2.56(61) / 3.01(101) / 3.13(96)
</equation>
<figure confidence="0.978302184210526">
ERRORCOOCC
20
50
20
50
20
50
20
50
20
50
20
50
20
50
N
5
5
5
5
5
5
5
1
1
1
1
1
1
1
error
MD
MT
DN
UV
UD
UT
DJ
</figure>
<page confidence="0.993089">
61
</page>
<table confidence="0.99777980952381">
error context example sentences
DJ JJ, NN1 Basically/RR ,/, I/PPIS1 helped/VVD them/PPHO2 liaise/VV0
with/IW the/AT local/JJ police/NN and/CC get/VV0 some/DD
&lt;ns type=&amp;quot;DJ&amp;quot;&gt;&lt;i&gt;electronical&lt;/i&gt;&lt;c&gt;electronic/JJ&lt;/c&gt;&lt;/ns&gt; equipmen-
t/NN1 that/CST they/PPHS2 needed/VVD.
The/AT show/NN1 will/VM be/VB0 at/II the/AT Central/JJ
Exhibition/NN1 Hall/NP1 and/CC it/PPH1 will/VM be/VB0
&lt;ns type=&amp;quot;DJ&amp;quot;&gt;&lt;i&gt;opened&lt;/i&gt;&lt;c&gt;open/JJ&lt;/c&gt;&lt;/ns&gt; until/ICS 7/MC.
UV TO_VV0_II, I/PPIS1 used/VMK to/TO &lt;ns type=&amp;quot;UV&amp;quot;&gt;&lt;i&gt;be&lt;/i&gt;&lt;/ns&gt; play/VV0 in/II the/AT
NNL1, II, NN2, school/NNL1 team/NN1 ... and/CC our/APP$ team/NN1 was/VBDZ one/MC1 of/IO the/AT
VV0_II best/JJT basketball/NN1 teams/NN2 ...
DN XX, XX_VV0, Never/RR the/AT less/DAR ,/, in/II summer/NNT1 we/PPIS2 can/VM n’t/XX resist/VV0
VM_XX_VV0, NN1 such/DA &lt;ns type=&amp;quot;DN&amp;quot;&gt;&lt;i&gt;hot&lt;/i&gt;&lt;c&gt;heat/NN1&lt;/c&gt;&lt;/ns&gt;!
... I/PPIS1 think/VV0 you/PPY should/VM have/VH0 a/AT1 &lt;ns type=&amp;quot;DN&amp;quot;&gt;&lt;i&gt;baby-
parking&lt;/i&gt;&lt;c&gt;kindergarten/NP1&lt;/c&gt;&lt;/ns&gt; ,/, in/II fact/NN1 a/AT1 certain/JJ num-
ber/NN1 of/IO women/NN2 could/VM n’t/XX see/VV0 the/AT Festival/NN1 because/CS
of/IO their/APP$ sons/NN2.
MD VBZ_RG, The/AT first/MD and/CC most/RR important/JJ thing/NN1 is/VBZ that/RG modern/JJ
RG_JJ_NN1 technology/NN1 has/VHZ made/VVN our/APP$ life/NN1 easier/JJR ,/, for/IF instance/NN1
&lt;ns type=&amp;quot;MD&amp;quot;&gt;&lt;c&gt;the/AT&lt;/c&gt;&lt;/ns&gt; rice/NN1 cooker/NN1 is/VBZ a/AT1 great/JJ
invention/NN1 ...
</table>
<figureCaption confidence="0.975094">
Figure 2: Examples for sample error types and specific error contexts. Error contexts are bolded.
</figureCaption>
<bodyText confidence="0.998864571428571">
connection.
The DN error with context { XX, XX_VV0,
VM_XX_VV0, NN1 } illustrates the top result found un-
der the MAXWEIGHTCLIQUE-L1 model for N = 50.
A number of this reasonably sized set are similar to the
first sentence, where the context appears interesting. In
this example, hot is used for heat; the other examples
of this type are from Spanish and Italian (similarly, e.g.,
live for life), where the error seems to be connected
to words where the English derivational morphology
is not simply affixation. However, there are some like
the second sentence, where (as for the DJ error) the
error context appears in a different clause, and likely
irrelevant.
The MD error in the last row we examine because (a
more complex version of) it was the focus of the case
study in Yannakoudakis et al. (2012), which from the
examples of that paper looked quite convincing as an
error context of relevance to SLA research. However, it
and the related examples of Yannakoudakis et al. (2012)
were not in the publicly available corpus,13 and in fact
there is only one example of this error and context in the
whole FCE corpus, illustrating the issue of data sparsity.
Further, this example also illustrates the issue of tagging
error: that is tagged as RG (degree adverb) where it
should be CST.
So as might be anticipated from the frequency num-
bers in Table 5, the MAXWEIGHTCLIQUE-L1 model
produces context that looks interesting from an SLA per-
spective, but is relatively limited in scope; the ERROR-
COOCC model produces a much larger set of candidates,
and can successfully find error context such that they
behave differently with respect to the L1s according
to the ANOVA F-statistic, but produces false positives.
Overall, a recurring issue illustrated for all models by
</bodyText>
<footnote confidence="0.9206795">
13We assume that the multiple examples come from the
larger CLC corpus.
</footnote>
<bodyText confidence="0.8932415">
the examples is the proposal of error context far away
from any likely relevance to SLA.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999835272727273">
While Native Language Identification (NLI) as a sub-
field of NLP has seen much new work in the last few
years — the papers from the shared task (Tetreault et
al., 2013) provide a recent sample — the emphasis on
optimising classification task results, for example by
using classifier ensembles (Malmasi et al., 2013), ver-
sus analysing features for relevance to other tasks has
varied. Below we discuss works which directly look
at how features might be related to language-learning
tasks or SLA research.
The seminal work of Koppel et al. (2005) that pre-
sented NLI as a classification task included, in addition
to standard lexical and PoS n-gram features, errors made
by the writers; these errors were automatically identi-
fied using Microsoft Word grammar checker. Kochmar
(2011) used the FCE corpus for NLI, including the man-
ually annotated errors as features, and presented an anal-
ysis of usefulness of features (including errors) with
respect to L1.
Wong and Dras (2011) used syntactic features on the
basis of SLA theory that posits that L1 constructions
may be reflected in some form of characteristic errors or
patterns in L2 constructions to some extent, or through
overuse or avoidance of particular constructions in L2
(Lado, 1957; Ellis, 2008); they did note distributional
differences of features related to L1. Wong et al. (2012)
induced topic models over function words and PoS n-
grams, where some of the topics appeared to reflect L1-
specific characteristics. These works, while interested
in the nature of the features, do not evaluate them except
via classification accuracy.
Swanson and Charniak (2012) similarly explore us-
ing syntax, where they propose a richer representation
</bodyText>
<page confidence="0.997426">
62
</page>
<bodyText confidence="0.999898272727273">
for L1-specific constructions through Tree Substitution
Grammar (TSG). Swanson and Charniak (2013) sub-
sequently examine both relevancy and redundancy of
features through a number of metrics (including the
χ2-statistic used in this paper). They then extend a
Bayesian induction model for TSG inference based on
a supervised mixture of hierarchical grammars, in order
to extract a filtered set of more linguistically informed
features that could benefit both NLI and SLA research;
an aim was to find relatively rare features that are nev-
ertheless useful for L1 prediction. Swanson and Char-
niak (2014) continue on from this with a data-driven
approach to inferring possible relationships between L1
and L2 structures, again using TSGs. Malmasi and Dras
(2014c) also propose a method for identifying potential
language transfer effects by using additional linguistic
features such as adaptor grammars and grammatical de-
pendencies to analyse differences in learner language.
This body of work thus shares some similarities with the
present paper, but our focus is on errors rather than on
the distributional differences, and we look at error con-
texts that may not constitute a TSG tree or grammatical
dependency.
Coming from a linguistic perspective, the works in
Jarvis and Crossley (2012) use Linear Discriminant
Analysis for classification of texts by L1, and identify
interesting features by a stepwise feature selection pro-
cess in the course of classification, rather than via the
measurement of their variability across L1s as here.
More recently, several of these NLI techniques have
been adapted and applied to languages other than En-
glish, such as Arabic and Chinese (Malmasi and Dras,
2014a; Malmasi and Dras, 2014b).
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999988880952381">
In this paper, prompted by work on using computa-
tional visualisation techniques to help SLA researchers
form hypotheses about errors and the environments in
which they are made, we have defined a new task for
finding interesting contexts for errors that vary with
the native language of the speaker. We proposed four
models, ranging from one based on simple error-feature
co-occurrence statistics to one based on the maximum
weighted clique on an L1-specific feature association
graph; these all managed to find contexts that were more
strongly associated with L1s than the raw errors alone,
and produced (albeit with many false positives in the
case of the simple model) some error contexts that look
potentially useful for SLA.
This paper is largely intended to prompt more work
on applying NLP techniques to SLA more broadly. As
such, there are many ways in which the work could be
further developed. First, to get rid of obviously incor-
rect cases, the size of the area over which the feature-
feature and feature-error scores are calculated could be
restricted, perhaps to the relevant clause or a certain
window size. Second, it may not be the case that the
ANOVA F-statistic or χ2 are the best evaluation mea-
sure: in medical work, for example, there is the notion
of clinical significance, which takes effect size into ac-
count and is often more relevant to the practitioner than
statistical significance. Similarly, the current features
may not be the most meaningful. As part of this, an im-
portant step would be to bring in SLA researchers, to as-
sess proposed error contexts and look at what evaluation
measures best relate to this. The role of the present work
would then be to rule out models for producing error
contexts (like L1ASSOC) that produce weaker results in
hypothesis testing: it would thus be complementary to
the visualisation work from which it stems, guiding SLA
researchers away from unproductive areas of the space
of possible hypotheses. And third, the size of the corpus
is (as always) an issue: as these error-annotated corpora
are few and far between, a semi-supervised approach
or one that in some way incorporated unannotated data
would be useful, perhaps using some of the extensive
recent work on error annotation.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9975016">
Immanuel M. Bomze, Marco Budinich, Panos Parda-
los, and Marcello Pelillo. 1999. The Maximum
Clique Problem. In D.-Z. Du and P. M. Pardalos, edi-
tors, Handbook of Combinatorial Optimization (supp.
Vol. A), pages 1–74. Kluwer Academic, Dordrecht,
Netherlands.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proc. of
the COLING/ACL Interactive Presentation Sessions,
pages 77–80, Stroudsburg, PA, USA.
Ted Briscoe, Ben Medlock, and Øistein Andersen. 2010.
Automated Assessment of ESOL Free Text Exami-
nations. Technical Report TR-790, University of
Cambridge, Computer Laboratory.
Mar´ıa Bel´en Di´ez-Bedmar and Szilvia Papp. 2008. The
use of the English article system by Chinese and Span-
ish learners. Language and Computers, 66(1):147–
176.
Rod Ellis. 2008. The Study of Second Language Acqui-
sition, 2nd edition. Oxford University Press, Oxford,
UK.
Sylviane Granger. 2011. How to Use Foreign and Sec-
ond Language Learner Corpora. In Alison Mackey
and Susan M. Gass, editors, Research Methods in
Second Language Acquisition: A Practical Guide.
Wiley-Blackwell.
Tania Ionin and Silvina Montrul. 2010. The role of L1
transfer in the interpretation of articles with definite
plurals. Language Learning, 60(4):877–925.
Sherri L. Jackson. 2009. Statistics: Plain and Simple.
Wadsworth, Cengage Learning, Belmont, CA, US.
Scott Jarvis and Scott Crossley, editors. 2012. Ap-
proaching Language Transfer Through Text Classi-
fication: Explorations in the Detection-based Ap-
proach. Multilingual Matters, Bristol, UK.
</reference>
<page confidence="0.988436">
63
</page>
<reference confidence="0.993469482758621">
Ekaterina Kochmar. 2011. Identification of a writer’s
native language by error analysis. MPhil thesis, Uni-
versity of Cambridge.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Automatically determining an anonymous author’s
native language. In Intelligence and Security In-
formatics, volume 3495 of LNCS, pages 209–217.
Springer-Verlag.
Robert Lado. 1957. Linguistics Across Cultures: Ap-
plied Linguistics for Language Teachers. Univ. of
Michigan Press, Ann Arbor, MI, US.
James P. Lantolf. 2001. Sociocultural Theory and
Second Language Learning. Oxford University Press,
Oxford, UK.
Shervin Malmasi and Mark Dras. 2014a. Arabic Na-
tive Language Identification. In Proceedings of the
Arabic Natural Language Processing Workshop (co-
located with EMNLP 2014), Doha, Qatar, October.
Association for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014b. Chinese Na-
tive Language Identification. Proceedings of the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Shervin Malmasi and Mark Dras. 2014c. Language
Transfer Hypotheses with Linear SVM Weights. Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras.
2013. NLI Shared Task 2013: MQ Submission. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 124–133, Atlanta, Georgia, June. Association
for Computational Linguistics.
Lourdes Ortega. 2009. Understanding Second Lan-
guage Acquisition. Hodder Education, Oxford, UK.
Patric ¨Osterg˚ard. 1999. A New Algorithm for the
Maximum-Weight Clique Problem. Electronic Notes
in Discrete Mathematics, 3:153–156, May.
Betsy Parrish. 1987. A New Look at Methodologies in
the Study of Article Acquisition for Learners of ESL.
Language Learning, 37(3):361–384.
Manfred Pienemann. 2005. Cross-linguistic Aspects of
Processability Theory. John Benjamins, Amsterdam,
Netherlands.
Benjamin Swanson and Eugene Charniak. 2012. Native
Language Detection with Tree Substitution Gram-
mars. In Proc. Meeting Assoc. Computat. Linguistics
(ACL), pages 193–197.
Ben Swanson and Eugene Charniak. 2013. Extracting
the native language signal for second language ac-
quisition. In Proc. Conf. North American Assoc. for
Computat. Linguistics: Human Language Technolo-
gies (NAACL-HLT), pages 85–94, Atlanta, Georgia,
June.
Ben Swanson and Eugene Charniak. 2014. Data Driven
Language Transfer Hypotheses. In Proc. Conf. Euro-
pean Assoc. for Computat. Linguistics (EACL), pages
169–173, Gothenburg, Sweden, April.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. In Proceedings of the Eighth
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications (BEA), pages 48–57, Atlanta,
Georgia, June.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. Conf. Empirical Methods in Natural Language
Processing (EMNLP), pages 1600–1610.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proc. Conf. Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
699–709.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A New Dataset and Method for Automatically
Grading ESOL Texts. In Proc. Meeting Assoc. Com-
putat. Linguistics (ACL), pages 180–189.
Helen Yannakoudakis, Ted Briscoe, and Theodora Alex-
opoulou. 2012. Automating Second Language Ac-
quisition Research: Integrating Information Visualisa-
tion and Machine Learning. In Proc. EACL Workshop
of LINGVIS &amp; UNCLH, pages 35–43.
Richard Young. 1996. Form-Function Relations in Arti-
cles in English Interlanguage. In R. Bayley and D. R.
Preston, editors, Second Language Acquisition and
Linguistic Variation, pages 135–175. John Benjamins,
Amsterdam, The Netherlands.
</reference>
<page confidence="0.999418">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.267251">
<title confidence="0.9273682">From Visualisation to Hypothesis for Second Language Acquisition Shervin Centre for Language Macquarie</title>
<author confidence="0.976252">NSW Sydney</author>
<email confidence="0.993755">shervin.malmasi@mq.edu.au</email>
<author confidence="0.975467">Mark</author>
<affiliation confidence="0.881081">Centre for Language</affiliation>
<title confidence="0.45648">Macquarie</title>
<author confidence="0.890919">NSW Sydney</author>
<email confidence="0.988225">mark.dras@mq.edu.au</email>
<abstract confidence="0.999453777777778">One research goal in Second Language Acquisition (SLA) is to formulate and test hypotheses about errors and the environments in which they are made, a process which often involves substantial effort; large amounts of data and computational visualisation techniques promise help here. In this paper we have defined a new task for finding contexts for errors that vary with the native language of the speaker that are potentially useful for SLA research. We propose four models for approaching this task, and find that one based only on error-feature cooccurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts, with an apparent trade-off between false positives and very specific contexts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Immanuel M Bomze</author>
<author>Marco Budinich</author>
<author>Panos Pardalos</author>
<author>Marcello Pelillo</author>
</authors>
<title>The Maximum Clique Problem.</title>
<date>1999</date>
<booktitle>Handbook of Combinatorial Optimization (supp. Vol. A),</booktitle>
<pages>1--74</pages>
<editor>In D.-Z. Du and P. M. Pardalos, editors,</editor>
<publisher>Kluwer Academic,</publisher>
<location>Dordrecht, Netherlands.</location>
<contexts>
<context position="20124" citStr="Bomze et al., 1999" startWordPosition="3328" endWordPosition="3331">features from k = 1, and for k = 3 from the top 50. 9We recalculated this over the subset of eight languages used in this paper. 10We choose this threshold value as it is the one used in the graph definition of Yannakoudakis et al. (2012). 59 model r ERRORCOOCC 0.95 L1ASSOC 0.97 MAXWEIGHTCLIQUE 0.95 MAXWEIGHTCLIQUE-L1 0.92 Table 4: Average correlation coefficient r between Fstatistic and X2-statistic for each model Given this graph, it is possible to characterise the finding of related features with strong aggregate associations with errors as an instance of the MAXIMUM WEIGHT CLIQUE PROBLEM (Bomze et al., 1999). As the name suggests, this finds a clique of maximum weight, here the strongest aggregate feature–error association. While this is an NP-hard problem, there are quite efficient algorithms for solving it; we use one proposed by ¨Osterg˚ard (1999).11 MAXWEIGHTCLIQUE-L1 We also look at a variant of MAXWEIGHTCLIQUE where we construct the graphs based only on relationships among features for a particular L1. That is, there will be eight weighted graphs per error of interest. 4 Results and Discussion 4.1 Overall Results We only present the F-statistic results here; the X2- statistic showed very si</context>
</contexts>
<marker>Bomze, Budinich, Pardalos, Pelillo, 1999</marker>
<rawString>Immanuel M. Bomze, Marco Budinich, Panos Pardalos, and Marcello Pelillo. 1999. The Maximum Clique Problem. In D.-Z. Du and P. M. Pardalos, editors, Handbook of Combinatorial Optimization (supp. Vol. A), pages 1–74. Kluwer Academic, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING/ACL Interactive Presentation Sessions,</booktitle>
<pages>77--80</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11692" citStr="Briscoe et al., 2006" startWordPosition="1893" endWordPosition="1896">ams (n = 1, 2,3) as features in this work. Note that we use the terminology of Yannakoudakis et al. (2012) here: what had their origin as features in the essay classification task are still referred to as features in the visualisation tool, although the task carried out there is not a classification one. Similarly, we refer to our PoS n-grams as features, although we are not classifying errors using these features and so are not carrying out feature selection for the typical purpose of optimising classification performance. For this, as did Yannakoudakis et al. (2012), we use the RASP parser (Briscoe et al., 2006) for tagging; the tags are consequently from the CLAWS2 tagset,4 which are more fine-grained in terms of linguistic analysis than the more frequently used Penn Treebank tags. For our task, we then used the subset of the FCE corpus where the languages overlapped with the TOEFL11 corpus: we refer to this as FCESUB. This gives 799 scripts over 8 languages, distributed as in Table 1; a positive byproduct is that the L1s are more similar in size than the full FCE corpus. 4http://ucrel.lancs.ac.uk/claws2tags. html language mean CHI 0.885790 FRE 0.460894 GER 0.366587 ITA 0.581401 JAP 1.058159 KOR 1.0</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proc. of the COLING/ACL Interactive Presentation Sessions, pages 77–80, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
<author>Øistein Andersen</author>
</authors>
<title>Automated Assessment of ESOL Free Text Examinations.</title>
<date>2010</date>
<tech>Technical Report TR-790,</tech>
<institution>University of Cambridge, Computer Laboratory.</institution>
<contexts>
<context position="5777" citStr="Briscoe et al. (2010)" startWordPosition="918" endWordPosition="921">of some of the models to see how the task and the models might contribute to SLA research. Because we draw heavily on the work of Yannakoudakis et al. (2012), we first review relevant aspects of that work in §2; we then present our task definition and experimental setup in §3; we give results along with a discussion in §4; we follow with some more detail on related work in §5; and we conclude in §6. 2 Developing Hypotheses: A Visualisation Tool The context of the Yannakoudakis et al. (2012) work is automated grading of English as a Second or Other Language (ESOL) exam scripts, as described in Briscoe et al. (2010). The automated grading takes a classification approach, using a binary discriminative learner, with useful features including lexical and part-of-speech (PoS) n-grams. The publicly available dataset on which the work was carried out consists of texts from the First Certificate in English (FCE) exam, aimed at upper-intermediate students of English across various L1s, and was presented in Yannakoudakis et al. (2011). This FCE corpus2 consists of a subset of 1244 texts of the Cambridge Learner Corpus,3 and is manually annotated with errors and their corrections, as well as a classification accor</context>
</contexts>
<marker>Briscoe, Medlock, Andersen, 2010</marker>
<rawString>Ted Briscoe, Ben Medlock, and Øistein Andersen. 2010. Automated Assessment of ESOL Free Text Examinations. Technical Report TR-790, University of Cambridge, Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mar´ıa Bel´en Di´ez-Bedmar</author>
<author>Szilvia Papp</author>
</authors>
<title>The use of the English article system by Chinese and Spanish learners.</title>
<date>2008</date>
<journal>Language and Computers,</journal>
<volume>66</volume>
<issue>1</issue>
<pages>176</pages>
<marker>Di´ez-Bedmar, Papp, 2008</marker>
<rawString>Mar´ıa Bel´en Di´ez-Bedmar and Szilvia Papp. 2008. The use of the English article system by Chinese and Spanish learners. Language and Computers, 66(1):147– 176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rod Ellis</author>
</authors>
<title>The Study of Second Language Acquisition, 2nd edition.</title>
<date>2008</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="2576" citStr="Ellis (2008)" startWordPosition="392" endWordPosition="393">ysis (the so-called Bickerton semantic wheel), use the simple Wordsmith tools designed to extract data for lexicographers to identify errors in a semi-automatic way, and evaluate whether Chinese and Spanish L1 speakers do behave differently via hypothesis testing (ANOVA, chisquare and z-tests, in their case). They conclude that Chinese and Spanish do have characteristic differences, with patterns of zero article and definite article use differing according to semantic context. Such studies are typically carried out on relatively small datasets, and use fairly elementary tools. Sources such as Ellis (2008) and Ortega (2009) give good overviews of such studies and of SLA research in general. A goal of this paper is to investigate a particular way in which Natural Language Processing (NLP) can usefully contribute to SLA. In terms of existing work, the subfield of Native Language Identification (NLI) has been quite active recently, which looks at predicting the L1 of writers writing in a common L2 within a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven app</context>
<context position="32867" citStr="Ellis, 2008" startWordPosition="5392" endWordPosition="5393">gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1- specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of fe</context>
</contexts>
<marker>Ellis, 2008</marker>
<rawString>Rod Ellis. 2008. The Study of Second Language Acquisition, 2nd edition. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>How to Use Foreign and Second Language Learner Corpora.</title>
<date>2011</date>
<booktitle>Research Methods in Second Language Acquisition: A Practical Guide.</booktitle>
<editor>In Alison Mackey and Susan M. Gass, editors,</editor>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="1887" citStr="Granger, 2011" startWordPosition="288" endWordPosition="289">ces for learners Pienemann (2005), sociocultural factors (Lantolf, 2001), and so on. One longstanding question, dating back to at least Lado (1957), is expressed by Ortega (2009) in the following way: “What is the role played by first language in L2 development, vis-`a-vis the role of other universal development forces?” An example of SLA research that looks at this question is the study of Di´ez-Bedmar and Papp (2008), comparing Chinese and Spanish learners of English with respect to the English article system (a, an, the) using corpora of essays by native and non-native speakers of English (Granger, 2011). Drawing on the 175 nonnative texts, they take a particular theoretical analysis (the so-called Bickerton semantic wheel), use the simple Wordsmith tools designed to extract data for lexicographers to identify errors in a semi-automatic way, and evaluate whether Chinese and Spanish L1 speakers do behave differently via hypothesis testing (ANOVA, chisquare and z-tests, in their case). They conclude that Chinese and Spanish do have characteristic differences, with patterns of zero article and definite article use differing according to semantic context. Such studies are typically carried out on</context>
<context position="3221" citStr="Granger (2011)" startWordPosition="499" endWordPosition="500">erviews of such studies and of SLA research in general. A goal of this paper is to investigate a particular way in which Natural Language Processing (NLP) can usefully contribute to SLA. In terms of existing work, the subfield of Native Language Identification (NLI) has been quite active recently, which looks at predicting the L1 of writers writing in a common L2 within a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated </context>
</contexts>
<marker>Granger, 2011</marker>
<rawString>Sylviane Granger. 2011. How to Use Foreign and Second Language Learner Corpora. In Alison Mackey and Susan M. Gass, editors, Research Methods in Second Language Acquisition: A Practical Guide. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tania Ionin</author>
<author>Silvina Montrul</author>
</authors>
<title>The role of L1 transfer in the interpretation of articles with definite plurals.</title>
<date>2010</date>
<journal>Language Learning,</journal>
<volume>60</volume>
<issue>4</issue>
<contexts>
<context position="22243" citStr="Ionin and Montrul (2010)" startWordPosition="3687" endWordPosition="3690">ly linked to the L1, as noted in Table 2 and §3.2.12 (For the error type MT also, no model produces an error context more strongly associated with the L1 for the single best choice where N = 1, but does for larger values of N.) 11Code for the used wclique is available at http://tcs. legacy.ics.tkk.fi/˜pat/wclique.html. 12The fact that determiner errors are very widely studied in terms of analysing cross-linguistic influence suggests a broad consensus that they vary strongly with L1. In addition to Di´ezBedmar and Papp (2008), a sample of other studies includes Parrish (1987), Young (1996) and Ionin and Montrul (2010). With respect to the individual models, the simple ERRORCOOCC scores highly, giving the best result about half the time, and the best results can occur for any of k = 1, 2, 3. The number of instances returned for each error plus error context is larger than for the other models as well, which is not surprising as the model aims to find contexts strongly associated with the errors rather than with L1s. However, these are then likely to be features that are fairly common across L1s; we look at some examples in §4.2. L1ASSOC performs fairly poorly on our evaluation measure, although in many case</context>
</contexts>
<marker>Ionin, Montrul, 2010</marker>
<rawString>Tania Ionin and Silvina Montrul. 2010. The role of L1 transfer in the interpretation of articles with definite plurals. Language Learning, 60(4):877–925.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sherri L Jackson</author>
</authors>
<title>Statistics: Plain and Simple.</title>
<date>2009</date>
<location>Wadsworth, Cengage Learning, Belmont, CA, US.</location>
<contexts>
<context position="14074" citStr="Jackson (2009)" startWordPosition="2288" endWordPosition="2289">with the null hypothesis. The data in Table 2 shows that the MD error does vary significantly with L1; a post-hoc Tukey HSD test lets us identify which specific languages exhibit this difference and shows that, for example (and as can be observed in the means), German L1 speakers are significantly different from Korean L1 speakers in the occurrence of MD errors. For our task we are not interested in significance per se. Rather, we are interested in whether we can find occurrences of errors plus contexts that are more strongly associated with, or that vary across, L1s, e.g. that an 5See, e.g., Jackson (2009). 6We note that the texts differ significantly in length by L1, so it would not be suitable to normalise as occurrences per document. 58 type name F-stat p-val N DJ Wrong Derived 3.27 .002 332 Adjective DN Wrong Derived 0.70 .671 294 Noun MD Missing Determiner 18.03 .000 1702 MT Missing Preposition 2.81 .007 985 UD Unnecessary Determiner 1.20 .301 807 UT Unnecessary Preposition 0.26 .968 689 UV Unnecessary Verb 0.78 .606 317 Table 3: Error types chosen for evaluation, including Fstatistic, ANOVA p-value and corpus count of sentences containing error. MD error in the context of RG_JJ_NN1, JJ_NN</context>
</contexts>
<marker>Jackson, 2009</marker>
<rawString>Sherri L. Jackson. 2009. Statistics: Plain and Simple. Wadsworth, Cengage Learning, Belmont, CA, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Jarvis</author>
<author>Scott Crossley</author>
<author>editors</author>
</authors>
<title>Approaching Language Transfer Through Text Classification: Explorations in the Detection-based Approach. Multilingual Matters,</title>
<date>2012</date>
<location>Bristol, UK.</location>
<marker>Jarvis, Crossley, editors, 2012</marker>
<rawString>Scott Jarvis and Scott Crossley, editors. 2012. Approaching Language Transfer Through Text Classification: Explorations in the Detection-based Approach. Multilingual Matters, Bristol, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Kochmar</author>
</authors>
<title>Identification of a writer’s native language by error analysis. MPhil thesis,</title>
<date>2011</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="32393" citStr="Kochmar (2011)" startWordPosition="5314" endWordPosition="5315">13) provide a recent sample — the emphasis on optimising classification task results, for example by using classifier ensembles (Malmasi et al., 2013), versus analysing features for relevance to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function wo</context>
</contexts>
<marker>Kochmar, 2011</marker>
<rawString>Ekaterina Kochmar. 2011. Identification of a writer’s native language by error analysis. MPhil thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Automatically determining an anonymous author’s native language.</title>
<date>2005</date>
<booktitle>In Intelligence and Security Informatics,</booktitle>
<volume>3495</volume>
<pages>209--217</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="32158" citStr="Koppel et al. (2005)" startWordPosition="5276" endWordPosition="5279">l of error context far away from any likely relevance to SLA. 5 Related Work While Native Language Identification (NLI) as a subfield of NLP has seen much new work in the last few years — the papers from the shared task (Tetreault et al., 2013) provide a recent sample — the emphasis on optimising classification task results, for example by using classifier ensembles (Malmasi et al., 2013), versus analysing features for relevance to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 construction</context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Automatically determining an anonymous author’s native language. In Intelligence and Security Informatics, volume 3495 of LNCS, pages 209–217. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Lado</author>
</authors>
<title>Linguistics Across Cultures: Applied Linguistics for Language Teachers.</title>
<date>1957</date>
<publisher>Univ. of Michigan Press,</publisher>
<location>Ann Arbor, MI, US.</location>
<contexts>
<context position="1420" citStr="Lado (1957)" startWordPosition="209" endWordPosition="210">k, and find that one based only on error-feature cooccurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts, with an apparent trade-off between false positives and very specific contexts. 1 Introduction SLA researchers are interested in a wide variety of aspects of humans learning a new language (L2) different from their native one (L1): cognitive issues and developmental sequences for learners Pienemann (2005), sociocultural factors (Lantolf, 2001), and so on. One longstanding question, dating back to at least Lado (1957), is expressed by Ortega (2009) in the following way: “What is the role played by first language in L2 development, vis-`a-vis the role of other universal development forces?” An example of SLA research that looks at this question is the study of Di´ez-Bedmar and Papp (2008), comparing Chinese and Spanish learners of English with respect to the English article system (a, an, the) using corpora of essays by native and non-native speakers of English (Granger, 2011). Drawing on the 175 nonnative texts, they take a particular theoretical analysis (the so-called Bickerton semantic wheel), use the s</context>
<context position="32853" citStr="Lado, 1957" startWordPosition="5390" endWordPosition="5391">l and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1- specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and re</context>
</contexts>
<marker>Lado, 1957</marker>
<rawString>Robert Lado. 1957. Linguistics Across Cultures: Applied Linguistics for Language Teachers. Univ. of Michigan Press, Ann Arbor, MI, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James P Lantolf</author>
</authors>
<title>Sociocultural Theory and Second Language Learning.</title>
<date>2001</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="1345" citStr="Lantolf, 2001" startWordPosition="195" endWordPosition="196">ially useful for SLA research. We propose four models for approaching this task, and find that one based only on error-feature cooccurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts, with an apparent trade-off between false positives and very specific contexts. 1 Introduction SLA researchers are interested in a wide variety of aspects of humans learning a new language (L2) different from their native one (L1): cognitive issues and developmental sequences for learners Pienemann (2005), sociocultural factors (Lantolf, 2001), and so on. One longstanding question, dating back to at least Lado (1957), is expressed by Ortega (2009) in the following way: “What is the role played by first language in L2 development, vis-`a-vis the role of other universal development forces?” An example of SLA research that looks at this question is the study of Di´ez-Bedmar and Papp (2008), comparing Chinese and Spanish learners of English with respect to the English article system (a, an, the) using corpora of essays by native and non-native speakers of English (Granger, 2011). Drawing on the 175 nonnative texts, they take a particul</context>
</contexts>
<marker>Lantolf, 2001</marker>
<rawString>James P. Lantolf. 2001. Sociocultural Theory and Second Language Learning. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Mark Dras</author>
</authors>
<title>Arabic Native Language Identification.</title>
<date>2014</date>
<booktitle>In Proceedings of the Arabic Natural Language Processing Workshop (colocated with EMNLP 2014),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="34067" citStr="Malmasi and Dras (2014" startWordPosition="5575" endWordPosition="5578">y and redundancy of features through a number of metrics (including the χ2-statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a</context>
</contexts>
<marker>Malmasi, Dras, 2014</marker>
<rawString>Shervin Malmasi and Mark Dras. 2014a. Arabic Native Language Identification. In Proceedings of the Arabic Natural Language Processing Workshop (colocated with EMNLP 2014), Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Mark Dras</author>
</authors>
<title>Chinese Native Language Identification.</title>
<date>2014</date>
<booktitle>Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34067" citStr="Malmasi and Dras (2014" startWordPosition="5575" endWordPosition="5578">y and redundancy of features through a number of metrics (including the χ2-statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a</context>
</contexts>
<marker>Malmasi, Dras, 2014</marker>
<rawString>Shervin Malmasi and Mark Dras. 2014b. Chinese Native Language Identification. Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Mark Dras</author>
</authors>
<title>Language Transfer Hypotheses with Linear SVM Weights.</title>
<date>2014</date>
<booktitle>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="34067" citStr="Malmasi and Dras (2014" startWordPosition="5575" endWordPosition="5578">y and redundancy of features through a number of metrics (including the χ2-statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical dependency. Coming from a linguistic perspective, the works in Jarvis and Crossley (2012) use Linear Discriminant Analysis for classification of texts by L1, a</context>
</contexts>
<marker>Malmasi, Dras, 2014</marker>
<rawString>Shervin Malmasi and Mark Dras. 2014c. Language Transfer Hypotheses with Linear SVM Weights. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Malmasi</author>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>NLI Shared Task 2013: MQ Submission.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>124--133</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="31929" citStr="Malmasi et al., 2013" startWordPosition="5238" endWordPosition="5241">the L1s according to the ANOVA F-statistic, but produces false positives. Overall, a recurring issue illustrated for all models by 13We assume that the multiple examples come from the larger CLC corpus. the examples is the proposal of error context far away from any likely relevance to SLA. 5 Related Work While Native Language Identification (NLI) as a subfield of NLP has seen much new work in the last few years — the papers from the shared task (Tetreault et al., 2013) provide a recent sample — the emphasis on optimising classification task results, for example by using classifier ensembles (Malmasi et al., 2013), versus analysing features for relevance to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (</context>
</contexts>
<marker>Malmasi, Wong, Dras, 2013</marker>
<rawString>Shervin Malmasi, Sze-Meng Jojo Wong, and Mark Dras. 2013. NLI Shared Task 2013: MQ Submission. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 124–133, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lourdes Ortega</author>
</authors>
<title>Understanding Second Language Acquisition. Hodder Education,</title>
<date>2009</date>
<location>Oxford, UK.</location>
<contexts>
<context position="1451" citStr="Ortega (2009)" startWordPosition="214" endWordPosition="215">ly on error-feature cooccurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts, with an apparent trade-off between false positives and very specific contexts. 1 Introduction SLA researchers are interested in a wide variety of aspects of humans learning a new language (L2) different from their native one (L1): cognitive issues and developmental sequences for learners Pienemann (2005), sociocultural factors (Lantolf, 2001), and so on. One longstanding question, dating back to at least Lado (1957), is expressed by Ortega (2009) in the following way: “What is the role played by first language in L2 development, vis-`a-vis the role of other universal development forces?” An example of SLA research that looks at this question is the study of Di´ez-Bedmar and Papp (2008), comparing Chinese and Spanish learners of English with respect to the English article system (a, an, the) using corpora of essays by native and non-native speakers of English (Granger, 2011). Drawing on the 175 nonnative texts, they take a particular theoretical analysis (the so-called Bickerton semantic wheel), use the simple Wordsmith tools designed </context>
</contexts>
<marker>Ortega, 2009</marker>
<rawString>Lourdes Ortega. 2009. Understanding Second Language Acquisition. Hodder Education, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patric ¨Osterg˚ard</author>
</authors>
<title>A New Algorithm for the Maximum-Weight Clique Problem.</title>
<date>1999</date>
<booktitle>Electronic Notes in Discrete Mathematics,</booktitle>
<pages>3--153</pages>
<marker>¨Osterg˚ard, 1999</marker>
<rawString>Patric ¨Osterg˚ard. 1999. A New Algorithm for the Maximum-Weight Clique Problem. Electronic Notes in Discrete Mathematics, 3:153–156, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Betsy Parrish</author>
</authors>
<title>A New Look at Methodologies in the Study of Article Acquisition for Learners of ESL.</title>
<date>1987</date>
<journal>Language Learning,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="22200" citStr="Parrish (1987)" startWordPosition="3682" endWordPosition="3683">ency of occurrence is very strongly linked to the L1, as noted in Table 2 and §3.2.12 (For the error type MT also, no model produces an error context more strongly associated with the L1 for the single best choice where N = 1, but does for larger values of N.) 11Code for the used wclique is available at http://tcs. legacy.ics.tkk.fi/˜pat/wclique.html. 12The fact that determiner errors are very widely studied in terms of analysing cross-linguistic influence suggests a broad consensus that they vary strongly with L1. In addition to Di´ezBedmar and Papp (2008), a sample of other studies includes Parrish (1987), Young (1996) and Ionin and Montrul (2010). With respect to the individual models, the simple ERRORCOOCC scores highly, giving the best result about half the time, and the best results can occur for any of k = 1, 2, 3. The number of instances returned for each error plus error context is larger than for the other models as well, which is not surprising as the model aims to find contexts strongly associated with the errors rather than with L1s. However, these are then likely to be features that are fairly common across L1s; we look at some examples in §4.2. L1ASSOC performs fairly poorly on ou</context>
</contexts>
<marker>Parrish, 1987</marker>
<rawString>Betsy Parrish. 1987. A New Look at Methodologies in the Study of Article Acquisition for Learners of ESL. Language Learning, 37(3):361–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Pienemann</author>
</authors>
<title>Cross-linguistic Aspects of Processability Theory.</title>
<date>2005</date>
<publisher>John Benjamins,</publisher>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="1306" citStr="Pienemann (2005)" startWordPosition="190" endWordPosition="191">e language of the speaker that are potentially useful for SLA research. We propose four models for approaching this task, and find that one based only on error-feature cooccurrence and another based on determining maximum weight cliques in a feature association graph discover strongly distinguishing contexts, with an apparent trade-off between false positives and very specific contexts. 1 Introduction SLA researchers are interested in a wide variety of aspects of humans learning a new language (L2) different from their native one (L1): cognitive issues and developmental sequences for learners Pienemann (2005), sociocultural factors (Lantolf, 2001), and so on. One longstanding question, dating back to at least Lado (1957), is expressed by Ortega (2009) in the following way: “What is the role played by first language in L2 development, vis-`a-vis the role of other universal development forces?” An example of SLA research that looks at this question is the study of Di´ez-Bedmar and Papp (2008), comparing Chinese and Spanish learners of English with respect to the English article system (a, an, the) using corpora of essays by native and non-native speakers of English (Granger, 2011). Drawing on the 17</context>
</contexts>
<marker>Pienemann, 2005</marker>
<rawString>Manfred Pienemann. 2005. Cross-linguistic Aspects of Processability Theory. John Benjamins, Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Swanson</author>
<author>Eugene Charniak</author>
</authors>
<title>Native Language Detection with Tree Substitution Grammars.</title>
<date>2012</date>
<booktitle>In Proc. Meeting Assoc. Computat. Linguistics (ACL),</booktitle>
<pages>193--197</pages>
<contexts>
<context position="33233" citStr="Swanson and Charniak (2012)" startWordPosition="5447" endWordPosition="5450">tures on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1- specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a number of metrics (including the χ2-statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare feature</context>
</contexts>
<marker>Swanson, Charniak, 2012</marker>
<rawString>Benjamin Swanson and Eugene Charniak. 2012. Native Language Detection with Tree Substitution Grammars. In Proc. Meeting Assoc. Computat. Linguistics (ACL), pages 193–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
<author>Eugene Charniak</author>
</authors>
<title>Extracting the native language signal for second language acquisition.</title>
<date>2013</date>
<booktitle>In Proc. Conf. North American Assoc. for Computat. Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>85--94</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="3606" citStr="Swanson and Charniak (2013)" startWordPosition="561" endWordPosition="564"> a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above. </context>
<context position="15278" citStr="Swanson and Charniak (2013)" startWordPosition="2487" endWordPosition="2490">ext of RG_JJ_NN1, JJ_NN1_II and VBZ_RG is more strongly associated with L1s; and we are also interested in which of our proposed methods for identifying an error’s feature context does this best. For this purpose, then, we use just the F-statistic from the ANOVA test, this time with the dependent variable as the ratio of occurrences of error plus error context per 10 sentences: a higher F-statistic shows a stronger association with L1s.7 We also consider the x2-statistic from Pearson’s chisquared test, noting that it is also used in SLA hypothesis testing and that it was additionally found by Swanson and Charniak (2013) to be good at distinguishing interesting features in their related task (see §5 for more detail). The F-statistic and x2-statistic are closely related: a random variate of the F-distribution is the ratio of two chi-squared variates scaled by their degrees of freedom. A difference is that x2 compares observed versus expected counts rather than proportions: to take account of the differing text lengths, our observed frequency is the number of sentences with error and error context per L1; our expected frequency is the total number of sentences with that error and error context scaled according </context>
<context position="33410" citStr="Swanson and Charniak (2013)" startWordPosition="5471" endWordPosition="5474">ugh overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1- specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a number of metrics (including the χ2-statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 an</context>
</contexts>
<marker>Swanson, Charniak, 2013</marker>
<rawString>Ben Swanson and Eugene Charniak. 2013. Extracting the native language signal for second language acquisition. In Proc. Conf. North American Assoc. for Computat. Linguistics: Human Language Technologies (NAACL-HLT), pages 85–94, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
<author>Eugene Charniak</author>
</authors>
<title>Data Driven Language Transfer Hypotheses. In</title>
<date>2014</date>
<booktitle>Proc. Conf. European Assoc. for Computat. Linguistics (EACL),</booktitle>
<pages>169--173</pages>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="3638" citStr="Swanson and Charniak (2014)" startWordPosition="566" endWordPosition="569">; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above. In this paper, we look at approa</context>
<context position="33910" citStr="Swanson and Charniak (2014)" startWordPosition="5550" endWordPosition="5554">e a richer representation 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a number of metrics (including the χ2-statistic used in this paper). They then extend a Bayesian induction model for TSG inference based on a supervised mixture of hierarchical grammars, in order to extract a filtered set of more linguistically informed features that could benefit both NLI and SLA research; an aim was to find relatively rare features that are nevertheless useful for L1 prediction. Swanson and Charniak (2014) continue on from this with a data-driven approach to inferring possible relationships between L1 and L2 structures, again using TSGs. Malmasi and Dras (2014c) also propose a method for identifying potential language transfer effects by using additional linguistic features such as adaptor grammars and grammatical dependencies to analyse differences in learner language. This body of work thus shares some similarities with the present paper, but our focus is on errors rather than on the distributional differences, and we look at error contexts that may not constitute a TSG tree or grammatical de</context>
</contexts>
<marker>Swanson, Charniak, 2014</marker>
<rawString>Ben Swanson and Eugene Charniak. 2014. Data Driven Language Transfer Hypotheses. In Proc. Conf. European Assoc. for Computat. Linguistics (EACL), pages 169–173, Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
</authors>
<title>A report on the first native language identification shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA),</booktitle>
<pages>48--57</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="3097" citStr="Tetreault et al., 2013" startWordPosition="479" endWordPosition="482">arried out on relatively small datasets, and use fairly elementary tools. Sources such as Ellis (2008) and Ortega (2009) give good overviews of such studies and of SLA research in general. A goal of this paper is to investigate a particular way in which Natural Language Processing (NLP) can usefully contribute to SLA. In terms of existing work, the subfield of Native Language Identification (NLI) has been quite active recently, which looks at predicting the L1 of writers writing in a common L2 within a classification task framework; see for example the recent NLI shared task with 29 entrants (Tetreault et al., 2013).1 From within linguistics, there has been much interest in how data-driven approaches can contribute to SLA. Granger (2011) discusses a body of work based on the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in </context>
<context position="10865" citStr="Tetreault et al., 2013" startWordPosition="1749" endWordPosition="1752"> Spanish SPA 200 Turkish TUR 75 Table 1: FCESUB, broken down by language (2012). The full FCE corpus consists of 1244 scripts over 16 languages; script counts range from 2 (Dutch) to 200 (Spanish). The features used by Yannakoudakis et al. (2012) were derived from their essay classification task. As we are interested in associations with L1, we instead use features from a system submitted to the NLI shared task (Anonymous, 2013), which was applied to a dataset of Test of English as a Foreign Language (TOEFL) scripts: the task and its designated corpus are described in the task overview paper (Tetreault et al., 2013). In this work we use a system trained on the TOEFL11 corpus consisting of texts written in English from speakers of 11 different L1s, with 1100 essays per L1 and balanced across topic. We only use PoS n-grams (n = 1, 2,3) as features in this work. Note that we use the terminology of Yannakoudakis et al. (2012) here: what had their origin as features in the essay classification task are still referred to as features in the visualisation tool, although the task carried out there is not a classification one. Similarly, we refer to our PoS n-grams as features, although we are not classifying erro</context>
<context position="31782" citStr="Tetreault et al., 2013" startWordPosition="5216" endWordPosition="5219">ERRORCOOCC model produces a much larger set of candidates, and can successfully find error context such that they behave differently with respect to the L1s according to the ANOVA F-statistic, but produces false positives. Overall, a recurring issue illustrated for all models by 13We assume that the multiple examples come from the larger CLC corpus. the examples is the proposal of error context far away from any likely relevance to SLA. 5 Related Work While Native Language Identification (NLI) as a subfield of NLP has seen much new work in the last few years — the papers from the shared task (Tetreault et al., 2013) provide a recent sample — the emphasis on optimising classification task results, for example by using classifier ensembles (Malmasi et al., 2013), versus analysing features for relevance to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Koc</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, 2013</marker>
<rawString>Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA), pages 48–57, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Exploiting parse structures for native language identification.</title>
<date>2011</date>
<booktitle>In Proc. Conf. Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1600--1610</pages>
<contexts>
<context position="32587" citStr="Wong and Dras (2011)" startWordPosition="5345" endWordPosition="5348">nce to other tasks has varied. Below we discuss works which directly look at how features might be related to language-learning tasks or SLA research. The seminal work of Koppel et al. (2005) that presented NLI as a classification task included, in addition to standard lexical and PoS n-gram features, errors made by the writers; these errors were automatically identified using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1- specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classi</context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting parse structures for native language identification. In Proc. Conf. Empirical Methods in Natural Language Processing (EMNLP), pages 1600–1610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
<author>Mark Johnson</author>
</authors>
<title>Exploring Adaptor Grammars for Native Language Identification.</title>
<date>2012</date>
<booktitle>In Proc. Conf. Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>699--709</pages>
<contexts>
<context position="32955" citStr="Wong et al. (2012)" startWordPosition="5404" endWordPosition="5407">ied using Microsoft Word grammar checker. Kochmar (2011) used the FCE corpus for NLI, including the manually annotated errors as features, and presented an analysis of usefulness of features (including errors) with respect to L1. Wong and Dras (2011) used syntactic features on the basis of SLA theory that posits that L1 constructions may be reflected in some form of characteristic errors or patterns in L2 constructions to some extent, or through overuse or avoidance of particular constructions in L2 (Lado, 1957; Ellis, 2008); they did note distributional differences of features related to L1. Wong et al. (2012) induced topic models over function words and PoS ngrams, where some of the topics appeared to reflect L1- specific characteristics. These works, while interested in the nature of the features, do not evaluate them except via classification accuracy. Swanson and Charniak (2012) similarly explore using syntax, where they propose a richer representation 62 for L1-specific constructions through Tree Substitution Grammar (TSG). Swanson and Charniak (2013) subsequently examine both relevancy and redundancy of features through a number of metrics (including the χ2-statistic used in this paper). They</context>
</contexts>
<marker>Wong, Dras, Johnson, 2012</marker>
<rawString>Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson. 2012. Exploring Adaptor Grammars for Native Language Identification. In Proc. Conf. Empirical Methods in Natural Language Processing (EMNLP), pages 699–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A New Dataset and Method for Automatically Grading ESOL Texts.</title>
<date>2011</date>
<booktitle>In Proc. Meeting Assoc. Computat. Linguistics (ACL),</booktitle>
<pages>180--189</pages>
<contexts>
<context position="6195" citStr="Yannakoudakis et al. (2011)" startWordPosition="980" endWordPosition="983">ing Hypotheses: A Visualisation Tool The context of the Yannakoudakis et al. (2012) work is automated grading of English as a Second or Other Language (ESOL) exam scripts, as described in Briscoe et al. (2010). The automated grading takes a classification approach, using a binary discriminative learner, with useful features including lexical and part-of-speech (PoS) n-grams. The publicly available dataset on which the work was carried out consists of texts from the First Certificate in English (FCE) exam, aimed at upper-intermediate students of English across various L1s, and was presented in Yannakoudakis et al. (2011). This FCE corpus2 consists of a subset of 1244 texts of the Cambridge Learner Corpus,3 and is manually annotated with errors and their corrections, as well as a classification according to an error typology, as in Figure 1. Yannakoudakis et al. (2012) present their English Profile (EP) visualiser as a way to “visually analyse as well as perform a linguistic interpretation of discriminative features that characterise learner English”, using the features of this essay classification task. They define a measure of co-occurrence of features, among themselves and with errors, as a core part of the</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011. A New Dataset and Method for Automatically Grading ESOL Texts. In Proc. Meeting Assoc. Computat. Linguistics (ACL), pages 180–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Theodora Alexopoulou</author>
</authors>
<title>Automating Second Language Acquisition Research: Integrating Information Visualisation and Machine Learning.</title>
<date>2012</date>
<booktitle>In Proc. EACL Workshop of LINGVIS &amp; UNCLH,</booktitle>
<pages>35--43</pages>
<contexts>
<context position="3881" citStr="Yannakoudakis et al. (2012)" startWordPosition="605" endWordPosition="608">n the the methodology of carrying out corpus-based approaches to SLA with a focus on NLP tools; Jarvis and Crossley (2012) in an edited collection present recent work by linguists who extend the corpus-based setup by using a text classification approach, looking at what feature selection might say for SLA. From within NLP, Swanson and Charniak (2013) and Swanson and Charniak (2014) take a data-driven approach to SLA investigations much in the spirit of this work. One particular approach to finding aspects of texts characteristic of their L1s that has motivated the present work is described in Yannakoudakis et al. (2012), the goal of which is to develop visualisation tools for SLA researchers. They present graphs of the relationships between errors and their contexts, such that SLA researchers can navigate through the graphs to find contexts for particular errors that can lead to hypotheses like that of Di´ez-Bedmar and Papp (2008) above. In this paper, we look at approaches to finding such hypothesis candidates automatically in the context of L1–L2 interaction by analysing the graphs used in the visualisations 1http://sites.google.com/site/ nlisharedtask2013/ 56 Proceedings of TextGraphs-9: the workshop on G</context>
<context position="5313" citStr="Yannakoudakis et al. (2012)" startWordPosition="835" endWordPosition="839">eature occurs in sk. of Yannakoudakis et al. (2012). Specifically, we do the following: • We propose a new task that is more directly oriented to SLA research than NLI has been for the most part, with the goal of identifying error-related contexts that are characteristic of L1s. • We evaluate a number of models for finding such contexts, ranging from a simple baseline to treating the problem as a graph-theoretic maximum weighted clique one. • We examine the results of some of the models to see how the task and the models might contribute to SLA research. Because we draw heavily on the work of Yannakoudakis et al. (2012), we first review relevant aspects of that work in §2; we then present our task definition and experimental setup in §3; we give results along with a discussion in §4; we follow with some more detail on related work in §5; and we conclude in §6. 2 Developing Hypotheses: A Visualisation Tool The context of the Yannakoudakis et al. (2012) work is automated grading of English as a Second or Other Language (ESOL) exam scripts, as described in Briscoe et al. (2010). The automated grading takes a classification approach, using a binary discriminative learner, with useful features including lexical a</context>
<context position="8891" citStr="Yannakoudakis et al. (2012)" startWordPosition="1432" endWordPosition="1436">tch the features (e.g. Unix is very powerful system but there is one thing against it), along with a discussion of relationships to various L1s. It is this process of finding interesting features and linking them to particular errors and L1s that we present an approach to automating in this paper. 3 Task Definition &amp; Experimental Setup At a general level, our goal is to find which kinds of constructions (in a loose sense) centred around errors are particularly characteristic of various L1s. The specific task we define for this paper, then, is to select a set of features (in the terminology of Yannakoudakis et al. (2012))—which we refer to as the ERROR CONTEXT—that, when combined with the error, show a strong association with L1, in a manner we describe below. So, for example, this may involve finding that an MD error in the context of RG_JJ_NN1, JJ_NN1_II and VBZ_RG shows a strong association with L1. We investigate a number of models for this selection process: the task then is the identification of which models produce poor error contexts (which will not rank highly in hypothesis testing) and which produce good ones (potentially worth considering by an SLA researcher). Below we discuss the data we use, the</context>
<context position="10488" citStr="Yannakoudakis et al. (2012)" startWordPosition="1686" endWordPosition="1689">ked&lt;/i&gt;&lt;c&gt;shook&lt;/c&gt;&lt;/ns&gt; their Inflection hands,...&lt;/p&gt; Missing Determiner &lt;p&gt;I am &lt;ns type=&amp;quot;MD&amp;quot;&gt;&lt;c&gt;a&lt;/c&gt;&lt;/ns&gt; really good singer.&lt;/p&gt; Figure 1: FCE corpus examples. Error types indicated by &lt;ns type&gt;...&lt;/ns&gt;; errors indicated by &lt;i&gt;...&lt;/i&gt;; corrections indicated by &lt;c&gt;...&lt;/c&gt;. language size Chinese CHI 66 French FRE 146 German GER 69 Italian ITA 76 Japanese JAP 81 Korean KOR 86 Spanish SPA 200 Turkish TUR 75 Table 1: FCESUB, broken down by language (2012). The full FCE corpus consists of 1244 scripts over 16 languages; script counts range from 2 (Dutch) to 200 (Spanish). The features used by Yannakoudakis et al. (2012) were derived from their essay classification task. As we are interested in associations with L1, we instead use features from a system submitted to the NLI shared task (Anonymous, 2013), which was applied to a dataset of Test of English as a Foreign Language (TOEFL) scripts: the task and its designated corpus are described in the task overview paper (Tetreault et al., 2013). In this work we use a system trained on the TOEFL11 corpus consisting of texts written in English from speakers of 11 different L1s, with 1100 essays per L1 and balanced across topic. We only use PoS n-grams (n = 1, 2,3) </context>
<context position="16151" citStr="Yannakoudakis et al. (2012)" startWordPosition="2639" endWordPosition="2642">ir degrees of freedom. A difference is that x2 compares observed versus expected counts rather than proportions: to take account of the differing text lengths, our observed frequency is the number of sentences with error and error context per L1; our expected frequency is the total number of sentences with that error and error context scaled according to the proportion of sentences labelled with that L1 relative to the corpus as a whole. 3.3 Errors Chosen From the 74 error types in the FCE corpus, we select a subset to evaluate our models. In addition to the MD error used in the case study of Yannakoudakis et al. (2012), we choose a subset which has a range of F-statistic values as described above: some show very similar patterns across L1s (i.e. with low F-statistic), such as DN Wrong Derived Noun (e.g. hot vs heat); others do vary significantly with L1, such as DJ Wrong Derived Adjective (e.g. reasonally vs reasonable). Having errors with a range of F-statistic values lets us evaluate whether finding good error contexts works only for strongly L1- associated errors, weakly L1-associated errors, or across 7As we are only using the F-statistic to evaluate ranks, we do not need a multiple comparison adjustmen</context>
<context position="18701" citStr="Yannakoudakis et al. (2012)" startWordPosition="3073" endWordPosition="3076"> task.9 The relationship between errors and features (in the form of error-feature co-occurrence scores) is not taken into account here. Again, we look at results for when k = 1..3 features are chosen, and for k = 2, 3, we add the individual error-feature scores together for the ranking. MAXWEIGHTCLIQUE Both of the preceding models look only at one factor that might be relevant: errorfeature scores (finding features that are related to the errors) and a measure of the association of features with L1s; but there is no link between them, and interaction of features is not taken into account. In Yannakoudakis et al. (2012), the visualiser provides to the SLA researcher a graph showing the relatedness of features, based on Equation (1), and the SLA researcher combines this with error-feature scores to find interesting candidate error contexts; we create a similar graph and aim to imitate the process by incorporating error-feature scores as follows. We define a weighted undirected graph G = (V, A) such that V is the set of features used in the above models (i.e. PoS n-grams from ERRORCOOCC); A is defined such that (vi, vj) E A for vertices vi, vj E V if 0.8 &lt; scoreff(vi, vj) &lt; 1.0 where scoreff() is as defined as</context>
<context position="30474" citStr="Yannakoudakis et al. (2012)" startWordPosition="4993" endWordPosition="4996">ly sized set are similar to the first sentence, where the context appears interesting. In this example, hot is used for heat; the other examples of this type are from Spanish and Italian (similarly, e.g., live for life), where the error seems to be connected to words where the English derivational morphology is not simply affixation. However, there are some like the second sentence, where (as for the DJ error) the error context appears in a different clause, and likely irrelevant. The MD error in the last row we examine because (a more complex version of) it was the focus of the case study in Yannakoudakis et al. (2012), which from the examples of that paper looked quite convincing as an error context of relevance to SLA research. However, it and the related examples of Yannakoudakis et al. (2012) were not in the publicly available corpus,13 and in fact there is only one example of this error and context in the whole FCE corpus, illustrating the issue of data sparsity. Further, this example also illustrates the issue of tagging error: that is tagged as RG (degree adverb) where it should be CST. So as might be anticipated from the frequency numbers in Table 5, the MAXWEIGHTCLIQUE-L1 model produces context tha</context>
</contexts>
<marker>Yannakoudakis, Briscoe, Alexopoulou, 2012</marker>
<rawString>Helen Yannakoudakis, Ted Briscoe, and Theodora Alexopoulou. 2012. Automating Second Language Acquisition Research: Integrating Information Visualisation and Machine Learning. In Proc. EACL Workshop of LINGVIS &amp; UNCLH, pages 35–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Young</author>
</authors>
<title>Form-Function Relations in Articles in English Interlanguage.</title>
<date>1996</date>
<booktitle>Second Language Acquisition and Linguistic Variation,</booktitle>
<pages>135--175</pages>
<editor>In R. Bayley and D. R. Preston, editors,</editor>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="22214" citStr="Young (1996)" startWordPosition="3684" endWordPosition="3685">ce is very strongly linked to the L1, as noted in Table 2 and §3.2.12 (For the error type MT also, no model produces an error context more strongly associated with the L1 for the single best choice where N = 1, but does for larger values of N.) 11Code for the used wclique is available at http://tcs. legacy.ics.tkk.fi/˜pat/wclique.html. 12The fact that determiner errors are very widely studied in terms of analysing cross-linguistic influence suggests a broad consensus that they vary strongly with L1. In addition to Di´ezBedmar and Papp (2008), a sample of other studies includes Parrish (1987), Young (1996) and Ionin and Montrul (2010). With respect to the individual models, the simple ERRORCOOCC scores highly, giving the best result about half the time, and the best results can occur for any of k = 1, 2, 3. The number of instances returned for each error plus error context is larger than for the other models as well, which is not surprising as the model aims to find contexts strongly associated with the errors rather than with L1s. However, these are then likely to be features that are fairly common across L1s; we look at some examples in §4.2. L1ASSOC performs fairly poorly on our evaluation m</context>
</contexts>
<marker>Young, 1996</marker>
<rawString>Richard Young. 1996. Form-Function Relations in Articles in English Interlanguage. In R. Bayley and D. R. Preston, editors, Second Language Acquisition and Linguistic Variation, pages 135–175. John Benjamins, Amsterdam, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
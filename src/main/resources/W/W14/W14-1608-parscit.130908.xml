<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000375">
<title confidence="0.89151">
Looking for Hyponyms in Vector Space
</title>
<author confidence="0.805407">
Marek Rei
</author>
<affiliation confidence="0.528286">
SwiftKey
</affiliation>
<address confidence="0.554039">
95 Southwark Bridge Rd
London, UK
</address>
<email confidence="0.997367">
marek@swiftkey.net
</email>
<author confidence="0.994616">
Ted Briscoe
</author>
<affiliation confidence="0.9917965">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.602745">
Cambridge, UK
</address>
<email confidence="0.997485">
ted.briscoe@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948666666667">
The task of detecting and generating hy-
ponyms is at the core of semantic under-
standing of language, and has numerous
practical applications. We investigate how
neural network embeddings perform on
this task, compared to dependency-based
vector space models, and evaluate a range
of similarity measures on hyponym gener-
ation. A new asymmetric similarity mea-
sure and a combination approach are de-
scribed, both of which significantly im-
prove precision. We release three new
datasets of lexical vector representations
trained on the BNC and our evaluation
dataset for hyponym generation.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968921875">
Hyponymy is a relation between two word senses,
indicating that the meaning of one word is also
contained in the other. It can be thought of as a
type-of relation; for example car, ship and train
are all hyponyms of vehicle. We denote a hy-
ponymy relation between words a and b as (a → b),
showing that a is a hyponym of b, and b is a hyper-
nym of a. Hyponymy relations are closely related
to the concept of entailment, and this notation is
consistent with indicating the direction of infer-
ence – if a is true, b must be true as well.
Automatic detection and generation of hy-
ponyms has many practical applications in nearly
all natural language processing tasks. Information
retrieval, information extraction and question an-
swering can be improved by performing appropri-
ate query expansions. For example, a user search-
ing for arthritis treatment is most likely also inter-
ested in results containing the hyponyms of treat-
ment, such as arthritis therapy, arthritis medica-
tion, and arthritis rehabilitation. Summarisation
systems can increase coherence and reduce repe-
tition by correctly handling hyponymous words in
the input text. Entailment and inference systems
can improve sentence-level entailment resolution
by detecting the presence and direction of word-
level hyponymy relations. Distributionally simi-
lar words have been used for smoothing language
models and word co-occurrence probabilities (Da-
gan et al., 1999; Weeds and Weir, 2005), and hy-
ponyms can be more suitable for this application.
We distinguish between three different tasks
related to hyponyms. Given a directional word
pair, the goal of hyponym detection is to deter-
mine whether one word is a hyponym of the other
(Zhitomirsky-Geffet and Dagan, 2009; Kotlerman
et al., 2010; Baroni and Lenci, 2011). In con-
trast, hyponym acquisition is the task of extract-
ing all possible hyponym relations from a given
text (Hearst, 1992; Caraballo, 1999; Pantel and
Ravichandran, 2004; Snow et al., 2005). Such sys-
tems often make use of heuristic rules and patterns
for extracting relations from surface text, and pop-
ulate a database with hyponymous word pairs. Fi-
nally, the task of hyponym generation is to re-
turn a list of all possible hyponyms, given only
a single word as input. This is most relevant to
practical applications, as many systems require a
set of appropriate substitutes for a specific term.
Automated ontology creation (Biemann, 2005) is
a related field that also makes use of distributional
similarity measures. However, it is mostly focused
on building prototype-based ontologies through
clustering (Ushioda, 1996; Bisson et al., 2000;
Wagner, 2000; Paaß et al., 2004; Cimiano and
Staab, 2005), and is not directly applicable to hy-
ponym generation.
While most work has been done on hyponym
detection (and the related task of lexical substitu-
tion), barely any evaluation has been done for hy-
ponym generation. We have found that systems for
hyponym detection often perform poorly on hy-
ponym generation, as the latter requires returning
results from a much less restricted candidate set,
</bodyText>
<page confidence="0.992084">
68
</page>
<note confidence="0.6881465">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68–77,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999240153846154">
and therefore a task-specific evaluation is required.
In this paper we focus on hyponym generation
and approach it by scoring a very large candidate
set of potential hyponyms. Distributional similar-
ity methods are especially interesting for this task,
as they can be easily applied to different domains,
genres and languages without requiring annotated
training data or manual pattern construction. We
perform a systematic comparison of different vec-
tor space models and similarity measures, in order
to better understand the properties of a successful
method for hyponym generation.
The main contributions of this paper are:
</bodyText>
<listItem confidence="0.99985875">
1. Systematic evaluation of different vector
space models and similarity measures on the
task of hyponym generation.
2. Proposal of new properties for modelling the
directional hyponymy relation.
3. Release of three lexical vector datasets,
trained using neural network, window-based,
and dependency-based features.
</listItem>
<sectionHeader confidence="0.551843" genericHeader="method">
2 Vector space models
</sectionHeader>
<bodyText confidence="0.980094083333333">
In order to use similarity measures for hyponym
detection, every word needs to be mapped to a
point in vector space. The method of choosing
appropriate features for these vectors is crucial to
achieving the optimal performance. We compare
five different approaches:
Window: As a simple baseline, we created vec-
tors by counting word co-occurrences in a fixed
context window. Every word that occurs within a
window of three words before or after is counted
as a feature for the target word. Pointwise mutual
information is then used for weighting.
CW: Collobert and Weston (2008) constructed
a neural network language model that is trained to
predict the next word in the sequence, and simul-
taneously learns vector representations for each
word. The vectors for context words are concate-
nated and used as input for the neural network,
which uses a sample of possible outputs for gra-
dient calculation to speed up the training process.
Turian et al. (2010) recreated their experiments
and made the vectors available online.1
HLBL: Mnih and Hinton (2007) created word
representations using the hierarchical log-bilinear
</bodyText>
<footnote confidence="0.951408">
1http://metaoptimize.com/projects/wordreprs/
</footnote>
<bodyText confidence="0.999890367346939">
model – a neural network that takes the concate-
nated vectors of context words as input, and is
trained to predict the vector representation of the
next word, which is then transformed into a prob-
ability distribution over possible words. To speed
up training and testing, they use a hierarchical data
structure for filtering down the list of candidates.
Both CW and HLBL vectors were trained using
37M words from RCV1.
Word2vec: We created word representations
using the word2vec2 toolkit. The tool is based
on a feedforward neural network language model,
with modifications to make representation learn-
ing more efficient (Mikolov et al., 2013a). We
make use of the skip-gram model, which takes
each word in a sequence as an input to a log-linear
classifier with a continuous projection layer, and
predicts words within a certain range before and
after the input word. The window size was set to
5 and vectors were trained with both 100 and 500
dimensions.
Dependencies: Finally, we created vector rep-
resentations for words by using dependency rela-
tions from a parser as features. Every incoming
and outgoing dependency relation is counted as a
feature, together with the connected term. For ex-
ample, given the dependency relation (play, dobj,
guitar), the tuple (&gt;dobj, guitar) is extracted as a
feature for play, and (&lt;dobj, play) as a feature for
guitar. We use only features that occur more than
once in the dataset, and weight them using point-
wise mutual information to construct feature vec-
tors for every term. Features with negative weights
were retained, as they proved to be beneficial for
some similarity measures.
The window-based, dependency-based and
word2vec vector sets were all trained on 112M
words from the British National Corpus, with pre-
processing steps for lowercasing and lemmatis-
ing. Any numbers were grouped and substituted
by more generic tokens. For constructing the
dependency-based vector representations, we used
the parsed version of the BNC created by Ander-
sen et al. (2008) with the RASP toolkit (Briscoe
et al., 2006). When saved as plain text, the 500-
dimensional word2vec vectors and dependency-
based vectors are comparable in size (602MB and
549MB), whereas the window-based vectors are
twice as large (1,004MB). We make these vector
</bodyText>
<footnote confidence="0.980941">
2https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.999375">
69
</page>
<bodyText confidence="0.998950826086957">
sets publically available for download.3
Recently, Mikolov et al. (2013b) published in-
teresting results about linguistic regularities in
vector space models. They proposed that the rela-
tionship between two words can be characterised
by their vector offset, for example, we could find
the vector for word “queen” by performing the op-
eration “king - man + woman” on corresponding
vectors. They also applied this approach to hy-
ponym relations such as (shirt → clothing) and
(bowl → dish). We evaluate how well this method
applies to hyponym generation with each of the
vector space models mentioned above. Using the
training data, we learn a vector for the hyponymy
relation by averaging over all the offset vectors
for hyponym-hypernym pairs. This vector is then
added to the hypernym during query time, and
the result is compared to hyponym candidates us-
ing cosine similarity. For sparse high-dimensional
vector space models it was not feasible to use the
full offset vector during experiments, therefore we
retain only the top 1,000 highest-weighted fea-
tures.
</bodyText>
<sectionHeader confidence="0.985156" genericHeader="method">
3 Similarity measures
</sectionHeader>
<bodyText confidence="0.99998024">
We compare the performance of a range of simi-
larity measures, both directional and symmetrical,
on the task of hyponym generation.
Cosine similarity is defined as the angle be-
tween two feature vectors and has become a stan-
dard measure of similarity between weighted vec-
tors in information retrieval (IR).
Lin similarity, created by Lin (1998), uses the
ratio of shared feature weights compared to all fea-
ture weights. It measures the weighted proportion
of features that are shared by both words.
DiceGen2 is one possible method for generalis-
ing the Dice measure to real-valued weights (Cur-
ran, 2003; Grefenstette, 1994). The dot product of
the weight vectors is normalised by the total sum
of all weights. The same formula can also be con-
sidered as a possible generalisation for the Jaccard
measure.
WeedsPrec and WeedsRec were proposed by
Weeds et al. (2004) who suggested using precision
and recall as directional measures of word simi-
larity. In this framework, the features are treated
similarly to retrieved documents in information re-
trieval – the vector of the broader term b is used as
the gold standard, and the vector of the narrower
</bodyText>
<footnote confidence="0.678833">
3http://www.marekrei.com/projects/vectorsets/
</footnote>
<bodyText confidence="0.999785">
term a is in the role of retrieval results. Precision
is then calculated by comparing the intersection
(items correctly returned) to the values of the nar-
rower term only (all items returned). In contrast,
WeedsRec quantifies how well the features of the
breader term are covered by the narrower term.
Balprec is a measure created by Szpektor and
Dagan (2008). They proposed combining Weed-
sPrec together with the Lin measure by taking
their geometric average. This aims to balance the
WeedsPrec score, as the Lin measure will penalise
cases where one vector contains very few features.
ClarkeDE, proposed by Clarke (2009), is an
asymmetric degree of entailment measure, based
on the concept of distributional generality (Weeds
et al., 2004). It quantifies the weighted coverage of
the features of the narrower term a by the features
of the broader term b.
BalAPInc, a measure described by Kotlerman
et al. (2010), combines the APInc score with Lin
similarity by taking their geometric average. The
APInc measure finds the proportion of shared fea-
tures relative to the features for the narrower term,
but this can lead to unreliable results when the
number of features is very small. The motivation
behind combining these measures is that the sym-
metric Lin measure will decrease the final score
for such word pairs, thereby balancing the results.
</bodyText>
<sectionHeader confidence="0.446332" genericHeader="method">
4 Properties of a directional measure
</sectionHeader>
<bodyText confidence="0.984249636363636">
Finding similar words in a vector space, given
a symmetric similarity measure, is a relatively
straightforward task. However finding hyponyms
is arguably more difficult, as the relation is asym-
metric, and looking at the distance or angle be-
tween the two words may not be enough.
Kotlerman et al. (2010) investigate the related
problem of detecting directional lexical entail-
ment, and they propose three desirable properties
that a directional distributional similarity measure
should capture:
</bodyText>
<listItem confidence="0.996465428571429">
1. The relevance of the shared features to the
narrower term.
2. The relevance of the shared features to the
broader term.
3. That relevance is less reliable if the num-
ber of features of either the narrower or the
broader term is small.
</listItem>
<page confidence="0.988791">
70
</page>
<bodyText confidence="0.995558926470589">
Given a term pair (a → b) we refer to a as the
narrower term and b as the broader term. The fea-
tures of a that are also found in b (have non-zero
weights for both a and b) are referred to as shared
features.
They show that existing measures which cor-
respond to these criteria perform better and con-
struct the BalAPInc measure based on the princi-
ples. However, it is interesting to note that these
properties do not explicitly specify any directional
aspects of the measure, and symmetric similarity
scores can also fulfil the requirements.
Based on investigating hyponym distributions
in our training data, we suggest two additions to
this list of desired properties, one of which specif-
ically targets the asymmetric properties of the de-
sired similarity measures:
4. The shared features are more important to
the directional score calculation, compared to
non-shared features.
5. Highly weighted features of the broader term
are more important to the score calculation,
compared to features of the narrower term.
Most existing directional similarity scores mea-
sure how many features of the narrower term are
present for the broader term. If a entails b, then
it is assumed that the possible contexts of a are a
subset of contexts for b, but b occurs in a wider
range of contexts compared to a. This intuition is
used by directional measures such as ClarkeDE,
WeedsPrec and BalAPInc. In contrast, we found
that many features of the narrower term are often
highly specific to that term and do not generalise
even to hypernyms. Since these features have a
very high weight for the narrower term, their ab-
sence with the broader term will have a big nega-
tive impact on the similarity score.
We hypothesise that many terms have certain
individual features that are common to them but
not to other related words. Since most weighting
schemes reward high relative co-occurrence, these
features are also likely to receive high weights.
Therefore, we suggest that features which are not
found for both terms should have a decreased im-
pact on the score calculation, as many of them are
not expected to be shared between hyponyms and
hypernyms. However, removing them completely
is also not advisable, as they allow the measure
to estimate the overall relative importance of the
shared features to the specific term.
We also propose that among the shared features,
those ranked higher for the broader term are more
important to the directional measure. In the hy-
ponymy relation (a → b), the term b is more gen-
eral and covers a wider range of semantic con-
cepts. This also means it is more likely to be
used in contexts that apply to different hyponyms
of b. For example, some of the high-ranking fea-
tures for food are blandly-flavoured, high-calorie
and uneaten. These are properties that co-occur
often with the term food, but can also be applied
to most hyponyms of food. Therefore, we hypoth-
esise that the presence of these features for the nar-
rower term is a good indication of a hyponymy re-
lation. This is somewhat in contrast to most previ-
ous work, where the weights of the narrower term
have been used as the main guideline for similarity
calculation.
</bodyText>
<sectionHeader confidence="0.992696" genericHeader="method">
5 Weighted cosine
</sectionHeader>
<bodyText confidence="0.999988">
We now aim to construct a similarity measure that
follows all five of the properties mentioned above.
Cosine similarity is one of the symmetric similar-
ity measures which corresponds to the first three
desired properties, and our experiments showed
that it performs remarkably well at the task of hy-
ponym generation. Therefore, we decided to mod-
ify cosine similarity to also reflect the final two
properties and produce a more appropriate asym-
metric score.
The standard feature vectors for each word con-
tain weights indicating how important this feature
is to the word. We specify additional weights that
measure how important the feature is to that spe-
cific directional relation between the two terms.
Weighted cosine similarity, shown in Table 1, can
then be used to calculate a modified similarity
score. Fa denotes the set of weighted features for
word a, wa(f) is the weight of feature f for word
a, and z(f) is the additional weight for feature f,
given the directional word pair (a, b).
Based on the new desired properties we want
to downweight the importance of features that are
not present for both terms. For this, we choose
the simple solution of scaling them with a small
constant C ∈ [0, 1]. Next, we also want to assign
higher z(f) values to the shared features that have
high weights for the broader term b. We use the
relative rank of feature f in Fb, rb(f), as the indi-
cator of its importance and scale this value to the
range from C to 1. This results in the importance
</bodyText>
<page confidence="0.881644">
71
</page>
<equation confidence="0.996048833333333">
WeightedCosine(Fa,Fb) = Ef∈Fa∩Fb(z(f)×wa(f))×(z(f)×wb(f))
�E �E
f∈Fa (z(f)×wa(f))2× f∈Fb (z(f)×wb(f))2
r(1 − rb(f) ) x (1 − C) + C if f E Fa n Fb
z(f)
= S |Fb|+1
</equation>
<tableCaption confidence="0.936452">
Table 1: Weighted cosine similarity measure
</tableCaption>
<bodyText confidence="0.96892452173913">
C otherwise
function decreasing linearly as the rank number
increases, but the weights for the shared features
always remain higher compared to the non-shared
features. Tied feature values are handled by as-
signing them the average rank value. Adding 1
to the denominator of the relative rank calculation
avoids exceptions with empty vectors, and also en-
sures that the value will always be strictly greater
than C. While the basic function is still the sym-
metric cosine, the z(f) values will be different de-
pending on the order of the arguments.
The parameter C controls the relative impor-
tance of the ‘unimportant’ features to the direc-
tional relation. Setting it to 0 will ignore these
features completely, while setting it to 1 will result
in the traditional cosine measure. Experiments on
the development data showed that the exact value
of this parameter is not very important, as long as
it is not too close to the extreme values of 0 or 1.
We use the value C = 0.5 for reporting our results,
meaning that the non-shared features are half as
important, compared to the shared features.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="method">
6 Dataset
</sectionHeader>
<bodyText confidence="0.999976653061225">
As WordNet (Miller, 1995) contains numerous
manually annotated hyponymy relations, we can
use it to construct suitable datasets for evaluat-
ing hyponym generation. While WordNet terms
are annotated with only the closest hyponyms, we
are considering all indirect/inherited hyponyms
to be relevant – for example, given relations
(genomics → genetics) and (genetics → biology),
then genomics is also regarded as a hyponym of
biology. WordNet relations are defined between
synsets, but we refrain from the task of word sense
disambiguation and count word a as a valid hy-
ponym for word b if it is valid for any sense of b.
Synonymy can be thought of as a symmetric is-
a relation, and most real-world applications would
require synonyms to also be returned, together
with hyponyms. Therefore, in our dataset we con-
sider synonyms as hyponyms in both directions.
We also performed experiments without synonyms
and found that this had limited effect on the re-
sults – while the accuracy of all similarity mea-
sures slightly decreased (due to fewer numbers of
correct answers), the relative ranking remained the
same. As shown in the next section, the number of
synonyms is typically small compared to the num-
ber of all inherited hyponyms.
To construct the dataset, we first found all
single-word nouns in WordNet that are contained
at least 10 times in the British National Corpus
(BNC). Next, we retained only words that have
at least 10 hyponyms, such that they occur 10 or
more times in the BNC. This selection process
aims to discard WordNet hypernyms that are very
rare in practical use, and would not have enough
examples for learning informative vector represen-
tations. The final dataset contains the remaining
terms, together with all of their hyponyms, includ-
ing the rare/unseen hyponyms. As expected, some
general terms, such as group or location, have a
large number of inherited hyponyms. On average,
each hypernym in the dataset has 233 hyponyms,
but the distribution is roughly exponential, and the
median is only 36.
In order to better facilitate future experiments
with supervised methods, such as described by Ba-
roni et al. (2012), we randomly separated the data
into training (1230 hypernyms), validation (922),
and test (922) sets, and we make these datasets
publically available online.4
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.9998394">
We evaluate how well different vector space mod-
els and similarity measures perform on the task of
hyponym generation. Given a single word as in-
put, the system needs to return a ranked list of
words with correct hyponyms at the top. As the
list of candidates for scoring we use all words in
the BNC that occur at least 10 times (a total of
86,496 words). All the experiments are performed
using tokenised and lemmatised words.
As the main evaluation measure, we report
</bodyText>
<footnote confidence="0.970338">
4http://www.marekrei.com/projects/hypgen/
</footnote>
<page confidence="0.995178">
72
</page>
<table confidence="0.99932875">
Cosine Cosine+offset
MAP P@1 P@5 MAP P@1 P@5
Window 2.18 19.76 12.20 2.19 19.76 12.25
CW-100 0.66 3.80 3.21 0.59 3.91 2.89
HLBL-100 1.01 10.31 6.04 1.01 10.31 6.06
Word2vec-100 1.78 15.96 10.12 1.50 12.38 8.71
Word2vec-500 2.06 19.76 11.92 1.77 17.05 10.71
Dependencies 2.73 25.41 14.90 2.73 25.52 14.92
</table>
<tableCaption confidence="0.966251">
Table 2: Experiments using different vector space models for hyponym generation on the test set. We
report results using regular cosine similarity and the vector offset method described in Section 2.
</tableCaption>
<bodyText confidence="0.974022">
Mean Average Precision (MAP), which averages
precision values at various recall points in the re-
turned list. It combines both precision and recall,
as well as the quality of the ranking, into a sin-
gle measure, and is therefore well-suited for com-
paring different methods. The reported MAP val-
ues are very low – this is due to many rare Word-
Net hyponyms not occurring in the candidate set,
for which all systems are automatically penalised.
However, this allows us to evaluate recall, making
the results comparable between different systems
and background datasets. We also report precision
at top-1 and top-5 returned hyponyms.
As a baseline we report the results of a tra-
ditional hyponym acquisition system. For this,
we implemented the pattern-based matching pro-
cess described by Hearst (1992), and also used by
Snow et al. (2005). These patterns look for ex-
plicit examples of hyponym relations mentioned
in the text, for example:
</bodyText>
<equation confidence="0.552596">
X such as {Y1, Y2, ... , (and|or)} Yom,
</equation>
<bodyText confidence="0.9999656">
where X will be extracted as the hypernym, and Y1
to Yom, as hyponyms. We ran the patterns over the
BNC and extracted 21,704 hyponym pairs, which
were then ranked according to the number of times
they were found.
</bodyText>
<subsectionHeader confidence="0.998331">
7.1 Evaluation of vector spaces
</subsectionHeader>
<bodyText confidence="0.999982">
Table 2 contains experiments with different vector
space models. We report here results using cosine,
as it is an established measure and a competitive
baseline. For our task, the HLBL vectors perform
better than CW vectors, even though they were
trained on the same data. Both of them are out-
performed by word2vec-100 vectors, which have
the same dimensionality but are trained on much
more text. Increasing the dimensionality with
word2vec-500 gives a further improvement. In-
terestingly, the simple window-based vectors per-
form just as well as the ones trained with neural
networks. However, the advantage of word2vec-
500 is that the representations are more compact
and require only about half the space. Finally,
the dependency-based vectors outperform all other
vector types, giving 2.73% MAP and 25.41% pre-
cision at the top-ranked result. While the other
models are built by using neighbouring words as
context, this model looks at dependency relations,
thereby taking both semantic and syntactic roles
into account. The results indicate that word2vec
and window-based models are more suitable when
the general topic of words needs to be captured,
whereas dependency-based vectors are preferred
when the task requires both topical and functional
similarity between words. Our experiments also
included the evaluation of other similarity mea-
sures on different vector space models, and we we
found these results to be representative.
Contrary to previous work, the vector offset
method, described in Section 2, did not pro-
vide substantial improvements on the hyponym
generation task. For the neural network-based
vectors this approach generally decreased perfor-
mance, compared to using direct cosine similar-
ity. There are some marginal improvements for
window and dependency-based models. Unfortu-
nately, the original work did not include baseline
performance using cosine similarity, without ap-
plying vector modifications. It is possible that this
method does not generalise to all word relations
equally well. As part of future work, it is worth
exploring if a hypernym-specific strategy of se-
lecting training examples could improve the per-
formance.
</bodyText>
<page confidence="0.997925">
73
</page>
<table confidence="0.999961307692308">
Validation Test
MAP P@1 P@5 MAP P@1 P@5
Pattern-based 0.53 7.06 4.58 0.51 8.14 4.45
Cosine 2.48 21.06 12.96 2.73 25.41 14.90
Lin 1.87 16.50 10.75 2.01 21.17 12.23
DiceGen2 2.27 18.57 12.62 2.44 21.82 14.55
WeedsPrec 0.13 0.00 0.09 0.12 0.11 0.04
WeedsRec 0.72 0.33 2.45 0.69 0.54 2.41
BalPrec 1.78 15.31 10.55 1.88 17.48 11.34
ClarkeDE 0.23 0.00 0.02 0.24 0.00 0.09
BalAPInc 1.64 14.22 9.12 1.68 15.85 9.66
WeightedCosine 2.59 21.39 13.59 2.85 25.84 15.46
Combined 3.27 23.02 16.09 3.51 27.69 18.02
</table>
<tableCaption confidence="0.973895333333333">
Table 3: Evaluation of different vector similarity measures on the validation and test set of hyponym
generation. We report Mean Average Precision (MAP), precision at rank 1 (P@1), and precision at rank
5 (P@5).
</tableCaption>
<subsectionHeader confidence="0.996238">
7.2 Evaluation of similarity measures
</subsectionHeader>
<bodyText confidence="0.999897285714286">
Table 3 contains experiments with different sim-
ilarity measures, using the dependency-based
model, and Table 4 contains sample output from
the best system. The results show that the pattern-
based baseline does rather poorly on this task.
MAP is low due to the system having very lim-
ited recall, but higher precision at top ranks would
have been expected. Analysis showed that this
system was unable to find any hyponyms for more
than half (513/922) of the hypernyms in the vali-
dation set, leading to such poor recall that it also
affects Precision@1. While the pattern-based sys-
tem did extract a relatively large number of hy-
ponyms from the corpus (21,704 pairs), these are
largely concentrated on a small number of hyper-
nyms (e.g., area, company, material, country) that
are more likely to be mentioned in matching con-
texts.
Cosine, DiceGen2 and Lin – all symmetric
similarity measures – perform relatively well on
this task, whereas established directional measures
perform unexpectedly poorly. This can perhaps be
explained by considering the distribution of hy-
ponyms. Given a word, the most likely candi-
dates for a high cosine similarity are synonyms,
antonyms, hypernyms and hyponyms of that word
– these are words that are likely to be used in simi-
lar topics, contexts, and syntactic roles. By def-
inition, there are an equal number of hyponym
and hypernym relations in WordNet, but this ra-
tio changes rapidly as we remove lower-frequency
words. Figure 1 shows the number of relations ex-
tracted from WordNet, as we restrict the minimum
frequency of the main word. It can be seen that the
number of hyponyms increases much faster com-
pared to the other three relations. This also applies
to real-world data – when averaging over word in-
stances found in the BNC, hyponyms cover 85% of
these relations. Therefore, the high performance
of cosine can be explained by distributionally sim-
ilar words having a relatively high likelihood of
being hyponyms.
</bodyText>
<figure confidence="0.9931504">
100
80
60
40
20
0
0 10 20 30 40 50 60 70 80 90 100
min freq
hyponyms hypernyms
synonyms antonyms
</figure>
<figureCaption confidence="0.987384666666667">
Figure 1: Average number of different relations
per word in WordNet, as we restrict the minimum
word frequency.
</figureCaption>
<bodyText confidence="0.99041875">
One possible reason for the poor performance
of directional measures is that most of them quan-
tify how well the features of the narrower term are
included in the broader term. In contrast, we found
that for hyponym generation it is more important
to measure how well the features of the broader
term are included in the narrower term. This
avg related words
</bodyText>
<page confidence="0.996844">
74
</page>
<bodyText confidence="0.872964833333333">
researcher, biologist, psychologist, economist, observer, physicist, sociologist
football, golf, club, tennis, athletics, rugby, cricket, game, recreation, entertainment
therapy, medication, patient, procedure, surgery, remedy, regimen, medicine
scientist
sport
treatment
</bodyText>
<tableCaption confidence="0.961101">
Table 4: Examples of top results using the combined system. WordNet hyponyms are marked in bold.
</tableCaption>
<bodyText confidence="0.999985311111112">
is supported by WeedsRec outperforming Weed-
sPrec, although the opposite was intended by their
design.
Another explanation for the low performance
is that these directional measures are often devel-
oped in an artificial context. For example, Kotler-
man et al. (2010) evaluated lexical entailment de-
tection on a dataset where the symmetric Lin sim-
ilarity measure was used to select word pairs for
manual annotation. This creates a different task,
as correct terms that do not have a high symmetric
similarity will be excluded from evaluation. The
BalAPInc measure performed best in that setting,
but does not do as well for hyponym generation,
where candidates are filtered only based on mini-
mum frequency.
The weighted cosine measure, proposed in Sec-
tion 5, outperformed all other similarity measures
on both hyponym generation datasets. The im-
provement over cosine is relatively small; how-
ever, it is consistent and the improvement in MAP
is statistically significant on both datasets (p &lt;
0.05), using the Approximate Randomisation Test
(Noreen, 1989; Cohen, 1995) with 106 iterations.
This further supports the properties of a directional
similarity measure described in Section 4.
Finally, we created a new system by combining
together two separate approaches: the weighted
cosine measure using the dependency-based vec-
tor space, and the normal cosine similarity using
word2vec-500 vectors. We found that the former
is good at modelling the grammatical roles and di-
rectional containment, whereas the latter can pro-
vide useful information about the topic and seman-
tics of the word. Turney (2012) also demonstrated
the importance of both topical (domain) and func-
tional vector space models when working with se-
mantic relations. We combined these approaches
by calculating both scores for each word pair and
taking their geometric average, or 0 if it could not
be calculated. This final system gives considerable
improvements across all evaluation metrics, and is
significantly (p &lt; 0.05) better compared to cosine
or weighted cosine methods individually. Table 4
contains some example output from this system.
</bodyText>
<sectionHeader confidence="0.994294" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999974105263158">
Hyponym generation has a wide range of pos-
sible applications in NLP, such as query expan-
sion, entailment detection, and language model
smoothing. Pattern-based hyponym acquisition
can be used to find relevant hyponyms, but these
approaches rely on both words being mentioned
together in a specific context, leading to very low
recall. Vector similarity methods are interesting
for this task, as they can be easily applied to differ-
ent domains and languages without any supervised
learning or manual pattern construction. We cre-
ated a dataset for evaluating hyponym generation
systems and experimented with a range of vector
space models and similarity measures.
Our results show that choosing an appropriate
vector space model is equally important to using a
suitable similarity measure. We achieved the high-
est performance using dependency-based vector
representations, which outperformed neural net-
work and window-based models. Symmetric sim-
ilarity measures, especially cosine similarity, per-
formed surprisingly well on this task. This can
be attributed to an unbalanced distribution of hy-
ponyms, compared to other high-similarity words.
The choice of vector space can be highly depen-
dent on the specific task, and we have made avail-
able our vector datasets created from the same
source using three different methods.
We proposed two new properties for detecting
hyponyms, and used them to construct a new di-
rectional similarity measure. This weighted co-
sine measure significantly outperformed all others,
showing that a theoretically-motivated directional
measure is still the most accurate method for mod-
elling hyponymy relations. Finally, we combined
together two different methods, achieving further
substantial improvements on all evaluation met-
rics.
</bodyText>
<sectionHeader confidence="0.996048" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.875620666666667">
Øistein E. Andersen, Julien Nioche, Edward J. Briscoe,
and John Carroll. 2008. The BNC parsed with
RASP4UIMA. In Proceedings of the Sixth Interna-
</reference>
<page confidence="0.993267">
75
</page>
<reference confidence="0.997826563106797">
tional Language Resources and Evaluation Confer-
ence (LREC08), Marrakech, Morocco.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
Edinburgh.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 23–32.
Chris Biemann. 2005. Ontology learning from text: A
survey of methods. LDV Forum, 20(2002):75–93.
Gilles Bisson, Claire N´edellec, and Dolores Ca˜namero.
2000. Designing clustering methods for ontology
building-The Mo’K workbench. In ECAI Ontology
Learning Workshop.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, number July, pages 77–80, Syd-
ney, Australia. Association for Computational Lin-
guistics.
Sharon A. Caraballo. 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. Pro-
ceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics on Computa-
tional Linguistics, pages 120–126.
Philipp Cimiano and Steffen Staab. 2005. Learning
concept hierarchies from text with a guided hierar-
chical clustering algorithm. In ICML-Workshop on
Learning and Extending Lexical Ontologies by using
Machine Learning Methods.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, number March, pages 112–
119. Association for Computational Linguistics.
Paul R Cohen. 1995. Empirical Methods for Artificial
Intelligence. The MIT Press, Cambridge, MA.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. Proceed-
ings of the 25th international conference on Ma-
chine learning.
James R. Curran. 2003. From distributional to seman-
tic similarity. Ph.D. thesis, University of Edinburgh.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 31:1–31.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th conference on Computational linguistics
(COLING ’92), number July, page 539, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(04):359–389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768–774. Association for Compu-
tational Linguistics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient Estimation of Word Repre-
sentations in Vector Space. ICLR Workshop, pages
1–12.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic Regularities in Continuous Space
Word Representations. (June):746–751.
George A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39–41.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
Proceedings of the 24th international conference on
Machine learning - ICML ’07, pages 641–648.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses: An Introduction. Wiley,
New York.
Gerhard Paaß, J¨org Kindermann, and Edda Leopold.
2004. Learning prototype ontologies by hierachical
latent semantic analysis.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT/NAACL.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING ’08), pages 849–856,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.841978">
76
</page>
<reference confidence="0.999671107142857">
Joseph Turian, Lev Ratinov, and Y Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
Akira Ushioda. 1996. Hierarchical clustering of words
and application to NLP tasks. In Fourth Workshop
on Very Large Corpora, pages 28–41.
Andreas Wagner. 2000. Enriching a lexical semantic
net with selectional preferences by means of statisti-
cal corpus analysis. In ECAI Workshop on Ontology
Learning.
Julie Weeds and David Weir. 2005. Co-occurrence
retrieval: A flexible framework for lexical distribu-
tional similarity. Computational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. Proceedings of the 20th international
conference on Computational Linguistics - COLING
’04.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009.
Bootstrapping Distributional Feature Vector Quality.
Computational Linguistics, 35(3):435–461, Septem-
ber.
</reference>
<page confidence="0.999112">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304845">
<title confidence="0.999069">Looking for Hyponyms in Vector Space</title>
<author confidence="0.943591">Marek</author>
<affiliation confidence="0.704843">95 Southwark Bridge London,</affiliation>
<email confidence="0.957612">marek@swiftkey.net</email>
<author confidence="0.855651">Ted</author>
<affiliation confidence="0.98043">Computer University of</affiliation>
<address confidence="0.883489">Cambridge,</address>
<email confidence="0.999696">ted.briscoe@cl.cam.ac.uk</email>
<abstract confidence="0.998480375">The task of detecting and generating hyponyms is at the core of semantic understanding of language, and has numerous practical applications. We investigate how neural network embeddings perform on this task, compared to dependency-based vector space models, and evaluate a range of similarity measures on hyponym generation. A new asymmetric similarity measure and a combination approach are described, both of which significantly improve precision. We release three new datasets of lexical vector representations trained on the BNC and our evaluation dataset for hyponym generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Øistein E Andersen</author>
<author>Julien Nioche</author>
<author>Edward J Briscoe</author>
<author>John Carroll</author>
</authors>
<title>The BNC parsed with RASP4UIMA.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation Conference (LREC08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="8184" citStr="Andersen et al. (2008)" startWordPosition="1282" endWordPosition="1286">occur more than once in the dataset, and weight them using pointwise mutual information to construct feature vectors for every term. Features with negative weights were retained, as they proved to be beneficial for some similarity measures. The window-based, dependency-based and word2vec vector sets were all trained on 112M words from the British National Corpus, with preprocessing steps for lowercasing and lemmatising. Any numbers were grouped and substituted by more generic tokens. For constructing the dependency-based vector representations, we used the parsed version of the BNC created by Andersen et al. (2008) with the RASP toolkit (Briscoe et al., 2006). When saved as plain text, the 500- dimensional word2vec vectors and dependencybased vectors are comparable in size (602MB and 549MB), whereas the window-based vectors are twice as large (1,004MB). We make these vector 2https://code.google.com/p/word2vec/ 69 sets publically available for download.3 Recently, Mikolov et al. (2013b) published interesting results about linguistic regularities in vector space models. They proposed that the relationship between two words can be characterised by their vector offset, for example, we could find the vector </context>
</contexts>
<marker>Andersen, Nioche, Briscoe, Carroll, 2008</marker>
<rawString>Øistein E. Andersen, Julien Nioche, Edward J. Briscoe, and John Carroll. 2008. The BNC parsed with RASP4UIMA. In Proceedings of the Sixth International Language Resources and Evaluation Conference (LREC08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="2578" citStr="Baroni and Lenci, 2011" startWordPosition="404" endWordPosition="407">inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Aut</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung-chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>23--32</pages>
<contexts>
<context position="20973" citStr="Baroni et al. (2012)" startWordPosition="3420" endWordPosition="3424">to discard WordNet hypernyms that are very rare in practical use, and would not have enough examples for learning informative vector representations. The final dataset contains the remaining terms, together with all of their hyponyms, including the rare/unseen hyponyms. As expected, some general terms, such as group or location, have a large number of inherited hyponyms. On average, each hypernym in the dataset has 233 hyponyms, but the distribution is roughly exponential, and the median is only 36. In order to better facilitate future experiments with supervised methods, such as described by Baroni et al. (2012), we randomly separated the data into training (1230 hypernyms), validation (922), and test (922) sets, and we make these datasets publically available online.4 7 Experiments We evaluate how well different vector space models and similarity measures perform on the task of hyponym generation. Given a single word as input, the system needs to return a ranked list of words with correct hyponyms at the top. As the list of candidates for scoring we use all words in the BNC that occur at least 10 times (a total of 86,496 words). All the experiments are performed using tokenised and lemmatised words.</context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Ontology learning from text: A survey of methods.</title>
<date>2005</date>
<journal>LDV Forum,</journal>
<volume>20</volume>
<issue>2002</issue>
<contexts>
<context position="3218" citStr="Biemann, 2005" startWordPosition="512" endWordPosition="513">uisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies through clustering (Ushioda, 1996; Bisson et al., 2000; Wagner, 2000; Paaß et al., 2004; Cimiano and Staab, 2005), and is not directly applicable to hyponym generation. While most work has been done on hyponym detection (and the related task of lexical substitution), barely any evaluation has been done for hyponym generation. We have found that systems for hyponym detection often perform poorly on hyponym generation, as the latter requires returnin</context>
</contexts>
<marker>Biemann, 2005</marker>
<rawString>Chris Biemann. 2005. Ontology learning from text: A survey of methods. LDV Forum, 20(2002):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilles Bisson</author>
<author>Claire N´edellec</author>
<author>Dolores Ca˜namero</author>
</authors>
<title>Designing clustering methods for ontology building-The Mo’K workbench.</title>
<date>2000</date>
<booktitle>In ECAI Ontology Learning Workshop.</booktitle>
<marker>Bisson, N´edellec, Ca˜namero, 2000</marker>
<rawString>Gilles Bisson, Claire N´edellec, and Dolores Ca˜namero. 2000. Designing clustering methods for ontology building-The Mo’K workbench. In ECAI Ontology Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, number July,</booktitle>
<pages>77--80</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="8229" citStr="Briscoe et al., 2006" startWordPosition="1291" endWordPosition="1294">t them using pointwise mutual information to construct feature vectors for every term. Features with negative weights were retained, as they proved to be beneficial for some similarity measures. The window-based, dependency-based and word2vec vector sets were all trained on 112M words from the British National Corpus, with preprocessing steps for lowercasing and lemmatising. Any numbers were grouped and substituted by more generic tokens. For constructing the dependency-based vector representations, we used the parsed version of the BNC created by Andersen et al. (2008) with the RASP toolkit (Briscoe et al., 2006). When saved as plain text, the 500- dimensional word2vec vectors and dependencybased vectors are comparable in size (602MB and 549MB), whereas the window-based vectors are twice as large (1,004MB). We make these vector 2https://code.google.com/p/word2vec/ 69 sets publically available for download.3 Recently, Mikolov et al. (2013b) published interesting results about linguistic regularities in vector space models. They proposed that the relationship between two words can be characterised by their vector offset, for example, we could find the vector for word “queen” by performing the operation </context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, number July, pages 77–80, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>120--126</pages>
<contexts>
<context position="2718" citStr="Caraballo, 1999" startWordPosition="429" endWordPosition="430">tionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 120–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Steffen Staab</author>
</authors>
<title>Learning concept hierarchies from text with a guided hierarchical clustering algorithm.</title>
<date>2005</date>
<booktitle>In ICML-Workshop on Learning and Extending Lexical Ontologies by using Machine Learning Methods.</booktitle>
<contexts>
<context position="3479" citStr="Cimiano and Staab, 2005" startWordPosition="549" endWordPosition="552"> from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies through clustering (Ushioda, 1996; Bisson et al., 2000; Wagner, 2000; Paaß et al., 2004; Cimiano and Staab, 2005), and is not directly applicable to hyponym generation. While most work has been done on hyponym detection (and the related task of lexical substitution), barely any evaluation has been done for hyponym generation. We have found that systems for hyponym detection often perform poorly on hyponym generation, as the latter requires returning results from a much less restricted candidate set, 68 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68–77, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics and therefore a task-spe</context>
</contexts>
<marker>Cimiano, Staab, 2005</marker>
<rawString>Philipp Cimiano and Steffen Staab. 2005. Learning concept hierarchies from text with a guided hierarchical clustering algorithm. In ICML-Workshop on Learning and Extending Lexical Ontologies by using Machine Learning Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: an overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, number March,</booktitle>
<pages>112--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11381" citStr="Clarke (2009)" startWordPosition="1795" endWordPosition="1796">is in the role of retrieval results. Precision is then calculated by comparing the intersection (items correctly returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity by taking their geometric average. The APInc measure finds the proportion of shared features relative to the features for the narrower term, but this can lead to unreliable results when the number of features is very small. The motivation behind combining these </context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, number March, pages 112– 119. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="30242" citStr="Cohen, 1995" startWordPosition="4918" endWordPosition="4919">ct terms that do not have a high symmetric similarity will be excluded from evaluation. The BalAPInc measure performed best in that setting, but does not do as well for hyponym generation, where candidates are filtered only based on minimum frequency. The weighted cosine measure, proposed in Section 5, outperformed all other similarity measures on both hyponym generation datasets. The improvement over cosine is relatively small; however, it is consistent and the improvement in MAP is statistically significant on both datasets (p &lt; 0.05), using the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) with 106 iterations. This further supports the properties of a directional similarity measure described in Section 4. Finally, we created a new system by combining together two separate approaches: the weighted cosine measure using the dependency-based vector space, and the normal cosine similarity using word2vec-500 vectors. We found that the former is good at modelling the grammatical roles and directional containment, whereas the latter can provide useful information about the topic and semantics of the word. Turney (2012) also demonstrated the importance of both topical (domain) and funct</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R Cohen. 1995. Empirical Methods for Artificial Intelligence. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>Proceedings of the 25th international conference on Machine learning.</booktitle>
<contexts>
<context position="5596" citStr="Collobert and Weston (2008)" startWordPosition="872" endWordPosition="875"> dependency-based features. 2 Vector space models In order to use similarity measures for hyponym detection, every word needs to be mapped to a point in vector space. The method of choosing appropriate features for these vectors is crucial to achieving the optimal performance. We compare five different approaches: Window: As a simple baseline, we created vectors by counting word co-occurrences in a fixed context window. Every word that occurs within a window of three words before or after is counted as a feature for the target word. Pointwise mutual information is then used for weighting. CW: Collobert and Weston (2008) constructed a neural network language model that is trained to predict the next word in the sequence, and simultaneously learns vector representations for each word. The vectors for context words are concatenated and used as input for the neural network, which uses a sample of possible outputs for gradient calculation to speed up the training process. Turian et al. (2010) recreated their experiments and made the vectors available online.1 HLBL: Mnih and Hinton (2007) created word representations using the hierarchical log-bilinear 1http://metaoptimize.com/projects/wordreprs/ model – a neural </context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of the 25th international conference on Machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From distributional to semantic similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="10170" citStr="Curran, 2003" startWordPosition="1600" endWordPosition="1602">ity measures We compare the performance of a range of similarity measures, both directional and symmetrical, on the task of hyponym generation. Cosine similarity is defined as the angle between two feature vectors and has become a standard measure of similarity between weighted vectors in information retrieval (IR). Lin similarity, created by Lin (1998), uses the ratio of shared feature weights compared to all feature weights. It measures the weighted proportion of features that are shared by both words. DiceGen2 is one possible method for generalising the Dice measure to real-valued weights (Curran, 2003; Grefenstette, 1994). The dot product of the weight vectors is normalised by the total sum of all weights. The same formula can also be considered as a possible generalisation for the Jaccard measure. WeedsPrec and WeedsRec were proposed by Weeds et al. (2004) who suggested using precision and recall as directional measures of word similarity. In this framework, the features are treated similarly to retrieved documents in information retrieval – the vector of the broader term b is used as the gold standard, and the vector of the narrower 3http://www.marekrei.com/projects/vectorsets/ term a is</context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>James R. Curran. 2003. From distributional to semantic similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Similarity-based models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>31--1</pages>
<contexts>
<context position="2227" citStr="Dagan et al., 1999" startWordPosition="345" endWordPosition="349">mple, a user searching for arthritis treatment is most likely also interested in results containing the hyponyms of treatment, such as arthritis therapy, arthritis medication, and arthritis rehabilitation. Summarisation systems can increase coherence and reduce repetition by correctly handling hyponymous words in the input text. Entailment and inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patte</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando C. N. Pereira. 1999. Similarity-based models of word cooccurrence probabilities. Machine Learning, 31:1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="10191" citStr="Grefenstette, 1994" startWordPosition="1603" endWordPosition="1604">e compare the performance of a range of similarity measures, both directional and symmetrical, on the task of hyponym generation. Cosine similarity is defined as the angle between two feature vectors and has become a standard measure of similarity between weighted vectors in information retrieval (IR). Lin similarity, created by Lin (1998), uses the ratio of shared feature weights compared to all feature weights. It measures the weighted proportion of features that are shared by both words. DiceGen2 is one possible method for generalising the Dice measure to real-valued weights (Curran, 2003; Grefenstette, 1994). The dot product of the weight vectors is normalised by the total sum of all weights. The same formula can also be considered as a possible generalisation for the Jaccard measure. WeedsPrec and WeedsRec were proposed by Weeds et al. (2004) who suggested using precision and recall as directional measures of word similarity. In this framework, the features are treated similarly to retrieved documents in information retrieval – the vector of the broader term b is used as the gold standard, and the vector of the narrower 3http://www.marekrei.com/projects/vectorsets/ term a is in the role of retri</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics (COLING ’92), number July,</booktitle>
<pages>539</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2701" citStr="Hearst, 1992" startWordPosition="427" endWordPosition="428">ions. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. Howe</context>
<context position="22960" citStr="Hearst (1992)" startWordPosition="3745" endWordPosition="3746">ing, into a single measure, and is therefore well-suited for comparing different methods. The reported MAP values are very low – this is due to many rare WordNet hyponyms not occurring in the candidate set, for which all systems are automatically penalised. However, this allows us to evaluate recall, making the results comparable between different systems and background datasets. We also report precision at top-1 and top-5 returned hyponyms. As a baseline we report the results of a traditional hyponym acquisition system. For this, we implemented the pattern-based matching process described by Hearst (1992), and also used by Snow et al. (2005). These patterns look for explicit examples of hyponym relations mentioned in the text, for example: X such as {Y1, Y2, ... , (and|or)} Yom, where X will be extracted as the hypernym, and Y1 to Yom, as hyponyms. We ran the patterns over the BNC and extracted 21,704 hyponym pairs, which were then ranked according to the number of times they were found. 7.1 Evaluation of vector spaces Table 2 contains experiments with different vector space models. We report here results using cosine, as it is an established measure and a competitive baseline. For our task, t</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics (COLING ’92), number July, page 539, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>04</issue>
<contexts>
<context position="2553" citStr="Kotlerman et al., 2010" startWordPosition="400" endWordPosition="403">ut text. Entailment and inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes</context>
<context position="11672" citStr="Kotlerman et al. (2010)" startWordPosition="1841" endWordPosition="1844">rrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity by taking their geometric average. The APInc measure finds the proportion of shared features relative to the features for the narrower term, but this can lead to unreliable results when the number of features is very small. The motivation behind combining these measures is that the symmetric Lin measure will decrease the final score for such word pairs, thereby balancing the results. 4 Properties of a directional measure Finding similar words in a vector space, given a symmetric similarity measure, is a relatively straightforward task. However fin</context>
<context position="29442" citStr="Kotlerman et al. (2010)" startWordPosition="4788" endWordPosition="4792">r, biologist, psychologist, economist, observer, physicist, sociologist football, golf, club, tennis, athletics, rugby, cricket, game, recreation, entertainment therapy, medication, patient, procedure, surgery, remedy, regimen, medicine scientist sport treatment Table 4: Examples of top results using the combined system. WordNet hyponyms are marked in bold. is supported by WeedsRec outperforming WeedsPrec, although the opposite was intended by their design. Another explanation for the low performance is that these directional measures are often developed in an artificial context. For example, Kotlerman et al. (2010) evaluated lexical entailment detection on a dataset where the symmetric Lin similarity measure was used to select word pairs for manual annotation. This creates a different task, as correct terms that do not have a high symmetric similarity will be excluded from evaluation. The BalAPInc measure performed best in that setting, but does not do as well for hyponym generation, where candidates are filtered only based on minimum frequency. The weighted cosine measure, proposed in Section 5, outperformed all other similarity measures on both hyponym generation datasets. The improvement over cosine </context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(04):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguisticsVolume 2,</booktitle>
<pages>768--774</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9913" citStr="Lin (1998)" startWordPosition="1558" endWordPosition="1559">lt is compared to hyponym candidates using cosine similarity. For sparse high-dimensional vector space models it was not feasible to use the full offset vector during experiments, therefore we retain only the top 1,000 highest-weighted features. 3 Similarity measures We compare the performance of a range of similarity measures, both directional and symmetrical, on the task of hyponym generation. Cosine similarity is defined as the angle between two feature vectors and has become a standard measure of similarity between weighted vectors in information retrieval (IR). Lin similarity, created by Lin (1998), uses the ratio of shared feature weights compared to all feature weights. It measures the weighted proportion of features that are shared by both words. DiceGen2 is one possible method for generalising the Dice measure to real-valued weights (Curran, 2003; Grefenstette, 1994). The dot product of the weight vectors is normalised by the total sum of all weights. The same formula can also be considered as a possible generalisation for the Jaccard measure. WeedsPrec and WeedsRec were proposed by Weeds et al. (2004) who suggested using precision and recall as directional measures of word similari</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguisticsVolume 2, pages 768–774. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<booktitle>2013a. Efficient Estimation of Word Representations in Vector Space. ICLR Workshop,</booktitle>
<pages>1--12</pages>
<marker>Mikolov, Chen, Corrado, Dean, </marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. ICLR Workshop, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<contexts>
<context position="6821" citStr="Mikolov et al., 2013" startWordPosition="1061" endWordPosition="1064">rk that takes the concatenated vectors of context words as input, and is trained to predict the vector representation of the next word, which is then transformed into a probability distribution over possible words. To speed up training and testing, they use a hierarchical data structure for filtering down the list of candidates. Both CW and HLBL vectors were trained using 37M words from RCV1. Word2vec: We created word representations using the word2vec2 toolkit. The tool is based on a feedforward neural network language model, with modifications to make representation learning more efficient (Mikolov et al., 2013a). We make use of the skip-gram model, which takes each word in a sequence as an input to a log-linear classifier with a continuous projection layer, and predicts words within a certain range before and after the input word. The window size was set to 5 and vectors were trained with both 100 and 500 dimensions. Dependencies: Finally, we created vector representations for words by using dependency relations from a parser as features. Every incoming and outgoing dependency relation is counted as a feature, together with the connected term. For example, given the dependency relation (play, dobj,</context>
<context position="8560" citStr="Mikolov et al. (2013" startWordPosition="1337" endWordPosition="1340">essing steps for lowercasing and lemmatising. Any numbers were grouped and substituted by more generic tokens. For constructing the dependency-based vector representations, we used the parsed version of the BNC created by Andersen et al. (2008) with the RASP toolkit (Briscoe et al., 2006). When saved as plain text, the 500- dimensional word2vec vectors and dependencybased vectors are comparable in size (602MB and 549MB), whereas the window-based vectors are twice as large (1,004MB). We make these vector 2https://code.google.com/p/word2vec/ 69 sets publically available for download.3 Recently, Mikolov et al. (2013b) published interesting results about linguistic regularities in vector space models. They proposed that the relationship between two words can be characterised by their vector offset, for example, we could find the vector for word “queen” by performing the operation “king - man + woman” on corresponding vectors. They also applied this approach to hyponym relations such as (shirt → clothing) and (bowl → dish). We evaluate how well this method applies to hyponym generation with each of the vector space models mentioned above. Using the training data, we learn a vector for the hyponymy relation</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013b. Linguistic Regularities in Continuous Space Word Representations. (June):746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="18864" citStr="Miller, 1995" startWordPosition="3075" endWordPosition="3076"> on the order of the arguments. The parameter C controls the relative importance of the ‘unimportant’ features to the directional relation. Setting it to 0 will ignore these features completely, while setting it to 1 will result in the traditional cosine measure. Experiments on the development data showed that the exact value of this parameter is not very important, as long as it is not too close to the extreme values of 0 or 1. We use the value C = 0.5 for reporting our results, meaning that the non-shared features are half as important, compared to the shared features. 6 Dataset As WordNet (Miller, 1995) contains numerous manually annotated hyponymy relations, we can use it to construct suitable datasets for evaluating hyponym generation. While WordNet terms are annotated with only the closest hyponyms, we are considering all indirect/inherited hyponyms to be relevant – for example, given relations (genomics → genetics) and (genetics → biology), then genomics is also regarded as a hyponym of biology. WordNet relations are defined between synsets, but we refrain from the task of word sense disambiguation and count word a as a valid hyponym for word b if it is valid for any sense of b. Synonymy</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>Proceedings of the 24th international conference on Machine learning - ICML ’07,</booktitle>
<pages>641--648</pages>
<contexts>
<context position="6068" citStr="Mnih and Hinton (2007)" startWordPosition="949" endWordPosition="952"> before or after is counted as a feature for the target word. Pointwise mutual information is then used for weighting. CW: Collobert and Weston (2008) constructed a neural network language model that is trained to predict the next word in the sequence, and simultaneously learns vector representations for each word. The vectors for context words are concatenated and used as input for the neural network, which uses a sample of possible outputs for gradient calculation to speed up the training process. Turian et al. (2010) recreated their experiments and made the vectors available online.1 HLBL: Mnih and Hinton (2007) created word representations using the hierarchical log-bilinear 1http://metaoptimize.com/projects/wordreprs/ model – a neural network that takes the concatenated vectors of context words as input, and is trained to predict the vector representation of the next word, which is then transformed into a probability distribution over possible words. To speed up training and testing, they use a hierarchical data structure for filtering down the list of candidates. Both CW and HLBL vectors were trained using 37M words from RCV1. Word2vec: We created word representations using the word2vec2 toolkit. </context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. Proceedings of the 24th international conference on Machine learning - ICML ’07, pages 641–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer Intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="30228" citStr="Noreen, 1989" startWordPosition="4916" endWordPosition="4917">task, as correct terms that do not have a high symmetric similarity will be excluded from evaluation. The BalAPInc measure performed best in that setting, but does not do as well for hyponym generation, where candidates are filtered only based on minimum frequency. The weighted cosine measure, proposed in Section 5, outperformed all other similarity measures on both hyponym generation datasets. The improvement over cosine is relatively small; however, it is consistent and the improvement in MAP is statistically significant on both datasets (p &lt; 0.05), using the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) with 106 iterations. This further supports the properties of a directional similarity measure described in Section 4. Finally, we created a new system by combining together two separate approaches: the weighted cosine measure using the dependency-based vector space, and the normal cosine similarity using word2vec-500 vectors. We found that the former is good at modelling the grammatical roles and directional containment, whereas the latter can provide useful information about the topic and semantics of the word. Turney (2012) also demonstrated the importance of both topical (dom</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer Intensive Methods for Testing Hypotheses: An Introduction. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Paaß</author>
<author>J¨org Kindermann</author>
<author>Edda Leopold</author>
</authors>
<title>Learning prototype ontologies by hierachical latent semantic analysis.</title>
<date>2004</date>
<contexts>
<context position="3453" citStr="Paaß et al., 2004" startWordPosition="545" endWordPosition="548">xtracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies through clustering (Ushioda, 1996; Bisson et al., 2000; Wagner, 2000; Paaß et al., 2004; Cimiano and Staab, 2005), and is not directly applicable to hyponym generation. While most work has been done on hyponym detection (and the related task of lexical substitution), barely any evaluation has been done for hyponym generation. We have found that systems for hyponym detection often perform poorly on hyponym generation, as the latter requires returning results from a much less restricted candidate set, 68 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68–77, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistic</context>
</contexts>
<marker>Paaß, Kindermann, Leopold, 2004</marker>
<rawString>Gerhard Paaß, J¨org Kindermann, and Edda Leopold. 2004. Learning prototype ontologies by hierachical latent semantic analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL.</booktitle>
<contexts>
<context position="2749" citStr="Pantel and Ravichandran, 2004" startWordPosition="431" endWordPosition="434">words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Proceedings of HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2769" citStr="Snow et al., 2005" startWordPosition="435" endWordPosition="438">ing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies thr</context>
<context position="22997" citStr="Snow et al. (2005)" startWordPosition="3751" endWordPosition="3754">is therefore well-suited for comparing different methods. The reported MAP values are very low – this is due to many rare WordNet hyponyms not occurring in the candidate set, for which all systems are automatically penalised. However, this allows us to evaluate recall, making the results comparable between different systems and background datasets. We also report precision at top-1 and top-5 returned hyponyms. As a baseline we report the results of a traditional hyponym acquisition system. For this, we implemented the pattern-based matching process described by Hearst (1992), and also used by Snow et al. (2005). These patterns look for explicit examples of hyponym relations mentioned in the text, for example: X such as {Y1, Y2, ... , (and|or)} Yom, where X will be extracted as the hypernym, and Y1 to Yom, as hyponyms. We ran the patterns over the BNC and extracted 21,704 hyponym pairs, which were then ranked according to the number of times they were found. 7.1 Evaluation of vector spaces Table 2 contains experiments with different vector space models. We report here results using cosine, as it is an established measure and a competitive baseline. For our task, the HLBL vectors perform better than C</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING ’08),</booktitle>
<pages>849--856</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11119" citStr="Szpektor and Dagan (2008)" startWordPosition="1752" endWordPosition="1755">es of word similarity. In this framework, the features are treated similarly to retrieved documents in information retrieval – the vector of the broader term b is used as the gold standard, and the vector of the narrower 3http://www.marekrei.com/projects/vectorsets/ term a is in the role of retrieval results. Precision is then calculated by comparing the intersection (items correctly returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are covered by the narrower term. Balprec is a measure created by Szpektor and Dagan (2008). They proposed combining WeedsPrec together with the Lin measure by taking their geometric average. This aims to balance the WeedsPrec score, as the Lin measure will penalise cases where one vector contains very few features. ClarkeDE, proposed by Clarke (2009), is an asymmetric degree of entailment measure, based on the concept of distributional generality (Weeds et al., 2004). It quantifies the weighted coverage of the features of the narrower term a by the features of the broader term b. BalAPInc, a measure described by Kotlerman et al. (2010), combines the APInc score with Lin similarity </context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING ’08), pages 849–856, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5971" citStr="Turian et al. (2010)" startWordPosition="935" endWordPosition="938">co-occurrences in a fixed context window. Every word that occurs within a window of three words before or after is counted as a feature for the target word. Pointwise mutual information is then used for weighting. CW: Collobert and Weston (2008) constructed a neural network language model that is trained to predict the next word in the sequence, and simultaneously learns vector representations for each word. The vectors for context words are concatenated and used as input for the neural network, which uses a sample of possible outputs for gradient calculation to speed up the training process. Turian et al. (2010) recreated their experiments and made the vectors available online.1 HLBL: Mnih and Hinton (2007) created word representations using the hierarchical log-bilinear 1http://metaoptimize.com/projects/wordreprs/ model – a neural network that takes the concatenated vectors of context words as input, and is trained to predict the vector representation of the next word, which is then transformed into a probability distribution over possible words. To speed up training and testing, they use a hierarchical data structure for filtering down the list of candidates. Both CW and HLBL vectors were trained u</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Y Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="30774" citStr="Turney (2012)" startWordPosition="5000" endWordPosition="5001"> (p &lt; 0.05), using the Approximate Randomisation Test (Noreen, 1989; Cohen, 1995) with 106 iterations. This further supports the properties of a directional similarity measure described in Section 4. Finally, we created a new system by combining together two separate approaches: the weighted cosine measure using the dependency-based vector space, and the normal cosine similarity using word2vec-500 vectors. We found that the former is good at modelling the grammatical roles and directional containment, whereas the latter can provide useful information about the topic and semantics of the word. Turney (2012) also demonstrated the importance of both topical (domain) and functional vector space models when working with semantic relations. We combined these approaches by calculating both scores for each word pair and taking their geometric average, or 0 if it could not be calculated. This final system gives considerable improvements across all evaluation metrics, and is significantly (p &lt; 0.05) better compared to cosine or weighted cosine methods individually. Table 4 contains some example output from this system. 8 Conclusion Hyponym generation has a wide range of possible applications in NLP, such</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter D. Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Ushioda</author>
</authors>
<title>Hierarchical clustering of words and application to NLP tasks.</title>
<date>1996</date>
<booktitle>In Fourth Workshop on Very Large Corpora,</booktitle>
<pages>28--41</pages>
<contexts>
<context position="3399" citStr="Ushioda, 1996" startWordPosition="537" endWordPosition="538">ten make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies through clustering (Ushioda, 1996; Bisson et al., 2000; Wagner, 2000; Paaß et al., 2004; Cimiano and Staab, 2005), and is not directly applicable to hyponym generation. While most work has been done on hyponym detection (and the related task of lexical substitution), barely any evaluation has been done for hyponym generation. We have found that systems for hyponym detection often perform poorly on hyponym generation, as the latter requires returning results from a much less restricted candidate set, 68 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68–77, Baltimore, Maryland USA, June 26-27</context>
</contexts>
<marker>Ushioda, 1996</marker>
<rawString>Akira Ushioda. 1996. Hierarchical clustering of words and application to NLP tasks. In Fourth Workshop on Very Large Corpora, pages 28–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Wagner</author>
</authors>
<title>Enriching a lexical semantic net with selectional preferences by means of statistical corpus analysis.</title>
<date>2000</date>
<booktitle>In ECAI Workshop on Ontology Learning.</booktitle>
<contexts>
<context position="3434" citStr="Wagner, 2000" startWordPosition="543" endWordPosition="544">patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of appropriate substitutes for a specific term. Automated ontology creation (Biemann, 2005) is a related field that also makes use of distributional similarity measures. However, it is mostly focused on building prototype-based ontologies through clustering (Ushioda, 1996; Bisson et al., 2000; Wagner, 2000; Paaß et al., 2004; Cimiano and Staab, 2005), and is not directly applicable to hyponym generation. While most work has been done on hyponym detection (and the related task of lexical substitution), barely any evaluation has been done for hyponym generation. We have found that systems for hyponym detection often perform poorly on hyponym generation, as the latter requires returning results from a much less restricted candidate set, 68 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 68–77, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Compu</context>
</contexts>
<marker>Wagner, 2000</marker>
<rawString>Andreas Wagner. 2000. Enriching a lexical semantic net with selectional preferences by means of statistical corpus analysis. In ECAI Workshop on Ontology Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics.</title>
<date>2005</date>
<contexts>
<context position="2250" citStr="Weeds and Weir, 2005" startWordPosition="350" endWordPosition="353">ng for arthritis treatment is most likely also interested in results containing the hyponyms of treatment, such as arthritis therapy, arthritis medication, and arthritis rehabilitation. Summarisation systems can increase coherence and reduce repetition by correctly handling hyponymous words in the input text. Entailment and inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting rela</context>
</contexts>
<marker>Weeds, Weir, 2005</marker>
<rawString>Julie Weeds and David Weir. 2005. Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th international conference on Computational Linguistics - COLING ’04.</booktitle>
<contexts>
<context position="10431" citStr="Weeds et al. (2004)" startWordPosition="1643" endWordPosition="1646">ilarity between weighted vectors in information retrieval (IR). Lin similarity, created by Lin (1998), uses the ratio of shared feature weights compared to all feature weights. It measures the weighted proportion of features that are shared by both words. DiceGen2 is one possible method for generalising the Dice measure to real-valued weights (Curran, 2003; Grefenstette, 1994). The dot product of the weight vectors is normalised by the total sum of all weights. The same formula can also be considered as a possible generalisation for the Jaccard measure. WeedsPrec and WeedsRec were proposed by Weeds et al. (2004) who suggested using precision and recall as directional measures of word similarity. In this framework, the features are treated similarly to retrieved documents in information retrieval – the vector of the broader term b is used as the gold standard, and the vector of the narrower 3http://www.marekrei.com/projects/vectorsets/ term a is in the role of retrieval results. Precision is then calculated by comparing the intersection (items correctly returned) to the values of the narrower term only (all items returned). In contrast, WeedsRec quantifies how well the features of the breader term are</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. Proceedings of the 20th international conference on Computational Linguistics - COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Bootstrapping Distributional Feature Vector Quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="2529" citStr="Zhitomirsky-Geffet and Dagan, 2009" startWordPosition="396" endWordPosition="399">handling hyponymous words in the input text. Entailment and inference systems can improve sentence-level entailment resolution by detecting the presence and direction of wordlevel hyponymy relations. Distributionally similar words have been used for smoothing language models and word co-occurrence probabilities (Dagan et al., 1999; Weeds and Weir, 2005), and hyponyms can be more suitable for this application. We distinguish between three different tasks related to hyponyms. Given a directional word pair, the goal of hyponym detection is to determine whether one word is a hyponym of the other (Zhitomirsky-Geffet and Dagan, 2009; Kotlerman et al., 2010; Baroni and Lenci, 2011). In contrast, hyponym acquisition is the task of extracting all possible hyponym relations from a given text (Hearst, 1992; Caraballo, 1999; Pantel and Ravichandran, 2004; Snow et al., 2005). Such systems often make use of heuristic rules and patterns for extracting relations from surface text, and populate a database with hyponymous word pairs. Finally, the task of hyponym generation is to return a list of all possible hyponyms, given only a single word as input. This is most relevant to practical applications, as many systems require a set of</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping Distributional Feature Vector Quality. Computational Linguistics, 35(3):435–461, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
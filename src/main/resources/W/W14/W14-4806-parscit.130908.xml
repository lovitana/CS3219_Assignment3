<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007026">
<title confidence="0.9982165">
Bilingual Termbank Creation via Log-Likelihood Comparison and
Phrase-Based Statistical Machine Translation
</title>
<author confidence="0.929843">
Rejwanul Haque, Sergio Penkale, Andy Way†
</author>
<affiliation confidence="0.556606">
Lingo24, Edinburgh, UK
</affiliation>
<email confidence="0.953451">
{rejwanul.haque, sergio.penkale}@lingo24.com
</email>
<affiliation confidence="0.961493">
†CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
</affiliation>
<address confidence="0.858198">
Dublin 9, Ireland
</address>
<email confidence="0.997795">
away@computing.dcu.ie
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934333333333">
Bilingual termbanks are important for many natural language processing (NLP) applications, es-
pecially in translation workflows in industrial settings. In this paper, we apply a log-likelihood
comparison method to extract monolingual terminology from the source and target sides of a
parallel corpus. Then, using a Phrase-Based Statistical Machine Translation model, we create a
bilingual terminology with the extracted monolingual term lists. We manually evaluate our novel
terminology extraction model on English-to-Spanish and English-to-Hindi data sets, and observe
excellent performance for all domains. Furthermore, we report the performance of our monolin-
gual terminology extraction model comparing with a number of the state-of-the-art terminology
extraction models on the English-to-Hindi datasets.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993085625">
Terminology plays an important role in various NLP tasks including Machine Translation (MT) and
Information Retrieval. It is also exploited in human translation workflows, where it plays a key role
in ensuring translation consistency and reducing ambiguity across large translation projects involving
multiple files and translators over a long period of time. The creation of monolingual and bilingual
terminological resources using human experts are, however, expensive and time-consuming tasks. In
contrast, automatic terminology extraction is much faster and less expensive, but cannot be guaranteed
to be error-free. Accordingly, in real NLP applications, a manual inspection is required to amend or
discard anomalous items from an automatically extracted terminology list.
The automatic terminology extraction task starts with selecting candidate terms from the input domain
corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are
regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word
sequences are regarded as candidate terms (Deane, 2005).
Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et
al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and
TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which
is composed of two consecutive and independent processes:
</bodyText>
<listItem confidence="0.99623575">
1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences)
independently from the source and target sides of a parallel corpus,
2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based
Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003).
</listItem>
<bodyText confidence="0.8645224">
We then evaluate our novel bilingual terminology extraction model on various domain corpora consid-
ering English-to-Spanish and low-resourced and less-explored English-to-Hindi language-pairs and see
excellent performance for all data sets.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.987042">
42
</page>
<note confidence="0.9847055">
Proceedings of the 4th International Workshop on Computational Terminology, pages 42–51,
Dublin, Ireland, August 23 2014.
</note>
<bodyText confidence="0.998830333333333">
The remainder of the paper is organized as follows. In Section 2, we discuss related work. In Section
3, we describe our two-stage terminology extraction model. Section 4 presents the results and analyses
of our experiments, while Section 5 concludes, and provides avenues for further work.
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999567">
Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can
be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches
dominate this field, with some of the leading work including the use of frequency-based filtering (Daille
et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Gar-
side, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm
(Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007).
In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006)
demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, sim-
ilarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both
sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the
most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998),
Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignment-
based terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike
our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might
be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied
statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the lat-
ter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In
contrast, we build a PB-SMT model (Koehn et al., 2003) from the input parallel corpus, which we use
to align a source term to one or more target terms. While Rayson and Garside (2000) and Gelbukh et al.
(2010) only allowed the extraction of single-word terms, we focus on extraction of up to 3-gram terms.
</bodyText>
<sectionHeader confidence="0.998152" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99948175">
In this section, we describe our two-stage bilingual terminology extraction model. In the first stage, we
extract monolingual terms independently from either side of a sentence-aligned domain-specific parallel
corpus. In the second stage, the extracted source terms are aligned to one or more extracted target terms
using a PB-SMT model.
</bodyText>
<subsectionHeader confidence="0.999432">
3.1 Monolingual Terminology Extraction
</subsectionHeader>
<bodyText confidence="0.999834111111111">
The monolingual term extraction task involves the identification of terms from a list of candidate terms
formed from all n-gram word sequences from the monolingual domain corpus (i.e. in our case, each side
of the domain parallel corpus, cf. Section 4.1). On both source and target sides, we used lists of language-
specific stop-words and punctuation marks in order to filter out anomalous items from the candidate
termlists. In order to rank the candidate terms in those lists, we used a log-likelihood comparison method
that compares the frequencies of each candidate term in both the domain corpus and the large general
corpus used as a reference.1
The log-likelihood (LL) value of a candidate term (Cn) is calculated using equation (1) from Gelbukh
et al. (2010).
</bodyText>
<equation confidence="0.99426">
LL = 2 * ((Fd * log(Fd/Ed)) + (Fg * log(Fg/Eg))) (1)
</equation>
<bodyText confidence="0.999826">
where Fd and Fg are the frequencies of Cn in the domain corpus and the generic reference corpus,
respectively. Ed and Eg are the expected frequencies of Cn, which are calculated using (2) and (3).
</bodyText>
<equation confidence="0.9971165">
Ed = Nnd * (Fd + Fg)/(Nnd + Nng) (2)
Eg = Nng * (Fd + Fg)/(Nnd + Nng) (3)
</equation>
<footnote confidence="0.99809">
1Before the term-extraction process begins, we apply a number of preprocessing methods including tokenisation to the input
domain corpus and the generic reference corpus.
</footnote>
<page confidence="0.999868">
43
</page>
<bodyText confidence="0.999946833333333">
where Nnd and Nng are the numbers of n-grams in the domain corpus and reference corpus, respectively.
Thus, each candidate term is associated with a weight (LL value) which is used to sort the candidate
terms: those candidates with the highest weights have the most significant differences in frequency in the
two corpora. However, we are interested in those candidate terms that are likely to be terms in the domain
corpus. Gelbukh et al. (2010) used the condition in (4) in order to filter out those candidate terms whose
relative frequencies are bigger in the domain corpus than in the reference corpus, and we do likewise.
</bodyText>
<equation confidence="0.996772">
Fd/Nnd &gt; Fg/Nn (4)
g
</equation>
<bodyText confidence="0.999026">
In contrast with Gelbukh et al. (2010), we extract multi-word terms up to 3-grams, whereas they focused
solely on extracting single word terms.
</bodyText>
<subsectionHeader confidence="0.999981">
3.2 Creating a Bilingual Termbank
</subsectionHeader>
<bodyText confidence="0.998757125">
We obtained source and target termlists from the bilingual domain corpus using the approach described
in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the
extracted source and target termlists.
This section provides a mathematical derivation of the PB-SMT model to show how we scored can-
didate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilin-
gual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability
P(eI1|fJ1 ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually
comprise M translational features, and the language model, as in (5):
</bodyText>
<equation confidence="0.996799">
log P(eI1|fJ1 ) = XM λmhm(fJ1 , eI1, sK1 ) + λLMlog P(eI1) (5)
m=1
</equation>
<bodyText confidence="0.997685333333333">
where eI1 = e1, ..., eI is the probable candidate translation for the given input sentence fJ1 = f1, ..., fJ
and sK1 = s1, ..., sk denotes a segmentation of the source and target sentences respectively into the se-
quences of phrases (ˆf1,..., ˆfk) and (ˆe1, ..., ˆek) such that (we set i0 := 0):
</bodyText>
<equation confidence="0.956266">
∀k E [1, K] sk := (ik; bk, jk), (bk corresponds to starting index of fk)
ˆfk := ˆfbk, ..., ˆfjk
Each feature hm in (5) can be rewritten as in (6):
hm(fJ1 , eI1, sK1 ) = XK ˆhm(ˆfk, ˆek, sk) (6)
k=1
</equation>
<bodyText confidence="0.848897">
Therefore, the translational features in (5) can be rewritten as in (7):
</bodyText>
<equation confidence="0.980305">
eik,
ˆek := ˆ ˆ
eik−1+1, ...,
M λmhm(fJ1 , eI1, sK1 ) = XM λm XK ˆhm(ˆfk, ˆek, sk) (7)
X m=1 k=1
m=1
</equation>
<bodyText confidence="0.999495">
In equation (7), ˆhm is a feature defined on phrase-pairs (ˆfk, ˆek), and λm is the feature weight of ˆhm.
These weights (λm) are optimized using minimum error-rate training (MERT) (Och, 2003) on a held-out
500 sentence-pair development set for each of the experiments.
We create a list of probable source–target term-pairs by taking each source and target term from the
source and target termlists, respectively, provided that those source–target term-pairs are present in the
PB-SMT phrase-table. We calculate a weight (w) for each source–target term-pair (essentially, a phrase-
pair, i.e. (ˆek, ˆfk)) using (8):2
</bodyText>
<equation confidence="0.868703666666667">
w(ˆek, ˆfk) = XM λmˆhm(ˆfk, ˆek) (8)
m=1
2Equation (8) is derived from the right-hand side of equation (7) for a single source–target phrase-pair.
</equation>
<page confidence="0.975823">
44
</page>
<bodyText confidence="0.9998578">
In order to calculate w, we used the four standard PB-SMT translational features (ˆhm), namely forward
phrase translation log-probability (log P(ˆek |ˆfk)), its inverse (log P( ˆfk|ˆek)), the lexical log-probability
(log Plex(ˆek |ˆfk)), and its inverse (log Plex(ˆfk|ˆek)). We considered a higher threshold value for weights
and considered those term-pairs whose weights exceeded this threshold. For each source term, we con-
sidered a maximum of the four highest-weighted target terms.
</bodyText>
<table confidence="0.994892473684211">
Domain Parallel Corpus
Domain Sentences Words (English)
English-to-Spanish
Banking, Finance and Economics 50,112 548,594
Engineering 91,896 1,165,384
IT 33,148 367,046
Tourism and Travel 50,042 723,088
Science 79,858 1,910,482
Arts and Culture 9,124 100,620
English-to-Hindi
EILMT 7,096 173,770
EMILLE 9,907 159,024
Launchpad 67,663 380,546
KDE4 84,089 324,289
Reference Corpus
Language Sentences Words
English 4,000,000 82,048,154
Spanish 4,132,386 128,005,190
Hindi 10,000,000 182,066,982
</table>
<tableCaption confidence="0.999275">
Table 1: Corpus Statistics.
</tableCaption>
<sectionHeader confidence="0.989445" genericHeader="evaluation">
4 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.996828">
4.1 Data Used
</subsectionHeader>
<bodyText confidence="0.999955">
We conducted experiments on several data domains for two different language-pairs, English-to-Spanish
and English-to-Hindi. For English-to-Spanish, we worked with client-provided data taken from six dif-
ferent domains in the form of translation memories. For English-to-Hindi, we used three parallel corpora
from three different sources (EILMT, EMILLE and Launchpad) taken from HindEnCorp3 (Bojar et al.,
2014) released for the WMT14 shared translation task,4 and a parallel corpus of KDE4 localization files5
(Tiedemann, 2009). The EMILLE corpus contains leaflets from the UK Government and various local
authorities. The domain of the EILMT6 corpus is tourism.
We used data from a collection of translated documents from the United Nations (MultiUN)7 (Tiede-
mann, 2009) and the European Parliament (Koehn et al., 2005) as the monolingual English and Spanish
reference corpora. We used the HindEnCorp monolingual corpus (Bojar et al., 2014) as the monolingual
Hindi reference corpus. The statistics of the data used in our experiments are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.992049">
4.2 Runtime Performance
</subsectionHeader>
<bodyText confidence="0.986352333333333">
Our terminology extraction model is composed of two main processes: (i) Moses training and tuning
(restricting the number of iterations of MERT to a maximum of 6), and (ii) terminology extraction. In
Table 2, we report the actual runtimes of these two processes on the six domain corpora. As Table
</bodyText>
<footnote confidence="0.9997132">
3http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4http://www.statmt.org/wmt14/
5http://opus.lingfil.uu.se/KDE4.php
6English-to-Indian Language Machine Translation (EILMT) is a Ministry of IT, Govt. of India sponsored project.
7http://opus.lingfil.uu.se/MultiUN.php
</footnote>
<page confidence="0.998908">
45
</page>
<bodyText confidence="0.9993635">
2 demonstrates, both MT system-building (training and tuning combined) and terminology extraction
processes are very short on each corpus. Given the crucial influence of bilingual terminology on quality
in translation workflows, we believe that the creation of such assets from scratch in less than 30 minutes
may prove to be a significant breakthrough for translators.
</bodyText>
<table confidence="0.999712857142857">
MT System Terminology
Building Extraction
English-to-Spanish
Banking, Finance and Economics 05:49 04:23
Engineering 06:47 04:33
IT 04:10 04:31
Tourism and Travel 05:34 04:24
Science 15:26 04:52
Arts and Culture 03:20 04:16
English-to-Hindi
EILMT 12:41 15:47
EMILLE 05:41 17.18
Launchpad 04:37 24.11
KDE4 04:05 16:50
</table>
<tableCaption confidence="0.938509">
Table 2: Runtimes (minutes:seconds) for MT system-building and bilingual terminology extraction on
the different domain data sets.
</tableCaption>
<subsectionHeader confidence="0.998299">
4.3 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.999885851851852">
Of course, it is one thing to rapidly create translation assets such as bilingual termbanks, and another en-
tirely to ensure the quality of such resources. Accordingly, we evaluated the performance of our bilingual
terminology extraction model on each English-to-Spanish and English-to-Hindi domain corpus reported
in Table 1, with the evaluation goals being twofold: (i) measuring the accuracy of the monolingual ter-
minology extraction process, and (ii) measuring the accuracy of our novel bilingual terminology creation
model.
As mentioned in Section 3.2, a source term may be aligned with up to four target terms. For evaluation
purposes, we considered the top-100 source terms based on the LL values (cf. (1)) and their target coun-
terparts (i.e. one to four target terms). The quality of the extracted terms was judged by native Spanish
and Hindi speakers, both with excellent English skills, and the evaluation results are reported in Table
3. Note that we were not able to measure recall of the term extraction model on the domain corpora due
to the unavailability of a reference terminology set. The evaluator counted the number of valid terms in
the source term list for the domain in question, and the percentage of valid terms with respect to the total
number of terms (i.e. 100) is reported in the second column in Table 3. We refer to this as VST (Valid
Source Terms). For each valid source term there are one to four target terms that are ranked according to
the weights in (8). In theory, therefore, the top-ranked target term is the most suitable target translation of
the aligned source term. The evaluator counted the number of instances where the top-ranked target term
was a suitable target translation of the source term; the percentage with respect to the number of valid
source terms is shown in the third column in Table 3, and denoted as VTT (Valid Target Terms). The
evaluator also reported the number of cases where any of the four target terms was a suitable translation
of the source term; the percentage with respect to the number of valid source terms is given in the fourth
column in Table 3. Furthermore, the evaluator counted the number of instances where any of the four
target terms with minor editing can be regarded as suitable target translation; the percentage with respect
to the number of valid source terms is reported in the last column of Table 3. In Table 4, we show three
English–Spanish term-pairs extracted by our automatic term extractor where the target terms (Spanish)
are slightly incorrect. In all these examples the edit distance between the correct term and the one pro-
posed by our automatic extraction method is quite low, meaning that just a few keystrokes can transform
</bodyText>
<page confidence="0.998566">
46
</page>
<bodyText confidence="0.9546685">
the candidate term into the correct one. In these cases editing the candidate term is much cheaper (in
terms of time) than creating the translations from scratch.
</bodyText>
<table confidence="0.999849571428571">
VST VTT1 VTT4 VTTME4
(%) (%) (%) (%)
English-to-Spanish
Banking, Finance and Economics 76 92.1 93.4 94.7
Engineering 84 90.5 91.7 94.1
IT 89 90.0 97.8 97.8
Tourism and Travel 72 86.1 93.1 93.1
Science 94 93.6 93.6 93.6
Arts and Culture 89 91.9 96.5 96.5
English-to-Hindi
EILMT 91 81.3 83.5 96.7
EMILLE 79 62.1 83.5 98.7
Launchpad 88 95.4 98.8 98.8
KDE4 79 88.6 89.8 94.9
</table>
<tableCaption confidence="0.996554">
Table 3: Manual evaluation results obtained on the top-100 term pairs. VST: Valid Source Terms, VTT1:
</tableCaption>
<figure confidence="0.788138454545455">
Valid Target Terms (1-best), VTT4: Valid Target Terms (4-best), VTTME4: Valid Target Terms with
Minor Editing (4-best).
Source Terms Target Terms Target Terms Edit
(using Bilingual Term Extractor) corrected with Minor Editing Distance
Shutter Obturaci´on Obturador 5
comment: wrong choice of inflection is likely caused by the term being most frequently used as
‘shutter speed’
Lenses Objetivos EF Objetivos 3
comment: The quali�er ‘EF’ should not be present in the target, as it is not in the source
Leave Cancel Cancelaci´on Vacaciones Cancelaci´on de Vacaciones 3
comment: The preposition ‘de’ is missing in the target term
</figure>
<tableCaption confidence="0.985037">
Table 4: Slightly wrong target terms corrected with minor editing.
</tableCaption>
<bodyText confidence="0.99978575">
In Table 3, we see that the accuracy of the monolingual term extraction model varies from 72% to 94%
for both English-to-Spanish and English-to-Hindi. For English-to-Spanish, the accuracy of our bilingual
terminology creation model ranges from 86.1% to 93.6%, 91.7% to 97.8% and 93.1% to 97.8% when
the 1-best, 4-best and 4-best with slightly edited target terms are considered, respectively. For English-
to-Hindi, the accuracy of our bilingual terminology creation model ranges from 62.1% to 95.4%, 83.5%
to 98.8% and 94.9% to 98.8% when the 1-best, 4-best and 4-best with slightly edited target terms are
considered, respectively.
We are greatly encouraged by these results, as they demonstrate that our novel bilingual termbank
creation method is robust in the face of the somewhat noisy monolingual term-extraction results; as a
consequence, if better methods for suggesting monolingual term candidates are proposed, we expect the
performance of our bilingual term-creation model to improve accordingly.
We calculated the distributions of unigram, bigram and trigram in the valid source terms (cf. Table 3)
and reported in Table 5. We also calculated the percentages of their distributions in the valid source terms
averaged over all 10 data sets. As can be seen from Table 3, the percentage of the average distribution of
the trigram terms is quite low (i.e. 2.5%). This result justifies our decision for extraction of up to 3-gram
terms.
</bodyText>
<page confidence="0.998218">
47
</page>
<table confidence="0.999787142857143">
Unigram Bigram Trigram
English-to-Spanish
Banking, Finance and Economics 55 20 1
Engineering 64 18 2
IT 75 12 2
Tourism and Travel 49 18 5
Science 91 3 0
Arts and Culture 76 10 3
English-to-Hindi
EILMT 73 17 1
EMILLE 35 37 7
Launchpad 85 3 0
KDE4 74 5 0
Average 80.4% 17.0 % 2.5%
</table>
<tableCaption confidence="0.86731">
Table 5: Distributions of unigram, bigram and trigram in the valid source term pairs (cf. second column
in Table 3).
</tableCaption>
<subsectionHeader confidence="0.837254">
4.4 Comparison: Monolingual Terminology Extraction
</subsectionHeader>
<bodyText confidence="0.998258">
In this section we report the performance of our monolingual terminology extraction model (cf. Section
3.1) comparing with the performance of several state-of-the-art terminology extraction algorithms capa-
ble of recognising multiword terms. In order to extract monolingual multiword terms we used the JATE
toolkit8 (Zhang et al., 2008). This toolkit first extracts candidate terms from a corpus using linguistic
tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The
JATE toolkit is currently available only for the English language. For evaluation purposes, we considered
the source-side of the English-to-Hindi domain corpora.
</bodyText>
<table confidence="0.998317615384615">
Algorithm Reference EILMT EMILLE Launchpad KDE4
LLC (Bilingual) cf. VST in Table 3 91 79 88 79
LLC 77 53 80 71
STF 46 04 54 44
ACTF 42 15 62 48
TF-IDF 50 36 45 17
Glossex Kozakov et al. (2004) 76 43 76 71
JK Justeson &amp; Katz (1995) 42 13 58 42
NC-Value Frantzi et al. (2000) 46 34 52 25
RIDF Church &amp; Gale (1995) 27 16 23 21
TermEx Sclano et al. (2007) 42 08 46 41
C-Value Ananiadou (1994) 49 44 62 40
Weirdness Ahmed et al. (1999) 77 57 82 63
</table>
<tableCaption confidence="0.97009">
Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Fre-
quency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz
</tableCaption>
<bodyText confidence="0.999801">
For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)).
The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the
candidate terms giving an indication of the likelihood of being a good domain-specific term. The quality
of the extracted terms (top-100 highest weighted) was judged by an evaluator with excellent English
skills, and the evaluation results are reported in Table 6. The evaluator counted the number of valid terms
</bodyText>
<footnote confidence="0.980799">
8https://code.google.com/p/jatetoolkit/
</footnote>
<page confidence="0.999101">
48
</page>
<bodyText confidence="0.999949407407407">
in the highest weighted 100 terms that were extracted using different state-of-the-art term extraction
algorithms.
The third row of Table 6 represents the percentage of the valid source terms extracted by our log-
likelihood comparison (LLC) based monolingual term extraction algorithm. The next three rows rep-
resent three basic monolingual term extraction algorithms (STF: simple term frequency, ACTF: aver-
age corpus term frequency and TF-IDF) available in the JATE toolkit. The last seven rows represent
seven state-of-the-art terminology extraction algorithms. As can be seen from Table 6, LLC is the best-
performing algorithm with the Weirdness (Ahmad et al., 1999) and the Glossex (Kozakov et al., 2004)
algorithms on the EILMT and the KDE4 corpora, respectively. The LLC is also the second-best per-
forming algorithm on the EMILLE and the Lauchpad corpora.
We see in Table 6 that the percentage of valid source terms is quite low on the EMILLE corpus.
This might be caused by it containing information leaflets in a variety of domains (consumer, education,
housing, health, legal, social), which might bring down the percentage of valid source terms on this
corpus.
Note that the percentage of valid source terms (VST) reported in Table 3 is calculated taking the
top-100 source terms from the bilingual term-pair list that were extracted using the method described in
Section 3.2. For comparison purposes we again report this percentage (VST in Table 3) in the second row
in Table 6. Our bilingual term extraction method discards any anomalous pairs from the initial candidate
term-pair list (cf. Section 3.2). This essentially removes some of the source entries that are not pertinent
to the domain. As a result, the percentage of the valid source terms extracted applying our bilingual
terminology extraction method (Table 3) is higher than the percentage of the valid source terms extracted
applying our monolingual terminology extraction algorithm (LLC) (Table 6). We clearly see from Tables
3 and 6 that this bilingual approach to term extraction not only achieves remarkable performance on the
bilingual task, but that when used in a monolingual context it outperforms most state-of-the-art extraction
algorithms, and is comparable with the best ones. We should also note that JATE’s implementation of
these algorithms (including Weirdness) uses language-dependent modules such as a lemmatizer, unlike
our implementation of LLC which is language-independent.
</bodyText>
<sectionHeader confidence="0.998183" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999262818181818">
In this paper we presented a bilingual multi-word terminology extraction model based on two inde-
pendent consecutive processes. Firstly, we employed a log-likelihood comparison method to extract
source and target terms independently from both sides of a parallel domain corpus. Secondly, we used
a PB-SMT model to align source terms to one or more target terms. The manual evaluation results
on ten different domain corpora of two syntactically divergent language-pairs showed the accuracy of
our bilingual terminology extraction model to be very high, especially in the light of the rather noisier
monolingual candidate terms presented to it. Given the reported high levels of performance – minimum
levels of 93.1% and 94.9% in the 4-best set-up across all six domains for English-to-Spanish and all four
domains for English-to-Hindi, respectively – we are convinced that the extracted bilingual multiword
termbanks are useful ‘as is’, and with a small amount of post-processing from domain experts would be
completely error-free.
The proposed bilingual terminology extraction model has been tested on a highly investigated
language-pair, English-to-Spanish, and a less-explored and low-resourced English-to-Indic language-
pair, English-to-Hindi. Interestingly, the performance of the bilingual terminology extraction model
is excellent for the both language-pairs. We also tested several state-of-the-art monolingual terminol-
ogy extraction algorithms including our own (log-likelihood comparison) on the source-side of the four
English-to-Hindi domain data sets. According to the manual evaluation results, our monolingual multi-
word term extraction model proves to be the best-performing algorithm on two domain data sets and the
second best-performing algorithm on the remaining two domain data sets. Our monolingual multiword
terminology extraction method is clearly comparable to the state-of-the-art monolingual terminology
extraction algorithms.
In this work, we considered all n-gram word sequences from the domain corpus as candidate terms.
</bodyText>
<page confidence="0.998109">
49
</page>
<bodyText confidence="0.999846">
In future work, we would like to incorporate the candidate phrasal term identification model of Deane
(2005), which would omit irrelevant multiword units, and help us extend our evaluation beyond the top-
100 terms. We also plan to demonstrate the impact of the created termbanks on translator productivity in
a number of workflows – different language pairs, domains, and levels of post-editing – in an industrial
setting.
</bodyText>
<sectionHeader confidence="0.990312" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.987455666666667">
This work was partially supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of
CNGL at Dublin City University, and by Grant 610879 for the Falcon project funded by the European
Commission.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999701138888888">
S. Ananiadou. 1994. A methodology for automatic term recognition. In COLING: 15th International Conference
on Computational Linguistics, pages 1034–1038.
K. Ahmad, L. Gillam and L. Tostevin. 1999. University of Surrey Participation in TREC8: Weirdness Indexing for
Logical Document Extrapolation and Retrieval (WILDER). In the Eighth Text REtrieval Conference (TREC-8).
National Institute of Standards and Technology, Gaithersburg, MD., pp.717–724.
R. Basili, A. Moschitti, M. Pazienza and F. Zanzotto. 2001. A contrastive approach to term extraction. In Pro-
ceedings of the 4th Conference on Terminology and Artificial Intelligence (TIA 2001). Nancy, France, 10pp.
K. Church and W. Gale. 1995. Inverse Document Frequency (IDF): A Measure of Deviation from Poisson. In
Proceedings of the 3rd Workshop on Very Large Corpora, pages 121–130. Cambridge, MA.
B. Daille, E. Gaussier and J-M. Lang´e. 1994. Towards automatic extraction of monolingual and bilingual termi-
nology. In COLING 94, The 15th International Conference on Computational Linguistics, Proceedings. Kyoto,
Japan, pp.515–521.
P. Deane. 2007. A nonparametric method for extraction of candidate phrasal terms. In ACL-05: 43rd Annual
Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, USA, pp.605–613.
K. Frantzi, S. Ananiadou and H. Mima. 2000. Automatic Recognition of Multi-word Terms: the C-value/NC-value
Method. International Journal of Digital Libraries. 3(2): 115–130.
E. Gaussier. 1998. Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Cor-
pora. In COLING-ACL ’98, 36th Annual Meeting of the Association for Computational Linguistics and 17th
International Conference on Computational Linguistics, Proceedings of the Conference, Volume II. Montreal,
Quebec, Canada, pp.444–450.
A. Gelbukh, G. Sidorov, E. Lavin-Villa and L. Chanona-Hernandez. 2010. Automatic Term Extraction Using
Log-Likelihood Based Comparison with General Reference Corpus. In 15th International Conference on Ap-
plications of Natural Language to Information Systems, NLDB 2010, Proceedings. LNCS vol. 6177. Berlin:
Springer. pp.248–255.
L. Ha, G. Fernandez, R. Mitkov and G. Corpas. 2008. Mutual bilingual terminology extraction. In LREC 2008:
6th Language Resources and Evaluation Conference. Marrakech, Morocco, pp.1818–1824.
T. He, T., X. Zhang and Y. Xinghuo. 2006. An Approach to Automatically Constructing Domain Ontology. In
Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, PACLIC 2006.
Wuhan, China, pp.150–157.
J. S. Justeson, and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural language engineering, 1(1) 9–27.
S. Kim, T. Baldwin and M-Y. Kan. 2009. An Unsupervised Approach to Domain-Specific Term Extraction. In
Proceedings of the Australasian Language Technology Association Workshop 2009. Sydney, Australia, pp.94–
98.
P. Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit X: The Tenth
Machine Translation Summit. Phuket, Thailand, pp.79–86.
</reference>
<page confidence="0.94847">
50
</page>
<reference confidence="0.999854282051282">
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical
Machine Translation. In ACL 2007, Proceedings of the Interactive Poster and Demonstration Sessions. Prague,
Czech Republic, pp.177–180.
P. Koehn, F. Och and H. Ney. 2003. Statistical Phrase-Based Translation. In HLT-NAACL 2003: conference
combining Human Language Technology conference series and the North American Chapter of the Association
for Computational Linguistics conference series. Edmonton, Canada, pp. 48–54.
L. Kozakov, Y. Park, T. H. Fin, Y. Drissi, Y. N. Doganata, and T. Cofino. 2004. Glossary extraction and knowledge
in large organisations via semantic web technologies. In Proceedings of the 6th International Semantic Web
Conference and the 2nd Asian Semantic Web Conference.
J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In 31st Annual
Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Columbus, Ohio,
USA, pp.17–22.
E. Lefever, L. Macken and V. Hoste. 2009. Language-Independent Bilingual Terminology Extraction from a
Multilingual Parallel Corpus. In EACL ’09 Proceedings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics. Athens, Greece, pp.496–504.
F. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting of the
Association for Computational Linguistics, Proceedings of the Conference. Sapporo, Japan, pp.160–167.
F. Och and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Transla-
tion. In 40th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.
Philadelphia, PA, USA, pp.295–302.
O. Bojar, V. Diatka, P. Rychl´y, P. Straˇn´ak, A. Tamchyna, and D. Zeman. 2014. Hindi-English and Hindi-only
Corpus for Machine Translation. In Proceedings of the Ninth International Language Resources and Evaluation
Conference (LREC’14). Reykjavik, Iceland.
P. Pantel and D. Lin. 2001. A Statistical Corpus-Based Term Extractor. In E. Stroulia and S. Matwin (eds.)
Advances in Artificial Intelligence, 14th Biennial Conference of the Canadian Society for Computational Studies
of Intelligence, AI 2001, Ottawa, Canada, Proceedings. LNCS vol. 2056. Berlin: Springer, pp.36–46.
P. Rayson and R. Garside. 2000. Comparing corpora using frequency profiling. In Proceedings of the Workshop
on Comparing Corpora, held in conjunction with the 38th Annual Meeting of the Association for Computational
Linguistics (ACL 2000). Hong Kong, pp.1–6.
F. Sclano and P. Velardi. 2007. TermExtractor: a web application to learn the shared terminology of emergent web
communities. In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software
and Applications (I-ESA 2007). Funchal, Madeira Island, Portugal, pp.287–290.
J. Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces.
In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.) Recent Advances in Natural Language
Processing (vol V), pages 237–248, John Benjamins, Amsterdam/Philadelphia.
Z. Zhang, J. Iria, C. Brewster and F. Ciravegna. 2008. A Comparative Evaluation of Term Recognition Algorithms.
In Proceedings of The sixth international conference on Language Resources and Evaluation, (LREC 2008),
pages 2108–2113, Marrakech, Morocco.
</reference>
<page confidence="0.999121">
51
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.586784">
<title confidence="0.973212">Bilingual Termbank Creation via Log-Likelihood Comparison Phrase-Based Statistical Machine Translation</title>
<author confidence="0.789486">Sergio Penkale Haque</author>
<author confidence="0.789486">Andy</author>
<address confidence="0.955471">Lingo24, Edinburgh, UK</address>
<affiliation confidence="0.998246">Centre for Global Intelligent School of Computing, Dublin City</affiliation>
<address confidence="0.830102">Dublin 9,</address>
<email confidence="0.954945">away@computing.dcu.ie</email>
<abstract confidence="0.9981183">Bilingual termbanks are important for many natural language processing (NLP) applications, especially in translation workflows in industrial settings. In this paper, we apply a log-likelihood comparison method to extract monolingual terminology from the source and target sides of a parallel corpus. Then, using a Phrase-Based Statistical Machine Translation model, we create a bilingual terminology with the extracted monolingual term lists. We manually evaluate our novel terminology extraction model on English-to-Spanish and English-to-Hindi data sets, and observe excellent performance for all domains. Furthermore, we report the performance of our monolingual terminology extraction model comparing with a number of the state-of-the-art terminology extraction models on the English-to-Hindi datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ananiadou</author>
</authors>
<title>A methodology for automatic term recognition.</title>
<date>1994</date>
<booktitle>In COLING: 15th International Conference on Computational Linguistics,</booktitle>
<pages>1034--1038</pages>
<contexts>
<context position="21218" citStr="Ananiadou (1994)" startWordPosition="3300" endWordPosition="3301"> algorithms to recognise terms specific to the domain corpus. The JATE toolkit is currently available only for the English language. For evaluation purposes, we considered the source-side of the English-to-Hindi domain corpora. Algorithm Reference EILMT EMILLE Launchpad KDE4 LLC (Bilingual) cf. VST in Table 3 91 79 88 79 LLC 77 53 80 71 STF 46 04 54 44 ACTF 42 15 62 48 TF-IDF 50 36 45 17 Glossex Kozakov et al. (2004) 76 43 76 71 JK Justeson &amp; Katz (1995) 42 13 58 42 NC-Value Frantzi et al. (2000) 46 34 52 25 RIDF Church &amp; Gale (1995) 27 16 23 21 TermEx Sclano et al. (2007) 42 08 46 41 C-Value Ananiadou (1994) 49 44 62 40 Weirdness Ahmed et al. (1999) 77 57 82 63 Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Frequency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)). The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the candidate terms giving an indication of the likelihood of being a good domain-specific term. The quality of the extracted terms (top-100 highest weighted) was judged by an evaluator with ex</context>
</contexts>
<marker>Ananiadou, 1994</marker>
<rawString>S. Ananiadou. 1994. A methodology for automatic term recognition. In COLING: 15th International Conference on Computational Linguistics, pages 1034–1038.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ahmad</author>
<author>L Gillam</author>
<author>L Tostevin</author>
</authors>
<title>University of Surrey Participation in TREC8: Weirdness Indexing for Logical Document Extrapolation and Retrieval (WILDER).</title>
<date>1999</date>
<booktitle>In the Eighth Text REtrieval Conference (TREC-8). National Institute of Standards and Technology,</booktitle>
<pages>717--724</pages>
<location>Gaithersburg, MD.,</location>
<contexts>
<context position="4451" citStr="Ahmad et al., 1999" startWordPosition="613" endWordPosition="616">le Section 5 concludes, and provides avenues for further work. 2 Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (200</context>
<context position="22651" citStr="Ahmad et al., 1999" startWordPosition="3511" endWordPosition="3514">racted using different state-of-the-art term extraction algorithms. The third row of Table 6 represents the percentage of the valid source terms extracted by our loglikelihood comparison (LLC) based monolingual term extraction algorithm. The next three rows represent three basic monolingual term extraction algorithms (STF: simple term frequency, ACTF: average corpus term frequency and TF-IDF) available in the JATE toolkit. The last seven rows represent seven state-of-the-art terminology extraction algorithms. As can be seen from Table 6, LLC is the bestperforming algorithm with the Weirdness (Ahmad et al., 1999) and the Glossex (Kozakov et al., 2004) algorithms on the EILMT and the KDE4 corpora, respectively. The LLC is also the second-best performing algorithm on the EMILLE and the Lauchpad corpora. We see in Table 6 that the percentage of valid source terms is quite low on the EMILLE corpus. This might be caused by it containing information leaflets in a variety of domains (consumer, education, housing, health, legal, social), which might bring down the percentage of valid source terms on this corpus. Note that the percentage of valid source terms (VST) reported in Table 3 is calculated taking the </context>
</contexts>
<marker>Ahmad, Gillam, Tostevin, 1999</marker>
<rawString>K. Ahmad, L. Gillam and L. Tostevin. 1999. University of Surrey Participation in TREC8: Weirdness Indexing for Logical Document Extrapolation and Retrieval (WILDER). In the Eighth Text REtrieval Conference (TREC-8). National Institute of Standards and Technology, Gaithersburg, MD., pp.717–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>A Moschitti</author>
<author>M Pazienza</author>
<author>F Zanzotto</author>
</authors>
<title>A contrastive approach to term extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the 4th Conference on Terminology and Artificial Intelligence (TIA 2001).</booktitle>
<location>Nancy, France,</location>
<contexts>
<context position="2554" citStr="Basili et al., 2001" startWordPosition="342" endWordPosition="345">ed terminology list. The automatic terminology extraction task starts with selecting candidate terms from the input domain corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word sequences are regarded as candidate terms (Deane, 2005). Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from the source and target sides of a parallel corpus, 2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003). We then evaluate our novel bilingual terminology extraction model on various domain corpora considering English-to-Spanish and </context>
<context position="4390" citStr="Basili et al., 2001" startWordPosition="603" endWordPosition="606">n 4 presents the results and analyses of our experiments, while Section 5 concludes, and provides avenues for further work. 2 Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extra</context>
</contexts>
<marker>Basili, Moschitti, Pazienza, Zanzotto, 2001</marker>
<rawString>R. Basili, A. Moschitti, M. Pazienza and F. Zanzotto. 2001. A contrastive approach to term extraction. In Proceedings of the 4th Conference on Terminology and Artificial Intelligence (TIA 2001). Nancy, France, 10pp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>Inverse Document Frequency (IDF): A Measure of Deviation from Poisson.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora,</booktitle>
<pages>121--130</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="21141" citStr="Church &amp; Gale (1995)" startWordPosition="3282" endWordPosition="3285">idate terms from a corpus using linguistic tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The JATE toolkit is currently available only for the English language. For evaluation purposes, we considered the source-side of the English-to-Hindi domain corpora. Algorithm Reference EILMT EMILLE Launchpad KDE4 LLC (Bilingual) cf. VST in Table 3 91 79 88 79 LLC 77 53 80 71 STF 46 04 54 44 ACTF 42 15 62 48 TF-IDF 50 36 45 17 Glossex Kozakov et al. (2004) 76 43 76 71 JK Justeson &amp; Katz (1995) 42 13 58 42 NC-Value Frantzi et al. (2000) 46 34 52 25 RIDF Church &amp; Gale (1995) 27 16 23 21 TermEx Sclano et al. (2007) 42 08 46 41 C-Value Ananiadou (1994) 49 44 62 40 Weirdness Ahmed et al. (1999) 77 57 82 63 Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Frequency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)). The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the candidate terms giving an indication of the likelihood of being a good domain-specific term. The quality of the </context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>K. Church and W. Gale. 1995. Inverse Document Frequency (IDF): A Measure of Deviation from Poisson. In Proceedings of the 3rd Workshop on Very Large Corpora, pages 121–130. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
<author>E Gaussier</author>
<author>J-M Lang´e</author>
</authors>
<title>Towards automatic extraction of monolingual and bilingual terminology.</title>
<date>1994</date>
<booktitle>In COLING 94, The 15th International Conference on Computational Linguistics, Proceedings.</booktitle>
<pages>515--521</pages>
<location>Kyoto, Japan,</location>
<marker>Daille, Gaussier, Lang´e, 1994</marker>
<rawString>B. Daille, E. Gaussier and J-M. Lang´e. 1994. Towards automatic extraction of monolingual and bilingual terminology. In COLING 94, The 15th International Conference on Computational Linguistics, Proceedings. Kyoto, Japan, pp.515–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Deane</author>
</authors>
<title>A nonparametric method for extraction of candidate phrasal terms.</title>
<date>2007</date>
<booktitle>In ACL-05: 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>605--613</pages>
<location>Ann Arbor, Michigan, USA,</location>
<marker>Deane, 2007</marker>
<rawString>P. Deane. 2007. A nonparametric method for extraction of candidate phrasal terms. In ACL-05: 43rd Annual Meeting of the Association for Computational Linguistics. Ann Arbor, Michigan, USA, pp.605–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Frantzi</author>
<author>S Ananiadou</author>
<author>H Mima</author>
</authors>
<title>Automatic Recognition of Multi-word Terms: the C-value/NC-value Method.</title>
<date>2000</date>
<journal>International Journal of Digital Libraries.</journal>
<volume>3</volume>
<issue>2</issue>
<pages>115--130</pages>
<contexts>
<context position="2229" citStr="Frantzi et al., 2000" startWordPosition="295" endWordPosition="298">g human experts are, however, expensive and time-consuming tasks. In contrast, automatic terminology extraction is much faster and less expensive, but cannot be guaranteed to be error-free. Accordingly, in real NLP applications, a manual inspection is required to amend or discard anomalous items from an automatically extracted terminology list. The automatic terminology extraction task starts with selecting candidate terms from the input domain corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word sequences are regarded as candidate terms (Deane, 2005). Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from the source and target </context>
<context position="4273" citStr="Frantzi et al., 2000" startWordPosition="585" endWordPosition="588">ws. In Section 2, we discuss related work. In Section 3, we describe our two-stage terminology extraction model. Section 4 presents the results and analyses of our experiments, while Section 5 concludes, and provides avenues for further work. 2 Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelih</context>
<context position="21103" citStr="Frantzi et al. (2000)" startWordPosition="3273" endWordPosition="3276">2008). This toolkit first extracts candidate terms from a corpus using linguistic tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The JATE toolkit is currently available only for the English language. For evaluation purposes, we considered the source-side of the English-to-Hindi domain corpora. Algorithm Reference EILMT EMILLE Launchpad KDE4 LLC (Bilingual) cf. VST in Table 3 91 79 88 79 LLC 77 53 80 71 STF 46 04 54 44 ACTF 42 15 62 48 TF-IDF 50 36 45 17 Glossex Kozakov et al. (2004) 76 43 76 71 JK Justeson &amp; Katz (1995) 42 13 58 42 NC-Value Frantzi et al. (2000) 46 34 52 25 RIDF Church &amp; Gale (1995) 27 16 23 21 TermEx Sclano et al. (2007) 42 08 46 41 C-Value Ananiadou (1994) 49 44 62 40 Weirdness Ahmed et al. (1999) 77 57 82 63 Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Frequency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)). The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the candidate terms giving an indication of the likelihood of being a good dom</context>
</contexts>
<marker>Frantzi, Ananiadou, Mima, 2000</marker>
<rawString>K. Frantzi, S. Ananiadou and H. Mima. 2000. Automatic Recognition of Multi-word Terms: the C-value/NC-value Method. International Journal of Digital Libraries. 3(2): 115–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gaussier</author>
</authors>
<title>Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora.</title>
<date>1998</date>
<booktitle>In COLING-ACL ’98, 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Proceedings of the Conference, Volume II.</booktitle>
<pages>444--450</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="5035" citStr="Gaussier (1998)" startWordPosition="701" endWordPosition="702"> algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obta</context>
</contexts>
<marker>Gaussier, 1998</marker>
<rawString>E. Gaussier. 1998. Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora. In COLING-ACL ’98, 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Proceedings of the Conference, Volume II. Montreal, Quebec, Canada, pp.444–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gelbukh</author>
<author>G Sidorov</author>
<author>E Lavin-Villa</author>
<author>L Chanona-Hernandez</author>
</authors>
<title>Automatic Term Extraction Using Log-Likelihood Based Comparison with General Reference Corpus.</title>
<date>2010</date>
<booktitle>In 15th International Conference on Applications of Natural Language to Information Systems, NLDB 2010, Proceedings. LNCS</booktitle>
<volume>vol.</volume>
<pages>6177</pages>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<contexts>
<context position="4786" citStr="Gelbukh et al. (2010)" startWordPosition="665" endWordPosition="668">g work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced la</context>
<context position="7129" citStr="Gelbukh et al. (2010)" startWordPosition="1026" endWordPosition="1029">rom the monolingual domain corpus (i.e. in our case, each side of the domain parallel corpus, cf. Section 4.1). On both source and target sides, we used lists of languagespecific stop-words and punctuation marks in order to filter out anomalous items from the candidate termlists. In order to rank the candidate terms in those lists, we used a log-likelihood comparison method that compares the frequencies of each candidate term in both the domain corpus and the large general corpus used as a reference.1 The log-likelihood (LL) value of a candidate term (Cn) is calculated using equation (1) from Gelbukh et al. (2010). LL = 2 * ((Fd * log(Fd/Ed)) + (Fg * log(Fg/Eg))) (1) where Fd and Fg are the frequencies of Cn in the domain corpus and the generic reference corpus, respectively. Ed and Eg are the expected frequencies of Cn, which are calculated using (2) and (3). Ed = Nnd * (Fd + Fg)/(Nnd + Nng) (2) Eg = Nng * (Fd + Fg)/(Nnd + Nng) (3) 1Before the term-extraction process begins, we apply a number of preprocessing methods including tokenisation to the input domain corpus and the generic reference corpus. 43 where Nnd and Nng are the numbers of n-grams in the domain corpus and reference corpus, respectively</context>
</contexts>
<marker>Gelbukh, Sidorov, Lavin-Villa, Chanona-Hernandez, 2010</marker>
<rawString>A. Gelbukh, G. Sidorov, E. Lavin-Villa and L. Chanona-Hernandez. 2010. Automatic Term Extraction Using Log-Likelihood Based Comparison with General Reference Corpus. In 15th International Conference on Applications of Natural Language to Information Systems, NLDB 2010, Proceedings. LNCS vol. 6177. Berlin: Springer. pp.248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ha</author>
<author>G Fernandez</author>
<author>R Mitkov</author>
<author>G Corpas</author>
</authors>
<title>Mutual bilingual terminology extraction.</title>
<date>2008</date>
<booktitle>In LREC 2008: 6th Language Resources and Evaluation Conference.</booktitle>
<pages>1818--1824</pages>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="5053" citStr="Ha et al. (2008)" startWordPosition="703" endWordPosition="706"> et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obtaining a bilingual </context>
</contexts>
<marker>Ha, Fernandez, Mitkov, Corpas, 2008</marker>
<rawString>L. Ha, G. Fernandez, R. Mitkov and G. Corpas. 2008. Mutual bilingual terminology extraction. In LREC 2008: 6th Language Resources and Evaluation Conference. Marrakech, Morocco, pp.1818–1824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T He</author>
<author>X Zhang T</author>
<author>Y Xinghuo</author>
</authors>
<title>An Approach to Automatically Constructing Domain Ontology.</title>
<date>2006</date>
<booktitle>In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, PACLIC</booktitle>
<pages>150--157</pages>
<location>Wuhan, China,</location>
<contexts>
<context position="4622" citStr="He et al. (2006)" startWordPosition="641" endWordPosition="644">h can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in p</context>
</contexts>
<marker>He, T, Xinghuo, 2006</marker>
<rawString>T. He, T., X. Zhang and Y. Xinghuo. 2006. An Approach to Automatically Constructing Domain Ontology. In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, PACLIC 2006. Wuhan, China, pp.150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Justeson</author>
<author>S M Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in text. Natural language engineering,</title>
<date>1995</date>
<volume>1</volume>
<issue>1</issue>
<pages>9--27</pages>
<contexts>
<context position="21060" citStr="Justeson &amp; Katz (1995)" startWordPosition="3264" endWordPosition="3267">ms we used the JATE toolkit8 (Zhang et al., 2008). This toolkit first extracts candidate terms from a corpus using linguistic tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The JATE toolkit is currently available only for the English language. For evaluation purposes, we considered the source-side of the English-to-Hindi domain corpora. Algorithm Reference EILMT EMILLE Launchpad KDE4 LLC (Bilingual) cf. VST in Table 3 91 79 88 79 LLC 77 53 80 71 STF 46 04 54 44 ACTF 42 15 62 48 TF-IDF 50 36 45 17 Glossex Kozakov et al. (2004) 76 43 76 71 JK Justeson &amp; Katz (1995) 42 13 58 42 NC-Value Frantzi et al. (2000) 46 34 52 25 RIDF Church &amp; Gale (1995) 27 16 23 21 TermEx Sclano et al. (2007) 42 08 46 41 C-Value Ananiadou (1994) 49 44 62 40 Weirdness Ahmed et al. (1999) 77 57 82 63 Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Frequency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)). The automatic term extraction algorithms in JATE assign weights (domain representativeness) to the candidate terms giving an indic</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>J. S. Justeson, and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for identification in text. Natural language engineering, 1(1) 9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>T Baldwin</author>
<author>M-Y Kan</author>
</authors>
<title>An Unsupervised Approach to Domain-Specific Term Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop 2009.</booktitle>
<pages>94--98</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4409" citStr="Kim et al., 2009" startWordPosition="607" endWordPosition="610">lts and analyses of our experiments, while Section 5 concludes, and provides avenues for further work. 2 Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupi</context>
</contexts>
<marker>Kim, Baldwin, Kan, 2009</marker>
<rawString>S. Kim, T. Baldwin and M-Y. Kan. 2009. An Unsupervised Approach to Domain-Specific Term Extraction. In Proceedings of the Australasian Language Technology Association Workshop 2009. Sydney, Australia, pp.94– 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: a parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X: The Tenth Machine Translation Summit.</booktitle>
<location>Phuket, Thailand,</location>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit X: The Tenth Machine Translation Summit. Phuket, Thailand, pp.79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In ACL 2007, Proceedings of the Interactive Poster and Demonstration Sessions.</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8948" citStr="Koehn et al., 2007" startWordPosition="1338" endWordPosition="1341">ulti-word terms up to 3-grams, whereas they focused solely on extracting single word terms. 3.2 Creating a Bilingual Termbank We obtained source and target termlists from the bilingual domain corpus using the approach described in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the extracted source and target termlists. This section provides a mathematical derivation of the PB-SMT model to show how we scored candidate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability P(eI1|fJ1 ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (5): log P(eI1|fJ1 ) = XM λmhm(fJ1 , eI1, sK1 ) + λLMlog P(eI1) (5) m=1 where eI1 = e1, ..., eI is the probable candidate translation for the given input sentence fJ1 = f1, ..., fJ and sK1 = s1, ..., sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆf1,..., ˆfk) and (ˆe1, ..., ˆek) such that (we set i0 := 0): ∀k E [1, K] sk := (i</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 2007, Proceedings of the Interactive Poster and Demonstration Sessions. Prague, Czech Republic, pp.177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003: conference combining Human Language Technology conference series and the North American Chapter of the Association for Computational Linguistics conference series.</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="3025" citStr="Koehn et al., 2003" startWordPosition="412" endWordPosition="415">ue (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from the source and target sides of a parallel corpus, 2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003). We then evaluate our novel bilingual terminology extraction model on various domain corpora considering English-to-Spanish and low-resourced and less-explored English-to-Hindi language-pairs and see excellent performance for all data sets. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 42 Proceedings of the 4th International Workshop on Computational Terminology, pages 42–51, Dublin, Ireland, August 23 2014. The remainder of the </context>
<context position="5721" citStr="Koehn et al., 2003" startWordPosition="800" endWordPosition="803"> proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In contrast, we build a PB-SMT model (Koehn et al., 2003) from the input parallel corpus, which we use to align a source term to one or more target terms. While Rayson and Garside (2000) and Gelbukh et al. (2010) only allowed the extraction of single-word terms, we focus on extraction of up to 3-gram terms. 3 Methodology In this section, we describe our two-stage bilingual terminology extraction model. In the first stage, we extract monolingual terms independently from either side of a sentence-aligned domain-specific parallel corpus. In the second stage, the extracted source terms are aligned to one or more extracted target terms using a PB-SMT mod</context>
<context position="8615" citStr="Koehn et al., 2003" startWordPosition="1284" endWordPosition="1287">e terms that are likely to be terms in the domain corpus. Gelbukh et al. (2010) used the condition in (4) in order to filter out those candidate terms whose relative frequencies are bigger in the domain corpus than in the reference corpus, and we do likewise. Fd/Nnd &gt; Fg/Nn (4) g In contrast with Gelbukh et al. (2010), we extract multi-word terms up to 3-grams, whereas they focused solely on extracting single word terms. 3.2 Creating a Bilingual Termbank We obtained source and target termlists from the bilingual domain corpus using the approach described in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the extracted source and target termlists. This section provides a mathematical derivation of the PB-SMT model to show how we scored candidate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability P(eI1|fJ1 ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (5): log P(eI1|fJ1 ) = XM λmhm(fJ1 , eI1, sK1 ) + λLMl</context>
</contexts>
<marker>Koehn, Och, Ney, 2003</marker>
<rawString>P. Koehn, F. Och and H. Ney. 2003. Statistical Phrase-Based Translation. In HLT-NAACL 2003: conference combining Human Language Technology conference series and the North American Chapter of the Association for Computational Linguistics conference series. Edmonton, Canada, pp. 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kozakov</author>
<author>Y Park</author>
<author>T H Fin</author>
<author>Y Drissi</author>
<author>Y N Doganata</author>
<author>T Cofino</author>
</authors>
<title>Glossary extraction and knowledge in large organisations via semantic web technologies.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th International Semantic Web Conference and the 2nd Asian Semantic Web Conference.</booktitle>
<contexts>
<context position="4483" citStr="Kozakov et al., 2004" startWordPosition="618" endWordPosition="621">ovides avenues for further work. 2 Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Le</context>
<context position="21022" citStr="Kozakov et al. (2004)" startWordPosition="3255" endWordPosition="3258"> to extract monolingual multiword terms we used the JATE toolkit8 (Zhang et al., 2008). This toolkit first extracts candidate terms from a corpus using linguistic tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The JATE toolkit is currently available only for the English language. For evaluation purposes, we considered the source-side of the English-to-Hindi domain corpora. Algorithm Reference EILMT EMILLE Launchpad KDE4 LLC (Bilingual) cf. VST in Table 3 91 79 88 79 LLC 77 53 80 71 STF 46 04 54 44 ACTF 42 15 62 48 TF-IDF 50 36 45 17 Glossex Kozakov et al. (2004) 76 43 76 71 JK Justeson &amp; Katz (1995) 42 13 58 42 NC-Value Frantzi et al. (2000) 46 34 52 25 RIDF Church &amp; Gale (1995) 27 16 23 21 TermEx Sclano et al. (2007) 42 08 46 41 C-Value Ananiadou (1994) 49 44 62 40 Weirdness Ahmed et al. (1999) 77 57 82 63 Table 6: Monolingual evaluation results. LLC: Log-Likelihood Comparison, STF: Simple Term Frequency, ACTF: Average Corpus Term Frequency, JK: Justeson Katz For comparison, we considered the top-100 source terms based on the log-likelihood values (cf. (1)). The automatic term extraction algorithms in JATE assign weights (domain representativeness) </context>
<context position="22690" citStr="Kozakov et al., 2004" startWordPosition="3518" endWordPosition="3521">rt term extraction algorithms. The third row of Table 6 represents the percentage of the valid source terms extracted by our loglikelihood comparison (LLC) based monolingual term extraction algorithm. The next three rows represent three basic monolingual term extraction algorithms (STF: simple term frequency, ACTF: average corpus term frequency and TF-IDF) available in the JATE toolkit. The last seven rows represent seven state-of-the-art terminology extraction algorithms. As can be seen from Table 6, LLC is the bestperforming algorithm with the Weirdness (Ahmad et al., 1999) and the Glossex (Kozakov et al., 2004) algorithms on the EILMT and the KDE4 corpora, respectively. The LLC is also the second-best performing algorithm on the EMILLE and the Lauchpad corpora. We see in Table 6 that the percentage of valid source terms is quite low on the EMILLE corpus. This might be caused by it containing information leaflets in a variety of domains (consumer, education, housing, health, legal, social), which might bring down the percentage of valid source terms on this corpus. Note that the percentage of valid source terms (VST) reported in Table 3 is calculated taking the top-100 source terms from the bilingual</context>
</contexts>
<marker>Kozakov, Park, Fin, Drissi, Doganata, Cofino, 2004</marker>
<rawString>L. Kozakov, Y. Park, T. H. Fin, Y. Drissi, Y. N. Doganata, and T. Cofino. 2004. Glossary extraction and knowledge in large organisations via semantic web technologies. In Proceedings of the 6th International Semantic Web Conference and the 2nd Asian Semantic Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In 31st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.</booktitle>
<pages>17--22</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="2206" citStr="Kupiec, 1993" startWordPosition="293" endWordPosition="294">resources using human experts are, however, expensive and time-consuming tasks. In contrast, automatic terminology extraction is much faster and less expensive, but cannot be guaranteed to be error-free. Accordingly, in real NLP applications, a manual inspection is required to amend or discard anomalous items from an automatically extracted terminology list. The automatic terminology extraction task starts with selecting candidate terms from the input domain corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word sequences are regarded as candidate terms (Deane, 2005). Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from</context>
<context position="5018" citStr="Kupiec (1993)" startWordPosition="699" endWordPosition="700">009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, ra</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>J. Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In 31st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Columbus, Ohio, USA, pp.17–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lefever</author>
<author>L Macken</author>
<author>V Hoste</author>
</authors>
<title>Language-Independent Bilingual Terminology Extraction from a Multilingual Parallel Corpus.</title>
<date>2009</date>
<booktitle>In EACL ’09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>496--504</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="5079" citStr="Lefever et al. (2009)" startWordPosition="708" endWordPosition="711">ex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sentential alignmentbased terminology extraction module that links linguistically motivated phrases in parallel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In contrast, we</context>
</contexts>
<marker>Lefever, Macken, Hoste, 2009</marker>
<rawString>E. Lefever, L. Macken and V. Hoste. 2009. Language-Independent Bilingual Terminology Extraction from a Multilingual Parallel Corpus. In EACL ’09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Athens, Greece, pp.496–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10092" citStr="Och, 2003" startWordPosition="1557" endWordPosition="1558">d (ˆe1, ..., ˆek) such that (we set i0 := 0): ∀k E [1, K] sk := (ik; bk, jk), (bk corresponds to starting index of fk) ˆfk := ˆfbk, ..., ˆfjk Each feature hm in (5) can be rewritten as in (6): hm(fJ1 , eI1, sK1 ) = XK ˆhm(ˆfk, ˆek, sk) (6) k=1 Therefore, the translational features in (5) can be rewritten as in (7): eik, ˆek := ˆ ˆ eik−1+1, ..., M λmhm(fJ1 , eI1, sK1 ) = XM λm XK ˆhm(ˆfk, ˆek, sk) (7) X m=1 k=1 m=1 In equation (7), ˆhm is a feature defined on phrase-pairs (ˆfk, ˆek), and λm is the feature weight of ˆhm. These weights (λm) are optimized using minimum error-rate training (MERT) (Och, 2003) on a held-out 500 sentence-pair development set for each of the experiments. We create a list of probable source–target term-pairs by taking each source and target term from the source and target termlists, respectively, provided that those source–target term-pairs are present in the PB-SMT phrase-table. We calculate a weight (w) for each source–target term-pair (essentially, a phrasepair, i.e. (ˆek, ˆfk)) using (8):2 w(ˆek, ˆfk) = XM λmˆhm(ˆfk, ˆek) (8) m=1 2Equation (8) is derived from the right-hand side of equation (7) for a single source–target phrase-pair. 44 In order to calculate w, we</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In 41st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Sapporo, Japan, pp.160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference.</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="9081" citStr="Och and Ney, 2002" startWordPosition="1358" endWordPosition="1361">ned source and target termlists from the bilingual domain corpus using the approach described in Section 3.1. We use a PB-SMT model (Koehn et al., 2003) to create a bilingual termbank from the extracted source and target termlists. This section provides a mathematical derivation of the PB-SMT model to show how we scored candidate term-pairs using the PB-SMT model. We built a source-to-target PB-SMT model from the bilingual domain corpus using the Moses toolkit (Koehn et al., 2007). In PB-SMT, the posterior probability P(eI1|fJ1 ) is directly modelled as a (log-linear) combination of features (Och and Ney, 2002), that usually comprise M translational features, and the language model, as in (5): log P(eI1|fJ1 ) = XM λmhm(fJ1 , eI1, sK1 ) + λLMlog P(eI1) (5) m=1 where eI1 = e1, ..., eI is the probable candidate translation for the given input sentence fJ1 = f1, ..., fJ and sK1 = s1, ..., sk denotes a segmentation of the source and target sentences respectively into the sequences of phrases (ˆf1,..., ˆfk) and (ˆe1, ..., ˆek) such that (we set i0 := 0): ∀k E [1, K] sk := (ik; bk, jk), (bk corresponds to starting index of fk) ˆfk := ˆfbk, ..., ˆfjk Each feature hm in (5) can be rewritten as in (6): hm(fJ1</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. Och and H. Ney. 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation. In 40th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Philadelphia, PA, USA, pp.295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>V Diatka</author>
<author>P Rychl´y</author>
<author>P Straˇn´ak</author>
<author>A Tamchyna</author>
<author>D Zeman</author>
</authors>
<title>Hindi-English and Hindi-only Corpus for Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC’14). Reykjavik,</booktitle>
<marker>Bojar, Diatka, Rychl´y, Straˇn´ak, Tamchyna, Zeman, 2014</marker>
<rawString>O. Bojar, V. Diatka, P. Rychl´y, P. Straˇn´ak, A. Tamchyna, and D. Zeman. 2014. Hindi-English and Hindi-only Corpus for Machine Translation. In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC’14). Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>A Statistical Corpus-Based Term Extractor. In</title>
<date>2001</date>
<booktitle>Advances in Artificial Intelligence, 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence, AI 2001, Ottawa, Canada, Proceedings. LNCS</booktitle>
<volume>vol.</volume>
<pages>36--46</pages>
<editor>E. Stroulia and S. Matwin (eds.)</editor>
<publisher>Springer,</publisher>
<location>Berlin:</location>
<contexts>
<context position="4361" citStr="Pantel and Lin, 2001" startWordPosition="598" endWordPosition="601">nology extraction model. Section 4 presents the results and analyses of our experiments, while Section 5 concludes, and provides avenues for further work. 2 Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research o</context>
</contexts>
<marker>Pantel, Lin, 2001</marker>
<rawString>P. Pantel and D. Lin. 2001. A Statistical Corpus-Based Term Extractor. In E. Stroulia and S. Matwin (eds.) Advances in Artificial Intelligence, 14th Biennial Conference of the Canadian Society for Computational Studies of Intelligence, AI 2001, Ottawa, Canada, Proceedings. LNCS vol. 2056. Berlin: Springer, pp.36–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>R Garside</author>
</authors>
<title>Comparing corpora using frequency profiling.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Comparing Corpora, held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1--6</pages>
<contexts>
<context position="2520" citStr="Rayson and Garside, 2000" startWordPosition="336" endWordPosition="339">ous items from an automatically extracted terminology list. The automatic terminology extraction task starts with selecting candidate terms from the input domain corpus, usually in two different ways: (i) linguistic processors are used to identify noun phrases that are regarded as candidate terms (Kupiec, 1993; Frantzi et al., 2000), and (ii) non-linguistic n-gram word sequences are regarded as candidate terms (Deane, 2005). Various statistical measures have been used to rank candidate terms, such as C-Value (Ananiadou et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood comparison (Rayson and Garside, 2000), and TF-IDF (Basili et al., 2001). In this paper, we present our bilingual terminology extraction model, which is composed of two consecutive and independent processes: 1. A log-likelihood comparison method is employed to rank candidate terms (n-gram word sequences) independently from the source and target sides of a parallel corpus, 2. The extracted source terms are aligned to one or more extracted target terms using a Phrase-Based Statistical Machine Translation (PB-SMT) model (Koehn et al., 2003). We then evaluate our novel bilingual terminology extraction model on various domain corpora c</context>
<context position="4338" citStr="Rayson and Garside, 2000" startWordPosition="593" endWordPosition="597">scribe our two-stage terminology extraction model. Section 4 presents the results and analyses of our experiments, while Section 5 concludes, and provides avenues for further work. 2 Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most</context>
<context position="5850" citStr="Rayson and Garside (2000)" startWordPosition="824" endWordPosition="827">llel texts. Unlike our approach, theirs relies on linguistic analysis tools such as PoS taggers or lemmatizers, which might be unavailable for under-resourced languages (e.g., Hindi). Gaussier (1998) and Ha et al. (2008) applied statistical approaches to acquire parallel term-pairs directly from a sentence-aligned corpus, with the latter focusing on improving monolingual term extraction, rather than on obtaining a bilingual term list. In contrast, we build a PB-SMT model (Koehn et al., 2003) from the input parallel corpus, which we use to align a source term to one or more target terms. While Rayson and Garside (2000) and Gelbukh et al. (2010) only allowed the extraction of single-word terms, we focus on extraction of up to 3-gram terms. 3 Methodology In this section, we describe our two-stage bilingual terminology extraction model. In the first stage, we extract monolingual terms independently from either side of a sentence-aligned domain-specific parallel corpus. In the second stage, the extracted source terms are aligned to one or more extracted target terms using a PB-SMT model. 3.1 Monolingual Terminology Extraction The monolingual term extraction task involves the identification of terms from a list </context>
</contexts>
<marker>Rayson, Garside, 2000</marker>
<rawString>P. Rayson and R. Garside. 2000. Comparing corpora using frequency profiling. In Proceedings of the Workshop on Comparing Corpora, held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics (ACL 2000). Hong Kong, pp.1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sclano</author>
<author>P Velardi</author>
</authors>
<title>TermExtractor: a web application to learn the shared terminology of emergent web communities.</title>
<date>2007</date>
<booktitle>In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software and Applications (I-ESA 2007).</booktitle>
<pages>287--290</pages>
<location>Funchal, Madeira Island, Portugal,</location>
<contexts>
<context position="4521" citStr="Sclano and Velardi, 2007" startWordPosition="624" endWordPosition="627"> Related Work Several algorithms have been proposed to extract terminology from a domain-specific corpus, which can be divided into three broad categories: linguistic, statistical and hybrid. Statistical or hybrid approaches dominate this field, with some of the leading work including the use of frequency-based filtering (Daille et al., 1994), NC-Value (Frantzi et al., 2000), log-likelihood and mutual information (Rayson and Garside, 2000; Pantel and Lin, 2001), TF-IDF (Basili et al., 2001; Kim et al., 2009), weirdness algorithm (Ahmad et al., 1999), Glossex (Kozakov et al., 2004) and Termex (Sclano and Velardi, 2007). In this work, we focus on extracting bilingual terminology from a parallel corpus. He et al. (2006) demonstrate that using log-likelihood for term discovery performs better than TF-IDF. Accordingly, similarly to Rayson and Garside (2000) and Gelbukh et al. (2010), we extract terms independently from both sides of a parallel corpus using log-likelihood comparisons with a generic reference corpus. Some of the most influential research on bilingual terminology extraction includes Kupiec (1993), Gaussier (1998), Ha et al. (2008) and Lefever et al. (2009). Lefever et al. (2009) proposed a sub-sen</context>
</contexts>
<marker>Sclano, Velardi, 2007</marker>
<rawString>F. Sclano and P. Velardi. 2007. TermExtractor: a web application to learn the shared terminology of emergent web communities. In Proceedings of the 3rd International Conference on Interoperability for Enterprise Software and Applications (I-ESA 2007). Funchal, Madeira Island, Portugal, pp.287–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing (vol V),</booktitle>
<pages>237--248</pages>
<editor>In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.)</editor>
<location>John Benjamins, Amsterdam/Philadelphia.</location>
<contexts>
<context position="12239" citStr="Tiedemann, 2009" startWordPosition="1858" endWordPosition="1859">00,000 182,066,982 Table 1: Corpus Statistics. 4 Experiments and Discussion 4.1 Data Used We conducted experiments on several data domains for two different language-pairs, English-to-Spanish and English-to-Hindi. For English-to-Spanish, we worked with client-provided data taken from six different domains in the form of translation memories. For English-to-Hindi, we used three parallel corpora from three different sources (EILMT, EMILLE and Launchpad) taken from HindEnCorp3 (Bojar et al., 2014) released for the WMT14 shared translation task,4 and a parallel corpus of KDE4 localization files5 (Tiedemann, 2009). The EMILLE corpus contains leaflets from the UK Government and various local authorities. The domain of the EILMT6 corpus is tourism. We used data from a collection of translated documents from the United Nations (MultiUN)7 (Tiedemann, 2009) and the European Parliament (Koehn et al., 2005) as the monolingual English and Spanish reference corpora. We used the HindEnCorp monolingual corpus (Bojar et al., 2014) as the monolingual Hindi reference corpus. The statistics of the data used in our experiments are shown in Table 1. 4.2 Runtime Performance Our terminology extraction model is composed o</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J. Tiedemann. 2009. News from OPUS - A Collection of Multilingual Parallel Corpora with Tools and Interfaces. In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.) Recent Advances in Natural Language Processing (vol V), pages 237–248, John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
<author>J Iria</author>
<author>C Brewster</author>
<author>F Ciravegna</author>
</authors>
<title>A Comparative Evaluation of Term Recognition Algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of The sixth international conference on Language Resources and Evaluation, (LREC</booktitle>
<pages>2108--2113</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="20487" citStr="Zhang et al., 2008" startWordPosition="3164" endWordPosition="3167">ulture 76 10 3 English-to-Hindi EILMT 73 17 1 EMILLE 35 37 7 Launchpad 85 3 0 KDE4 74 5 0 Average 80.4% 17.0 % 2.5% Table 5: Distributions of unigram, bigram and trigram in the valid source term pairs (cf. second column in Table 3). 4.4 Comparison: Monolingual Terminology Extraction In this section we report the performance of our monolingual terminology extraction model (cf. Section 3.1) comparing with the performance of several state-of-the-art terminology extraction algorithms capable of recognising multiword terms. In order to extract monolingual multiword terms we used the JATE toolkit8 (Zhang et al., 2008). This toolkit first extracts candidate terms from a corpus using linguistic tools and then applies term extraction algorithms to recognise terms specific to the domain corpus. The JATE toolkit is currently available only for the English language. For evaluation purposes, we considered the source-side of the English-to-Hindi domain corpora. Algorithm Reference EILMT EMILLE Launchpad KDE4 LLC (Bilingual) cf. VST in Table 3 91 79 88 79 LLC 77 53 80 71 STF 46 04 54 44 ACTF 42 15 62 48 TF-IDF 50 36 45 17 Glossex Kozakov et al. (2004) 76 43 76 71 JK Justeson &amp; Katz (1995) 42 13 58 42 NC-Value Frant</context>
</contexts>
<marker>Zhang, Iria, Brewster, Ciravegna, 2008</marker>
<rawString>Z. Zhang, J. Iria, C. Brewster and F. Ciravegna. 2008. A Comparative Evaluation of Term Recognition Algorithms. In Proceedings of The sixth international conference on Language Resources and Evaluation, (LREC 2008), pages 2108–2113, Marrakech, Morocco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008187">
<title confidence="0.984301">
DiscoTK: Using Discourse Structure for Machine Translation Evaluation
</title>
<author confidence="0.969907">
Shafiq Joty Francisco Guzm´an Lluis M`arquez and Preslav Nakov
</author>
<affiliation confidence="0.959621">
ALT Research Group
Qatar Computing Research Institute — Qatar Foundation
</affiliation>
<email confidence="0.992954">
{sjoty,fguzman,lmarquez,pnakov}@qf.org.qa
</email>
<sectionHeader confidence="0.99731" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997885">
We present novel automatic metrics for
machine translation evaluation that use
discourse structure and convolution ker-
nels to compare the discourse tree of an
automatic translation with that of the hu-
man reference. We experiment with five
transformations and augmentations of a
base discourse tree representation based
on the rhetorical structure theory, and we
combine the kernel scores for each of them
into a single score. Finally, we add other
metrics from the ASIYA MT evaluation
toolkit, and we tune the weights of the
combination on actual human judgments.
Experiments on the WMT12 and WMT13
metrics shared task datasets show corre-
lation with human judgments that outper-
forms what the best systems that partici-
pated in these years achieved, both at the
segment and at the system level.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999638736842105">
The rapid development of statistical machine
translation (SMT) that we have seen in recent
years would not have been possible without au-
tomatic metrics for measuring SMT quality. In
particular, the development of BLEU (Papineni
et al., 2002) revolutionized the SMT field, al-
lowing not only to compare two systems in a
way that strongly correlates with human judg-
ments, but it also enabled the rise of discrimina-
tive log-linear models, which use optimizers such
as MERT (Och, 2003), and later MIRA (Watanabe
et al., 2007; Chiang et al., 2008) and PRO (Hop-
kins and May, 2011), to optimize BLEU, or an ap-
proximation thereof, directly. While over the years
other strong metrics such as TER (Snover et al.,
2006) and Meteor (Lavie and Denkowski, 2009)
have emerged, BLEU remains the de-facto stan-
dard, despite its simplicity.
Recently, there has been steady increase in
BLEU scores for well-resourced language pairs
such as Spanish-English and Arabic-English.
However, it was also observed that BLEU-like n-
gram matching metrics are unreliable for high-
quality translation output (Doddington, 2002;
Lavie and Agarwal, 2007). In fact, researchers al-
ready worry that BLEU will soon be unable to dis-
tinguish automatic from human translations.1 This
is a problem for most present-day metrics, which
cannot tell apart raw machine translation output
from a fully fluent professionally post-edited ver-
sion thereof (Denkowski and Lavie, 2012).
Another concern is that BLEU-like n-gram
matching metrics tend to favor phrase-based SMT
systems over rule-based systems and other SMT
paradigms. In particular, they are unable to cap-
ture the syntactic and semantic structure of sen-
tences, and are thus insensitive to improvement
in these aspects. Furthermore, it has been shown
that lexical similarity is both insufficient and not
strictly necessary for two sentences to convey
the same meaning (Culy and Riehemann, 2003;
Coughlin, 2003; Callison-Burch et al., 2006).
The above issues have motivated a large amount
of work dedicated to design better evaluation met-
rics. The Metrics task at the Workshop on Ma-
chine Translation (WMT) has been instrumental in
this quest. Below we present QCRI’s submission
to the Metrics task of WMT14, which consists of
the DiscoTK family of discourse-based metrics.
In particular, we experiment with five different
transformations and augmentations of a discourse
tree representation, and we combine the kernel
scores for each of them into a single score which
we call DISCOTKlight. Next, we add to the com-
bination other metrics from the ASIYA MT eval-
uation toolkit (Gim´enez and M`arquez, 2010), to
produce the DISCOTKparty metric.
</bodyText>
<footnote confidence="0.9961665">
1This would not mean that computers have achieved hu-
man proficiency; it would rather show BLEU’s inadequacy.
</footnote>
<page confidence="0.963455">
402
</page>
<bodyText confidence="0.913318466666667">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
Finally, we tune the relative weights of the met-
rics in the combination using human judgments
in a learning-to-rank framework. This proved
to be quite beneficial: the tuned version of the
DISCOTKparty metric was the best performing
metric in the WMT14 Metrics shared task.
The rest of the paper is organized as follows:
Section 2 introduces our basic discourse metrics
and the tree representations they are based on.
Section 3 describes our metric combinations. Sec-
tion 4 presents our experiments and results on
datasets from previous years. Finally, Section 5
concludes and suggests directions for future work.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="method">
2 Discourse-Based Metrics
</sectionHeader>
<bodyText confidence="0.999994209302326">
In our recent work (Guzm´an et al., 2014), we used
the information embedded in the discourse-trees
(DTs) to compare the output of an MT system to
a human reference. More specifically, we used
a state-of-the-art sentence-level discourse parser
(Joty et al., 2012) to generate discourse trees for
the sentences in accordance with the Rhetorical
Structure Theory (RST) of discourse (Mann and
Thompson, 1988). Then, we computed the simi-
larity between DTs of the human references and
the system translations using a convolution tree
kernel (Collins and Duffy, 2001), which efficiently
computes the number of common subtrees. Note
that this kernel was originally designed for syntac-
tic parsing, and the subtrees are subject to the con-
straint that their nodes are taken with all or none
of their children, i.e., if we take a direct descen-
dant of a given node, we must also take all siblings
of that descendant. This imposes some limitations
on the type of substructures that can be compared,
and motivates the enriched tree representations ex-
plained in subsections 2.1–2.4.
The motivation to compare discourse trees, is
that translations should preserve the coherence re-
lations. For example, consider the three discourse
trees (DTs) shown in Figure 1. Notice that the
Attribution relation in the reference translation is
also realized in the system translation in (b) but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis.
In (Guzm´an et al., 2014), we have shown that
discourse structure provides additional informa-
tion for MT evaluation, which is not captured by
existing metrics that use lexical, syntactic and se-
mantic information; thus, discourse should be con-
sidered when developing new rich metrics.
Here, we extend our previous work by devel-
oping metrics that are based on new representa-
tions of the DTs. In the remainder of this section,
we will focus on the individual DT representations
that we will experiment with; then, the following
section will describe the metric combinations and
tuning used to produce the DiscoTK metrics.
</bodyText>
<subsectionHeader confidence="0.917069">
2.1 DR-LEX1
</subsectionHeader>
<bodyText confidence="0.999870375">
Figure 2a shows our first representation of the DT.
The lexical items, i.e., words, constitute the leaves
of the tree. The words in an Elementary Discourse
Unit (EDU) are grouped under a predefined tag
EDU, to which the nuclearity status of the EDU
is attached: nucleus vs. satellite. Coherence re-
lations, such as Attribution, Elaboration, and En-
ablement, between adjacent text spans constitute
the internal nodes of the tree. Like the EDUs, the
nuclearity statuses of the larger discourse units are
attached to the relation labels. Notice that with
this representation the tree kernel can easily be ex-
tended to find subtree matches at the word level,
i.e., by including an additional layer of dummy
leaves as was done in (Moschitti et al., 2007). We
applied the same solution in our representations.
</bodyText>
<subsectionHeader confidence="0.97363">
2.2 DR-NOLEX
</subsectionHeader>
<bodyText confidence="0.9999522">
Our second representation DR-NOLEX (Figure 2b)
is a simple variation of DR-LEX1, where we ex-
clude the lexical items. This allows us to measure
the similarity between two translations in terms of
their discourse structures alone.
</bodyText>
<subsectionHeader confidence="0.864205">
2.3 DR-LEX2
</subsectionHeader>
<bodyText confidence="0.9999695625">
One limitation of DR-LEX1 and DR-NOLEX is that
they do not separate the structure, i.e., the skele-
ton, of the tree from its labels. Therefore, when
measuring the similarity between two DTs, they
do not allow the tree kernel to give partial credit
to subtrees that differ in labels but match in their
structures. DR-LEX2, a variation of DR-LEX1, ad-
dresses this limitation as shown in Figure 2c. It
uses predefined tags SPAN and EDU to build the
skeleton of the tree, and considers the nuclearity
and/or relation labels as properties (added as chil-
dren) of these tags. For example, a SPAN has two
properties, namely its nuclearity and its relation,
and an EDU has one property, namely its nucle-
arity. The words of an EDU are placed under the
predefined tag NGRAM.
</bodyText>
<page confidence="0.997204">
403
</page>
<figure confidence="0.996679642857143">
Elaboration ROOT
SPAN Attribution
Nucleus Satellite
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(a) A reference (human-written) translation.
Attribution
ROOT
SPANSatellite SPANNucleus
In Germany voices , the ECB should be the lender of last resort .
(b) A higher quality (system-generated) translation.
SPANROOT
In Germany the ECB should be for the creditors of last resort .
(c) A lower quality (system-generated) translation.
</figure>
<figureCaption confidence="0.9927285">
Figure 1: Three discourse trees for the translations of a source sentence: (a) the reference, (b) a higher
quality automatic translation, and (c) a lower quality automatic translation.
</figureCaption>
<subsectionHeader confidence="0.396443">
2.4 DR-LEX1.1 and DR-LEX2.1
</subsectionHeader>
<bodyText confidence="0.999919">
Although both DR-LEX1 and DR-LEX2 allow the
tree kernel to find matches at the word level, the
words are compared in a bag-of-words fashion,
i.e., if the trees share a common word, the ker-
nel will find a match regardless of its position in
the tree. Therefore, a word that has occurred in
an EDU with status Nucleus in one tree could be
matched with the same word under a Satellite in
the other tree. In other words, the kernel based
on these representations is insensitive to the nu-
clearity status and the relation labels under which
the words are matched. DR-LEX1.1, an exten-
sion of DR-LEX1, and DR-LEX2.1, an extension
of DR-LEX2, are sensitive to these variations at
the lexical level. DR-LEX1.1 (Figure 2d) and DR-
LEX2.1 (Figure 2e) propagate the nuclearity sta-
tuses and/or the relation labels to the lexical items
by including three more subtrees at the EDU level.
</bodyText>
<sectionHeader confidence="0.927387" genericHeader="method">
3 Metric Combination and Tuning
</sectionHeader>
<bodyText confidence="0.9995714">
In this section, we describe our Discourse Tree
Kernel (DiscoTK) metrics. We have two main
versions: DISCOTKlight, which combines the five
DR-based metrics, and DISCOTKparty, which fur-
ther adds the Asiya metrics.
</bodyText>
<subsectionHeader confidence="0.97276">
3.1 DISCOTKlight
</subsectionHeader>
<bodyText confidence="0.993288181818182">
In the previous section, we have presented several
discourse tree representations that can be used to
compare the output of a machine translation sys-
tem to a human reference. Each representation
stresses a different aspect of the discourse tree.
In order to make our estimations more robust,
we propose DISCOTKlight, a metric that takes ad-
vantage of all the previous discourse representa-
tions by linearly interpolating their scores. Here
are the processing steps needed to compute this
metric:
</bodyText>
<listItem confidence="0.97538376">
(i) Parsing: We parsed each sentence in order to
produce discourse trees for the human references
and for the outputs of the systems.
(ii) Tree enrichment/simplification: For each
sentence-level discourse tree, we generated the
five different tree representations: DR-NOLEX,
DR-LEX1, DR-LEX1.1, DR-LEX2, DR-LEX2.1.
(iii) Estimation: We calculated the per-sentence
similarity scores between tree representations of
the system hypothesis and the human reference
using the extended convolution tree kernel as de-
scribed in the previous section. To compute the
system-level similarity scores, we calculated the
average sentence-level similarity; note that this en-
sures that our metric is “the same” at the system
and at the segment level.
(iv) Normalization: In order to make the scores of
the different representations comparable, we per-
formed a min–max normalization2 for each met-
ric and for each language pair.
(v) Combination: Finally, for each sentence, we
computed DISCOTKlight as the average of the
normalized similarity scores of the different repre-
sentations. For system-level experiments, we per-
formed linear interpolation of system-level scores.
</listItem>
<footnote confidence="0.907064">
2Where x&apos; = (x − min)/(max − min).
</footnote>
<page confidence="0.996606">
404
</page>
<figure confidence="0.997778413793103">
ENABLEMENT-ROOT
EDU-NUCLEUS JOINT-SATELLITE
EDU-NUCLEUS
EDU-SATELLITE
ELABORATION-NUCLEUS
(b) DT for DR-NOLEX.
ELABORATION-NUCLEUS
EDU-NUCLEUS EDU-SATELLITE
and , last .. company , &amp;quot; added Cermak .
to better .. titles published
ELABORATION-NUCLEUS
EDU-NUCLEUS EDU-SATELLITE
&amp;quot; The new .. products
(a) DT for DR-LEX1.
SPAN
NUC REL EDU EDU
NUCLEUS ELABORATION NUC NGRAM NUC NGRAM
ELABORATION-NUCLEUS
EDU-NUCLEUS
LEX LEX:NUC LEX:REL
..
LEX:NUC:REL
NUCLEUS to better .. titles SATELLITE published to better .. to:N better:N .. to:ELAB better:ELAB .. to:N:ELAB better:N:ELAB ..
(c) DT for DR-LEX2. (d) DT for DR-LEX1.1.
SPAN
NUC REL EDU EDU
NUCLEUS ELABORATION NUC LEX LEX:NUC LEX:REL LEX:NUC:REL NUC LEX LEX:NUC ..
NUCLEUS to better .. to:N better:N .. to:ELAB better:ELAB .. to:N:ELAB better:N:ELAB .. SATELLITE published published:S
(e) DT for DR-LEX2.1.
</figure>
<figureCaption confidence="0.772567">
Figure 2: Five different representations of the discourse tree (DT) for the sentence “The new organisa-
tional structure will also allow us to enter the market with a joint offer of advertising products, to better
link the creation of content for all the titles published and, last but not least, to continue to streamline
significantly the business management of the company,” added Cermak. Note that to avoid visual clutter,
(b)–(e) show alternative representations only for the highlighted subtree in (a).
</figureCaption>
<subsectionHeader confidence="0.952496">
3.2 DISCOTKparty
</subsectionHeader>
<bodyText confidence="0.999816523809524">
One of the weaknesses of the above discourse-
based metrics is that they use unigram lexical
information, which does not capture reordering.
Thus, in order to make more informed and ro-
bust estimations, we extended DISCOTKlight with
the composing metrics of the ASIYA’s ULC met-
ric (Gim´enez and M`arquez, 2010), which is a uni-
form linear combination of twelve individual met-
rics and was the best-performing metric at the sys-
tem and at the segment levels at the WMT08 and
WMT09 metrics tasks.
In order to compute the individual metrics from
ULC, we used the ASIYA toolkit,3 but we de-
parted from ASIYA’s ULC by replacing TER
and Meteor with newer versions thereof that take
into account synonymy lookup and paraphras-
ing (‘TERp-A’ and ‘Meteor-pa’ in ASIYA’s ter-
minology). We then combined the five compo-
nents in DISCOTKlight and the twelve individ-
ual metrics from ULC; we call this combination
DISCOTKparty.
</bodyText>
<footnote confidence="0.736421">
3http://nlp.lsi.upc.edu/asiya/
</footnote>
<bodyText confidence="0.97351247826087">
We combined the scores using linear interpola-
tion in two different ways:
(i) Uniform combination of min-max normalized
scores at the segment level. We obtained system-
level scores by computing the average over the
segment scores.
(ii) Trained interpolation at the sentence level.
We determined the interpolation weights for the
above-described combination of 5+12 = 17 met-
rics using a pairwise learning-to-rank framework
and classification with logistic regression, as we
had done in (Guzm´an et al., 2014). We obtained
the final test-time sentence-level scores by pass-
ing the interpolated raw scores through a sigmoid
function. In contrast, for the final system-level
scores, we averaged the per-sentence interpolated
raw scores.
We also tried to learn the interpolation weights
at the system level, experimenting with both re-
gression and classification. However, the amount
of data available for this type of training was
small, and the learned weights did not perform sig-
nificantly better than the uniform combination.
</bodyText>
<page confidence="0.997926">
405
</page>
<subsectionHeader confidence="0.999611">
3.3 Post-processing
</subsectionHeader>
<bodyText confidence="0.9846064">
Discourse-based metrics, especially DR-NOLEX,
tend to produce many ties when there is not
enough information to do complete discourse
analysis. This contributes to lower τ scores for
DISCOTKlight. To alleviate this issue, we used a
simple tie-breaking strategy, in which ties between
segment scores for different systems are resolved
by using perturbations proportional to the global
system-level scores produced by the same metric,
i.e.,x&amp;quot;eg=se s
sys = xsys + E ∗ �s xsys.
cally chosen to avoid collisions with scores not in-
volved in the tie. This post-processing is not part
of the metric; it is only applied to our segment-
level submission to the WMT’14 metrics task.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="evaluation">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.997263720930233">
In this section, we present some of our experi-
ments to decide on the best DiscoTK metric vari-
ant and tuning set. For tuning, testing and compar-
ison, we worked with some of the datasets avail-
able from previous WMT metrics shared tasks,
i.e., 2011, 2012 and 2013. From previous ex-
periments (Guzm´an et al., 2014), we know that
the tuned metrics perform very well on cross-
validation for the same-year dataset. We further
know that tuning can be performed by concatenat-
ing data from all the into-English language pairs,
which yields better results than training separately
by language pair. For the WMT14 metrics task,
we investigated in more depth whether the tuned
metrics generalize well to new datasets. Addition-
ally, we tested the effect of concatenating datasets
from different years.
Table 1 shows the main results of our experi-
ments with the DiscoTK metrics. We evaluated
the performance of the metrics on the WMT12
and WMT13 datasets both at the segment and the
system level, and we used WMT11 as an addi-
tional tuning dataset. We measured the perfor-
mance of the metrics in terms of correlation with
human judgements. At the segment level, we eval-
uated using Kendall’s Tau (τ), recalculated follow-
ing the WMT14 official Kendall’s Tau implemen-
tation. At the system level, we used Spearman’s
rank correlation (ρ) and Pearson’s correlation co-
efficient (r). In all cases, we averaged the results
over all into-English language pairs. The symbol
‘∅’ represents the untuned versions of our metrics,
i.e., applying a uniform linear combination of the
individual metrics.
We trained the tuned versions of the DiscoTK
measures using different datasets (WMT11,
WMT12 and WMT13) in order to study across-
corpora generalization and the effect of training
dataset size. The symbol ‘+’ stands for concatena-
tion of datasets. We trained the tuned versions at
the segment level using Maximum Entropy clas-
sifiers for pairwise ranking (cf. Section 3). For
the sake of comparison, the first group of rows
contains the results of the best-performing met-
rics at the WMT12 and WMT13 metrics shared
tasks and the last group of rows contains the re-
sults of the ASIYA combination of metrics, i.e.,
DISCOTKparty without the discourse components.
Several conclusions can be drawn from Table 1.
First, DISCOTKparty is better than DISCOTKlight
in all settings, indicating that the discourse-based
metrics are very well complemented by the hetero-
geneous metric set from ASIYA. DISCOTKlight
achieves competitive scores at the system level
(which would put the metric among the best par-
ticipants in WMT12 and WMT13); however, as
expected, it is not robust enough at the segment
level. On the other hand, the tuned versions of
DISCOTKparty are very competitive and improve
over the already strong ASIYA in each configu-
ration both at the segment- and the system-level.
The improvements are small but consistent, show-
ing that using discourse increases the correlation
with human judgments.
Focusing on the results at the segment level, it
is clear that the tuned versions offer an advantage
over the simple uniform linear combinations. In-
terestingly, for the tuned variants, given a test set,
the results are consistent across tuning sets, ruling
out over-fitting; this shows that the generalization
is very good. This result aligns well with what
we observed in our previous studies (Guzm´an et
al., 2014). Learning with more data (WMT11+12
or WMT12+13) does not seem to help much,
but it does not hurt performance either. Overall,
the τ correlation results obtained with the tuned
DISCOTKparty metric are much better than the
best results of any participant metrics at WMT12
and WMT13 (20.1% and 9.5% relative improve-
ment, respectively).
At the system level, we observe that tuning over
the DISCOTKlight metric is not helpful (results
are actually slightly lower), while tuning the more
complex DISCOTKparty metric yields slightly bet-
ter results.
Here, E is automati-
</bodyText>
<page confidence="0.996909">
406
</page>
<table confidence="0.999738833333333">
Segment Level System Level
WMT12 WMT13 WMT12 WMT13
Metric Tuning T T p r p r
SEMPOS na – – 0.902 0.922 – –
SPEDE07PP na 0.254 – – – – –
METEOR-WMT13 na – 0.264 – – 0.935 0.950
0 0.171 0.162 0.884 0.922 0.880 0.911
WMT11 0.207 0.201 0.860 0.872 0.890 0.909
DISCOTKlight WMT12 – 0.200 – – 0.889 0.910
WMT13 0.206 – 0.865 0.871 – –
WMT11+12 – 0.197 – – 0.890 0.910
WMT11+13 0.207 – 0.865 0.871 – –
0 0.257 0.231 0.907 0.915 0.941 0.928
WMT11 0.302 0.282 0.915 0.940 0.934 0.946
DISCOTKparty WMT12 – 0.284 – – 0.936 0.940
WMT13 0.305 – 0.912 0.935 – –
WMT11+12 – 0.289 – – 0.936 0.943
WMT11+13 0.304 – 0.912 0.934 – –
0 0.273 0.252 0.899 0.909 0.932 0.922
WMT11 0.301 0.279 0.913 0.935 0.934 0.944
ASIYA WMT12 – 0.277 – – 0.932 0.938
WMT13 0.303 – 0.908 0.932 – –
WMT11+12 – 0.277 – – 0.934 0.940
WMT11+13 0.303 – 0.908 0.933 – –
</table>
<tableCaption confidence="0.970085">
Table 1: Evaluation results on WMT12 and WMT13 datasets at segment and system level for the main
combined DiscoTK measures proposed in this paper.
</tableCaption>
<bodyText confidence="0.999337689655172">
The scores of our best metric are higher than
those of the best participants in WMT12 and
WMT13, according to Spearman’s p, which was
the official metric in those years. Overall, our met-
rics are comparable to the state-of-the-art at the
system level. The differences between Spearman’s
p and Pearson’s r coefficients are not dramatic,
with r values being always higher than p.
Given the above results, we submitted the fol-
lowing runs to the WMT14 Metrics shared task:
(i) DISCOTKparty tuned on the concatenation
of datasets WMT11+12+13, as our primary run;
(ii) Untuned DISCOTKparty, to verify that we are
not over-fitting the training set; and (iii) Untuned
DISCOTKlight, to see the performance of a metric
using discourse structures and word unigrams.
The results for the WMT14 Metrics shared task
have shown that our primary run, DISCOTKparty
tuned, was the best-performing metric both at the
segment- and at the system-level (Mach´aˇcek and
Bojar, 2014). This metric yielded significantly
better results than its untuned counterpart, con-
firming the importance of weight tuning and the
absence of over-fitting during tuning. Finally, the
untuned DISCOTKlight achieved relatively com-
petitive, albeit slightly worse results for all lan-
guage pairs, except for Hindi-English, where sys-
tem translations resembled a “word salad”, and
were very hard to discourse-parse accurately.
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999981">
We have presented experiments with novel auto-
matic metrics for machine translation evaluation
that take discourse structure into account. In par-
ticular, we used RST-style discourse parse trees,
which we compared using convolution kernels.
We further combined these kernels with metrics
from ASIYA, also tuning the weights. The re-
sulting DISCOTKparty tuned metric was the best-
performing at the segment- and system-level at the
WMT14 metrics task.
In an internal evaluation on the WMT12 and
WMT13 metrics datasets, this tuned combina-
tion showed correlation with human judgments
that outperforms the best systems that participated
in these shared tasks. The discourse-only met-
ric ranked near the top at the system-level for
WMT12 and WMT13; however, it is weak at the
segment-level since it is sensitive to parsing errors,
and most sentences have very little internal dis-
course structure.
In the future, we plan to work on an inte-
grated representation of syntactic, semantic and
discourse-based tree structures, which would al-
low us to design evaluation metrics based on more
fine-grained features, and would also allow us to
train such metrics using kernel methods. Further-
more, we want to make use of discourse parse in-
formation beyond the sentence level.
</bodyText>
<page confidence="0.997613">
407
</page>
<sectionHeader confidence="0.996295" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856943396226">
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU
in machine translation research. In Proceedings of
the Eleventh Conference of the European Chapter
of the Association for Computational Linguistics,
EACL’06, pages 249–256, Trento, Italy.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP’08, pages 224–233, Honolulu,
Hawaii.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Neural Informa-
tion Processing Systems, NIPS’01, pages 625–632,
Vancouver, Canada.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of the Machine Translation Summit
IX, MT Summit’03, pages 23–27, New Orleans, LA,
USA.
Christopher Culy and Susanne Riehemann. 2003. The
limits of n-gram translation evaluation metrics. In
Proceedings of the Machine Translation Summit IX,
MT Summit’03, pages 1–8, New Orleans, LA, USA.
Michael Denkowski and Alon Lavie. 2012. Chal-
lenges in predicting machine translation utility for
human post-editors. In Proceedings of the Tenth
Conference of the Association for Machine Trans-
lation in the Americas, AMTA’12, pages 40–49, San
Diego, CA, USA.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT’02, pages 138–145, San
Francisco, CA, USA.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Linguistic
Measures for Automatic Machine Translation Eval-
uation. Machine Translation, 24(3–4):77–86.
Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and
Preslav Nakov. 2014. Using discourse structure
improves machine translation evaluation. In Pro-
ceedings of 52nd Annual Meeting of the Association
for Computational Linguistics, ACL’14, Baltimore,
MD, USA.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP’11, pages 1352–1362, Edinburgh, Scot-
land, UK.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceed-
ings of the Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL’12,
pages 904–915, Jeju Island, Korea.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In
Proceedings of the Second Workshop on Statisti-
cal Machine Translation, WMT’07, pages 228–231,
Prague, Czech Republic.
Alon Lavie and Michael Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2-3):105–115.
Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2014. Results of
the WMT14 Metrics Shared Task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, WMT’14, Baltimore, MD, USA.
William Mann and Sandra Thompson. 1988. Rhetor-
ical Structure Theory: Toward a Functional Theory
of Text Organization. Text, 8(3):243–281.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, ACL’07, pages 776–783, Prague, Czech
Republic.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, ACL’03, pages 160–167, Sap-
poro, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Association
for Computational Linguistics, ACL’02, pages 311–
318, Philadelphia, PA, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the Seventh Biennial Con-
ference of the Association for Machine Translation
in the Americas, AMTA’06, pages 223–231, Cam-
bridge, MA, USA.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, EMNLP-CoNLL’07, pages
764–773, Prague, Czech Republic.
</reference>
<page confidence="0.997892">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806924">
<title confidence="0.999339">DiscoTK: Using Discourse Structure for Machine Translation Evaluation</title>
<author confidence="0.998776">Joty Francisco Guzm´an Lluis M`arquez</author>
<affiliation confidence="0.9100005">ALT Research Qatar Computing Research Institute — Qatar</affiliation>
<abstract confidence="0.998521095238095">We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with five transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other from the evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics, EACL’06,</booktitle>
<pages>249--256</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="3010" citStr="Callison-Burch et al., 2006" startWordPosition="461" endWordPosition="464"> translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call DISCOTKlight. Next, we add to the combination other metrics from</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics, EACL’06, pages 249–256, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’08,</booktitle>
<pages>224--233</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="1606" citStr="Chiang et al., 2008" startWordPosition="245" endWordPosition="248">se years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers al</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’08, pages 224–233, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Neural Information Processing Systems, NIPS’01,</booktitle>
<pages>625--632</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5198" citStr="Collins and Duffy, 2001" startWordPosition="801" endWordPosition="804"> suggests directions for future work. 2 Discourse-Based Metrics In our recent work (Guzm´an et al., 2014), we used the information embedded in the discourse-trees (DTs) to compare the output of an MT system to a human reference. More specifically, we used a state-of-the-art sentence-level discourse parser (Joty et al., 2012) to generate discourse trees for the sentences in accordance with the Rhetorical Structure Theory (RST) of discourse (Mann and Thompson, 1988). Then, we computed the similarity between DTs of the human references and the system translations using a convolution tree kernel (Collins and Duffy, 2001), which efficiently computes the number of common subtrees. Note that this kernel was originally designed for syntactic parsing, and the subtrees are subject to the constraint that their nodes are taken with all or none of their children, i.e., if we take a direct descendant of a given node, we must also take all siblings of that descendant. This imposes some limitations on the type of substructures that can be compared, and motivates the enriched tree representations explained in subsections 2.1–2.4. The motivation to compare discourse trees, is that translations should preserve the coherence</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Neural Information Processing Systems, NIPS’01, pages 625–632, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Coughlin</author>
</authors>
<title>Correlating automated and human assessments of machine translation quality.</title>
<date>2003</date>
<booktitle>In Proceedings of the Machine Translation Summit IX, MT Summit’03,</booktitle>
<pages>23--27</pages>
<location>New Orleans, LA, USA.</location>
<contexts>
<context position="2980" citStr="Coughlin, 2003" startWordPosition="459" endWordPosition="460">part raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call DISCOTKlight. Next, we add to the </context>
</contexts>
<marker>Coughlin, 2003</marker>
<rawString>Deborah Coughlin. 2003. Correlating automated and human assessments of machine translation quality. In Proceedings of the Machine Translation Summit IX, MT Summit’03, pages 23–27, New Orleans, LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Culy</author>
<author>Susanne Riehemann</author>
</authors>
<title>The limits of n-gram translation evaluation metrics.</title>
<date>2003</date>
<booktitle>In Proceedings of the Machine Translation Summit IX, MT Summit’03,</booktitle>
<pages>1--8</pages>
<location>New Orleans, LA, USA.</location>
<contexts>
<context position="2964" citStr="Culy and Riehemann, 2003" startWordPosition="455" endWordPosition="458">trics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call DISCOTKlight. Next</context>
</contexts>
<marker>Culy, Riehemann, 2003</marker>
<rawString>Christopher Culy and Susanne Riehemann. 2003. The limits of n-gram translation evaluation metrics. In Proceedings of the Machine Translation Summit IX, MT Summit’03, pages 1–8, New Orleans, LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Challenges in predicting machine translation utility for human post-editors.</title>
<date>2012</date>
<booktitle>In Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas, AMTA’12,</booktitle>
<pages>40--49</pages>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="2492" citStr="Denkowski and Lavie, 2012" startWordPosition="382" endWordPosition="385">ite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design bette</context>
</contexts>
<marker>Denkowski, Lavie, 2012</marker>
<rawString>Michael Denkowski and Alon Lavie. 2012. Challenges in predicting machine translation utility for human post-editors. In Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas, AMTA’12, pages 40–49, San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research, HLT’02,</booktitle>
<pages>138--145</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2155" citStr="Doddington, 2002" startWordPosition="332" endWordPosition="333"> 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensiti</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research, HLT’02, pages 138–145, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation,</title>
<date>2010</date>
<pages>24--3</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation, 24(3–4):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Guzm´an</author>
<author>Shafiq Joty</author>
<author>Llu´ıs M`arquez</author>
<author>Preslav Nakov</author>
</authors>
<title>Using discourse structure improves machine translation evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, ACL’14,</booktitle>
<location>Baltimore, MD, USA.</location>
<marker>Guzm´an, Joty, M`arquez, Nakov, 2014</marker>
<rawString>Francisco Guzm´an, Shafiq Joty, Llu´ıs M`arquez, and Preslav Nakov. 2014. Using discourse structure improves machine translation evaluation. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics, ACL’14, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’11,</booktitle>
<pages>1352--1362</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="1638" citStr="Hopkins and May, 2011" startWordPosition="251" endWordPosition="255"> segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon </context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP’11, pages 1352–1362, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
</authors>
<title>A Novel Discriminative Framework for Sentence-Level Discourse Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12,</booktitle>
<pages>904--915</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4900" citStr="Joty et al., 2012" startWordPosition="755" endWordPosition="758">of the paper is organized as follows: Section 2 introduces our basic discourse metrics and the tree representations they are based on. Section 3 describes our metric combinations. Section 4 presents our experiments and results on datasets from previous years. Finally, Section 5 concludes and suggests directions for future work. 2 Discourse-Based Metrics In our recent work (Guzm´an et al., 2014), we used the information embedded in the discourse-trees (DTs) to compare the output of an MT system to a human reference. More specifically, we used a state-of-the-art sentence-level discourse parser (Joty et al., 2012) to generate discourse trees for the sentences in accordance with the Rhetorical Structure Theory (RST) of discourse (Mann and Thompson, 1988). Then, we computed the similarity between DTs of the human references and the system translations using a convolution tree kernel (Collins and Duffy, 2001), which efficiently computes the number of common subtrees. Note that this kernel was originally designed for syntactic parsing, and the subtrees are subject to the constraint that their nodes are taken with all or none of their children, i.e., if we take a direct descendant of a given node, we must a</context>
</contexts>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A Novel Discriminative Framework for Sentence-Level Discourse Analysis. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’12, pages 904–915, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, WMT’07,</booktitle>
<pages>228--231</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2181" citStr="Lavie and Agarwal, 2007" startWordPosition="334" endWordPosition="337">MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, WMT’07, pages 228–231, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="1811" citStr="Lavie and Denkowski, 2009" startWordPosition="281" endWordPosition="284">sible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fu</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23(2-3):105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT14 Metrics Shared Task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT’14,</booktitle>
<location>Baltimore, MD, USA.</location>
<marker>Mach´acek, Bojar, 2014</marker>
<rawString>Matou&amp;quot;s Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2014. Results of the WMT14 Metrics Shared Task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, WMT’14, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="5042" citStr="Mann and Thompson, 1988" startWordPosition="776" endWordPosition="779">ection 3 describes our metric combinations. Section 4 presents our experiments and results on datasets from previous years. Finally, Section 5 concludes and suggests directions for future work. 2 Discourse-Based Metrics In our recent work (Guzm´an et al., 2014), we used the information embedded in the discourse-trees (DTs) to compare the output of an MT system to a human reference. More specifically, we used a state-of-the-art sentence-level discourse parser (Joty et al., 2012) to generate discourse trees for the sentences in accordance with the Rhetorical Structure Theory (RST) of discourse (Mann and Thompson, 1988). Then, we computed the similarity between DTs of the human references and the system translations using a convolution tree kernel (Collins and Duffy, 2001), which efficiently computes the number of common subtrees. Note that this kernel was originally designed for syntactic parsing, and the subtrees are subject to the constraint that their nodes are taken with all or none of their children, i.e., if we take a direct descendant of a given node, we must also take all siblings of that descendant. This imposes some limitations on the type of substructures that can be compared, and motivates the e</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William Mann and Sandra Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL’07,</booktitle>
<pages>776--783</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7475" citStr="Moschitti et al., 2007" startWordPosition="1175" endWordPosition="1178">words in an Elementary Discourse Unit (EDU) are grouped under a predefined tag EDU, to which the nuclearity status of the EDU is attached: nucleus vs. satellite. Coherence relations, such as Attribution, Elaboration, and Enablement, between adjacent text spans constitute the internal nodes of the tree. Like the EDUs, the nuclearity statuses of the larger discourse units are attached to the relation labels. Notice that with this representation the tree kernel can easily be extended to find subtree matches at the word level, i.e., by including an additional layer of dummy leaves as was done in (Moschitti et al., 2007). We applied the same solution in our representations. 2.2 DR-NOLEX Our second representation DR-NOLEX (Figure 2b) is a simple variation of DR-LEX1, where we exclude the lexical items. This allows us to measure the similarity between two translations in terms of their discourse structures alone. 2.3 DR-LEX2 One limitation of DR-LEX1 and DR-NOLEX is that they do not separate the structure, i.e., the skeleton, of the tree from its labels. Therefore, when measuring the similarity between two DTs, they do not allow the tree kernel to give partial credit to subtrees that differ in labels but match </context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL’07, pages 776–783, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL’03,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1545" citStr="Och, 2003" startWordPosition="236" endWordPosition="237">orms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL’03, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, ACL’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="1306" citStr="Papineni et al., 2002" startWordPosition="193" endWordPosition="196">inally, we add other metrics from the ASIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, ACL’02, pages 311– 318, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Seventh Biennial Conference of the Association for Machine Translation in the Americas, AMTA’06,</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1772" citStr="Snover et al., 2006" startWordPosition="275" endWordPosition="278">ent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart r</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the Seventh Biennial Conference of the Association for Machine Translation in the Americas, AMTA’06, pages 223–231, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’07,</booktitle>
<pages>764--773</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1584" citStr="Watanabe et al., 2007" startWordPosition="241" endWordPosition="244">hat participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). I</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL’07, pages 764–773, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
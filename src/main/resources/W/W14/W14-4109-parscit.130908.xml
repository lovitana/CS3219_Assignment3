<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001135">
<title confidence="0.985275">
A Process for Predicting MOOC Attrition
</title>
<author confidence="0.883137">
Mike Sharkey
</author>
<affiliation confidence="0.682401">
President
</affiliation>
<address confidence="0.768983">
Blue Canary
Chandler, AZ USA
</address>
<email confidence="0.998091">
mike@bluecanarydata.com
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903833333333">
The goal of this shared task was to predict
attrition in a MOOC through use of the
data and logs generated by the course.
Our approach to the task reinforces the
idea that the process of gathering and
structuring the data is more important (and
more time consuming) than the predictive
model itself. The result of the analysis
was that a subset of 15 different data fea-
tures did a sufficiently good job at predict-
ing whether or not a student would exhibit
any activity in the following week.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999749">
Blue Canary is a higher education analytics com-
pany located in Chandler, Arizona USA. The
company has extensive experience in dealing with
academic course/enrollment/retention data and is
proud to collaborate with other researchers on the
EMNLP 2014 shared task. The goal of the task is
to use data from one MOOC, create a model to
predict course attrition, and then apply that model
to five other MOOCs in order to observe the effi-
cacy of the model across courses. The goal of this
paper is to document the process that Blue Canary
went through in order to generate the model.
</bodyText>
<sectionHeader confidence="0.909897" genericHeader="introduction">
2 Understanding the Problem
</sectionHeader>
<bodyText confidence="0.999353636363636">
In order to successfully complete a task such as
this, the team needed the right context to the prob-
lem. The context for this particular challenge (us-
ing MOOC data to predict attrition) was very fa-
miliar to the Blue Canary team. First, the team
has developed retention-oriented predictive mod-
els for a number of institutions in the past. This
experience was vital. Second, the team has
worked with data at scale. The MOOC course had
20,000 enrolled students with a log file that gen-
erated 1.6 million rows of data. The Blue Canary
</bodyText>
<sectionHeader confidence="0.64850325" genericHeader="method">
Robert Sanders
Sr. Software Engineer
Clairvoyant, LLC
Chandler, AZ USA
</sectionHeader>
<email confidence="0.810224">
robert.sanders@clairvoyantsoft.com
</email>
<bodyText confidence="0.998282444444444">
team has experience working with a large online
university that had over 300,000 students generat-
ing millions of rows of data on a daily basis.
Lastly, all of the team members have participated
in at least one MOOC, so the processes and inter-
actions associated with such a course are known.
The combination of all of these factors gave the
Blue Canary team the necessary context to tackle
the attrition problem from the ground up.
</bodyText>
<sectionHeader confidence="0.968189" genericHeader="method">
3 Approach to the Problem
</sectionHeader>
<bodyText confidence="0.999909">
As with other such data initiatives, the process is
a stepwise iterative one. Each step and iteration
provides more insight, allowing the team to refine
the prediction.
</bodyText>
<subsectionHeader confidence="0.997963">
3.1 Step 1: Feature Extraction
</subsectionHeader>
<bodyText confidence="0.9998838">
Feature extraction is the process of defining the
independent variables (or inputs) for the predic-
tive model. This is arguably the most important
step in the process of developing a predictive
model. It requires a deep understanding of the
source data from a technical side as well as a con-
textual understanding of how the data relate to the
front-end user experience.
Blue Canary used two techniques for feature ex-
traction. The first was experience. Having looked
at course activity data and developed predictive
models for other courses, we knew the kinds of
features that would likely have an impact on the
prediction. This experience gave us simplistic
features like “number of videos watched” and “to-
tal minutes spent in class” to more nuanced fea-
tures like “attempted quiz without referring to
other materials”.
The second technique was using visualizations
to explore data relationships. The team used the
Tableau visualization tool to ingest course activity
data and map it across users &amp; weeks. Looking at
these relationships visually helped to determine if
we should include the features in the modeling or
not.
</bodyText>
<page confidence="0.941163">
50
</page>
<note confidence="0.371096">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 50–54,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.997595">
3.2 Step 2: Define Outcome/Prediction
</subsectionHeader>
<bodyText confidence="0.999971555555556">
Once the list of features have been developed,
next step is to define exactly what it is we are pre-
dicting. At a high level, it sounds easy – will the
student retain in the class? From a data perspec-
tive, though, we need to define what it means to
retain. Does it mean that the student submitted the
assignment for the week? Watched a video?
Simply logged in? Zeroing in on a reliable defi-
nition of retention is a part of the process.
</bodyText>
<subsectionHeader confidence="0.998624">
3.3 Step 3: Run the Predictive Model
</subsectionHeader>
<bodyText confidence="0.999991666666667">
With the input and output data in place, the team
needs to run a model to derive a prediction. Blue
Canary has consistently used machine learning
techniques (as opposed to statistical modeling).
As Bogard (2011) alludes to in a blog post com-
paring the two approaches, Blue Canary’s tech-
nical expertise combined with an unknown under-
lying relationship make machine learning our pre-
ferred method of analysis. For this analysis, Blue
Canary implemented a random forest method us-
ing the SciKit python toolset (http://scikit-
learn.org/).
</bodyText>
<subsectionHeader confidence="0.979147">
3.4 Step 4: Observe/Validate/Iterate
</subsectionHeader>
<bodyText confidence="0.999741272727273">
The last step in the process is to observe the out-
comes of the modeling, validate the results (both
quantitatively and qualitatively) and iterate to im-
prove. When looking at the modeling results, we
focused on accuracy. More specifically, we fo-
cused on the true positive rate (recall) and the true
negative rate individually. The combination of
these components equal the accuracy of the
model, but we thought it was important to look at
both since the application of any such solution
would involve treatments for both parties.
</bodyText>
<table confidence="0.998519428571429">
Value Definition
True Positive # predicted to retain /
# actually retained
True Negative # predicted to attrite /
# actual attrition
Accuracy (True positive + True
negative) / population
</table>
<tableCaption confidence="0.999837">
Table 1: Definition of model accuracy values
</tableCaption>
<subsectionHeader confidence="0.987716">
3.5 Acknowledging Prior Research
</subsectionHeader>
<bodyText confidence="0.999979272727273">
It should be noted that Blue Canary has stood on
the shoulders of others who have tackled similar
problems in the past. Our choice for analytical
methods and features has been inspired by earlier
predictive projects like Purdue’s Course Signals
(Arnold and Pistilli, 2012) and research done at
American Public University (Boston et. al., 2011).
We also referenced contemporary MOOC re-
search that explored the descriptive (Breslow et.
al., 2013), predictive (Taylor et. al., 2014), and so-
cial (Rosé et. al., 2014) contributors to attrition.
</bodyText>
<sectionHeader confidence="0.943808" genericHeader="method">
4 Predicting Attrition for PSY-001
</sectionHeader>
<bodyText confidence="0.999092428571429">
The course in question was from a 2013 Georgia
Tech/Coursera MOOC called “Introduction to
Psychology as a Science”. Blue Canary executed
seven iterative steps as explained in the previous
section. At the end we came up with a model that
used 15 features to predict retention and attrition
at an 88% accuracy rate.
</bodyText>
<subsectionHeader confidence="0.982891">
4.1 Iteration 1: Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999478">
The first iteration didn’t result in any prediction.
The goal was to explore the data and extract an
initial set of features for processing. We also cre-
ated our training, testing, and hold back data using
a 70/15/15 split. Table 2 lists the features we ini-
tially extracted from the activity data.
</bodyText>
<listItem confidence="0.998437384615385">
• id
• user_id
• username
• week_id
• week_num
• week_start_date
• week_end_date
• session_count
• url_wiki_edit_count
• url_wiki_view_count
• url_quiz_count
• url_lecture_count
• url_forum_count
• is_english
• ip_count
• most_common_browser
• most_common_browser_date
•
browser_count
• unique_quizzes_attempted
• total_quiz_attempts
• average_attempts_per_quiz
• videos_accessed_count
• average_video_per_session
• did_peer_review
• actually_attended
</listItem>
<tableCaption confidence="0.995701">
Table 2: Initial list of features
</tableCaption>
<page confidence="0.997218">
51
</page>
<bodyText confidence="0.99974125">
These features were very basic. We didn’t spend
much time on more advanced features. The goal
of this first was simply to lay the foundation for
our data analysis pipeline.
</bodyText>
<subsectionHeader confidence="0.923685">
4.2 Iteration 2: Test Analytical API’s
</subsectionHeader>
<bodyText confidence="0.9999168">
With a bulk of the features in place, our next goal
was to connect the machine learning toolset to the
pipeline. We used Weka (http://www.cs.wai-
kato.ac.nz/ml/weka/) since the team had some ex-
perience with the tool. Since our approach was to
construct the pipeline as a smooth-running appli-
cation, we utilized the Weka API’s to feed data in
and get results out.
Unfortunately, we ran into technical problems
with the API’s and got out of memory exception
errors. We were unable to troubleshoot and de-
cided to move on to another toolset. In addition,
though, we added more features, mainly from
parsing the URL strings in the access log files (Ta-
ble 3).
</bodyText>
<listItem confidence="0.997043571428571">
• event_count
• total_minutes_spent
• url_quiz_submits_count
• url_quiz_actual_submits_count
• url_quiz_percent_of_actual_submits
• url_quiz_at-
tempt_in_more_than_one_session
• url_quiz_retry
• url_quiz_attempt_but_no_submit
• url_quiz_submit_no_help
• url_human_grading_count
• url_forum_search_count
• url_class_preferences_count
• url_signature_count
</listItem>
<tableCaption confidence="0.987958">
Table 3: URL features added
</tableCaption>
<subsectionHeader confidence="0.950651">
4.3 Iteration 3: Too Good to be True
</subsectionHeader>
<bodyText confidence="0.9999711">
We switched to SciKit as our analytical tool of
choice, but we still used the Random Forest
method. We ran our first analysis and got the cor-
responding accuracy rates. As explained in sec-
tion 3.4, we produce accuracy rates for ‘False’
(correctly predicting that the student won’t attend
next week), ‘True’ (correctly predicting that the
student will attend next week) and ‘Average’ (ac-
curacy – the weighted average of False and True).
The results for our first run were as follows:
</bodyText>
<table confidence="0.63026225">
Measure Rate
False 99%
True 87%
Accuracy 96%
</table>
<bodyText confidence="0.999471368421053">
The team was skeptical about such high accu-
racy rates, especially given that it was our first
run. We suspected that there was some sort of
leakage – information about the prediction field
may have leaked into one of the features. That
suspicion was confirmed when we dug deeper into
the model.
The predominant feature was “is_english”. We
looked at the user agent data in the activity logs
and parsed the language parameter to determine if
the web browser language was set to English or
not. It turns out that when there was no activity
for the week, we populated this field with null val-
ues. Since the majority of the students had Eng-
lish as their language, the model was seeing
“is_english” = TRUE when there was activity and
“is_english” = FALSE when there wasn’t activity.
This was a great example of the kinds of errors
one finds early on in the analysis.
</bodyText>
<subsectionHeader confidence="0.983945">
4.4 Iteration 4: First Real Model
</subsectionHeader>
<bodyText confidence="0.924244133333333">
For the next iteration, we fixed the “is_english”
field and ran the model again. This run was our
first valid predictive model for the dataset and the
results were:
Measure Rate
False 92%
True 55%
Accuracy 89%
Note that we are doing a very good job at predict-
ing students who won’t attend next week. This is
due to the fact that there are a large number of stu-
dents don’t attend. We estimated that about
20,000 students signed up for the class, 11,000 of
them showed any activity at all, and less than
3,000 completed the course.
</bodyText>
<subsectionHeader confidence="0.987852">
4.5 Iteration 5: Defining the Outcome
</subsectionHeader>
<bodyText confidence="0.999978444444444">
For experimentation purposes, we wanted to see
if changing the definition of “attending” would
have any effect on the modeling. Our original def-
inition of attending was that there were ANY user
actions in the data (viewing a page, posting a dis-
cussion item, taking a quiz, etc.). We decided to
add variations to that definition such as “viewing
at least one lecture”, “submitting at least one
quiz”, or “will never attend again” (as opposed to
</bodyText>
<page confidence="0.998917">
52
</page>
<tableCaption confidence="0.8956915">
just not attending next week). The table below is
a sampling of some of the results we generated:
</tableCaption>
<table confidence="0.993644">
Measure Out_i Out_a Out_b Out_c
False 92% 94% 97% 87%
True 55% 45% 47% 90%
Accuracy 89% 91% 95% 89%
</table>
<bodyText confidence="0.996729636363636">
This exercise showed some interesting results.
Specifically, we saw how we would improve our
ability to predict students who wouldn’t attend
(False) but decrease the True accuracy. We did
see significant improvement in the case where the
outcome was “will never attend again”. However,
we decided to stay with our base definition of at-
tendance as “no activity in the following week”.
Validating these alternate definitions of attend-
ance is a task that would be worthwhile for addi-
tional research.
</bodyText>
<subsectionHeader confidence="0.981445">
4.6 Iteration 6: Team Collaboration
</subsectionHeader>
<bodyText confidence="0.999941642857143">
Blue Canary prides itself on collaboration not
only amongst researchers in the learning analytics
field, but also collaboration inside of our own
company. We made sure to share information
about this shared task with others in the company,
and that collaboration allowed us to positively ex-
pand our feature set. One employee had come
across MOOC research that had found good pre-
dictive results when using an aggregate engage-
ment/activity score (Poellhuber, 2014). We de-
cided to utilize a similar feature where the number
of sessions, pages, days, and hours of activity in a
given week were combined into an engagement
score.
</bodyText>
<subsectionHeader confidence="0.939376">
4.7 Iteration 7: Winnowing the Field
</subsectionHeader>
<bodyText confidence="0.972411928571429">
As a final step, we wanted to reduce the number
of features used in the modeling process so as to
improve cycle times. We knew that the majority
of the fields had little to no predictive value, so we
ran models where we just used the top 10, 15, or
20 features. In the end, all permutations gave sim-
ilar accuracy scores and we decided to use the top
15 features. Those features resulted in accuracy
rates of:
modeling. This led us to conclude that we were at
the point of diminishing returns and we decided to
finalize the model with the 15 features and their
corresponding importance level as illustrated in
Table 4 (below).
</bodyText>
<table confidence="0.998496411764706">
Feature Import.
total_minutes_spent_previous_wk 0.336
initial_activity_score_previous_wk 0.072
final_activity_score_previous_wk 0.071
final_activity_score_up_to_wk 0.070
event_count_up_to_wk 0.068
most_com- 0.059
mon_browser_count_up_to_wk
initial_activity_score_up_to_wk 0.049
url_wiki_view_count_up_to_wk 0.041
session_count_up_to_wk 0.038
url_quiz_count_up_to_wk 0.037
total_minutes_spent_up_to_wk 0.037
url_lecture_count_up_to_wk 0.037
browser_count_up_to_wk 0.031
ip_count_up_to_wk 0.031
session_count_previous_wk 0.023
</table>
<tableCaption confidence="0.970432">
Table 4: Features and Importance
</tableCaption>
<sectionHeader confidence="0.998409" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.718359111111111">
The overarching conclusion from this research
can be summarized in two points:
1. Machine learning models can do an above
average job at predicting retention/attri-
tion in MOOC’s
2. The predictive factors are not surprising –
they are variants of measures of the stu-
dent’s engagement and activity in the
course
</bodyText>
<subsectionHeader confidence="0.65255">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.986209214285714">
Looking at the features in Table 4, one can see that
almost all of the important features are measures
of activity. Minutes, events, views and even the
aggregated activity feature are all measuring sim-
ilar characteristics. The takeaway here is that
there shouldn’t be an expectation of some unique
marker that predicts retention. There’s no secret
in the secret sauce.
Measure Rate
False 92%
True 54%
Accuracy 88%
The accuracy rates are similar to the rates we
had been getting in the past two iterations of the
</bodyText>
<sectionHeader confidence="0.999698" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.97939825">
The authors would like to thank Mohammed An-
sari, Andy Allen, Satish Divakarla, David Mor-
gan, and the entire Blue Canary and Clairvoyant
team for their support in this shared task.
</bodyText>
<page confidence="0.998274">
53
</page>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999790857142857">
Matt Bogard. (2011, January 29) Culture War: Classi-
cal Statistics vs. Machine Learning. Retrieved from
http://econometricsense.blog-
spot.com/2011/01/classical-statistics-vs-ma-
chine.html
Arnold, K. E., &amp; Pistilli, M. D. (2012, April). Course
Signals at Purdue: Using learning analytics to in-
crease student success. In Proceedings of the 2nd In-
ternational Conference on Learning Analytics and
Knowledge (pp. 267-270). ACM.
Boston, W. E., Ice, P., &amp; Gibson, A. M. (2011). Com-
prehensive assessment of student retention in online
learning environments. Online Journal of Distance
Learning Administration, 14(4).
Breslow, L., Pritchard, D. E., DeBoer, J., Stump, G. S.,
Ho, A. D., &amp; Seaton, D. T. (2013). Studying learn-
ing in the worldwide classroom: Research into
edX’s first MOOC. Research &amp; Practice in Assess-
ment, 8, 13-25.
Taylor, C., Veeramachaneni, K., &amp; O&apos;Reilly, U. M.
(2014). Likely to stop? Predicting Stopout in Mas-
sive Open Online Courses. arXiv preprint
arXiv:1408.3382.
Rosé, C. P., Carlson, R., Yang, D., Wen, M., Resnick,
L., Goldman, P., &amp; Sherer, J. (2014, March). Social
factors that contribute to attrition in moocs. In Pro-
ceedings of the first ACM conference on Learning@
scale conference (pp. 197-198). ACM.
Poellhuber, B., Roy, N., Bouchoucha, I., Anderson, T.
(2014, April). The Relationship Between the Moti-
vational Profiles, Engagement Profiles and Persis-
tence of MOOC Participants. Retrieved from
http://www.moocresearch.com/wp-content/up-
loads/2014/06/MOOC-Research-InitiativePoelhu-
ber9187v4a.pdf, September 1, 2014.
</reference>
<page confidence="0.999025">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616582">
<title confidence="0.999184">A Process for Predicting MOOC Attrition</title>
<author confidence="0.970637">Mike</author>
<affiliation confidence="0.725485">Blue</affiliation>
<address confidence="0.809265">Chandler, AZ USA</address>
<email confidence="0.999855">mike@bluecanarydata.com</email>
<abstract confidence="0.998447923076923">The goal of this shared task was to predict attrition in a MOOC through use of the data and logs generated by the course. Our approach to the task reinforces the idea that the process of gathering and structuring the data is more important (and more time consuming) than the predictive model itself. The result of the analysis was that a subset of 15 different data features did a sufficiently good job at predicting whether or not a student would exhibit any activity in the following week.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matt Bogard</author>
</authors>
<title>29) Culture War: Classical Statistics vs. Machine Learning.</title>
<date>2011</date>
<note>Retrieved from http://econometricsense.blogspot.com/2011/01/classical-statistics-vs-machine.html</note>
<contexts>
<context position="4570" citStr="Bogard (2011)" startWordPosition="765" endWordPosition="766">exactly what it is we are predicting. At a high level, it sounds easy – will the student retain in the class? From a data perspective, though, we need to define what it means to retain. Does it mean that the student submitted the assignment for the week? Watched a video? Simply logged in? Zeroing in on a reliable definition of retention is a part of the process. 3.3 Step 3: Run the Predictive Model With the input and output data in place, the team needs to run a model to derive a prediction. Blue Canary has consistently used machine learning techniques (as opposed to statistical modeling). As Bogard (2011) alludes to in a blog post comparing the two approaches, Blue Canary’s technical expertise combined with an unknown underlying relationship make machine learning our preferred method of analysis. For this analysis, Blue Canary implemented a random forest method using the SciKit python toolset (http://scikitlearn.org/). 3.4 Step 4: Observe/Validate/Iterate The last step in the process is to observe the outcomes of the modeling, validate the results (both quantitatively and qualitatively) and iterate to improve. When looking at the modeling results, we focused on accuracy. More specifically, we </context>
</contexts>
<marker>Bogard, 2011</marker>
<rawString>Matt Bogard. (2011, January 29) Culture War: Classical Statistics vs. Machine Learning. Retrieved from http://econometricsense.blogspot.com/2011/01/classical-statistics-vs-machine.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Arnold</author>
<author>M D Pistilli</author>
</authors>
<title>Course Signals at Purdue: Using learning analytics to increase student success.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge</booktitle>
<pages>267--270</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5994" citStr="Arnold and Pistilli, 2012" startWordPosition="990" endWordPosition="993">th since the application of any such solution would involve treatments for both parties. Value Definition True Positive # predicted to retain / # actually retained True Negative # predicted to attrite / # actual attrition Accuracy (True positive + True negative) / population Table 1: Definition of model accuracy values 3.5 Acknowledging Prior Research It should be noted that Blue Canary has stood on the shoulders of others who have tackled similar problems in the past. Our choice for analytical methods and features has been inspired by earlier predictive projects like Purdue’s Course Signals (Arnold and Pistilli, 2012) and research done at American Public University (Boston et. al., 2011). We also referenced contemporary MOOC research that explored the descriptive (Breslow et. al., 2013), predictive (Taylor et. al., 2014), and social (Rosé et. al., 2014) contributors to attrition. 4 Predicting Attrition for PSY-001 The course in question was from a 2013 Georgia Tech/Coursera MOOC called “Introduction to Psychology as a Science”. Blue Canary executed seven iterative steps as explained in the previous section. At the end we came up with a model that used 15 features to predict retention and attrition at an 88</context>
</contexts>
<marker>Arnold, Pistilli, 2012</marker>
<rawString>Arnold, K. E., &amp; Pistilli, M. D. (2012, April). Course Signals at Purdue: Using learning analytics to increase student success. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge (pp. 267-270). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Boston</author>
<author>P Ice</author>
<author>A M Gibson</author>
</authors>
<title>Comprehensive assessment of student retention in online learning environments.</title>
<date>2011</date>
<journal>Online Journal of Distance Learning Administration,</journal>
<volume>14</volume>
<issue>4</issue>
<marker>Boston, Ice, Gibson, 2011</marker>
<rawString>Boston, W. E., Ice, P., &amp; Gibson, A. M. (2011). Comprehensive assessment of student retention in online learning environments. Online Journal of Distance Learning Administration, 14(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breslow</author>
<author>D E Pritchard</author>
<author>J DeBoer</author>
<author>G S Stump</author>
<author>A D Ho</author>
<author>D T Seaton</author>
</authors>
<title>Studying learning in the worldwide classroom: Research into edX’s first MOOC.</title>
<date>2013</date>
<journal>Research &amp; Practice in Assessment,</journal>
<volume>8</volume>
<pages>13--25</pages>
<marker>Breslow, Pritchard, DeBoer, Stump, Ho, Seaton, 2013</marker>
<rawString>Breslow, L., Pritchard, D. E., DeBoer, J., Stump, G. S., Ho, A. D., &amp; Seaton, D. T. (2013). Studying learning in the worldwide classroom: Research into edX’s first MOOC. Research &amp; Practice in Assessment, 8, 13-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Taylor</author>
<author>K Veeramachaneni</author>
<author>U M O&apos;Reilly</author>
</authors>
<title>Likely to stop? Predicting Stopout in Massive Open Online Courses. arXiv preprint arXiv:1408.3382.</title>
<date>2014</date>
<marker>Taylor, Veeramachaneni, O&apos;Reilly, 2014</marker>
<rawString>Taylor, C., Veeramachaneni, K., &amp; O&apos;Reilly, U. M. (2014). Likely to stop? Predicting Stopout in Massive Open Online Courses. arXiv preprint arXiv:1408.3382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rosé</author>
<author>R Carlson</author>
<author>D Yang</author>
<author>M Wen</author>
<author>L Resnick</author>
<author>P Goldman</author>
<author>J Sherer</author>
</authors>
<title>Social factors that contribute to attrition in moocs.</title>
<date>2014</date>
<booktitle>In Proceedings of the first ACM conference on Learning@ scale conference</booktitle>
<pages>197--198</pages>
<publisher>ACM.</publisher>
<marker>Rosé, Carlson, Yang, Wen, Resnick, Goldman, Sherer, 2014</marker>
<rawString>Rosé, C. P., Carlson, R., Yang, D., Wen, M., Resnick, L., Goldman, P., &amp; Sherer, J. (2014, March). Social factors that contribute to attrition in moocs. In Proceedings of the first ACM conference on Learning@ scale conference (pp. 197-198). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Poellhuber</author>
<author>N Roy</author>
<author>I Bouchoucha</author>
<author>T Anderson</author>
</authors>
<date>2014</date>
<booktitle>The Relationship Between the Motivational Profiles, Engagement Profiles and Persistence of MOOC Participants. Retrieved from http://www.moocresearch.com/wp-content/uploads/2014/06/MOOC-Research-InitiativePoelhuber9187v4a.pdf,</booktitle>
<marker>Poellhuber, Roy, Bouchoucha, Anderson, 2014</marker>
<rawString>Poellhuber, B., Roy, N., Bouchoucha, I., Anderson, T. (2014, April). The Relationship Between the Motivational Profiles, Engagement Profiles and Persistence of MOOC Participants. Retrieved from http://www.moocresearch.com/wp-content/uploads/2014/06/MOOC-Research-InitiativePoelhuber9187v4a.pdf, September 1, 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
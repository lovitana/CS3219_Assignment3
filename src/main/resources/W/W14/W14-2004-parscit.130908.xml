<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011736">
<title confidence="0.995095">
Evaluating Evaluation Metrics for Minimalist Parsing
</title>
<author confidence="0.982373">
Thomas Graf Bradley Marcinek
</author>
<affiliation confidence="0.845088">
Department of Linguistics Department of Linguistics
Stony Brook University Stony Brook University
</affiliation>
<email confidence="0.99512">
mail@thomasgraf.net bradley.marcinek@stonybrook.edu
</email>
<sectionHeader confidence="0.993781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999709">
In response to Kobele et al. (2012), we
evaluate four ways of linking the process-
ing difficulty of sentences to the behav-
ior of the top-down parser for Minimal-
ist grammars developed in Stabler (2012).
We investigate the predictions these four
metrics make for a number of relative
clause constructions, and we conclude that
at this point, none of them capture the full
range of attested patterns.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934826923077">
Minimalist grammars (MGs; (Stabler, 1997)) are
a mildly context-sensitive formalism inspired by
Minimalist syntax (Chomsky, 1995), the domi-
nant theory in generative syntax. MGs allow us to
evaluate syntactic proposals with respect to com-
putational and cognitive criteria such as genera-
tive capacity (Harkema, 2001; Michaelis, 2001) or
the memory structures they require (Kobele et al.,
2007; Graf, 2012).
A new kind of top-down parser for MGs has re-
cently been presented by Stabler (2011b; 2012).
Stabler’s parser is noteworthy because it uses
derivation trees as a data structure in order to
reduce MG parsing to a special case of parsing
context-free grammars (CFGs). This raises the
question, though, whether derivation trees are a
psychologically plausible data structure, and if so,
to which extent the Stabler parser makes it possi-
ble to test the psycholinguistic predictions of com-
peting syntactic analyses.
In order to address this question, a linking hy-
pothesis is needed that connects the behavior of
the parser to a processing difficulty metric. Ko-
bele et al. (2012) — henceforth KGH — propose
that the difficulty of sentence s correlates with
the maximum number of parse steps the parser
has to keep a parse item in memory while pro-
cessing s. This metric is called maximum tenure
(Max). Max is appealing because of its simplicity
and sensitivity to differences in linguistic analysis,
which makes it easy to determine the psycholin-
guistic predictions of a specific syntactic analyses.
In this paper, we show that Max does not make
the right predictions for I) relative clauses embed-
ded in a sentential complement and II) subjects
gaps versus object gaps in relative clauses. We
present a number of simple alternative measures
that handle these phenomena correctly, but we also
show that these metrics fail in other cases (all re-
sults are summarized in Tab. 1 on page 8). We con-
clude that the prospect of a simple direct link be-
tween syntactic analysis and processing difficulty
is tempting but not sufficiently developed at this
point.
The paper starts with a quick introduction to
MGs (Sec. 2.1) and how they are parsed (Sec. 2.2).
Section 3 then introduces three alternatives to
Max. Max is then shown to fare worse than those
three with respect to well-known contrasts involv-
ing relative clauses (Sec. 4). Section 5 briefly
looks at three other constructions that pose prob-
lems for the alternative metrics.
</bodyText>
<sectionHeader confidence="0.99703" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.999688">
2.1 Minimalist Grammars
</subsectionHeader>
<bodyText confidence="0.999928076923077">
MGs (Stabler, 1997; Stabler, 2011a) are a highly
lexicalized formalism in which structures are built
via the operations Merge and Move. Intuitively,
Merge enforces local dependencies via subcatego-
rization, whereas Move establishes long-distance
filler-gap dependencies.
Every lexical item comes with a non-empty list
of unchecked features, and each feature has either
positive or negative polarity and is checked by ei-
ther Merge or Move. Suppose that I) s is a tree
whose head has a positive Merge feature F+ as its
first unchecked feature, and II) t is a tree whose
head has a matching negative Merge feature F−
</bodyText>
<page confidence="0.982891">
28
</page>
<bodyText confidence="0.94448715">
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 28–36,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
as its first unchecked feature. Then Merge checks
F+ and F− and combines s and t into the tree
l(s, t) or l(t, s), where l is a label projected by the
head of s and s is linearized to the left of t iff s
consists of exactly one node. Move, on the other
hand, applies to a single tree s whose head h has
a positive Move feature f+ as its first unchecked
feature. Suppose that t is a subtree of s whose
head has the matching negative Move feature f−
as its first unchecked feature. Then Move checks
f+ and f− and returns the tree l(t, s0), where l is
a label projected by h and s0 is obtained by remov-
ing t from s. Crucially, Move may apply to s iff
there is exactly one subtree like t. This restriction
is known as the Shortest Move Constraint (SMC).
For example, the sentence John left involves (at
least) the following steps under a simplified Mini-
malist analysis (Adger, 2003):
</bodyText>
<equation confidence="0.997504857142857">
Merge(John :: D− nom−, left:: D+ V−)
= [VP left:: V− John:: nom− ] (1)
Merge(e :: V+ nom+ T−, (1))
= [TP e :: nom+ T−[VP left
John:: nom−] ] (2)
Move((2)) = [TP John [T’ e :: T−
[VP left ] ] ] (3)
</equation>
<bodyText confidence="0.999590384615384">
This derivation can be represented more succinctly
as the derivation tree in Fig 1, where all leaves are
labeled by lexical items while unary and binary
branching nodes are labeled Move and Merge, re-
spectively.
Even though MGs (with the SMC) are weakly
equivalent to MCFGs (Michaelis, 2001) and thus
mildly context-sensitive in the sense of Joshi
(1985), their derivation tree languages can be gen-
erated by CFGs (modulo relabeling of interior
nodes). As we will see next, this makes it pos-
sible to treat MG parsing as a special case of CFG
parsing.
</bodyText>
<subsectionHeader confidence="0.999487">
2.2 Parsing Minimalist Grammars
</subsectionHeader>
<bodyText confidence="0.999209385964912">
Thanks to the SMC, the mapping from deriva-
tion trees to phrase structure trees is determin-
istic. Consequently, MG parsing reduces to as-
signing context-free derivation trees to input sen-
tences, rather than the more complex phrase struc-
ture trees. The major difference from CFGs is
that the linear order of nodes in an MG deriva-
tion tree does not necessarily match the linear or-
der of words in the input sentence — for instance
because a moving phrase remains in its base posi-
tion in the derivation tree. But as long as one can
tell for every MG operation how its output is lin-
earized, these discrepancies in linear order can be
taken care of in the inference rules of the parser.
Stabler (2011b; 2012) shows how exactly this is
done for a parser that constructs derivation trees
in a top-down fashion. Intuitively, MG top-down
parsing is CFG top-down parsing with a slightly
different algorithm for traversing/expanding the
tree.
Instead of presenting the parser’s full set of in-
ference rules, we adopt KGH’s index notation to
indicate how the parser constructs a given deriva-
tion. For instance, if a derivation contains the node
5Merge38, this means that the parser makes a pre-
diction at step 5 that Merge occurs at this posi-
tion in the derivation and keeps this prediction in
memory until step 38, at which point the parser
replaces it by suitable predictions for the argu-
ments for Merge, i.e. the daughters of the Merge
node. Similarly, 22the :: N+ D−28 denotes that
the parser conjectures this lexical item at step 22
and finally gets to scan it in the input string at step
28.
In principle the parser could simply predict a
complete derivation and then scan the input string
to see if the two match. In order to obtain an in-
cremental parser, however, scanning steps have to
take place as soon as possible. The MG parser im-
plements this as follows: predictions are put into a
priority queue, and the prediction with the highest
priority is worked on first. The priority of the pre-
dictions corresponds to the linear order that holds
between the constituents that are obtained from
them. For example, if the parser replaces a predic-
tion for a Merge node yielding l(s, t) by predic-
tions ps and pt that eventually derive s and t, then
ps has higher priority than pt iff s is predicted to
precede t. Since Move only takes one argument s,
replacing a Move prediction by the prediction of
s trivially involves no such priority management.
However, if movement is to a position to the left
of s (as is standard for MGs), none of the lexical
items contained within s can be scanned until the
entire subtree moving out of s has been predicted
and scanned.
If a prediction does not have the highest prior-
</bodyText>
<page confidence="0.989799">
29
</page>
<figure confidence="0.996705">
John
TP
T&apos;
Move
Merge
ε :: V+ nom+ T−
ε
VP
Merge
left t left:: D+ V− John
</figure>
<figureCaption confidence="0.99997">
Figure 1: Minimalist phrase structure tree (left) and MG derivation tree (right) for John left
</figureCaption>
<bodyText confidence="0.999937375">
ity, it remains in the queue for a few steps before it
is expanded into other predictions or discharged
by scanning a word from the input string. The
number of steps a prediction stays in the queue
is called its tenure. With KGH’s index notation,
the tenure of each node is the difference between
its indices. Given a parse, its maximum tenure
Max is the smallest n such that the parser stored
no prediction in its queue for more than n steps.
KGH demonstrate that Max can be used to gauge
how hard it is for humans to process certain struc-
tures. This amounts to equating processing dif-
ficulty with memory retention requirements. But
as we show in the remainder of this paper, Max
faces problems with relative clause constructions
that were not considered by KGH.
</bodyText>
<sectionHeader confidence="0.994707" genericHeader="method">
3 Alternative Metrics
</sectionHeader>
<bodyText confidence="0.697296">
:: D− nom−
</bodyText>
<subsectionHeader confidence="0.992948">
3.1 Three New Metrics
</subsectionHeader>
<bodyText confidence="0.999968028571429">
In an attempt to home in on the shortcomings of
Max, we contrast it with a number of alternative
metrics. Since the main advantage of Max is its
simplicity, which makes it possible to quickly de-
termine the processing predictions of a given syn-
tactic analysis, the metrics we consider are also
kept as simple as possible.
MaxLex the maximum tenure of all leaves in the
derivation
Box the maximum number of nodes with tenure
strictly greater than 2
BoxLex the maximum number of leaves with
tenure strictly greater than 2
MaxLex is simply the restriction of Max to leaf
nodes. Box and BoxLex provide a measure of how
many items have to be stored in memory during
the parse and hence incur some non-trivial amount
of tenure. The threshold is set to 2 rather than 1 to
exclude lexical items that are right siblings of an-
other lexical item. In such a case, a single predic-
tion is immediately followed by two consecutive
scan steps, which could just as well be thought
of as one scan step spanning two words. Nodes
with tenure over 2 are highlighted by a box in our
derivation trees, hence the name for these two met-
rics.
All four measures are also divided into two sub-
types depending on whether unpronounced leaves
(e.g. the empty T-head in Fig. 1) are taken into ac-
count — this is inspired by the exclusion of un-
pronounced material in the TAG-parser of Ram-
bow and Joshi (1995). When reporting the val-
ues for the metrics, we thus give slashed values of
the form m/n, where m is the value with unpro-
nounced leaves and n the value without them.
</bodyText>
<subsectionHeader confidence="0.99887">
3.2 Methodological Remarks
</subsectionHeader>
<bodyText confidence="0.999849962962963">
The following sections investigate the predictions
of our difficulty metrics with respect to the em-
bedding of sentential complements versus relative
clauses, subject gaps versus object gaps in relative
clauses, left embedding, and verb clusters. In or-
der for this comparison to be meaningful, we have
to make the same methodological assumptions as
KGH.
First, the difficulty metric only has to account
for overall sentence difficulty, it does not neces-
sarily correlate with difficulty at a specific word.
More importantly, though, all reported processing
difficulties are assumed to be due to memory load.
This is a very strong assumption. A plethora of
alternative accounts are available in the literature.
The contrast between subject gaps and object gaps
alone has been explained by information-theoretic
notions such as surprisal (Hale, 2003; Levy, 2013),
the active filler strategy (Frazier and D’Arcais,
1989), or theta role assignment (Pritchett, 1992),
to name but a few (see Lin (2006) and Wu (2009)
for extensive surveys).
Even those accounts that attribute processing
difficulty to memory requirements make ancillary
assumptions that are not reflected by the simple
memory model entertained here. Gibson’s Depen-
dency Locality Theory (1998), for instance, cru-
</bodyText>
<page confidence="0.996134">
30
</page>
<bodyText confidence="0.9998355">
cially relies on discourse reference as a means for
determining how much of a memory burden is in-
curred by each word.
We take no stance as to whether these accounts
are correct. Our primary interest is the feasibility
of a memory-based evaluation metric for Stabler’s
top-down parser. Memory is more likely to play
a role in the constructions we look at in the next
two sections than in, say, attachment ambiguities
or local syntactic coherence effects (Tabor et al.,
2004). It may well turn out that memory is not
involved at all, but for the purpose of comparing
several memory-based metrics, they are the safest
starting point.
</bodyText>
<sectionHeader confidence="0.9999" genericHeader="method">
4 Relative Clauses
</sectionHeader>
<subsectionHeader confidence="0.997956">
4.1 Empirical Generalizations
</subsectionHeader>
<bodyText confidence="0.996135333333333">
Two major properties of relative clauses are firmly
established in the literature (see Gibson (1998) and
references therein).
</bodyText>
<listItem confidence="0.949722">
• SC/RC &lt; RC/SC
</listItem>
<bodyText confidence="0.971842">
A sentential complement containing a rela-
tive clause is easier to process than a relative
clause containing a sentential complement.
</bodyText>
<listItem confidence="0.94936">
• SubjRC &lt; ObjRC
</listItem>
<bodyText confidence="0.9148168125">
A relative clause containing a subject gap is
easier to parse than a relative clause contain-
ing an object gap.
These generalizations were obtained via self-
paced reading experiments and ERP studies with
minimal pairs such as (1) and (2), respectively.
(1) a. The fact [SC that the employeei [RC
who the manager hired ti] stole office
supplies] worried the executive.
b. The executivei [RC who the fact [SC
that the employee stole offices sup-
plies] worried ti] hired the manager.
(2) a. The reporteri [RC who ti attacked the
senator] admitted the error.
b. The reporteri [RC who the senator at-
tacked ti] admitted the error.
</bodyText>
<subsectionHeader confidence="0.963512">
4.2 SC/RC and RC/SC
</subsectionHeader>
<bodyText confidence="0.963644982142857">
We first consider the contrast between relative
clauses embedded inside a sentential complement
(SC/RC) and relative clauses containing a sen-
tential complement (SC/RC). Figures 2 and 3 on
pages 5 and 6 show the augmented derivations for
(1a) and (1b), respectively. For the sake of read-
ability, we omit all features in our derivation trees
and instead use standard X� labels to indicate pro-
jection and dashed branches for movement.
Like KGH, we adopt a promotion analysis of
relative clauses (Vergnaud, 1974; Kayne, 1994).
That is to say, the head noun is selected by an
empty determiner to form a DP, which starts out
as an argument of the embedded verb and under-
goes movement into the specifier of the relative
clause (which is treated as an NP). The entire rel-
ative clause is then selected by the determiner that
would usually select the head noun under the tra-
ditional, head-external analysis (Montague, 1970;
Chomsky, 1977).1
In both derivations the maximum tenure obtains
at two points in the matrix clause: I) the unpro-
nounced T-head, and II) the Merge step that intro-
duces the remainder of the VP. The parser must
first build the entire subject before it can proceed
scanning or expanding material to its right. Con-
sequently, the tenure of these nodes increases with
the size of the subject, and since both the SC/RC
pattern and the RC/SC pattern necessarily involve
large subjects, maximum tenure for both types of
sentences is predicted to be relatively high. The
parser shows a slightly lower Max value for SC/
RC than for RC/SC — 32/32 versus 33/33.
Although this shows that strictly speaking Max
is not incompatible with the generalization that
SC/RC is easier to process than RC/SC, the differ-
ence is so small that even the presence of one more
word in the SC/RC sentence could tip the balance
towards RC/SC, which seems rather unlikely.
The contrast emerges more clearly with the
other measures. MaxLex yields the values 32/9
versus 33/17, so it fares better than Max only
if one ignores unpronounced leaves. This is ex-
pected since one of the nodes incurring the highest
tenure value is the unpronounced T-head. The Box
values are 14/11 and 5/3, and those of BoxLex
are 12/9 and 3/1.
The box values fare better in this case because
they are sensitive to the number of dependencies
that cannot be discharged immediately. The way
the MG parser traverses the tree, a sentential com-
1The promotion analysis was chosen to maintain consis-
tency with KGH. But our observations hold for every anal-
ysis that involves some movement dependency between the
gap and the specifier of the relative clause. This includes the
more common head-external analyses mentioned above.
</bodyText>
<page confidence="0.999864">
31
</page>
<figure confidence="0.967961645161291">
0CP1
1TP3
3
4
T04
VP5
5V
32
0CP1
1TP3
3
4
T04
VP5
5V
33
0CP1
1TP3
3
4
T04
VP5
5V
34
0CP1
1TP3
3
4
T04
VP5
5V
</figure>
<page confidence="0.993001">
35
</page>
<sectionHeader confidence="0.989631" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897969387755">
David Adger. 2003. Core Syntax: A Minimalist Ap-
proach. Oxford University Press, Oxford.
Emmon Bach, Colin Brown, and William Marslen-
Wilson. 1986. Crossed and nested dependencies in
German and Dutch: A psycholinguistic study. Lan-
guage and Cognitive Processes, 1:249–262.
Noam Chomsky. 1977. Essays on Form and Interpre-
tation. New York, North Holland.
Noam Chomsky. 1995. The Minimalist Program. MIT
Press, Cambridge, Mass.
Lyn Frazier and Flores D’Arcais. 1989. Filler driven
parsing: A study of gap filling in Dutch. Journal of
Memory and Language, 28:331–344.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68:1–76.
Thomas Graf. 2012. Locality and the complexity of
minimalist derivation tree languages. In Philippe
de Groot and Mark-Jan Nederhof, editors, Formal
Grammar 2010/2011, volume 7395 of Lecture Notes
in Computer Science, pages 208–227, Heidelberg.
Springer.
John T. Hale. 2003. Grammar, Uncertainty and Sen-
tence Processing. Ph.D. thesis, John Hopkins Uni-
versity.
Henk Harkema. 2001. A characterization of min-
imalist languages. In Philippe de Groote, Glyn
Morrill, and Christian Retor´e, editors, Logical As-
pects of Computational Linguistics (LACL’01), vol-
ume 2099 of Lecture Notes in Artificial Intelligence,
pages 193–211. Springer, Berlin.
Aravind Joshi. 1985. Tree-adjoining grammars: How
much context sensitivity is required to provide rea-
sonable structural descriptions? In David Dowty,
Lauri Karttunen, and Arnold Zwicky, editors, Nat-
ural Language Parsing, pages 206–250. Cambridge
University Press, Cambridge.
Richard S. Kayne. 1994. The Antisymmetry of Syntax.
MIT Press, Cambridge, Mass.
Gregory M. Kobele, Christian Retor´e, and Sylvain Sal-
vati. 2007. An automata-theoretic approach to min-
imalism. In James Rogers and Stephan Kepser, edi-
tors, Model Theoretic Syntax at 10, pages 71–80.
Gregory M. Kobele, Sabrina Gerth, and John T. Hale.
2012. Memory resource allocation in top-down
minimalist parsing. In Proceedings of Formal
Grammar 2012.
Nayoung Kwon, Robert Kluender, Marta Kutas, and
Maria Polinsky. 2013. Subject/object processing
asymmetries in korean relative clauses: Evidence
from ERP data. Language, 89:537–585.
Roger Levy. 2013. Memory and surprisal in human
sentence comprehension. In Roger P. G. van Gom-
pel, editor, Sentence Processing, pages 78–114. Psy-
chology Press, Hove.
Chien-Jer Charles Lin. 2006. Grammar and Parsing:
A Typological Investigation of Relative-Clause Pro-
cessing. Ph.D. thesis, University of Arizona.
Jens Michaelis. 2001. Transforming linear context-
free rewriting systems into minimalist grammars.
Lecture Notes in Artificial Intelligence, 2099:228–
244.
Richard Montague. 1970. English as a formal lan-
guage. In Bruno Visentini and et al., editors, Lin-
guaggi nella Societ e nella Tecnica, pages 189–224.
Edizioni di Comunit, Milan.
Bradley L. Pritchett. 1992. Grammatical Competence
and Parsing Performance. University of Chicago
Press, Chicago.
Owen Rambow and Aravind Joshi. 1995. A process-
ing model for free word order languages. Technical
Report IRCS-95-13, University of Pennsylvania.
Philip Resnik. 1992. Left-corner parsing and psycho-
logical plausibility. In Proceedings of COLING-92,
pages 191–197.
Edward P. Stabler. 1997. Derivational minimalism. In
Christian Retor´e, editor, Logical Aspects of Compu-
tational Linguistics, volume 1328 of Lecture Notes
in Computer Science, pages 68–95. Springer, Berlin.
Edward P. Stabler. 2011a. Computational perspec-
tives on minimalism. In Cedric Boeckx, editor,
Oxford Handbook of Linguistic Minimalism, pages
617–643. Oxford University Press, Oxford.
Edward P. Stabler. 2011b. Top-down recognizers for
MCFGs and MGs. In Proceedings of the 2011 Work-
shop on Cognitive Modeling and Computational
Linguistics. to appear.
Edward P. Stabler. 2012. Bayesian, minimalist, incre-
mental syntactic analysis. Topics in Cognitive Sci-
ence, 5:611–633.
Whitney Tabor, Bruno Galantucci, and Daniel Richard-
son. 2004. Effects of merely local syntactic coher-
ence on sentence processing. Journal of Memory
and Language, 50:355–370.
Jean-Roger Vergnaud. 1974. French Relative Clauses.
Ph.D. thesis, MIT.
Fuyun Wu. 2009. Factors Affecting Relative Clause
Processing in Mandarin. Ph.D. thesis, University of
Southern California.
</reference>
<page confidence="0.99894">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913862">
<title confidence="0.999987">Evaluating Evaluation Metrics for Minimalist Parsing</title>
<author confidence="0.999719">Thomas Graf Bradley Marcinek</author>
<affiliation confidence="0.996852">Department of Linguistics Department of Linguistics</affiliation>
<author confidence="0.931103">Stony Brook University Stony Brook</author>
<email confidence="0.990086">mail@thomasgraf.netbradley.marcinek@stonybrook.edu</email>
<abstract confidence="0.999535545454545">In response to Kobele et al. (2012), we evaluate four ways of linking the processing difficulty of sentences to the behavior of the top-down parser for Minimalist grammars developed in Stabler (2012). We investigate the predictions these four metrics make for a number of relative clause constructions, and we conclude that at this point, none of them capture the full range of attested patterns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Adger</author>
</authors>
<title>Core Syntax: A Minimalist Approach.</title>
<date>2003</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="4800" citStr="Adger, 2003" startWordPosition="791" endWordPosition="792"> to a single tree s whose head h has a positive Move feature f+ as its first unchecked feature. Suppose that t is a subtree of s whose head has the matching negative Move feature f− as its first unchecked feature. Then Move checks f+ and f− and returns the tree l(t, s0), where l is a label projected by h and s0 is obtained by removing t from s. Crucially, Move may apply to s iff there is exactly one subtree like t. This restriction is known as the Shortest Move Constraint (SMC). For example, the sentence John left involves (at least) the following steps under a simplified Minimalist analysis (Adger, 2003): Merge(John :: D− nom−, left:: D+ V−) = [VP left:: V− John:: nom− ] (1) Merge(e :: V+ nom+ T−, (1)) = [TP e :: nom+ T−[VP left John:: nom−] ] (2) Move((2)) = [TP John [T’ e :: T− [VP left ] ] ] (3) This derivation can be represented more succinctly as the derivation tree in Fig 1, where all leaves are labeled by lexical items while unary and binary branching nodes are labeled Move and Merge, respectively. Even though MGs (with the SMC) are weakly equivalent to MCFGs (Michaelis, 2001) and thus mildly context-sensitive in the sense of Joshi (1985), their derivation tree languages can be generat</context>
</contexts>
<marker>Adger, 2003</marker>
<rawString>David Adger. 2003. Core Syntax: A Minimalist Approach. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmon Bach</author>
<author>Colin Brown</author>
<author>William MarslenWilson</author>
</authors>
<title>Crossed and nested dependencies in German and Dutch: A psycholinguistic study.</title>
<date>1986</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>1--249</pages>
<marker>Bach, Brown, MarslenWilson, 1986</marker>
<rawString>Emmon Bach, Colin Brown, and William MarslenWilson. 1986. Crossed and nested dependencies in German and Dutch: A psycholinguistic study. Language and Cognitive Processes, 1:249–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Essays on Form and Interpretation.</title>
<date>1977</date>
<location>New York, North Holland.</location>
<contexts>
<context position="14645" citStr="Chomsky, 1977" startWordPosition="2501" endWordPosition="2502">s in our derivation trees and instead use standard X� labels to indicate projection and dashed branches for movement. Like KGH, we adopt a promotion analysis of relative clauses (Vergnaud, 1974; Kayne, 1994). That is to say, the head noun is selected by an empty determiner to form a DP, which starts out as an argument of the embedded verb and undergoes movement into the specifier of the relative clause (which is treated as an NP). The entire relative clause is then selected by the determiner that would usually select the head noun under the traditional, head-external analysis (Montague, 1970; Chomsky, 1977).1 In both derivations the maximum tenure obtains at two points in the matrix clause: I) the unpronounced T-head, and II) the Merge step that introduces the remainder of the VP. The parser must first build the entire subject before it can proceed scanning or expanding material to its right. Consequently, the tenure of these nodes increases with the size of the subject, and since both the SC/RC pattern and the RC/SC pattern necessarily involve large subjects, maximum tenure for both types of sentences is predicted to be relatively high. The parser shows a slightly lower Max value for SC/ RC tha</context>
</contexts>
<marker>Chomsky, 1977</marker>
<rawString>Noam Chomsky. 1977. Essays on Form and Interpretation. New York, North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>The Minimalist Program.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="782" citStr="Chomsky, 1995" startWordPosition="108" endWordPosition="109"> University mail@thomasgraf.net bradley.marcinek@stonybrook.edu Abstract In response to Kobele et al. (2012), we evaluate four ways of linking the processing difficulty of sentences to the behavior of the top-down parser for Minimalist grammars developed in Stabler (2012). We investigate the predictions these four metrics make for a number of relative clause constructions, and we conclude that at this point, none of them capture the full range of attested patterns. 1 Introduction Minimalist grammars (MGs; (Stabler, 1997)) are a mildly context-sensitive formalism inspired by Minimalist syntax (Chomsky, 1995), the dominant theory in generative syntax. MGs allow us to evaluate syntactic proposals with respect to computational and cognitive criteria such as generative capacity (Harkema, 2001; Michaelis, 2001) or the memory structures they require (Kobele et al., 2007; Graf, 2012). A new kind of top-down parser for MGs has recently been presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing context-free grammars (CFGs). This raises the question, though, whether derivation trees </context>
</contexts>
<marker>Chomsky, 1995</marker>
<rawString>Noam Chomsky. 1995. The Minimalist Program. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
<author>Flores D’Arcais</author>
</authors>
<title>Filler driven parsing: A study of gap filling in Dutch.</title>
<date>1989</date>
<journal>Journal of Memory and Language,</journal>
<pages>28--331</pages>
<marker>Frazier, D’Arcais, 1989</marker>
<rawString>Lyn Frazier and Flores D’Arcais. 1989. Filler driven parsing: A study of gap filling in Dutch. Journal of Memory and Language, 28:331–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>Linguistic complexity: Locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<pages>68--1</pages>
<contexts>
<context position="12883" citStr="Gibson (1998)" startWordPosition="2203" endWordPosition="2204">e correct. Our primary interest is the feasibility of a memory-based evaluation metric for Stabler’s top-down parser. Memory is more likely to play a role in the constructions we look at in the next two sections than in, say, attachment ambiguities or local syntactic coherence effects (Tabor et al., 2004). It may well turn out that memory is not involved at all, but for the purpose of comparing several memory-based metrics, they are the safest starting point. 4 Relative Clauses 4.1 Empirical Generalizations Two major properties of relative clauses are firmly established in the literature (see Gibson (1998) and references therein). • SC/RC &lt; RC/SC A sentential complement containing a relative clause is easier to process than a relative clause containing a sentential complement. • SubjRC &lt; ObjRC A relative clause containing a subject gap is easier to parse than a relative clause containing an object gap. These generalizations were obtained via selfpaced reading experiments and ERP studies with minimal pairs such as (1) and (2), respectively. (1) a. The fact [SC that the employeei [RC who the manager hired ti] stole office supplies] worried the executive. b. The executivei [RC who the fact [SC tha</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>Edward Gibson. 1998. Linguistic complexity: Locality of syntactic dependencies. Cognition, 68:1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Graf</author>
</authors>
<title>Locality and the complexity of minimalist derivation tree languages.</title>
<date>2012</date>
<booktitle>In Philippe de Groot and Mark-Jan Nederhof, editors, Formal Grammar 2010/2011,</booktitle>
<volume>7395</volume>
<pages>208--227</pages>
<publisher>Springer.</publisher>
<location>Heidelberg.</location>
<contexts>
<context position="1056" citStr="Graf, 2012" startWordPosition="151" endWordPosition="152">investigate the predictions these four metrics make for a number of relative clause constructions, and we conclude that at this point, none of them capture the full range of attested patterns. 1 Introduction Minimalist grammars (MGs; (Stabler, 1997)) are a mildly context-sensitive formalism inspired by Minimalist syntax (Chomsky, 1995), the dominant theory in generative syntax. MGs allow us to evaluate syntactic proposals with respect to computational and cognitive criteria such as generative capacity (Harkema, 2001; Michaelis, 2001) or the memory structures they require (Kobele et al., 2007; Graf, 2012). A new kind of top-down parser for MGs has recently been presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing context-free grammars (CFGs). This raises the question, though, whether derivation trees are a psychologically plausible data structure, and if so, to which extent the Stabler parser makes it possible to test the psycholinguistic predictions of competing syntactic analyses. In order to address this question, a linking hypothesis is needed that connects the beha</context>
</contexts>
<marker>Graf, 2012</marker>
<rawString>Thomas Graf. 2012. Locality and the complexity of minimalist derivation tree languages. In Philippe de Groot and Mark-Jan Nederhof, editors, Formal Grammar 2010/2011, volume 7395 of Lecture Notes in Computer Science, pages 208–227, Heidelberg. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Hale</author>
</authors>
<date>2003</date>
<booktitle>Grammar, Uncertainty and Sentence Processing. Ph.D. thesis,</booktitle>
<institution>John Hopkins University.</institution>
<contexts>
<context position="11676" citStr="Hale, 2003" startWordPosition="2010" endWordPosition="2011"> clusters. In order for this comparison to be meaningful, we have to make the same methodological assumptions as KGH. First, the difficulty metric only has to account for overall sentence difficulty, it does not necessarily correlate with difficulty at a specific word. More importantly, though, all reported processing difficulties are assumed to be due to memory load. This is a very strong assumption. A plethora of alternative accounts are available in the literature. The contrast between subject gaps and object gaps alone has been explained by information-theoretic notions such as surprisal (Hale, 2003; Levy, 2013), the active filler strategy (Frazier and D’Arcais, 1989), or theta role assignment (Pritchett, 1992), to name but a few (see Lin (2006) and Wu (2009) for extensive surveys). Even those accounts that attribute processing difficulty to memory requirements make ancillary assumptions that are not reflected by the simple memory model entertained here. Gibson’s Dependency Locality Theory (1998), for instance, cru30 cially relies on discourse reference as a means for determining how much of a memory burden is incurred by each word. We take no stance as to whether these accounts are corr</context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>John T. Hale. 2003. Grammar, Uncertainty and Sentence Processing. Ph.D. thesis, John Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henk Harkema</author>
</authors>
<title>A characterization of minimalist languages.</title>
<date>2001</date>
<booktitle>Logical Aspects of Computational Linguistics (LACL’01), volume 2099 of Lecture Notes in Artificial Intelligence,</booktitle>
<pages>193--211</pages>
<editor>In Philippe de Groote, Glyn Morrill, and Christian Retor´e, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="966" citStr="Harkema, 2001" startWordPosition="137" endWordPosition="138">the behavior of the top-down parser for Minimalist grammars developed in Stabler (2012). We investigate the predictions these four metrics make for a number of relative clause constructions, and we conclude that at this point, none of them capture the full range of attested patterns. 1 Introduction Minimalist grammars (MGs; (Stabler, 1997)) are a mildly context-sensitive formalism inspired by Minimalist syntax (Chomsky, 1995), the dominant theory in generative syntax. MGs allow us to evaluate syntactic proposals with respect to computational and cognitive criteria such as generative capacity (Harkema, 2001; Michaelis, 2001) or the memory structures they require (Kobele et al., 2007; Graf, 2012). A new kind of top-down parser for MGs has recently been presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing context-free grammars (CFGs). This raises the question, though, whether derivation trees are a psychologically plausible data structure, and if so, to which extent the Stabler parser makes it possible to test the psycholinguistic predictions of competing syntactic analyses</context>
</contexts>
<marker>Harkema, 2001</marker>
<rawString>Henk Harkema. 2001. A characterization of minimalist languages. In Philippe de Groote, Glyn Morrill, and Christian Retor´e, editors, Logical Aspects of Computational Linguistics (LACL’01), volume 2099 of Lecture Notes in Artificial Intelligence, pages 193–211. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In</title>
<date>1985</date>
<booktitle>Natural Language Parsing,</booktitle>
<pages>206--250</pages>
<editor>David Dowty, Lauri Karttunen, and Arnold Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5352" citStr="Joshi (1985)" startWordPosition="895" endWordPosition="896">g steps under a simplified Minimalist analysis (Adger, 2003): Merge(John :: D− nom−, left:: D+ V−) = [VP left:: V− John:: nom− ] (1) Merge(e :: V+ nom+ T−, (1)) = [TP e :: nom+ T−[VP left John:: nom−] ] (2) Move((2)) = [TP John [T’ e :: T− [VP left ] ] ] (3) This derivation can be represented more succinctly as the derivation tree in Fig 1, where all leaves are labeled by lexical items while unary and binary branching nodes are labeled Move and Merge, respectively. Even though MGs (with the SMC) are weakly equivalent to MCFGs (Michaelis, 2001) and thus mildly context-sensitive in the sense of Joshi (1985), their derivation tree languages can be generated by CFGs (modulo relabeling of interior nodes). As we will see next, this makes it possible to treat MG parsing as a special case of CFG parsing. 2.2 Parsing Minimalist Grammars Thanks to the SMC, the mapping from derivation trees to phrase structure trees is deterministic. Consequently, MG parsing reduces to assigning context-free derivation trees to input sentences, rather than the more complex phrase structure trees. The major difference from CFGs is that the linear order of nodes in an MG derivation tree does not necessarily match the linea</context>
</contexts>
<marker>Joshi, 1985</marker>
<rawString>Aravind Joshi. 1985. Tree-adjoining grammars: How much context sensitivity is required to provide reasonable structural descriptions? In David Dowty, Lauri Karttunen, and Arnold Zwicky, editors, Natural Language Parsing, pages 206–250. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Kayne</author>
</authors>
<title>The Antisymmetry of Syntax.</title>
<date>1994</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="14238" citStr="Kayne, 1994" startWordPosition="2429" endWordPosition="2430"> b. The reporteri [RC who the senator attacked ti] admitted the error. 4.2 SC/RC and RC/SC We first consider the contrast between relative clauses embedded inside a sentential complement (SC/RC) and relative clauses containing a sentential complement (SC/RC). Figures 2 and 3 on pages 5 and 6 show the augmented derivations for (1a) and (1b), respectively. For the sake of readability, we omit all features in our derivation trees and instead use standard X� labels to indicate projection and dashed branches for movement. Like KGH, we adopt a promotion analysis of relative clauses (Vergnaud, 1974; Kayne, 1994). That is to say, the head noun is selected by an empty determiner to form a DP, which starts out as an argument of the embedded verb and undergoes movement into the specifier of the relative clause (which is treated as an NP). The entire relative clause is then selected by the determiner that would usually select the head noun under the traditional, head-external analysis (Montague, 1970; Chomsky, 1977).1 In both derivations the maximum tenure obtains at two points in the matrix clause: I) the unpronounced T-head, and II) the Merge step that introduces the remainder of the VP. The parser must</context>
</contexts>
<marker>Kayne, 1994</marker>
<rawString>Richard S. Kayne. 1994. The Antisymmetry of Syntax. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory M Kobele</author>
<author>Christian Retor´e</author>
<author>Sylvain Salvati</author>
</authors>
<title>An automata-theoretic approach to minimalism.</title>
<date>2007</date>
<pages>71--80</pages>
<editor>In James Rogers and Stephan Kepser, editors, Model</editor>
<marker>Kobele, Retor´e, Salvati, 2007</marker>
<rawString>Gregory M. Kobele, Christian Retor´e, and Sylvain Salvati. 2007. An automata-theoretic approach to minimalism. In James Rogers and Stephan Kepser, editors, Model Theoretic Syntax at 10, pages 71–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory M Kobele</author>
<author>Sabrina Gerth</author>
<author>John T Hale</author>
</authors>
<title>Memory resource allocation in top-down minimalist parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of Formal Grammar</booktitle>
<contexts>
<context position="1730" citStr="Kobele et al. (2012)" startWordPosition="259" endWordPosition="263">een presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing context-free grammars (CFGs). This raises the question, though, whether derivation trees are a psychologically plausible data structure, and if so, to which extent the Stabler parser makes it possible to test the psycholinguistic predictions of competing syntactic analyses. In order to address this question, a linking hypothesis is needed that connects the behavior of the parser to a processing difficulty metric. Kobele et al. (2012) — henceforth KGH — propose that the difficulty of sentence s correlates with the maximum number of parse steps the parser has to keep a parse item in memory while processing s. This metric is called maximum tenure (Max). Max is appealing because of its simplicity and sensitivity to differences in linguistic analysis, which makes it easy to determine the psycholinguistic predictions of a specific syntactic analyses. In this paper, we show that Max does not make the right predictions for I) relative clauses embedded in a sentential complement and II) subjects gaps versus object gaps in relative</context>
</contexts>
<marker>Kobele, Gerth, Hale, 2012</marker>
<rawString>Gregory M. Kobele, Sabrina Gerth, and John T. Hale. 2012. Memory resource allocation in top-down minimalist parsing. In Proceedings of Formal Grammar 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nayoung Kwon</author>
<author>Robert Kluender</author>
<author>Marta Kutas</author>
<author>Maria Polinsky</author>
</authors>
<title>Subject/object processing asymmetries in korean relative clauses:</title>
<date>2013</date>
<booktitle>Evidence from ERP data. Language,</booktitle>
<pages>89--537</pages>
<marker>Kwon, Kluender, Kutas, Polinsky, 2013</marker>
<rawString>Nayoung Kwon, Robert Kluender, Marta Kutas, and Maria Polinsky. 2013. Subject/object processing asymmetries in korean relative clauses: Evidence from ERP data. Language, 89:537–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Memory and surprisal in human sentence comprehension.</title>
<date>2013</date>
<booktitle>Sentence Processing,</booktitle>
<pages>78--114</pages>
<editor>In Roger P. G. van Gompel, editor,</editor>
<publisher>Psychology Press,</publisher>
<location>Hove.</location>
<contexts>
<context position="11689" citStr="Levy, 2013" startWordPosition="2012" endWordPosition="2013">n order for this comparison to be meaningful, we have to make the same methodological assumptions as KGH. First, the difficulty metric only has to account for overall sentence difficulty, it does not necessarily correlate with difficulty at a specific word. More importantly, though, all reported processing difficulties are assumed to be due to memory load. This is a very strong assumption. A plethora of alternative accounts are available in the literature. The contrast between subject gaps and object gaps alone has been explained by information-theoretic notions such as surprisal (Hale, 2003; Levy, 2013), the active filler strategy (Frazier and D’Arcais, 1989), or theta role assignment (Pritchett, 1992), to name but a few (see Lin (2006) and Wu (2009) for extensive surveys). Even those accounts that attribute processing difficulty to memory requirements make ancillary assumptions that are not reflected by the simple memory model entertained here. Gibson’s Dependency Locality Theory (1998), for instance, cru30 cially relies on discourse reference as a means for determining how much of a memory burden is incurred by each word. We take no stance as to whether these accounts are correct. Our prim</context>
</contexts>
<marker>Levy, 2013</marker>
<rawString>Roger Levy. 2013. Memory and surprisal in human sentence comprehension. In Roger P. G. van Gompel, editor, Sentence Processing, pages 78–114. Psychology Press, Hove.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chien-Jer Charles Lin</author>
</authors>
<title>Grammar and Parsing: A Typological Investigation of Relative-Clause Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Arizona.</institution>
<contexts>
<context position="11825" citStr="Lin (2006)" startWordPosition="2034" endWordPosition="2035">ly has to account for overall sentence difficulty, it does not necessarily correlate with difficulty at a specific word. More importantly, though, all reported processing difficulties are assumed to be due to memory load. This is a very strong assumption. A plethora of alternative accounts are available in the literature. The contrast between subject gaps and object gaps alone has been explained by information-theoretic notions such as surprisal (Hale, 2003; Levy, 2013), the active filler strategy (Frazier and D’Arcais, 1989), or theta role assignment (Pritchett, 1992), to name but a few (see Lin (2006) and Wu (2009) for extensive surveys). Even those accounts that attribute processing difficulty to memory requirements make ancillary assumptions that are not reflected by the simple memory model entertained here. Gibson’s Dependency Locality Theory (1998), for instance, cru30 cially relies on discourse reference as a means for determining how much of a memory burden is incurred by each word. We take no stance as to whether these accounts are correct. Our primary interest is the feasibility of a memory-based evaluation metric for Stabler’s top-down parser. Memory is more likely to play a role </context>
</contexts>
<marker>Lin, 2006</marker>
<rawString>Chien-Jer Charles Lin. 2006. Grammar and Parsing: A Typological Investigation of Relative-Clause Processing. Ph.D. thesis, University of Arizona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Michaelis</author>
</authors>
<title>Transforming linear contextfree rewriting systems into minimalist grammars.</title>
<date>2001</date>
<journal>Lecture Notes in Artificial Intelligence,</journal>
<volume>2099</volume>
<pages>244</pages>
<contexts>
<context position="984" citStr="Michaelis, 2001" startWordPosition="139" endWordPosition="140"> the top-down parser for Minimalist grammars developed in Stabler (2012). We investigate the predictions these four metrics make for a number of relative clause constructions, and we conclude that at this point, none of them capture the full range of attested patterns. 1 Introduction Minimalist grammars (MGs; (Stabler, 1997)) are a mildly context-sensitive formalism inspired by Minimalist syntax (Chomsky, 1995), the dominant theory in generative syntax. MGs allow us to evaluate syntactic proposals with respect to computational and cognitive criteria such as generative capacity (Harkema, 2001; Michaelis, 2001) or the memory structures they require (Kobele et al., 2007; Graf, 2012). A new kind of top-down parser for MGs has recently been presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing context-free grammars (CFGs). This raises the question, though, whether derivation trees are a psychologically plausible data structure, and if so, to which extent the Stabler parser makes it possible to test the psycholinguistic predictions of competing syntactic analyses. In order to addr</context>
<context position="5289" citStr="Michaelis, 2001" startWordPosition="885" endWordPosition="886">or example, the sentence John left involves (at least) the following steps under a simplified Minimalist analysis (Adger, 2003): Merge(John :: D− nom−, left:: D+ V−) = [VP left:: V− John:: nom− ] (1) Merge(e :: V+ nom+ T−, (1)) = [TP e :: nom+ T−[VP left John:: nom−] ] (2) Move((2)) = [TP John [T’ e :: T− [VP left ] ] ] (3) This derivation can be represented more succinctly as the derivation tree in Fig 1, where all leaves are labeled by lexical items while unary and binary branching nodes are labeled Move and Merge, respectively. Even though MGs (with the SMC) are weakly equivalent to MCFGs (Michaelis, 2001) and thus mildly context-sensitive in the sense of Joshi (1985), their derivation tree languages can be generated by CFGs (modulo relabeling of interior nodes). As we will see next, this makes it possible to treat MG parsing as a special case of CFG parsing. 2.2 Parsing Minimalist Grammars Thanks to the SMC, the mapping from derivation trees to phrase structure trees is deterministic. Consequently, MG parsing reduces to assigning context-free derivation trees to input sentences, rather than the more complex phrase structure trees. The major difference from CFGs is that the linear order of node</context>
</contexts>
<marker>Michaelis, 2001</marker>
<rawString>Jens Michaelis. 2001. Transforming linear contextfree rewriting systems into minimalist grammars. Lecture Notes in Artificial Intelligence, 2099:228– 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>English as a formal language.</title>
<date>1970</date>
<booktitle>Linguaggi nella Societ e nella Tecnica,</booktitle>
<pages>189--224</pages>
<editor>In Bruno Visentini and et al., editors,</editor>
<location>Milan.</location>
<contexts>
<context position="14629" citStr="Montague, 1970" startWordPosition="2499" endWordPosition="2500">omit all features in our derivation trees and instead use standard X� labels to indicate projection and dashed branches for movement. Like KGH, we adopt a promotion analysis of relative clauses (Vergnaud, 1974; Kayne, 1994). That is to say, the head noun is selected by an empty determiner to form a DP, which starts out as an argument of the embedded verb and undergoes movement into the specifier of the relative clause (which is treated as an NP). The entire relative clause is then selected by the determiner that would usually select the head noun under the traditional, head-external analysis (Montague, 1970; Chomsky, 1977).1 In both derivations the maximum tenure obtains at two points in the matrix clause: I) the unpronounced T-head, and II) the Merge step that introduces the remainder of the VP. The parser must first build the entire subject before it can proceed scanning or expanding material to its right. Consequently, the tenure of these nodes increases with the size of the subject, and since both the SC/RC pattern and the RC/SC pattern necessarily involve large subjects, maximum tenure for both types of sentences is predicted to be relatively high. The parser shows a slightly lower Max valu</context>
</contexts>
<marker>Montague, 1970</marker>
<rawString>Richard Montague. 1970. English as a formal language. In Bruno Visentini and et al., editors, Linguaggi nella Societ e nella Tecnica, pages 189–224. Edizioni di Comunit, Milan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley L Pritchett</author>
</authors>
<title>Grammatical Competence and Parsing Performance.</title>
<date>1992</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="11790" citStr="Pritchett, 1992" startWordPosition="2026" endWordPosition="2027">s as KGH. First, the difficulty metric only has to account for overall sentence difficulty, it does not necessarily correlate with difficulty at a specific word. More importantly, though, all reported processing difficulties are assumed to be due to memory load. This is a very strong assumption. A plethora of alternative accounts are available in the literature. The contrast between subject gaps and object gaps alone has been explained by information-theoretic notions such as surprisal (Hale, 2003; Levy, 2013), the active filler strategy (Frazier and D’Arcais, 1989), or theta role assignment (Pritchett, 1992), to name but a few (see Lin (2006) and Wu (2009) for extensive surveys). Even those accounts that attribute processing difficulty to memory requirements make ancillary assumptions that are not reflected by the simple memory model entertained here. Gibson’s Dependency Locality Theory (1998), for instance, cru30 cially relies on discourse reference as a means for determining how much of a memory burden is incurred by each word. We take no stance as to whether these accounts are correct. Our primary interest is the feasibility of a memory-based evaluation metric for Stabler’s top-down parser. Me</context>
</contexts>
<marker>Pritchett, 1992</marker>
<rawString>Bradley L. Pritchett. 1992. Grammatical Competence and Parsing Performance. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Aravind Joshi</author>
</authors>
<title>A processing model for free word order languages.</title>
<date>1995</date>
<tech>Technical Report IRCS-95-13,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10638" citStr="Rambow and Joshi (1995)" startWordPosition="1843" endWordPosition="1847">ather than 1 to exclude lexical items that are right siblings of another lexical item. In such a case, a single prediction is immediately followed by two consecutive scan steps, which could just as well be thought of as one scan step spanning two words. Nodes with tenure over 2 are highlighted by a box in our derivation trees, hence the name for these two metrics. All four measures are also divided into two subtypes depending on whether unpronounced leaves (e.g. the empty T-head in Fig. 1) are taken into account — this is inspired by the exclusion of unpronounced material in the TAG-parser of Rambow and Joshi (1995). When reporting the values for the metrics, we thus give slashed values of the form m/n, where m is the value with unpronounced leaves and n the value without them. 3.2 Methodological Remarks The following sections investigate the predictions of our difficulty metrics with respect to the embedding of sentential complements versus relative clauses, subject gaps versus object gaps in relative clauses, left embedding, and verb clusters. In order for this comparison to be meaningful, we have to make the same methodological assumptions as KGH. First, the difficulty metric only has to account for o</context>
</contexts>
<marker>Rambow, Joshi, 1995</marker>
<rawString>Owen Rambow and Aravind Joshi. 1995. A processing model for free word order languages. Technical Report IRCS-95-13, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Left-corner parsing and psychological plausibility.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>191--197</pages>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Left-corner parsing and psychological plausibility. In Proceedings of COLING-92, pages 191–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward P Stabler</author>
</authors>
<title>Derivational minimalism.</title>
<date>1997</date>
<booktitle>Logical Aspects of Computational Linguistics,</booktitle>
<volume>1328</volume>
<pages>68--95</pages>
<editor>In Christian Retor´e, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="694" citStr="Stabler, 1997" startWordPosition="97" endWordPosition="98">k Department of Linguistics Department of Linguistics Stony Brook University Stony Brook University mail@thomasgraf.net bradley.marcinek@stonybrook.edu Abstract In response to Kobele et al. (2012), we evaluate four ways of linking the processing difficulty of sentences to the behavior of the top-down parser for Minimalist grammars developed in Stabler (2012). We investigate the predictions these four metrics make for a number of relative clause constructions, and we conclude that at this point, none of them capture the full range of attested patterns. 1 Introduction Minimalist grammars (MGs; (Stabler, 1997)) are a mildly context-sensitive formalism inspired by Minimalist syntax (Chomsky, 1995), the dominant theory in generative syntax. MGs allow us to evaluate syntactic proposals with respect to computational and cognitive criteria such as generative capacity (Harkema, 2001; Michaelis, 2001) or the memory structures they require (Kobele et al., 2007; Graf, 2012). A new kind of top-down parser for MGs has recently been presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing c</context>
<context position="3133" citStr="Stabler, 1997" startWordPosition="497" endWordPosition="498">. 1 on page 8). We conclude that the prospect of a simple direct link between syntactic analysis and processing difficulty is tempting but not sufficiently developed at this point. The paper starts with a quick introduction to MGs (Sec. 2.1) and how they are parsed (Sec. 2.2). Section 3 then introduces three alternatives to Max. Max is then shown to fare worse than those three with respect to well-known contrasts involving relative clauses (Sec. 4). Section 5 briefly looks at three other constructions that pose problems for the alternative metrics. 2 Preliminaries 2.1 Minimalist Grammars MGs (Stabler, 1997; Stabler, 2011a) are a highly lexicalized formalism in which structures are built via the operations Merge and Move. Intuitively, Merge enforces local dependencies via subcategorization, whereas Move establishes long-distance filler-gap dependencies. Every lexical item comes with a non-empty list of unchecked features, and each feature has either positive or negative polarity and is checked by either Merge or Move. Suppose that I) s is a tree whose head has a positive Merge feature F+ as its first unchecked feature, and II) t is a tree whose head has a matching negative Merge feature F− 28 Pr</context>
</contexts>
<marker>Stabler, 1997</marker>
<rawString>Edward P. Stabler. 1997. Derivational minimalism. In Christian Retor´e, editor, Logical Aspects of Computational Linguistics, volume 1328 of Lecture Notes in Computer Science, pages 68–95. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward P Stabler</author>
</authors>
<title>Computational perspectives on minimalism.</title>
<date>2011</date>
<booktitle>In Cedric Boeckx, editor, Oxford Handbook of Linguistic Minimalism,</booktitle>
<pages>617--643</pages>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1140" citStr="Stabler (2011" startWordPosition="167" endWordPosition="168">e constructions, and we conclude that at this point, none of them capture the full range of attested patterns. 1 Introduction Minimalist grammars (MGs; (Stabler, 1997)) are a mildly context-sensitive formalism inspired by Minimalist syntax (Chomsky, 1995), the dominant theory in generative syntax. MGs allow us to evaluate syntactic proposals with respect to computational and cognitive criteria such as generative capacity (Harkema, 2001; Michaelis, 2001) or the memory structures they require (Kobele et al., 2007; Graf, 2012). A new kind of top-down parser for MGs has recently been presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing context-free grammars (CFGs). This raises the question, though, whether derivation trees are a psychologically plausible data structure, and if so, to which extent the Stabler parser makes it possible to test the psycholinguistic predictions of competing syntactic analyses. In order to address this question, a linking hypothesis is needed that connects the behavior of the parser to a processing difficulty metric. Kobele et al. (2012) — hencefo</context>
<context position="3148" citStr="Stabler, 2011" startWordPosition="499" endWordPosition="500"> We conclude that the prospect of a simple direct link between syntactic analysis and processing difficulty is tempting but not sufficiently developed at this point. The paper starts with a quick introduction to MGs (Sec. 2.1) and how they are parsed (Sec. 2.2). Section 3 then introduces three alternatives to Max. Max is then shown to fare worse than those three with respect to well-known contrasts involving relative clauses (Sec. 4). Section 5 briefly looks at three other constructions that pose problems for the alternative metrics. 2 Preliminaries 2.1 Minimalist Grammars MGs (Stabler, 1997; Stabler, 2011a) are a highly lexicalized formalism in which structures are built via the operations Merge and Move. Intuitively, Merge enforces local dependencies via subcategorization, whereas Move establishes long-distance filler-gap dependencies. Every lexical item comes with a non-empty list of unchecked features, and each feature has either positive or negative polarity and is checked by either Merge or Move. Suppose that I) s is a tree whose head has a positive Merge feature F+ as its first unchecked feature, and II) t is a tree whose head has a matching negative Merge feature F− 28 Proceedings of th</context>
<context position="6272" citStr="Stabler (2011" startWordPosition="1060" endWordPosition="1061">is deterministic. Consequently, MG parsing reduces to assigning context-free derivation trees to input sentences, rather than the more complex phrase structure trees. The major difference from CFGs is that the linear order of nodes in an MG derivation tree does not necessarily match the linear order of words in the input sentence — for instance because a moving phrase remains in its base position in the derivation tree. But as long as one can tell for every MG operation how its output is linearized, these discrepancies in linear order can be taken care of in the inference rules of the parser. Stabler (2011b; 2012) shows how exactly this is done for a parser that constructs derivation trees in a top-down fashion. Intuitively, MG top-down parsing is CFG top-down parsing with a slightly different algorithm for traversing/expanding the tree. Instead of presenting the parser’s full set of inference rules, we adopt KGH’s index notation to indicate how the parser constructs a given derivation. For instance, if a derivation contains the node 5Merge38, this means that the parser makes a prediction at step 5 that Merge occurs at this position in the derivation and keeps this prediction in memory until st</context>
</contexts>
<marker>Stabler, 2011</marker>
<rawString>Edward P. Stabler. 2011a. Computational perspectives on minimalism. In Cedric Boeckx, editor, Oxford Handbook of Linguistic Minimalism, pages 617–643. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward P Stabler</author>
</authors>
<title>Top-down recognizers for MCFGs and MGs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Workshop on Cognitive Modeling and Computational Linguistics.</booktitle>
<note>to appear.</note>
<contexts>
<context position="1140" citStr="Stabler (2011" startWordPosition="167" endWordPosition="168">e constructions, and we conclude that at this point, none of them capture the full range of attested patterns. 1 Introduction Minimalist grammars (MGs; (Stabler, 1997)) are a mildly context-sensitive formalism inspired by Minimalist syntax (Chomsky, 1995), the dominant theory in generative syntax. MGs allow us to evaluate syntactic proposals with respect to computational and cognitive criteria such as generative capacity (Harkema, 2001; Michaelis, 2001) or the memory structures they require (Kobele et al., 2007; Graf, 2012). A new kind of top-down parser for MGs has recently been presented by Stabler (2011b; 2012). Stabler’s parser is noteworthy because it uses derivation trees as a data structure in order to reduce MG parsing to a special case of parsing context-free grammars (CFGs). This raises the question, though, whether derivation trees are a psychologically plausible data structure, and if so, to which extent the Stabler parser makes it possible to test the psycholinguistic predictions of competing syntactic analyses. In order to address this question, a linking hypothesis is needed that connects the behavior of the parser to a processing difficulty metric. Kobele et al. (2012) — hencefo</context>
<context position="3148" citStr="Stabler, 2011" startWordPosition="499" endWordPosition="500"> We conclude that the prospect of a simple direct link between syntactic analysis and processing difficulty is tempting but not sufficiently developed at this point. The paper starts with a quick introduction to MGs (Sec. 2.1) and how they are parsed (Sec. 2.2). Section 3 then introduces three alternatives to Max. Max is then shown to fare worse than those three with respect to well-known contrasts involving relative clauses (Sec. 4). Section 5 briefly looks at three other constructions that pose problems for the alternative metrics. 2 Preliminaries 2.1 Minimalist Grammars MGs (Stabler, 1997; Stabler, 2011a) are a highly lexicalized formalism in which structures are built via the operations Merge and Move. Intuitively, Merge enforces local dependencies via subcategorization, whereas Move establishes long-distance filler-gap dependencies. Every lexical item comes with a non-empty list of unchecked features, and each feature has either positive or negative polarity and is checked by either Merge or Move. Suppose that I) s is a tree whose head has a positive Merge feature F+ as its first unchecked feature, and II) t is a tree whose head has a matching negative Merge feature F− 28 Proceedings of th</context>
<context position="6272" citStr="Stabler (2011" startWordPosition="1060" endWordPosition="1061">is deterministic. Consequently, MG parsing reduces to assigning context-free derivation trees to input sentences, rather than the more complex phrase structure trees. The major difference from CFGs is that the linear order of nodes in an MG derivation tree does not necessarily match the linear order of words in the input sentence — for instance because a moving phrase remains in its base position in the derivation tree. But as long as one can tell for every MG operation how its output is linearized, these discrepancies in linear order can be taken care of in the inference rules of the parser. Stabler (2011b; 2012) shows how exactly this is done for a parser that constructs derivation trees in a top-down fashion. Intuitively, MG top-down parsing is CFG top-down parsing with a slightly different algorithm for traversing/expanding the tree. Instead of presenting the parser’s full set of inference rules, we adopt KGH’s index notation to indicate how the parser constructs a given derivation. For instance, if a derivation contains the node 5Merge38, this means that the parser makes a prediction at step 5 that Merge occurs at this position in the derivation and keeps this prediction in memory until st</context>
</contexts>
<marker>Stabler, 2011</marker>
<rawString>Edward P. Stabler. 2011b. Top-down recognizers for MCFGs and MGs. In Proceedings of the 2011 Workshop on Cognitive Modeling and Computational Linguistics. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward P Stabler</author>
</authors>
<title>Bayesian, minimalist, incremental syntactic analysis.</title>
<date>2012</date>
<booktitle>Topics in Cognitive Science,</booktitle>
<pages>5--611</pages>
<marker>Stabler, 2012</marker>
<rawString>Edward P. Stabler. 2012. Bayesian, minimalist, incremental syntactic analysis. Topics in Cognitive Science, 5:611–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Whitney Tabor</author>
<author>Bruno Galantucci</author>
<author>Daniel Richardson</author>
</authors>
<title>Effects of merely local syntactic coherence on sentence processing.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<pages>50--355</pages>
<contexts>
<context position="12576" citStr="Tabor et al., 2004" startWordPosition="2153" endWordPosition="2156">y assumptions that are not reflected by the simple memory model entertained here. Gibson’s Dependency Locality Theory (1998), for instance, cru30 cially relies on discourse reference as a means for determining how much of a memory burden is incurred by each word. We take no stance as to whether these accounts are correct. Our primary interest is the feasibility of a memory-based evaluation metric for Stabler’s top-down parser. Memory is more likely to play a role in the constructions we look at in the next two sections than in, say, attachment ambiguities or local syntactic coherence effects (Tabor et al., 2004). It may well turn out that memory is not involved at all, but for the purpose of comparing several memory-based metrics, they are the safest starting point. 4 Relative Clauses 4.1 Empirical Generalizations Two major properties of relative clauses are firmly established in the literature (see Gibson (1998) and references therein). • SC/RC &lt; RC/SC A sentential complement containing a relative clause is easier to process than a relative clause containing a sentential complement. • SubjRC &lt; ObjRC A relative clause containing a subject gap is easier to parse than a relative clause containing an ob</context>
</contexts>
<marker>Tabor, Galantucci, Richardson, 2004</marker>
<rawString>Whitney Tabor, Bruno Galantucci, and Daniel Richardson. 2004. Effects of merely local syntactic coherence on sentence processing. Journal of Memory and Language, 50:355–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Roger Vergnaud</author>
</authors>
<title>French Relative Clauses.</title>
<date>1974</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="14224" citStr="Vergnaud, 1974" startWordPosition="2427" endWordPosition="2428">itted the error. b. The reporteri [RC who the senator attacked ti] admitted the error. 4.2 SC/RC and RC/SC We first consider the contrast between relative clauses embedded inside a sentential complement (SC/RC) and relative clauses containing a sentential complement (SC/RC). Figures 2 and 3 on pages 5 and 6 show the augmented derivations for (1a) and (1b), respectively. For the sake of readability, we omit all features in our derivation trees and instead use standard X� labels to indicate projection and dashed branches for movement. Like KGH, we adopt a promotion analysis of relative clauses (Vergnaud, 1974; Kayne, 1994). That is to say, the head noun is selected by an empty determiner to form a DP, which starts out as an argument of the embedded verb and undergoes movement into the specifier of the relative clause (which is treated as an NP). The entire relative clause is then selected by the determiner that would usually select the head noun under the traditional, head-external analysis (Montague, 1970; Chomsky, 1977).1 In both derivations the maximum tenure obtains at two points in the matrix clause: I) the unpronounced T-head, and II) the Merge step that introduces the remainder of the VP. T</context>
</contexts>
<marker>Vergnaud, 1974</marker>
<rawString>Jean-Roger Vergnaud. 1974. French Relative Clauses. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuyun Wu</author>
</authors>
<title>Factors Affecting Relative Clause Processing in Mandarin.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<contexts>
<context position="11839" citStr="Wu (2009)" startWordPosition="2037" endWordPosition="2038">nt for overall sentence difficulty, it does not necessarily correlate with difficulty at a specific word. More importantly, though, all reported processing difficulties are assumed to be due to memory load. This is a very strong assumption. A plethora of alternative accounts are available in the literature. The contrast between subject gaps and object gaps alone has been explained by information-theoretic notions such as surprisal (Hale, 2003; Levy, 2013), the active filler strategy (Frazier and D’Arcais, 1989), or theta role assignment (Pritchett, 1992), to name but a few (see Lin (2006) and Wu (2009) for extensive surveys). Even those accounts that attribute processing difficulty to memory requirements make ancillary assumptions that are not reflected by the simple memory model entertained here. Gibson’s Dependency Locality Theory (1998), for instance, cru30 cially relies on discourse reference as a means for determining how much of a memory burden is incurred by each word. We take no stance as to whether these accounts are correct. Our primary interest is the feasibility of a memory-based evaluation metric for Stabler’s top-down parser. Memory is more likely to play a role in the constru</context>
</contexts>
<marker>Wu, 2009</marker>
<rawString>Fuyun Wu. 2009. Factors Affecting Relative Clause Processing in Mandarin. Ph.D. thesis, University of Southern California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
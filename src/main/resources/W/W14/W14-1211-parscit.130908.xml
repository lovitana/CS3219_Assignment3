<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008247">
<title confidence="0.966552">
EACL - Expansion of Abbreviations in CLinical text
</title>
<author confidence="0.998465">
Lisa Tengstrand*, Be´ata Megyesi*, Aron Henriksson+, Martin Duneld+ and Maria Kvist+
</author>
<affiliation confidence="0.9865695">
*Department of Linguistics and Philology,
Uppsala University, Sweden
</affiliation>
<email confidence="0.899926">
tengstrand@ling.su.se,beata.megyesi@lingfil.uu.se
</email>
<affiliation confidence="0.842395">
+Department of Computer and System Sciences,
Stockholm University, Sweden
</affiliation>
<email confidence="0.992813">
aronhen@dsv.su.se,xmartin@dsv.su.se,maria.kvist@karolinska.se
</email>
<sectionHeader confidence="0.993742" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983904761905">
In the medical domain, especially in clin-
ical texts, non-standard abbreviations are
prevalent, which impairs readability for
patients. To ease the understanding of the
physicians’ notes, abbreviations need to be
identified and expanded to their original
forms. We present a distributional seman-
tic approach to find candidates of the origi-
nal form of the abbreviation, and combine
this with Levenshtein distance to choose
the correct candidate among the semanti-
cally related words. We apply the method
to radiology reports and medical journal
texts, and compare the results to general
Swedish. The results show that the cor-
rect expansion of the abbreviation can be
found in 40% of the cases, an improve-
ment by 24 percentage points compared to
the baseline (0.16), and an increase by 22
percentage points compared to using word
space models alone (0.18).
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999713280701755">
Abbreviations are prevalent in text, especially in
certain text types where the author has either lim-
ited space or time to write the written message and
therefore shortens some words or phrases. This
might, however, make it difficult for the reader
to understand the meaning of the actual abbre-
viation. Although some abbreviations are well-
known, and frequently used by most of us (e.g.,
i.e., pm, etc.), most of the abbreviations used in
specialized domains are often less known to the
public. Interpreting them is not an easy task, as ab-
breviations are often ambiguous and their correct
meaning depends on the context in which they ap-
pear. For example, military and governmental staff
would naturally read EACL as Emergency Action
Checklist, people in the food and beverage busi-
ness might think of the company name EACL, lin-
guists would probably interpret it as the European
Chapter of Chinese Linguistics, while computa-
tional linguists would generally claim that EACL
stands for the European Chapter of the Associa-
tion for Computational Linguistics. However, the
readers of this particular article know, as the title
suggests, that the intended meaning here is the Ex-
pansion of Abbreviations in CLinical text.
It has been shown that abbreviations are fre-
quently occurring in various domains and genres,
such as in historical documents, messages in so-
cial media, as well as in different registers used
by specialists within a particular field of exper-
tise. Clinical texts produced by health care per-
sonnel is an example of the latter. The clinical
texts are communication artifacts, and the clini-
cal setting requires that information is expressed
in an efficient way, resulting in short telegraphic
messages. Physicians and nurses need to docu-
ment their work to describe findings, treatments
and procedures precisely and compactly, often un-
der time pressure.
In recent years, governments and health care ac-
tors have started making electronic health records
accessible, not only to other caretakers, but also
to patients in order to enable them to participate
actively in their own health care processes. How-
ever, several studies have shown that patients have
difficulties to comprehend their own health care
reports and other medical texts due to the different
linguistic features that characterize these, aswell
as to medical jargon and technical terminology
(Elhadad, 2006; Rudd et al., 1999; Keselman et
al., 2007). It has also been shown that physicians
rarely adapt their writing style in order to produce
documents that are accessible to lay readers (Al-
lvin, 2010). Besides the use of different termi-
nologies and technical terms, an important obsta-
cle for patients to comprehend medical texts is the
frequent use of – for the patients unknown – ab-
</bodyText>
<page confidence="0.992058">
94
</page>
<note confidence="0.9941595">
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94–103,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999116782608696">
breviations (Keselman et al., 2007; Adnan et al.,
2010).
In health records, abbreviations, which consti-
tute linguistic units that are inherently difficult to
decode, are commonly used and often non stan-
dard (Skeppstedt, 2012). An important step in
order to increase readability for lay readers is to
translate abbreviated words into their correspond-
ing full length words.
The aim of this study is to explore a distri-
butional semantic approach combined with word
normalization, measured by Levenshtein distance,
to abbreviation expansion. Using distributional
semantic models, which can be applied to large
amounts of data, has been shown to be a viable
approach to extracting candidates for the underly-
ing, original word of an abbreviation. In order to
find the correct expansion among the semantically
related candidates, we apply the Levenshtein dis-
tance measure. We report on experiments on com-
parative studies of various text types in Swedish,
including radiology reports, medical journals and
texts taken from a corpus of general Swedish.
</bodyText>
<sectionHeader confidence="0.98112" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.998295441558442">
An abbreviation is a shorter – abbreviated – form
of a word or phrase, often originating from a tech-
nical term or a named entity. Abbreviations are
typically formed in one of three ways: by (i) clip-
ping the last character sequence of the word (e.g.,
pat for patient or pathology), (ii) merging the ini-
tial letter(s) of the words to form an acronym (e.g.,
UUfor Uppsala University), or (iii) merging some
of the letters – often the initial letter of the sylla-
bles – in the word (e.g., msg for message). Abbre-
viations can also be formed as a combination of
these three categories (e.g., EACL for Expansion
of Abbreviations in CLinical text).
Automatically expanding abbreviations to their
original form has been of interest to computational
linguists as a means to improve text-to-speech, in-
formation retrieval and information extraction sys-
tems. Rule-based systems as well as statistical and
machine learning methods have been proposed to
detect and expand abbreviations. A common com-
ponent of most solutions is their reliance on the as-
sumption that an abbreviation and its correspond-
ing definition will appear in the same text.
Taghva and Gilbreth (1999) present a method
for automatic acronym-definition extraction in
technical literature, where acronym detection is
based on case and token length constraints. The
surrounding text is subsequently searched for pos-
sible definitions corresponding to the detected
acronym using an inexact pattern-matching algo-
rithm. The resulting set of candidate definitions
is then narrowed down by applying the Longest
Common Subsequence (LCS) algorithm (Nakatsu
et al., 1982) to the candidate pairs. They report
98% precision and 93% recall when excluding
acronyms of two or fewer characters.
Park and Byrd (2001), along somewhat similar
lines, propose a hybrid text mining approach for
abbreviation expansion in technical literature. Or-
thographic constraints and stop lists are first used
to detect abbreviations; candidate definitions are
then extracted from the adjacent text based on a set
of pre-specified conditions. The abbreviations and
definitions are converted into patterns, for which
transformation rules are constructed. An initial
rule-base comprising the most frequent rules is
subsequently employed for automatic abbreviation
expansion. They report 98% precision and 94%
recall as an average over three document types.
In the medical domain, most approaches to
abbreviation resolution also rely on the co-
occurrence of abbreviations and definitions in a
text, typically by exploiting the fact that abbrevi-
ations are sometimes defined on their first men-
tion. These studies extract candidate abbreviation-
definition pairs by assuming that either the defi-
nition or the abbreviation is written in parenthe-
ses (Schwartz and Hearst, 2003). The process of
determining which of the extracted abbreviation-
definition pairs are likely to be correct is then
performed either by rule-based (Ao and Takagi,
2005) or machine learning (Chang et al., 2002;
Movshovitz-Attias and Cohen, 2012) methods.
Most of these studies have been conducted on
English corpora; however, there is one study on
Swedish medical text (Dann´ells, 2006). There are
problems with this popular approach to abbrevia-
tion expansion: Yu et al. (2002) found that around
75% of all abbreviations in the biomedical litera-
ture are never defined.
The application of this method to clinical text
is even more problematic, as it seems highly un-
likely that abbreviations would be defined in this
way. The telegraphic style of clinical narrative,
with its many non-standard abbreviations, is rea-
sonably explained by time constraints in the clin-
ical setting. There has been some work on iden-
</bodyText>
<page confidence="0.998017">
95
</page>
<bodyText confidence="0.9980927875">
tifying such undefined abbreviations in clinical
text (Isenius et al., 2012), as well as on finding
the intended abbreviation expansion among candi-
dates in an abbreviation dictionary (Gaudan et al.,
2005).
Henriksson et al. (2012; 2014) present a method
for expanding abbreviations in clinical text that
does not require abbreviations to be defined, or
even co-occur, in the text. The method is based
on distributional semantic models by effectively
treating abbreviations and their corresponding def-
inition as synonymous, at least in the sense of shar-
ing distributional properties. Distributional se-
mantics (see Cohen and Widdows (2009) for an
overview) is based on the observation that words
that occur in similar contexts tend to be semanti-
cally related (Harris, 1954). These relationships
are captured in a Random Indexing (RI) word
space model (Kanerva et al., 2000), where se-
mantic similarity between words is represented as
proximity in high-dimensional vector space. The
RI word space representation of a corpus is ob-
tained by assigning to each unique word an ini-
tially empty, n-dimensional context vector, as well
as a static, n-dimensional index vector, which con-
tains a small number of randomly distributed non-
zero elements (-1s and 1s), with the rest of the
elements set to zero1. For each occurrence of a
word in the corpus, the index vectors of the sur-
rounding words are added to the target word’s con-
text vector. The semantic similarity between two
words can then be estimated by calculating, for in-
stance, the cosine similarity between their context
vectors. A set of word space models are induced
from unstructured clinical data and subsequently
combined in various ways with different parame-
ter settings (i.e., sliding window size for extracting
word contexts). The models and their combina-
tions are evaluated for their ability to map a given
abbreviation to its corresponding definition. The
best model achieves 42% recall. Improvement of
the post-processing of candidate definitions is sug-
gested in order to obtain enhanced performance on
this task.
The estimate of word relatedness that is ob-
tained from a word space model is purely statis-
tical and has no linguistic knowledge. When word
pairs should not only share distributional proper-
ties, but also have similar orthographic represen-
1Generating sparse vectors of a sufficiently high dimen-
sionality in this manner ensures that the index vectors will be
nearly orthogonal.
tations – as is the case for abbreviation-definition
pairs – normalization procedures could be ap-
plied. Given a set of candidate definitions for a
given abbreviation, the task of identifying plausi-
ble candidates can be viewed as a normalization
problem. Petterson et al. (2013) utilize a string
distance measure, Levenshtein distance (Leven-
shtein, 1966), in order to normalize historical
spelling of words into modern spelling. Adjusting
parameters, i.e., the maximum allowed distance
between source and target, according to observed
distances between known word pairs of historical
and modern spelling, gives a normalization accu-
racy of 77%. In addition to using a Levenshtein
distance weighting factor of 1, they experiment
with context free and context-sensitive weights for
frequently occurring edits between word pairs in a
training corpus. The context-free weights are cal-
culated on the basis of one-to-one standard edits
involving two characters; in this setting the nor-
malization accuracy is increased to 78.7%. Fre-
quently occurring edits that involve more than two
characters, e.g., substituting two characters for
one, serve as the basis for calculating context-
sensitive weights and gives a normalization accu-
racy of 79.1%. Similar ideas are here applied to
abbreviation expansion by utilizing a normaliza-
tion procedure for candidate expansion selection.
</bodyText>
<sectionHeader confidence="0.986689" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999936666666667">
The current study aims to replicate and extend
a subset of the experiments conducted by Hen-
riksson et al. (2012), namely those that concern
the abbreviation expansion task. This includes
the various word space combinations and the pa-
rameter optimization. The evaluation procedure
is similar to the one described in (Henriksson et
al., 2012). The current study, however, focuses on
post-processing of the semantically related words
by introducing a filter and a normalization proce-
dure in an attempt to improve performance. An
overview of the approach is depicted in Figure 1.
Abbreviation expansion can be viewed as a two-
step procedure, where the first step involves de-
tection, or extraction, of abbreviations, and the
second step involves identifying plausible expan-
sions. Here, the first step is achieved by extracting
abbreviations from a clinical corpus with clinical
abbreviation detection software and using a list of
known medical abbreviations. The second step is
performed by first extracting a set of semantically
</bodyText>
<page confidence="0.97182">
96
</page>
<figure confidence="0.827612">
expansion
word
filtering
Levenshtein
distance
normal-
ization
</figure>
<figureCaption confidence="0.9633275">
Figure 1: The abbreviation expansion process of
the current study.
</figureCaption>
<bodyText confidence="0.9999802">
similar words for each abbreviation and treating
these as initial expansions. More plausible expan-
sions of each abbreviation are then obtained by fil-
tering the expansion words and applying a normal-
ization procedure.
</bodyText>
<subsectionHeader confidence="0.978681">
3.1 Data
3.1.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999924333333334">
Four corpora are used in the experiments: two
clinical corpora, a medical (non-clinical) corpus
and a general Swedish corpus (Table 1).
The clinical corpora are subsets of the Stock-
holm EPR Corpus (Dalianis et al., 2009), com-
prising health records for over one million pa-
tients from 512 clinical units in the Stockholm re-
gion over a five-year period (2006-2010)2. One
of the clinical corpora contains records from vari-
ous clinical units, for the first five months of 2008,
henceforth referred to as SEPR, and the other con-
tains radiology examination reports, produced in
2009 and 2010, the Stockholm EPR X-ray Corpus
(Kvist and Velupillai, 2013) henceforth referred to
as SEPR-X. The clinical corpora were lemmatized
</bodyText>
<footnote confidence="0.796251">
2This research has been approved by the Regional Ethical
Review Board in Stockholm (Etikpr¨ovningsnamnden i Stock-
holm), permission number 2012/2028-31/5
</footnote>
<bodyText confidence="0.992555166666667">
using Granska (Knutsson et al., 2003).
The experiments in the current study also in-
clude a medical corpus. The electronic editions of
L¨akartidningen (Journal of the Swedish Medical
Association), with issues from 1996 to 2010, have
been compiled into a corpus (Kokkinakis, 2012),
here referred to as LTK.
To compare the medical texts to general
Swedish, the third version of the Stockholm Ume˚a
Corpus (SUC 3.0) (K¨allgren, 1998) is used. It is
a balanced corpus and consists of written Swedish
texts from the early 1990’s from various genres.
</bodyText>
<table confidence="0.9980808">
Corpus #Tokens #Types #Lemmas
SEPR 109,663,052 853,341 431,932
SEPR-X 20,290,064 200,703 162,387
LTK 24,406,549 551,456 498,811
SUC 1,166,593 97,124 65,268
</table>
<tableCaption confidence="0.999707">
Table 1: Statistical descriptions of the corpora
</tableCaption>
<subsectionHeader confidence="0.788255">
3.1.2 Reference standards
</subsectionHeader>
<bodyText confidence="0.999963517241379">
A list of medical abbreviation-definition pairs is
used as test data and treated as the reference stan-
dard in the evaluation. The list is derived from
Cederblom (2005) and comprises 6384 unique ab-
breviations from patient records, referrals and sci-
entific articles. To increase the size of the test
data, the 40 most frequent abbreviations are ex-
tracted by a heuristics-based clinical abbreviation
detection tool called SCAN (Isenius et al., 2012).
A domain expert validated these abbreviations and
manually provided the correct expansion(s).
An inherent property of word space models is
that they model semantic relationships between
unigrams. There are, however, abbreviations that
expand into multiword expressions. Ongoing re-
search on modeling semantic composition with
word space models exists, but, in the current study
abbreviations that expanded to multiword defini-
tions were simply removed from the test data set.
The two sets of abbreviation-expansion pairs were
merged into a single test set, containing 1231
unique entries in total.
In order to obtain statistically reliable seman-
tic relations in the word space, the terms of inter-
est must be sufficiently frequent in the data. As a
result, only abbreviation-expansion pairs with fre-
quencies over 50 in SEPR and SEPR-X, respec-
tively, were included in each test set. The SEPR
test set contains 328 entries and the SEPR-X test
</bodyText>
<table confidence="0.627590384615385">
clinical text
abbreviation
extraction
word space
induction
baseline corpus
abbreviations
clinical word space
expansion
word
extraction
abbreviation-candidate expansions
evaluation
</table>
<page confidence="0.996763">
97
</page>
<bodyText confidence="0.9994325">
set contains 211 entries. Each of the two test data
sets is split into a development set (80%) for model
selection, and a test set (20%) for final perfor-
mance estimation.
</bodyText>
<subsectionHeader confidence="0.996518">
3.2 Expansion word extraction
</subsectionHeader>
<bodyText confidence="0.99966256">
For the experiments where semantically related
words were used for extraction of expansion
words, the top 100 most correlated words for each
of the abbreviations were retrieved from each of
the word space model configurations that achieved
the best results in the parameter optimization ex-
periments.
The optimal parameter settings of a word space
vary with the task and data at hand. It has been
shown that when modeling paradigmatic (e.g.,
synonymous) relations in word spaces, a fairly
small context window size is preferable (Sahlgren,
2006). Following the best results of Henriksson et
al. (2012), we experiment with window sizes of
1+1, 2+2, and 4+4.
Two word space algorithms are explored: Ran-
dom Indexing (RI), to retrieve the words that occur
in a similar context as the query term, and Random
Permutation (RP), which also incorporates word
order information when accumulating the context
vectors (Sahlgren et al., 2008). In order to exploit
the advantages of both algorithms, and to combine
models with different parameter settings, RI and
RP model combinations are also evaluated. The
models and their combinations are:
</bodyText>
<listItem confidence="0.971091058823529">
• Random Indexing (RI): words with a contextually high
similarity are returned; word order within the context
window is ignored.
• Random Permutation (RP): words that are contextu-
ally similar and used in the same relative positions are
returned; these are more likely to share grammatical
properties.
• RP-filtered RI candidates (RI RP): returns the top ten
terms in the RI model that are among the top thirty
terms in the RP model.
• RI-filtered RP candidates (RP RI): returns the top ten
terms in the RP model that are among the top thirty
terms in the RI model.
• RI and RP combination of similarity scores (RI+RP):
sums the cosine similarity scores from the two models
for each candidate term and returns the candidates with
the highest aggregate score.
</listItem>
<bodyText confidence="0.999762833333333">
All models are induced with three different con-
text window sizes for the two clinical corpora,
SEPR and SEPR-X. For each corpus, two variants
are used for word space induction, one where stop
words are removed and one where stop words are
retained. All word spaces are induced with a di-
mensionality of 1000.
For parameter optimization and model selec-
tion, the models and model combinations are
queried for semantically similar words. For each
of the abbreviations in the development set, the ten
most similar words are retrieved. Recall is com-
puted with regard to this list of candidate words,
whether the correct expansion is among these ten
candidates. Since the size of the test data is rather
limited, 3-fold cross validation is performed on
the development set for the parameter optimiza-
tion experiments. For both SEPR and SEPR-X de-
velopment sets, a combination of a RI model with
a context window size of 4+4 and a RP model with
4+4 context window size in the summing similar-
ity scores setting were among the most successful
with recall scores of 0.25 for SEPR and 0.17 for
SEPR-X.
</bodyText>
<subsectionHeader confidence="0.99924">
3.3 Filtering expansion words
</subsectionHeader>
<bodyText confidence="0.999995689655173">
Given the expansion words, extracted from clini-
cal word spaces or baseline corpora (the baselines
are more thoroughly accounted for in 3.5), a filter
was applied in order to generate candidate expan-
sions. The filter was defined as a set of require-
ments, which had to be met in order for the expan-
sion word to be extracted as a candidate expansion.
The requirements were that the intitial letter of the
abbreviation and expansion word had to be iden-
tical. All the letters of the abbreviation also had
to be present in the expansion word in the same
order.
String length difference was also a part of the
requirements: the expansion word had to be at
least one character longer than the abbreviation.
In order to define an upper bound for expansion to-
ken length, string length differences of the SEPR
and SEPR-X development sets were obtained.
The distribution of string length differences for
abbreviation-expansion pairs in the SEPR devel-
opment set ranged from 1 to 21 characters. If a
maximum string length difference of 14 was al-
lowed, 95.2% of the abbreviation-expansion pairs
were covered. As for the string length differences
in the SEPR-X development set, the distribution
ranged from 1 to 21 characters. If a string length
difference of up to and including 14 characters
was allowed, 96.3% of the abbreviation-expansion
pairs were covered. Thus, a maximum difference
</bodyText>
<page confidence="0.995256">
98
</page>
<bodyText confidence="0.999933333333333">
in string length of 14 was also required for the ex-
pansion word to be extracted as a candidate expan-
sion.
</bodyText>
<subsectionHeader confidence="0.988162">
3.4 Levenshtein distance normalization
</subsectionHeader>
<bodyText confidence="0.999965391304348">
Given the set of filtered candidate expansions for
the abbreviations, choosing the correct one can be
seen as a normalization problem. The goal is to
map a source word to a target word, similarly to
for instance methods for spelling correction. The
target word is chosen from a list of words, and the
choice is based on the distance between the source
and the target where a small distance implies high
plausibility. However, we cannot adopt the same
assumptions as for the problem of spelling correc-
tion, where the most common distance between a
source word and the correct target word is 1 (Ku-
kich, 1992). Intuitively, we can expect that there
are abbreviations that expand to words within a
larger distance than 1. It would seem somewhat
useless to abbreviate words by one character only,
although it is not entirely improbable.
Similarly to measuring the string length differ-
ence in order to define an upper bound for filtering
candidate expansions, the Levenshtein distances
for abbreviation-expansion pairs in the develop-
ment sets were obtained.
For the SEPR and SEPR-X development sets,
allowing a Levenshtein distance up to and in-
cluding 14 covers 97.8% and 96.6% of the
abbreviation-expansion pairs, as shown in Table 2.
Given the filtered candidate expansions, the
Levenshtein distance for the abbreviation and each
of the candidate expansions were computed. For
each one of the candidate expansions, the Leven-
shtein distance beween the entry and the abbrevi-
ation was associated with the entry. The result-
ing list was sorted in ascending order according to
Levenshtein distance.
Going through the candidate expansion list, if
the Levenshtein distance was less than or identical
to the upper bound for Levenshtein distance (14),
the candidate expansion was added to the expan-
sion list that was subsequently used in the evalu-
ation. In the Levenshtein distance normalization
experiments, a combination of semantically re-
lated words and words from LTK was used. When
compiling the expansion list, semantically related
words were prioritized. This implied that word
space candidate expansion would occupy the top
positions in the expansion list, in ascending order
</bodyText>
<table confidence="0.999785454545455">
SEPR SEPR SEPR-X SEPR-X
LD Avg % SDev Avg % SDev
1 1 0.3 0.4 0.2
2 4.6 0.4 5 0.6
3 13 1.2 14.7 1.3
4 12.2 1 15.1 0.6
5 12.7 1.3 14.5 2.2
6 12.7 0.8 12.9 0.9
7 8.4 0.7 7.8 0.3
8 10.4 1.5 9.8 2
9 5.7 0.7 4.9 0.5
10 4.1 0.7 2.9 0.3
11 3 0.5 2.6 0.4
12 3 0.6 2.6 0.4
13 3.8 5.5 1.3 0.5
14 3.5 1.1 2.2 0.8
15 1.3 0.5 1.3 0.5
16 1.6 0.4 0.4 0.2
17 0.2 0.1
18 0.8 0.3 1 0.1
20 0.2 0.1
21 0.2 0.1 0.5 0
</table>
<tableCaption confidence="0.996066">
Table 2: Levenshtein distance distribution for
</tableCaption>
<bodyText confidence="0.968206416666667">
abbreviation-expansion pairs. Average proportion
over 5 folds at each Levensthein distance with
standard deviation (SDev) in SEPR and SEPR-X
development sets.
according to Levenshtein distance. The size of the
list was restricted to ten, and the remaining posi-
tions, if there were any, were populated by LTK
candidate expansions in ascending order accord-
ing to Levenshtein distance to the abbreviation. If
there were more than one candidate expansion at
a specific Levenshtein distance, ranking of these
was randomized.
</bodyText>
<subsectionHeader confidence="0.914571">
3.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.999929818181818">
The evaluation procedure of the abbreviation ex-
pansion implied assessing the ability of finding the
correct expansions for abbreviations. In order to
evaluate the performance gain of using semantic
similarity to produce the list of candidate expan-
sions over using the filtering and normalization
procedure alone, a baseline was created. For the
baseline, expansion words were instead extracted
from the baseline corpora, the corpus of general
Swedish SUC 3.0 and the medical corpus LTK.
A list of all the lemma forms from each baseline
</bodyText>
<page confidence="0.996914">
99
</page>
<bodyText confidence="0.99991859375">
corpus (separately) was provided for each abbre-
viation as initial expansion words. The filter and
normalization procedure was then applied to these
expansion words.
The reference standard contained abbreviation-
expansion pairs, as described in 3.1.2. If any of the
correct expansions (some of the abbreviations had
multiple correct expansions) was present in the ex-
pansion list provided for each abbreviation in the
test set, this was regarded as a true positive. Preci-
sion was computed with regard to the position of
the correct expansion in the list and the number of
expansions in the expansion list, as suggested in
Henriksson (2013). For an abbreviation that ex-
panded to one word only, this implied that the ex-
pansion list besides holding the correct expansion,
also contained nine incorrect expansions, which
was taken into account when computing precision.
The list size was static: ten expansions were pro-
vided for each abbreviation, and this resulted in
an overall low precision. Few of the abbreviations
in the development set expanded to more than one
word, giving a precision of 0.17-0.18 for all exper-
iments.
Results of baseline abbreviation expansion in
the development sets are given in table 3. Recall
is given as an average of 5 folds, as cross valida-
tion was performed. The baseline achieves over-
all low recall, with the lowest score of 0.08 for the
SEPR-X development set using SUC for candidate
expansion extraction. The rest of the recall results
are around 0.11.
</bodyText>
<table confidence="0.999364">
Corpus SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
SUC 0.10 0.05 0.08 0.06
LTK 0.11 0.06 0.11 0.11
</table>
<tableCaption confidence="0.984237">
Table 3: Baseline average recall for SEPR and
SEPR-X development sets.
</tableCaption>
<bodyText confidence="0.911145866666667">
Results from abbreviation expansion using se-
mantically related words with filtering and nor-
malization to refine the selection of expansions on
SEPR and SEPR-X development sets are shown in
Table 4. Recall is given as an average of 5 folds,
as cross validation was performed. The seman-
tically related words are extracted from the word
space model configuration that had the top recall
scores in the parameter optimization experiments
described in 3.2, namely the combination of an
RI model and an RP model both with 4+4 context
window sizes. Recall is increased by 14 percent-
age points for SEPR and 20 percentage points for
SEPR-X when applying filtering and normaliza-
tion to the semantically related words.
</bodyText>
<table confidence="0.998553333333333">
SEPR SEPR SEPR-X SEPR-X
Recall SDev Recall SDev
0.39 0.05 0.37 0.1
</table>
<tableCaption confidence="0.997775">
Table 4: Abbreviation expansion results for SEPR
</tableCaption>
<bodyText confidence="0.706041">
and SEPR-X development sets using the best
model from parameter optimization experiments
(RI.4+4+RP.4+4).
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="method">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.998562">
4.1 Expansion word extraction
</subsectionHeader>
<bodyText confidence="0.999980428571429">
The models and model combinations that had the
best recall scores in the word space parameter op-
timization were also evaluated on the test set. The
models that had top recall scores in 3.2 achieved
0.2 and 0.18 for SEPR and SEPR-X test sets re-
spectively, compared to 0.25 and 0.17 in the word
space parameter optimization.
</bodyText>
<subsectionHeader confidence="0.9994295">
4.2 Filtering expansion words and
Levenshtein normalization
</subsectionHeader>
<bodyText confidence="0.999351">
Abbreviation expansion with filtering and normal-
ization was evaluated on the SEPR and SEPR-X
test sets. The results are summarized in Table 5.
</bodyText>
<table confidence="0.9892816">
SEPR SEPR-X
SUC 0.09 0.16
LTK 0.08 0.14
Expansion word extraction 0.20 0.18
Filtering and normalization 0.38 0.40
</table>
<tableCaption confidence="0.934899">
Table 5: SEPR and SEPR-X test set results in ab-
breviation expansion.
</tableCaption>
<bodyText confidence="0.999064545454545">
Baseline recall scores were 0.09 and 0.08 for
SUC and LTK respectively, showing a lower score
for LTK compared to the results on the SEPR de-
velopment set. For abbreviation expansion (with
filtering and normalization) using semantically re-
lated words in combination with LTK, the best re-
call score was 0.38 for the SEPR test set, com-
pared to 0.39 for the same model evaluated on the
SEPR development set. Compared to the results of
using semantically related words only (expansion
word extraction), recall increased by 18 percent-
</bodyText>
<page confidence="0.982107">
100
</page>
<bodyText confidence="0.9999817">
age points for the same model when filtering and
normalization was applied.
Evaluation on the SEPR-X test set gave higher
recall scores for both baseline corpora compared
to the baseline results for the SEPR-X develop-
ment set: the SUC result increased by 8 percentage
points for recall. For LTK, there was an increase in
recall of 3 percentage points. For the SEPR-X test
set, recall increased by 22 percentage points when
filtering and normalization was applied to seman-
tically related words extracted from the best model
configuration.
In comparison to the results of Henriksson et
al (2012), where recall of the best model is 0.31
without and 0.42 with post-processing of the ex-
pansion words for word spaces induced from the
data set (i.e., an increase in recall by 11 percentage
points), the filtering and normalization procedure
for expansion words of the current study yielded
an increase by 18 percentage points.
</bodyText>
<sectionHeader confidence="0.999627" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999972604651163">
The filter combined with the Levenshtein normali-
sation procedure to refine candidate expansion se-
lection showed a slight improvement compared to
using post-processing, although the normalization
procedure should be elaborated in order to be able
to confidently claim that Levenshtein distance nor-
malization is a better approach to expansion candi-
date selection. A suggestion for future work is to
introduce weights based on frequently occurring
edits between abbreviations and expansions and to
apply these in abbreviation normalization.
The approach presented in this study is limited
to abbreviations that translate into one full length
word. Future research should include handling
multiword expressions, not only unigrams, in or-
der to process acronyms and initialisms.
Recall of the development sets in the word
space parameter optimization experiments showed
higher scores for SEPR (0.25) compared to SEPR-
X (0.17). An explanation to this could be that the
amount of data preprocessing done prior to word
space induction might have varied, in terms of ex-
cluding sentences with little or no clinical con-
tent. This will of course affect word space co-
occurrence information, as word context is accu-
mulated without taking sentence boundaries into
account.
The lemmatization of the clinical text used for
word space induction left some words in their
original form, causing test data and semantically
related words to be morphologically discrepant.
Lemmatization adapted to clinical text might have
improved results. Spelling errors were also fre-
quent in the clinical text, and abbreviations were
sometimes normalized into a misspelled variant of
the correct expansion. In the future, spelling cor-
rection could be added and combined with abbre-
viation expansion.
The impact that this apporach to abbreviation
expansion might have on readability of clinical
texts should also be assessed by means of an ex-
trinsic evaluation, a matter to be pursued in future
research.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999961875">
We presented automatic expansion of abbrevia-
tions consisting of unigram full-length words in
clinical texts. We applied a distributional semantic
approach by using word space models and com-
bined this with Levenshtein distance measures to
choose the correct candidate among the semanti-
cally related words. The results show that the cor-
rect expansion of the abbreviation can be found
in 40% of the cases, an improvement by 24 per-
centage points compared to the baseline (0.16) and
an increase by 22 percentage points compared to
using word space models alone (0.18). Applying
Levenshtein distance to refine the selection of se-
mantically related candidate expansions yields a
total recall of 0.38 and 0.40 for radiology reports
and medical health records, respectively.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999961142857143">
The study was partly funded by the V˚ardal Fun-
dation and supported by the Swedish Foundation
for Strategic Research through the project High-
Performance Data Mining for Drug Effect Detec-
tion (ref. no. IIS11-0053) at Stockholm Univer-
sity, Sweden. The authors would also like to direct
thanks to the reviewers for valuable comments.
</bodyText>
<sectionHeader confidence="0.982629" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.832905571428571">
M. Adnan, J. Warren, and M. Orr. 2010. Assess-
ing text characteristics of electronic discharge sum-
maries and their implications for patient readability.
In Proceedings of the Fourth Australasian Workshop
on Health Informatics and Knowledge Management-
Volume 108, pages 77–84. Australian Computer So-
ciety, Inc.
</bodyText>
<page confidence="0.997126">
101
</page>
<reference confidence="0.996699180952381">
H. Allvin. 2010. Patientjournalen som genre: En text-
och genreanalys om patientjournalers relation till pa-
tientdatalagen. Master’s thesis, Stockholm Univer-
sity.
H. Ao and T. Takagi. 2005. ALICE: an algorithm
to extract abbreviations from MEDLINE. Journal
of the American Medical Informatics Association,
12(5):576–586.
S. Cederblom. 2005. Medicinska f¨orkortningar och
akronymer (In Swedish). Studentlitteratur.
J.T. Chang, H. Sch¨utze, and R.B. Altman. 2002. Creat-
ing an online dictionary of abbreviations from med-
line. Journal of the American Medical Informatics
Association, 9:612–620.
T. Cohen and D. Widdows. 2009. Empirical dis-
tributional semantics: Methods and biomedical ap-
plications. Journal of Biomedical Informatics,
42(2):390–405.
H. Dalianis, M. Hassel, and S. Velupillai. 2009. The
Stockholm EPR Corpus – Characteristics and some
initial findings. In Proceedings of the 14th Interna-
tional Symposium on Health Information Manage-
ment Research, pages 243–249.
D. Dann´ells. 2006. Automatic acronym recognition.
In Proceedings of the 11th conference on European
chapter of the Association for Computational Lin-
guistics (EACL), pages 167–170.
N. Elhadad. 2006. User-sensitive text summarization:
Application to the medical domain. Ph.D. thesis,
Columbia University.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in
MEDLINE. Bioinformatics, 21(18):3658–3664,
September.
Z.S. Harris. 1954. Distributional structure. Word,
10:146–162.
A. Henriksson, H. Moen, M. Skeppstedt, A. Eklund,
V. Daudaravicius, and M. Hassel. 2012. Syn-
onym Extraction of Medical Terms from Clinical
Text Using Combinations of Word Space Models.
In Proceedings of Semantic Mining in Biomedicine
(SMBM 2012), pages 10–17.
A. Henriksson, H. Moen, M. Skeppstedt, V. Daudar-
avicius, and M. Duneld. 2014. Synonym extrac-
tion and abbreviation expansion with ensembles of
semantic spaces. Journal of Biomedical Semantics,
5(6).
A. Henriksson. 2013. Semantic Spaces of Clini-
cal Text: Leveraging Distributional Semantics for
Natural Language Processing of Electronic Health
Records. Licentiate thesis, Department of Computer
and Systems Sciences, Stockholm University.
N. Isenius, S. Velupillai, and M. Kvist. 2012.
Initial Results in the Development of SCAN: a
Swedish Clinical Abbreviation Normalizer. In Pro-
ceedings of the CLEF 2012 Workshop on Cross-
Language Evaluation of Methods, Applications, and
Resources for eHealth Document Analysis (CLEFe-
Health2012).
G. K¨allgren. 1998. Documentation of the Stockholm-
Ume˚a corpus. Department ofLinguistics, Stockholm
University.
P. Kanerva, J. Kristoferson, and A. Holst. 2000. Ran-
dom indexing of text samples for latent semantic
analysis. In Proceedings of the 22nd annual con-
ference of the cognitive science society, page 1036.
A. Keselman, L. Slaughter, C. Arnott-Smith, H. Kim,
G. Divita, A. Browne, C. Tsai, and Q. Zeng-Treitler.
2007. Towards consumer-friendly PHRs: patients
experience with reviewing their health records.
In AMIA Annual Symposium Proceedings, volume
2007, pages 399–403.
O. Knutsson, J. Bigert, and V. Kann. 2003. A ro-
bust shallow parser for Swedish. In Proceedings of
Nodalida.
D. Kokkinakis. 2012. The Journal of the
Swedish Medical Association-a Corpus Resource
for Biomedical Text Mining in Swedish. In Pro-
ceedings of Third Workshop on Building and Eval-
uating Resources for Biomedical Text Mining Work-
shop Programme, page 40.
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys
(CSUR), 24(4):377–439.
M. Kvist and S. Velupillai. 2013. Professional Lan-
guage in Swedish Radiology Reports – Charac-
terization for Patient-Adapted Text Simplification.
In Scandinavian Conference on Health Informatics
2013, pages 55–59.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
D. Movshovitz-Attias and W.W. Cohen. 2012.
Alignment-HMM-based Extraction of Abbrevia-
tions from Biomedical Text. In Proceedings of the
2012 Workshop on Biomedical Natural Language
Processing (BioNLP 2012), pages 47–55.
N. Nakatsu, Y. Kambayashi, and S. Yajima. 1982. A
longest common subsequence algorithm suitable for
similar text strings. Acta Informatica, 18(2):171–
179.
Y. Park and R.J. Byrd. 2001. Hybrid text mining for
finding abbreviations and their definitions. In Pro-
ceedings of the 2001 conference on empirical meth-
ods in natural language processing, pages 126–133.
</reference>
<page confidence="0.980142">
102
</page>
<reference confidence="0.9995710625">
E. Pettersson, B. Megyesi, and J. Nivre. 2013. Nor-
malisation of historical text using context-sensitive
weighted levenshtein distance and compound split-
ting. In Proceedings of the 19th Nordic Conference
of Computational Linguistics (NODALIDA 2013),
pages 163–179.
R.E. Rudd, B.A. Moeykens, and T.C. Colton. 1999.
Health and literacy: a review of medical and pub-
lic health literature. Office of Educational Research
and Improvement.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of the 30th Annual Meeting of the Cog-
nitive Science Society, pages 1300–1305.
M. Sahlgren. 2006. The Word-space model. Ph.D.
thesis, Stockholm University.
A.S. Schwartz and M.A. Hearst. 2003. A simple al-
gorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of Pacific Sympo-
sium on Biocomputing, pages 451–462.
M. Skeppstedt. 2012. From Disorder to Order: Ex-
tracting clinical findings from unstructured text. Li-
centiate thesis, Department of Computer and Sys-
tems Sciences, Stockholm University.
K. Taghva and J. Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition,
1(4):191–198.
H. Yu, G. Hripcsak, and C. Friedman. 2002. Map-
ping abbreviations to full forms in biomedical arti-
cles. Journal of the American Medical Informatics
Association, 9(3):262–272.
</reference>
<page confidence="0.999293">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.744661">
<title confidence="0.949357">EACL - Expansion of Abbreviations in CLinical text</title>
<author confidence="0.880982">Be´ata Megyesi Tengstrand</author>
<author confidence="0.880982">Aron Martin</author>
<author confidence="0.880982">Maria</author>
<affiliation confidence="0.99212075">Department of Linguistics and Uppsala University, of Computer and System Stockholm University,</affiliation>
<abstract confidence="0.993798863636363">In the medical domain, especially in clinical texts, non-standard abbreviations are prevalent, which impairs readability for patients. To ease the understanding of the physicians’ notes, abbreviations need to be identified and expanded to their original forms. We present a distributional semantic approach to find candidates of the original form of the abbreviation, and combine this with Levenshtein distance to choose the correct candidate among the semantically related words. We apply the method to radiology reports and medical journal texts, and compare the results to general Swedish. The results show that the correct expansion of the abbreviation can be found in 40% of the cases, an improvement by 24 percentage points compared to the baseline (0.16), and an increase by 22 percentage points compared to using word space models alone (0.18).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Allvin</author>
</authors>
<title>Patientjournalen som genre: En textoch genreanalys om patientjournalers relation till patientdatalagen. Master’s thesis,</title>
<date>2010</date>
<institution>Stockholm University.</institution>
<contexts>
<context position="3839" citStr="Allvin, 2010" startWordPosition="587" endWordPosition="589">accessible, not only to other caretakers, but also to patients in order to enable them to participate actively in their own health care processes. However, several studies have shown that patients have difficulties to comprehend their own health care reports and other medical texts due to the different linguistic features that characterize these, aswell as to medical jargon and technical terminology (Elhadad, 2006; Rudd et al., 1999; Keselman et al., 2007). It has also been shown that physicians rarely adapt their writing style in order to produce documents that are accessible to lay readers (Allvin, 2010). Besides the use of different terminologies and technical terms, an important obstacle for patients to comprehend medical texts is the frequent use of – for the patients unknown – ab94 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94–103, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics breviations (Keselman et al., 2007; Adnan et al., 2010). In health records, abbreviations, which constitute linguistic units that are inherently difficult to decode, are commonly used an</context>
</contexts>
<marker>Allvin, 2010</marker>
<rawString>H. Allvin. 2010. Patientjournalen som genre: En textoch genreanalys om patientjournalers relation till patientdatalagen. Master’s thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ao</author>
<author>T Takagi</author>
</authors>
<title>ALICE: an algorithm to extract abbreviations from MEDLINE.</title>
<date>2005</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>12</volume>
<issue>5</issue>
<contexts>
<context position="8259" citStr="Ao and Takagi, 2005" startWordPosition="1269" endWordPosition="1272">l as an average over three document types. In the medical domain, most approaches to abbreviation resolution also rely on the cooccurrence of abbreviations and definitions in a text, typically by exploiting the fact that abbreviations are sometimes defined on their first mention. These studies extract candidate abbreviationdefinition pairs by assuming that either the definition or the abbreviation is written in parentheses (Schwartz and Hearst, 2003). The process of determining which of the extracted abbreviationdefinition pairs are likely to be correct is then performed either by rule-based (Ao and Takagi, 2005) or machine learning (Chang et al., 2002; Movshovitz-Attias and Cohen, 2012) methods. Most of these studies have been conducted on English corpora; however, there is one study on Swedish medical text (Dann´ells, 2006). There are problems with this popular approach to abbreviation expansion: Yu et al. (2002) found that around 75% of all abbreviations in the biomedical literature are never defined. The application of this method to clinical text is even more problematic, as it seems highly unlikely that abbreviations would be defined in this way. The telegraphic style of clinical narrative, with</context>
</contexts>
<marker>Ao, Takagi, 2005</marker>
<rawString>H. Ao and T. Takagi. 2005. ALICE: an algorithm to extract abbreviations from MEDLINE. Journal of the American Medical Informatics Association, 12(5):576–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cederblom</author>
</authors>
<title>Medicinska f¨orkortningar och akronymer (In Swedish).</title>
<date>2005</date>
<publisher>Studentlitteratur.</publisher>
<contexts>
<context position="16025" citStr="Cederblom (2005)" startWordPosition="2482" endWordPosition="2483">l texts to general Swedish, the third version of the Stockholm Ume˚a Corpus (SUC 3.0) (K¨allgren, 1998) is used. It is a balanced corpus and consists of written Swedish texts from the early 1990’s from various genres. Corpus #Tokens #Types #Lemmas SEPR 109,663,052 853,341 431,932 SEPR-X 20,290,064 200,703 162,387 LTK 24,406,549 551,456 498,811 SUC 1,166,593 97,124 65,268 Table 1: Statistical descriptions of the corpora 3.1.2 Reference standards A list of medical abbreviation-definition pairs is used as test data and treated as the reference standard in the evaluation. The list is derived from Cederblom (2005) and comprises 6384 unique abbreviations from patient records, referrals and scientific articles. To increase the size of the test data, the 40 most frequent abbreviations are extracted by a heuristics-based clinical abbreviation detection tool called SCAN (Isenius et al., 2012). A domain expert validated these abbreviations and manually provided the correct expansion(s). An inherent property of word space models is that they model semantic relationships between unigrams. There are, however, abbreviations that expand into multiword expressions. Ongoing research on modeling semantic composition</context>
</contexts>
<marker>Cederblom, 2005</marker>
<rawString>S. Cederblom. 2005. Medicinska f¨orkortningar och akronymer (In Swedish). Studentlitteratur.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Chang</author>
<author>H Sch¨utze</author>
<author>R B Altman</author>
</authors>
<title>Creating an online dictionary of abbreviations from medline.</title>
<date>2002</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<pages>9--612</pages>
<marker>Chang, Sch¨utze, Altman, 2002</marker>
<rawString>J.T. Chang, H. Sch¨utze, and R.B. Altman. 2002. Creating an online dictionary of abbreviations from medline. Journal of the American Medical Informatics Association, 9:612–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohen</author>
<author>D Widdows</author>
</authors>
<title>Empirical distributional semantics: Methods and biomedical applications.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>42</volume>
<issue>2</issue>
<contexts>
<context position="9638" citStr="Cohen and Widdows (2009)" startWordPosition="1485" endWordPosition="1488">undefined abbreviations in clinical text (Isenius et al., 2012), as well as on finding the intended abbreviation expansion among candidates in an abbreviation dictionary (Gaudan et al., 2005). Henriksson et al. (2012; 2014) present a method for expanding abbreviations in clinical text that does not require abbreviations to be defined, or even co-occur, in the text. The method is based on distributional semantic models by effectively treating abbreviations and their corresponding definition as synonymous, at least in the sense of sharing distributional properties. Distributional semantics (see Cohen and Widdows (2009) for an overview) is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954). These relationships are captured in a Random Indexing (RI) word space model (Kanerva et al., 2000), where semantic similarity between words is represented as proximity in high-dimensional vector space. The RI word space representation of a corpus is obtained by assigning to each unique word an initially empty, n-dimensional context vector, as well as a static, n-dimensional index vector, which contains a small number of randomly distributed nonzero elements (-</context>
</contexts>
<marker>Cohen, Widdows, 2009</marker>
<rawString>T. Cohen and D. Widdows. 2009. Empirical distributional semantics: Methods and biomedical applications. Journal of Biomedical Informatics, 42(2):390–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dalianis</author>
<author>M Hassel</author>
<author>S Velupillai</author>
</authors>
<title>The Stockholm EPR Corpus – Characteristics and some initial findings.</title>
<date>2009</date>
<booktitle>In Proceedings of the 14th International Symposium on Health Information Management Research,</booktitle>
<pages>243--249</pages>
<contexts>
<context position="14433" citStr="Dalianis et al., 2009" startWordPosition="2234" endWordPosition="2237">extracting a set of semantically 96 expansion word filtering Levenshtein distance normalization Figure 1: The abbreviation expansion process of the current study. similar words for each abbreviation and treating these as initial expansions. More plausible expansions of each abbreviation are then obtained by filtering the expansion words and applying a normalization procedure. 3.1 Data 3.1.1 Corpora Four corpora are used in the experiments: two clinical corpora, a medical (non-clinical) corpus and a general Swedish corpus (Table 1). The clinical corpora are subsets of the Stockholm EPR Corpus (Dalianis et al., 2009), comprising health records for over one million patients from 512 clinical units in the Stockholm region over a five-year period (2006-2010)2. One of the clinical corpora contains records from various clinical units, for the first five months of 2008, henceforth referred to as SEPR, and the other contains radiology examination reports, produced in 2009 and 2010, the Stockholm EPR X-ray Corpus (Kvist and Velupillai, 2013) henceforth referred to as SEPR-X. The clinical corpora were lemmatized 2This research has been approved by the Regional Ethical Review Board in Stockholm (Etikpr¨ovningsnamnd</context>
</contexts>
<marker>Dalianis, Hassel, Velupillai, 2009</marker>
<rawString>H. Dalianis, M. Hassel, and S. Velupillai. 2009. The Stockholm EPR Corpus – Characteristics and some initial findings. In Proceedings of the 14th International Symposium on Health Information Management Research, pages 243–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dann´ells</author>
</authors>
<title>Automatic acronym recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th conference on European chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>167--170</pages>
<marker>Dann´ells, 2006</marker>
<rawString>D. Dann´ells. 2006. Automatic acronym recognition. In Proceedings of the 11th conference on European chapter of the Association for Computational Linguistics (EACL), pages 167–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Elhadad</author>
</authors>
<title>User-sensitive text summarization: Application to the medical domain.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="3643" citStr="Elhadad, 2006" startWordPosition="554" endWordPosition="555"> describe findings, treatments and procedures precisely and compactly, often under time pressure. In recent years, governments and health care actors have started making electronic health records accessible, not only to other caretakers, but also to patients in order to enable them to participate actively in their own health care processes. However, several studies have shown that patients have difficulties to comprehend their own health care reports and other medical texts due to the different linguistic features that characterize these, aswell as to medical jargon and technical terminology (Elhadad, 2006; Rudd et al., 1999; Keselman et al., 2007). It has also been shown that physicians rarely adapt their writing style in order to produce documents that are accessible to lay readers (Allvin, 2010). Besides the use of different terminologies and technical terms, an important obstacle for patients to comprehend medical texts is the frequent use of – for the patients unknown – ab94 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94–103, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Li</context>
</contexts>
<marker>Elhadad, 2006</marker>
<rawString>N. Elhadad. 2006. User-sensitive text summarization: Application to the medical domain. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gaudan</author>
<author>H Kirsch</author>
<author>D Rebholz-Schuhmann</author>
</authors>
<title>Resolving abbreviations to their senses in MEDLINE.</title>
<date>2005</date>
<journal>Bioinformatics,</journal>
<volume>21</volume>
<issue>18</issue>
<contexts>
<context position="9205" citStr="Gaudan et al., 2005" startWordPosition="1420" endWordPosition="1423"> 75% of all abbreviations in the biomedical literature are never defined. The application of this method to clinical text is even more problematic, as it seems highly unlikely that abbreviations would be defined in this way. The telegraphic style of clinical narrative, with its many non-standard abbreviations, is reasonably explained by time constraints in the clinical setting. There has been some work on iden95 tifying such undefined abbreviations in clinical text (Isenius et al., 2012), as well as on finding the intended abbreviation expansion among candidates in an abbreviation dictionary (Gaudan et al., 2005). Henriksson et al. (2012; 2014) present a method for expanding abbreviations in clinical text that does not require abbreviations to be defined, or even co-occur, in the text. The method is based on distributional semantic models by effectively treating abbreviations and their corresponding definition as synonymous, at least in the sense of sharing distributional properties. Distributional semantics (see Cohen and Widdows (2009) for an overview) is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954). These relationships are capture</context>
</contexts>
<marker>Gaudan, Kirsch, Rebholz-Schuhmann, 2005</marker>
<rawString>S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann. 2005. Resolving abbreviations to their senses in MEDLINE. Bioinformatics, 21(18):3658–3664, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--146</pages>
<contexts>
<context position="9772" citStr="Harris, 1954" startWordPosition="1510" endWordPosition="1511"> abbreviation dictionary (Gaudan et al., 2005). Henriksson et al. (2012; 2014) present a method for expanding abbreviations in clinical text that does not require abbreviations to be defined, or even co-occur, in the text. The method is based on distributional semantic models by effectively treating abbreviations and their corresponding definition as synonymous, at least in the sense of sharing distributional properties. Distributional semantics (see Cohen and Widdows (2009) for an overview) is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954). These relationships are captured in a Random Indexing (RI) word space model (Kanerva et al., 2000), where semantic similarity between words is represented as proximity in high-dimensional vector space. The RI word space representation of a corpus is obtained by assigning to each unique word an initially empty, n-dimensional context vector, as well as a static, n-dimensional index vector, which contains a small number of randomly distributed nonzero elements (-1s and 1s), with the rest of the elements set to zero1. For each occurrence of a word in the corpus, the index vectors of the surround</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z.S. Harris. 1954. Distributional structure. Word, 10:146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Henriksson</author>
<author>H Moen</author>
<author>M Skeppstedt</author>
<author>A Eklund</author>
<author>V Daudaravicius</author>
<author>M Hassel</author>
</authors>
<title>Synonym Extraction of Medical Terms from Clinical Text Using Combinations of Word Space Models.</title>
<date>2012</date>
<booktitle>In Proceedings of Semantic Mining in Biomedicine (SMBM</booktitle>
<pages>10--17</pages>
<contexts>
<context position="9230" citStr="Henriksson et al. (2012" startWordPosition="1424" endWordPosition="1427">ons in the biomedical literature are never defined. The application of this method to clinical text is even more problematic, as it seems highly unlikely that abbreviations would be defined in this way. The telegraphic style of clinical narrative, with its many non-standard abbreviations, is reasonably explained by time constraints in the clinical setting. There has been some work on iden95 tifying such undefined abbreviations in clinical text (Isenius et al., 2012), as well as on finding the intended abbreviation expansion among candidates in an abbreviation dictionary (Gaudan et al., 2005). Henriksson et al. (2012; 2014) present a method for expanding abbreviations in clinical text that does not require abbreviations to be defined, or even co-occur, in the text. The method is based on distributional semantic models by effectively treating abbreviations and their corresponding definition as synonymous, at least in the sense of sharing distributional properties. Distributional semantics (see Cohen and Widdows (2009) for an overview) is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954). These relationships are captured in a Random Indexing (R</context>
<context position="12932" citStr="Henriksson et al. (2012)" startWordPosition="2003" endWordPosition="2007"> weights are calculated on the basis of one-to-one standard edits involving two characters; in this setting the normalization accuracy is increased to 78.7%. Frequently occurring edits that involve more than two characters, e.g., substituting two characters for one, serve as the basis for calculating contextsensitive weights and gives a normalization accuracy of 79.1%. Similar ideas are here applied to abbreviation expansion by utilizing a normalization procedure for candidate expansion selection. 3 Method The current study aims to replicate and extend a subset of the experiments conducted by Henriksson et al. (2012), namely those that concern the abbreviation expansion task. This includes the various word space combinations and the parameter optimization. The evaluation procedure is similar to the one described in (Henriksson et al., 2012). The current study, however, focuses on post-processing of the semantically related words by introducing a filter and a normalization procedure in an attempt to improve performance. An overview of the approach is depicted in Figure 1. Abbreviation expansion can be viewed as a twostep procedure, where the first step involves detection, or extraction, of abbreviations, a</context>
<context position="18228" citStr="Henriksson et al. (2012)" startWordPosition="2819" endWordPosition="2822">pansion word extraction For the experiments where semantically related words were used for extraction of expansion words, the top 100 most correlated words for each of the abbreviations were retrieved from each of the word space model configurations that achieved the best results in the parameter optimization experiments. The optimal parameter settings of a word space vary with the task and data at hand. It has been shown that when modeling paradigmatic (e.g., synonymous) relations in word spaces, a fairly small context window size is preferable (Sahlgren, 2006). Following the best results of Henriksson et al. (2012), we experiment with window sizes of 1+1, 2+2, and 4+4. Two word space algorithms are explored: Random Indexing (RI), to retrieve the words that occur in a similar context as the query term, and Random Permutation (RP), which also incorporates word order information when accumulating the context vectors (Sahlgren et al., 2008). In order to exploit the advantages of both algorithms, and to combine models with different parameter settings, RI and RP model combinations are also evaluated. The models and their combinations are: • Random Indexing (RI): words with a contextually high similarity are </context>
<context position="30303" citStr="Henriksson et al (2012)" startWordPosition="4835" endWordPosition="4838">ll increased by 18 percent100 age points for the same model when filtering and normalization was applied. Evaluation on the SEPR-X test set gave higher recall scores for both baseline corpora compared to the baseline results for the SEPR-X development set: the SUC result increased by 8 percentage points for recall. For LTK, there was an increase in recall of 3 percentage points. For the SEPR-X test set, recall increased by 22 percentage points when filtering and normalization was applied to semantically related words extracted from the best model configuration. In comparison to the results of Henriksson et al (2012), where recall of the best model is 0.31 without and 0.42 with post-processing of the expansion words for word spaces induced from the data set (i.e., an increase in recall by 11 percentage points), the filtering and normalization procedure for expansion words of the current study yielded an increase by 18 percentage points. 5 Discussion The filter combined with the Levenshtein normalisation procedure to refine candidate expansion selection showed a slight improvement compared to using post-processing, although the normalization procedure should be elaborated in order to be able to confidently</context>
</contexts>
<marker>Henriksson, Moen, Skeppstedt, Eklund, Daudaravicius, Hassel, 2012</marker>
<rawString>A. Henriksson, H. Moen, M. Skeppstedt, A. Eklund, V. Daudaravicius, and M. Hassel. 2012. Synonym Extraction of Medical Terms from Clinical Text Using Combinations of Word Space Models. In Proceedings of Semantic Mining in Biomedicine (SMBM 2012), pages 10–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Henriksson</author>
<author>H Moen</author>
<author>M Skeppstedt</author>
<author>V Daudaravicius</author>
<author>M Duneld</author>
</authors>
<title>Synonym extraction and abbreviation expansion with ensembles of semantic spaces.</title>
<date>2014</date>
<journal>Journal of Biomedical Semantics,</journal>
<volume>5</volume>
<issue>6</issue>
<marker>Henriksson, Moen, Skeppstedt, Daudaravicius, Duneld, 2014</marker>
<rawString>A. Henriksson, H. Moen, M. Skeppstedt, V. Daudaravicius, and M. Duneld. 2014. Synonym extraction and abbreviation expansion with ensembles of semantic spaces. Journal of Biomedical Semantics, 5(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Henriksson</author>
</authors>
<title>Semantic Spaces of Clinical Text: Leveraging Distributional Semantics for Natural Language Processing of Electronic Health Records. Licentiate thesis,</title>
<date>2013</date>
<institution>Department of Computer and Systems Sciences, Stockholm University.</institution>
<contexts>
<context position="26478" citStr="Henriksson (2013)" startWordPosition="4209" endWordPosition="4210"> was provided for each abbreviation as initial expansion words. The filter and normalization procedure was then applied to these expansion words. The reference standard contained abbreviationexpansion pairs, as described in 3.1.2. If any of the correct expansions (some of the abbreviations had multiple correct expansions) was present in the expansion list provided for each abbreviation in the test set, this was regarded as a true positive. Precision was computed with regard to the position of the correct expansion in the list and the number of expansions in the expansion list, as suggested in Henriksson (2013). For an abbreviation that expanded to one word only, this implied that the expansion list besides holding the correct expansion, also contained nine incorrect expansions, which was taken into account when computing precision. The list size was static: ten expansions were provided for each abbreviation, and this resulted in an overall low precision. Few of the abbreviations in the development set expanded to more than one word, giving a precision of 0.17-0.18 for all experiments. Results of baseline abbreviation expansion in the development sets are given in table 3. Recall is given as an aver</context>
</contexts>
<marker>Henriksson, 2013</marker>
<rawString>A. Henriksson. 2013. Semantic Spaces of Clinical Text: Leveraging Distributional Semantics for Natural Language Processing of Electronic Health Records. Licentiate thesis, Department of Computer and Systems Sciences, Stockholm University. N. Isenius, S. Velupillai, and M. Kvist. 2012.</rawString>
</citation>
<citation valid="false">
<title>Initial Results in the Development of SCAN: a Swedish Clinical Abbreviation Normalizer.</title>
<booktitle>In Proceedings of the CLEF 2012 Workshop on CrossLanguage Evaluation of Methods, Applications, and Resources for eHealth Document Analysis (CLEFeHealth2012).</booktitle>
<marker></marker>
<rawString>Initial Results in the Development of SCAN: a Swedish Clinical Abbreviation Normalizer. In Proceedings of the CLEF 2012 Workshop on CrossLanguage Evaluation of Methods, Applications, and Resources for eHealth Document Analysis (CLEFeHealth2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K¨allgren</author>
</authors>
<title>Documentation of the StockholmUme˚a corpus. Department ofLinguistics,</title>
<date>1998</date>
<institution>Stockholm University.</institution>
<marker>K¨allgren, 1998</marker>
<rawString>G. K¨allgren. 1998. Documentation of the StockholmUme˚a corpus. Department ofLinguistics, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kanerva</author>
<author>J Kristoferson</author>
<author>A Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the 22nd annual conference of the cognitive science society,</booktitle>
<pages>1036</pages>
<contexts>
<context position="9872" citStr="Kanerva et al., 2000" startWordPosition="1524" endWordPosition="1527">hod for expanding abbreviations in clinical text that does not require abbreviations to be defined, or even co-occur, in the text. The method is based on distributional semantic models by effectively treating abbreviations and their corresponding definition as synonymous, at least in the sense of sharing distributional properties. Distributional semantics (see Cohen and Widdows (2009) for an overview) is based on the observation that words that occur in similar contexts tend to be semantically related (Harris, 1954). These relationships are captured in a Random Indexing (RI) word space model (Kanerva et al., 2000), where semantic similarity between words is represented as proximity in high-dimensional vector space. The RI word space representation of a corpus is obtained by assigning to each unique word an initially empty, n-dimensional context vector, as well as a static, n-dimensional index vector, which contains a small number of randomly distributed nonzero elements (-1s and 1s), with the rest of the elements set to zero1. For each occurrence of a word in the corpus, the index vectors of the surrounding words are added to the target word’s context vector. The semantic similarity between two words c</context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>P. Kanerva, J. Kristoferson, and A. Holst. 2000. Random indexing of text samples for latent semantic analysis. In Proceedings of the 22nd annual conference of the cognitive science society, page 1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Keselman</author>
<author>L Slaughter</author>
<author>C Arnott-Smith</author>
<author>H Kim</author>
<author>G Divita</author>
<author>A Browne</author>
<author>C Tsai</author>
<author>Q Zeng-Treitler</author>
</authors>
<title>Towards consumer-friendly PHRs: patients experience with reviewing their health records.</title>
<date>2007</date>
<booktitle>In AMIA Annual Symposium Proceedings,</booktitle>
<volume>volume</volume>
<pages>399--403</pages>
<contexts>
<context position="3686" citStr="Keselman et al., 2007" startWordPosition="560" endWordPosition="563"> procedures precisely and compactly, often under time pressure. In recent years, governments and health care actors have started making electronic health records accessible, not only to other caretakers, but also to patients in order to enable them to participate actively in their own health care processes. However, several studies have shown that patients have difficulties to comprehend their own health care reports and other medical texts due to the different linguistic features that characterize these, aswell as to medical jargon and technical terminology (Elhadad, 2006; Rudd et al., 1999; Keselman et al., 2007). It has also been shown that physicians rarely adapt their writing style in order to produce documents that are accessible to lay readers (Allvin, 2010). Besides the use of different terminologies and technical terms, an important obstacle for patients to comprehend medical texts is the frequent use of – for the patients unknown – ab94 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94–103, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics breviations (Keselman et al., 200</context>
</contexts>
<marker>Keselman, Slaughter, Arnott-Smith, Kim, Divita, Browne, Tsai, Zeng-Treitler, 2007</marker>
<rawString>A. Keselman, L. Slaughter, C. Arnott-Smith, H. Kim, G. Divita, A. Browne, C. Tsai, and Q. Zeng-Treitler. 2007. Towards consumer-friendly PHRs: patients experience with reviewing their health records. In AMIA Annual Symposium Proceedings, volume 2007, pages 399–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Knutsson</author>
<author>J Bigert</author>
<author>V Kann</author>
</authors>
<title>A robust shallow parser for Swedish.</title>
<date>2003</date>
<booktitle>In Proceedings of Nodalida.</booktitle>
<contexts>
<context position="15120" citStr="Knutsson et al., 2003" startWordPosition="2340" endWordPosition="2343">2 clinical units in the Stockholm region over a five-year period (2006-2010)2. One of the clinical corpora contains records from various clinical units, for the first five months of 2008, henceforth referred to as SEPR, and the other contains radiology examination reports, produced in 2009 and 2010, the Stockholm EPR X-ray Corpus (Kvist and Velupillai, 2013) henceforth referred to as SEPR-X. The clinical corpora were lemmatized 2This research has been approved by the Regional Ethical Review Board in Stockholm (Etikpr¨ovningsnamnden i Stockholm), permission number 2012/2028-31/5 using Granska (Knutsson et al., 2003). The experiments in the current study also include a medical corpus. The electronic editions of L¨akartidningen (Journal of the Swedish Medical Association), with issues from 1996 to 2010, have been compiled into a corpus (Kokkinakis, 2012), here referred to as LTK. To compare the medical texts to general Swedish, the third version of the Stockholm Ume˚a Corpus (SUC 3.0) (K¨allgren, 1998) is used. It is a balanced corpus and consists of written Swedish texts from the early 1990’s from various genres. Corpus #Tokens #Types #Lemmas SEPR 109,663,052 853,341 431,932 SEPR-X 20,290,064 200,703 162,</context>
</contexts>
<marker>Knutsson, Bigert, Kann, 2003</marker>
<rawString>O. Knutsson, J. Bigert, and V. Kann. 2003. A robust shallow parser for Swedish. In Proceedings of Nodalida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kokkinakis</author>
</authors>
<title>The Journal of the Swedish Medical Association-a Corpus Resource for Biomedical Text Mining in Swedish.</title>
<date>2012</date>
<booktitle>In Proceedings of Third Workshop on Building and Evaluating Resources for Biomedical Text Mining Workshop Programme,</booktitle>
<pages>40</pages>
<contexts>
<context position="15361" citStr="Kokkinakis, 2012" startWordPosition="2379" endWordPosition="2380">iology examination reports, produced in 2009 and 2010, the Stockholm EPR X-ray Corpus (Kvist and Velupillai, 2013) henceforth referred to as SEPR-X. The clinical corpora were lemmatized 2This research has been approved by the Regional Ethical Review Board in Stockholm (Etikpr¨ovningsnamnden i Stockholm), permission number 2012/2028-31/5 using Granska (Knutsson et al., 2003). The experiments in the current study also include a medical corpus. The electronic editions of L¨akartidningen (Journal of the Swedish Medical Association), with issues from 1996 to 2010, have been compiled into a corpus (Kokkinakis, 2012), here referred to as LTK. To compare the medical texts to general Swedish, the third version of the Stockholm Ume˚a Corpus (SUC 3.0) (K¨allgren, 1998) is used. It is a balanced corpus and consists of written Swedish texts from the early 1990’s from various genres. Corpus #Tokens #Types #Lemmas SEPR 109,663,052 853,341 431,932 SEPR-X 20,290,064 200,703 162,387 LTK 24,406,549 551,456 498,811 SUC 1,166,593 97,124 65,268 Table 1: Statistical descriptions of the corpora 3.1.2 Reference standards A list of medical abbreviation-definition pairs is used as test data and treated as the reference stand</context>
</contexts>
<marker>Kokkinakis, 2012</marker>
<rawString>D. Kokkinakis. 2012. The Journal of the Swedish Medical Association-a Corpus Resource for Biomedical Text Mining in Swedish. In Proceedings of Third Workshop on Building and Evaluating Resources for Biomedical Text Mining Workshop Programme, page 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="22762" citStr="Kukich, 1992" startWordPosition="3587" endWordPosition="3589">zation Given the set of filtered candidate expansions for the abbreviations, choosing the correct one can be seen as a normalization problem. The goal is to map a source word to a target word, similarly to for instance methods for spelling correction. The target word is chosen from a list of words, and the choice is based on the distance between the source and the target where a small distance implies high plausibility. However, we cannot adopt the same assumptions as for the problem of spelling correction, where the most common distance between a source word and the correct target word is 1 (Kukich, 1992). Intuitively, we can expect that there are abbreviations that expand to words within a larger distance than 1. It would seem somewhat useless to abbreviate words by one character only, although it is not entirely improbable. Similarly to measuring the string length difference in order to define an upper bound for filtering candidate expansions, the Levenshtein distances for abbreviation-expansion pairs in the development sets were obtained. For the SEPR and SEPR-X development sets, allowing a Levenshtein distance up to and including 14 covers 97.8% and 96.6% of the abbreviation-expansion pair</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>K. Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys (CSUR), 24(4):377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kvist</author>
<author>S Velupillai</author>
</authors>
<title>Professional Language in Swedish Radiology Reports – Characterization for Patient-Adapted Text Simplification.</title>
<date>2013</date>
<booktitle>In Scandinavian Conference on Health Informatics</booktitle>
<pages>55--59</pages>
<contexts>
<context position="14858" citStr="Kvist and Velupillai, 2013" startWordPosition="2304" endWordPosition="2307">sed in the experiments: two clinical corpora, a medical (non-clinical) corpus and a general Swedish corpus (Table 1). The clinical corpora are subsets of the Stockholm EPR Corpus (Dalianis et al., 2009), comprising health records for over one million patients from 512 clinical units in the Stockholm region over a five-year period (2006-2010)2. One of the clinical corpora contains records from various clinical units, for the first five months of 2008, henceforth referred to as SEPR, and the other contains radiology examination reports, produced in 2009 and 2010, the Stockholm EPR X-ray Corpus (Kvist and Velupillai, 2013) henceforth referred to as SEPR-X. The clinical corpora were lemmatized 2This research has been approved by the Regional Ethical Review Board in Stockholm (Etikpr¨ovningsnamnden i Stockholm), permission number 2012/2028-31/5 using Granska (Knutsson et al., 2003). The experiments in the current study also include a medical corpus. The electronic editions of L¨akartidningen (Journal of the Swedish Medical Association), with issues from 1996 to 2010, have been compiled into a corpus (Kokkinakis, 2012), here referred to as LTK. To compare the medical texts to general Swedish, the third version of </context>
</contexts>
<marker>Kvist, Velupillai, 2013</marker>
<rawString>M. Kvist and S. Velupillai. 2013. Professional Language in Swedish Radiology Reports – Characterization for Patient-Adapted Text Simplification. In Scandinavian Conference on Health Informatics 2013, pages 55–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<booktitle>In Soviet physics doklady,</booktitle>
<volume>10</volume>
<pages>707</pages>
<contexts>
<context position="11799" citStr="Levenshtein, 1966" startWordPosition="1833" endWordPosition="1835">guistic knowledge. When word pairs should not only share distributional properties, but also have similar orthographic represen1Generating sparse vectors of a sufficiently high dimensionality in this manner ensures that the index vectors will be nearly orthogonal. tations – as is the case for abbreviation-definition pairs – normalization procedures could be applied. Given a set of candidate definitions for a given abbreviation, the task of identifying plausible candidates can be viewed as a normalization problem. Petterson et al. (2013) utilize a string distance measure, Levenshtein distance (Levenshtein, 1966), in order to normalize historical spelling of words into modern spelling. Adjusting parameters, i.e., the maximum allowed distance between source and target, according to observed distances between known word pairs of historical and modern spelling, gives a normalization accuracy of 77%. In addition to using a Levenshtein distance weighting factor of 1, they experiment with context free and context-sensitive weights for frequently occurring edits between word pairs in a training corpus. The context-free weights are calculated on the basis of one-to-one standard edits involving two characters;</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V.I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Soviet physics doklady, volume 10, page 707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Movshovitz-Attias</author>
<author>W W Cohen</author>
</authors>
<title>Alignment-HMM-based Extraction of Abbreviations from Biomedical Text.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP</booktitle>
<pages>47--55</pages>
<contexts>
<context position="8335" citStr="Movshovitz-Attias and Cohen, 2012" startWordPosition="1280" endWordPosition="1283">in, most approaches to abbreviation resolution also rely on the cooccurrence of abbreviations and definitions in a text, typically by exploiting the fact that abbreviations are sometimes defined on their first mention. These studies extract candidate abbreviationdefinition pairs by assuming that either the definition or the abbreviation is written in parentheses (Schwartz and Hearst, 2003). The process of determining which of the extracted abbreviationdefinition pairs are likely to be correct is then performed either by rule-based (Ao and Takagi, 2005) or machine learning (Chang et al., 2002; Movshovitz-Attias and Cohen, 2012) methods. Most of these studies have been conducted on English corpora; however, there is one study on Swedish medical text (Dann´ells, 2006). There are problems with this popular approach to abbreviation expansion: Yu et al. (2002) found that around 75% of all abbreviations in the biomedical literature are never defined. The application of this method to clinical text is even more problematic, as it seems highly unlikely that abbreviations would be defined in this way. The telegraphic style of clinical narrative, with its many non-standard abbreviations, is reasonably explained by time constr</context>
</contexts>
<marker>Movshovitz-Attias, Cohen, 2012</marker>
<rawString>D. Movshovitz-Attias and W.W. Cohen. 2012. Alignment-HMM-based Extraction of Abbreviations from Biomedical Text. In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 47–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nakatsu</author>
<author>Y Kambayashi</author>
<author>S Yajima</author>
</authors>
<title>A longest common subsequence algorithm suitable for similar text strings.</title>
<date>1982</date>
<journal>Acta Informatica,</journal>
<volume>18</volume>
<issue>2</issue>
<pages>179</pages>
<contexts>
<context position="6922" citStr="Nakatsu et al., 1982" startWordPosition="1069" endWordPosition="1072">f most solutions is their reliance on the assumption that an abbreviation and its corresponding definition will appear in the same text. Taghva and Gilbreth (1999) present a method for automatic acronym-definition extraction in technical literature, where acronym detection is based on case and token length constraints. The surrounding text is subsequently searched for possible definitions corresponding to the detected acronym using an inexact pattern-matching algorithm. The resulting set of candidate definitions is then narrowed down by applying the Longest Common Subsequence (LCS) algorithm (Nakatsu et al., 1982) to the candidate pairs. They report 98% precision and 93% recall when excluding acronyms of two or fewer characters. Park and Byrd (2001), along somewhat similar lines, propose a hybrid text mining approach for abbreviation expansion in technical literature. Orthographic constraints and stop lists are first used to detect abbreviations; candidate definitions are then extracted from the adjacent text based on a set of pre-specified conditions. The abbreviations and definitions are converted into patterns, for which transformation rules are constructed. An initial rule-base comprising the most </context>
</contexts>
<marker>Nakatsu, Kambayashi, Yajima, 1982</marker>
<rawString>N. Nakatsu, Y. Kambayashi, and S. Yajima. 1982. A longest common subsequence algorithm suitable for similar text strings. Acta Informatica, 18(2):171– 179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Park</author>
<author>R J Byrd</author>
</authors>
<title>Hybrid text mining for finding abbreviations and their definitions.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 conference on empirical methods in natural language processing,</booktitle>
<pages>126--133</pages>
<contexts>
<context position="7060" citStr="Park and Byrd (2001)" startWordPosition="1092" endWordPosition="1095">ghva and Gilbreth (1999) present a method for automatic acronym-definition extraction in technical literature, where acronym detection is based on case and token length constraints. The surrounding text is subsequently searched for possible definitions corresponding to the detected acronym using an inexact pattern-matching algorithm. The resulting set of candidate definitions is then narrowed down by applying the Longest Common Subsequence (LCS) algorithm (Nakatsu et al., 1982) to the candidate pairs. They report 98% precision and 93% recall when excluding acronyms of two or fewer characters. Park and Byrd (2001), along somewhat similar lines, propose a hybrid text mining approach for abbreviation expansion in technical literature. Orthographic constraints and stop lists are first used to detect abbreviations; candidate definitions are then extracted from the adjacent text based on a set of pre-specified conditions. The abbreviations and definitions are converted into patterns, for which transformation rules are constructed. An initial rule-base comprising the most frequent rules is subsequently employed for automatic abbreviation expansion. They report 98% precision and 94% recall as an average over </context>
</contexts>
<marker>Park, Byrd, 2001</marker>
<rawString>Y. Park and R.J. Byrd. 2001. Hybrid text mining for finding abbreviations and their definitions. In Proceedings of the 2001 conference on empirical methods in natural language processing, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pettersson</author>
<author>B Megyesi</author>
<author>J Nivre</author>
</authors>
<title>Normalisation of historical text using context-sensitive weighted levenshtein distance and compound splitting.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA</booktitle>
<pages>163--179</pages>
<marker>Pettersson, Megyesi, Nivre, 2013</marker>
<rawString>E. Pettersson, B. Megyesi, and J. Nivre. 2013. Normalisation of historical text using context-sensitive weighted levenshtein distance and compound splitting. In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013), pages 163–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Rudd</author>
<author>B A Moeykens</author>
<author>T C Colton</author>
</authors>
<title>Health and literacy: a review of medical and public health literature.</title>
<date>1999</date>
<journal>Office of Educational Research and Improvement.</journal>
<contexts>
<context position="3662" citStr="Rudd et al., 1999" startWordPosition="556" endWordPosition="559">ngs, treatments and procedures precisely and compactly, often under time pressure. In recent years, governments and health care actors have started making electronic health records accessible, not only to other caretakers, but also to patients in order to enable them to participate actively in their own health care processes. However, several studies have shown that patients have difficulties to comprehend their own health care reports and other medical texts due to the different linguistic features that characterize these, aswell as to medical jargon and technical terminology (Elhadad, 2006; Rudd et al., 1999; Keselman et al., 2007). It has also been shown that physicians rarely adapt their writing style in order to produce documents that are accessible to lay readers (Allvin, 2010). Besides the use of different terminologies and technical terms, an important obstacle for patients to comprehend medical texts is the frequent use of – for the patients unknown – ab94 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94–103, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics breviatio</context>
</contexts>
<marker>Rudd, Moeykens, Colton, 1999</marker>
<rawString>R.E. Rudd, B.A. Moeykens, and T.C. Colton. 1999. Health and literacy: a review of medical and public health literature. Office of Educational Research and Improvement.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
<author>A Holst</author>
<author>P Kanerva</author>
</authors>
<title>Permutations as a means to encode order in word space.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>1300--1305</pages>
<contexts>
<context position="18556" citStr="Sahlgren et al., 2008" startWordPosition="2872" endWordPosition="2875">timal parameter settings of a word space vary with the task and data at hand. It has been shown that when modeling paradigmatic (e.g., synonymous) relations in word spaces, a fairly small context window size is preferable (Sahlgren, 2006). Following the best results of Henriksson et al. (2012), we experiment with window sizes of 1+1, 2+2, and 4+4. Two word space algorithms are explored: Random Indexing (RI), to retrieve the words that occur in a similar context as the query term, and Random Permutation (RP), which also incorporates word order information when accumulating the context vectors (Sahlgren et al., 2008). In order to exploit the advantages of both algorithms, and to combine models with different parameter settings, RI and RP model combinations are also evaluated. The models and their combinations are: • Random Indexing (RI): words with a contextually high similarity are returned; word order within the context window is ignored. • Random Permutation (RP): words that are contextually similar and used in the same relative positions are returned; these are more likely to share grammatical properties. • RP-filtered RI candidates (RI RP): returns the top ten terms in the RI model that are among the</context>
</contexts>
<marker>Sahlgren, Holst, Kanerva, 2008</marker>
<rawString>M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permutations as a means to encode order in word space. In Proceedings of the 30th Annual Meeting of the Cognitive Science Society, pages 1300–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>The Word-space model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="18172" citStr="Sahlgren, 2006" startWordPosition="2812" endWordPosition="2813"> (20%) for final performance estimation. 3.2 Expansion word extraction For the experiments where semantically related words were used for extraction of expansion words, the top 100 most correlated words for each of the abbreviations were retrieved from each of the word space model configurations that achieved the best results in the parameter optimization experiments. The optimal parameter settings of a word space vary with the task and data at hand. It has been shown that when modeling paradigmatic (e.g., synonymous) relations in word spaces, a fairly small context window size is preferable (Sahlgren, 2006). Following the best results of Henriksson et al. (2012), we experiment with window sizes of 1+1, 2+2, and 4+4. Two word space algorithms are explored: Random Indexing (RI), to retrieve the words that occur in a similar context as the query term, and Random Permutation (RP), which also incorporates word order information when accumulating the context vectors (Sahlgren et al., 2008). In order to exploit the advantages of both algorithms, and to combine models with different parameter settings, RI and RP model combinations are also evaluated. The models and their combinations are: • Random Index</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>M. Sahlgren. 2006. The Word-space model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Schwartz</author>
<author>M A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In Proceedings of Pacific Symposium on Biocomputing,</booktitle>
<pages>451--462</pages>
<contexts>
<context position="8093" citStr="Schwartz and Hearst, 2003" startWordPosition="1243" endWordPosition="1246"> constructed. An initial rule-base comprising the most frequent rules is subsequently employed for automatic abbreviation expansion. They report 98% precision and 94% recall as an average over three document types. In the medical domain, most approaches to abbreviation resolution also rely on the cooccurrence of abbreviations and definitions in a text, typically by exploiting the fact that abbreviations are sometimes defined on their first mention. These studies extract candidate abbreviationdefinition pairs by assuming that either the definition or the abbreviation is written in parentheses (Schwartz and Hearst, 2003). The process of determining which of the extracted abbreviationdefinition pairs are likely to be correct is then performed either by rule-based (Ao and Takagi, 2005) or machine learning (Chang et al., 2002; Movshovitz-Attias and Cohen, 2012) methods. Most of these studies have been conducted on English corpora; however, there is one study on Swedish medical text (Dann´ells, 2006). There are problems with this popular approach to abbreviation expansion: Yu et al. (2002) found that around 75% of all abbreviations in the biomedical literature are never defined. The application of this method to </context>
</contexts>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>A.S. Schwartz and M.A. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. In Proceedings of Pacific Symposium on Biocomputing, pages 451–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Skeppstedt</author>
</authors>
<title>From Disorder to Order: Extracting clinical findings from unstructured text. Licentiate thesis,</title>
<date>2012</date>
<institution>Department of Computer and Systems Sciences, Stockholm University.</institution>
<contexts>
<context position="4478" citStr="Skeppstedt, 2012" startWordPosition="686" endWordPosition="687">ifferent terminologies and technical terms, an important obstacle for patients to comprehend medical texts is the frequent use of – for the patients unknown – ab94 Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 94–103, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics breviations (Keselman et al., 2007; Adnan et al., 2010). In health records, abbreviations, which constitute linguistic units that are inherently difficult to decode, are commonly used and often non standard (Skeppstedt, 2012). An important step in order to increase readability for lay readers is to translate abbreviated words into their corresponding full length words. The aim of this study is to explore a distributional semantic approach combined with word normalization, measured by Levenshtein distance, to abbreviation expansion. Using distributional semantic models, which can be applied to large amounts of data, has been shown to be a viable approach to extracting candidates for the underlying, original word of an abbreviation. In order to find the correct expansion among the semantically related candidates, we</context>
</contexts>
<marker>Skeppstedt, 2012</marker>
<rawString>M. Skeppstedt. 2012. From Disorder to Order: Extracting clinical findings from unstructured text. Licentiate thesis, Department of Computer and Systems Sciences, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Taghva</author>
<author>J Gilbreth</author>
</authors>
<title>Recognizing acronyms and their definitions.</title>
<date>1999</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="6464" citStr="Taghva and Gilbreth (1999)" startWordPosition="1005" endWordPosition="1008">formed as a combination of these three categories (e.g., EACL for Expansion of Abbreviations in CLinical text). Automatically expanding abbreviations to their original form has been of interest to computational linguists as a means to improve text-to-speech, information retrieval and information extraction systems. Rule-based systems as well as statistical and machine learning methods have been proposed to detect and expand abbreviations. A common component of most solutions is their reliance on the assumption that an abbreviation and its corresponding definition will appear in the same text. Taghva and Gilbreth (1999) present a method for automatic acronym-definition extraction in technical literature, where acronym detection is based on case and token length constraints. The surrounding text is subsequently searched for possible definitions corresponding to the detected acronym using an inexact pattern-matching algorithm. The resulting set of candidate definitions is then narrowed down by applying the Longest Common Subsequence (LCS) algorithm (Nakatsu et al., 1982) to the candidate pairs. They report 98% precision and 93% recall when excluding acronyms of two or fewer characters. Park and Byrd (2001), al</context>
</contexts>
<marker>Taghva, Gilbreth, 1999</marker>
<rawString>K. Taghva and J. Gilbreth. 1999. Recognizing acronyms and their definitions. International Journal on Document Analysis and Recognition, 1(4):191–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>G Hripcsak</author>
<author>C Friedman</author>
</authors>
<title>Mapping abbreviations to full forms in biomedical articles.</title>
<date>2002</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="8567" citStr="Yu et al. (2002)" startWordPosition="1317" endWordPosition="1320"> abbreviationdefinition pairs by assuming that either the definition or the abbreviation is written in parentheses (Schwartz and Hearst, 2003). The process of determining which of the extracted abbreviationdefinition pairs are likely to be correct is then performed either by rule-based (Ao and Takagi, 2005) or machine learning (Chang et al., 2002; Movshovitz-Attias and Cohen, 2012) methods. Most of these studies have been conducted on English corpora; however, there is one study on Swedish medical text (Dann´ells, 2006). There are problems with this popular approach to abbreviation expansion: Yu et al. (2002) found that around 75% of all abbreviations in the biomedical literature are never defined. The application of this method to clinical text is even more problematic, as it seems highly unlikely that abbreviations would be defined in this way. The telegraphic style of clinical narrative, with its many non-standard abbreviations, is reasonably explained by time constraints in the clinical setting. There has been some work on iden95 tifying such undefined abbreviations in clinical text (Isenius et al., 2012), as well as on finding the intended abbreviation expansion among candidates in an abbrevi</context>
</contexts>
<marker>Yu, Hripcsak, Friedman, 2002</marker>
<rawString>H. Yu, G. Hripcsak, and C. Friedman. 2002. Mapping abbreviations to full forms in biomedical articles. Journal of the American Medical Informatics Association, 9(3):262–272.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
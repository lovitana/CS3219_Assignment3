<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9928965">
Improving the precision of automatically constructed human-oriented
translation dictionaries
</title>
<author confidence="0.754309">
Alexandra Antonova
</author>
<affiliation confidence="0.499775">
Yandex
</affiliation>
<address confidence="0.370305">
16, Leo Tolstoy St., Moscow, Russia
</address>
<email confidence="0.683483">
antonova@yandex-team.ru
</email>
<sectionHeader confidence="0.989142" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994088">
In this paper we address the problem of
automatic acquisition of a human-oriented
translation dictionary from a large-scale
parallel corpus. The initial translation
equivalents can be extracted with the help
of the techniques and tools developed for
the phrase-table construction in statistical
machine translation. The acquired transla-
tion equivalents usually provide good lexi-
con coverage, but they also contain a large
amount of noise. We propose a super-
vised learning algorithm for the detection
of noisy translations, which takes into ac-
count the context and syntax features, av-
eraged over the sentences in which a given
phrase pair occurred. Across nine Euro-
pean language pairs the number of seri-
ous translation errors is reduced by 43.2%,
compared to a baseline which uses only
phrase-level statistics.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997276235294117">
The automatic acquisition of translation equiva-
lents from parallel texts has been extensively stud-
ied since the 1990s. At the beginning, the acquired
bilingual lexicons had much poorer quality as
compared to the human-built translation dictionar-
ies. The limited size of available parallel corpora
often resulted in small coverage and the imper-
fections of alignment methods introduced a con-
siderable amount of noisy translations. However,
the automatimacally acquired lexicons served as
internal resources for statistical machine trans-
lation (SMT) (Brown et al., 1993), information
retrieval (IR) (McEvan et al., 2002; Velupillai,
2008), or computer-assisted lexicography (Atkins,
1994; Hartmann, 1994).
The current progress in search of web-based
parallel documents (Resnik, 2003; Smith, 2013)
</bodyText>
<note confidence="0.714356666666667">
Alexey Misyurev
Yandex
16, Leo Tolstoy St., Moscow, Russia
</note>
<email confidence="0.355817">
misyurev@yandex-team.ru
</email>
<bodyText confidence="0.997107538461538">
makes it possible to automatically construct large-
scale bilingual lexicons. These lexicons can al-
ready compare in coverage to the traditional trans-
lation dictionaries. Hence a new interesting pos-
sibility arises - to produce automatically acquired
human-oriented translation dictionaries, that have
a practical application. A machine translation sys-
tem can output an automatically generated dictio-
nary entry in response to the short queries. The
percentage of short queries can be quite large, and
the system benefits from showing several possible
translations instead of a single result of machine
translation (Figure 1).
</bodyText>
<figureCaption confidence="0.974947">
Figure 1: Examples of dictionary entries in two
online statistical machine translation systems.
</figureCaption>
<bodyText confidence="0.999835666666667">
The initial translation equivalents for a bilin-
gual lexicon can be extracted with the help of
the techniques and tools developed for the phrase-
table construction in SMT. The widely used word
alignment and phrase extraction algorithms are de-
scribed in Brown et.al (1993) and Och (2004).
Though an SMT phrase-table actually consists of
translation equivalents, it may differ substantially
from a traditional dictionary (Table 1).
</bodyText>
<page confidence="0.984449">
58
</page>
<note confidence="0.9345565">
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 58–66,
Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics
Human-oriented dic- SMT phrase-table
tionary
</note>
<bodyText confidence="0.96860793258427">
Lemmatized entries Words and phrases in
are preferred. all forms are accept-
able.
Only linguistically Any multiword phrase
motivated phrases are is acceptable.
acceptable.
Precision is important. Having lots of low-
Any noise is undesir- probability noise is
able. acceptable, since it is
generally overridden
by better translations.
Table 1: Differences between a human-oriented
dictionary and an SMT phrase-table.
While the problems of lemmatization and se-
lection of linguistically motivated phrases can be
addressed by applying appropriate morphological
and syntactic tools, the problem of noise reduc-
tion is essential for the dictionary quality. The cur-
rent progress in the automatic acquisition of simi-
lar Web documents in different languages (Resnik,
2003; Smith, 2013) allows to collect large-scale
corpora. But the automatically found documents
can be non-parallel, or contain spam, machine
translation, language recognition mistakes, badly
parsed HTML-markup. The noisy parallel sen-
tences can be the source of lots of noisy transla-
tions — unrelated, misspelled, or belonging to a
different language. For example, non-parallel sen-
tences
The apartment is at a height of 36
floors! (English)
La plage est a` 1 minute en
voiture. (French: The beach is 1
minute by car.)
may produce a wrong translation ”apartment -
plage”. Or, automatically translated sentences
The figures in the foreground and back-
ground play off each other well. (En-
glish)
Les chiffres du premier plan et jouer
hors de l’autre bien. (French: The dig-
its of the foreground and play out of the
other well.)
may produce a wrong phrase translation ”figures
in the foreground - chiffres du premier plan”.
An intuitive approach would be to apply noise
filtering to the corpus, not to the lexicon. One
could discard those sentences that deviate too
much from the expected behavior. For example,
sentences that have many unknown words and few
symmetrically aligned words are unlikely to be re-
ally parallel. However, natural language demon-
strates a great variability. A single sentence pair
can deviate strongly from the expected behavior,
and still contain some good translations. On the
other hand, many noisy translations can still pen-
etrate the lexicon, and further noise detection is
necessary.
In a bilingual lexicon we want not just to lower
the probabilities of noisy translations, but to re-
move them completely. This can be regarded as a
binary classification task — the phrase pairs are to
be classified into good and noisy ones.
Different types of information can be com-
bined in a feature vector. We take advantage of
the phrase-level features, such as co-occurrence
counts or translation probabilities, and also pro-
pose a number of sentence-level context features.
To calculate the sentence-level features for a given
phrase-pair, we average the characteristics of all
the sentences where it occurs.
We test the proposed algorithm experimentally,
by constructing the bilingual lexicons for nine lan-
guage pairs. The manually annotated samples
of phrase pairs serve as the data for training su-
pervised classifiers. The experiment shows that
the use of the sentence-level features increases
the classification accuracy, compared to a baseline
which uses only phrase frequencies and translation
probabilities. We compare the accuracy of differ-
ent classifiers and evaluate the importance of dif-
ferent features.
The rest of the paper is organized as follows. In
Section 2 we outline the related work. Section 3
describes our approach to the noise reduction in a
bilingual lexicon and discusses the proposed fea-
tures. We describe our experiments on training
classifiers in Section 4. Section 5 concludes the
paper.
</bodyText>
<sectionHeader confidence="0.995855" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999412">
The methods of extracting a bilingual lexicon from
parallel texts as a part of the alignment process
are discussed in Brown (1993), Melamed (1996),
Tufis¸ and Barbu (2001). Melamed (1996) pro-
poses a method of noise reduction that allows
</bodyText>
<page confidence="0.997988">
59
</page>
<bodyText confidence="0.999972795918367">
to re-estimate and filter out indirect word asso-
ciations. However, he works with a carefully
prepared Hansards parallel corpus and the noise
comes only from the imperfections of statistical
modeling.
Sahlgren (2004) proposes a co-occurrence-
based approach, representing words as high-
dimensional random index vectors. The vectors
of translation equivalents are expected to have
high correlation. Yet, he notes that low-frequency
words do not produce reliable statistics for this
method.
The methods of bilingual lexicon extraction
from comparable texts (Rapp, 1995; Fung, 1998;
Otero, 2007) also deal with the problem of noise
reduction. However, the precision/recall ratio of a
lexicon extracted from comparable corpus is gen-
erally lower. For the purpose of building a human-
oriented dictionary, the parallel texts may provide
the larger coverage and better quality of the trans-
lation equivalents.
The noise reduction task is addressed by some
of the SMT phrase-table pruning techniques. The
most straightforward approach is thresholding on
the translation probability (Koehn et al., 2003).
Moore (2004) proposes the log-likelihood ratio
and Fisher’s exact test to re-estimate word asso-
ciation strength. Johnson et al. (2007) applies
Fisher’s exact test to dramatically reduce the num-
ber of phrase pairs in the phrase-table. They get
rid of phrases that appear as alignment artifacts or
are unlikely to occur again. The implementation
of their algorithm requires a special index of all
parallel corpus in order to enable a quick look-up
for a given phrase pair. Eck et al. (2007) assesses
the phrase pairs based on the actual usage statistics
when translating a large amount of text. Entropy-
based criteria are proposed in Ling et al. (2012),
Zens et al. (2012).
Automatically acquired bilingual lexicons are
capable to reflect many word meanings and trans-
lation patterns, which are often not obvious even
to the professional lexicographers (Sharoff, 2004).
Their content can also be updated regularly to in-
corporate more parallel texts and capture the trans-
lations of new words and expressions. Thus, the
methods allowing to improve the quality of au-
tomatic bilingual lexicons are of practical impor-
tance.
</bodyText>
<sectionHeader confidence="0.766075" genericHeader="method">
3 Noise detection features
</sectionHeader>
<bodyText confidence="0.999930777777778">
We treat the noise recognition task as a binary
classification problem. A set of nonlexical con-
text features is designed to be sensitive to differ-
ent types of noise in the parallel corpus. We ex-
pect that the combination of these features with
the phrase-level features based on co-occurrence
statistics can improve the accuracy of the classifi-
cation and the overall quality of a bilingual lexi-
con.
</bodyText>
<subsectionHeader confidence="0.999633">
3.1 Context feature extraction algorithm
</subsectionHeader>
<bodyText confidence="0.999618">
The procedure of getting the context features
is outlined in Algorithm 1. Unlike Johnson et
al. (2007) we do not rely on any pre-constructed
index of the parallel sentences, because it requires
a lot of RAM on large corpora. Instead we re-
run the phrase extraction algorithm of the Moses
toolkit (Koehn et al., 2007) and update the con-
text features at the moment when a phrase pair t is
found.
</bodyText>
<figure confidence="0.383269555555556">
Algorithm 1 Calculate context features for all lex-
icon entries
Require: Parallel corpus — C; {word-aligned
sentences}
Require: Bilingual lexicon — D; {this is a
phrase-table, derived from C and modified as
descibed in 4.1}
Ensure: V = {¯v(d): d ∈ D}; {resulting fea-
tures}
</figure>
<bodyText confidence="0.266976">
for all d ∈ D do
</bodyText>
<equation confidence="0.955612615384615">
¯v(d) ← 0;
n(d) ← 0;
for all s ∈ C do
T ← PhraseExtraction(s);{Moses func-
tion}
for all t ∈ T do
if t ∈ D then
¯v(t) ← ¯v(t) + SentFeats(s); {Alg. 2}
n(t) ← n(t) + 1;
for all d ∈ D do
¯v(d) ← ¯v(d)/(1 + n(d)); {average,
+1 smoothing}
return V
</equation>
<subsectionHeader confidence="0.996577">
3.2 Sentence-level features
</subsectionHeader>
<bodyText confidence="0.999876">
The phrase extraction algorithms do not preserve
the information about the sentences in which a
given phrase pair occurred, assuming that all the
sentences are equally good. As a result, the
</bodyText>
<page confidence="0.991161">
60
</page>
<bodyText confidence="0.999950916666667">
phrase-level statistics is insufficient in case of a
noisy corpus.
The sentence-level features are designed to
partly restore the information which is lost dur-
ing the phrase extraction process. We try to es-
timate the general characteristics of the whole set
of parallel sentences where a given phrase pair oc-
curred. The proposed sentence-level features rely
on the different sources of information, which are
discussed in 3.2.1, 3.2.2 and 3.2.3. Table 2 pro-
vides illustrating examples of noisy phrase pairs
and sample sentences.
</bodyText>
<subsectionHeader confidence="0.805171">
3.2.1 Word-alignment annotation
</subsectionHeader>
<bodyText confidence="0.9988894">
We use the intersection of direct and reverse
Giza++ (Och and Ney, 2004) alignments as a
heuristic rule to find words reliably aligned to each
other. The alignment information gives rise to sev-
eral sentence-level features:
</bodyText>
<listItem confidence="0.951324">
• UnsafeAlign - percentage of words that are
not symmetrically aligned to each other.
• UnsafeJump - average distance between
the translations of subsequent input words.
• UnsafeDigAlign percentage of unequal
digits among the symmetrically aligned
words.
</listItem>
<bodyText confidence="0.999885222222222">
The UnsafeAlign and UnsafeJump values can
vary in different sentences. However, their being
too large on the whole set of sentences where a
given phrase pair occurred possibly indicates some
systematic noise.
The translations of digits are not included to the
dictionary by themselves. But if a pair of digits is
wrongly aligned, then its nearest context may also
be aligned wrongly.
</bodyText>
<subsectionHeader confidence="0.9887715">
3.2.2 One-side morphological and syntactic
annotation
</subsectionHeader>
<bodyText confidence="0.991069">
The target side of our parallel sentences has been
processed by a rule-based parser. The syntax gives
rise to:
</bodyText>
<listItem confidence="0.917006833333334">
• UnsafeStruct - percentage of words having
no dependence on any other word in the parse
tree.
The morphological annotation participates in:
• OOV - percentage of out-of-vocabulary
words in the sentence.
</listItem>
<bodyText confidence="0.9969866">
The low parse tree connectivity may indicate that
the sentence is ungrammatical or produced by a
poor-quality machine translation system. Sen-
tences containing many out-of-vocabulary words
probably do not belong to the given language. We
compute out-of-vocabulary words according to an
external vocabulary, which is embedded in tagging
and parsing tools. However, instead one can use a
collection of unigrams filtered by some frequency
threshold..
</bodyText>
<figure confidence="0.638736777777778">
gratuit — internet access, Slem = 215
Sample sentence:
Petit d´ejeuner continental de luxe gratuit
Business center with free wireless Internet ac-
cess
UnsafeAlign = 0.387
a` — you, Slem = 586
La plainte a` transmettre
You should submit your complaint
</figure>
<equation confidence="0.953252076923077">
UnsafeJump = 1.75
juin — May, Slem = 35
Membre depuis: 17 juin 2011
Member since: 01 May 2012
UnsafeAlignDig = 0.08
le — Fr, Slem = 24
Edvaldo et le p`ere Antenore
Edvaldo and Fr Antenore
OOV = 0.117
Paris — England, Slem = 54
TERTIALIS (Paris, Paris)
(England)
Punct = 0.117
</equation>
<bodyText confidence="0.6281132">
Table 2: Examples of noisy French-English trans-
lations to which different sentence-level features
may be sensitive. Slem — is the number of
sentences where a lemmatized phrase pair co-
occurred. Sample sentences are provided.
</bodyText>
<subsectionHeader confidence="0.894765">
3.2.3 Surface text
</subsectionHeader>
<bodyText confidence="0.996571">
The surface word tokens can be used for:
</bodyText>
<listItem confidence="0.9450302">
• Punct - percentage of non-word/punctuation
tokens in the sentence.
• Uniqueness - the percentage of unique uni-
grams in both source and target language sen-
tences.
</listItem>
<bodyText confidence="0.985082">
Sentences with lots of punctuation can be un-
natural or contain enumeration. Large enumera-
tion lists are often not exactly parallel and can be
</bodyText>
<page confidence="0.99665">
61
</page>
<bodyText confidence="0.997673333333333">
aligned incorrectly, because punctuation tokens,
like many commas, are easily mapped to each
other. The low Uniqueness possibly indicates
that the sentences containing a given phrase pair
are similar to each other. This can lead to overes-
timated translation probabilities.
</bodyText>
<figure confidence="0.703507142857143">
Algorithm 2 Get features of one sentence pair
(SentFeats)
Require: sentsrc = (w1, ..., wm);
Require: sentdst = (w1, ..., wn);
Require: Alignment matrix — Mm,n : x ∈
{0,1}; {intersection of two Giza++ align-
ments}
</figure>
<equation confidence="0.971133363636364">
Require: oov = (x1, ..., xn), x ∈ {0,1};
{xi = 1 ⇐⇒ sentdst[i] is out-of-
vocabulary}
Require: pnt = (x1, ..., xn), x ∈ {0,1};
{xi = 1 ⇐⇒ sentdst[i] is punctuation}
Require: nohead = (x1, ..., xn), x ∈ {0,1};
{xi = 1 ⇐⇒ sentdst[i] is not dependent
on any other word in the parse}
Ensure: v¯ = (v1, ..., v7); {features}
v¯← 0;
v2 ← n1
</equation>
<bodyText confidence="0.88483175">
xEnohead
Let A be the set of pairs of indices of symmet-
rically aligned words, ordered by the source in-
dices:
</bodyText>
<equation confidence="0.8973215">
A ← {(i, j)  |M(i, j) = 1};
v3 ← 1 |A
−+ |n; {UnsafeAlign}
for all (i, j) ∈ A do
</equation>
<bodyText confidence="0.745494">
if words with indices i, j are unequal digits
then
</bodyText>
<equation confidence="0.984131583333333">
v4 ← v4 + 1;
v4 ← v4
|A|; {UnsafeAlignDig}
1
v5 ←|A |E(i,j)EA ji − ji_1; {Unsa f eJump}
E
v6 ← n1
xEoov
E
v7 ← n1
xEpnt
return v¯
</equation>
<subsectionHeader confidence="0.960608">
3.3 Phrase-level statistics
</subsectionHeader>
<bodyText confidence="0.997658">
Multiple phrase-level features can be derived from
the occurrence and co-occurrence counts, that are
calculated during the phrase extraction procedure
as described in Koehn et. al (2003).
</bodyText>
<listItem confidence="0.995344166666667">
• C(f), C(e), C(e, f) —surface phrase occur-
rence counts.
• Clem(f), Clem(e), Clem(e, f) — same for
lemmatized phrases.
• S(e, f), Slem(e, f) — the number of sen-
tences, in which the surface (or lemmatized)
phrases co-occurred.
•
P(e|f), P(f|e) — translation probabilities
of surface phrases.
• Plem(e|f), Plem(f|e) — translation proba-
bilities of lemmatized phrases.
</listItem>
<bodyText confidence="0.996539">
Some of these features are highly correlated, and
it is hard to tell in advance which subset leads to
better performance.
</bodyText>
<sectionHeader confidence="0.999228" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.999578875">
We conducted experiments on nine language
pairs: German-English, German-Russian, French-
English, French-Russian, Italian-English, Italian-
Russian, Spanish-English, Spanish-Russian and
English-Russian. The parallel corpora consisted
of the sentence-aligned documents automatically
collected from multilingual web-sites.
We implemented the procedure of bilingual lex-
icon construction and the algorithm calculating the
sentence-level features (Section 3).
The annotated phrase pair samples, one for
each language pair, provided positive and nega-
tive examples for training a supervised classifier.
We compared the accuracy of several classifiers
trained on different feature sets. The importance
of different features was evaluated .
</bodyText>
<subsectionHeader confidence="0.997892">
4.1 Bilingual lexicon creation
</subsectionHeader>
<bodyText confidence="0.999977666666667">
We used Giza++ for word alignment and Moses
toolkit for phrase extraction procedure. The fol-
lowing automatic annotation had been provided.
The source side of the parallel corpora had been
processed by a part-of-speech tagger, and each
word had been assigned a lemma based on its tag.
The target side of the parallel corpora, which was
always either English or Russian, was processed
by a rule-based dependency parser, which also
supplied morphological annotations and lemmas.
In the case of English-Russian corpus, the source
side had also been processed by the parser.
</bodyText>
<equation confidence="0.552235">
x; {UnsafeStruct}
x; {OOV }
x; {Punct}
</equation>
<page confidence="0.98437">
62
</page>
<bodyText confidence="0.998980083333333">
The extracted English phrases were restricted
to at most 3 words, provided that they were con-
nected in the dependency tree. The same restric-
tions were imposed on the Russian phrases. The
extracted phrases for all other languages were re-
stricted to single words to avoid the ungrammati-
cal multiword expressions.
Each extracted phrase pair was assigned a lem-
matized key consisting of lemmas of all words
in it. The co-occurrence counts were summed
over all phrase pairs sharing the same key, giv-
ing the aggregate count Clem(e, f). Then a sin-
gle pair was chosen to serve as a best substitute
for a lemmatized lexicon entry. The choice was
made heuristically, based on the morphological at-
tributes and co-occurrence counts.
As a preliminary lexicon cleanup we removed
the phrase pairs which contained punctuation sym-
bols or digits on either side. We also removed the
pairs that co-occurred only once in the corpus. An
example of differences between the size of original
phrase table and the size of bilingual lexicon af-
ter lemmatization and preliminary cleanup is rep-
resented in Table 3.
</bodyText>
<table confidence="0.710801">
Millions of phrase pairs
fr-en fr-ru
Initial 1-3 phrase-table 16.4 30.8
After lemmatization 7.9 6.4
After preliminary cleanup 1.6 0.8
</table>
<tableCaption confidence="0.87123">
Table 3: The number of phrase pairs on different
</tableCaption>
<bodyText confidence="0.75135975">
stages of French-English and French-Russian dic-
tionary creation. Phrase pairs in the initial phrase
table are restricted to at most 1 source word and at
most 3 target words.
</bodyText>
<subsectionHeader confidence="0.985852">
4.2 Experimental data
</subsectionHeader>
<bodyText confidence="0.9940665">
For the experiment we selected random1 transla-
tion equivalents from the nine translation lexicons,
to which no further noise reduction had been ap-
plied. The resulting translation equivalents were
assessed by human experts. The annotation task
was to determine how well a phrase pair fits for a
human-oriented translation dictionary. The anno-
tators classified each translation according to the
following gradation:
Class 0 — difficult to assess.
</bodyText>
<footnote confidence="0.963922">
1Random was used proportionally to the square root of
joint frequency, in order to balance rare and frequent phrase
pairs in the sample.
</footnote>
<bodyText confidence="0.985231368421053">
Class 1 — totally wrong or noisy (e.g.
misspelled);
Class 2 — incorrect or incomplete trans-
lation;
Class 3 — not a mistake, but unneces-
sary translation;
Class 4 — good, but not vital;
Class 5 — vital translation (must be
present in human-built dictionary);
The pairs annotated as 0 usually represented
the translations of unfamiliar words, abbreviations
and the like. Such phrases were excluded from
training and testing. We didn’t use ”acceptable,
but unnecessary” translation pairs either, because
they do not influence the quality of the lexicon.
We treated as negative the phrase pairs that were
annotated as 1 or 2. Analogously, the positive ex-
amples had to belong to 4 or 5 class. The annota-
tion statistics is given in Table 4.
</bodyText>
<table confidence="0.9993546">
Size %Negative %Positive
2340 56.6 28.7
2366 59.9 21.4
2388 55.5 27.2
2384 69.0 24.0
2397 50.3 37.6
2438 72.1 24.5
2461 44.5 31.2
2325 57.0 24.4
2346 27.8 33.2
</table>
<tableCaption confidence="0.73271">
Table 4: Statistics of the annotated data: the num-
ber of annotated phrase pairs, the percentage of
negative and positive examples.
</tableCaption>
<subsectionHeader confidence="0.999604">
4.3 Training setting
</subsectionHeader>
<bodyText confidence="0.999436">
The experiments were run with two different fea-
ture sets:
</bodyText>
<listItem confidence="0.999184">
• Baseline — features based on co-occurrence
counts.
• Full — baseline and sentence-level features.
</listItem>
<bodyText confidence="0.999761142857143">
We had to choose a subset of co-occurrence-based
features experimentally (see, Section 3.3). The
best subset for our data consisted of three features:
log(Slem), log(P(e|f)), log(P(f|e)). In the full
feature set we combined the baseline features and
the sentence-level features calculated as described
in Algorithm 2.
</bodyText>
<figure confidence="0.9970094">
Language
it-ru
it-en
es-ru
es-en
de-ru
de-en
fr-ru
fr-en
en-ru
</figure>
<page confidence="0.998011">
63
</page>
<bodyText confidence="0.998503">
We considered three metrics related to the im-
provement of the lexicon quality:
</bodyText>
<listItem confidence="0.9397848">
• Err — the percentage of prediction errors;
• Err-1 — the percentage of class 1 examples
which were classified as positive.
• F1 — the harmonic mean of precision and re-
call w.r.t. the positive and negative examples;
</listItem>
<bodyText confidence="0.999775">
We used the standard packages of the R pro-
gramming language, to train and tune differ-
ent classifiers: random forest (RF), support vec-
tor machines (SVM), logistic regression (GLM),
Naive Bayes classifier, neural networks, k-Nearest
Neighbors and some of the combinations of these
methods with SVD. To assess the predictive accu-
racy we used repeated random sub-sampling val-
idation. In each of 40 iterations, a 10% test set
was randomly chosen from the dataset, the model
was trained on the rest of the data, and then tested.
The resulting accuracy was averaged over the iter-
ations.
</bodyText>
<table confidence="0.9996628">
Classifier Full feature set Base feature set
%Err %Err-1 %Err %Err-1
RF 19.80 8.31 24.00 14.62
SVM 19.63 9.36 23.49 12.91
GLM 22.74 6.35 25.23 7.30
</table>
<tableCaption confidence="0.694138333333333">
Table 5: Percentage of prediction errors of dif-
ferent classifiers, averaged over the nine language
pairs.
</tableCaption>
<bodyText confidence="0.999411555555556">
The results of RF, SVM and GLM are reported
in Table 5. Though the composition of different
classifiers could perform slightly better, it would
require an individual tuning for each language
pair. For clearness, we use a single classifier (RF)
for the rest of the experiments.
The experiment showed that training on the full
feature set reduced the total amount of prediction
errors by 17.5%, compared to the baseline setting.
The number of false positives among the class 1
examples reduced by 43%. It is also important that
better results were obtained on each of the nine
language pairs, not only on average. In Table 6
the baseline results are shown in brackets and one
can see that F1 diminishes in the baseline setting,
while the percentage of errors goes up. The classi-
fication accuracy depends on the size of the train-
ing set (Table 7).
</bodyText>
<table confidence="0.9997669">
Lang %Err %Err-1 F1
de-en 18.0 (+3.6) 4.0 (+5.2) .562 (-.050)
de-ru 25.7 (+4.0) 13.5 (+6.7) .672 (-.040)
es-en 16.4 (+3.8) 3.2 (+4.0) .610 (-.059)
es-ru 20.6 (+4.7) 8.3 (+6.0) .643 (-.064)
fr-en 20.5 (+1.5) 6.0 (+5.8) .603 (-.031)
fr-ru 21.4 (+6.1) 15.5 (+10.8) .704
it-en 15.2 (+3.3) 3.5 (+2.9) .663 (-.059)
it-ru 19.6 (+5.5) 9.4 (+6.7) .670
en-ru 20.8 (+5.6) 11.5 (+8.8) .797 (-.048)
</table>
<tableCaption confidence="0.971619">
Table 6: Classification quality of the classifier
trained on all features, compared to the baseline
trained only on phrase-level features. The relative
change of the baseline values is given in brackets.
</tableCaption>
<table confidence="0.9970305">
Examples 1700 680 272 108 43
Accuracy .803 .794 .780 .757 .709
</table>
<tableCaption confidence="0.997285">
Table 7: Classification accuracy w.r.t different size
</tableCaption>
<bodyText confidence="0.934447571428571">
of training set averaged over eight language pairs.
We measured the impact of different features,
as described in Breiman (2001), with the help of
the standard function of the R library ”random-
Forest” (Table 8). The three baseline features
were ranked as most important, followed by Un-
safeAlign, OOV, UnsafeJump and others.
</bodyText>
<table confidence="0.999010545454546">
Feature Importance
log(Sl&apos;_.) 35.679
log(P(e|f)) 33.9729
log(P(f|e)) 28.8637
UnsafeAlign 24.3705
OOV 22.8306
UnsafeJump 20.1108
Punct 15.4501
UnsafeStruct 15.1157
Uniqueness 13.5049
UnsafeDigAlign 12.915
</table>
<tableCaption confidence="0.992437">
Table 8: Feature importance measured by
</tableCaption>
<bodyText confidence="0.994662875">
the mean decrease of classification accu-
racy (Breiman, 2001). The value is averaged over
the nine language pairs.
We explored the dependence of the prediction
accuracy on the co-occurrence frequency of a
phrase pair for the classifiers trained on the full
feature set and on the baseline feature set. The re-
sults for German-English and French-English lan-
</bodyText>
<page confidence="0.997931">
64
</page>
<bodyText confidence="0.999905">
guage pairs are shown in Figure 2. The accu-
racy function was smoothed with cubic smooth-
ing spline. The differences in the distribution of
classification errors between language pairs sug-
gest that the nature of the noise can vary for dif-
ferent corpora. The general U shape of the curves
in Figure 2 is partly due to the fact that there are
many true negatives in the low-frequency area, and
many true positives in the high-frequency area.
</bodyText>
<figure confidence="0.9854605">
0 500 1000 1500 2000
0 500 1000 1500
</figure>
<figureCaption confidence="0.998292">
Figure 2: Prediction accuracy of different classi-
</figureCaption>
<bodyText confidence="0.9710887">
fiers w.r.t. the phrase pairs sorted by the ascend-
ing co-occurrence count. The upper plot relates
to the German-English pair, the bottom relates to
French-English pair. The labels rf, svm, glm re-
fer to the classifiers trained on the full feature set;
rf-b, svm-b, glm-b refer to the baseline setting.
Table 9 reports the top English translations of
the French word ”connexion” before the noise re-
duction and shows which variants were recognized
as positive and negative by the RF classifier.
</bodyText>
<table confidence="0.9994892">
English C(e, f) p(f|e) p(elf) RF
connection 58018 0.689 0.374 +
wireless 32630 0.450 0.211 -
free 31775 0.113 0.205 -
wifi 16272 0.382 0.105 -
login 4910 0.443 0.032 +
connectivity 394 0.055 0.003 +
logon 290 0.185 0.002 +
access 276 0.001 0.002 -
link 148 0.001 0.001 -
</table>
<tableCaption confidence="0.994038">
Table 9: English translations of the French word
</tableCaption>
<bodyText confidence="0.74895575">
”connexion”. C(e, f) is the co-occurrence count,
p(f|e), p(e|f) are the translation probabilities of
lemmatized pairs. The last column shows the clas-
sification result.
</bodyText>
<sectionHeader confidence="0.999026" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998475">
The main contributions of this paper are the fol-
lowing. We address the problem of noise reduc-
tion in automatic construction of human-oriented
translation dictionary. We introduce an approach
to increase the precision of automatically acquired
bilingual lexicon, which allows to mitigate the
negative impact of a noisy corpus. Our noise
reduction method relies on the supervised learn-
ing on a small set of annotated translation pairs.
In addition to the phrase-level statistics, such as
co-occurrence counts and translation probabilities,
we propose a set of non-lexical context features
based on the analysis of sentences in which a
phrase pair occurred. The experiment demon-
strates a substantial improvement in the accuracy
of the detection of noisy translations, compared to
a baseline which uses only phrase-level statistics.
We have shown that the proposed noise de-
tection method is applicable to various language
pairs. The alignment-based features can be easily
obtained for any parallel corpus, even if other tools
do not exist. We hope that our noise detection ap-
proach can also be adapted for SMT phrase-tables,
if the initial parallel sentences are still available.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9824664">
B. T. Sue Atkins. 1994. A corpus-based dictionary.
In Oxford-Hachette French Dictionary, Introduction
xix-xxxii. Oxford: Oxford University Press.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing 45 5-32.
</reference>
<figure confidence="0.802502857142857">
rf
svm
glm
rf−b
svm−b
glm−b
rf
svm
glm
rf−b
svm−b
glm−b
0.6 0.7 0.8 0.9
0.70 0.75 0.80 0.85 0.90 0.95
</figure>
<page confidence="0.99571">
65
</page>
<reference confidence="0.997713336363637">
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter estimation. Computational Linguistics,
19(2):263–312, June.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2007.
Translation model pruning via usage statistics for
statistical machine translation. In Human Language
Technologies 2007: The Conference of the NAACL;
Companion Volume, Short Papers, pages 21–24,
Rochester, New York, April. Association for Com-
putational Linguistics
Pascale Fung. 1998. A Statistical View on Bilingual
Lexicon Extraction from Parallel Corpora to Non-
parallel Corpora. Parallel Text Processing: Align-
ment and Use of Translation Corpora. Kluwer Aca-
demic Publishers
Hartmann, R.R.K. 1994. The use of parallel text cor-
pora in the generation of translation equivalents for
bilingual lexicography. In W. Martin, et al. (Eds.),
Euralex 1994 Proceedings (pp. 291-297). Amster-
dam: Vrije Universiteit.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of EMNLP-CoNLL, ACL, Prague, Czech Re-
public, pages 967-975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond˘rej Bojar, Alexandra
Constantin, and Evan Herbst 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180, Prague, Czech Republic
Akira Kumano and Hideki Hirakawa. 1994. Build-
ing An MT Dictionary From Parallel Texts Based
On Linguistic And Statistical Information. COLING
1994: 76-81
Wang Ling, Jo˜ao Grac¸a, Isabel Trancoso and Alan
Black 2012. Entropy-based Pruning for Phrase-
based Machine Translation. In Proceedings of
EMNLP-CoNLL, Association for Computational
Linguistics, Jeju Island, Korea, pp. 972-983
C. J. A. McEwan, I. Ounis, and I. Ruthven. 2002.
Building bilingual dictionaries from parallel web
documents. In Proceedings of the 24th BCS-IRSG
European Colloquium on IR Research, pp. 303-323.
Springer-Verlag.
I. Dan Melamed. 1996. Automatic construction
of clean broad-coverage translation lexicons. In
Proceedings of the 2nd Conference of the Associa-
tion for Machine Translation in the Americas, pages
125–134, Montreal, Canada
I. Dan Melamed. 2000. Models of Translational
Equivalence among Words. Computational Linguis-
tics 26(2), 221-249, June.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Proceed-
ings of the 2004 Conference on Empirical Methods
in Natural Language Processing, Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proceedings of the
38th Annual Meeting of the ACL, pp. 440-447,
Hongkong, China.
Franz Josef Och and Hermann Ney. 2004. The
Alignment Template Approach to Statistical Ma-
chine Translation. Computational Linguistics, vol.
30 (2004), pp. 417-449.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable English and Spanish corpora.
Proceedings of MT Summit XI, pages 191–198.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the ACL 33,
320-322.
Resnik, Philip and Noah A. Smith. 2003. The web as
a parallel corpus.. Computational Linguistics, 29,
pp.349–380
Magnus Sahlgren. 2004. Automatic Bilingual Lexi-
con Acquisition Using Random Indexing. Journal
of Natural Language Engineering, Special Issue on
Parallel Texts, 11.
Serge Sharoff. 2004. Harnessing the lawless: using
comparable corpora to find translation equivalents.
Journal of Applied Linguistics 1(3), 333-350.
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch and
Adam Lopez. 2013. Dirt Cheap Web-Scale Par-
allel Text from the Common Crawl. To appear in
Proceedings of ACL 2013.
Dan Tufis¸ and Ana-Maria Barbu. 2001. Computa-
tional Bilingual Lexicography: Automatic Extrac-
tion of Translation Dictionaries. In International
Journal on Science and Technology of Informa-
tion, Romanian Academy, ISSN 1453-8245, 4/3-4,
pp.325-352
Velupillai, Sumithra, Martin Hassel, and Hercules
Dalianis. 2008. ”Automatic Dictionary Construc-
tion and Identification of Parallel Text Pairs. In Pro-
ceedings of the International Symposium on Using
Corpora in Contrastive and Translation Studies (UC-
CTS).
Richard Zens, Daisy Stanton and Peng Xu. 2012.
A Systematic Comparison of Phrase Table Pruning
Techniques. In Proceedings of EMNLP-CoNLL,
ACL, Jeju Island, Korea, pp. 972-983.
</reference>
<page confidence="0.989015">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.654409">
<title confidence="0.963936">Improving the precision of automatically constructed human-oriented translation dictionaries</title>
<author confidence="0.857738">Alexandra</author>
<address confidence="0.816254">16, Leo Tolstoy St., Moscow,</address>
<email confidence="0.996975">antonova@yandex-team.ru</email>
<abstract confidence="0.997880047619048">In this paper we address the problem of automatic acquisition of a human-oriented translation dictionary from a large-scale parallel corpus. The initial translation equivalents can be extracted with the help of the techniques and tools developed for the phrase-table construction in statistical machine translation. The acquired translation equivalents usually provide good lexicon coverage, but they also contain a large amount of noise. We propose a supervised learning algorithm for the detection of noisy translations, which takes into account the context and syntax features, averaged over the sentences in which a given phrase pair occurred. Across nine European language pairs the number of serious translation errors is reduced by 43.2%, compared to a baseline which uses only phrase-level statistics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B T Sue Atkins</author>
</authors>
<title>A corpus-based dictionary.</title>
<date>1994</date>
<booktitle>In Oxford-Hachette French Dictionary, Introduction xix-xxxii.</booktitle>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<contexts>
<context position="1697" citStr="Atkins, 1994" startWordPosition="242" endWordPosition="243"> has been extensively studied since the 1990s. At the beginning, the acquired bilingual lexicons had much poorer quality as compared to the human-built translation dictionaries. The limited size of available parallel corpora often resulted in small coverage and the imperfections of alignment methods introduced a considerable amount of noisy translations. However, the automatimacally acquired lexicons served as internal resources for statistical machine translation (SMT) (Brown et al., 1993), information retrieval (IR) (McEvan et al., 2002; Velupillai, 2008), or computer-assisted lexicography (Atkins, 1994; Hartmann, 1994). The current progress in search of web-based parallel documents (Resnik, 2003; Smith, 2013) Alexey Misyurev Yandex 16, Leo Tolstoy St., Moscow, Russia misyurev@yandex-team.ru makes it possible to automatically construct largescale bilingual lexicons. These lexicons can already compare in coverage to the traditional translation dictionaries. Hence a new interesting possibility arises - to produce automatically acquired human-oriented translation dictionaries, that have a practical application. A machine translation system can output an automatically generated dictionary entry </context>
</contexts>
<marker>Atkins, 1994</marker>
<rawString>B. T. Sue Atkins. 1994. A corpus-based dictionary. In Oxford-Hachette French Dictionary, Introduction xix-xxxii. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random Forests.</title>
<date>2001</date>
<journal>Machine Learning</journal>
<volume>45</volume>
<pages>5--32</pages>
<contexts>
<context position="24100" citStr="Breiman (2001)" startWordPosition="3851" endWordPosition="3852">3 (-.031) fr-ru 21.4 (+6.1) 15.5 (+10.8) .704 it-en 15.2 (+3.3) 3.5 (+2.9) .663 (-.059) it-ru 19.6 (+5.5) 9.4 (+6.7) .670 en-ru 20.8 (+5.6) 11.5 (+8.8) .797 (-.048) Table 6: Classification quality of the classifier trained on all features, compared to the baseline trained only on phrase-level features. The relative change of the baseline values is given in brackets. Examples 1700 680 272 108 43 Accuracy .803 .794 .780 .757 .709 Table 7: Classification accuracy w.r.t different size of training set averaged over eight language pairs. We measured the impact of different features, as described in Breiman (2001), with the help of the standard function of the R library ”randomForest” (Table 8). The three baseline features were ranked as most important, followed by UnsafeAlign, OOV, UnsafeJump and others. Feature Importance log(Sl&apos;_.) 35.679 log(P(e|f)) 33.9729 log(P(f|e)) 28.8637 UnsafeAlign 24.3705 OOV 22.8306 UnsafeJump 20.1108 Punct 15.4501 UnsafeStruct 15.1157 Uniqueness 13.5049 UnsafeDigAlign 12.915 Table 8: Feature importance measured by the mean decrease of classification accuracy (Breiman, 2001). The value is averaged over the nine language pairs. We explored the dependence of the prediction a</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random Forests. Machine Learning 45 5-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1580" citStr="Brown et al., 1993" startWordPosition="226" endWordPosition="229">h uses only phrase-level statistics. 1 Introduction The automatic acquisition of translation equivalents from parallel texts has been extensively studied since the 1990s. At the beginning, the acquired bilingual lexicons had much poorer quality as compared to the human-built translation dictionaries. The limited size of available parallel corpora often resulted in small coverage and the imperfections of alignment methods introduced a considerable amount of noisy translations. However, the automatimacally acquired lexicons served as internal resources for statistical machine translation (SMT) (Brown et al., 1993), information retrieval (IR) (McEvan et al., 2002; Velupillai, 2008), or computer-assisted lexicography (Atkins, 1994; Hartmann, 1994). The current progress in search of web-based parallel documents (Resnik, 2003; Smith, 2013) Alexey Misyurev Yandex 16, Leo Tolstoy St., Moscow, Russia misyurev@yandex-team.ru makes it possible to automatically construct largescale bilingual lexicons. These lexicons can already compare in coverage to the traditional translation dictionaries. Hence a new interesting possibility arises - to produce automatically acquired human-oriented translation dictionaries, th</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2):263–312, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Translation model pruning via usage statistics for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the NAACL; Companion Volume, Short Papers,</booktitle>
<pages>21--24</pages>
<publisher>Association for Computational Linguistics</publisher>
<location>Rochester, New York,</location>
<contexts>
<context position="8786" citStr="Eck et al. (2007)" startWordPosition="1328" endWordPosition="1331">table pruning techniques. The most straightforward approach is thresholding on the translation probability (Koehn et al., 2003). Moore (2004) proposes the log-likelihood ratio and Fisher’s exact test to re-estimate word association strength. Johnson et al. (2007) applies Fisher’s exact test to dramatically reduce the number of phrase pairs in the phrase-table. They get rid of phrases that appear as alignment artifacts or are unlikely to occur again. The implementation of their algorithm requires a special index of all parallel corpus in order to enable a quick look-up for a given phrase pair. Eck et al. (2007) assesses the phrase pairs based on the actual usage statistics when translating a large amount of text. Entropybased criteria are proposed in Ling et al. (2012), Zens et al. (2012). Automatically acquired bilingual lexicons are capable to reflect many word meanings and translation patterns, which are often not obvious even to the professional lexicographers (Sharoff, 2004). Their content can also be updated regularly to incorporate more parallel texts and capture the translations of new words and expressions. Thus, the methods allowing to improve the quality of automatic bilingual lexicons ar</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2007</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2007. Translation model pruning via usage statistics for statistical machine translation. In Human Language Technologies 2007: The Conference of the NAACL; Companion Volume, Short Papers, pages 21–24, Rochester, New York, April. Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A Statistical View on Bilingual Lexicon Extraction from Parallel Corpora to Nonparallel Corpora. Parallel Text Processing: Alignment and Use of Translation Corpora.</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers</publisher>
<contexts>
<context position="7783" citStr="Fung, 1998" startWordPosition="1171" endWordPosition="1172">d of noise reduction that allows 59 to re-estimate and filter out indirect word associations. However, he works with a carefully prepared Hansards parallel corpus and the noise comes only from the imperfections of statistical modeling. Sahlgren (2004) proposes a co-occurrencebased approach, representing words as highdimensional random index vectors. The vectors of translation equivalents are expected to have high correlation. Yet, he notes that low-frequency words do not produce reliable statistics for this method. The methods of bilingual lexicon extraction from comparable texts (Rapp, 1995; Fung, 1998; Otero, 2007) also deal with the problem of noise reduction. However, the precision/recall ratio of a lexicon extracted from comparable corpus is generally lower. For the purpose of building a humanoriented dictionary, the parallel texts may provide the larger coverage and better quality of the translation equivalents. The noise reduction task is addressed by some of the SMT phrase-table pruning techniques. The most straightforward approach is thresholding on the translation probability (Koehn et al., 2003). Moore (2004) proposes the log-likelihood ratio and Fisher’s exact test to re-estimate</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>Pascale Fung. 1998. A Statistical View on Bilingual Lexicon Extraction from Parallel Corpora to Nonparallel Corpora. Parallel Text Processing: Alignment and Use of Translation Corpora. Kluwer Academic Publishers</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R K Hartmann</author>
</authors>
<title>The use of parallel text corpora in the generation of translation equivalents for bilingual lexicography. In</title>
<date>1994</date>
<booktitle>Proceedings</booktitle>
<pages>291--297</pages>
<institution>Vrije Universiteit.</institution>
<location>Amsterdam:</location>
<contexts>
<context position="1714" citStr="Hartmann, 1994" startWordPosition="244" endWordPosition="245">nsively studied since the 1990s. At the beginning, the acquired bilingual lexicons had much poorer quality as compared to the human-built translation dictionaries. The limited size of available parallel corpora often resulted in small coverage and the imperfections of alignment methods introduced a considerable amount of noisy translations. However, the automatimacally acquired lexicons served as internal resources for statistical machine translation (SMT) (Brown et al., 1993), information retrieval (IR) (McEvan et al., 2002; Velupillai, 2008), or computer-assisted lexicography (Atkins, 1994; Hartmann, 1994). The current progress in search of web-based parallel documents (Resnik, 2003; Smith, 2013) Alexey Misyurev Yandex 16, Leo Tolstoy St., Moscow, Russia misyurev@yandex-team.ru makes it possible to automatically construct largescale bilingual lexicons. These lexicons can already compare in coverage to the traditional translation dictionaries. Hence a new interesting possibility arises - to produce automatically acquired human-oriented translation dictionaries, that have a practical application. A machine translation system can output an automatically generated dictionary entry in response to th</context>
</contexts>
<marker>Hartmann, 1994</marker>
<rawString>Hartmann, R.R.K. 1994. The use of parallel text corpora in the generation of translation equivalents for bilingual lexicography. In W. Martin, et al. (Eds.), Euralex 1994 Proceedings (pp. 291-297). Amsterdam: Vrije Universiteit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL, ACL,</booktitle>
<pages>967--975</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8432" citStr="Johnson et al. (2007)" startWordPosition="1267" endWordPosition="1270">h the problem of noise reduction. However, the precision/recall ratio of a lexicon extracted from comparable corpus is generally lower. For the purpose of building a humanoriented dictionary, the parallel texts may provide the larger coverage and better quality of the translation equivalents. The noise reduction task is addressed by some of the SMT phrase-table pruning techniques. The most straightforward approach is thresholding on the translation probability (Koehn et al., 2003). Moore (2004) proposes the log-likelihood ratio and Fisher’s exact test to re-estimate word association strength. Johnson et al. (2007) applies Fisher’s exact test to dramatically reduce the number of phrase pairs in the phrase-table. They get rid of phrases that appear as alignment artifacts or are unlikely to occur again. The implementation of their algorithm requires a special index of all parallel corpus in order to enable a quick look-up for a given phrase pair. Eck et al. (2007) assesses the phrase pairs based on the actual usage statistics when translating a large amount of text. Entropybased criteria are proposed in Ling et al. (2012), Zens et al. (2012). Automatically acquired bilingual lexicons are capable to reflec</context>
<context position="9981" citStr="Johnson et al. (2007)" startWordPosition="1520" endWordPosition="1523">atic bilingual lexicons are of practical importance. 3 Noise detection features We treat the noise recognition task as a binary classification problem. A set of nonlexical context features is designed to be sensitive to different types of noise in the parallel corpus. We expect that the combination of these features with the phrase-level features based on co-occurrence statistics can improve the accuracy of the classification and the overall quality of a bilingual lexicon. 3.1 Context feature extraction algorithm The procedure of getting the context features is outlined in Algorithm 1. Unlike Johnson et al. (2007) we do not rely on any pre-constructed index of the parallel sentences, because it requires a lot of RAM on large corpora. Instead we rerun the phrase extraction algorithm of the Moses toolkit (Koehn et al., 2007) and update the context features at the moment when a phrase pair t is found. Algorithm 1 Calculate context features for all lexicon entries Require: Parallel corpus — C; {word-aligned sentences} Require: Bilingual lexicon — D; {this is a phrase-table, derived from C and modified as descibed in 4.1} Ensure: V = {¯v(d): d ∈ D}; {resulting features} for all d ∈ D do ¯v(d) ← 0; n(d) ← 0;</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of EMNLP-CoNLL, ACL, Prague, Czech Republic, pages 967-975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>127--133</pages>
<contexts>
<context position="8296" citStr="Koehn et al., 2003" startWordPosition="1247" endWordPosition="1250">for this method. The methods of bilingual lexicon extraction from comparable texts (Rapp, 1995; Fung, 1998; Otero, 2007) also deal with the problem of noise reduction. However, the precision/recall ratio of a lexicon extracted from comparable corpus is generally lower. For the purpose of building a humanoriented dictionary, the parallel texts may provide the larger coverage and better quality of the translation equivalents. The noise reduction task is addressed by some of the SMT phrase-table pruning techniques. The most straightforward approach is thresholding on the translation probability (Koehn et al., 2003). Moore (2004) proposes the log-likelihood ratio and Fisher’s exact test to re-estimate word association strength. Johnson et al. (2007) applies Fisher’s exact test to dramatically reduce the number of phrase pairs in the phrase-table. They get rid of phrases that appear as alignment artifacts or are unlikely to occur again. The implementation of their algorithm requires a special index of all parallel corpus in order to enable a quick look-up for a given phrase pair. Eck et al. (2007) assesses the phrase pairs based on the actual usage statistics when translating a large amount of text. Entro</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 127–133.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ond˘rej Bojar, Alexandra Constantin, and Evan Herbst</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic</location>
<contexts>
<context position="10194" citStr="Koehn et al., 2007" startWordPosition="1558" endWordPosition="1561">e to different types of noise in the parallel corpus. We expect that the combination of these features with the phrase-level features based on co-occurrence statistics can improve the accuracy of the classification and the overall quality of a bilingual lexicon. 3.1 Context feature extraction algorithm The procedure of getting the context features is outlined in Algorithm 1. Unlike Johnson et al. (2007) we do not rely on any pre-constructed index of the parallel sentences, because it requires a lot of RAM on large corpora. Instead we rerun the phrase extraction algorithm of the Moses toolkit (Koehn et al., 2007) and update the context features at the moment when a phrase pair t is found. Algorithm 1 Calculate context features for all lexicon entries Require: Parallel corpus — C; {word-aligned sentences} Require: Bilingual lexicon — D; {this is a phrase-table, derived from C and modified as descibed in 4.1} Ensure: V = {¯v(d): d ∈ D}; {resulting features} for all d ∈ D do ¯v(d) ← 0; n(d) ← 0; for all s ∈ C do T ← PhraseExtraction(s);{Moses function} for all t ∈ T do if t ∈ D then ¯v(t) ← ¯v(t) + SentFeats(s); {Alg. 2} n(t) ← n(t) + 1; for all d ∈ D do ¯v(d) ← ¯v(d)/(1 + n(d)); {average, +1 smoothing} </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond˘rej Bojar, Alexandra Constantin, and Evan Herbst 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180, Prague, Czech Republic</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Kumano</author>
<author>Hideki Hirakawa</author>
</authors>
<title>Building An MT Dictionary From Parallel Texts Based On Linguistic And Statistical Information. COLING</title>
<date>1994</date>
<pages>76--81</pages>
<marker>Kumano, Hirakawa, 1994</marker>
<rawString>Akira Kumano and Hideki Hirakawa. 1994. Building An MT Dictionary From Parallel Texts Based On Linguistic And Statistical Information. COLING 1994: 76-81</rawString>
</citation>
<citation valid="false">
<authors>
<author>Wang Ling</author>
</authors>
<title>Jo˜ao Grac¸a, Isabel Trancoso and Alan Black 2012. Entropy-based Pruning for Phrasebased Machine Translation.</title>
<booktitle>In Proceedings of EMNLP-CoNLL, Association for Computational Linguistics, Jeju Island, Korea,</booktitle>
<pages>972--983</pages>
<marker>Ling, </marker>
<rawString>Wang Ling, Jo˜ao Grac¸a, Isabel Trancoso and Alan Black 2012. Entropy-based Pruning for Phrasebased Machine Translation. In Proceedings of EMNLP-CoNLL, Association for Computational Linguistics, Jeju Island, Korea, pp. 972-983</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J A McEwan</author>
<author>I Ounis</author>
<author>I Ruthven</author>
</authors>
<title>Building bilingual dictionaries from parallel web documents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 24th BCS-IRSG European Colloquium on IR Research,</booktitle>
<pages>303--323</pages>
<publisher>Springer-Verlag.</publisher>
<marker>McEwan, Ounis, Ruthven, 2002</marker>
<rawString>C. J. A. McEwan, I. Ounis, and I. Ruthven. 2002. Building bilingual dictionaries from parallel web documents. In Proceedings of the 24th BCS-IRSG European Colloquium on IR Research, pp. 303-323. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic construction of clean broad-coverage translation lexicons.</title>
<date>1996</date>
<booktitle>In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>125--134</pages>
<location>Montreal, Canada</location>
<contexts>
<context position="7115" citStr="Melamed (1996)" startWordPosition="1072" endWordPosition="1073">phrase frequencies and translation probabilities. We compare the accuracy of different classifiers and evaluate the importance of different features. The rest of the paper is organized as follows. In Section 2 we outline the related work. Section 3 describes our approach to the noise reduction in a bilingual lexicon and discusses the proposed features. We describe our experiments on training classifiers in Section 4. Section 5 concludes the paper. 2 Previous work The methods of extracting a bilingual lexicon from parallel texts as a part of the alignment process are discussed in Brown (1993), Melamed (1996), Tufis¸ and Barbu (2001). Melamed (1996) proposes a method of noise reduction that allows 59 to re-estimate and filter out indirect word associations. However, he works with a carefully prepared Hansards parallel corpus and the noise comes only from the imperfections of statistical modeling. Sahlgren (2004) proposes a co-occurrencebased approach, representing words as highdimensional random index vectors. The vectors of translation equivalents are expected to have high correlation. Yet, he notes that low-frequency words do not produce reliable statistics for this method. The methods of biling</context>
</contexts>
<marker>Melamed, 1996</marker>
<rawString>I. Dan Melamed. 1996. Automatic construction of clean broad-coverage translation lexicons. In Proceedings of the 2nd Conference of the Association for Machine Translation in the Americas, pages 125–134, Montreal, Canada</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of Translational Equivalence among Words.</title>
<date>2000</date>
<journal>Computational Linguistics</journal>
<volume>26</volume>
<issue>2</issue>
<pages>221--249</pages>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of Translational Equivalence among Words. Computational Linguistics 26(2), 221-249, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>On Log-Likelihood-Ratios and the Significance of Rare Events.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="8310" citStr="Moore (2004)" startWordPosition="1251" endWordPosition="1252">methods of bilingual lexicon extraction from comparable texts (Rapp, 1995; Fung, 1998; Otero, 2007) also deal with the problem of noise reduction. However, the precision/recall ratio of a lexicon extracted from comparable corpus is generally lower. For the purpose of building a humanoriented dictionary, the parallel texts may provide the larger coverage and better quality of the translation equivalents. The noise reduction task is addressed by some of the SMT phrase-table pruning techniques. The most straightforward approach is thresholding on the translation probability (Koehn et al., 2003). Moore (2004) proposes the log-likelihood ratio and Fisher’s exact test to re-estimate word association strength. Johnson et al. (2007) applies Fisher’s exact test to dramatically reduce the number of phrase pairs in the phrase-table. They get rid of phrases that appear as alignment artifacts or are unlikely to occur again. The implementation of their algorithm requires a special index of all parallel corpus in order to enable a quick look-up for a given phrase pair. Eck et al. (2007) assesses the phrase pairs based on the actual usage statistics when translating a large amount of text. Entropybased criter</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. On Log-Likelihood-Ratios and the Significance of Rare Events. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>440--447</pages>
<location>Hongkong,</location>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. Proceedings of the 38th Annual Meeting of the ACL, pp. 440-447, Hongkong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<pages>417--449</pages>
<contexts>
<context position="11658" citStr="Och and Ney, 2004" startWordPosition="1815" endWordPosition="1818">atistics is insufficient in case of a noisy corpus. The sentence-level features are designed to partly restore the information which is lost during the phrase extraction process. We try to estimate the general characteristics of the whole set of parallel sentences where a given phrase pair occurred. The proposed sentence-level features rely on the different sources of information, which are discussed in 3.2.1, 3.2.2 and 3.2.3. Table 2 provides illustrating examples of noisy phrase pairs and sample sentences. 3.2.1 Word-alignment annotation We use the intersection of direct and reverse Giza++ (Och and Ney, 2004) alignments as a heuristic rule to find words reliably aligned to each other. The alignment information gives rise to several sentence-level features: • UnsafeAlign - percentage of words that are not symmetrically aligned to each other. • UnsafeJump - average distance between the translations of subsequent input words. • UnsafeDigAlign percentage of unequal digits among the symmetrically aligned words. The UnsafeAlign and UnsafeJump values can vary in different sentences. However, their being too large on the whole set of sentences where a given phrase pair occurred possibly indicates some sys</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, vol. 30 (2004), pp. 417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gamallo Otero</author>
</authors>
<title>Learning bilingual lexicons from comparable English and Spanish corpora.</title>
<date>2007</date>
<booktitle>Proceedings of MT Summit XI,</booktitle>
<pages>191--198</pages>
<contexts>
<context position="7797" citStr="Otero, 2007" startWordPosition="1173" endWordPosition="1174">eduction that allows 59 to re-estimate and filter out indirect word associations. However, he works with a carefully prepared Hansards parallel corpus and the noise comes only from the imperfections of statistical modeling. Sahlgren (2004) proposes a co-occurrencebased approach, representing words as highdimensional random index vectors. The vectors of translation equivalents are expected to have high correlation. Yet, he notes that low-frequency words do not produce reliable statistics for this method. The methods of bilingual lexicon extraction from comparable texts (Rapp, 1995; Fung, 1998; Otero, 2007) also deal with the problem of noise reduction. However, the precision/recall ratio of a lexicon extracted from comparable corpus is generally lower. For the purpose of building a humanoriented dictionary, the parallel texts may provide the larger coverage and better quality of the translation equivalents. The noise reduction task is addressed by some of the SMT phrase-table pruning techniques. The most straightforward approach is thresholding on the translation probability (Koehn et al., 2003). Moore (2004) proposes the log-likelihood ratio and Fisher’s exact test to re-estimate word associat</context>
</contexts>
<marker>Otero, 2007</marker>
<rawString>Pablo Gamallo Otero. 2007. Learning bilingual lexicons from comparable English and Spanish corpora. Proceedings of MT Summit XI, pages 191–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the ACL</booktitle>
<volume>33</volume>
<pages>320--322</pages>
<contexts>
<context position="7771" citStr="Rapp, 1995" startWordPosition="1169" endWordPosition="1170">oses a method of noise reduction that allows 59 to re-estimate and filter out indirect word associations. However, he works with a carefully prepared Hansards parallel corpus and the noise comes only from the imperfections of statistical modeling. Sahlgren (2004) proposes a co-occurrencebased approach, representing words as highdimensional random index vectors. The vectors of translation equivalents are expected to have high correlation. Yet, he notes that low-frequency words do not produce reliable statistics for this method. The methods of bilingual lexicon extraction from comparable texts (Rapp, 1995; Fung, 1998; Otero, 2007) also deal with the problem of noise reduction. However, the precision/recall ratio of a lexicon extracted from comparable corpus is generally lower. For the purpose of building a humanoriented dictionary, the parallel texts may provide the larger coverage and better quality of the translation equivalents. The noise reduction task is addressed by some of the SMT phrase-table pruning techniques. The most straightforward approach is thresholding on the translation probability (Koehn et al., 2003). Moore (2004) proposes the log-likelihood ratio and Fisher’s exact test to</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the ACL 33, 320-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Noah A Smith</author>
</authors>
<title>The web as a parallel corpus..</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>349--380</pages>
<marker>Resnik, Smith, 2003</marker>
<rawString>Resnik, Philip and Noah A. Smith. 2003. The web as a parallel corpus.. Computational Linguistics, 29, pp.349–380</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>Automatic Bilingual Lexicon Acquisition Using Random Indexing.</title>
<date>2004</date>
<journal>Journal of Natural Language Engineering, Special Issue on Parallel Texts,</journal>
<volume>11</volume>
<contexts>
<context position="7424" citStr="Sahlgren (2004)" startWordPosition="1120" endWordPosition="1121">l lexicon and discusses the proposed features. We describe our experiments on training classifiers in Section 4. Section 5 concludes the paper. 2 Previous work The methods of extracting a bilingual lexicon from parallel texts as a part of the alignment process are discussed in Brown (1993), Melamed (1996), Tufis¸ and Barbu (2001). Melamed (1996) proposes a method of noise reduction that allows 59 to re-estimate and filter out indirect word associations. However, he works with a carefully prepared Hansards parallel corpus and the noise comes only from the imperfections of statistical modeling. Sahlgren (2004) proposes a co-occurrencebased approach, representing words as highdimensional random index vectors. The vectors of translation equivalents are expected to have high correlation. Yet, he notes that low-frequency words do not produce reliable statistics for this method. The methods of bilingual lexicon extraction from comparable texts (Rapp, 1995; Fung, 1998; Otero, 2007) also deal with the problem of noise reduction. However, the precision/recall ratio of a lexicon extracted from comparable corpus is generally lower. For the purpose of building a humanoriented dictionary, the parallel texts ma</context>
</contexts>
<marker>Sahlgren, 2004</marker>
<rawString>Magnus Sahlgren. 2004. Automatic Bilingual Lexicon Acquisition Using Random Indexing. Journal of Natural Language Engineering, Special Issue on Parallel Texts, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Harnessing the lawless: using comparable corpora to find translation equivalents.</title>
<date>2004</date>
<journal>Journal of Applied Linguistics</journal>
<volume>1</volume>
<issue>3</issue>
<pages>333--350</pages>
<contexts>
<context position="9162" citStr="Sharoff, 2004" startWordPosition="1388" endWordPosition="1389">phrases that appear as alignment artifacts or are unlikely to occur again. The implementation of their algorithm requires a special index of all parallel corpus in order to enable a quick look-up for a given phrase pair. Eck et al. (2007) assesses the phrase pairs based on the actual usage statistics when translating a large amount of text. Entropybased criteria are proposed in Ling et al. (2012), Zens et al. (2012). Automatically acquired bilingual lexicons are capable to reflect many word meanings and translation patterns, which are often not obvious even to the professional lexicographers (Sharoff, 2004). Their content can also be updated regularly to incorporate more parallel texts and capture the translations of new words and expressions. Thus, the methods allowing to improve the quality of automatic bilingual lexicons are of practical importance. 3 Noise detection features We treat the noise recognition task as a binary classification problem. A set of nonlexical context features is designed to be sensitive to different types of noise in the parallel corpus. We expect that the combination of these features with the phrase-level features based on co-occurrence statistics can improve the acc</context>
</contexts>
<marker>Sharoff, 2004</marker>
<rawString>Serge Sharoff. 2004. Harnessing the lawless: using comparable corpora to find translation equivalents. Journal of Applied Linguistics 1(3), 333-350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Smith</author>
<author>Herve Saint-Amand</author>
<author>Magdalena Plamada</author>
<author>Philipp Koehn</author>
<author>Chris Callison-Burch</author>
<author>Adam Lopez</author>
</authors>
<title>Dirt Cheap Web-Scale Parallel Text from the Common Crawl. To appear in</title>
<date>2013</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>Smith, Saint-Amand, Plamada, Koehn, Callison-Burch, Lopez, 2013</marker>
<rawString>Jason Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch and Adam Lopez. 2013. Dirt Cheap Web-Scale Parallel Text from the Common Crawl. To appear in Proceedings of ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis¸</author>
<author>Ana-Maria Barbu</author>
</authors>
<title>Computational Bilingual Lexicography: Automatic Extraction of Translation Dictionaries.</title>
<date>2001</date>
<booktitle>In International Journal on Science and Technology of Information, Romanian Academy, ISSN</booktitle>
<pages>1453--8245</pages>
<marker>Tufis¸, Barbu, 2001</marker>
<rawString>Dan Tufis¸ and Ana-Maria Barbu. 2001. Computational Bilingual Lexicography: Automatic Extraction of Translation Dictionaries. In International Journal on Science and Technology of Information, Romanian Academy, ISSN 1453-8245, 4/3-4, pp.325-352</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumithra Velupillai</author>
<author>Martin Hassel</author>
<author>Hercules Dalianis</author>
</authors>
<title>Automatic Dictionary Construction and Identification of Parallel Text Pairs.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Symposium on Using Corpora in Contrastive and Translation Studies (UCCTS).</booktitle>
<marker>Velupillai, Hassel, Dalianis, 2008</marker>
<rawString>Velupillai, Sumithra, Martin Hassel, and Hercules Dalianis. 2008. ”Automatic Dictionary Construction and Identification of Parallel Text Pairs. In Proceedings of the International Symposium on Using Corpora in Contrastive and Translation Studies (UCCTS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Daisy Stanton</author>
<author>Peng Xu</author>
</authors>
<title>A Systematic Comparison of Phrase Table Pruning Techniques.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL, ACL, Jeju Island, Korea,</booktitle>
<pages>972--983</pages>
<contexts>
<context position="8967" citStr="Zens et al. (2012)" startWordPosition="1359" endWordPosition="1362">Fisher’s exact test to re-estimate word association strength. Johnson et al. (2007) applies Fisher’s exact test to dramatically reduce the number of phrase pairs in the phrase-table. They get rid of phrases that appear as alignment artifacts or are unlikely to occur again. The implementation of their algorithm requires a special index of all parallel corpus in order to enable a quick look-up for a given phrase pair. Eck et al. (2007) assesses the phrase pairs based on the actual usage statistics when translating a large amount of text. Entropybased criteria are proposed in Ling et al. (2012), Zens et al. (2012). Automatically acquired bilingual lexicons are capable to reflect many word meanings and translation patterns, which are often not obvious even to the professional lexicographers (Sharoff, 2004). Their content can also be updated regularly to incorporate more parallel texts and capture the translations of new words and expressions. Thus, the methods allowing to improve the quality of automatic bilingual lexicons are of practical importance. 3 Noise detection features We treat the noise recognition task as a binary classification problem. A set of nonlexical context features is designed to be </context>
</contexts>
<marker>Zens, Stanton, Xu, 2012</marker>
<rawString>Richard Zens, Daisy Stanton and Peng Xu. 2012. A Systematic Comparison of Phrase Table Pruning Techniques. In Proceedings of EMNLP-CoNLL, ACL, Jeju Island, Korea, pp. 972-983.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
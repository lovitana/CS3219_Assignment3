<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.958999">
Grounding Language with Points and Paths in Continuous Spaces
</title>
<author confidence="0.996232">
Jacob Andreas and Dan Klein
</author>
<affiliation confidence="0.9987275">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.993846">
{jda,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.997343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909052631579">
We present a model for generating path-
valued interpretations of natural language
text. Our model encodes a map from
natural language descriptions to paths,
mediated by segmentation variables which
break the language into a discrete set of
events, and alignment variables which
reorder those events. Within an event,
lexical weights capture the contribution of
each word to the aligned path segment.
We demonstrate the applicability of our
model on three diverse tasks: a new color
description task, a new financial news task
and an established direction-following
task. On all three, the model outperforms
strong baselines, and on a hard variant of
the direction-following task it achieves
results close to the state-of-the-art system
described in Vogel and Jurafsky (2010).
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908555555555">
This paper introduces a probabilistic model for
predicting grounded, real-valued trajectories from
natural language text. A long tradition of re-
search in compositional semantics has focused on
discrete representations of meaning. The origi-
nal focus of such work was on logical translation:
mapping statements of natural language to a for-
mal language like first-order logic (Zettlemoyer
and Collins, 2005) or database queries (Zelle and
Mooney, 1996). Subsequent work has integrated
this logical translation with interpretation against
a symbolic database (Liang et al., 2013).
There has been a recent increase in interest
in perceptual grounding, where lexical semantics
anchor in perceptual variables (points, distances,
etc.) derived from images or video. Bruni et al.
(2014) describe a procedure for constructing word
representations using text- and image-based dis-
</bodyText>
<figure confidence="0.985821625">
% Change 1.02
1.01
1.00
0.99
0.98
10 12 14 10 12 14
Hour of day
U.S. stocks rebound after bruising two-day swoon
</figure>
<figureCaption confidence="0.8400945">
Figure 1: Example stock data. The chart displays
index value over a two-day period (divided by the
dotted line), while the accompanying headline de-
scribes the observed behavior.
</figureCaption>
<bodyText confidence="0.998742136363636">
tributional information. Yu and Siskind (2013)
describe a model for identifying scenes given de-
scriptions, and Golland et al. (2010), Kollar et al.
(2010), and Krishnamurthy and Kollar (2013) de-
scribe models for identifying individual compo-
nents of scenes described by text. These all have
the form of matching problems between text and
observed groundings—what has been missing so
far is the ability to generate grounded interpreta-
tions from scratch, given only text.
Our work continues in the tradition of this per-
ceptual grounding work, but makes two contribu-
tions. First, our approach is able to predict simple
world states (and their evolution): for a general
class of continuous domains, we produce a repre-
sentation of p(world  |text) that admits easy sam-
pling and maximization. This makes it possible to
produce grounded interpretations of text without
reference to a pre-existing scene. Simultaneously,
we extend the range of temporal phenomena that
can be modeled—unlike the aforementioned spa-
tial semantics work, we consider language that de-
</bodyText>
<page confidence="0.98426">
58
</page>
<note confidence="0.6874795">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58–67,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999859945945946">
scribes time-evolving trajectories, and unlike Yu
and Siskind (2013), we allow these trajectories to
have event substructure, and model temporal or-
dering. Our class of models generalizes to a vari-
ety of different domains: a new color-picking task,
a new financial news task, and a more challenging
variant of the direction-following task established
by Vogel and Jurafsky (2010).
As an example of the kinds of phenomena we
want to model, consider Figure 1, which shows
the value of the Dow Jones Industrial Average
over June 3rd and 4th 2008, along with a finan-
cial news headline from June 4th. There are sev-
eral effects of interest here. One phenomenon we
want to capture is that the lexical semantics of in-
dividual words must be combined: swoon roughly
describes a drop while bruising indicates that the
drop was severe. We isolate this lexical combi-
nation in Section 4, where we consider a limited
model of color descriptions (Figure 2). A second
phenomenon is that the description is composed
of two separate events, a swoon and a rebound;
moreover, those events do not occur in their tex-
tual order, as revealed by after. In Section 5, we
extend the model to include segmentation and or-
dering variables and apply it to this stock data.
The situation where language describes a
path through some continuous space—literal or
metaphorical—is more general than stock head-
lines. Our claim is that a variety of problems
in language share these same characteristics. To
demonstrate generality of the model, we also ap-
ply it in Section 6 to a challenging variant of the
direction-following task described by Vogel and
Jurafsky (2010) (Figure 3), where we achieve re-
sults close to a state-of-the-art system that makes
stronger assumptions about the task.
</bodyText>
<sectionHeader confidence="0.721972" genericHeader="method">
2 Three tasks in grounded semantics
</sectionHeader>
<bodyText confidence="0.999509833333333">
The problem of inferring a structured state repre-
sentation from sensory input is a hard one, but we
can begin to tackle grounded semantics by restrict-
ing ourselves to cases where we have sequences
of real-valued observations directly described by
text. In this paper we’ll consider the problems
of recognizing colors, describing time series, and
following navigational instructions. While these
tasks have been independently studied, we believe
that this is the first work which presents them in
a unified framework, and carries them out with a
single family of models.
</bodyText>
<figure confidence="0.9193995">
dark pastel blue
(a) (b)
</figure>
<figureCaption confidence="0.9975275">
Figure 2: Example color data: (a) a named color;
(b) its coordinates in color space.
</figureCaption>
<bodyText confidence="0.999042743589744">
Colors Figure 2 shows a color called dark pas-
tel blue. English speakers, even if unfamiliar with
the specific color, can identify roughly what the
name signifies because of prior knowledge of the
meanings of the individual words.
Because the color domain exhibits lexical com-
positionality but not event structure, we present it
here to isolate the non-temporal compositional ef-
fects in our model. Any color visible to the human
eye can be identified with three coordinates, which
we’ll take to be hue, saturation and value (HSV).
As can be seen in Figure 2 the “hue” axis corre-
sponds to the differentiation made by basic color
names in most languages. Other modifiers act on
the saturation and value axes: either simple ones
like dark (which decreases value), or more compli-
cated ones like pastel (which increases value and
decreases saturation). Given a set of named colors
and their HSV coordinates, a learning algorithm
should be able to identify the effects of each word
in the vocabulary and predict the appearance of
new colors with previously-unseen combinations
of modifiers.
Compositional interpretations of color have re-
ceived attention in linguistics and philosophy of
language (Kennedy and McNally, 2010), but while
work in grounded computational semantics like
that of Krishnamurthy and Kollar (2013) has suc-
ceeded in learning simple color predicates, our
model is the first to capture the machine learning
of color in a fine-grained, compositional way.
Time series As a first step into temporal struc-
ture, we’ll consider language describing the be-
havior of stock market indices. Here, again, there
is a simple parameterization—in this case just a
single number describing the total value of the
index—but as shown by the headline example in
Figure 1, the language used to describe changes
in the stock market can be quite complex. Head-
</bodyText>
<page confidence="0.997827">
59
</page>
<figureCaption confidence="0.6761718">
right round the white water [... I but stay quite close ’cause
you don’t otherwise you’re going to be in that stone creek
Figure 3: Example map data: a portion of a map,
and a single line from a dialog which describes
navigation relative to the two visible landmarks.
</figureCaption>
<bodyText confidence="0.999316116666667">
lines may describe multiple events, or multi-part
events like rebound or extend; stocks do not sim-
ply rise or fall, but stagger, stumble, swoon, and
so on. There are compositional effects here as
well: distinction is made between falling and
falling sharply; gradual trends are distinguished
from those which occur suddenly, at the beginning
or end of the trading day. Along with temporal
structure, the problem requires a more sophisti-
cated treatment of syntax than the colors case—
now we have to identify which subspans of the
sentence are associated with each event observed,
and determine the correspondence between sur-
face order and actual order in time.
The learning of correspondences between text
and time series has attracted more interest in nat-
ural language generation than in semantics (Yu et
al., 2007). Research on natural language process-
ing and stock data, meanwhile, has largely focused
on prediction of future events (Kogan et al., 2009).
Direction following We’ll conclude by apply-
ing our model to the well-studied problem of
following navigational directions. A variety of
reinforcement-learning approaches for following
directions on a map were previously investigated
by Vogel and Jurafsky (2010) using a corpus as-
sembled by Anderson et al. (1991). An example
portion of a path and its accompanying instruction
is shown in Figure 3. While also representable as
a set of real valued coordinates, here 2-d, this data
set looks very different—a typical example con-
sists of more than a hundred sentences of the kind
shown in Figure 3, accompanying a long path. The
language, a transcript of a spoken dialog, is also
considerably less formal than the language found
in the Wall Street Journal examples, involving dis-
fluency, redundancy and occasionally errors. Nev-
ertheless the underlying structure of this problem
and the stock problem are fundamentally similar.
In addition to Vogel and Jurafsky, Tellex et al.
(2011) give a weakly-supervised model for map-
ping single sentences to commands, and Brana-
van et al. (2009) give an alternative reinforcement-
learning approach for following long command se-
quences. An intermediate between this approach
and ours is the work of Chen and Mooney (2011)
and Artzi and Zettlemoyer (2013), which boot-
strap a semantic parser to generate logical forms
specifying the output path, rather than predicting
the path directly.
Between them, these tasks span a wide range of
linguistic phenomena relevant to grounded seman-
tics, and provide a demonstration of the useful-
ness and general applicability of our model. While
development of the perceptual groundwork neces-
sary to generalize these results to more complex
state spaces remains a major problem, our three
examples provide a starting point for studying the
relationship between perception, time and the se-
mantics of natural language.
</bodyText>
<sectionHeader confidence="0.997285" genericHeader="method">
3 Preliminaries
</sectionHeader>
<bodyText confidence="0.999757095238095">
In the experiments that follow, each training ex-
ample will consist of:
– Natural language text, consisting of a con-
stituency parse tree or trees. For a given ex-
ample, we will denote the associated trees
(Ti, T2, ...). These are also observed at test
time, and used to predict new groundings.
– A vector-valued, grounded observation, or
a sequence of observations (a path), which
we will denote V for a given example. We
will further assume that each of these paths
has been pre-segmented (discussed in detail
in Section 5) into a sequence (V1, V2,...).
These are only observed during training.
The probabilistic backbone of our model is a
collection of linear and log-linear predictors. Thus
it will be useful to work with vector-valued rep-
resentations of both the language and the path,
which we accomplish with a pair of feature func-
tions ϕt and ϕ,,. As the model is defined only
in terms of these linear representations, we can
</bodyText>
<page confidence="0.98184">
60
</page>
<listItem confidence="0.7703124">
■ Label at root of T
■ Lemmatized leaves of T
■ Last element of V
■ Curvature of quadratic
approx.to V (stocks only)
</listItem>
<bodyText confidence="0.214645">
Cartesian prod. of Ot(T) with:
</bodyText>
<equation confidence="0.579963333333333">
ϕa(T, Ai, Ai−1) ■ H[Ai is aligned]
■ H[Ai−1 is aligned]
■ A1 − Ai−1 (if both aligned)
</equation>
<tableCaption confidence="0.9395565">
Table 1: Features used for linear parameterization
of the grounding model.
</tableCaption>
<bodyText confidence="0.999953931034482">
simplify notation by writing Ti = ϕt(7) and
Vi = ϕv(Vi). As the ultimate prediction task is to
produce paths, and not their featurized representa-
tions, we will assume that it is also straightforward
to compute ϕ−1
v , which projects path features back
into the original grounding domain.
All parse trees are predicted from input text us-
ing the Berkeley Parser (Petrov and Klein, 2007).
Feature representations for both trees and paths are
simple and largely domain-independent; they are
explicitly enumerated in Table 1.
The general framework presented here leaves
one significant problem unaddressed: given a large
state vector encoding properties of multiple ob-
jects, how do we resolve an utterance about a sin-
gle object to the correct subset of indices in the
vector? While none of the tasks considered in this
paper require an argument resolution step of this
kind, interpretation of noun phrases is one of the
better-studied problems in compositional seman-
tics (Zelle and Mooney (1996), inter alia), and
we expect generalization of this approach to be
straightforward using these tools.
We will consider the color, stock, and naviga-
tion tasks in turn. It is possible to view the models
we give for all three as instantiations of the same
graphical model, but for ease of presentation we
will introduce this model incrementally.
</bodyText>
<sectionHeader confidence="0.958556" genericHeader="method">
4 Predicting vectors
</sectionHeader>
<bodyText confidence="0.9999365">
Prediction of a color variable from text has the
form of a regression problem: given a vector of
lexical features extracted from the name, we wish
to predict the entries of a vector in color space. It
seems linguistically plausible that this regression
is sparse and linear: that most words, if they pro-
vide any constraints at all, tend to express prefer-
ences about a subset of the available dimensions;
and that composition within the domain of a sin-
gle event largely consists of words additively pre-
dicting that event’s parameters, without complex
nonlinear interactions. This is motivated by the
observation that pragmatic concerns force linguis-
tic descriptors to orient themselves along a small
set of perceptual bases: once we have words for
north and east, we tend to describe intermediates
as northeast rather than inventing an additional
word which means “a little of both”.
As discussed above, we can represent a color as
a point in a three-dimensional HSV space. Let T
denote features on the parse tree of the color name,
and V its representation in color space (consistent
with the definition of ϕv given in Table 1). Linear-
ity suggests the following model:
</bodyText>
<equation confidence="0.933160285714286">
p(T,V ) oc e−∥θ⊤t T−V ∥2 (1)
The learning problem is then:
∑ � �
� 2
argmin �θ⊤ t T − V � (2)
θt 2
T,V
</equation>
<bodyText confidence="0.999609">
which, with a sparse prior on θt, is the proba-
bilistic formulation of Lasso regression (Tibshi-
rani, 1996), for which standard tools are available
in the optimization literature.
To predict color space values from a new (fea-
turized) name T, we output:
</bodyText>
<equation confidence="0.9168995">
argmax
V p(T,V ) = θ⊤t T
</equation>
<subsectionHeader confidence="0.975661">
4.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999988058823529">
We collect a set of color names and their
corresponding HSV triples from the English
Wikipedia’s List of Colors, retaining only those
color names in which every word appears at least
three times in the training corpus. This leaves a
set of 419 colors, which we randomly divide into
a 377-item training set and 42-item test set. The
model’s goal will be to learn to identify new col-
ors given only their names.
We consider two evaluations: one which mea-
sures the model’s ability to distinguish the named
color from a random alternative—analogous to the
evaluation in Yu and Siskind (2013)—and one
which measures the absolute difference between
predicted and true color values. In particular, in
the first evaluation the model is presented with
the name of a color and a pair of candidates, one
</bodyText>
<equation confidence="0.4642935">
ϕt(T)
ϕv(V )
</equation>
<page confidence="0.980472">
61
</page>
<table confidence="0.995954833333333">
Method Sel. T HI S1 V1
5 Predicting paths
0.50 0.30 0.38 0.39
0.78 0.05 0.26 0.17
0.81 0.07 0.21 0.13
0.86 - - -
</table>
<tableCaption confidence="0.92314">
Table 2: Results for the color selection task.
</tableCaption>
<bodyText confidence="0.995997652173913">
Sel(ection accuracy) is frequency with which the
system was able to correctly identify the color de-
scribed when paired with a random alternative.
Other columns are the magnitude of the average
prediction error along the axes of the color space.
Full model selection accuracy is a statistically sig-
nificant (p &lt; 0.05) improvement over the baseline
using a paired sign test.
the color corresponding to the name and another
drawn randomly from the test set, and report the
fraction of times the true color is assigned a higher
probability than the random alternative. In the sec-
ond, we report the absolute value of the difference
between true and predicted hue, saturation, and lu-
minosity.
We compare against two baselines: one which
looks only at the last word in the color name (al-
most always a hue category), and so captures no
compositional effects, and another which outputs
random values for all three coordinates. Results
are shown in Table 2. The model with all lexical
features outperforms both baselines on selection
and all but one absolute error metric.
</bodyText>
<subsectionHeader confidence="0.985675">
4.2 Error analysis
</subsectionHeader>
<bodyText confidence="0.999981220588236">
An informal experiment in which the color selec-
tion task was repeated on one of the authors’ col-
leagues (the “Human” row in Table 2) yielded an
accuracy of 86%, only 5% better than the system.
While not intended as a rigorous upper bound on
performance, this suggests that the model capac-
ity and training data are sufficient to capture most
interesting color behavior. The errors that do oc-
cur appear to mostly be of two kinds. In one case,
a base color is seen only with a small (or related)
set of modifiers, from which the system is unable
to infer the meaning of the base color (e.g. from
Japanese indigo, lavender indigo, and electric in-
digo, the learning algorithm infers that indigo is
bright purple). In the other, no part of the color
word is seen in training, and the system outputs an
unrelated “default” color (teal is predicted to be
bright red).
The idea that a sentence’s meaning is fundamen-
tally described by a set of events, each associated
with a set of predicates, is well-developed in neo-
Davidsonian formal semantics (Parsons, 1990).
We adopt the skeleton of this formal approach by
tying our model to (latent) partitions of the in-
put sentence into disjoint events. Rather than at-
tempting to pass through a symbolic meaning rep-
resentation, however, this event structure will be
used to map text directly into the grounding do-
main. We assume that this domain has pre-existing
structure—in particular, that in our input paths V,
the boundaries of events have already been iden-
tified, and that the problem of aligning text to
portions of the segment only requires aligning to
segment indices rather than fine-grained time in-
dices. This is a strong assumption, and one that
future work may wish to revisit, but there exist
both computational tools from the changepoint de-
tection literature (Basseville and Nikiforov, 1995)
and pieces of evidence from cognitive science (Za-
cks and Swallow, 2007) which suggest that assum-
ing a pre-linguistic structuring of events is a rea-
sonable starting point.
In the text domain, we make the corresponding
assumption that each of these events is syntacti-
cally local—that a given span of the input sentence
provides information about at most one of these
segmented events.
The main structural difference between the
color example in Figure 2 and the stock market ex-
ample in Figure 1 is the introduction of a time di-
mension orthogonal to the dimensions of the state
space. To accommodate this change, we extend
the model described in the previous subsection in
the following way: Instead of a single vector, each
tree representation T is paired with a sequence of
path features V = (V1, V2,. .. , VM). For the time
being we continue to assume that there is only
one input tree per training example. As before,
we wish to model the probability p(T, V), but the
problem becomes harder: a single sentence might
describe multiple events, but we don’t know what
the correspondence is between regions of the sen-
tence and segments V.
Though the ultimate goal is still prediction of V
vectors from novel T instances, we cannot do this
without also inferring a set of latent alignments be-
tween portions of the path and input sentence dur-
ing training. To allow a sentence to explain mul-
</bodyText>
<figure confidence="0.9293885">
Random
Last word
Full model
Human
62
[Stocks rose] [Stocks rose, then fell]
</figure>
<figureCaption confidence="0.7540998">
Figure 4: Factor graph for stocks grounding
model. Only a subset of the alignment candidates
are shown. Otc maps text to constraints, Oacv maps
constraints to grounded segments, and Ota deter-
mines which constraints act on which segments.
tiple events, we’ll break each T apart into a set of
alignment candidates Ti. We’ll allow as an align-
ment candidate any subtree of T, and additionally
any subtree from which a single constituent has
been deleted.
</figureCaption>
<bodyText confidence="0.999802256410256">
We then introduce two groups of latent vari-
ables: alignment variables A = (A1, A2, ...),
which together describe a mapping from pieces
of the input sentence to segments of the ob-
served path, and what we’ll call “constraint” vari-
ables C = (C1, C2, ...), which express each
aligned tree segment’s prediction about what its
corresponding path should look like (so that the
possibly-numerous parts of the tree aligned to a
single segment can independently express prefer-
ences about the segment’s path features).
In addition to ensuring that the alignment is
consistent with the bracketing of the tree, it might
be desirable to impose additional global con-
straints on the alignment. There are various ways
to do this in a graphical modeling framework; the
most straightforward is to add a combinatorial fac-
tor touching all alignment variables which checks
for satisfaction of the global constraint. In gen-
eral this makes alignment intractable. If the total
number of alignments licensed by this combina-
torial factor is small (i.e. if acceptable alignments
are sparse within the exponentially-large set of all
possible assignments to A), it is possible to di-
rectly sum them out during inference. Otherwise
approximate techniques (as discussed in the fol-
lowing section) will be necessary.
As discussed in Section 2, our financial time-
lines cover two-day periods, and it seems natural
to treat each day as a separate event. Then
the simple regression model described in the
preceding section, extended to include alignment
and constraint variables, has the form of the factor
graph shown in Figure 4. In particular, the joint
distribution p(T, V) is the product of four groups
of factors:
Alignment factors Ota, which use a log-linear
model to score neighboring pairs of factors with
a feature function ϕa:
</bodyText>
<equation confidence="0.9974">
Ota(Ti, Ai, Ai−1) =
(3)
EA′i,A′i e
−1
θ⊤a ϕa(Ti,A′i,Ai−1)
</equation>
<bodyText confidence="0.8379115">
Constraint factors Otc, which map text features
onto constraint values:
</bodyText>
<equation confidence="0.861301">
2
Otc(Ti, Ci) = e−||θ⊤ t Ti−Ci||� (4)
</equation>
<bodyText confidence="0.914217">
Prediction factors Oacv which encourage pre-
dicted constraints and path features to agree:
</bodyText>
<equation confidence="0.929051666666667">
�
1 if Ai =� j
Oacv(Ai, Ci, Vj) = e−||Ci−Vj||2 o.w. (5)
</equation>
<bodyText confidence="0.999483857142857">
A global factor Oa∗(A1, A2, · · · ) which places
an arbitrary combinatorial constraint on the
alignment.
Note the essential similarity between Equations 1
and 4—in general, it can be shown that this factor
model reduces to the regression model we gave for
colors when there is only one of each Ti and Vj.
</bodyText>
<subsectionHeader confidence="0.992359">
5.1 Learning
</subsectionHeader>
<bodyText confidence="0.9929717">
In order to make learning in the stocks domain
tractable, we introduce the following global
constraints on alignment: every terminal must be
aligned, and two constituents cannot be aligned
to the same segment. Together, these simplify
learning by ensuring that the number of terms
in the sum over A and C is polynomial (in fact
O(n2)) in the length of the input sentence. We
wish to find the maximum a posteriori estimate
p(Bt, Ba|T, V) for Bt and Ba, which we can do
</bodyText>
<figure confidence="0.986372888888889">
V1 V2
acv
Al Cl A2 C2
a⇤
tc
ta
T1 T2
· · ·
eθ⊤ a ϕa(Ti,Ai,Ai−1)
</figure>
<page confidence="0.996467">
63
</page>
<bodyText confidence="0.931562">
using the Expectation–Maximization algorithm.
To find regression scoring weights Bt, we have:
</bodyText>
<equation confidence="0.958567071428572">
E step:
[∑ ] [∑ ]
M = E Ti(Ti)⊤ ; N = E TiV ⊤ (6)
Ai
i i
M step:
Bt = M−&apos;N (7)
To find alignment scoring weights Ba, we must
maximize:
∑ ⎡ ⎛ ⎞ ⎤
i eθ⊤a ϕa(Ai,Ai−1,T i)
E⎣log ⎝ ⎠
∑ ⎦ (8)
A′ i,A′ i−1 eθ⊤ a ϕa(A′ i,A′ i−1,Ti)
</equation>
<bodyText confidence="0.98695">
which can be done using a variety of convex op-
timization tools; we used L-BFGS (Liu and No-
cedal, 1989).
The predictive distribution p(V|T) can also be
straightforwardly computed using the standard in-
ference procedures for graphical models.
</bodyText>
<subsectionHeader confidence="0.994044">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99994276">
Our stocks dataset consists of a set of headlines
from the “Market Snapshot” column of the Wall
Street Journal’s MarketWatch website,1 paired
with hourly stock charts for each day described
in a headline. Data is collected over a roughly
decade-long period between 2001 and 2012; af-
ter removing weekends and days with incomplete
stock data, we have a total of 2218 headline/time
series pairs. As headlines most often discuss a
single day or a short multi-day period, each train-
ing example consists of two days’ worth of stock
data concatenated together. We use a 90%/10%
train/test split, with all test examples following all
training examples chronologically.
We compare against two baselines: one which
uses no text (and so learns only the overall mar-
ket trend during the training period), and another
which uses a fixed alignment instead of summing,
aligning the entire tree to the second day’s time se-
ries. Prediction error is the sum of squared errors
between the predicted and gold time series.
We report both the magnitude of the prediction
error, and the model’s ability to distinguish be-
tween the described path and a randomly-selected
alternative. The system scores poorly on squared
</bodyText>
<footnote confidence="0.972811">
1http://www.marketwatch.com/Search?m=
Column&amp;mp=Market%20Snapshot
</footnote>
<figure confidence="0.423851">
1.02
% Change 1.01
1.00
0.99
0.98
10 12 14 10 12 14
Hour of day
[U.S. stocks end lower]2 [as economic worries persist]1
</figure>
<figureCaption confidence="0.87282">
Figure 5: Example output from the stocks task.
</figureCaption>
<bodyText confidence="0.982210757575757">
The model prediction is given in blue (solid), and
the reference time series in green (dashed). Brack-
ets indicate the predicted boundaries of event-
introducing spans, and subscripts their order in the
sentence. The model correctly identifies that end
lower refers to the current day, and persist pro-
vides information about the previous day.
Method Sel. acc. T Pred. err. 1
No text 0.51
Fixed alignment
Full model
Human
Table 3: Results for the stocks task. Sel(ection
accuracy) measures the frequency with which the
system correctly identifies the stock described in
the headline when paired with a random alterna-
tive. Pred(iction error) is the mean sum of squared
errors between the real and predicted paths. Full
model selection accuracy is a statistically signif-
icant improvement (p &lt; 0.05) over the baseline
using a paired sign test.
error (which disproportionately penalizes large de-
viations from the correct answer, preferring con-
servative models), but outperforms both base-
lines on the task of choosing the described stock
history—when it is wrong, its errors are often
large in magnitude, but its predictions more fre-
quently resemble the correct time series than the
other systems.
Figure 5 shows example system output for an
example sentence. The model correctly identifies
the two events, orders them in time and gets their
approximate trend correct. Table 4 shows some
</bodyText>
<figure confidence="0.953469875">
0.59
0.61
0.72
0.0012
0.0011
0.0018
–
64
% Change 1.02
1.01
1.00
0.99
0.98
10 12 14 10 12 14
Hour of day
[U.S. stocks extend losing stretch],
</figure>
<figureCaption confidence="0.988873">
Figure 6: Example error from the stocks task. The
</figureCaption>
<bodyText confidence="0.903171285714286">
system’s prediction, in blue (solid), fails to seg-
ment the input into two events, and thus incor-
rectly extends the losing trend to the entire output
time span.
features learned by the model—as desired, it cor-
rectly interprets a variety of different expressions
used to describe stock behavior.
</bodyText>
<subsectionHeader confidence="0.99828">
5.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.999836928571429">
As suggested by Table 4, learned weights for the
trajectory-grounded features Bt are largely correct.
Thus, most incorrect outputs from the system in-
volve alignment to time. Many multipart events
(like rebound) can be reasonably explained using
the curvature feature without splitting the text into
two segments; as a result, the system tends to be
fairly conservative about segmentation and often
under-segments. This results in examples like Fig-
ure 6, in which the downward trend suggested by
losing is incorrectly extended to the entire out-
put curve. Here, another informal experiment us-
ing humans as the predictors indicates that pre-
dictions are farther from human-level performance
</bodyText>
<table confidence="0.433816333333333">
Word Sign Magnitude ·103
rise 0.27 −0.78
swoon −0.57 0
sharply −0.22 0.28
slammed −0.36 0
lifted 0.66 0
</table>
<tableCaption confidence="0.628004">
Table 4: Learned parameter settings for overall
</tableCaption>
<bodyText confidence="0.612228">
daily change, which the path featurization decom-
poses into a sign and a magnitude.
than they are on the colors task.
</bodyText>
<sectionHeader confidence="0.974104" genericHeader="method">
6 Generalizing the model
</sectionHeader>
<bodyText confidence="0.999995789473684">
Last we consider the problem of following navi-
gational directions. The difference between this
and the previous task is largely one of scale: rather
than attempting to predict the values of only two
segments, we have a long string of them. The text,
rather than a single tree, consists of a sequence of
tens or hundreds of pre-segmented utterances.
There is one additional complication—rather
than being defined in an absolute space, as they are
in the case of stocks, constraints in the maps do-
main are provided relative to a set of known land-
marks (like the white water and stone creek in Fig-
ure 3). We resolve landmarks automatically based
on string matching, in a manner similar to Vogel
and Jurafsky (2010), and assign each sentence in
the discourse with a single referred-to landmark li.
If no landmark is explicitly named, it inherits from
the previous utterance. We continue to score con-
straints as before, but update the prediction factor:
</bodyText>
<equation confidence="0.68587425">
�
0..
1 if Ai � j
acv (Ai Ci Vj) = e−||li+Ci−V;||2 2 o.w. (9)
</equation>
<bodyText confidence="0.99941112">
The factor graph is shown in Figure 7; ob-
serve that this is simply an unrolled version of
Figure 4—the basic structure of the model is un-
changed. While pre-segmentation of the discourse
means we can avoid aligning internal constituents
of trees, we still need to treat every utterance as an
alignment candidate, without a sparse combinato-
rial constraint. As a result, the sum over A and
C is no longer tractable to compute explicitly, and
approximate inference will be necessary.
For the experiments described in this paper, we
do this with a sequence of Monte Carlo approxi-
mations. We run a Gibbs sampler, iteratively re-
sampling each Ai and Ci as well as the parameter
vectors Bt and Ba to obtain estimates of ]EBt and
EBa. The resampling steps for Bt and Ba are them-
selves difficult to perform exactly, so we perform
an internal Metropolis-Hastings run (with a Gaus-
sian proposal distribution) to obtain samples from
the marginal distributions over Bt and Ba.
We approximate the mode of the posterior dis-
tribution by its mean. To follow a new set of direc-
tions in the prediction phase, we fix the parameter
vectors and instead sample over A, C and V, and
output ]EV. To complete the prediction process
</bodyText>
<page confidence="0.996788">
65
</page>
<figure confidence="0.999459642857143">
V1 V2
acv
a⇤
Al C1 A2 C2 A3 C3
tc
ta
T1 T2 T3
AN
CN
TN
VM
· · ·
· · ·
· · ·
</figure>
<figureCaption confidence="0.999971">
Figure 7: Factor graph for the general grounding model. Note that Figure 4 is a subgraph.
</figureCaption>
<bodyText confidence="0.995216">
we must invert ϕ,,, which we do by producing the
shortest path licensed by the features.
</bodyText>
<subsectionHeader confidence="0.914705">
6.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999947303030303">
The Map Task Corpus consists of 128 dia-
logues describing paths on 16 maps, accompa-
nied by transcriptions of spoken instructions, pre-
segmented using prosodic cues. See Vogel and Ju-
rafsky (2010) for a more detailed description of the
corpus in a language learning setting. For com-
parability, we’ll use the same evaluation as Vogel
and Jurafsky, which rewards the system for mov-
ing between pairs of landmarks that also appear in
the reference path, and penalizes it for additional
superfluous movement. Note that we are solv-
ing a significantly harder problem: the version ad-
dressed by Vogel and Jurafsky is a discrete search
problem, and the system has hard-coded knowl-
edge that all paths pass along one of the four sides
of each landmark. Our system, by contrast, can
navigate to any point in R2, and must learn that
most paths stay close to a named landmark.
At test time, the system is given a new sequence
of text instructions, and must output the corre-
sponding path. It is scored on the fraction of
correct transitions in its output path (precision),
and the fraction of transitions in the gold path
recovered (recall). Vogel and Jurafsky compare
their system to a policy-gradient algorithm for us-
ing language to follow natural language instruc-
tions described by Branavan et al. (2009), and we
present both systems for comparison.
Results are shown in Table 5. Our system sub-
stantially outperforms the policy gradient baseline
of Branavan et al., and performs close (particularly
with respect to transition recall) to the system of
Vogel and Jurafsky, with fewer assumptions.
</bodyText>
<table confidence="0.9959515">
System Prec. Recall F1
Branavan et al. (09) 0.31 0.44 0.36
Vogel &amp; Jurafsky (10) 0.46 0.51 0.48
This work 0.43 0.51 0.45
</table>
<tableCaption confidence="0.9935005">
Table 5: Results for the navigation task. Higher is
better for all of precision, recall and F1.
</tableCaption>
<subsectionHeader confidence="0.996">
6.2 Error analysis
</subsectionHeader>
<bodyText confidence="0.999989222222222">
As in the case of stocks, most of the prediction
errors on this task are a result of misalignment.
In particular, many of the dialogues make passing
reference to already-visited landmarks, or define
destinations in empty regions of the map in terms
of multiple landmarks simultaneously. In each of
these cases, the system is prone to directly visit-
ing the named landmark or landmarks instead of
ignoring or interpolating as necessary.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999993111111111">
We have presented a probabilistic model for
grounding natural language text in vector-valued
state sequences. The model is capable of seg-
menting text into a series of events, ordering these
events in time, and compositionally determining
their internal structure. We have evaluated on a va-
riety of new and established applications involving
colors, time series and navigation, demonstrating
improvements over strong baselines in all cases.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992594">
This work was partially supported by BBN under
DARPA contract HR0011-12-C-0014. The first
author is supported by a National Science Foun-
dation Graduate Research Fellowship.
</bodyText>
<page confidence="0.985696">
66
</page>
<sectionHeader confidence="0.998225" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999758980392157">
Anne H Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllister,
Jim Miller, et al. 1991. The HCRC map task corpus.
Language and speech, 34(4):351–366.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49–62.
Michele Basseville and Igor V Nikiforov. 1995. De-
tection of abrupt changes: theory and applications.
Journal of the Royal Statistical Society-Series A
Statistics in Society, 158(1):185.
SRK Branavan, Harr Chen, Luke S Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 82–90. Association for Com-
putational Linguistics.
Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014.
Multimodal distributional semantics. Journal of Ar-
tificial Intelligence Research, 49:1–47.
David L Chen and Raymond J Mooney. 2011. Learn-
ing to interpret natural language navigation instruc-
tions from observations. In AAAI, volume 2.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 conference on
Empirical Methods in Natural Language Process-
ing, pages 410–419. Association for Computational
Linguistics.
Christopher Kennedy and Louise McNally. 2010.
Color, context, and compositionality. Synthese,
174(1):79–98.
Shimon Kogan, Dimitry Levin, Bryan R Routledge,
Jacob S Sagi, and Noah A Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 272–280. Association for Computa-
tional Linguistics.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Grounding verbs of motion in natu-
ral language commands to robots. In International
Symposium on Experimental Robotics.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: connecting
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Percy Liang, Michael I Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389–446.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical programming, 45(1-3):503–528.
Terence Parsons. 1990. Events in the semantics of En-
glish. MIT Press.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of Human
Language Technologies: The 2007 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics. Assocation for
Computational Linguistics.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In In Proceedings of the Na-
tional Conference on Artificial Intelligence.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267–288.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 806–814. Association for
Computational Linguistics.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from videos described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics. Association for Computational Linguistics.
Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.
2007. Choosing the content of textual summaries of
large time-series data sets. Natural Language Engi-
neering, 13(1):25–49.
Jeffrey M Zacks and Khena M Swallow. 2007. Event
segmentation. Current Directions in Psychological
Science, 16(2):80–84.
John M Zelle and Raymond J Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence, pages 1050–1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence, pages 658–
666.
</reference>
<page confidence="0.999498">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.668721">
<title confidence="0.999993">Grounding Language with Points and Paths in Continuous Spaces</title>
<author confidence="0.989915">Andreas</author>
<affiliation confidence="0.999886">Computer Science University of California,</affiliation>
<abstract confidence="0.999924">We present a model for generating pathvalued interpretations of natural language text. Our model encodes a map from natural language descriptions to paths, mediated by segmentation variables which break the language into a discrete set of events, and alignment variables which reorder those events. Within an event, lexical weights capture the contribution of each word to the aligned path segment. We demonstrate the applicability of our model on three diverse tasks: a new color description task, a new financial news task and an established direction-following task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system</abstract>
<note confidence="0.67679">described in Vogel and Jurafsky (2010).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne H Anderson</author>
<author>Miles Bader</author>
<author>Ellen Gurman Bard</author>
<author>Elizabeth Boyle</author>
<author>Gwyneth Doherty</author>
<author>Simon Garrod</author>
<author>Stephen Isard</author>
<author>Jacqueline Kowtko</author>
<author>Jan McAllister</author>
<author>Jim Miller</author>
</authors>
<title>The HCRC map task corpus. Language and speech,</title>
<date>1991</date>
<pages>34--4</pages>
<contexts>
<context position="9227" citStr="Anderson et al. (1991)" startWordPosition="1460" endWordPosition="1463">der in time. The learning of correspondences between text and time series has attracted more interest in natural language generation than in semantics (Yu et al., 2007). Research on natural language processing and stock data, meanwhile, has largely focused on prediction of future events (Kogan et al., 2009). Direction following We’ll conclude by applying our model to the well-studied problem of following navigational directions. A variety of reinforcement-learning approaches for following directions on a map were previously investigated by Vogel and Jurafsky (2010) using a corpus assembled by Anderson et al. (1991). An example portion of a path and its accompanying instruction is shown in Figure 3. While also representable as a set of real valued coordinates, here 2-d, this data set looks very different—a typical example consists of more than a hundred sentences of the kind shown in Figure 3, accompanying a long path. The language, a transcript of a spoken dialog, is also considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of this problem and the stock problem are fundamental</context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, 1991</marker>
<rawString>Anne H Anderson, Miles Bader, Ellen Gurman Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller, et al. 1991. The HCRC map task corpus. Language and speech, 34(4):351–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="10201" citStr="Artzi and Zettlemoyer (2013)" startWordPosition="1618" endWordPosition="1621">en dialog, is also considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of this problem and the stock problem are fundamentally similar. In addition to Vogel and Jurafsky, Tellex et al. (2011) give a weakly-supervised model for mapping single sentences to commands, and Branavan et al. (2009) give an alternative reinforcementlearning approach for following long command sequences. An intermediate between this approach and ours is the work of Chen and Mooney (2011) and Artzi and Zettlemoyer (2013), which bootstrap a semantic parser to generate logical forms specifying the output path, rather than predicting the path directly. Between them, these tasks span a wide range of linguistic phenomena relevant to grounded semantics, and provide a demonstration of the usefulness and general applicability of our model. While development of the perceptual groundwork necessary to generalize these results to more complex state spaces remains a major problem, our three examples provide a starting point for studying the relationship between perception, time and the semantics of natural language. 3 Pre</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Basseville</author>
<author>Igor V Nikiforov</author>
</authors>
<title>Detection of abrupt changes: theory and applications.</title>
<date>1995</date>
<journal>Journal of the Royal Statistical Society-Series A Statistics in Society,</journal>
<volume>158</volume>
<issue>1</issue>
<contexts>
<context position="18860" citStr="Basseville and Nikiforov, 1995" startWordPosition="3095" endWordPosition="3098">g to pass through a symbolic meaning representation, however, this event structure will be used to map text directly into the grounding domain. We assume that this domain has pre-existing structure—in particular, that in our input paths V, the boundaries of events have already been identified, and that the problem of aligning text to portions of the segment only requires aligning to segment indices rather than fine-grained time indices. This is a strong assumption, and one that future work may wish to revisit, but there exist both computational tools from the changepoint detection literature (Basseville and Nikiforov, 1995) and pieces of evidence from cognitive science (Zacks and Swallow, 2007) which suggest that assuming a pre-linguistic structuring of events is a reasonable starting point. In the text domain, we make the corresponding assumption that each of these events is syntactically local—that a given span of the input sentence provides information about at most one of these segmented events. The main structural difference between the color example in Figure 2 and the stock market example in Figure 1 is the introduction of a time dimension orthogonal to the dimensions of the state space. To accommodate th</context>
</contexts>
<marker>Basseville, Nikiforov, 1995</marker>
<rawString>Michele Basseville and Igor V Nikiforov. 1995. Detection of abrupt changes: theory and applications. Journal of the Royal Statistical Society-Series A Statistics in Society, 158(1):185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>Harr Chen</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>82--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9994" citStr="Branavan et al. (2009)" startWordPosition="1585" endWordPosition="1589">s, here 2-d, this data set looks very different—a typical example consists of more than a hundred sentences of the kind shown in Figure 3, accompanying a long path. The language, a transcript of a spoken dialog, is also considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of this problem and the stock problem are fundamentally similar. In addition to Vogel and Jurafsky, Tellex et al. (2011) give a weakly-supervised model for mapping single sentences to commands, and Branavan et al. (2009) give an alternative reinforcementlearning approach for following long command sequences. An intermediate between this approach and ours is the work of Chen and Mooney (2011) and Artzi and Zettlemoyer (2013), which bootstrap a semantic parser to generate logical forms specifying the output path, rather than predicting the path directly. Between them, these tasks span a wide range of linguistic phenomena relevant to grounded semantics, and provide a demonstration of the usefulness and general applicability of our model. While development of the perceptual groundwork necessary to generalize thes</context>
<context position="32377" citStr="Branavan et al. (2009)" startWordPosition="5406" endWordPosition="5409">ledge that all paths pass along one of the four sides of each landmark. Our system, by contrast, can navigate to any point in R2, and must learn that most paths stay close to a named landmark. At test time, the system is given a new sequence of text instructions, and must output the corresponding path. It is scored on the fraction of correct transitions in its output path (precision), and the fraction of transitions in the gold path recovered (recall). Vogel and Jurafsky compare their system to a policy-gradient algorithm for using language to follow natural language instructions described by Branavan et al. (2009), and we present both systems for comparison. Results are shown in Table 5. Our system substantially outperforms the policy gradient baseline of Branavan et al., and performs close (particularly with respect to transition recall) to the system of Vogel and Jurafsky, with fewer assumptions. System Prec. Recall F1 Branavan et al. (09) 0.31 0.44 0.36 Vogel &amp; Jurafsky (10) 0.46 0.51 0.48 This work 0.43 0.51 0.45 Table 5: Results for the navigation task. Higher is better for all of precision, recall and F1. 6.2 Error analysis As in the case of stocks, most of the prediction errors on this task are </context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>SRK Branavan, Harr Chen, Luke S Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 82–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="1755" citStr="Bruni et al. (2014)" startWordPosition="252" endWordPosition="255">ositional semantics has focused on discrete representations of meaning. The original focus of such work was on logical translation: mapping statements of natural language to a formal language like first-order logic (Zettlemoyer and Collins, 2005) or database queries (Zelle and Mooney, 1996). Subsequent work has integrated this logical translation with interpretation against a symbolic database (Liang et al., 2013). There has been a recent increase in interest in perceptual grounding, where lexical semantics anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock data. The chart displays index value over a two-day period (divided by the dotted line), while the accompanying headline describes the observed behavior. tributional information. Yu and Siskind (2013) describe a model for identifying scenes given descriptions, and Golland et al. (2010), Kollar et al. (2010), and Krishnamurthy and Kollar (2013) describe models for identi</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artificial Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In AAAI,</booktitle>
<volume>2</volume>
<contexts>
<context position="10168" citStr="Chen and Mooney (2011)" startWordPosition="1613" endWordPosition="1616">age, a transcript of a spoken dialog, is also considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of this problem and the stock problem are fundamentally similar. In addition to Vogel and Jurafsky, Tellex et al. (2011) give a weakly-supervised model for mapping single sentences to commands, and Branavan et al. (2009) give an alternative reinforcementlearning approach for following long command sequences. An intermediate between this approach and ours is the work of Chen and Mooney (2011) and Artzi and Zettlemoyer (2013), which bootstrap a semantic parser to generate logical forms specifying the output path, rather than predicting the path directly. Between them, these tasks span a wide range of linguistic phenomena relevant to grounded semantics, and provide a demonstration of the usefulness and general applicability of our model. While development of the perceptual groundwork necessary to generalize these results to more complex state spaces remains a major problem, our three examples provide a starting point for studying the relationship between perception, time and the sem</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David L Chen and Raymond J Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In AAAI, volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>410--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2269" citStr="Golland et al. (2010)" startWordPosition="335" endWordPosition="338"> anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock data. The chart displays index value over a two-day period (divided by the dotted line), while the accompanying headline describes the observed behavior. tributional information. Yu and Siskind (2013) describe a model for identifying scenes given descriptions, and Golland et al. (2010), Kollar et al. (2010), and Krishnamurthy and Kollar (2013) describe models for identifying individual components of scenes described by text. These all have the form of matching problems between text and observed groundings—what has been missing so far is the ability to generate grounded interpretations from scratch, given only text. Our work continues in the tradition of this perceptual grounding work, but makes two contributions. First, our approach is able to predict simple world states (and their evolution): for a general class of continuous domains, we produce a representation of p(world</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 conference on Empirical Methods in Natural Language Processing, pages 410–419. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Kennedy</author>
<author>Louise McNally</author>
</authors>
<title>Color, context, and compositionality.</title>
<date>2010</date>
<journal>Synthese,</journal>
<volume>174</volume>
<issue>1</issue>
<contexts>
<context position="7053" citStr="Kennedy and McNally, 2010" startWordPosition="1106" endWordPosition="1109"> differentiation made by basic color names in most languages. Other modifiers act on the saturation and value axes: either simple ones like dark (which decreases value), or more complicated ones like pastel (which increases value and decreases saturation). Given a set of named colors and their HSV coordinates, a learning algorithm should be able to identify the effects of each word in the vocabulary and predict the appearance of new colors with previously-unseen combinations of modifiers. Compositional interpretations of color have received attention in linguistics and philosophy of language (Kennedy and McNally, 2010), but while work in grounded computational semantics like that of Krishnamurthy and Kollar (2013) has succeeded in learning simple color predicates, our model is the first to capture the machine learning of color in a fine-grained, compositional way. Time series As a first step into temporal structure, we’ll consider language describing the behavior of stock market indices. Here, again, there is a simple parameterization—in this case just a single number describing the total value of the index—but as shown by the headline example in Figure 1, the language used to describe changes in the stock </context>
</contexts>
<marker>Kennedy, McNally, 2010</marker>
<rawString>Christopher Kennedy and Louise McNally. 2010. Color, context, and compositionality. Synthese, 174(1):79–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan R Routledge</author>
<author>Jacob S Sagi</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>272--280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8913" citStr="Kogan et al., 2009" startWordPosition="1414" endWordPosition="1417">ning or end of the trading day. Along with temporal structure, the problem requires a more sophisticated treatment of syntax than the colors case— now we have to identify which subspans of the sentence are associated with each event observed, and determine the correspondence between surface order and actual order in time. The learning of correspondences between text and time series has attracted more interest in natural language generation than in semantics (Yu et al., 2007). Research on natural language processing and stock data, meanwhile, has largely focused on prediction of future events (Kogan et al., 2009). Direction following We’ll conclude by applying our model to the well-studied problem of following navigational directions. A variety of reinforcement-learning approaches for following directions on a map were previously investigated by Vogel and Jurafsky (2010) using a corpus assembled by Anderson et al. (1991). An example portion of a path and its accompanying instruction is shown in Figure 3. While also representable as a set of real valued coordinates, here 2-d, this data set looks very different—a typical example consists of more than a hundred sentences of the kind shown in Figure 3, ac</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan R Routledge, Jacob S Sagi, and Noah A Smith. 2009. Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 272–280. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Kollar</author>
<author>Stefanie Tellex</author>
<author>Deb Roy</author>
<author>Nicholas Roy</author>
</authors>
<title>Grounding verbs of motion in natural language commands to robots.</title>
<date>2010</date>
<booktitle>In International Symposium on Experimental Robotics.</booktitle>
<contexts>
<context position="2291" citStr="Kollar et al. (2010)" startWordPosition="339" endWordPosition="342">ariables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock data. The chart displays index value over a two-day period (divided by the dotted line), while the accompanying headline describes the observed behavior. tributional information. Yu and Siskind (2013) describe a model for identifying scenes given descriptions, and Golland et al. (2010), Kollar et al. (2010), and Krishnamurthy and Kollar (2013) describe models for identifying individual components of scenes described by text. These all have the form of matching problems between text and observed groundings—what has been missing so far is the ability to generate grounded interpretations from scratch, given only text. Our work continues in the tradition of this perceptual grounding work, but makes two contributions. First, our approach is able to predict simple world states (and their evolution): for a general class of continuous domains, we produce a representation of p(world |text) that admits ea</context>
</contexts>
<marker>Kollar, Tellex, Roy, Roy, 2010</marker>
<rawString>Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. 2010. Grounding verbs of motion in natural language commands to robots. In International Symposium on Experimental Robotics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Thomas Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: connecting natural language to the physical world. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="2328" citStr="Krishnamurthy and Kollar (2013)" startWordPosition="344" endWordPosition="347">s, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock data. The chart displays index value over a two-day period (divided by the dotted line), while the accompanying headline describes the observed behavior. tributional information. Yu and Siskind (2013) describe a model for identifying scenes given descriptions, and Golland et al. (2010), Kollar et al. (2010), and Krishnamurthy and Kollar (2013) describe models for identifying individual components of scenes described by text. These all have the form of matching problems between text and observed groundings—what has been missing so far is the ability to generate grounded interpretations from scratch, given only text. Our work continues in the tradition of this perceptual grounding work, but makes two contributions. First, our approach is able to predict simple world states (and their evolution): for a general class of continuous domains, we produce a representation of p(world |text) that admits easy sampling and maximization. This ma</context>
<context position="7150" citStr="Krishnamurthy and Kollar (2013)" startWordPosition="1120" endWordPosition="1123">uration and value axes: either simple ones like dark (which decreases value), or more complicated ones like pastel (which increases value and decreases saturation). Given a set of named colors and their HSV coordinates, a learning algorithm should be able to identify the effects of each word in the vocabulary and predict the appearance of new colors with previously-unseen combinations of modifiers. Compositional interpretations of color have received attention in linguistics and philosophy of language (Kennedy and McNally, 2010), but while work in grounded computational semantics like that of Krishnamurthy and Kollar (2013) has succeeded in learning simple color predicates, our model is the first to capture the machine learning of color in a fine-grained, compositional way. Time series As a first step into temporal structure, we’ll consider language describing the behavior of stock market indices. Here, again, there is a simple parameterization—in this case just a single number describing the total value of the index—but as shown by the headline example in Figure 1, the language used to describe changes in the stock market can be quite complex. Head59 right round the white water [... I but stay quite close ’caus</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: connecting natural language to the physical world. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="1553" citStr="Liang et al., 2013" startWordPosition="222" endWordPosition="225">n Vogel and Jurafsky (2010). 1 Introduction This paper introduces a probabilistic model for predicting grounded, real-valued trajectories from natural language text. A long tradition of research in compositional semantics has focused on discrete representations of meaning. The original focus of such work was on logical translation: mapping statements of natural language to a formal language like first-order logic (Zettlemoyer and Collins, 2005) or database queries (Zelle and Mooney, 1996). Subsequent work has integrated this logical translation with interpretation against a symbolic database (Liang et al., 2013). There has been a recent increase in interest in perceptual grounding, where lexical semantics anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock data. The chart displays index value over a two-day period (divided by the dotted line), while the accompanying headline describes the observed behavior. tributional info</context>
</contexts>
<marker>Liang, Jordan, Klein, 2013</marker>
<rawString>Percy Liang, Michael I Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical programming,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="24158" citStr="Liu and Nocedal, 1989" startWordPosition="4020" endWordPosition="4024">)) in the length of the input sentence. We wish to find the maximum a posteriori estimate p(Bt, Ba|T, V) for Bt and Ba, which we can do V1 V2 acv Al Cl A2 C2 a⇤ tc ta T1 T2 · · · eθ⊤ a ϕa(Ti,Ai,Ai−1) 63 using the Expectation–Maximization algorithm. To find regression scoring weights Bt, we have: E step: [∑ ] [∑ ] M = E Ti(Ti)⊤ ; N = E TiV ⊤ (6) Ai i i M step: Bt = M−&apos;N (7) To find alignment scoring weights Ba, we must maximize: ∑ ⎡ ⎛ ⎞ ⎤ i eθ⊤a ϕa(Ai,Ai−1,T i) E⎣log ⎝ ⎠ ∑ ⎦ (8) A′ i,A′ i−1 eθ⊤ a ϕa(A′ i,A′ i−1,Ti) which can be done using a variety of convex optimization tools; we used L-BFGS (Liu and Nocedal, 1989). The predictive distribution p(V|T) can also be straightforwardly computed using the standard inference procedures for graphical models. 5.2 Evaluation Our stocks dataset consists of a set of headlines from the “Market Snapshot” column of the Wall Street Journal’s MarketWatch website,1 paired with hourly stock charts for each day described in a headline. Data is collected over a roughly decade-long period between 2001 and 2012; after removing weekends and days with incomplete stock data, we have a total of 2218 headline/time series pairs. As headlines most often discuss a single day or a shor</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Parsons</author>
</authors>
<title>Events in the semantics of English.</title>
<date>1990</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18074" citStr="Parsons, 1990" startWordPosition="2968" endWordPosition="2969">e, a base color is seen only with a small (or related) set of modifiers, from which the system is unable to infer the meaning of the base color (e.g. from Japanese indigo, lavender indigo, and electric indigo, the learning algorithm infers that indigo is bright purple). In the other, no part of the color word is seen in training, and the system outputs an unrelated “default” color (teal is predicted to be bright red). The idea that a sentence’s meaning is fundamentally described by a set of events, each associated with a set of predicates, is well-developed in neoDavidsonian formal semantics (Parsons, 1990). We adopt the skeleton of this formal approach by tying our model to (latent) partitions of the input sentence into disjoint events. Rather than attempting to pass through a symbolic meaning representation, however, this event structure will be used to map text directly into the grounding domain. We assume that this domain has pre-existing structure—in particular, that in our input paths V, the boundaries of events have already been identified, and that the problem of aligning text to portions of the segment only requires aligning to segment indices rather than fine-grained time indices. This</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>Terence Parsons. 1990. Events in the semantics of English. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technologies: The 2007 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Assocation for Computational Linguistics.</booktitle>
<contexts>
<context position="12439" citStr="Petrov and Klein, 2007" startWordPosition="1998" endWordPosition="2001">rvature of quadratic approx.to V (stocks only) Cartesian prod. of Ot(T) with: ϕa(T, Ai, Ai−1) ■ H[Ai is aligned] ■ H[Ai−1 is aligned] ■ A1 − Ai−1 (if both aligned) Table 1: Features used for linear parameterization of the grounding model. simplify notation by writing Ti = ϕt(7) and Vi = ϕv(Vi). As the ultimate prediction task is to produce paths, and not their featurized representations, we will assume that it is also straightforward to compute ϕ−1 v , which projects path features back into the original grounding domain. All parse trees are predicted from input text using the Berkeley Parser (Petrov and Klein, 2007). Feature representations for both trees and paths are simple and largely domain-independent; they are explicitly enumerated in Table 1. The general framework presented here leaves one significant problem unaddressed: given a large state vector encoding properties of multiple objects, how do we resolve an utterance about a single object to the correct subset of indices in the vector? While none of the tasks considered in this paper require an argument resolution step of this kind, interpretation of noun phrases is one of the better-studied problems in compositional semantics (Zelle and Mooney </context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of Human Language Technologies: The 2007 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Assocation for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Tellex</author>
<author>Thomas Kollar</author>
<author>Steven Dickerson</author>
<author>Matthew R Walter</author>
<author>Ashis Gopal Banerjee</author>
<author>Seth Teller</author>
<author>Nicholas Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation. In</title>
<date>2011</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="9894" citStr="Tellex et al. (2011)" startWordPosition="1569" endWordPosition="1572">ying instruction is shown in Figure 3. While also representable as a set of real valued coordinates, here 2-d, this data set looks very different—a typical example consists of more than a hundred sentences of the kind shown in Figure 3, accompanying a long path. The language, a transcript of a spoken dialog, is also considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of this problem and the stock problem are fundamentally similar. In addition to Vogel and Jurafsky, Tellex et al. (2011) give a weakly-supervised model for mapping single sentences to commands, and Branavan et al. (2009) give an alternative reinforcementlearning approach for following long command sequences. An intermediate between this approach and ours is the work of Chen and Mooney (2011) and Artzi and Zettlemoyer (2013), which bootstrap a semantic parser to generate logical forms specifying the output path, rather than predicting the path directly. Between them, these tasks span a wide range of linguistic phenomena relevant to grounded semantics, and provide a demonstration of the usefulness and general app</context>
</contexts>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R. Walter, Ashis Gopal Banerjee, Seth Teller, and Nicholas Roy. 2011. Understanding natural language commands for robotic navigation and mobile manipulation. In In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>267--288</pages>
<contexts>
<context position="14786" citStr="Tibshirani, 1996" startWordPosition="2396" endWordPosition="2398">t, we tend to describe intermediates as northeast rather than inventing an additional word which means “a little of both”. As discussed above, we can represent a color as a point in a three-dimensional HSV space. Let T denote features on the parse tree of the color name, and V its representation in color space (consistent with the definition of ϕv given in Table 1). Linearity suggests the following model: p(T,V ) oc e−∥θ⊤t T−V ∥2 (1) The learning problem is then: ∑ � � � 2 argmin �θ⊤ t T − V � (2) θt 2 T,V which, with a sparse prior on θt, is the probabilistic formulation of Lasso regression (Tibshirani, 1996), for which standard tools are available in the optimization literature. To predict color space values from a new (featurized) name T, we output: argmax V p(T,V ) = θ⊤t T 4.1 Evaluation We collect a set of color names and their corresponding HSV triples from the English Wikipedia’s List of Colors, retaining only those color names in which every word appears at least three times in the training corpus. This leaves a set of 419 colors, which we randomly divide into a 377-item training set and 42-item test set. The model’s goal will be to learn to identify new colors given only their names. We co</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Dan Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>806--814</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="961" citStr="Vogel and Jurafsky (2010)" startWordPosition="137" endWordPosition="140"> paths, mediated by segmentation variables which break the language into a discrete set of events, and alignment variables which reorder those events. Within an event, lexical weights capture the contribution of each word to the aligned path segment. We demonstrate the applicability of our model on three diverse tasks: a new color description task, a new financial news task and an established direction-following task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010). 1 Introduction This paper introduces a probabilistic model for predicting grounded, real-valued trajectories from natural language text. A long tradition of research in compositional semantics has focused on discrete representations of meaning. The original focus of such work was on logical translation: mapping statements of natural language to a formal language like first-order logic (Zettlemoyer and Collins, 2005) or database queries (Zelle and Mooney, 1996). Subsequent work has integrated this logical translation with interpretation against a symbolic database (Liang et al., 2013). There </context>
<context position="3749" citStr="Vogel and Jurafsky (2010)" startWordPosition="560" endWordPosition="563">ementioned spatial semantics work, we consider language that de58 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58–67, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics scribes time-evolving trajectories, and unlike Yu and Siskind (2013), we allow these trajectories to have event substructure, and model temporal ordering. Our class of models generalizes to a variety of different domains: a new color-picking task, a new financial news task, and a more challenging variant of the direction-following task established by Vogel and Jurafsky (2010). As an example of the kinds of phenomena we want to model, consider Figure 1, which shows the value of the Dow Jones Industrial Average over June 3rd and 4th 2008, along with a financial news headline from June 4th. There are several effects of interest here. One phenomenon we want to capture is that the lexical semantics of individual words must be combined: swoon roughly describes a drop while bruising indicates that the drop was severe. We isolate this lexical combination in Section 4, where we consider a limited model of color descriptions (Figure 2). A second phenomenon is that the descr</context>
<context position="5000" citStr="Vogel and Jurafsky (2010)" startWordPosition="775" endWordPosition="778">separate events, a swoon and a rebound; moreover, those events do not occur in their textual order, as revealed by after. In Section 5, we extend the model to include segmentation and ordering variables and apply it to this stock data. The situation where language describes a path through some continuous space—literal or metaphorical—is more general than stock headlines. Our claim is that a variety of problems in language share these same characteristics. To demonstrate generality of the model, we also apply it in Section 6 to a challenging variant of the direction-following task described by Vogel and Jurafsky (2010) (Figure 3), where we achieve results close to a state-of-the-art system that makes stronger assumptions about the task. 2 Three tasks in grounded semantics The problem of inferring a structured state representation from sensory input is a hard one, but we can begin to tackle grounded semantics by restricting ourselves to cases where we have sequences of real-valued observations directly described by text. In this paper we’ll consider the problems of recognizing colors, describing time series, and following navigational instructions. While these tasks have been independently studied, we believ</context>
<context position="9176" citStr="Vogel and Jurafsky (2010)" startWordPosition="1450" endWordPosition="1453">the correspondence between surface order and actual order in time. The learning of correspondences between text and time series has attracted more interest in natural language generation than in semantics (Yu et al., 2007). Research on natural language processing and stock data, meanwhile, has largely focused on prediction of future events (Kogan et al., 2009). Direction following We’ll conclude by applying our model to the well-studied problem of following navigational directions. A variety of reinforcement-learning approaches for following directions on a map were previously investigated by Vogel and Jurafsky (2010) using a corpus assembled by Anderson et al. (1991). An example portion of a path and its accompanying instruction is shown in Figure 3. While also representable as a set of real valued coordinates, here 2-d, this data set looks very different—a typical example consists of more than a hundred sentences of the kind shown in Figure 3, accompanying a long path. The language, a transcript of a spoken dialog, is also considerably less formal than the language found in the Wall Street Journal examples, involving disfluency, redundancy and occasionally errors. Nevertheless the underlying structure of</context>
<context position="29314" citStr="Vogel and Jurafsky (2010)" startWordPosition="4860" endWordPosition="4863">een this and the previous task is largely one of scale: rather than attempting to predict the values of only two segments, we have a long string of them. The text, rather than a single tree, consists of a sequence of tens or hundreds of pre-segmented utterances. There is one additional complication—rather than being defined in an absolute space, as they are in the case of stocks, constraints in the maps domain are provided relative to a set of known landmarks (like the white water and stone creek in Figure 3). We resolve landmarks automatically based on string matching, in a manner similar to Vogel and Jurafsky (2010), and assign each sentence in the discourse with a single referred-to landmark li. If no landmark is explicitly named, it inherits from the previous utterance. We continue to score constraints as before, but update the prediction factor: � 0.. 1 if Ai � j acv (Ai Ci Vj) = e−||li+Ci−V;||2 2 o.w. (9) The factor graph is shown in Figure 7; observe that this is simply an unrolled version of Figure 4—the basic structure of the model is unchanged. While pre-segmentation of the discourse means we can avoid aligning internal constituents of trees, we still need to treat every utterance as an alignment</context>
<context position="31282" citStr="Vogel and Jurafsky (2010)" startWordPosition="5218" endWordPosition="5222"> set of directions in the prediction phase, we fix the parameter vectors and instead sample over A, C and V, and output ]EV. To complete the prediction process 65 V1 V2 acv a⇤ Al C1 A2 C2 A3 C3 tc ta T1 T2 T3 AN CN TN VM · · · · · · · · · Figure 7: Factor graph for the general grounding model. Note that Figure 4 is a subgraph. we must invert ϕ,,, which we do by producing the shortest path licensed by the features. 6.1 Evaluation The Map Task Corpus consists of 128 dialogues describing paths on 16 maps, accompanied by transcriptions of spoken instructions, presegmented using prosodic cues. See Vogel and Jurafsky (2010) for a more detailed description of the corpus in a language learning setting. For comparability, we’ll use the same evaluation as Vogel and Jurafsky, which rewards the system for moving between pairs of landmarks that also appear in the reference path, and penalizes it for additional superfluous movement. Note that we are solving a significantly harder problem: the version addressed by Vogel and Jurafsky is a discrete search problem, and the system has hard-coded knowledge that all paths pass along one of the four sides of each landmark. Our system, by contrast, can navigate to any point in R</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Dan Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806–814. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounded language learning from videos described with sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2183" citStr="Yu and Siskind (2013)" startWordPosition="321" endWordPosition="324">as been a recent increase in interest in perceptual grounding, where lexical semantics anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock data. The chart displays index value over a two-day period (divided by the dotted line), while the accompanying headline describes the observed behavior. tributional information. Yu and Siskind (2013) describe a model for identifying scenes given descriptions, and Golland et al. (2010), Kollar et al. (2010), and Krishnamurthy and Kollar (2013) describe models for identifying individual components of scenes described by text. These all have the form of matching problems between text and observed groundings—what has been missing so far is the ability to generate grounded interpretations from scratch, given only text. Our work continues in the tradition of this perceptual grounding work, but makes two contributions. First, our approach is able to predict simple world states (and their evoluti</context>
<context position="3439" citStr="Yu and Siskind (2013)" startWordPosition="511" endWordPosition="514">ous domains, we produce a representation of p(world |text) that admits easy sampling and maximization. This makes it possible to produce grounded interpretations of text without reference to a pre-existing scene. Simultaneously, we extend the range of temporal phenomena that can be modeled—unlike the aforementioned spatial semantics work, we consider language that de58 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 58–67, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics scribes time-evolving trajectories, and unlike Yu and Siskind (2013), we allow these trajectories to have event substructure, and model temporal ordering. Our class of models generalizes to a variety of different domains: a new color-picking task, a new financial news task, and a more challenging variant of the direction-following task established by Vogel and Jurafsky (2010). As an example of the kinds of phenomena we want to model, consider Figure 1, which shows the value of the Dow Jones Industrial Average over June 3rd and 4th 2008, along with a financial news headline from June 4th. There are several effects of interest here. One phenomenon we want to cap</context>
<context position="15558" citStr="Yu and Siskind (2013)" startWordPosition="2529" endWordPosition="2532">x V p(T,V ) = θ⊤t T 4.1 Evaluation We collect a set of color names and their corresponding HSV triples from the English Wikipedia’s List of Colors, retaining only those color names in which every word appears at least three times in the training corpus. This leaves a set of 419 colors, which we randomly divide into a 377-item training set and 42-item test set. The model’s goal will be to learn to identify new colors given only their names. We consider two evaluations: one which measures the model’s ability to distinguish the named color from a random alternative—analogous to the evaluation in Yu and Siskind (2013)—and one which measures the absolute difference between predicted and true color values. In particular, in the first evaluation the model is presented with the name of a color and a pair of candidates, one ϕt(T) ϕv(V ) 61 Method Sel. T HI S1 V1 5 Predicting paths 0.50 0.30 0.38 0.39 0.78 0.05 0.26 0.17 0.81 0.07 0.21 0.13 0.86 - - - Table 2: Results for the color selection task. Sel(ection accuracy) is frequency with which the system was able to correctly identify the color described when paired with a random alternative. Other columns are the magnitude of the average prediction error along th</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from videos described with sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Yu</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Chris Mellish</author>
</authors>
<title>Choosing the content of textual summaries of large time-series data sets.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="8773" citStr="Yu et al., 2007" startWordPosition="1392" endWordPosition="1395">: distinction is made between falling and falling sharply; gradual trends are distinguished from those which occur suddenly, at the beginning or end of the trading day. Along with temporal structure, the problem requires a more sophisticated treatment of syntax than the colors case— now we have to identify which subspans of the sentence are associated with each event observed, and determine the correspondence between surface order and actual order in time. The learning of correspondences between text and time series has attracted more interest in natural language generation than in semantics (Yu et al., 2007). Research on natural language processing and stock data, meanwhile, has largely focused on prediction of future events (Kogan et al., 2009). Direction following We’ll conclude by applying our model to the well-studied problem of following navigational directions. A variety of reinforcement-learning approaches for following directions on a map were previously investigated by Vogel and Jurafsky (2010) using a corpus assembled by Anderson et al. (1991). An example portion of a path and its accompanying instruction is shown in Figure 3. While also representable as a set of real valued coordinates</context>
</contexts>
<marker>Yu, Reiter, Hunter, Mellish, 2007</marker>
<rawString>Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish. 2007. Choosing the content of textual summaries of large time-series data sets. Natural Language Engineering, 13(1):25–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey M Zacks</author>
<author>Khena M Swallow</author>
</authors>
<title>Event segmentation.</title>
<date>2007</date>
<journal>Current Directions in Psychological Science,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="18932" citStr="Zacks and Swallow, 2007" startWordPosition="3106" endWordPosition="3110">ure will be used to map text directly into the grounding domain. We assume that this domain has pre-existing structure—in particular, that in our input paths V, the boundaries of events have already been identified, and that the problem of aligning text to portions of the segment only requires aligning to segment indices rather than fine-grained time indices. This is a strong assumption, and one that future work may wish to revisit, but there exist both computational tools from the changepoint detection literature (Basseville and Nikiforov, 1995) and pieces of evidence from cognitive science (Zacks and Swallow, 2007) which suggest that assuming a pre-linguistic structuring of events is a reasonable starting point. In the text domain, we make the corresponding assumption that each of these events is syntactically local—that a given span of the input sentence provides information about at most one of these segmented events. The main structural difference between the color example in Figure 2 and the stock market example in Figure 1 is the introduction of a time dimension orthogonal to the dimensions of the state space. To accommodate this change, we extend the model described in the previous subsection in t</context>
</contexts>
<marker>Zacks, Swallow, 2007</marker>
<rawString>Jeffrey M Zacks and Khena M Swallow. 2007. Event segmentation. Current Directions in Psychological Science, 16(2):80–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1427" citStr="Zelle and Mooney, 1996" startWordPosition="205" endWordPosition="208">elines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010). 1 Introduction This paper introduces a probabilistic model for predicting grounded, real-valued trajectories from natural language text. A long tradition of research in compositional semantics has focused on discrete representations of meaning. The original focus of such work was on logical translation: mapping statements of natural language to a formal language like first-order logic (Zettlemoyer and Collins, 2005) or database queries (Zelle and Mooney, 1996). Subsequent work has integrated this logical translation with interpretation against a symbolic database (Liang et al., 2013). There has been a recent increase in interest in perceptual grounding, where lexical semantics anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock data. The chart displays index value over a </context>
<context position="13045" citStr="Zelle and Mooney (1996)" startWordPosition="2093" endWordPosition="2096"> and Klein, 2007). Feature representations for both trees and paths are simple and largely domain-independent; they are explicitly enumerated in Table 1. The general framework presented here leaves one significant problem unaddressed: given a large state vector encoding properties of multiple objects, how do we resolve an utterance about a single object to the correct subset of indices in the vector? While none of the tasks considered in this paper require an argument resolution step of this kind, interpretation of noun phrases is one of the better-studied problems in compositional semantics (Zelle and Mooney (1996), inter alia), and we expect generalization of this approach to be straightforward using these tools. We will consider the color, stock, and navigation tasks in turn. It is possible to view the models we give for all three as instantiations of the same graphical model, but for ease of presentation we will introduce this model incrementally. 4 Predicting vectors Prediction of a color variable from text has the form of a regression problem: given a vector of lexical features extracted from the name, we wish to predict the entries of a vector in color space. It seems linguistically plausible that</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M Zelle and Raymond J Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the National Conference on Artificial Intelligence, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="1382" citStr="Zettlemoyer and Collins, 2005" startWordPosition="198" endWordPosition="201">task. On all three, the model outperforms strong baselines, and on a hard variant of the direction-following task it achieves results close to the state-of-the-art system described in Vogel and Jurafsky (2010). 1 Introduction This paper introduces a probabilistic model for predicting grounded, real-valued trajectories from natural language text. A long tradition of research in compositional semantics has focused on discrete representations of meaning. The original focus of such work was on logical translation: mapping statements of natural language to a formal language like first-order logic (Zettlemoyer and Collins, 2005) or database queries (Zelle and Mooney, 1996). Subsequent work has integrated this logical translation with interpretation against a symbolic database (Liang et al., 2013). There has been a recent increase in interest in perceptual grounding, where lexical semantics anchor in perceptual variables (points, distances, etc.) derived from images or video. Bruni et al. (2014) describe a procedure for constructing word representations using text- and image-based dis% Change 1.02 1.01 1.00 0.99 0.98 10 12 14 10 12 14 Hour of day U.S. stocks rebound after bruising two-day swoon Figure 1: Example stock</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence, pages 658– 666.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.993657">
Semantic Query Expansion for Arabic Information Retrieval
</title>
<author confidence="0.966396">
Ashraf Y. Mahgoub Mohsen A. Rashwan
</author>
<affiliation confidence="0.842148666666667">
Computer Engineering Department, Cairo Electronics and Communications
University, Egypt Engineering Department, Cairo University,
Egypt
</affiliation>
<email confidence="0.994433">
ashraf.thunderstorme@gmail.com mrashwan@rdi-eg.com
</email>
<author confidence="0.993651">
Hazem Raafat Mohamed A. Zahran
</author>
<affiliation confidence="0.967137">
Computer Science Department, Kuwait Computer Engineering Department, Cairo
University, Kuwait City, Kuwait University, Egypt
</affiliation>
<email confidence="0.971886">
hazem@cs.ku.edu.kw moh.a.zahran@eng.cu.edu.eg
</email>
<author confidence="0.971506">
Magda B. Fayek
</author>
<affiliation confidence="0.860257">
Computer Engineering Department, Cairo
University, Egypt
</affiliation>
<email confidence="0.991093">
magdafayek@ieee.org
</email>
<sectionHeader confidence="0.993019" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997770857142857">
Traditional keyword based search is found to
have some limitations. Such as word sense
ambiguity, and the query intent ambiguity
which can hurt the precision. Semantic search
uses the contextual meaning of terms in
addition to the semantic matching techniques
in order to overcome these limitations. This
paper introduces a query expansion approach
using an ontology built from Wikipedia pages
in addition to other thesaurus to improve
search accuracy for Arabic language. Our
approach outperformed the traditional keyword
based approach in terms of both F-score and
NDCG measures.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998853230769231">
As traditional keyword based search
techniques are known to have some limitations,
many researchers are concerned with overcoming
these limitations by developing semantic
information retrieval techniques. These techniques
are concerned with the meaning the user seeks
rather than the exact words of the user&apos;s query.
We consider four main features that make users
prefer semantic based search systems over
keyword-based: Handling Generalizations,
Handling Morphological Variants, Handling
Concept matches, and Handling synonyms with
the correct sense (Word Sense Disambiguation).
</bodyText>
<sectionHeader confidence="0.968876" genericHeader="introduction">
2 Semantic-based Search Features
</sectionHeader>
<bodyText confidence="0.999762">
In this section we will discuss the main features
of semantic search that makes it more tempting
choice over the traditional keyword based
techniques.
</bodyText>
<subsectionHeader confidence="0.994528">
2.1 Handling Generalization
</subsectionHeader>
<bodyText confidence="0.994868833333333">
Handling generalizations allows the system
to provide the user with pages that contains
material relevant to sub-concepts of the user&apos;s
query. Consider the following example in Table 1
where a query contains a general term or concept
&amp;quot;فٌػ&amp;quot;(Violence).
</bodyText>
<page confidence="0.989223">
87
</page>
<note confidence="0.855592">
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
User&apos;s Query In Arabic Equivalent Query In
English
&amp;quot;بٍقٌزفا ىف فٌػ لبوػا&amp;quot; “Violence in Africa”
</note>
<tableCaption confidence="0.417802">
Table1: Example Query 1
</tableCaption>
<bodyText confidence="0.97653425">
Semantic-based search engines should be able to
recognise pages with sub-concepts like:
&amp;quot;ةدببا&amp;quot;(extermination), &amp;quot;غوق&amp;quot; (suppression),&amp;quot;بٌذؼت&amp;quot;
(torture) as relevant to user&apos;s query.
</bodyText>
<subsectionHeader confidence="0.999601">
2.2 Handling Morphological Variations
</subsectionHeader>
<bodyText confidence="0.9830365">
Handling morphological variations allows the
system to provide the user with pages that contain
words derived from the same root as those in
user&apos;s query. Consider the following example in
</bodyText>
<tableCaption confidence="0.654205">
Table 2.
</tableCaption>
<subsectionHeader confidence="0.998818">
2.4 Handling Synonyms With Correct Sense
</subsectionHeader>
<bodyText confidence="0.999692666666667">
Although the meaning of many Arabic words
depends on the word&apos;s diacritics, most Arabic text
is un-vowelized. For example, Table 4 shows the
word “بؼش” has more than a single meaning
depending on its diacritization. System should be
aware which meaning to consider for expansion.
</bodyText>
<table confidence="0.9549222">
Arabic vowelized English Arabic
word equivalent synonyms
بْؼَش People, nation نهأ,يٌٍطاىه
بَؼُش Branches عوزف
Table4: Different senses for word &amp;quot;بؼش&amp;quot;
</table>
<sectionHeader confidence="0.860888" genericHeader="related work">
3 Related Work
</sectionHeader>
<table confidence="0.950756">
User&apos;s Query In Arabic Equivalent Query In
English
&amp;quot;ظسولاا قزشلا ىف رىطتلا&amp;quot; “Development in the
Middle East”
</table>
<tableCaption confidence="0.621785">
Table2: Example Query 2
</tableCaption>
<bodyText confidence="0.9869225">
Pages that contain morphological variants of
the word “رىطتلا” (Development) such as “رُّىطَت”,
“زٌىطَت”, and “ثارُّىطَت” should also be considered
relevant to user&apos;s query.
</bodyText>
<subsectionHeader confidence="0.99966">
2.3 Handling Concept Matches
</subsectionHeader>
<bodyText confidence="0.99983425">
The system should also be aware of concepts
or named entities that may be addressed with
different words. Consider the following example
in Table 3.
</bodyText>
<table confidence="0.816842333333333">
User&apos;s Query In Arabic Equivalent Query In
English
&amp;quot;زصه&amp;quot; “Egypt”
</table>
<tableCaption confidence="0.610952">
Table3: Example Query 3
</tableCaption>
<bodyText confidence="0.998511875">
The term “زصه” has other equivalent
expressions like [“تٍبزؼلا زصه تٌرىهوج”, “ ضرأ
تًبٌكلا”, “بًٍدلا مأ”]. So documents that contain any of
these expressions should be considered relevant.
Query expansion techniques have been
considered by many researchers. The most
successful query expansion techniques depend on
automatic relevance feedback with no
consideration of semantic relations.
(Jinxi Xu and Ralph, 2001) used the highest
TF-IDF 50 terms extracted from the top 10
retrieved documents from AFP (i.e. the
TREC2001 corpus). These 50 terms where
weighted due to their TF-IDF scores and added to
the original query -with addition to terms from
other thesaurus-with the following formula:
</bodyText>
<equation confidence="0.9957045">
weight(t) = oldWeight( t) + 0.4 x L TFIDF(t, D)
tl t,D
</equation>
<bodyText confidence="0.999493066666667">
Where D is the top retrieved documents and t
is the original term. Larkey and Connell (2001)
used a similar technique, but with a different
scoring method.
Wikipedia has been considered as an
ontology source by many researchers. This is due
to its large coverage, up-to-date, and domain
independency. As in (Alkhalifa and Rodrguez,
2008), they proposed an automatic technique for
extending Named Entities of Arabic WordNet
using Wikipedia. They depended mainly on
Wikipedia&apos;s “redirect” pages and Cross-Lingual
links. Also a large scale taxonomy from
Wikipedia deriving technique was proposed by
(Pozetto and Strube, 2007).
</bodyText>
<page confidence="0.993457">
88
</page>
<bodyText confidence="0.99548175">
(Abouenour et al., 2010) proposed a system
that uses Arabic WordNet to enhance Arabic
question/answering. Synonyms from WordNet are
used to expand the question in order to extract the
most semantically relevant passages to the
question.
(Milne et al., 2007) proposed a system called
“KORU” for query expansion using Wikipedia&apos;s
most relevant articles to user&apos;s query. The system
allows the user to refine the set of Wikipedia
pages to be used for expansion. KORU used
“Redirect” pages for expansion; “Hyper Links”
and “Disambiguation Pages” to disambiguate
unrestricted text.
Our proposed system differs from KORU in
several points:
</bodyText>
<listItem confidence="0.985698733333333">
(1) Adding “Subcategories” to handle
generalization.
(2) Adding Wikipedia “Gloss” – First phrase
of the article – when there is no
“Redirect” pages available.
(3) Allowing the user to either expand all
terms in a single query, or expand each
term separately producing multiple
queries. The result lists of these multiple
queries are then combined into a single
result list.
(4) Adding terms from another two
supportive thesaurus, namely “Al Raed”
dictionary and our constructed
“Google_WordNet” dictionary.
</listItem>
<sectionHeader confidence="0.928771" genericHeader="method">
4 Proposed System
</sectionHeader>
<subsectionHeader confidence="0.995989">
4.1 Arabic Resources
</subsectionHeader>
<bodyText confidence="0.9682495">
We depend in our query expansion
mechanism on three Arabic resources: (1) Arabic
Wikipedia Dump, (2) “Al Raed” Dictionary. (2)
“Google_WordNet” Dictionary.
</bodyText>
<subsectionHeader confidence="0.864837">
4.1.1 Arabic Wikipedia
</subsectionHeader>
<bodyText confidence="0.997937304347826">
Our system depends mainly on Arabic
Wikipedia as the main semantic information
source. According to Wikipedia, the Arabic
Wikipedia is currently the 23rd largest edition of
Wikipedia by article count, and is the first Semitic
language to exceed 100,000 articles.
We were able to extract 397,552 Arabic
Semantic set, with 690,236 collocations. The term
“Semantic Set” stands for a set of expressions that
refer to the same Meaning or Entity. For example,
the following set of concepts forms a semantic set
for “بًٍبطٌزب” (Britain): [°بًٍبطٌزبل ةدحتولا تكلوولا&apos;,
°ادٌلزٌآو ىوظؼلا بًٍبطٌزبل ةدحتولا تكلوولا&apos;, °ةزتلكًأ&apos;, ° بًٍبطٌزب
ًوظؼلا&apos;].
To extract the semantic sets, we depend on
the “redirect” pages in addition to the article gloss
that may contain a semantic match. This match
appears in the first paragraph of the article in a
bold font. The categorization system of Wikipedia
is very useful in the task of expanding generic
queries in a more specified form. This is done by
adding “subcategories” of the original term to the
expanded terms.
</bodyText>
<subsubsectionHeader confidence="0.873959">
4.1.2 The Al Raed Monolingual Dictionary:
</subsubsectionHeader>
<bodyText confidence="0.908263333333333">
The “Al Raed” Dictionary is a monolingual
dictionary for modern words 1 . The dictionary
contains 204303 modern Arabic expressions.
</bodyText>
<subsubsectionHeader confidence="0.837515">
4.1.3 The Google_WordIet Dictionary
</subsubsectionHeader>
<bodyText confidence="0.995372166666667">
We collected all the words in WordNet, and
translated them to Arabic using Google Translate.
For each English word, Google Translate provides
different Arabic translations for the English word
each corresponds to a different sense, each sense
has a list of different possible English synonyms.
Using this useful information we were able to
extend WordNet Synset entries into a bilingual
Arabic-English dictionary that maps a set of
Arabic synonyms to its equivalent set of English
synonyms. The basic idea is that, two sets of
English synonyms (each allegedly belongs to a
different sense) can be fused together into one
sense if the number of overlapping words between
the two sets is two or more. Fusing two English
sets together will fuse also their Arabic
translations into one set, thus forming a list of
Arabic synonyms matched to a list of English
synonyms. Table 5 shows a sample of Google
Translate for the word “tough”. We can fuse the
first and the fourth sense together because they
have two words in common namely “strong” and
“robust”. The same applies to the second and the
third senses with “strict” and “tough” in common.
</bodyText>
<footnote confidence="0.634689333333333">
1 Available at
http://www.almaany.com/appendix.php?language=arabic&amp;cat
egory=دئازلا&amp;lang name=ًبزػ
</footnote>
<page confidence="0.999038">
89
</page>
<tableCaption confidence="0.910178">
Thus forming two new mappings as shown in
Table 6.
</tableCaption>
<table confidence="0.999526375">
يٍته solid, strong, robust, firm,
durable
مربص strict, rigorous, tough, rigid,
firm, stringent
ًسبق tough, harsh, rough, severe,
strict, stern
يىق strong, powerful, sturdy,
robust, vigorous
</table>
<tableCaption confidence="0.821817">
Table 5: A sample of Google Translate result for the
word “tough”
</tableCaption>
<table confidence="0.803021833333333">
يٍته, يىق solid, strong, robust, firm,
durable, powerful, sturdy,
vigorous
مربص, ًسبق strict, rigorous, tough, rigid,
firm, stringent, harsh, rough,
severe, stern
</table>
<tableCaption confidence="0.999133">
Table 6: Mapping between a set of Arabic synonyms to
a set of English synonyms.
</tableCaption>
<bodyText confidence="0.999819">
Finally, we use words of the same Arabic set as
an expansion to each other in queries.
</bodyText>
<subsectionHeader confidence="0.980148">
4.2 Indexing and Retrieval
</subsectionHeader>
<bodyText confidence="0.999324588235294">
Our system depends on “LUCENE”, which is
free open source information retrieval library
released under the Apache Software License.
LUCENE was originally written in Java, but it has
ported to other programming languages as well.
We use the “.Net” version of LUCENE.
LUCENE depends on the Vector Space Model
(VSM) of information retrieval, and the Boolean
model to determine how relevant a given
Document is to a User&apos;s query. LUCENE has very
useful set of features, as the “OR” and “AND”
operators that we depend on for our expanded
queries. Documents are analyzed before adding to
the index on two steps: diacritics and stop-words
Removal, and text Normalization. A list of 75
words (Contains: Pronouns, Prepositions...etc.)
has been used as stop-words.
</bodyText>
<subsectionHeader confidence="0.393421">
4.2.1 Normalization
</subsectionHeader>
<bodyText confidence="0.983596">
Three normalization rules were used:
</bodyText>
<listItem confidence="0.999951333333333">
• Replace “إ” with “ي”.
• Replace “ا”, “آ”, “أ” with “ا”
• Replace “ٍ” with “ة”
</listItem>
<subsectionHeader confidence="0.715124">
4.2.2 Stemming
</subsectionHeader>
<bodyText confidence="0.998479692307693">
We implemented Light-10 stemmer developed
by Larkey (2007), as it showed superior
performance over other stemming approaches.
Instead of stemming the whole corpus before
indexing, we grouped set of words with the same
stem and found in the same document into a
dictionary, and then use this dictionary in
expansion. This reduces the probability of
matching between two words sharing the same
stem but with different senses, as they must be
found in the same document in corpus to be used
in expansion.
Consider the following example in table 7:
</bodyText>
<table confidence="0.99330675">
Arabic Word Stem English
Equivalent
تػبطلا َعب طَ Obedience
ىىػبطلا عب طَ Plague
</table>
<tableCaption confidence="0.992896">
Table 7: Example of two words sharing the same stem
but have different senses.
</tableCaption>
<bodyText confidence="0.99997125">
We see that both words share the same stem
“عبط”, yet we don&apos;t expand the word “تػبط” with
the word “ىىػبطلا” as there is no document in the
corpus that contains both words.
</bodyText>
<subsectionHeader confidence="0.997658">
4.3 Query Expansion
</subsectionHeader>
<bodyText confidence="0.999985166666667">
To expand a query, we first locate named
entities or concepts that appear in the query in
Wikipedia. If a named entity or a concept has been
located, we add title of “redirect” pages that leads
to the same concept in addition to its
subcategories from Wikipedia&apos;s categorization
system. If not, we depend on the other two
dictionaries –Al Raed and Google_WordNet- for
expansion.
We investigated two methodologies for query
expansion; the first is the most common query
expansion methodology which is to produce a
single expanded query that contains all expanded
terms. The second methodology we introduced is
to expand each term one at a time producing
multiple queries, and then combine the results of
these queries into a single result list. The second
methodology was found less sensitive to noise
</bodyText>
<page confidence="0.995153">
90
</page>
<bodyText confidence="0.994734333333333">
because for each expanded query, there is only
one source of noise which is the term being
expanded, while other terms are left without
expansion. It also allows the system to boost
documents from one expanded query over other
documents according to the relevancy score of the
expanded term.
The following example explains this intuition:
For the query “ًحبضلأا مبكحأ”
</bodyText>
<equation confidence="0.888491625">
Single Expanded Query:
OR ًحبضلاا OR ًحبضلأا( )نكح OR مبكحا OR مبكحأ)
OR ىحض OR تئٍضه تٍحضإ تلٍل OR ًحبضأ OR تٍحضإ
(بهب ىحضٌ ةبش
Multiple Expanded Queries:
ًحبضلأا )نكح
ًحبضأ OR تٍحضإ
(بهب ىحضٌ ةبش OR ىحض OR تئٍضه تٍحضإ تلٍلOR
</equation>
<bodyText confidence="0.999535">
We see that the term “مبكحأ” gets fewer
expansions than the term “ًحبضلأا”; this is
because the term “ًحبضلأا” is less frequent in the
corpus thus it needs more expansions. We then
combine the results of the two queries by the
following algorithm:
</bodyText>
<listItem confidence="0.6493342">
1- Foreach expanded query QL
a. Foreach retrieved document DQL
for QL
b. If the final list contains DQL
increment the score of DQL by
</listItem>
<equation confidence="0.956662">
RF[tQ ( QL,DQL)
L] x Sc re
</equation>
<bodyText confidence="0.959453">
c. Else add DQL to final list
Where RF is a list of relevancy factors
calculated for each term in the original query. This
factor depends on the term frequency in corpus.
RF is calculated according to the following
formula:
</bodyText>
<equation confidence="0.945387">
(frequency[t] + 0.5 x log(frequency[stemmedj)
</equation>
<bodyText confidence="0.999833041666666">
Where t is the term we need to calculate its
relevancy score, frequency[t] is the numbers of
times the term t appeared in the corpus, and
frequency[stemmed_t] is the number of times
words that share the same stem of the term
appeared in the corpus. Then we sort the final list
in ascending order according to their scores.
Note that the multiple expanded queries
methodology consumes more time over the single
expanded query. This is because each expanded
query is sent to LUCENE separately. Then we
combine the returned documents lists of the
queries into a final documents list.
We also limit the maximum number of added
terms for each term in order to reduce the noise
effect of query expansion step; this maximum
number also depends on the term&apos;s relevancy
factor. We set the maximum number of added
terms to a single query to 50. Each term gets
expanded with number of terms proportional to its
relevancy score. This also increases the recall as
less frequent terms gets expanded more times than
most frequent terms, allowing LUCENE to find
more relevant pages for infrequent terms.
</bodyText>
<sectionHeader confidence="0.999816" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998317454545455">
For testing our system, we used a data set
constructed from “Zad Al Ma&apos;ad” book written by
the Islamic scholar “Ibn Al-Qyyim”. The data set
contains 25 queries and 2730 documents. Titles of
the book chapters are used as “Queries” and
sections of each chapter are used as set of relevant
documents for that query. Each query is tested
against the whole sections.
The following tables show the values of
precision, recall, f-score, and NDCG (Normalize
Discounted Cumulative Gain) of three runs.
</bodyText>
<table confidence="0.855290888888889">
R1: No expansion is used (base line).
R2: Single expanded query.
R3: Multiple expanded queries methodology.
R1 R2 R3
Precision @1 0.68 0.6 0.72
Precision @5 0.504 0.576 0.568
Precision @10 0.38 0.436 0.444
Precision @20 0.268 0.3 0.326
Precision @30 0.2038 0.232 0.2546
</table>
<tableCaption confidence="0.951141">
Table 8: Levels of Precision
</tableCaption>
<table confidence="0.9966226">
R1 R2 R3
Recall @1 0.1346 0.1067 0.1361
Recall @5 0.3258 0.35721 0.3465
Recall @10 0.3908 0.4292 0.4390
Recall @20 0.4804 0.5487 0.5393
Recall @30 0.5089 0.5806 0.5944
OR
ًحبضلاا
OR مبكحا OR مبكحأ ) - 1
OR ًحبضلأا( مبكحأ -2
</table>
<page confidence="0.960885">
91
</page>
<tableCaption confidence="0.99364">
Table 9: Levels of Recall
</tableCaption>
<table confidence="0.999771666666667">
R1 R2 R3
F-score @1 0.1919 0.1535 0.1948
F-score @5 0.3249 0.3635 0.3528
F-score @10 0.3067 0.3466 0.3516
F-score @20 0.2701 0.3122 0.3243
F-score @30 0.2334 0.2697 0.2868
</table>
<tableCaption confidence="0.960134">
Table 01: Levels of F-Score
</tableCaption>
<table confidence="0.999847">
R1 R2 R3
NDCG @1 0.68 0.6 0.72
NDCG @5 0.8053 0.8496 0.8349
NDCG @10 0.7659 0.8304 0.8316
NDCG @20 0.7392 0.7993 0.8186
NDCG @30 0.7323 0.7944 0.8001
</table>
<tableCaption confidence="0.997243">
Table 00: Levels of NDCG
</tableCaption>
<sectionHeader confidence="0.998599" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9994985">
In this paper we introduced a new technique
for semantic query expansion using a domain
independent semantic ontology constructed from
Arabic Wikipedia. We focused on four features
for semantic search: (1) Handling Generalizations.
(2) Handling Morphological Variants. (3)
Handling Concept Matches. (4) Handling
Synonyms with correct senses. We compared both
single expanded query and multiple expanded
queries approaches against the traditional keyword
based search. Both techniques showed better
results than the base line. While the Multiple
Expanded Queries approach performed better than
Single Expanded Query in most levels.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="acknowledgments">
7 ACKNOWLEDGMENT
</sectionHeader>
<bodyText confidence="0.99926125">
The authors wish to thank the anonymous
reviewers for their constructive comments and
suggestions. This work was supported by
RDI© (http://www.rdi-eg.com/)
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="references">
8 References
</sectionHeader>
<reference confidence="0.999906793103448">
David Milne Ian H. Witten David M. Nichols. 2007. A
knowledge-based search engine powered by
Wikipedia. Conference on Information and
Knowledge Management (CIKM).
Jinxi Xu, Alexander Fraser and Ralph Weischedel.
2001. Cross-lingual Retrieval at BBN. TREC10
Proceedings.
Lahsen Abouenour, Karim Bouzouba, and Paolo
Rosso. 2010. An evaluated semantic query
expansion and structure-based approach for
enhancing Arabic question/answering. International
Journal on Information and Communication
Technologies.
Leah S. Larkey and Margaret E. Connell. 2001. Arabic
Information Retrieval at UMass. TREC10
Proceedings.
Leah S. Larkey and Lisa Ballesteros and Margaret E.
Connell. 2007. Arabic Computational
Morphology Text, Speech and Language
Technology.
Musa Alkhalifa and Horacio Rodrguez. 2008.
Automatically Extending Named Entities coverage
of Arabic WordNet using Wikipedia. International
Journal on Information and Communication
Technologies.
Simone Paolo Ponzetto and Michael Strube. 2007.
Deriving a large scale taxonomy from Wikipedia.
AAAI&apos;07 Proceedings of the 22nd national
conference on Artificial intelligence.
</reference>
<page confidence="0.995991">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.207316">
<title confidence="0.998883">Semantic Query Expansion for Arabic Information Retrieval</title>
<author confidence="0.999819">Ashraf Y Mahgoub Mohsen A Rashwan</author>
<affiliation confidence="0.903938666666667">Computer Engineering Department, Cairo Electronics and Communications University, Egypt Engineering Department, Cairo University, Egypt</affiliation>
<email confidence="0.993508">ashraf.thunderstorme@gmail.commrashwan@rdi-eg.com</email>
<author confidence="0.999679">Hazem Raafat Mohamed A Zahran</author>
<affiliation confidence="0.893418">Computer Science Department, Kuwait Computer Engineering Department, Cairo University, Kuwait City, Kuwait University, Egypt</affiliation>
<title confidence="0.410351">hazem@cs.ku.edu.kw moh.a.zahran@eng.cu.edu.eg</title>
<author confidence="0.999857">Magda B Fayek</author>
<affiliation confidence="0.953709">Computer Engineering Department, University, Egypt</affiliation>
<email confidence="0.983736">magdafayek@ieee.org</email>
<abstract confidence="0.9927744">Traditional keyword based search is found to have some limitations. Such as word sense ambiguity, and the query intent ambiguity which can hurt the precision. Semantic search uses the contextual meaning of terms in addition to the semantic matching techniques in order to overcome these limitations. This paper introduces a query expansion approach using an ontology built from Wikipedia pages in addition to other thesaurus to improve search accuracy for Arabic language. Our approach outperformed the traditional keyword based approach in terms of both F-score and NDCG measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Milne Ian H Witten David M Nichols</author>
</authors>
<title>A knowledge-based search engine powered by Wikipedia.</title>
<date>2007</date>
<booktitle>Conference on Information and Knowledge Management (CIKM).</booktitle>
<marker>Nichols, 2007</marker>
<rawString>David Milne Ian H. Witten David M. Nichols. 2007. A knowledge-based search engine powered by Wikipedia. Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Alexander Fraser</author>
<author>Ralph Weischedel</author>
</authors>
<date>2001</date>
<booktitle>Cross-lingual Retrieval at BBN. TREC10 Proceedings.</booktitle>
<marker>Xu, Fraser, Weischedel, 2001</marker>
<rawString>Jinxi Xu, Alexander Fraser and Ralph Weischedel. 2001. Cross-lingual Retrieval at BBN. TREC10 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lahsen Abouenour</author>
<author>Karim Bouzouba</author>
<author>Paolo Rosso</author>
</authors>
<title>An evaluated semantic query expansion and structure-based approach for enhancing Arabic question/answering.</title>
<date>2010</date>
<journal>International Journal on Information and Communication Technologies.</journal>
<contexts>
<context position="5301" citStr="Abouenour et al., 2010" startWordPosition="759" endWordPosition="762"> documents and t is the original term. Larkey and Connell (2001) used a similar technique, but with a different scoring method. Wikipedia has been considered as an ontology source by many researchers. This is due to its large coverage, up-to-date, and domain independency. As in (Alkhalifa and Rodrguez, 2008), they proposed an automatic technique for extending Named Entities of Arabic WordNet using Wikipedia. They depended mainly on Wikipedia&apos;s “redirect” pages and Cross-Lingual links. Also a large scale taxonomy from Wikipedia deriving technique was proposed by (Pozetto and Strube, 2007). 88 (Abouenour et al., 2010) proposed a system that uses Arabic WordNet to enhance Arabic question/answering. Synonyms from WordNet are used to expand the question in order to extract the most semantically relevant passages to the question. (Milne et al., 2007) proposed a system called “KORU” for query expansion using Wikipedia&apos;s most relevant articles to user&apos;s query. The system allows the user to refine the set of Wikipedia pages to be used for expansion. KORU used “Redirect” pages for expansion; “Hyper Links” and “Disambiguation Pages” to disambiguate unrestricted text. Our proposed system differs from KORU in several</context>
</contexts>
<marker>Abouenour, Bouzouba, Rosso, 2010</marker>
<rawString>Lahsen Abouenour, Karim Bouzouba, and Paolo Rosso. 2010. An evaluated semantic query expansion and structure-based approach for enhancing Arabic question/answering. International Journal on Information and Communication Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>Margaret E Connell</author>
</authors>
<title>Arabic Information Retrieval at UMass.</title>
<date>2001</date>
<tech>TREC10 Proceedings.</tech>
<contexts>
<context position="4742" citStr="Larkey and Connell (2001)" startWordPosition="676" endWordPosition="679">have been considered by many researchers. The most successful query expansion techniques depend on automatic relevance feedback with no consideration of semantic relations. (Jinxi Xu and Ralph, 2001) used the highest TF-IDF 50 terms extracted from the top 10 retrieved documents from AFP (i.e. the TREC2001 corpus). These 50 terms where weighted due to their TF-IDF scores and added to the original query -with addition to terms from other thesaurus-with the following formula: weight(t) = oldWeight( t) + 0.4 x L TFIDF(t, D) tl t,D Where D is the top retrieved documents and t is the original term. Larkey and Connell (2001) used a similar technique, but with a different scoring method. Wikipedia has been considered as an ontology source by many researchers. This is due to its large coverage, up-to-date, and domain independency. As in (Alkhalifa and Rodrguez, 2008), they proposed an automatic technique for extending Named Entities of Arabic WordNet using Wikipedia. They depended mainly on Wikipedia&apos;s “redirect” pages and Cross-Lingual links. Also a large scale taxonomy from Wikipedia deriving technique was proposed by (Pozetto and Strube, 2007). 88 (Abouenour et al., 2010) proposed a system that uses Arabic WordN</context>
</contexts>
<marker>Larkey, Connell, 2001</marker>
<rawString>Leah S. Larkey and Margaret E. Connell. 2001. Arabic Information Retrieval at UMass. TREC10 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>Lisa Ballesteros</author>
<author>Margaret E Connell</author>
</authors>
<date>2007</date>
<booktitle>Arabic Computational Morphology Text, Speech and Language Technology.</booktitle>
<marker>Larkey, Ballesteros, Connell, 2007</marker>
<rawString>Leah S. Larkey and Lisa Ballesteros and Margaret E. Connell. 2007. Arabic Computational Morphology Text, Speech and Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Musa Alkhalifa</author>
<author>Horacio Rodrguez</author>
</authors>
<title>Automatically Extending Named Entities coverage of Arabic WordNet using Wikipedia.</title>
<date>2008</date>
<journal>International Journal on Information and Communication Technologies.</journal>
<contexts>
<context position="4987" citStr="Alkhalifa and Rodrguez, 2008" startWordPosition="714" endWordPosition="717">d from the top 10 retrieved documents from AFP (i.e. the TREC2001 corpus). These 50 terms where weighted due to their TF-IDF scores and added to the original query -with addition to terms from other thesaurus-with the following formula: weight(t) = oldWeight( t) + 0.4 x L TFIDF(t, D) tl t,D Where D is the top retrieved documents and t is the original term. Larkey and Connell (2001) used a similar technique, but with a different scoring method. Wikipedia has been considered as an ontology source by many researchers. This is due to its large coverage, up-to-date, and domain independency. As in (Alkhalifa and Rodrguez, 2008), they proposed an automatic technique for extending Named Entities of Arabic WordNet using Wikipedia. They depended mainly on Wikipedia&apos;s “redirect” pages and Cross-Lingual links. Also a large scale taxonomy from Wikipedia deriving technique was proposed by (Pozetto and Strube, 2007). 88 (Abouenour et al., 2010) proposed a system that uses Arabic WordNet to enhance Arabic question/answering. Synonyms from WordNet are used to expand the question in order to extract the most semantically relevant passages to the question. (Milne et al., 2007) proposed a system called “KORU” for query expansion </context>
</contexts>
<marker>Alkhalifa, Rodrguez, 2008</marker>
<rawString>Musa Alkhalifa and Horacio Rodrguez. 2008. Automatically Extending Named Entities coverage of Arabic WordNet using Wikipedia. International Journal on Information and Communication Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>AAAI&apos;07 Proceedings of the 22nd national conference on Artificial intelligence.</booktitle>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Deriving a large scale taxonomy from Wikipedia. AAAI&apos;07 Proceedings of the 22nd national conference on Artificial intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
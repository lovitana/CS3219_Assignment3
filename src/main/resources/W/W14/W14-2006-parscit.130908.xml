<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000421">
<title confidence="0.763783">
A Usage-Based Model of Early Grammatical Development
Barend Beekhuizen Rens Bod
</title>
<author confidence="0.510048">
LUCL ILLC
</author>
<affiliation confidence="0.999296">
Leiden University University of Amsterdam
</affiliation>
<email confidence="0.968344">
b.f.beekhuizen@hum.leidenuniv.nl l.w.m.bod@uva.nl
</email>
<author confidence="0.941922">
Afsaneh Fazly and Suzanne Stevenson
</author>
<affiliation confidence="0.998152">
Department of Computer Science
University of Toronto
</affiliation>
<email confidence="0.996335">
afsaneh,suzanne@cs.toronto.edu
</email>
<sectionHeader confidence="0.993848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952555555556">
The representations and processes yield-
ing the limited length and telegraphic style
of language production early on in acqui-
sition have received little attention in ac-
quisitional modeling. In this paper, we
present a model, starting with minimal lin-
guistic representations, that incrementally
builds up an inventory of increasingly long
and abstract grammatical representations
(form+meaning pairings), in line with the
usage-based conception of language ac-
quisition. We explore its performance on
a comprehension and a generation task,
showing that, over time, the model bet-
ter understands the processed utterances,
generates longer utterances, and better ex-
presses the situation these utterances in-
tend to refer to.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.958274396825397">
A striking aspect of language acquisition is the dif-
ference between children’s and adult’s utterances.
Simulating early grammatical production requires
a specification of the nature of the linguistic repre-
sentations underlying the short, telegraphic utter-
ances of children. In the usage-based view, young
children’s grammatical representions are thought
to be less abstract than adults’, e.g. by having
stricter constraints on what can be combined with
them (cf. Akhtar and Tomasello 1997; Bannard
et al. 2009; Ambridge et al. 2012). The represen-
tations and processes yielding the restricted length
of these early utterances, however, have received
little attention. Following Braine (1976), we adopt
the working hypothesis that the early learner’s
grammatical representations are more limited in
length (or: arity) than those of adults.
Similarly, in computational modeling of gram-
mar acquisition, comprehension has received more
Arie Verhagen
LUCL
Leiden University
a.verhagen@hum.leidenuniv.nl
attention than language generation. In this pa-
per we attempt to make the mechanisms underly-
ing early production explicit within a model that
can parse and generate utterances, and that in-
crementally learns constructions (Goldberg, 1995)
on the basis of its previous parses. The model’s
search through the hypothesis space of possible
grammatical patterns is highly restricted. Start-
ing from initially small and concrete representa-
tions, it learns incrementally long representations
(syntagmatic growth) as well as more abstract
ones (paradigmatic growth). Several models ad-
dress either paradigmatic (Alishahi and Stevenson,
2008; Chang, 2008; Bannard et al., 2009) or syn-
tagmatic (Freudenthal et al., 2010) growth. This
model aims to explain both, thereby contribut-
ing to the understanding of how different learning
mechanisms interact. As opposed to other models
involving grammars with semantic representations
(Alishahi and Stevenson, 2008; Chang, 2008), but
similar to Kwiatkowski et al. (2012), the model
starts without an inventory of mappings of single
words to meanings.
Based on motivation from usage-based and con-
struction grammar approaches, we define several
learning principles that allow the model to build
up an inventory of linguistic representations. The
model incrementally processes pairs of an utter-
ance U, consisting of a string of words wi ... wn,
and a set of situations S, one of which is the situa-
tion the speaker intends to refer to. The other situ-
ations contribute to propositional uncertainty (the
uncertainty over which proposition the speaker is
trying to express; Siskind 1996). The model tries
to identify the intended situation and to understand
how parts of the utterance refer to certain parts of
that situation. To do so, the model uses its growing
inventory of linguistic representations (Section 2)
to analyze U, producing a set of structured seman-
tic analyses or parses (Fig. 1, arrow 1; Section 3).
</bodyText>
<page confidence="0.994056">
46
</page>
<note confidence="0.7874085">
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 46–54,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999947928571429">
The resulting best parse, U and the selected situa-
tion are then stored in a memory buffer (arrow 2),
which is used to learn new constructions (arrow
3) using several learning mechanisms (Section 4).
The learned constructions can then be used to gen-
erate utterances as well. We describe two experi-
ments: in the comprehension experiment (Section
5), we evaluate the model’s ability to parse the
stream of input items. In the generation experi-
ment (Section 6), the model generates utterances
on the basis of a given situation and its linguistic
knowledge. We evaluate the generated utterances
given different amounts of training items to con-
sider the development of the model over time.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="introduction">
2 Representations
</sectionHeader>
<bodyText confidence="0.999855857142857">
We represent linguistic knowledge as construc-
tions: pairings of a signifying form and a signi-
fied (possibly incomplete) semantic representation
(Goldberg, 1995). The meaning is represented as
a graph with the nodes denoting entities, events,
and their relations, connected by directed unla-
beled edges. The conceptual content of each node
is given by a set of semantic features. We assume
that meaning representations are rooted trees. The
signifying form consists of a positive number of
constituents. Every constituent has two elements:
a phonological form, and a pointer to a node in the
signified meaning (in line with Verhagen 2009).
Both can be specified, or one can be left empty.
Constituents with unspecified phonological forms
are called open, denoted with E in the figures. The
head constituent of a construction is defined as
the constituent that has a pointer to the root node
of the signified meaning. We furthermore require
that no two constituents point to the same node of
the signified meaning.
This definition generalizes over lexical ele-
ments (one phonologically specified constituent)
as well as larger linguistic patterns. Fig. 2, for in-
stance, shows two larger constructions being com-
bined with each other. We call the set of construc-
tions the learner has at some moment in time the
constructicon C (cf. Goldberg 2003).
</bodyText>
<sectionHeader confidence="0.998744" genericHeader="method">
3 Parsing
</sectionHeader>
<subsectionHeader confidence="0.99998">
3.1 Parsing operations
</subsectionHeader>
<bodyText confidence="0.994748">
We first define a derivation d as an assembly
of constructions in C, using four parsing opera-
tions defined below. In parsing, derivations are
constrained by the utterance U and the situations
</bodyText>
<figureCaption confidence="0.99961">
Figure 1: The global flow of the model
</figureCaption>
<bodyText confidence="0.99993835">
S, whereas in production, only a situation s con-
strains the derivation. The leaf nodes of a deriva-
tion must consist of phonological constraints of
constructions that (in parsing) are satisfied by U.
All constructions used in a derivation must map to
the same situation s E S. A construction c maps to
s iff the meaning of c constitutes a subgraph of s,
with the features on each of the nodes in the mean-
ing of c being a subset of the features on the corre-
sponding node of s. Moreover, each construction
must map to a different part of s. This constitutes
a mutual exclusivity effect in analyzing U: every
part of the analysis must contribute to the compos-
ite meaning. A derivation d thus gives us a map-
ping between the composed meaning of all con-
structions used in d and one situation s E S. The
aggregate mapping specifies a subgraph of s that
constitutes the interpretation of that derivation.
The central parsing operation is the COMBINA-
TION operator o. In cz o cj, the leftmost open con-
stituent of cz is combined with cj. Fig. 2 illus-
trates COMBINATION. COMBINATION succeeds if
both the semantic pointer of the leftmost open con-
stituent of cz and the semantic pointer of the head
constituent of cj map to the same semantic node
of a situation s
Initially, the model has few constructions to an-
alyze the utterance with. Therefore, we define
three other operations that allow the model to cre-
ate a derivation over the full utterance without
combining constructions. First, a known or un-
known word that cannot be fit into a derivation,
can be IGNOREd. Second, an unknown word can
be used to fill an open constituent slot of a con-
struction with the BOOTSTRAP operator. Boot-
strapping entails that the unknown word will be
associated with the semantics of the node. Finally,
the learner can CONCATENATE multiple deriva-
tions, by linearly sequencing them, thus creating a
more complex derivation without combining con-
</bodyText>
<figure confidence="0.997063130434783">
analysis
1
1
2
(utterance, intended situation, analysis)
(utterance, intended situation, analysis)
3
...
(utterance, intended situation, analysis)
memory buffer
construction 1
construction 2
construction 3
...
construction n
constructicon
utterance
situation 1
situation 2
...
situation n
situations
input item
</figure>
<page confidence="0.56207">
47
</page>
<figure confidence="0.997080936170213">
PHON: Adam
SEM:
PHON: ɛ
SEM:
{object,entity}
{surface}
{move}
{patient,moved}
{location,goal}
PHON: put
SEM:
PHON: it
SEM:
=
{animate,Adam}
{move}
{agent,mover}
{location,goal}
situation
{move}
{agent,mover}
{patient,moved}
{location,goal}
{animate,Adam}
{object,entity}
{surface}
{move}
{move}
{agent,mover}
{location,goal}
{patient,moved}
{location,goal}
O
{animate,Adam}
{object,entity}
{surface}
PHON: Adam
PHON: ɛ
PHON: put
PHON: it
SEM:
SEM:
SEM:
SEM:
Leftmost open constituent of
this construction, pointing to
{move}-node of meaning
</figure>
<figureCaption confidence="0.997668">
Figure 2: Combining constructions. The dashed lines represent semantic pointers, either from con-
stituents to the constructional meaning (black) or from the constructions to the situation (red and blue).
</figureCaption>
<figure confidence="0.991393692307692">
meaning
c1
c2
meaning
concatenate
bootstrap
ignore
c1
4
w1
w2
W3
meaning
</figure>
<bodyText confidence="0.979537">
made, sidentified is selected at random from S.
The probability of a parse p is given by the sum
of the probabilities of the derivations d subsumed
under that parse, which in turn are defined as the
product of the probabilities of the constructions c
used in d.
</bodyText>
<figureCaption confidence="0.7546425">
Figure 3: The CONCATENATE, IGNORE and � P(d) (1)
BOOTSTRAP operators (internal details of the con- P(p) = P(c) (2)
structions left out). dEp
�
P(d) =
cEd
</figureCaption>
<bodyText confidence="0.999893142857143">
structions. This allows the learner to interpret a
larger part of the situation than with COMBINA-
TION only. The resulting sequences may be ana-
lyzed in the learning process as constituting one
larger construction, consisting of the parts of the
concatenated derivations. Fig. 3 illustrates these
three operations.
</bodyText>
<subsectionHeader confidence="0.999951">
3.2 Selecting the best analysis
</subsectionHeader>
<bodyText confidence="0.998993705882353">
Multiple derivations can be highly similar in the
way they map parts of U to parts of an s ∈ S. We
define a parse to be a set of derivations that have
the same internal structure and the same mappings
to a situation, but that use different constructions
in doing so (cf. multiple licensing; Kay 2002). We
take the most probable parse of U to be the best
analysis of U. The most probable parse points to a
situation, which the model then assumes to be the
identified situation or sidentified. If no parse can be
The probability of a construction P(c) is given
by its relative frequency (count) in the construc-
ticon C, smoothed with Laplace smoothing. We
assume that the simple parsing operations of IG-
NORE, BOOTSTRAP, and CONCATENATION reflect
usages of an unseen construction with a count of
0.
</bodyText>
<equation confidence="0.980201666666667">
c.count + 1
P(c) = E cf.count + |C |+ 1 (3)
c&apos;EC
</equation>
<bodyText confidence="0.999939">
The most probable parse, U and sidentified are
added to the memory buffer. The memory buffer
has a pre-set maximal length, discarding the oldest
exemplars upon reaching this length. In the future,
we plan to consider more realistic mechanisms for
the memory buffer, such as graceful degradation,
and attention effects.
</bodyText>
<page confidence="0.998427">
48
</page>
<sectionHeader confidence="0.979984" genericHeader="method">
4 Learning mechanisms
</sectionHeader>
<bodyText confidence="0.9999973">
The model uses the best parse of the utterance to
develop its knowledge of the constructions in the
constructicon C. Two simple operations, UPDATE
and ASSOCIATION, are used to create initial con-
structions and reinforce existing ones respectively.
Two additional operations, PARADIGMATIZATION
and SYNTAGMATIZATION, are key to the model’s
ability to extend these initial representations by
inducing novel constructions that are richer and
more abstract than existing ones.
</bodyText>
<subsectionHeader confidence="0.998248">
4.1 Direct learning from the best parse
</subsectionHeader>
<bodyText confidence="0.99998452173913">
The best parse is used to UPDATE C. For this
mechanism, the model uses the concrete mean-
ing of sidentified rather than the (potentially more
abstract) meaning of the constructions in the best
parse.1 Every construction in the parse is assigned
the subgraph of sidentified it maps to as its new
meaning, and the count of the adjusted construc-
tion is incremented with 1, or added to C with a
count of 1, if it does not yet exist. This includes
applications of the BOOTSTRAP operation, creat-
ing a mapping of the previously unknown word to
a situational meaning.
ASSOCIATE constitutes a form of simple cross-
situational learning over the memory buffer. The
intuition is that co-occurring word sequences
and meaning components that remain unanalyzed
across multiple parses might themselves comprise
the form-meaning pairing of a construction. If the
unanalyzed parts of two situations contain an over-
lapping subgraph, and the unanalyzed parts of two
utterances an overlapping subsequence of words,
the two are mapped to each other and added to C
with a count of 0.
</bodyText>
<subsectionHeader confidence="0.98413">
4.2 Qualitative extension of the best parse
</subsectionHeader>
<bodyText confidence="0.9999068">
Syntagmatization Some of the processes de-
scribed thus far yield analyses of the input in
which constructions are linearly associated but
lack appropriate relational structure among them.
The model requires a process, which we call SYN-
TAGMATIZATION, that enables it to induce further
hierarchical structure.
In order for the learner to acquire constructions
in which the different constituents point to differ-
ent parts of the construction’s meaning, the ASSO-
</bodyText>
<footnote confidence="0.913789">
1This follows Langacker’s (2009) claim that the processed
concrete usage events should leave traces in the learner’s
mind.
</footnote>
<bodyText confidence="0.999663254901961">
CIATE operation does not suffice. We assume that
the learner is able to learn such constructions by
using concatenated derivations. The process we
propose is SYNTAGMATIZATION. In this process,
the various concatenated derivations are taken as
constituents of a novel construction. This instanti-
ates the idea that joint processing of two (or more)
events gradually leads to a joint representation of
these, previously independent, events.
More precisely, the process starts by taking the
top nodes T of the derivations in the best parse,
where T consists of the single top node if no CON-
CATENATION has been applied, or the set of con-
catenated nodes of the parse tree if CONCATENA-
TION has been applied (e.g. for the derivation in
Fig. 3, |T |= 2). For each top node t ∈ T, we take
the root node of the construction’s meaning, and
define its semantic frame to consist of all children
(roles) and grandchildren (role-fillers) of the node
in the situation it maps to. The model then forms a
novel construction csyn by taking all the construc-
tions in the parse whose semantic root nodes point
to a node in this semantic frame, referring to those
as the set R of semantically related constructions.
As the novel meaning of csyn, the model takes the
subgraph of the situation mapped to by the joint
mapping of all constructional meanings of con-
structions in R.
R, as well as all phonologically specified con-
stituents of t itself, are then linearized as the con-
stituents of csyn. The novel construction thus con-
stitutes a construction with a higher arity, ‘joining’
several previously independent constructions. Fig.
4 illustrates the syntagmatization mechanism.
Paradigmatization Due to our usage-driven ap-
proach, all learning mechanisms so far give us
maximally concrete constructions. In order for the
model to generalize beyond the observed input,
some degree of abstraction is needed. The model
does so with the PARADIGMATIZATION mecha-
nism. This mechanism recursively looks for min-
imal abstractions (cf. Tomasello 2003, 123) over
the constructions in C and adds those to C, thus
creating a full-inheritance network (cf. Langacker
1989, 63-76).
An abstraction over a set of constructions is
made if there is an overlapping subgraph between
the meanings of the constructions, where every
node of the subgraph is the non-empty feature
set intersection between two mapped nodes of the
constructional meanings. Furthermore, the con-
</bodyText>
<page confidence="0.992607">
49
</page>
<figure confidence="0.999830138888889">
{volitional,...}
{animate,
hearer}
PHON: you
SEM
{volitional,...} {independent-
exist}
{act}
{independent-
exist}
PHON: ɛ
{act,move}
PHON: take
SEM
SEM
concatenate
A derivation over the utterance you take it.
{object,entity,
ball}
PHON: ball
SEM
PHON: you
SEM
{volitional,...}
{animate,
hearer}
PHON: take
SEM
{act}
A novel, syntagmatized construction
{object,entity,ba
ll}
{independent-
exist}
PHON: ball
SEM
</figure>
<figureCaption confidence="0.905868">
Figure 4: The SYNTAGMATIZATION mechanism. The mechanism takes a derivation as its input and
reinterprets it as a novel construction of higher arity).
</figureCaption>
<bodyText confidence="0.999925">
stituents must be mappable: both constructions
have the same number of constituents and the
paired constituents point to a mapped node of the
meaning. The meaning of the abstracted construc-
tion is then set to this overlapping subgraph, which
is the lowest possible semantic abstraction over
the constructions. The constituents of this new ab-
straction have a specified phonological form if the
more concrete constructions share the same word,
and an unspecified one otherwise. The count of an
abstracted construction is given by the cardinality
of the set of its direct descendants in the network.
This generalizes Bybee’s (1995) idea about type
frequency as a proxy for productivity to a network
structure. Fig. 5 illustrates the paradigmatization
mechanism.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="method">
5 Experimental set-up
</sectionHeader>
<bodyText confidence="0.999983">
The model is incrementally presented with U, S
pairings based on Alishahi &amp; Stevenson’s (2010)
generation procedure. In this procedure, an utter-
ance and a semantic frame expressing its meaning
(a situation) are generated. The generation pro-
cedure follows distributions occurring in a corpus
of child-directed speech. As we are interested in
the performance of the model under propositional
uncertainty, we add a parametrized number of ran-
domly sampled situations, so that S consists of the
situation the speaker intends to refer to (scorrect)
and a number of situations the speaker does not
intend to refer to.2 Here, we set the number of ad-
</bodyText>
<footnote confidence="0.8215445">
2We are currently researching the effects of sampling non-
correct situations that have a greater likelihood of overlap
</footnote>
<bodyText confidence="0.968386411764706">
ditional situations to be 1 or 5; the other parameter
of the model, the size of the memory buffer, is set
to 5 exemplars.
For the comprehension experiment, we eval-
uate the model’s performance parsing the input
items, averaging over every 50 U, S pairs. We
track the ability to identify the intended situation
from S. Identification succeeds if the best parse
maps to scorrect, i.e. if sidentified = scorrect. Next,
situation coverage expresses what proportion of
sidentified has been interpreted and thus how rich the
meanings of the used constructions are. It is de-
fined as the number of nodes of the interpretation
of the best parse, divided by the number of nodes
of sidentified. Finally, utterance coverage tells us
what proportion of U has been parsed with con-
structions (excluding IGNORED; including BOOT-
STRAPPED words). The measure expresses the
proportion of the signal that the learner (correctly
or incorrectly) is able to interpret.
For exploring language production, the model
receives a situation, and (given the constructicon)
finds the most probable, maximally expressive,
fully lexicalized derivation expressing it. That is:
among all derivations terminating in phonologi-
cally specified constituents, it selects the deriva-
tions that cover the most semantic nodes of the
given situation. In the case of multiple such
derivations, it selects the most probable one, fol-
lowing the probability model in Section 3. We
only allow for the COMBINATION operator in the
derivations, as BOOTSTRAPPING and IGNORE re-
with the intended situation, to reflect more realistic input (cf.
Siskind 1996).
</bodyText>
<page confidence="0.979613">
50
</page>
<figure confidence="0.999792257142857">
PHON: you
{animate,hearer}
SEM
{volitional,...}
PHON: take
PHON: you
{cause,move}
SEM
{animate,hearer}
SEM
{volitional,...}
{location,entity,chair}
{patient,...}
PHON: chair
PHON: take
SEM
SEM
{cause,move}
{location,entity}
{patient,...}
PHON: ɛ
SEM SEM SEM
PHON: you PHON: take PHON: table
SEM
{animate,hearer}
{volitional,...}
{cause,move}
{location,entity,table}
The set intersection of
{location,entity,chair} and
{location,entity, table}
A phonologically empty
constituent, generalizing
over chair and table
{patient,...}
</figure>
<figureCaption confidence="0.9192815">
Figure 5: The PARADIGMATIZATION mechanism. The construction on top is an abstraction obtained
over the two constructions at the bottom.
</figureCaption>
<bodyText confidence="0.999910666666667">
fer to words in a given U, and CONCATENATE is a
back-off method for analyzing more of U than the
constructicon allows for. The situations used in the
generation experiment do not occur in the training
items, so that we truly measure the model’s ability
to generate utterances for novel situations.
The phonologically specified leaf nodes of the
best derivation constitute the generated utterance
Ugen. Ugen is evaluated on the basis of its mean
length, in number of words, its situation cover-
age, as defined in the comprehension experiment,
and its utterance precision and utterance recall.
To calculate these, we take the maximally overlap-
ping subsequence Uoverlap between the actual utter-
ance Uact associated with the situation and Ugen.
Utterance precision (how many words are gener-
ated correctly) and utterance recall (how many of
the correct words are generated) are defined as:
</bodyText>
<equation confidence="0.9533326">
|Uoverlap|
Utterance precision = (4)
|Ugen|
Utterance recall = |Uoverlap |(5)
|Uact|
</equation>
<bodyText confidence="0.99346175">
Because the U, S-pairs on which the model was
trained, are generated randomly, we show results
for comprehension and production averaged over
5 simulations.
</bodyText>
<sectionHeader confidence="0.998703" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999852">
A central motivation for the development of this
model is to account for early grammatical produc-
tion: can we simulate the developmental pattern
of the growth of utterance length and a growing
potential for generalization? The same construc-
tions underlying these productions should, at the
same time, also account for the learner’s increas-
ing grasp of the meaning of U. To explore the
model’s performance in both domains, we present
a comprehension and a generation experiment.
</bodyText>
<subsectionHeader confidence="0.999293">
6.1 Comprehension results
</subsectionHeader>
<bodyText confidence="0.999902941176471">
Fig. 6a gives us the results over time of the com-
prehension measures given a propositional un-
certainty of 1, i.e. one situation besides scorrect
in S. Overall, the model understands the utter-
ances increasingly well. After 2000 input items,
the model identifies scorrect in 95% of the cases.
With higher levels of propositional uncertainty
(not shown here), performance is still relatively
robust: given 5 incorrect situations in S, scorrect
is identified in 62% of all cases (random guess-
ing gives a score of 17%, or s). Similarly, the
proportion of the situation interpreted and the pro-
portion of the utterance analyzed go up over time.
This means that the model builds up an increasing
repertoire of constructions that allow it to analyze
larger parts of the utterance and the situations it
identifies. It is important to realize that these mea-
</bodyText>
<page confidence="0.991398">
51
</page>
<figure confidence="0.997078590909091">
utterance length in words
measures
situation coverage
utterance coverage
identification
0 500 1000 1500 2000
time
(a) Comprehension results over time
time
(b) Length of Ugen over time
measures
situation coverage
utterance precision
utterance recall
0 500 1000 1500 2000
0.0 0.5 1.0 1.5 2.0 2.5 3.0
proportion 0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
proportion
0 500 1000 1500 2000
time
(c) Generation results over time
</figure>
<figureCaption confidence="0.99977">
Figure 6: Quantitative results for the comprehension and generation experiments
</figureCaption>
<bodyText confidence="0.773436">
sures do not display what proportion of the utter-
ance or situation is analyzed correctly.
</bodyText>
<subsectionHeader confidence="0.999333">
6.2 Generation results
</subsectionHeader>
<bodyText confidence="0.999888787878788">
Quantitative results Fig. 6b shows that the av-
erage utterance length increases over time. This
indicates that the number of constituents of the
used constructions grows. Next, Fig. 6c shows the
performance of the model on the generation task.
After 2000 input items, the model generates pro-
ductions expressing 93% of the situation, with an
utterance precision of 0.91, and an utterance recall
of 0.81. Given a propositional uncertainty of 5,
these go down to 79%, 0.76 and 0.59 respectively.
Comparing the utterance precision and recall
over time, we can see that the utterance preci-
sion is high from the start, whereas the recall
gradually increases. This is in line with the ob-
servation that children predominantly produce er-
rors of omission (leaving linguistic material out an
adult speaker would produce), and few errors of
comission (producing linguistic material an adult
speaker would not produce).
Qualitative results Tracking individual produc-
tions given specific situations over time allows us
to study in detail what the model is doing. Here,
we look at one case qualitatively. Given the sit-
uation for which the Uact is she put them away,
the model generates, over time, the utterances in
Table 1. The brackets show the internal hierarchi-
cal structure of the derivation. This development
illustrates several interesting aspects of the model.
First, as discussed earlier, the model mostly makes
errors of omission: earlier productions leave out
more words found in the adult utterances. Only at
t = 550, the model makes an error of commission,
using the word in erroneously.
</bodyText>
<equation confidence="0.513446">
t 50 500 550 600 950 1000 1050 1400
</equation>
<tableCaption confidence="0.995419">
Table 1: Generations over time t for one situation.
</tableCaption>
<bodyText confidence="0.999869037037037">
Starting from t = 600 (except at t = 950),
the model generates the correct utterance, but the
derivations leading to this production differ. At
t = 550, for instance, the learner combines a
completely non-phonologically specific construc-
tion for which the constituents refer to the agent,
action and goal location, with three ‘lexical’ con-
structions that fill in the words for those items..
The constructions used after t = 550 are all more
specific, combining 3, or even only 2 constructions
(t ≥ 1400) where the entire sequence of words
“put them away” arises from a single construction.
Using less abstract constructions over time
seems contrary to the usage-based idea that con-
structions become more abstract over the course of
acquisition. However, this result follows from the
way the probability model is defined. More spe-
cific constructions that are able to account for the
input will entail fewer combinations, and a deriva-
tion with fewer combination operations will often
be more likely than one with more such opera-
tions. Given equal expressivity of the situation,
the former derivation will be selected over the lat-
ter in generation.
The effect is indeed in line with another concept
hypothesized to play a role in language acquisition
on a usage-based account, viz. pre-emption (Gold-
</bodyText>
<figure confidence="0.997545">
[[she] put them [away]]
[[she] put]
[[she] [put] [in]]
[[she] put [them]]
[[she] put them away]
[[she] put them [away]]
[she [put]]
[[she] put [them] away]
</figure>
<page confidence="0.971616">
52
</page>
<table confidence="0.995002545454545">
{act} {rest,act}
{volitional,...} {indepen-
{volitional {destination,
, ...} location}
{animate, dent-exist}
{animate,...} {location,entity}
hearer} {object,artefact}
SEM SEM SEM
SEM SEM SEM
PHON:ɛ PHON:ɛ PHON:ɛ
PHON: you PHON PHON: it
</table>
<figureCaption confidence="0.996776">
Figure 7: Some representations at t = 2000
</figureCaption>
<bodyText confidence="0.999939098039216">
berg, 2006, 94-95). Pre-emption is the effect that
a language user will select a more concrete rep-
resentation over the combination of more abstract
ones. The effect can be reconceptualized in this
model as an epiphenomenon of the way the prob-
ability model works: simply because combining
fewer constructions in a derivation is often more
probable than combining more constructions, the
former derivation will be selected over the lat-
ter. Pre-emption is typically invoked to explain the
blocking of overgeneralization patterns, and an in-
teresting future step will be to see if the model can
simulate developmental patterns for well-known
cases of overgeneralization errors.
The potential for abstraction The paradigma-
tization operation allows the model to go beyond
observed concrete instances of form-meaning
pairings: without it, unseen situations could never
be fully expressed. Despite this potential, we have
seen that the model relies on highly concrete con-
structions. The concreteness of the used patterns,
however, does not imply the absence of more ab-
stract representations. Fig. 7 gives three exam-
ples of constructions in C in one simulation. Con-
struction (a) could be seen as a verb-island con-
struction (Tomasello, 1992, 23-24). The second
constituent is phonologically specified with put,
and the other arguments are open, but mapped to
specific semantic functions. This pattern allows
for the expression of many caused-motion events.
Construction (b) is the inverse of (a): the argu-
ments are phonologically specified, but the verb-
slot is open. This would be a case of a pronominal
argument frame [you V it], which have been found
to be helpful in the bootstrapping of verbal mean-
ings (Tomasello, 2001). Finally, (c) presents a case
of full abstraction. This construction licenses ut-
terances such as I sit here, you stay there and er-
roneous ones like he sits on (which, again, will be
pre-empted in the generation of utterances if more
concrete constructions licence he sits on it).
Summarizing, abstract constructions are ac-
quired, but only used for those cases in which no
concrete construction is available. This is in line
with the usage-based hypotheses that abstract con-
structions do emerge, but that for much of lan-
guage production, a language user can rely on
highly concrete patterns. A next step will be
to measure the development of abstractness and
length over the constructions themselves, rather
than the parses and generations they allow.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999989923076923">
This, admittedly complex, model forms an attempt
to model different learning mechanisms in interac-
tion from a usage-based constructionist perspec-
tive. Starting with an empty set of linguistic rep-
resentations, the model acquires words and gram-
matical constructions simultaneously. The learn-
ing mechanisms allow the model to build up in-
creasingly abstract, as well as increasingly long
constructions. With these developing representa-
tions, we showed how the model gets better over
time at understanding the input item, performing
relatively robustly under propositional uncertainty.
Moreover, in the generation experiment, the
model shows patterns of production (increasingly
long utterances) similar to those of children. An
important future step will be to look at these pro-
ductions more closely and investigate if they also
converge on more detailed patterns of develop-
ment in the production of children (e.g. item-
specificity, as hypothesized on the usage-based
view). Despite highly concrete constructions suf-
ficing for most of production, inspection of the ac-
quired representations tells us that more abstract
constructions are learned as well. Here, an inter-
esting next step would be to simulate patterns of
overgeneralization in children’s production.
</bodyText>
<sectionHeader confidence="0.997513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997912">
We would like to thank three anonymous review-
ers for their valuable and thoughtful comments.
We gratefully acknowledge the funding of BB
through NWO of the Netherlands (322.70.001)
and AF and SS through NSERC of Canada.
</bodyText>
<figure confidence="0.999811409090909">
(a)
(b) (c)
PHON: ɛ
SEM
{volitional,...,,
cause-location}
{animate,..}
PHON: put
SEM
{object,artefact}
{cause,move}
{affected,...,
stationary}
PHON: ɛ
SEM
{object, artefact}
PHON: ɛ
{location,
destination}
SEM
PHON: ɛ
SEM
</figure>
<page confidence="0.99432">
53
</page>
<sectionHeader confidence="0.995365" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993262179487179">
Nameera Akhtar and Michael Tomasello. 1997.
Young Children’s Productivity With Word Or-
der and Verb Morphology. Developmental Psy-
chology, 33(6):952–965.
Afra Alishahi and Suzanne Stevenson. 2008 A
Computational Model of Early Argument Struc-
ture Acquisition. Cognitive Science, 32(5):789–
834.
Afra Alishahi and Suzanne Stevenson. 2010. A
computational model of learning semantic roles
from child-directed language. Language and
Cognitive Processes, 25(1):50–93.
Ben Ambridge, Julian M Pine, and Caroline F
Rowland. 2012. Semantics versus statistics in
the retreat from locative overgeneralization er-
rors. Cognition, 123(2):260–79.
Colin Bannard, Elena Lieven, and Michael
Tomasello. 2009. Modeling children’s early
grammatical knowledge. Proceedings of the
National Academy of Sciences of the United
States ofAmerica, 106(41):17284–9.
Martin D.S. Braine. 1976. Children’s first word
combinations. University of Chicago Press,
Chicago, IL.
Joan Bybee. 1995. Regular morphology and the
lexicon. Language and Cognitive Processes, 10
(5):425–455.
Nancy C.-L. Chang. 2008. Constructing Gram-
mar: A computational model of the emergence
of early constructions. Dissertation, University
of California, Berkeley.
Daniel Freudenthal, Julian Pine, and Fernand Go-
bet. 2010. Explaining quantitative variation in
the rate of Optional Infinitive errors across lan-
guages: a comparison of MOSAIC and the Vari-
ational Learning Model. Journal of Child Lan-
guage, 37(3):643–69.
Adele E. Goldberg. 1995. Constructions. A
Construction Grammar Approach to Argument
Structure. Chicago University Press, Chicago,
IL.
Adele E Goldberg. 2003. Constructions: a new
theoretical approach to language. Trends in
Cognitive Sciences, 7(5):219–224.
Adele E. Goldberg. 2006. Constructions at Work.
The Nature of Generalization in Language. Ox-
ford University Press, Oxford.
Paul Kay. 2002. An Informal Sketch of a Formal
Architecture for Construction Grammar. Gram-
mars, 5:1–19.
Tom Kwiatkowski, Sharon Goldwater, Luke
Zettlemoyer, and Mark Steedman. 2012. A
Probabilistic Model of Syntactic and Seman-
tic Acquisition from Child-Directed Utterances
and their Meanings. In Proceedings EACL.
Ronald W. Langacker. 1989. Foundations of Cog-
nitive Grammar, Volume I. Stanford University
Press.
Ronald W. Langacker. 2009. A dynamic view of
usage and language acquisition. Cognitive Lin-
guistics, 20(3):627–640.
Jeffrey M Siskind. 1996. A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):39–
91.
Michael Tomasello. 1992. First Verbs: A study
of early grammatical development. Cambridge
University Press, Cambridge, UK.
Michael Tomasello. 2001 Perceiving intentions
and learning words in the second year of life.
In Melissa Bowerman and Stephen C. Levinson,
editors, Language Acquisition and Conceptual
Development, chapter 5, pages 132–158. Cam-
bridge University Press, Cambridge, UK.
Michael Tomasello. 2003. Constructing a lan-
guage: A Usage-Based Theory of Language
Acquisition. Harvard University Press, Cam-
bridge, MA.
</reference>
<bodyText confidence="0.8771785">
Arie Verhagen. 2009 The conception of construc-
tions as complex signs. Emergence of struc-
ture and reduction to usage. Constructions and
Frames, 1:119–152.
</bodyText>
<page confidence="0.998161">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.561267">
<title confidence="0.999837">A Usage-Based Model of Early Grammatical Development</title>
<author confidence="0.978721">Barend Beekhuizen Rens Bod</author>
<affiliation confidence="0.9938735">LUCL ILLC Leiden University University of Amsterdam</affiliation>
<email confidence="0.917894">b.f.beekhuizen@hum.leidenuniv.nll.w.m.bod@uva.nl</email>
<author confidence="0.731477">Fazly</author>
<affiliation confidence="0.9996695">Department of Computer University of</affiliation>
<email confidence="0.998012">afsaneh,suzanne@cs.toronto.edu</email>
<abstract confidence="0.991974">The representations and processes yielding the limited length and telegraphic style of language production early on in acquisition have received little attention in acquisitional modeling. In this paper, we present a model, starting with minimal linguistic representations, that incrementally builds up an inventory of increasingly long and abstract grammatical representations (form+meaning pairings), in line with the usage-based conception of language acquisition. We explore its performance on a comprehension and a generation task, showing that, over time, the model better understands the processed utterances, generates longer utterances, and better expresses the situation these utterances intend to refer to.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nameera Akhtar</author>
<author>Michael Tomasello</author>
</authors>
<title>Young Children’s Productivity With Word Order and Verb Morphology.</title>
<date>1997</date>
<journal>Developmental Psychology,</journal>
<volume>33</volume>
<issue>6</issue>
<contexts>
<context position="1532" citStr="Akhtar and Tomasello 1997" startWordPosition="208" endWordPosition="211">rocessed utterances, generates longer utterances, and better expresses the situation these utterances intend to refer to. 1 Introduction A striking aspect of language acquisition is the difference between children’s and adult’s utterances. Simulating early grammatical production requires a specification of the nature of the linguistic representations underlying the short, telegraphic utterances of children. In the usage-based view, young children’s grammatical representions are thought to be less abstract than adults’, e.g. by having stricter constraints on what can be combined with them (cf. Akhtar and Tomasello 1997; Bannard et al. 2009; Ambridge et al. 2012). The representations and processes yielding the restricted length of these early utterances, however, have received little attention. Following Braine (1976), we adopt the working hypothesis that the early learner’s grammatical representations are more limited in length (or: arity) than those of adults. Similarly, in computational modeling of grammar acquisition, comprehension has received more Arie Verhagen LUCL Leiden University a.verhagen@hum.leidenuniv.nl attention than language generation. In this paper we attempt to make the mechanisms underly</context>
</contexts>
<marker>Akhtar, Tomasello, 1997</marker>
<rawString>Nameera Akhtar and Michael Tomasello. 1997. Young Children’s Productivity With Word Order and Verb Morphology. Developmental Psychology, 33(6):952–965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afra Alishahi</author>
<author>Suzanne Stevenson</author>
</authors>
<date>2008</date>
<journal>A Computational Model of Early Argument Structure Acquisition. Cognitive Science,</journal>
<volume>32</volume>
<issue>5</issue>
<pages>834</pages>
<contexts>
<context position="2669" citStr="Alishahi and Stevenson, 2008" startWordPosition="368" endWordPosition="371">attention than language generation. In this paper we attempt to make the mechanisms underlying early production explicit within a model that can parse and generate utterances, and that incrementally learns constructions (Goldberg, 1995) on the basis of its previous parses. The model’s search through the hypothesis space of possible grammatical patterns is highly restricted. Starting from initially small and concrete representations, it learns incrementally long representations (syntagmatic growth) as well as more abstract ones (paradigmatic growth). Several models address either paradigmatic (Alishahi and Stevenson, 2008; Chang, 2008; Bannard et al., 2009) or syntagmatic (Freudenthal et al., 2010) growth. This model aims to explain both, thereby contributing to the understanding of how different learning mechanisms interact. As opposed to other models involving grammars with semantic representations (Alishahi and Stevenson, 2008; Chang, 2008), but similar to Kwiatkowski et al. (2012), the model starts without an inventory of mappings of single words to meanings. Based on motivation from usage-based and construction grammar approaches, we define several learning principles that allow the model to build up an i</context>
</contexts>
<marker>Alishahi, Stevenson, 2008</marker>
<rawString>Afra Alishahi and Suzanne Stevenson. 2008 A Computational Model of Early Argument Structure Acquisition. Cognitive Science, 32(5):789– 834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afra Alishahi</author>
<author>Suzanne Stevenson</author>
</authors>
<title>A computational model of learning semantic roles from child-directed language. Language and Cognitive</title>
<date>2010</date>
<booktitle>Processes,</booktitle>
<pages>25--1</pages>
<marker>Alishahi, Stevenson, 2010</marker>
<rawString>Afra Alishahi and Suzanne Stevenson. 2010. A computational model of learning semantic roles from child-directed language. Language and Cognitive Processes, 25(1):50–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Ambridge</author>
<author>Julian M Pine</author>
<author>Caroline F Rowland</author>
</authors>
<title>Semantics versus statistics in the retreat from locative overgeneralization errors.</title>
<date>2012</date>
<journal>Cognition,</journal>
<volume>123</volume>
<issue>2</issue>
<contexts>
<context position="1576" citStr="Ambridge et al. 2012" startWordPosition="216" endWordPosition="219">, and better expresses the situation these utterances intend to refer to. 1 Introduction A striking aspect of language acquisition is the difference between children’s and adult’s utterances. Simulating early grammatical production requires a specification of the nature of the linguistic representations underlying the short, telegraphic utterances of children. In the usage-based view, young children’s grammatical representions are thought to be less abstract than adults’, e.g. by having stricter constraints on what can be combined with them (cf. Akhtar and Tomasello 1997; Bannard et al. 2009; Ambridge et al. 2012). The representations and processes yielding the restricted length of these early utterances, however, have received little attention. Following Braine (1976), we adopt the working hypothesis that the early learner’s grammatical representations are more limited in length (or: arity) than those of adults. Similarly, in computational modeling of grammar acquisition, comprehension has received more Arie Verhagen LUCL Leiden University a.verhagen@hum.leidenuniv.nl attention than language generation. In this paper we attempt to make the mechanisms underlying early production explicit within a model</context>
</contexts>
<marker>Ambridge, Pine, Rowland, 2012</marker>
<rawString>Ben Ambridge, Julian M Pine, and Caroline F Rowland. 2012. Semantics versus statistics in the retreat from locative overgeneralization errors. Cognition, 123(2):260–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Elena Lieven</author>
<author>Michael Tomasello</author>
</authors>
<title>Modeling children’s early grammatical knowledge.</title>
<date>2009</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States ofAmerica,</booktitle>
<pages>106--41</pages>
<contexts>
<context position="1553" citStr="Bannard et al. 2009" startWordPosition="212" endWordPosition="215">tes longer utterances, and better expresses the situation these utterances intend to refer to. 1 Introduction A striking aspect of language acquisition is the difference between children’s and adult’s utterances. Simulating early grammatical production requires a specification of the nature of the linguistic representations underlying the short, telegraphic utterances of children. In the usage-based view, young children’s grammatical representions are thought to be less abstract than adults’, e.g. by having stricter constraints on what can be combined with them (cf. Akhtar and Tomasello 1997; Bannard et al. 2009; Ambridge et al. 2012). The representations and processes yielding the restricted length of these early utterances, however, have received little attention. Following Braine (1976), we adopt the working hypothesis that the early learner’s grammatical representations are more limited in length (or: arity) than those of adults. Similarly, in computational modeling of grammar acquisition, comprehension has received more Arie Verhagen LUCL Leiden University a.verhagen@hum.leidenuniv.nl attention than language generation. In this paper we attempt to make the mechanisms underlying early production </context>
</contexts>
<marker>Bannard, Lieven, Tomasello, 2009</marker>
<rawString>Colin Bannard, Elena Lieven, and Michael Tomasello. 2009. Modeling children’s early grammatical knowledge. Proceedings of the National Academy of Sciences of the United States ofAmerica, 106(41):17284–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin D S Braine</author>
</authors>
<title>Children’s first word combinations.</title>
<date>1976</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="1734" citStr="Braine (1976)" startWordPosition="239" endWordPosition="240">s and adult’s utterances. Simulating early grammatical production requires a specification of the nature of the linguistic representations underlying the short, telegraphic utterances of children. In the usage-based view, young children’s grammatical representions are thought to be less abstract than adults’, e.g. by having stricter constraints on what can be combined with them (cf. Akhtar and Tomasello 1997; Bannard et al. 2009; Ambridge et al. 2012). The representations and processes yielding the restricted length of these early utterances, however, have received little attention. Following Braine (1976), we adopt the working hypothesis that the early learner’s grammatical representations are more limited in length (or: arity) than those of adults. Similarly, in computational modeling of grammar acquisition, comprehension has received more Arie Verhagen LUCL Leiden University a.verhagen@hum.leidenuniv.nl attention than language generation. In this paper we attempt to make the mechanisms underlying early production explicit within a model that can parse and generate utterances, and that incrementally learns constructions (Goldberg, 1995) on the basis of its previous parses. The model’s search </context>
</contexts>
<marker>Braine, 1976</marker>
<rawString>Martin D.S. Braine. 1976. Children’s first word combinations. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bybee</author>
</authors>
<title>Regular morphology and the lexicon. Language and Cognitive Processes,</title>
<date>1995</date>
<volume>10</volume>
<pages>5--425</pages>
<marker>Bybee, 1995</marker>
<rawString>Joan Bybee. 1995. Regular morphology and the lexicon. Language and Cognitive Processes, 10 (5):425–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy C-L Chang</author>
</authors>
<title>Constructing Grammar: A computational model of the emergence of early constructions.</title>
<date>2008</date>
<institution>Dissertation, University of California, Berkeley.</institution>
<contexts>
<context position="2682" citStr="Chang, 2008" startWordPosition="372" endWordPosition="373">tion. In this paper we attempt to make the mechanisms underlying early production explicit within a model that can parse and generate utterances, and that incrementally learns constructions (Goldberg, 1995) on the basis of its previous parses. The model’s search through the hypothesis space of possible grammatical patterns is highly restricted. Starting from initially small and concrete representations, it learns incrementally long representations (syntagmatic growth) as well as more abstract ones (paradigmatic growth). Several models address either paradigmatic (Alishahi and Stevenson, 2008; Chang, 2008; Bannard et al., 2009) or syntagmatic (Freudenthal et al., 2010) growth. This model aims to explain both, thereby contributing to the understanding of how different learning mechanisms interact. As opposed to other models involving grammars with semantic representations (Alishahi and Stevenson, 2008; Chang, 2008), but similar to Kwiatkowski et al. (2012), the model starts without an inventory of mappings of single words to meanings. Based on motivation from usage-based and construction grammar approaches, we define several learning principles that allow the model to build up an inventory of l</context>
</contexts>
<marker>Chang, 2008</marker>
<rawString>Nancy C.-L. Chang. 2008. Constructing Grammar: A computational model of the emergence of early constructions. Dissertation, University of California, Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Freudenthal</author>
<author>Julian Pine</author>
<author>Fernand Gobet</author>
</authors>
<title>Explaining quantitative variation in the rate of Optional Infinitive errors across languages: a comparison of MOSAIC and the Variational Learning Model.</title>
<date>2010</date>
<journal>Journal of Child Language,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="2747" citStr="Freudenthal et al., 2010" startWordPosition="381" endWordPosition="384">ms underlying early production explicit within a model that can parse and generate utterances, and that incrementally learns constructions (Goldberg, 1995) on the basis of its previous parses. The model’s search through the hypothesis space of possible grammatical patterns is highly restricted. Starting from initially small and concrete representations, it learns incrementally long representations (syntagmatic growth) as well as more abstract ones (paradigmatic growth). Several models address either paradigmatic (Alishahi and Stevenson, 2008; Chang, 2008; Bannard et al., 2009) or syntagmatic (Freudenthal et al., 2010) growth. This model aims to explain both, thereby contributing to the understanding of how different learning mechanisms interact. As opposed to other models involving grammars with semantic representations (Alishahi and Stevenson, 2008; Chang, 2008), but similar to Kwiatkowski et al. (2012), the model starts without an inventory of mappings of single words to meanings. Based on motivation from usage-based and construction grammar approaches, we define several learning principles that allow the model to build up an inventory of linguistic representations. The model incrementally processes pair</context>
</contexts>
<marker>Freudenthal, Pine, Gobet, 2010</marker>
<rawString>Daniel Freudenthal, Julian Pine, and Fernand Gobet. 2010. Explaining quantitative variation in the rate of Optional Infinitive errors across languages: a comparison of MOSAIC and the Variational Learning Model. Journal of Child Language, 37(3):643–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele E Goldberg</author>
</authors>
<title>Constructions. A Construction Grammar Approach to Argument Structure.</title>
<date>1995</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="2277" citStr="Goldberg, 1995" startWordPosition="315" endWordPosition="316">rances, however, have received little attention. Following Braine (1976), we adopt the working hypothesis that the early learner’s grammatical representations are more limited in length (or: arity) than those of adults. Similarly, in computational modeling of grammar acquisition, comprehension has received more Arie Verhagen LUCL Leiden University a.verhagen@hum.leidenuniv.nl attention than language generation. In this paper we attempt to make the mechanisms underlying early production explicit within a model that can parse and generate utterances, and that incrementally learns constructions (Goldberg, 1995) on the basis of its previous parses. The model’s search through the hypothesis space of possible grammatical patterns is highly restricted. Starting from initially small and concrete representations, it learns incrementally long representations (syntagmatic growth) as well as more abstract ones (paradigmatic growth). Several models address either paradigmatic (Alishahi and Stevenson, 2008; Chang, 2008; Bannard et al., 2009) or syntagmatic (Freudenthal et al., 2010) growth. This model aims to explain both, thereby contributing to the understanding of how different learning mechanisms interact.</context>
<context position="5036" citStr="Goldberg, 1995" startWordPosition="743" endWordPosition="744">nerate utterances as well. We describe two experiments: in the comprehension experiment (Section 5), we evaluate the model’s ability to parse the stream of input items. In the generation experiment (Section 6), the model generates utterances on the basis of a given situation and its linguistic knowledge. We evaluate the generated utterances given different amounts of training items to consider the development of the model over time. 2 Representations We represent linguistic knowledge as constructions: pairings of a signifying form and a signified (possibly incomplete) semantic representation (Goldberg, 1995). The meaning is represented as a graph with the nodes denoting entities, events, and their relations, connected by directed unlabeled edges. The conceptual content of each node is given by a set of semantic features. We assume that meaning representations are rooted trees. The signifying form consists of a positive number of constituents. Every constituent has two elements: a phonological form, and a pointer to a node in the signified meaning (in line with Verhagen 2009). Both can be specified, or one can be left empty. Constituents with unspecified phonological forms are called open, denoted</context>
</contexts>
<marker>Goldberg, 1995</marker>
<rawString>Adele E. Goldberg. 1995. Constructions. A Construction Grammar Approach to Argument Structure. Chicago University Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele E Goldberg</author>
</authors>
<title>Constructions: a new theoretical approach to language. Trends</title>
<date>2003</date>
<booktitle>in Cognitive Sciences,</booktitle>
<volume>7</volume>
<issue>5</issue>
<contexts>
<context position="6216" citStr="Goldberg 2003" startWordPosition="936" endWordPosition="937">cal forms are called open, denoted with E in the figures. The head constituent of a construction is defined as the constituent that has a pointer to the root node of the signified meaning. We furthermore require that no two constituents point to the same node of the signified meaning. This definition generalizes over lexical elements (one phonologically specified constituent) as well as larger linguistic patterns. Fig. 2, for instance, shows two larger constructions being combined with each other. We call the set of constructions the learner has at some moment in time the constructicon C (cf. Goldberg 2003). 3 Parsing 3.1 Parsing operations We first define a derivation d as an assembly of constructions in C, using four parsing operations defined below. In parsing, derivations are constrained by the utterance U and the situations Figure 1: The global flow of the model S, whereas in production, only a situation s constrains the derivation. The leaf nodes of a derivation must consist of phonological constraints of constructions that (in parsing) are satisfied by U. All constructions used in a derivation must map to the same situation s E S. A construction c maps to s iff the meaning of c constitute</context>
</contexts>
<marker>Goldberg, 2003</marker>
<rawString>Adele E Goldberg. 2003. Constructions: a new theoretical approach to language. Trends in Cognitive Sciences, 7(5):219–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adele E Goldberg</author>
</authors>
<title>Constructions at Work. The Nature of Generalization in Language.</title>
<date>2006</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<marker>Goldberg, 2006</marker>
<rawString>Adele E. Goldberg. 2006. Constructions at Work. The Nature of Generalization in Language. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kay</author>
</authors>
<title>An Informal Sketch of a Formal Architecture for Construction Grammar. Grammars,</title>
<date>2002</date>
<pages>5--1</pages>
<contexts>
<context position="10587" citStr="Kay 2002" startWordPosition="1652" endWordPosition="1653">to interpret a larger part of the situation than with COMBINATION only. The resulting sequences may be analyzed in the learning process as constituting one larger construction, consisting of the parts of the concatenated derivations. Fig. 3 illustrates these three operations. 3.2 Selecting the best analysis Multiple derivations can be highly similar in the way they map parts of U to parts of an s ∈ S. We define a parse to be a set of derivations that have the same internal structure and the same mappings to a situation, but that use different constructions in doing so (cf. multiple licensing; Kay 2002). We take the most probable parse of U to be the best analysis of U. The most probable parse points to a situation, which the model then assumes to be the identified situation or sidentified. If no parse can be The probability of a construction P(c) is given by its relative frequency (count) in the constructicon C, smoothed with Laplace smoothing. We assume that the simple parsing operations of IGNORE, BOOTSTRAP, and CONCATENATION reflect usages of an unseen construction with a count of 0. c.count + 1 P(c) = E cf.count + |C |+ 1 (3) c&apos;EC The most probable parse, U and sidentified are added to </context>
</contexts>
<marker>Kay, 2002</marker>
<rawString>Paul Kay. 2002. An Informal Sketch of a Formal Architecture for Construction Grammar. Grammars, 5:1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Sharon Goldwater</author>
<author>Luke Zettlemoyer</author>
<author>Mark Steedman</author>
</authors>
<title>A Probabilistic Model of Syntactic and Semantic Acquisition from Child-Directed Utterances and their Meanings.</title>
<date>2012</date>
<booktitle>In Proceedings EACL.</booktitle>
<contexts>
<context position="3039" citStr="Kwiatkowski et al. (2012)" startWordPosition="423" endWordPosition="426">ed. Starting from initially small and concrete representations, it learns incrementally long representations (syntagmatic growth) as well as more abstract ones (paradigmatic growth). Several models address either paradigmatic (Alishahi and Stevenson, 2008; Chang, 2008; Bannard et al., 2009) or syntagmatic (Freudenthal et al., 2010) growth. This model aims to explain both, thereby contributing to the understanding of how different learning mechanisms interact. As opposed to other models involving grammars with semantic representations (Alishahi and Stevenson, 2008; Chang, 2008), but similar to Kwiatkowski et al. (2012), the model starts without an inventory of mappings of single words to meanings. Based on motivation from usage-based and construction grammar approaches, we define several learning principles that allow the model to build up an inventory of linguistic representations. The model incrementally processes pairs of an utterance U, consisting of a string of words wi ... wn, and a set of situations S, one of which is the situation the speaker intends to refer to. The other situations contribute to propositional uncertainty (the uncertainty over which proposition the speaker is trying to express; Sis</context>
</contexts>
<marker>Kwiatkowski, Goldwater, Zettlemoyer, Steedman, 2012</marker>
<rawString>Tom Kwiatkowski, Sharon Goldwater, Luke Zettlemoyer, and Mark Steedman. 2012. A Probabilistic Model of Syntactic and Semantic Acquisition from Child-Directed Utterances and their Meanings. In Proceedings EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald W Langacker</author>
</authors>
<title>Foundations of Cognitive Grammar, Volume I.</title>
<date>1989</date>
<publisher>Stanford University Press.</publisher>
<contexts>
<context position="15799" citStr="Langacker 1989" startWordPosition="2501" endWordPosition="2502">struction with a higher arity, ‘joining’ several previously independent constructions. Fig. 4 illustrates the syntagmatization mechanism. Paradigmatization Due to our usage-driven approach, all learning mechanisms so far give us maximally concrete constructions. In order for the model to generalize beyond the observed input, some degree of abstraction is needed. The model does so with the PARADIGMATIZATION mechanism. This mechanism recursively looks for minimal abstractions (cf. Tomasello 2003, 123) over the constructions in C and adds those to C, thus creating a full-inheritance network (cf. Langacker 1989, 63-76). An abstraction over a set of constructions is made if there is an overlapping subgraph between the meanings of the constructions, where every node of the subgraph is the non-empty feature set intersection between two mapped nodes of the constructional meanings. Furthermore, the con49 {volitional,...} {animate, hearer} PHON: you SEM {volitional,...} {independentexist} {act} {independentexist} PHON: ɛ {act,move} PHON: take SEM SEM concatenate A derivation over the utterance you take it. {object,entity, ball} PHON: ball SEM PHON: you SEM {volitional,...} {animate, hearer} PHON: take SEM</context>
</contexts>
<marker>Langacker, 1989</marker>
<rawString>Ronald W. Langacker. 1989. Foundations of Cognitive Grammar, Volume I. Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald W Langacker</author>
</authors>
<title>A dynamic view of usage and language acquisition.</title>
<date>2009</date>
<journal>Cognitive Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<marker>Langacker, 2009</marker>
<rawString>Ronald W. Langacker. 2009. A dynamic view of usage and language acquisition. Cognitive Linguistics, 20(3):627–640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey M Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning wordto-meaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="3649" citStr="Siskind 1996" startWordPosition="524" endWordPosition="525">12), the model starts without an inventory of mappings of single words to meanings. Based on motivation from usage-based and construction grammar approaches, we define several learning principles that allow the model to build up an inventory of linguistic representations. The model incrementally processes pairs of an utterance U, consisting of a string of words wi ... wn, and a set of situations S, one of which is the situation the speaker intends to refer to. The other situations contribute to propositional uncertainty (the uncertainty over which proposition the speaker is trying to express; Siskind 1996). The model tries to identify the intended situation and to understand how parts of the utterance refer to certain parts of that situation. To do so, the model uses its growing inventory of linguistic representations (Section 2) to analyze U, producing a set of structured semantic analyses or parses (Fig. 1, arrow 1; Section 3). 46 Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 46–54, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics The resulting best parse, U and the selected situation are then stored in a</context>
<context position="19785" citStr="Siskind 1996" startWordPosition="3120" endWordPosition="3121">odel receives a situation, and (given the constructicon) finds the most probable, maximally expressive, fully lexicalized derivation expressing it. That is: among all derivations terminating in phonologically specified constituents, it selects the derivations that cover the most semantic nodes of the given situation. In the case of multiple such derivations, it selects the most probable one, following the probability model in Section 3. We only allow for the COMBINATION operator in the derivations, as BOOTSTRAPPING and IGNORE rewith the intended situation, to reflect more realistic input (cf. Siskind 1996). 50 PHON: you {animate,hearer} SEM {volitional,...} PHON: take PHON: you {cause,move} SEM {animate,hearer} SEM {volitional,...} {location,entity,chair} {patient,...} PHON: chair PHON: take SEM SEM {cause,move} {location,entity} {patient,...} PHON: ɛ SEM SEM SEM PHON: you PHON: take PHON: table SEM {animate,hearer} {volitional,...} {cause,move} {location,entity,table} The set intersection of {location,entity,chair} and {location,entity, table} A phonologically empty constituent, generalizing over chair and table {patient,...} Figure 5: The PARADIGMATIZATION mechanism. The construction on top i</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Jeffrey M Siskind. 1996. A computational study of cross-situational techniques for learning wordto-meaning mappings. Cognition, 61(1-2):39– 91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>First Verbs: A study of early grammatical development.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="28200" citStr="Tomasello, 1992" startWordPosition="4442" endWordPosition="4443">atterns for well-known cases of overgeneralization errors. The potential for abstraction The paradigmatization operation allows the model to go beyond observed concrete instances of form-meaning pairings: without it, unseen situations could never be fully expressed. Despite this potential, we have seen that the model relies on highly concrete constructions. The concreteness of the used patterns, however, does not imply the absence of more abstract representations. Fig. 7 gives three examples of constructions in C in one simulation. Construction (a) could be seen as a verb-island construction (Tomasello, 1992, 23-24). The second constituent is phonologically specified with put, and the other arguments are open, but mapped to specific semantic functions. This pattern allows for the expression of many caused-motion events. Construction (b) is the inverse of (a): the arguments are phonologically specified, but the verbslot is open. This would be a case of a pronominal argument frame [you V it], which have been found to be helpful in the bootstrapping of verbal meanings (Tomasello, 2001). Finally, (c) presents a case of full abstraction. This construction licenses utterances such as I sit here, you st</context>
</contexts>
<marker>Tomasello, 1992</marker>
<rawString>Michael Tomasello. 1992. First Verbs: A study of early grammatical development. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>Perceiving intentions and learning words in the second year of life.</title>
<date>2001</date>
<booktitle>Language Acquisition and Conceptual Development, chapter 5,</booktitle>
<pages>132--158</pages>
<editor>In Melissa Bowerman and Stephen C. Levinson, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="28684" citStr="Tomasello, 2001" startWordPosition="4521" endWordPosition="4522">hree examples of constructions in C in one simulation. Construction (a) could be seen as a verb-island construction (Tomasello, 1992, 23-24). The second constituent is phonologically specified with put, and the other arguments are open, but mapped to specific semantic functions. This pattern allows for the expression of many caused-motion events. Construction (b) is the inverse of (a): the arguments are phonologically specified, but the verbslot is open. This would be a case of a pronominal argument frame [you V it], which have been found to be helpful in the bootstrapping of verbal meanings (Tomasello, 2001). Finally, (c) presents a case of full abstraction. This construction licenses utterances such as I sit here, you stay there and erroneous ones like he sits on (which, again, will be pre-empted in the generation of utterances if more concrete constructions licence he sits on it). Summarizing, abstract constructions are acquired, but only used for those cases in which no concrete construction is available. This is in line with the usage-based hypotheses that abstract constructions do emerge, but that for much of language production, a language user can rely on highly concrete patterns. A next s</context>
</contexts>
<marker>Tomasello, 2001</marker>
<rawString>Michael Tomasello. 2001 Perceiving intentions and learning words in the second year of life. In Melissa Bowerman and Stephen C. Levinson, editors, Language Acquisition and Conceptual Development, chapter 5, pages 132–158. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Tomasello</author>
</authors>
<title>Constructing a language: A Usage-Based Theory of Language Acquisition.</title>
<date>2003</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15683" citStr="Tomasello 2003" startWordPosition="2482" endWordPosition="2483">tituents of t itself, are then linearized as the constituents of csyn. The novel construction thus constitutes a construction with a higher arity, ‘joining’ several previously independent constructions. Fig. 4 illustrates the syntagmatization mechanism. Paradigmatization Due to our usage-driven approach, all learning mechanisms so far give us maximally concrete constructions. In order for the model to generalize beyond the observed input, some degree of abstraction is needed. The model does so with the PARADIGMATIZATION mechanism. This mechanism recursively looks for minimal abstractions (cf. Tomasello 2003, 123) over the constructions in C and adds those to C, thus creating a full-inheritance network (cf. Langacker 1989, 63-76). An abstraction over a set of constructions is made if there is an overlapping subgraph between the meanings of the constructions, where every node of the subgraph is the non-empty feature set intersection between two mapped nodes of the constructional meanings. Furthermore, the con49 {volitional,...} {animate, hearer} PHON: you SEM {volitional,...} {independentexist} {act} {independentexist} PHON: ɛ {act,move} PHON: take SEM SEM concatenate A derivation over the utteran</context>
</contexts>
<marker>Tomasello, 2003</marker>
<rawString>Michael Tomasello. 2003. Constructing a language: A Usage-Based Theory of Language Acquisition. Harvard University Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
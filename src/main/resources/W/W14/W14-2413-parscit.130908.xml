<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024818">
<title confidence="0.9966715">
Intermediary Semantic Representation
through Proposition Structures
</title>
<author confidence="0.997375">
Gabriel Stanovsky*, Jessica Ficler*, Ido Dagan, Yoav Goldberg
</author>
<affiliation confidence="0.984989">
Computer Science Department, Bar-Ilan University
</affiliation>
<note confidence="0.815156">
∗Both authors equally contributed to this paper
</note>
<email confidence="0.980863">
{gabriel.satanovsky,jessica.ficler,yoav.goldberg}@gmail.com
dagan@cs.biu.ac.il
</email>
<sectionHeader confidence="0.993595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951875">
We propose an intermediary-level seman-
tic representation, providing a higher level
of abstraction than syntactic parse trees,
while not committing to decisions in cases
such as quantification, grounding or verb-
specific roles assignments. The proposal
is centered around the proposition struc-
ture of the text, and includes also im-
plicit propositions which can be inferred
from the syntax but are not transparent in
parse trees, such as copular relations intro-
duced by appositive constructions. Other
benefits over dependency-trees are ex-
plicit marking of logical relations between
propositions, explicit marking of multi-
word predicate such as light-verbs, and a
consistent representation for syntactically-
different but semantically-similar struc-
tures. The representation is meant to
serve as a useful input layer for semantic-
oriented applications, as well as to provide
a better starting point for further levels of
semantic analysis such as semantic-role-
labeling and semantic-parsing.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999880648148149">
Parsers for semantic formalisms (such as Neo-
davidsonian (Artzi and Zettlemoyer, 2013) and
DRT (Kamp, 1988)) take unstructured natural lan-
guage text as input, and output a complete seman-
tic representation, aiming to capture the mean-
ing conveyed by the text. We suggest that this
task may be effectively separated into a sequential
combination of two different tasks. The first of
these tasks is syntactic abstraction over phenom-
ena such as expression of tense, negation, modal-
ity, and passive versus active voice, which are all
either expressed or implied from syntactic struc-
ture. The second task is semantic interpretation
over the syntactic abstraction, deriving quantifi-
cation, grounding, etc. Current semantic parsers
(such as Boxer (Bos, 2008)) tackle these tasks si-
multaneously, mixing syntactic and semantic is-
sues in a single framework. We believe that sepa-
rating semantic parsing into two well defined tasks
will help to better target and identify challenges
in syntactic and semantic domains. Challenges
which are often hidden due to the one-step archi-
tecture of current parsers.
Many of today’s semantic parsers, and semantic
applications in general, leverage dependency pars-
ing (De Marneffe and Manning, 2008a) as an ab-
straction layer, since it directly represents syntac-
tic dependency relations between predicates and
arguments. Some systems exploit Semantic Role
Labeling (SRL) (Carreras and M‘arquez, 2005),
where predicate-argument relationships are cap-
tured at a thematic (rather than syntactic) level,
though current SRL technology is less robust and
accurate for open domains than syntactic pars-
ing. While dependency structures and semantic
roles capture much of the proposition structure of
sentences, there are substantial aspects which are
not covered by these representations and therefore
need to be handled by semantic applications on
their own (or they end up being ignored).
Such aspects, as detailed in Section 3, include
propositions which are not expressed directly as
such but are rather implied by syntactic struc-
ture, like nominalizations, appositions and pre-
modifying adjectives. Further, the same proposi-
tion structure may be expressed in many differ-
ent ways by the syntactic structure, forcing sys-
tems to recognize this variability and making the
task of recognizing semantic roles harder. Other
aspects not addressed by common representations
include explicit marking of links between propo-
sitions within a sentence, which affect their asser-
tion or truth status, and the recognition of multi-
word predicates (e.g., considering “take a deci-
</bodyText>
<page confidence="0.91069">
66
</page>
<note confidence="0.4163325">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 66–70,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.990784">
Figure 1: Proposed representation for the sentence: “If you
leave the park, you will find the Peak Tram terminal”
</figureCaption>
<bodyText confidence="0.999972888888889">
sion” as a single predicate, rather than considering
decision as an argument).
In this position paper we propose an intermedi-
ary representation level for the first syntactic ab-
straction phase described above, intended to re-
place syntactic parsing as a more abstract repre-
sentation layer. It is designed to capture the full
proposition structure which is expressed, either
explicitly or implicitly, by the syntactic structure
of sentences. Thus, we aim to both extract im-
plicit propositions as well as to abstract away syn-
tactic variations which yield the same proposition
structure. At the same time, we aim to remain at
a representation level that corresponds to syntac-
tic properties and relationships, while avoiding se-
mantic interpretations, to be targeted by systems
implementing the further step of semantic inter-
pretation, as discussed above.
In addition, we suggest our representation as a
useful input for semantic applications which need
to recognize the proposition structure of sentences
in order to identify targeted information, such as
Question Answering(QA), Information Extraction
(IE) and multidocument summarization. We ex-
pect that our representation may be more useful
in comparison with current popular use of depen-
dency parsing, in such applications.
</bodyText>
<sectionHeader confidence="0.93442" genericHeader="method">
2 Representation Scheme
</sectionHeader>
<bodyText confidence="0.999952">
Our representation is centered around proposi-
tions, where a proposition is a statement for which
a truth-value can be assigned. We propose to rep-
resent sentences as a set of inter-linked proposi-
tions. Each proposition is composed of one pred-
icate and a set of arguments. An example rep-
resentation can be seen in Figure 1. Predicates
are usually centered around verbs, and we con-
sider multi-word verbs (e.g., “take apart”) as sin-
gle predicates. Both the predicates and arguments
are represented as sets of feature-value pairs. Each
argument is marked with a relation to its predicate,
and the same argument can appear in different
propositions. The relation-set we use is syntactic
in nature, including relations such as Subject,
Object, and Preposition with, in contrast
to semantic relations such as instrument.
Canonical Representation The same proposi-
tion can be realized syntactically in many forms.
An important goal of our proposal is abstracting
over idiosyncrasies in the syntactic structure and
presenting unified structures when possible. We
canonicalize on two levels:
</bodyText>
<listItem confidence="0.774158">
• We canonicalize each predicate and argument
by representing each predicate as its main
lemma, and indicating other aspects of the
predication (e.g., tense, negation and time) as
features; Similarly, we mark arguments with
features such as definiteness and plurality.
• We canonicalize the argument structure by
abstracting away over word order and phe-
nomena such as topicalization and pas-
sive/active voice, and present a unified rep-
resentation in terms of the argument roles (so
</listItem>
<bodyText confidence="0.985142277777778">
that, for example, in the sentence “the door
was opened” the argument “door” will re-
ceive the object role, with the passive be-
ing indicated as a feature of the predicate).
Relations Between Propositions Some propo-
sitions must be interpreted taking into account
their relations to other propositions. These in-
clude conditionals (“if congress does nothing,
President Bush will have won” (wsj 0112));
temporal relations (“UAL’s announcement came
after the market closed yesterday”(wsj 0112));
and conjunctions (“They operate ships and
banks.”(wsj 0083)).
We model such relations as typed links between
extracted propositions. Figure 1 presents an exam-
ple of handling a conditional relation: the depen-
dence between the propositions is made explicit by
the Cond(if) relation.
</bodyText>
<sectionHeader confidence="0.999069" genericHeader="method">
3 Implicit Propositions
</sectionHeader>
<bodyText confidence="0.999677666666667">
Crucially, our proposal aims to capture not only
explicit but also implicit propositions – proposi-
tions that can be inferred from the syntactic struc-
</bodyText>
<page confidence="0.997512">
67
</page>
<bodyText confidence="0.999979653846154">
ture but which are not explicitly marked in syn-
tactic dependency trees, as we elaborate below.
Some of these phenomena are relatively easy to
address by post-processing over syntactic parsers,
and could thus be included in a first implemen-
tation that produces our proposed representations.
Other phenomena are more subtle and would re-
quire further research, yet they seem important
while not being addressed by current techniques.
The syntactic structures giving rise to implicit
propositions include:
Copular sentences such as “This is not a triv-
ial issue.” (wsj 0108) introduces a proposition by
linking between a non-verbal predicate and its ar-
gument. We represent this by making “not a triv-
ial issue” a predicate, and “this” an argument of
type Predication.
Appositions, we distinguish between co-reference
and predicative appositions. In Co-reference in-
dication appositions (“The company, Random
House, doesn’t report its earnings.” (adaption of
wsj 0111)) we produce a proposition to indicate
the co-reference between two lexical items. Other
propositions relating to the entity use the main
clause as the referent for this entity. In this ex-
ample, we will produce:
</bodyText>
<listItem confidence="0.9887505">
1. Random House == the company.
2. The company doesn’t report its earnings.
</listItem>
<bodyText confidence="0.8946374">
In Predicative appositions (“Pierre Vinken, 61
years old, will join the board as a nonexecutive di-
rector Nov. 29.” (wsj 0001)) an apposition is used
in order to convey knowledge about an entity. In
our representation this will produce:
</bodyText>
<listItem confidence="0.9922665">
1. Pierre Vinken is 61 years old (which is canoni-
calized to the representation of copular sentences)
2. Pierre Vinken will join the board as a nonexec-
utive director Nov. 29.
</listItem>
<bodyText confidence="0.999635041666667">
Adjectives, as in the sentence “you emphasized
the high prevalence of mental illness” (wsj 0105).
Here an adjective is used to describe a definite sub-
ject and introduces another proposition, namely
the high prevalence of mental illness.
Nominalizations, for instance in the sentence
“Googles acquisition of Waze occurred yester-
day”, introduce the implicit proposition that
“Google acquired Waze”. Such propositions were
studied and annotated in the NOMLEX (Macleod
et al., 1998) and NOMBANK (Meyers et al., 2004)
resources. It remains an open issue how to repre-
sent or distinguish cases in which nominalization
introduce an underspecified proposition. For ex-
ample, consider “dancing” in “I read a book about
dancing”.
Possessives, such as “John’s book” introduce the
proposition that John has a book. Similarly, ex-
amples such as “John’s Failure” combine a pos-
sessive construction with nominalization and in-
troduce the proposition that John has failed.
Conjunctions - for example in “They operate
ships and banks.” (wsj 0083), introduce several
propositions in one sentence:
</bodyText>
<listItem confidence="0.9720185">
1. They operate ships
2. They operate banks
</listItem>
<bodyText confidence="0.99974845">
We mark that they co-refer to the same lexical unit
in the original sentence. Such cases are already
represented explicitly in the “collapsed” version
of Stanford-dependencies (De Marneffe and Man-
ning, 2008a).1
Implicit future tense indication, for instance
in “I’m going to vote for it” (wsj 0098) and
“The economy is about to slip into recession.”
(wsj 0036), verbs like “going to” and “about to”
are used as future-tense markers of the proposi-
tion following them, rather than predicates on their
own. We represent these as a single predicate
(“vote”) in which the tense is marked as a fea-
ture.2
Other phenomena, omitted for lack of space,
include propositional modifiers (e.g., relative
clause modifiers), propositional arguments (such
as ”John asserted that he will go home”), condi-
tionals, and the canonicalization of passive and
active voice.
</bodyText>
<sectionHeader confidence="0.990569" genericHeader="method">
4 Relation to Other Representations
</sectionHeader>
<bodyText confidence="0.996330636363636">
Our proposed representation is intended to serve
as a bridging layer between purely syntactic rep-
resentations such as dependency trees, and seman-
tic oriented applications. In particular, we explic-
itly represent many semantic relations expressed
in a sentence that are not captured by contempo-
rary proposition-directed semantic representations
(Baker et al., 1998; Kingsbury and Palmer, 2003;
Meyers et al., 2004; Carreras and M`arquez, 2005).
Compared to dependency-based representations
such as Stanford-dependency trees (De Marneffe
</bodyText>
<footnote confidence="0.998310142857143">
1A case of conjunctions requiring special treatment is in-
troduced by reciprocals, in which the entities roles are ex-
changeable. For example: “John and Mary bet against each
other on future rates” (adaption of wsj 0117).
2Care needs to be taken to distinguish from cases such as
“going to Italy” in which “going to” is not followed by a
verbal predicate.
</footnote>
<page confidence="0.999161">
68
</page>
<bodyText confidence="0.999910941176471">
and Manning, 2008b), we abstract away over
many syntactic details (e.g., the myriad of ways
of expressing tense, negation and modality, or the
difference between passive and active) which are
not necessary for semantic interpretation and mark
them instead using a unified set of features and ar-
gument types. We make explicit many relations
that can be inferred from the syntax but which
are not directly encoded in dependency relations.
We directly connect predicates with all of their ar-
guments in e.g., conjunctions and embedded con-
structions, and we do not commit to a tree struc-
ture. We also explicitly mark predicate and argu-
ment boundaries, and explicitly mark multi-word
predicates such as light-verb constructions.
Compared to proposition-based semantic rep-
resentations, we do not attempt to assign frame-
specific thematic roles, nor do we attempt to dis-
ambiguate or interpret word meanings. We restrict
ourselves to representing predicates by their (lem-
matized) surface forms, and labeling arguments
based on a “syntactic” role inventory, similar to the
label-sets available in dependency representations.
This design choice makes our representation much
easier to assign automatically to naturally occur-
ring text (perhaps pre-annotated using a syntactic
parser) than it is to assign semantic roles. At the
same time, as described in Section 3, we capture
many relations that are currently not annotated in
resources such as FrameNet, and provide a com-
prehensive set of propositions present in the sen-
tence (either explicitly or implicitly) as well as the
relations between them – an objective which is not
trivial even when presented with full semantic rep-
resentation.
Compared to more fine-grained semantic repre-
sentations used in semantic-parsers (i.e. lambda-
calculus (Zettlemoyer and Collins, 2005), neo-
davidsonian semantics (Artzi and Zettlemoyer,
2013), DRT (Kamp, 1988) or the DCS represen-
tation of Liang (2011)), we do not attempt to
tackle quantification, nor to ground the arguments
and predicates to a concrete domain-model or on-
tology. These important tasks are orthogonal to
our representation, and we believe that semantic-
parsers can benefit from our proposal by using it
as input in addition to or instead of the raw sen-
tence text – quantification, binding and grounding
are hard enough without needing to deal with the
subtleties of syntax or the identification of implicit
propositions.
</bodyText>
<sectionHeader confidence="0.918206" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993">
We proposed an intermediate semantic repre-
sentation through proposition extraction, which
captures both explicit and implicit propositions,
while staying relatively close to the syntactic
level. We believe that this kind of representation
will serve not only as an advantageous input for
semantically-centered applications, such as ques-
tion answering, summarization and information
extraction, but also serve as a rich representation
layer that can be used as input for systems aiming
to provide a finer level of semantic analysis, such
as semantic-parsers.
We are currently at the beginning of our in-
vestigation. In the near future we plan to semi-
automatically annotate the Penn Tree Bank (Mar-
cus et al., 1993) with these structures, as well as
to provide software for deriving (some of) the im-
plicit and explicit annotations from automatically
produced parse-trees. We believe such resources
will be of immediate use to semantic-oriented ap-
plications. In the longer term, we plan to inves-
tigate dedicated algorithms for automatically pro-
ducing such representation from raw text.
The architecture we describe can easily accom-
modate additional layers of abstraction, by en-
coding these layers as features of propositions,
predicates or arguments. Such layers can include
the marking of named entities, the truth status of
propositions and author commitment.
In the current version infinitive constructions
are treated as nested propositions, similar to their
representation in syntactic parse trees. Providing
a consistent, useful and transparent representation
for infinitive constructions is a challenging direc-
tion for future research.
Other extensions of the proposed representa-
tion are also possible. One appealing direction
is going beyond the sentence level and represent-
ing discourse level relations, including implied
propositions and predicate - argument relation-
ships expressed by discourse (Stern and Dagan,
2014; Ruppenhofer et al., 2010; Gerber and Chai,
2012). Such an extension may prove useful as an
intermediary representation for parsers of seman-
tic formalisms targeted at the discourse level (such
as DRT).
</bodyText>
<sectionHeader confidence="0.998265" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.895331">
This work was partially supported by the Eu-
ropean Community’s Seventh Framework Pro-
</bodyText>
<page confidence="0.997498">
69
</page>
<bodyText confidence="0.7351215">
gramme (FP7/2007-2013) under grant agreement
no. 287923 (EXCITEMENT).
</bodyText>
<sectionHeader confidence="0.971341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999429333333334">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised
learning of semantic parsers for mapping instructions to
actions. Transactions of the Association for Computa-
tional Linguistics, 1(1):49–62.
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998.
The berkeley framenet project. In Proceedings of ACL,
pages 86–90. Association for Computational Linguistics.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Proceedings of the 2008 Conference on Seman-
tics in Text Processing, pages 277–286. Association for
Computational Linguistics.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to
the conll-2005 shared task: Semantic role labeling. In
Proceedings of CONLL, pages 152–164.
Marie-Catherine De Marneffe and Christopher D Manning.
2008a. Stanford typed dependencies manual. Technical
report, Stanford University.
Marie-Catherine De Marneffe and Christopher D Manning.
2008b. The stanford typed dependencies representation.
In Coling 2008: Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation, pages
1–8.
Matthew Gerber and Joyce Y Chai. 2012. Semantic role la-
beling of implicit arguments for nominal predicates. Com-
putational Linguistics, 38(4):755–798.
Hans Kamp. 1988. Discourse representation theory. In Nat-
ural Language at the computer, pages 84–111. Springer.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the
next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency-based compositional semantics. In Proceed-
ings of ACL, pages 590–599.
Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie
Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon
of nominalizations. In Proceedings of EURALEX, vol-
ume 98, pages 187–193.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
english: The penn treebank. Computational linguistics,
19(2):313–330.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph Gr-
ishman. 2004. The nombank project: An interim report.
In HLT-NAACL 2004 workshop: Frontiers in corpus an-
notation, pages 24–31.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. Semeval-2010
task 10: Linking events and their participants in discourse.
In Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 45–50. Association for Compu-
tational Linguistics.
Asher Stern and Ido Dagan. 2014. Recognizing implied
predicate-argument relationships in textual inference. In
Proceedings of ACL. Association for Computational Lin-
guistics.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In UAI, pages
658–666. AUAI Press.
</reference>
<page confidence="0.998482">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341424">
<title confidence="0.9985635">Intermediary Semantic through Proposition Structures</title>
<author confidence="0.999671">Jessica Ido Dagan</author>
<author confidence="0.999671">Yoav</author>
<affiliation confidence="0.999542">Computer Science Department, Bar-Ilan University</affiliation>
<abstract confidence="0.954388407407407">authors equally contributed to this dagan@cs.biu.ac.il Abstract We propose an intermediary-level semantic representation, providing a higher level of abstraction than syntactic parse trees, while not committing to decisions in cases such as quantification, grounding or verbspecific roles assignments. The proposal is centered around the proposition structure of the text, and includes also implicit propositions which can be inferred from the syntax but are not transparent in parse trees, such as copular relations introduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1400" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="182" endWordPosition="185">oduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing. 1 Introduction Parsers for semantic formalisms (such as Neodavidsonian (Artzi and Zettlemoyer, 2013) and DRT (Kamp, 1988)) take unstructured natural language text as input, and output a complete semantic representation, aiming to capture the meaning conveyed by the text. We suggest that this task may be effectively separated into a sequential combination of two different tasks. The first of these tasks is syntactic abstraction over phenomena such as expression of tense, negation, modality, and passive versus active voice, which are all either expressed or implied from syntactic structure. The second task is semantic interpretation over the syntactic abstraction, deriving quantification, grou</context>
<context position="14389" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="2185" endWordPosition="2188">otated using a syntactic parser) than it is to assign semantic roles. At the same time, as described in Section 3, we capture many relations that are currently not annotated in resources such as FrameNet, and provide a comprehensive set of propositions present in the sentence (either explicitly or implicitly) as well as the relations between them – an objective which is not trivial even when presented with full semantic representation. Compared to more fine-grained semantic representations used in semantic-parsers (i.e. lambdacalculus (Zettlemoyer and Collins, 2005), neodavidsonian semantics (Artzi and Zettlemoyer, 2013), DRT (Kamp, 1988) or the DCS representation of Liang (2011)), we do not attempt to tackle quantification, nor to ground the arguments and predicates to a concrete domain-model or ontology. These important tasks are orthogonal to our representation, and we believe that semanticparsers can benefit from our proposal by using it as input in addition to or instead of the raw sentence text – quantification, binding and grounding are hard enough without needing to deal with the subtleties of syntax or the identification of implicit propositions. 5 Conclusion and Future Work We proposed an intermedia</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11993" citStr="Baker et al., 1998" startWordPosition="1815" endWordPosition="1818">ck of space, include propositional modifiers (e.g., relative clause modifiers), propositional arguments (such as ”John asserted that he will go home”), conditionals, and the canonicalization of passive and active voice. 4 Relation to Other Representations Our proposed representation is intended to serve as a bridging layer between purely syntactic representations such as dependency trees, and semantic oriented applications. In particular, we explicitly represent many semantic relations expressed in a sentence that are not captured by contemporary proposition-directed semantic representations (Baker et al., 1998; Kingsbury and Palmer, 2003; Meyers et al., 2004; Carreras and M`arquez, 2005). Compared to dependency-based representations such as Stanford-dependency trees (De Marneffe 1A case of conjunctions requiring special treatment is introduced by reciprocals, in which the entities roles are exchangeable. For example: “John and Mary bet against each other on future rates” (adaption of wsj 0117). 2Care needs to be taken to distinguish from cases such as “going to Italy” in which “going to” is not followed by a verbal predicate. 68 and Manning, 2008b), we abstract away over many syntactic details (e.g</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of ACL, pages 86–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Semantics in Text Processing,</booktitle>
<pages>277--286</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2063" citStr="Bos, 2008" startWordPosition="288" endWordPosition="289">uage text as input, and output a complete semantic representation, aiming to capture the meaning conveyed by the text. We suggest that this task may be effectively separated into a sequential combination of two different tasks. The first of these tasks is syntactic abstraction over phenomena such as expression of tense, negation, modality, and passive versus active voice, which are all either expressed or implied from syntactic structure. The second task is semantic interpretation over the syntactic abstraction, deriving quantification, grounding, etc. Current semantic parsers (such as Boxer (Bos, 2008)) tackle these tasks simultaneously, mixing syntactic and semantic issues in a single framework. We believe that separating semantic parsing into two well defined tasks will help to better target and identify challenges in syntactic and semantic domains. Challenges which are often hidden due to the one-step architecture of current parsers. Many of today’s semantic parsers, and semantic applications in general, leverage dependency parsing (De Marneffe and Manning, 2008a) as an abstraction layer, since it directly represents syntactic dependency relations between predicates and arguments. Some s</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Proceedings of the 2008 Conference on Semantics in Text Processing, pages 277–286. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CONLL,</booktitle>
<pages>152--164</pages>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of CONLL, pages 152–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Stanford typed dependencies manual.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008a. Stanford typed dependencies manual. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on CrossFramework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008b. The stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on CrossFramework and Cross-Domain Parser Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Y Chai</author>
</authors>
<title>Semantic role labeling of implicit arguments for nominal predicates.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>4</issue>
<marker>Gerber, Chai, 2012</marker>
<rawString>Matthew Gerber and Joyce Y Chai. 2012. Semantic role labeling of implicit arguments for nominal predicates. Computational Linguistics, 38(4):755–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>Discourse representation theory.</title>
<date>1988</date>
<booktitle>In Natural Language at the computer,</booktitle>
<pages>84--111</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1421" citStr="Kamp, 1988" startWordPosition="188" endWordPosition="189">her benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing. 1 Introduction Parsers for semantic formalisms (such as Neodavidsonian (Artzi and Zettlemoyer, 2013) and DRT (Kamp, 1988)) take unstructured natural language text as input, and output a complete semantic representation, aiming to capture the meaning conveyed by the text. We suggest that this task may be effectively separated into a sequential combination of two different tasks. The first of these tasks is syntactic abstraction over phenomena such as expression of tense, negation, modality, and passive versus active voice, which are all either expressed or implied from syntactic structure. The second task is semantic interpretation over the syntactic abstraction, deriving quantification, grounding, etc. Current s</context>
<context position="14407" citStr="Kamp, 1988" startWordPosition="2190" endWordPosition="2191">an it is to assign semantic roles. At the same time, as described in Section 3, we capture many relations that are currently not annotated in resources such as FrameNet, and provide a comprehensive set of propositions present in the sentence (either explicitly or implicitly) as well as the relations between them – an objective which is not trivial even when presented with full semantic representation. Compared to more fine-grained semantic representations used in semantic-parsers (i.e. lambdacalculus (Zettlemoyer and Collins, 2005), neodavidsonian semantics (Artzi and Zettlemoyer, 2013), DRT (Kamp, 1988) or the DCS representation of Liang (2011)), we do not attempt to tackle quantification, nor to ground the arguments and predicates to a concrete domain-model or ontology. These important tasks are orthogonal to our representation, and we believe that semanticparsers can benefit from our proposal by using it as input in addition to or instead of the raw sentence text – quantification, binding and grounding are hard enough without needing to deal with the subtleties of syntax or the identification of implicit propositions. 5 Conclusion and Future Work We proposed an intermediate semantic repres</context>
</contexts>
<marker>Kamp, 1988</marker>
<rawString>Hans Kamp. 1988. Discourse representation theory. In Natural Language at the computer, pages 84–111. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>Propbank: the next level of treebank.</title>
<date>2003</date>
<booktitle>In Proceedings of Treebanks and lexical Theories,</booktitle>
<volume>3</volume>
<contexts>
<context position="12021" citStr="Kingsbury and Palmer, 2003" startWordPosition="1819" endWordPosition="1822"> propositional modifiers (e.g., relative clause modifiers), propositional arguments (such as ”John asserted that he will go home”), conditionals, and the canonicalization of passive and active voice. 4 Relation to Other Representations Our proposed representation is intended to serve as a bridging layer between purely syntactic representations such as dependency trees, and semantic oriented applications. In particular, we explicitly represent many semantic relations expressed in a sentence that are not captured by contemporary proposition-directed semantic representations (Baker et al., 1998; Kingsbury and Palmer, 2003; Meyers et al., 2004; Carreras and M`arquez, 2005). Compared to dependency-based representations such as Stanford-dependency trees (De Marneffe 1A case of conjunctions requiring special treatment is introduced by reciprocals, in which the entities roles are exchangeable. For example: “John and Mary bet against each other on future rates” (adaption of wsj 0117). 2Care needs to be taken to distinguish from cases such as “going to Italy” in which “going to” is not followed by a verbal predicate. 68 and Manning, 2008b), we abstract away over many syntactic details (e.g., the myriad of ways of exp</context>
</contexts>
<marker>Kingsbury, Palmer, 2003</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Treebanks and lexical Theories, volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>590--599</pages>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of ACL, pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>Nomlex: A lexicon of nominalizations.</title>
<date>1998</date>
<booktitle>In Proceedings of EURALEX,</booktitle>
<volume>98</volume>
<pages>187--193</pages>
<contexts>
<context position="10106" citStr="Macleod et al., 1998" startWordPosition="1523" endWordPosition="1526">d (which is canonicalized to the representation of copular sentences) 2. Pierre Vinken will join the board as a nonexecutive director Nov. 29. Adjectives, as in the sentence “you emphasized the high prevalence of mental illness” (wsj 0105). Here an adjective is used to describe a definite subject and introduces another proposition, namely the high prevalence of mental illness. Nominalizations, for instance in the sentence “Googles acquisition of Waze occurred yesterday”, introduce the implicit proposition that “Google acquired Waze”. Such propositions were studied and annotated in the NOMLEX (Macleod et al., 1998) and NOMBANK (Meyers et al., 2004) resources. It remains an open issue how to represent or distinguish cases in which nominalization introduce an underspecified proposition. For example, consider “dancing” in “I read a book about dancing”. Possessives, such as “John’s book” introduce the proposition that John has a book. Similarly, examples such as “John’s Failure” combine a possessive construction with nominalization and introduce the proposition that John has failed. Conjunctions - for example in “They operate ships and banks.” (wsj 0083), introduce several propositions in one sentence: 1. T</context>
</contexts>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A lexicon of nominalizations. In Proceedings of EURALEX, volume 98, pages 187–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>Computational linguistics,</booktitle>
<pages>19--2</pages>
<contexts>
<context position="15675" citStr="Marcus et al., 1993" startWordPosition="2388" endWordPosition="2392">aptures both explicit and implicit propositions, while staying relatively close to the syntactic level. We believe that this kind of representation will serve not only as an advantageous input for semantically-centered applications, such as question answering, summarization and information extraction, but also serve as a rich representation layer that can be used as input for systems aiming to provide a finer level of semantic analysis, such as semantic-parsers. We are currently at the beginning of our investigation. In the near future we plan to semiautomatically annotate the Penn Tree Bank (Marcus et al., 1993) with these structures, as well as to provide software for deriving (some of) the implicit and explicit annotations from automatically produced parse-trees. We believe such resources will be of immediate use to semantic-oriented applications. In the longer term, we plan to investigate dedicated algorithms for automatically producing such representation from raw text. The architecture we describe can easily accommodate additional layers of abstraction, by encoding these layers as features of propositions, predicates or arguments. Such layers can include the marking of named entities, the truth </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The nombank project: An interim report.</title>
<date>2004</date>
<booktitle>In HLT-NAACL</booktitle>
<pages>24--31</pages>
<contexts>
<context position="10140" citStr="Meyers et al., 2004" startWordPosition="1529" endWordPosition="1532">presentation of copular sentences) 2. Pierre Vinken will join the board as a nonexecutive director Nov. 29. Adjectives, as in the sentence “you emphasized the high prevalence of mental illness” (wsj 0105). Here an adjective is used to describe a definite subject and introduces another proposition, namely the high prevalence of mental illness. Nominalizations, for instance in the sentence “Googles acquisition of Waze occurred yesterday”, introduce the implicit proposition that “Google acquired Waze”. Such propositions were studied and annotated in the NOMLEX (Macleod et al., 1998) and NOMBANK (Meyers et al., 2004) resources. It remains an open issue how to represent or distinguish cases in which nominalization introduce an underspecified proposition. For example, consider “dancing” in “I read a book about dancing”. Possessives, such as “John’s book” introduce the proposition that John has a book. Similarly, examples such as “John’s Failure” combine a possessive construction with nominalization and introduce the proposition that John has failed. Conjunctions - for example in “They operate ships and banks.” (wsj 0083), introduce several propositions in one sentence: 1. They operate ships 2. They operate </context>
<context position="12042" citStr="Meyers et al., 2004" startWordPosition="1823" endWordPosition="1826">g., relative clause modifiers), propositional arguments (such as ”John asserted that he will go home”), conditionals, and the canonicalization of passive and active voice. 4 Relation to Other Representations Our proposed representation is intended to serve as a bridging layer between purely syntactic representations such as dependency trees, and semantic oriented applications. In particular, we explicitly represent many semantic relations expressed in a sentence that are not captured by contemporary proposition-directed semantic representations (Baker et al., 1998; Kingsbury and Palmer, 2003; Meyers et al., 2004; Carreras and M`arquez, 2005). Compared to dependency-based representations such as Stanford-dependency trees (De Marneffe 1A case of conjunctions requiring special treatment is introduced by reciprocals, in which the entities roles are exchangeable. For example: “John and Mary bet against each other on future rates” (adaption of wsj 0117). 2Care needs to be taken to distinguish from cases such as “going to Italy” in which “going to” is not followed by a verbal predicate. 68 and Manning, 2008b), we abstract away over many syntactic details (e.g., the myriad of ways of expressing tense, negati</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. In HLT-NAACL 2004 workshop: Frontiers in corpus annotation, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2010 task 10: Linking events and their participants in discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>45--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2010. Semeval-2010 task 10: Linking events and their participants in discourse. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 45–50. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Stern</author>
<author>Ido Dagan</author>
</authors>
<title>Recognizing implied predicate-argument relationships in textual inference.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL. Association for Computational Linguistics.</booktitle>
<marker>Stern, Dagan, 2014</marker>
<rawString>Asher Stern and Ido Dagan. 2014. Recognizing implied predicate-argument relationships in textual inference. In Proceedings of ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI,</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<contexts>
<context position="14333" citStr="Zettlemoyer and Collins, 2005" startWordPosition="2178" endWordPosition="2181">automatically to naturally occurring text (perhaps pre-annotated using a syntactic parser) than it is to assign semantic roles. At the same time, as described in Section 3, we capture many relations that are currently not annotated in resources such as FrameNet, and provide a comprehensive set of propositions present in the sentence (either explicitly or implicitly) as well as the relations between them – an objective which is not trivial even when presented with full semantic representation. Compared to more fine-grained semantic representations used in semantic-parsers (i.e. lambdacalculus (Zettlemoyer and Collins, 2005), neodavidsonian semantics (Artzi and Zettlemoyer, 2013), DRT (Kamp, 1988) or the DCS representation of Liang (2011)), we do not attempt to tackle quantification, nor to ground the arguments and predicates to a concrete domain-model or ontology. These important tasks are orthogonal to our representation, and we believe that semanticparsers can benefit from our proposal by using it as input in addition to or instead of the raw sentence text – quantification, binding and grounding are hard enough without needing to deal with the subtleties of syntax or the identification of implicit propositions</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI, pages 658–666. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
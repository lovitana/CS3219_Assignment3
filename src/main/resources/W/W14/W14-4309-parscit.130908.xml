<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051267">
<title confidence="0.9981055">
Improving Classification-Based Natural Language Understanding with
Non-Expert Annotation
</title>
<author confidence="0.994782">
Fabrizio Morbini and Eric Forbell and Kenji Sagae
</author>
<affiliation confidence="0.9974795">
Institute for Creative Technologies
University of Southern California
</affiliation>
<address confidence="0.96779">
Los Angeles, CA 90094, USA
</address>
<email confidence="0.999826">
{morbini,forbell,sagae}@ict.usc.edu
</email>
<sectionHeader confidence="0.997402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999828">
Although data-driven techniques are com-
monly used for Natural Language Under-
standing in dialogue systems, their effi-
cacy is often hampered by the lack of ap-
propriate annotated training data in suffi-
cient amounts. We present an approach
for rapid and cost-effective annotation of
training data for classification-based lan-
guage understanding in conversational di-
alogue systems. Experiments using a web-
accessible conversational character that in-
teracts with a varied user population show
that a dramatic improvement in natural
language understanding and a substantial
reduction in expert annotation effort can
be achieved by leveraging non-expert an-
notation.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99979209375">
Robust Natural Language Understanding (NLU)
remains a challenge in conversational dialogue
systems that allow arbitrary natural language input
from users. Although data-driven approaches are
now commonly used to address the NLU problem
as one of classification, e.g. (Heintze et al., 2010;
Leuski and Traum, 2010; Moreira et al., 2011),
where input utterances are mapped automatically
into system-specific categories, the dependence of
such approaches on training data annotated with
semantic classes or dialogue acts creates a chicken
and egg problem: user utterances are needed to
create the annotated training data necessary for
NLU by classification, but these cannot be col-
lected without a working system that users can in-
teract with.
Common solutions to this problem include the
use of Wizard-of-Oz data collection, where a hu-
man expert manually provides the functionality of
data-driven modules while data is collected from
users, or the use of scenario authors who attempt
to anticipate user input to create an initial set of
training data. While these options offer practical
ways around the training data acquisition prob-
lem, they typically require substantial work from
system experts and provide suboptimal solutions:
data-driven approaches work best when utterances
in the training data are drawn from the same distri-
bution as those encountered in actual system use,
but the conditions under which training data is col-
lected (a human expert filling in for systems mod-
ules, or a human expert generating possible user
utterances) are quite different from those where
users interact with the final system. High qual-
ity results are often obtained through an iterative
process where an initial training set is authored
by a scenario designer, but NLU resources are
gradually updated based on real user data over
time (Gandhe et al., 2011). Although this can ulti-
mately produce training data composed primarily
of real user utterances, and therefore result in bet-
ter performance from data-driven models, an ex-
pert annotator is required to perform manual clas-
sification of user utterances. This is a laborious
process that assumes availability and willingness
of the annotator for as long as it takes to collect
enough user utterances, which may range from
weeks to months or even years, depending on the
size of the domain and the number and type of ut-
terance categories.
The main question we address is whether an-
notation by non-experts can be leveraged to speed
up utterance classification and lower its cost. We
present a technique that frames the annotation of
training data as a human intelligence task suit-
able for crowdsourcing. Although there are sim-
ilarities between our technique and active learning
(e.g. see (Gambck et al., 2011)), an important dif-
ference is that our technique does not reduce the
annotation effort by reducing the size of the data
to be labeled, but by casting the annotation task
into a simpler problem. This allows us to take ad-
vantage of the entire data generated by the users.
Through an experiment with a conversational dia-
</bodyText>
<page confidence="0.98831">
69
</page>
<bodyText confidence="0.899025285714286">
Proceedings of the SIGDIAL 2014 Conference, pages 69–73,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
logue system deployed on the web, we show that a
dramatic improvement in the quality of NLU can
be achieved with non-expert data annotation, re-
ducing the time required of an expert annotator by
70%.
</bodyText>
<sectionHeader confidence="0.767047" genericHeader="method">
2 Improving understanding with data
</sectionHeader>
<bodyText confidence="0.99996912195122">
Our approach for creating accurate utterance clas-
sifiers for NLU in conversational dialogue systems
is based on a simple strategy, which we describe
next in general terms. NLU is assumed to be per-
formed through multiclass classification.
The first step is to create a small initial train-
ing dataset To either through Wizard-of-Oz data
collection or by generation of utterances by a sys-
tem developer or content author. This training set
is used to train a NLU model Mo. Although this
model is likely to be inadequate, it allows users
to interact with an initial version of the system.
As input utterances are collected from real users,
these utterances are annotated with their desired
NLU output labels. Periodically, at time i, we add
to the initial training dataset To the annotated user
utterances accumulated up to that point. We train
a new NLU model Mi using this augmented train-
ing set, Ti.1 We also keep aside a small fraction
of utterances to test the performance of the NLU
models, that is, at each time i we also have an eval-
uation set Ei and the union of Ei and Ti is the en-
tire set of user utterances collected up to time i. As
more utterances are added and annotated, an NLU
model Mi is expected to surpass the initial model
Mo. In general, we replace the running NLU
model Mr whenever we have a better perform-
ing Mi model. This straightforward process can
be used to obtain increasingly more accurate lan-
guage understanding, at the cost of data annotation
in the form of labelling utterances with categories
that are defined according to the needs of the spe-
cific system and the specific domain. The cate-
gories may be based on dialogue acts, e.g. (Core
and Allen, 1997; Bunt et al., 2010), user informa-
tion needs, e.g. (Moreira et al., 2011), or stand
in for entire semantic frames, e.g. (DeVault and
Traum, 2013). The technical nature of the task of
categorizing utterances in schemes such as these
usually means that substantial time is required of
an expert annotator.
</bodyText>
<subsectionHeader confidence="0.996007">
2.1 Annotation as a human intelligence task
</subsectionHeader>
<bodyText confidence="0.999883">
Although the task of annotating NLU training data
involves assigning categories with technical defi-
</bodyText>
<footnote confidence="0.798838">
1For every time i and j with i &lt; j it holds that Ti ⊆ Tj.
</footnote>
<bodyText confidence="0.999946377358491">
nitions to utterances, and therefore would appear
to require knowledge of these technical defini-
tions, in fact the task requires primarily the type
of language understanding that is common to all
native speakers of a given language. Our main hy-
pothesis is that this annotation can be structured
as a trivial task that requires no specific exper-
tise, and that annotations performed this way can
have a substantial impact on the quality of utter-
ance classification. We define the NLU annotation
task as follows.
Before annotation begins, each utterance cate-
gory in the system is associated with one or more
canonical utterance(s) that capture the meaning
and communicative intent of that category. These
canonical utterances allow for trivial generation
of surface text from NLU categories by simple
lookup.
Given a set of dialogues between the system
and individual users, each dialogue is split into
segments. Segments are typically composed of
a user utterance and the preceding system utter-
ance. In cases where users produce multiple utter-
ances without intervening system utterances, seg-
ments may consist of a system utterance followed
by consecutive user utterances, so that there is ex-
actly one segment corresponding to each user ut-
terance u in the dialogue and each segment starts
with a system utterance and ends with u.
Annotators work on one dialogue at a time, and
within a dialogue they are presented with one seg-
ment at a time, following the order in which the
utterances appear in the dialogue. Assignment
of a system category to the utterance targeted in
a segment is a multiple choice task, involving n
choices, one of which is none of the above. The re-
maining n−1 choices are generated automatically
by running the current NLU utterance classifier on
the target utterance, and generating the canonical
surface realizations of each of the n − 1 highest
scoring categories. The annotator is then asked to
choose the option containing language that means
the same as the targeted utterance in the segment.
The choice of n depends on the quality of the cur-
rent NLU model, and must be made to balance the
length of the list of options with the expected num-
ber of times the appropriate choice is not in the list
of options. The more none of the above annota-
tions are performed, the less effective the annota-
tion exercise will be in reducing the effort required
by an annotation expert.
The utterances annotated as none of the above
are given to an expert for annotation. The impact
</bodyText>
<page confidence="0.988871">
70
</page>
<table confidence="0.9696965">
System Hello and welcome.
...
User Are you married?
System Yeah, forty-four years this September. I can’t be-
lieve Linda’s stuck with me this long.
System Okay, let’s get to it. Why are you here today?
User I’ve been having trouble sleeping lately.
System This must be hard for you.
System Are you feeling stressed at all?
User Yes, quite a bit
System So, I’ve got some questions I need to ask you, to get
to know you better.
System Just double checking, you haven’t been sleeping
good. Is that right?
User That’s right
System Have you been having any nightmares?
User Every once in a while, but mostly I can’t stay asleep
...
</table>
<figureCaption confidence="0.907240666666667">
Figure 1: Excerpt of a sample interaction with a
Simcoach character over the web. User input is
typed into a text box on a web page.
</figureCaption>
<bodyText confidence="0.999803363636364">
of this procedure on how quickly user utterances
are annotated as training examples for the NLU
and how much expert time is saved depends on
the quality of the initial classification model and
the choice of n. These are practical factors that are
expected to vary greatly from one dialogue system
to another. In the next section, we present one ap-
plication of this procedure to an existing conversa-
tional dialogue system deployed on the web, and
show examples of dialogue segments and annota-
tion options.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.99998865">
To test our hypothesis that language understand-
ing can be improved with much reduced expert ef-
fort, we applied the framework described above to
a system that implements a conversational char-
acter that talks with users about issues relating
to mental and behavioral disorders and presents
health care options. The system is publicly ac-
cessible at http://www.simcoach.org, and receives
traffic on the order of one hundred users per week.
Of these, about one quarter engage the system in
a meaningful dialogue with multiple turns, with
the dialogues containing on average 16 user utter-
ances. Because our process depends crucially on
user traffic to generate data for annotation, a web-
accessible system is ideally suited for it. An ex-
cerpt from a typical interaction with the system is
shown in Figure 1. The system and the NLU clas-
sifier based on Maximum Entropy models (Berger
et al., 1996) are described respectively in (Rizzo et
al., 2011) and (Sagae et al., 2009).
</bodyText>
<subsectionHeader confidence="0.999043">
3.1 Data collection
</subsectionHeader>
<bodyText confidence="0.999977916666666">
Starting with an initial system deployed with an
NLU model trained with data generated by an au-
thor attempting to anticipate user behavior, we ap-
plied the approach described in section 2 to im-
prove NLU accuracy over a period of approxi-
mately five months. The initial accuracy of the
NLU classifier was 62%, measured as the number
of utterances classified correctly divided by the to-
tal number of user utterances. This accuracy fig-
ure was obtained only after the five months of data
annotation, using the heldout set of manually an-
notated dialogues.
Although the data annotation procedure as de-
scribed in section 2 could in principle be per-
formed continuously as user data come in, we
instead performed all of our annotation in three
rounds, the first consisting of approximately 2,000
user utterances, the second one month later, con-
sisting of an additional 1,000 utterances. The last
round, collected about two months later, contained
about 2,000 utterances. We used five annotators2
working in parallel, and the average speed of each
annotator exceeded 500 utterances per hour.
The total number of NLU utterance classes in
the system is 378, although only 120 classes were
used by annotators in all rounds of annotation to
cover all of the utterances collected3. In our an-
notation exercise we set the number of multiple
choice items at n = 6, including 5 choices gener-
ated from categories chosen by the NLU classifier,
and one none of the above choice. Figure 2 shows
a sample dialogue segment with the corresponding
multiple choice items. During annotation, clicking
on a multiple choice item advances the annotation
by presenting the next segment containing a user
utterance to be annotated.
</bodyText>
<subsectionHeader confidence="0.933516">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999790875">
Of the utterances in the three rounds of data col-
lection, respectively 29%, 34% and 17% were
marked by annotators as none of the above. These
were given to a developer of the NLU system who
assigned a category to each of them. In this ex-
pert annotation step the choice is not restricted to
a small set of options, and may be any of the cat-
egories in the system. Given this rate of use of
</bodyText>
<footnote confidence="0.986402714285714">
2The non-expert annotators belonged to the same team
that developed the system but did not participate in the de-
velopment of the NLU module and the NLU classes used in
the particular dialogue system used.
3This difference is a further evidence of the difficulty of
correctly anticipating how the end users will interact with the
dialogue system.
</footnote>
<page confidence="0.998557">
71
</page>
<bodyText confidence="0.9715675">
System Okay, let’s get to it. Why are you here today?
User I’ve been having trouble sleeping lately.
Which of the following options correspond most
closely to the last user utterance? If none of them have
the same general meaning as the user utterance, select
”none of the above.”
</bodyText>
<figure confidence="0.993838333333333">
(a) I have been in a bad mood lately
(b) I have nightmares often
(c) I haven’t been sleeping well
(d) My family is worried about me
(e) I eat too much
(f) None of the above
</figure>
<figureCaption confidence="0.999182">
Figure 2: Example of a dialogue segment with cor-
</figureCaption>
<bodyText confidence="0.965241780487805">
responding multiple choice items. The annotation
task consists of choosing the item that has approx-
imately the same meaning and communicative in-
tent as the targeted utterance (the user utterance).
the none of the above category, the need for ex-
pert annotation is not eliminated, but the amount
of expert effort necessary is reduced by over 70%.
The NLU classification accuracy figures ob-
tained after each round of annotation are shown in
Table 1. In the table, Our Approach represents the
results obtained by the technique described here.
A large improvement is observed after the first
round of annotation, with a more modest improve-
ment observed after the other two rounds. The ini-
tial jump in accuracy after round 1 is explained
by the fact that the initial model based on a sys-
tem author’s expectation of what users may say to
the system (approximately 3,000 utterances) is im-
proved using utterances that users did in fact pro-
duce in real interactions with the system. Clearly,
a more well-matched distribution of utterances in
the training data produces higher accuracy.
To assess the value of our approach, we com-
pare it with two other reasonable experimental
conditions: a baseline where only expert annota-
tion is used (Expert Only), and a condition where
no expert annotation is used (No Expert). The Ex-
pert Only condition is meant to represent what can
be achieved with the same workload for the expert
used in OurApproach. This is achieved by random
selection of user utterances to create a set with
the same number of utterances set aside for ex-
pert annotation in Our Approach. The expert then
annotates each of these utterances to create train-
ing data. For the No Expert condition, we used
only utterances annotated by non-experts, leaving
out completely utterances labeled as none of the
NLU accuracy after
each annotation round [%]
Base 1st 2nd 3rd
round round round
</bodyText>
<table confidence="0.998756">
Our Approach 62 70 73 78
Expert Only 62 64 68 70
No Expert 62 64 65 71
</table>
<tableCaption confidence="0.998244">
Table 1: NLU accuracy obtained using the initial
</tableCaption>
<bodyText confidence="0.988309615384615">
training dataset T0, after one round of annotation
with T1 (2,013 utterances), after two rounds of an-
notation with T2 (additional 948 utterances), and
after three rounds with T3 (additional 1806 utter-
ances). Accuracy is estimated on the same heldout
set of dialogues E3 for all conditions, accounting
for roughly 10% of the annotated data.
above. Both Expert Only and No Expert condi-
tions achieve significantly lower performance than
the approach described here. This indicates that
expert annotation is important, but also that cheap
and fast non-expert annotation can provide sub-
stantial improvements to NLU.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987285714286">
We described a framework for annotation of train-
ing data by non-experts that can provide dramatic
improvements to natural language understanding
in dialogue systems that perform NLU through ut-
terance classification. Our approach transforms
the annotation NLU training data into a task that
can be performed by anyone with language profi-
ciency. Annotation is structured as a simple mul-
tiple choice task, easily delivered over the web.
Using our approach with a conversational char-
acter on the web, we improved NLU accuracy
from 62% to 78% using only less than 30% of the
effort it would be required of an expert to annotate
data without non-expert annotation.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998823">
We thank Kelly Christoffersen, Nicolai Kalisch
and Tomer Mor-Barak for data annotation and up-
dates to the SimCoach system, David Traum for
insightful discussions, and the anonymous review-
ers. The effort described here has been sponsored
by the U.S. Army. Any opinions, content or infor-
mation presented does not necessarily reflect the
position or the policy of the United States Govern-
ment, and no official endorsement should be in-
ferred.
</bodyText>
<page confidence="0.997491">
72
</page>
<sectionHeader confidence="0.996208" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999550111111111">
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39–71, March.
Harry Bunt, Jan Alexandersson, Jean Carletta, Jae-
Woong Choe, Alex Chengyu Fang, Koiti Hasida,
Kiyong Lee, Volha Petukhova, Andrei Popescu-
Belis, Laurent Romary, Claudia Soria, and David
Traum. 2010. Towards an iso standard for dia-
logue act annotation. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike
Rosner, and Daniel Tapias, editors, Proceedings
of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC’10), Val-
letta, Malta, may. European Language Resources
Association (ELRA).
Mark G. Core and James F. Allen. 1997. Coding di-
alogues with the DAMSL annotation scheme. In
David Traum, editor, Working Notes: AAAI Fall
Symposium on Communicative Action in Humans
and Machines, pages 28–35, Menlo Park, Califor-
nia. AAAI, American Association for Artificial In-
telligence.
David DeVault and David Traum. 2013. A method
for the approximation of incremental understanding
of explicit utterance meaning using predictive mod-
els in nite domains. In Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, GA, June.
Bjrn Gambck, Fredrik Olsson, and Oscar Tckstrm.
2011. Active learning for dialogue act classification.
In INTERSPEECH, pages 1329–1332. ISCA.
Sudeep Gandhe, Michael Rushforth, Priti Aggar-
wal, and David Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In 12th Annual Con-
ference of the International Speech Communication
Association (InterSpeech 2011), Florence, Italy, Au-
gust.
Silvan Heintze, Timo Baumann, and David Schlangen.
2010. Comparing local and sequential models
for statistical incremental natural language under-
standing. In Raquel Fern´andez, Yasuhiro Kata-
giri, Kazunori Komatani, Oliver Lemon, and Mikio
Nakano, editors, SIGDIAL Conference, pages 9–16.
The Association for Computer Linguistics.
Anton Leuski and David R. Traum. 2010. Practical
language processing for virtual humans. In Twenty-
Second Annual Conference on Innovative Applica-
tions of Artificial Intelligence (IAAI-10).
Catarina Moreira, Ana Cristina Mendes, Luisa Coheur,
and Bruno Martins. 2011. Towards the rapid devel-
opment of a natural language understanding mod-
ule. In Proceedings of the 10th International Con-
ference on Intelligent Virtual Agents, IVA’11, pages
309–315, Berlin, Heidelberg. Springer-Verlag.
Albert A. Rizzo, Belinda Lange, John G. Buckwalter,
E. Forbell, Julia Kim, Kenji Sagae, Josh Williams,
Barbara O. Rothbaum, JoAnn Difede, Greg Reger,
Thomas Parsons, and Patrick Kenny. 2011. An in-
telligent virtual human system for providing health-
care information and support. In Studies in Health
Technology and Informatics.
Kenji Sagae, Gwen Christian, David DeVault, and
David R. Traum. 2009. Towards natural language
understanding of partial speech recognition results
in dialogue systems. In Short Paper Proceedings of
the North American Chapter of the Association for
Computational Linguistics - Human Language Tech-
nologies (NAACL HLT) 2009 conference.
</reference>
<page confidence="0.999231">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771686">
<title confidence="0.9983245">Improving Classification-Based Natural Language Understanding Non-Expert Annotation</title>
<author confidence="0.980875">Morbini Forbell</author>
<affiliation confidence="0.997312">Institute for Creative University of Southern</affiliation>
<address confidence="0.998997">Los Angeles, CA 90094,</address>
<abstract confidence="0.988255222222222">Although data-driven techniques are commonly used for Natural Language Understanding in dialogue systems, their efficacy is often hampered by the lack of appropriate annotated training data in sufficient amounts. We present an approach for rapid and cost-effective annotation of training data for classification-based language understanding in conversational dialogue systems. Experiments using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comput. Linguist.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11248" citStr="Berger et al., 1996" startWordPosition="1857" endWordPosition="1860">orders and presents health care options. The system is publicly accessible at http://www.simcoach.org, and receives traffic on the order of one hundred users per week. Of these, about one quarter engage the system in a meaningful dialogue with multiple turns, with the dialogues containing on average 16 user utterances. Because our process depends crucially on user traffic to generate data for annotation, a webaccessible system is ideally suited for it. An excerpt from a typical interaction with the system is shown in Figure 1. The system and the NLU classifier based on Maximum Entropy models (Berger et al., 1996) are described respectively in (Rizzo et al., 2011) and (Sagae et al., 2009). 3.1 Data collection Starting with an initial system deployed with an NLU model trained with data generated by an author attempting to anticipate user behavior, we applied the approach described in section 2 to improve NLU accuracy over a period of approximately five months. The initial accuracy of the NLU classifier was 62%, measured as the number of utterances classified correctly divided by the total number of user utterances. This accuracy figure was obtained only after the five months of data annotation, using th</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Comput. Linguist., 22(1):39–71, March.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Harry Bunt</author>
<author>Jan Alexandersson</author>
<author>Jean Carletta</author>
<author>JaeWoong Choe</author>
</authors>
<title>Alex Chengyu Fang, Koiti Hasida, Kiyong Lee, Volha Petukhova, Andrei PopescuBelis, Laurent Romary, Claudia Soria, and</title>
<date>2010</date>
<booktitle>In Nicoletta Calzolari</booktitle>
<editor>(Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="6104" citStr="Bunt et al., 2010" startWordPosition="977" endWordPosition="980">ntire set of user utterances collected up to time i. As more utterances are added and annotated, an NLU model Mi is expected to surpass the initial model Mo. In general, we replace the running NLU model Mr whenever we have a better performing Mi model. This straightforward process can be used to obtain increasingly more accurate language understanding, at the cost of data annotation in the form of labelling utterances with categories that are defined according to the needs of the specific system and the specific domain. The categories may be based on dialogue acts, e.g. (Core and Allen, 1997; Bunt et al., 2010), user information needs, e.g. (Moreira et al., 2011), or stand in for entire semantic frames, e.g. (DeVault and Traum, 2013). The technical nature of the task of categorizing utterances in schemes such as these usually means that substantial time is required of an expert annotator. 2.1 Annotation as a human intelligence task Although the task of annotating NLU training data involves assigning categories with technical defi1For every time i and j with i &lt; j it holds that Ti ⊆ Tj. nitions to utterances, and therefore would appear to require knowledge of these technical definitions, in fact the </context>
</contexts>
<marker>Bunt, Alexandersson, Carletta, Choe, 2010</marker>
<rawString>Harry Bunt, Jan Alexandersson, Jean Carletta, JaeWoong Choe, Alex Chengyu Fang, Koiti Hasida, Kiyong Lee, Volha Petukhova, Andrei PopescuBelis, Laurent Romary, Claudia Soria, and David Traum. 2010. Towards an iso standard for dialogue act annotation. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogues with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>Working Notes: AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<pages>28--35</pages>
<editor>In David Traum, editor,</editor>
<publisher>AAAI, American Association for Artificial Intelligence.</publisher>
<location>Menlo Park, California.</location>
<contexts>
<context position="6084" citStr="Core and Allen, 1997" startWordPosition="973" endWordPosition="976"> of Ei and Ti is the entire set of user utterances collected up to time i. As more utterances are added and annotated, an NLU model Mi is expected to surpass the initial model Mo. In general, we replace the running NLU model Mr whenever we have a better performing Mi model. This straightforward process can be used to obtain increasingly more accurate language understanding, at the cost of data annotation in the form of labelling utterances with categories that are defined according to the needs of the specific system and the specific domain. The categories may be based on dialogue acts, e.g. (Core and Allen, 1997; Bunt et al., 2010), user information needs, e.g. (Moreira et al., 2011), or stand in for entire semantic frames, e.g. (DeVault and Traum, 2013). The technical nature of the task of categorizing utterances in schemes such as these usually means that substantial time is required of an expert annotator. 2.1 Annotation as a human intelligence task Although the task of annotating NLU training data involves assigning categories with technical defi1For every time i and j with i &lt; j it holds that Ti ⊆ Tj. nitions to utterances, and therefore would appear to require knowledge of these technical defin</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James F. Allen. 1997. Coding dialogues with the DAMSL annotation scheme. In David Traum, editor, Working Notes: AAAI Fall Symposium on Communicative Action in Humans and Machines, pages 28–35, Menlo Park, California. AAAI, American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>David Traum</author>
</authors>
<title>A method for the approximation of incremental understanding of explicit utterance meaning using predictive models in nite domains.</title>
<date>2013</date>
<booktitle>In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Atlanta, GA,</location>
<contexts>
<context position="6229" citStr="DeVault and Traum, 2013" startWordPosition="998" endWordPosition="1001">ected to surpass the initial model Mo. In general, we replace the running NLU model Mr whenever we have a better performing Mi model. This straightforward process can be used to obtain increasingly more accurate language understanding, at the cost of data annotation in the form of labelling utterances with categories that are defined according to the needs of the specific system and the specific domain. The categories may be based on dialogue acts, e.g. (Core and Allen, 1997; Bunt et al., 2010), user information needs, e.g. (Moreira et al., 2011), or stand in for entire semantic frames, e.g. (DeVault and Traum, 2013). The technical nature of the task of categorizing utterances in schemes such as these usually means that substantial time is required of an expert annotator. 2.1 Annotation as a human intelligence task Although the task of annotating NLU training data involves assigning categories with technical defi1For every time i and j with i &lt; j it holds that Ti ⊆ Tj. nitions to utterances, and therefore would appear to require knowledge of these technical definitions, in fact the task requires primarily the type of language understanding that is common to all native speakers of a given language. Our mai</context>
</contexts>
<marker>DeVault, Traum, 2013</marker>
<rawString>David DeVault and David Traum. 2013. A method for the approximation of incremental understanding of explicit utterance meaning using predictive models in nite domains. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, GA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bjrn Gambck</author>
<author>Fredrik Olsson</author>
<author>Oscar Tckstrm</author>
</authors>
<title>Active learning for dialogue act classification.</title>
<date>2011</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1329--1332</pages>
<publisher>ISCA.</publisher>
<contexts>
<context position="3701" citStr="Gambck et al., 2011" startWordPosition="560" endWordPosition="563"> assumes availability and willingness of the annotator for as long as it takes to collect enough user utterances, which may range from weeks to months or even years, depending on the size of the domain and the number and type of utterance categories. The main question we address is whether annotation by non-experts can be leveraged to speed up utterance classification and lower its cost. We present a technique that frames the annotation of training data as a human intelligence task suitable for crowdsourcing. Although there are similarities between our technique and active learning (e.g. see (Gambck et al., 2011)), an important difference is that our technique does not reduce the annotation effort by reducing the size of the data to be labeled, but by casting the annotation task into a simpler problem. This allows us to take advantage of the entire data generated by the users. Through an experiment with a conversational dia69 Proceedings of the SIGDIAL 2014 Conference, pages 69–73, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics logue system deployed on the web, we show that a dramatic improvement in the quality of NLU can be achieved with non-expert data annota</context>
</contexts>
<marker>Gambck, Olsson, Tckstrm, 2011</marker>
<rawString>Bjrn Gambck, Fredrik Olsson, and Oscar Tckstrm. 2011. Active learning for dialogue act classification. In INTERSPEECH, pages 1329–1332. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudeep Gandhe</author>
<author>Michael Rushforth</author>
<author>Priti Aggarwal</author>
<author>David Traum</author>
</authors>
<title>Evaluation of an integrated authoring tool for building advanced question-answering characters.</title>
<date>2011</date>
<booktitle>In 12th Annual Conference of the International Speech Communication Association (InterSpeech 2011),</booktitle>
<location>Florence, Italy,</location>
<contexts>
<context position="2799" citStr="Gandhe et al., 2011" startWordPosition="412" endWordPosition="415">ns: data-driven approaches work best when utterances in the training data are drawn from the same distribution as those encountered in actual system use, but the conditions under which training data is collected (a human expert filling in for systems modules, or a human expert generating possible user utterances) are quite different from those where users interact with the final system. High quality results are often obtained through an iterative process where an initial training set is authored by a scenario designer, but NLU resources are gradually updated based on real user data over time (Gandhe et al., 2011). Although this can ultimately produce training data composed primarily of real user utterances, and therefore result in better performance from data-driven models, an expert annotator is required to perform manual classification of user utterances. This is a laborious process that assumes availability and willingness of the annotator for as long as it takes to collect enough user utterances, which may range from weeks to months or even years, depending on the size of the domain and the number and type of utterance categories. The main question we address is whether annotation by non-experts c</context>
</contexts>
<marker>Gandhe, Rushforth, Aggarwal, Traum, 2011</marker>
<rawString>Sudeep Gandhe, Michael Rushforth, Priti Aggarwal, and David Traum. 2011. Evaluation of an integrated authoring tool for building advanced question-answering characters. In 12th Annual Conference of the International Speech Communication Association (InterSpeech 2011), Florence, Italy, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvan Heintze</author>
<author>Timo Baumann</author>
<author>David Schlangen</author>
</authors>
<title>Comparing local and sequential models for statistical incremental natural language understanding.</title>
<date>2010</date>
<booktitle>SIGDIAL Conference,</booktitle>
<pages>9--16</pages>
<editor>In Raquel Fern´andez, Yasuhiro Katagiri, Kazunori Komatani, Oliver Lemon, and Mikio Nakano, editors,</editor>
<publisher>The Association for Computer Linguistics.</publisher>
<contexts>
<context position="1241" citStr="Heintze et al., 2010" startWordPosition="166" endWordPosition="169"> in conversational dialogue systems. Experiments using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation. 1 Introduction Robust Natural Language Understanding (NLU) remains a challenge in conversational dialogue systems that allow arbitrary natural language input from users. Although data-driven approaches are now commonly used to address the NLU problem as one of classification, e.g. (Heintze et al., 2010; Leuski and Traum, 2010; Moreira et al., 2011), where input utterances are mapped automatically into system-specific categories, the dependence of such approaches on training data annotated with semantic classes or dialogue acts creates a chicken and egg problem: user utterances are needed to create the annotated training data necessary for NLU by classification, but these cannot be collected without a working system that users can interact with. Common solutions to this problem include the use of Wizard-of-Oz data collection, where a human expert manually provides the functionality of data-d</context>
</contexts>
<marker>Heintze, Baumann, Schlangen, 2010</marker>
<rawString>Silvan Heintze, Timo Baumann, and David Schlangen. 2010. Comparing local and sequential models for statistical incremental natural language understanding. In Raquel Fern´andez, Yasuhiro Katagiri, Kazunori Komatani, Oliver Lemon, and Mikio Nakano, editors, SIGDIAL Conference, pages 9–16. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anton Leuski</author>
<author>David R Traum</author>
</authors>
<title>Practical language processing for virtual humans.</title>
<date>2010</date>
<booktitle>In TwentySecond Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-10).</booktitle>
<contexts>
<context position="1265" citStr="Leuski and Traum, 2010" startWordPosition="170" endWordPosition="173">logue systems. Experiments using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation. 1 Introduction Robust Natural Language Understanding (NLU) remains a challenge in conversational dialogue systems that allow arbitrary natural language input from users. Although data-driven approaches are now commonly used to address the NLU problem as one of classification, e.g. (Heintze et al., 2010; Leuski and Traum, 2010; Moreira et al., 2011), where input utterances are mapped automatically into system-specific categories, the dependence of such approaches on training data annotated with semantic classes or dialogue acts creates a chicken and egg problem: user utterances are needed to create the annotated training data necessary for NLU by classification, but these cannot be collected without a working system that users can interact with. Common solutions to this problem include the use of Wizard-of-Oz data collection, where a human expert manually provides the functionality of data-driven modules while data</context>
</contexts>
<marker>Leuski, Traum, 2010</marker>
<rawString>Anton Leuski and David R. Traum. 2010. Practical language processing for virtual humans. In TwentySecond Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catarina Moreira</author>
<author>Ana Cristina Mendes</author>
<author>Luisa Coheur</author>
<author>Bruno Martins</author>
</authors>
<title>Towards the rapid development of a natural language understanding module.</title>
<date>2011</date>
<booktitle>In Proceedings of the 10th International Conference on Intelligent Virtual Agents, IVA’11,</booktitle>
<pages>309--315</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1288" citStr="Moreira et al., 2011" startWordPosition="174" endWordPosition="177">ts using a webaccessible conversational character that interacts with a varied user population show that a dramatic improvement in natural language understanding and a substantial reduction in expert annotation effort can be achieved by leveraging non-expert annotation. 1 Introduction Robust Natural Language Understanding (NLU) remains a challenge in conversational dialogue systems that allow arbitrary natural language input from users. Although data-driven approaches are now commonly used to address the NLU problem as one of classification, e.g. (Heintze et al., 2010; Leuski and Traum, 2010; Moreira et al., 2011), where input utterances are mapped automatically into system-specific categories, the dependence of such approaches on training data annotated with semantic classes or dialogue acts creates a chicken and egg problem: user utterances are needed to create the annotated training data necessary for NLU by classification, but these cannot be collected without a working system that users can interact with. Common solutions to this problem include the use of Wizard-of-Oz data collection, where a human expert manually provides the functionality of data-driven modules while data is collected from user</context>
<context position="6157" citStr="Moreira et al., 2011" startWordPosition="986" endWordPosition="989">i. As more utterances are added and annotated, an NLU model Mi is expected to surpass the initial model Mo. In general, we replace the running NLU model Mr whenever we have a better performing Mi model. This straightforward process can be used to obtain increasingly more accurate language understanding, at the cost of data annotation in the form of labelling utterances with categories that are defined according to the needs of the specific system and the specific domain. The categories may be based on dialogue acts, e.g. (Core and Allen, 1997; Bunt et al., 2010), user information needs, e.g. (Moreira et al., 2011), or stand in for entire semantic frames, e.g. (DeVault and Traum, 2013). The technical nature of the task of categorizing utterances in schemes such as these usually means that substantial time is required of an expert annotator. 2.1 Annotation as a human intelligence task Although the task of annotating NLU training data involves assigning categories with technical defi1For every time i and j with i &lt; j it holds that Ti ⊆ Tj. nitions to utterances, and therefore would appear to require knowledge of these technical definitions, in fact the task requires primarily the type of language understa</context>
</contexts>
<marker>Moreira, Mendes, Coheur, Martins, 2011</marker>
<rawString>Catarina Moreira, Ana Cristina Mendes, Luisa Coheur, and Bruno Martins. 2011. Towards the rapid development of a natural language understanding module. In Proceedings of the 10th International Conference on Intelligent Virtual Agents, IVA’11, pages 309–315, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert A Rizzo</author>
<author>Belinda Lange</author>
<author>John G Buckwalter</author>
<author>E Forbell</author>
<author>Julia Kim</author>
<author>Kenji Sagae</author>
<author>Josh Williams</author>
<author>Barbara O Rothbaum</author>
<author>JoAnn Difede</author>
<author>Greg Reger</author>
<author>Thomas Parsons</author>
<author>Patrick Kenny</author>
</authors>
<title>An intelligent virtual human system for providing healthcare information and support.</title>
<date>2011</date>
<booktitle>In Studies in Health Technology and Informatics.</booktitle>
<contexts>
<context position="11299" citStr="Rizzo et al., 2011" startWordPosition="1865" endWordPosition="1868">is publicly accessible at http://www.simcoach.org, and receives traffic on the order of one hundred users per week. Of these, about one quarter engage the system in a meaningful dialogue with multiple turns, with the dialogues containing on average 16 user utterances. Because our process depends crucially on user traffic to generate data for annotation, a webaccessible system is ideally suited for it. An excerpt from a typical interaction with the system is shown in Figure 1. The system and the NLU classifier based on Maximum Entropy models (Berger et al., 1996) are described respectively in (Rizzo et al., 2011) and (Sagae et al., 2009). 3.1 Data collection Starting with an initial system deployed with an NLU model trained with data generated by an author attempting to anticipate user behavior, we applied the approach described in section 2 to improve NLU accuracy over a period of approximately five months. The initial accuracy of the NLU classifier was 62%, measured as the number of utterances classified correctly divided by the total number of user utterances. This accuracy figure was obtained only after the five months of data annotation, using the heldout set of manually annotated dialogues. Alth</context>
</contexts>
<marker>Rizzo, Lange, Buckwalter, Forbell, Kim, Sagae, Williams, Rothbaum, Difede, Reger, Parsons, Kenny, 2011</marker>
<rawString>Albert A. Rizzo, Belinda Lange, John G. Buckwalter, E. Forbell, Julia Kim, Kenji Sagae, Josh Williams, Barbara O. Rothbaum, JoAnn Difede, Greg Reger, Thomas Parsons, and Patrick Kenny. 2011. An intelligent virtual human system for providing healthcare information and support. In Studies in Health Technology and Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Gwen Christian</author>
<author>David DeVault</author>
<author>David R Traum</author>
</authors>
<title>Towards natural language understanding of partial speech recognition results in dialogue systems.</title>
<date>2009</date>
<booktitle>In Short Paper Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)</booktitle>
<pages>conference.</pages>
<contexts>
<context position="11324" citStr="Sagae et al., 2009" startWordPosition="1870" endWordPosition="1873"> http://www.simcoach.org, and receives traffic on the order of one hundred users per week. Of these, about one quarter engage the system in a meaningful dialogue with multiple turns, with the dialogues containing on average 16 user utterances. Because our process depends crucially on user traffic to generate data for annotation, a webaccessible system is ideally suited for it. An excerpt from a typical interaction with the system is shown in Figure 1. The system and the NLU classifier based on Maximum Entropy models (Berger et al., 1996) are described respectively in (Rizzo et al., 2011) and (Sagae et al., 2009). 3.1 Data collection Starting with an initial system deployed with an NLU model trained with data generated by an author attempting to anticipate user behavior, we applied the approach described in section 2 to improve NLU accuracy over a period of approximately five months. The initial accuracy of the NLU classifier was 62%, measured as the number of utterances classified correctly divided by the total number of user utterances. This accuracy figure was obtained only after the five months of data annotation, using the heldout set of manually annotated dialogues. Although the data annotation </context>
</contexts>
<marker>Sagae, Christian, DeVault, Traum, 2009</marker>
<rawString>Kenji Sagae, Gwen Christian, David DeVault, and David R. Traum. 2009. Towards natural language understanding of partial speech recognition results in dialogue systems. In Short Paper Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT) 2009 conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.983313">
Results of the WMT14 Metrics Shared Task
</title>
<author confidence="0.919482">
Matouˇs Mach´aˇcek and Ondˇrej Bojar
</author>
<affiliation confidence="0.9140545">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.916056">
machacekmatous@gmail.com and bojar@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.992687" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999774647058824">
This paper presents the results of the
WMT14 Metrics Shared Task. We asked
participants of this task to score the
outputs of the MT systems involved in
WMT14 Shared Translation Task. We col-
lected scores of 23 metrics from 12 re-
search groups. In addition to that we com-
puted scores of 6 standard metrics (BLEU,
NIST, WER, PER, TER and CDER) as
baselines. The collected scores were eval-
uated in terms of system level correlation
(how well each metric’s scores correlate
with WMT14 official manual ranking of
systems) and in terms of segment level
correlation (how often a metric agrees with
humans in comparing two translations of a
particular sentence).
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996816157894737">
Automatic machine translation metrics play a very
important role in the development of MT systems
and their evaluation. There are many different
metrics of diverse nature and one would like to
assess their quality. For this reason, the Met-
rics Shared Task is held annually at the Workshop
of Statistical Machine Translation1, starting with
Koehn and Monz (2006) and following up to Bo-
jar et al. (2014).
In this task, we asked metrics developers to
score the outputs of WMT14 Shared Translation
Task (Bojar et al., 2014). We have collected the
computed metrics’ scores and use them to evalu-
ate quality of the metrics.
The systems’ outputs, human judgements and
evaluated metrics are described in Section 2. The
quality of the metrics in terms of system level cor-
relation is reported in Section 3. Segment level
correlation with a detailed discussion and a slight
</bodyText>
<footnote confidence="0.867138">
1http://www.statmt.org/wmt13
</footnote>
<bodyText confidence="0.7659835">
change in the calculation compared to the previous
year is reported in Section 4.
</bodyText>
<sectionHeader confidence="0.990399" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999880636363636">
We used the translations of MT systems involved
in WMT14 Shared Translation Task together with
reference translations as the test set for the Met-
rics Task. This dataset consists of 110 systems’
outputs and 10 reference translations in 10 trans-
lation directions (English from and into Czech,
French, German, Hindi and Russian). For most of
the translation directions each system’s output and
the reference translation contain 3003 sentences.
For more details please see the WMT14 overview
paper (Bojar et al., 2014).
</bodyText>
<subsectionHeader confidence="0.827529">
2.1 Manual MT Quality Judgements
</subsectionHeader>
<bodyText confidence="0.999993631578947">
During the WMT14 Translation Task, a large scale
manual annotation was conducted to compare the
systems. We used these collected human judge-
ments for the evalution of the automatic metrics.
The participants in the manual annotation were
asked to evaluate system outputs by ranking trans-
lated sentences relative to each other. For each
source segment that was included in the procedure,
the annotator was shown the outputs of five sys-
tems to which he or she was supposed to assign
ranks. Ties were allowed.
These collected rank labels for each five-tuple
of systems were then interpreted as 10 pairwise
comparisons of systems and used to assign each
system a score that reflects how high that system
was usually ranked by the annotators. Please see
the WMT14 overview paper for details on how this
score is computed. You can also find inter- and
intra-annotator agreement estimates there.
</bodyText>
<subsectionHeader confidence="0.885102">
2.2 Participants of the Metrics Shared Task
</subsectionHeader>
<bodyText confidence="0.811741">
Table 1 lists the participants of WMT14 Shared
Metrics Task, along with their metrics. We have
</bodyText>
<page confidence="0.982731">
293
</page>
<note confidence="0.788405">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293–301,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<author confidence="0.63912">
Metric Participant
</author>
<affiliation confidence="0.879205083333333">
APAC Hokkai-Gakuen University (Echizen’ya, 2014)
BEER ILLC – University of Amsterdam (Stanojevic and Sima’an, 2014)
RED-* Dublin City University (Wu and Yu, 2014)
DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014)
ELEXR University of Tehran (Mahmoudi et al., 2013)
LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014)
METEOR Carnegie Mellon University (Denkowski and Lavie, 2014)
AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014)
PARMESAN Charles University in Prague (Baranˇcikov´a, 2014)
TBLEU Charles University in Prague (Libovick´y and Pecina, 2014)
UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014)
VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014)
</affiliation>
<tableCaption confidence="0.996746">
Table 1: Participants of WMT14 Metrics Shared Task
</tableCaption>
<bodyText confidence="0.886465333333333">
collected 23 metrics from a total of 12 research
groups.
In addition to that we have computed the fol-
lowing two groups of standard metrics as base-
lines:
• Mteval. The metrics BLEU (Papineni
et al., 2002) and NIST (Dodding-
ton, 2002) were computed using the
script mteval-v13a.pl2 which is
used in the OpenMT Evaluation Cam-
paign and includes its own tokeniza-
tion. We run mteval with the flag
--international-tokenization
since it performs slightly better (Mach´aˇcek
and Bojar, 2013).
</bodyText>
<listItem confidence="0.860395">
• Moses Scorer. The metrics TER (Snover
</listItem>
<bodyText confidence="0.72823675">
et al., 2006), WER, PER and CDER (Leusch
et al., 2006) were computed using the Moses
scorer which is used in Moses model opti-
mization. To tokenize the sentences we used
the standard tokenizer script as available in
Moses toolkit.
We have normalized all metrics’ scores such
that better translations get higher scores.
</bodyText>
<sectionHeader confidence="0.952485" genericHeader="method">
3 System-Level Metric Analysis
</sectionHeader>
<bodyText confidence="0.999141777777778">
While the Spearman’s ρ correlation coefficient
was used as the main measure of system-level met-
rics’ quality in the past, we have decided to use
Pearson correlation coefficient as the main mea-
sure this year. At the end of this section we give
reasons for this change.
We use the following formula to compute the
Pearson’s r for each metric and translation direc-
tion:
</bodyText>
<footnote confidence="0.8883485">
2http://www.itl.nist.gov/iad/mig/
/tools/
</footnote>
<equation confidence="0.997773">
H)(Mi − M)
&apos; E1(Hi − �En
H)21(Mi − ¯M)2
(1)
</equation>
<bodyText confidence="0.999807264705882">
where H is the vector of human scores of all sys-
tems translating in the given direction, M is the
vector of the corresponding scores as predicted by
the given metric. H¯ and M¯ are their means re-
spectively.
Since we have normalized all metrics such that
better translations get higher score, we consider
metrics with values of Pearson’s r closer to 1 as
better.
You can find the system-level correlations for
translations into English in Table 2 and for trans-
lations out of English in Table 3. Each row in the
tables contains correlations of a metric in each of
the examined translation directions. The metrics
are sorted by average Pearson correlation coeffi-
cient across translation directions. The best results
in each direction are in bold.
The reported empirical confidence intervals of
system level correlations were obtained through
bootstrap resampling of 1000 samples (confidence
level of 95 %).
As in previous years, a lot of metrics outper-
formed BLEU in system level correlation. In
into-English directions, metric DISCOTK-PARTY-
TUNED has the highest correlation in two lan-
guage directions and it is also the best correlated
metric on average according to both Pearson and
Spearman’s coefficients. The second best corre-
lated metric on average (according to Pearson) is
LAYERED which is also the single best metric
in Hindi-to-English direction. Metrics REDSYS
and REDSYSSENT are quite unstable, they win
in French-to-English and Czech-to-English direc-
tions respectively but they perform very poorly in
</bodyText>
<equation confidence="0.997664">
r = En Hi −
i=1(
</equation>
<page confidence="0.986167">
294
</page>
<bodyText confidence="0.999724833333333">
other directions.
Except METEOR, none of the participants took
part in the last year metrics task. We can there-
fore compare current and last year results only
for METEOR and baseline metrics. METEOR, the
last year winner, performs generally well in some
directions but it horribly suffers when evaluating
translations from non-Latin script (Russian and es-
pecially Hindi). For the baseline metrics the re-
sults are quite similar across the years. In both
years BLEU performs best among baseline met-
rics, closely followed by CDER. NIST is in the
middle of the list in both years. The remaining
baseline metrics TER, WER and PER perform
much worse.
The results into German are markedly lower
and have broader confidence intervals than the re-
sults in other directions. This could be explained
by a very high number (18) of participating sys-
tems of similar quality. Both human judgements
and automatic metrics are negatively affected by
these circumstances. To preserve the reliability of
overall metrics’ performance across languages, we
decided to exclude English-to-German direction
from the average Pearson and Spearman’s corre-
lation coefficients.
In other out-of-English directions, the best cor-
related metric on average according to Pearson co-
efficient is NIST, even though it does not win in
any single direction. CDER is the second best ac-
cording to Pearson and the best metric according
to Spearman’s. Again it does not win in any single
direction. The metrics PER and WER are quite
unstable. Each of them wins in two directions but
performs very badly in others.
Compared to the last year results, the order of
metrics participating in both years is quite simi-
lar: NIST and CDER performed very well both
years, followed by BLEU. The metrics TER and
WER are again at the end of the list. An interest-
ing change is that PER perform much better this
year.
</bodyText>
<subsectionHeader confidence="0.8886015">
3.1 Reasons for Pearson correlation
coefficient
</subsectionHeader>
<bodyText confidence="0.99996196">
In the translation task, there are often similar sys-
tems with human scores very close to each other. It
can therefore easily happen that even a good met-
ric compares two similar systems differently from
humans. We believe that the penalty incurred by
the metric for such a swap should somehow reflect
that the systems were hard to separate.
Since the Spearman’s p converts both human
and metric scores to ranks and therefore disregards
the absolute differences in the scores, it does ex-
actly what we feel is not fair. The Pearson corre-
lation coefficient does not suffer from this prob-
lem. We are aware of the fact that Pearson cor-
relation coefficient also reflects whether the rela-
tion between manual and automatic scores is lin-
ear (as opposed to e.g. quadratic). We don’t think
this would be negatively affecting any of the met-
rics since overall, the systems are of a comparable
quality and the metrics are likely to behave lin-
early in this small range of scores.
Moreover, the general agreement to adopt Pear-
son instead of Spearman’s correlation coefficient
was already apparent during the WMT12 work-
shop. This change just did not get through for
WMT13.
</bodyText>
<sectionHeader confidence="0.973689" genericHeader="method">
4 Segment-Level Metric Analysis
</sectionHeader>
<bodyText confidence="0.999993941176471">
We measure the quality of metrics’ segment-level
scores using Kendall’s T rank correlation coeffi-
cient. In this type of evaluation, a metric is ex-
pected to predict the result of the manual pairwise
comparison of two systems. Note that the golden
truth is obtained from a compact annotation of five
systems at once, while an experiment with text-to-
speech evaluation techniques by Vazquez-Alvarez
and Huckvale (2002) suggests that a genuine pair-
wise comparison is likely to lead to more stable
results.
In the past, slightly different variations of
Kendall’s T computation were used in the Metrics
Tasks. Also some of the participants have noticed
a problem with ties in the WMT13 method. There-
fore, we discuss several possible variants in detail
in this paper.
</bodyText>
<subsectionHeader confidence="0.649742">
4.1 Notation for Kendall’s T computation
</subsectionHeader>
<bodyText confidence="0.987897">
The basic formula for Kendall’s T is:
</bodyText>
<equation confidence="0.985228">
|Concordant |− |Discordant|
T = ( 2)
|Concordant |+ |Discordant|
</equation>
<bodyText confidence="0.999992571428571">
where Concordant is the set of all human com-
parisons for which a given metric suggests the
same order and Discordant is the set of all human
comparisons for which a given metric disagrees.
In the original Kendall’s T, comparisons with hu-
man or metric ties are considered neither concor-
dant nor discordant. However in the past, Metrics
</bodyText>
<page confidence="0.990401">
295
</page>
<figure confidence="0.997967916666667">
Spearman’s
.894 ±.047
.856 ±.046
.868 ±.045
.857 ±.046
.841 ±.056
.833 ± .058
.807 ±.050
.786 ±.047
.736 ±.058
.698 ±.047
Average
.912 ±.043
.807 ±.049
.652 ± .046
.728 ± .051
.774 ±.046
.771 ±.043
o.901 ±.045
.746 ±.057
o.788 ±.046
o.824 ± .055
o.858 ±.044
o.855 ± .056
Pearson Correlation Coefficient
8 13 9 5 13
.977 ±.009 .943 ± .020 .956 ± .007 .975 ± .031 .870 ± .022 .944 ± .018
.973 ±.009 .893 ± .026 .976 ± .006 .941 ± .045 .854 ± .023 .927 ± .022
.970 ±.010 .921 ± .024 .862 ± .015 .983 ± .025 .856 ± .023 .918 ± .019
.968 ±.010 .915 ± .025 .898 ± .013 .948 ± .040 .837 ± .024 .913 ± .022
.959 ±.011 .867 ± .029 .920 ± .011 .934 ± .050 .848 ± .024 .906 ± .025
.959 ±.011 .854 ± .031 .927 ± .010 .938 ± .048 .842 ± .024 .904 ± .025
.952 ±.012 .832 ± .034 .954 ± .007 .957 ± .040 .803 ± .027 .900 ± .024
.954 ±.012 .823 ± .034 .826 ± .016 .965 ± .035 .802 ± .027 .874 ± .025
.955 ±.011 .811 ± .035 .784 ± .016 .983 ± .025 .800 ± .027 .867 ± .023
.975 ±.009 .927 ± .022 .457 ± .027 .980 ± .029 .805 ± .026 .829 ± .023
.952 ±.012 .762 ± .038 .610 ± .021 .974 ± .033 .809 ± .027 .821 ± .026
.946 ±.013 .867 ± .031 .411 ± .025 .883 ± .063 .799 ± .028 .781 ± .032
where the Spearman’s p average is out of sequence compared to the main Pearson average.
.952 ±.012 .832 ± .034 .956 ± .007 .909 ± .054 .789 ± .027 .888 ± .027
.966 ±.010 .895 ± .027 .914 ± .010 .824 ± .073 .812 ± .026 .882 ± .029
fr-en de-en hi-en cs-en ru-en Average
.953 ±.012 .823 ± .035 .959 ± .007 .946 ± .044 .787 ± .028 .894 ± .025
.963 ±.010 .817 ± .034 .790 ± .016 .982 ± .026 .816 ± .026 .874 ± .022
Table 2: System-level correlations of automatic evaluation metrics and the official WMT human scores when translating into English. The symbol “o” indicates
.971 ±.009 .857 ± .031 .535 ± .026 .945 ± .044 −.404 ± .045 .581 ± .031
.948 ±.012 .910 ± .026 .506 ± .026 .744 ± .095 .797 ± .027 .781 ± .037
.952 ±.012 .775 ± .038 .618 ± .021 .976 ± .031 .809 ± .027 .826 ± .026
.965 ±.011 .935 ± .022 .557 ± .025 .954 ± .038 .791 ± .027 .840 ± .024
.980 ±.008 .910 ± .024 .644 ± .023 .993 ± .018 .807 ± .027 .867 ± .020
.981 ±.008 .898 ± .026 .676 ± .022 .989 ± .021 .814 ± .026 .872 ± .021
Correlation coefficient
DISCOTK-PARTY-TUNED
LAYERED
DISCOTK-PARTY
UPC-STOUT
VERTA-W
VERTA-EQ
CDER
REDSYS
NIST
METEOR
WER
PER
TBLEU
Direction
BLEU NRC
Considered Systems
BLEU
UPC-IPA
ELEXR
AMBER
TER
DISCOTK-LIGHT
REDSYSSENT
APAC
296
.806 ±.039
Average
.696 ±.037
.713 ±.040
.817 ± .041
.785 ±.050
.915 ±.048
.745 ±.035
.850 ±.030
.840 ±.036
o.809 ±.036
Spearman’s
(excl. en-de)
o.962 ±.038
.962 ±.038
o.809 ±.039
.768 ±.036
o.805 ±.039
o.823 ±.037
.799 ± .041
18
.200 ±.046
en-de
n/a
.285 ±.045
.301 ±.044
.208 ±.045
.208 ±.045
.278 ±.045
.324 ±.045
.260 ±.044
.205 ±.046
.239 ±.046
.346 ±.044
.190 ±.047
.216 ±.046
.263 ±.045
.241 ±.045
.357 ±.045
13 12 10 9
.941 ±.022 .981 ± .006 .985 ± .006 .927 ± .012 .959 ± .012
Pearson Correlation Coefficient
en-fr en-hi en-cs en-ru Average
.949 ±.020 .949 ± .010 .982 ± .006 .938 ± .011 .955 ± .012
.954 ±.019 .829 ± .017 .978 ± .007 .931 ± .012 .923 ± .014
.885 ±.029 .962 ± .009 .979 ± .007 .938 ± .011 .941 ± .014
.933 ±.022 .971 ± .007 .974 ± .008 .901 ± .014 .945 ± .013
.932 ±.023 .968 ± .008 .973 ± .008 .912 ± .013 .946 ± .013
.950 ±.020 .940 ± .011 .973 ± .008 .929 ± .012 .948 ± .013
.936 ±.023 .931 ± .011 .988 ± .005 .941 ± .011 .949 ± .013
.937 ±.022 .973 ± .007 .976 ± .007 .915 ± .013 .950 ± .012
.941 ±.021 .975 ± .007 .976 ± .007 .923 ± .013 .954 ± .012
.928 ±.023 .990 ± .004 .972 ± .008 .926 ± .012 .954 ± .012
.960 ±.018 .516 ± .026 .976 ± .007 .932 ± .011 .846 ± .016
AMBER
PER
BLEU NRC
Direction
n/a n/a .962 ± .009 n/a .962 ± .009
.940 ±.021 n/a .969 ± .008 .921 ± .013 .943 ± .014
.940 ±.021 n/a .938 ± .011 .919 ± .013 .933 ± .015
.940 ±.021 n/a n/a n/a .940 ± .021
.941 ±.021 n/a n/a n/a .941 ± .021
UPC-IPA
PARMESAN
WER
TER
TBLEU
APAC
BLEU
METEOR
CDER
NIST
REDSYS
ELEXR
REDSYSSENT
UPC-STOUT
Correlation coefficient
Considered Systems
</figure>
<tableCaption confidence="0.96754">
Table 3: System-level correlations of automatic evaluation metrics and the official WMT human scores when translating out of English. The symbol “o” indicates
</tableCaption>
<bodyText confidence="0.976587">
where the Spearman’s p average is out of sequence compared to the main Pearson average.
</bodyText>
<page confidence="0.931792">
297
</page>
<bodyText confidence="0.994581428571429">
Tasks (Callison-Burch et al. (2012) and earlier),
comparisons with human ties were considered as
discordant.
To easily see which pairs are counted as concor-
dant and which as discordant, we have developed
the following tabular notation. This is for example
the WMT12 method:
</bodyText>
<equation confidence="0.9852432">
Metric
WMT12 &lt; = &gt;
1 -1 -1
X X X
-1 -1 1
</equation>
<bodyText confidence="0.999992944444445">
Given such a matrix Ch,m where h, m E {&lt;, =
, &gt;}3 and a metric we compute the Kendall’s τ the
following way:
We insert each extracted human pairwise com-
parison into exactly one of the nine sets Sh,m ac-
cording to human and metric ranks. For example
the set S&lt;,&gt; contains all comparisons where the
left-hand system was ranked better than right-hand
system by humans and it was ranked the other way
round by the metric in question.
To compute the numerator of Kendall’s τ, we
take the coefficients from the matrix Ch,m, use
them to multiply the sizes of the corresponding
sets Sh,m and then sum them up. We do not in-
clude sets for which the value of Ch,m is X. To
compute the denominator of Kendall’s τ, we sim-
ply sum the sizes of all the sets Sh,m except those
where Ch,m = X. To define it formally:
</bodyText>
<equation confidence="0.997312666666667">
E Ch,m|Sh,m|
h,mE{&lt;,=,&gt;}
Ch,m7 X
�
h,mE{&lt;,=,&gt;}
Ch,m7 X
</equation>
<subsectionHeader confidence="0.970625">
4.2 Discussion on Kendall’s τ computation
</subsectionHeader>
<bodyText confidence="0.999859333333333">
In 2013, we thought that metric ties should not be
penalized and we decided to excluded them like
the human ties. We will denote this method as
WMT13:
It turned out, however, that it was not a good idea:
metrics could game the scoring by avoiding hard
</bodyText>
<footnote confidence="0.7522695">
3Here the relation &lt; always means “is better than” even
for metrics where the better system receives a higher score.
</footnote>
<bodyText confidence="0.95369875">
cases and assigning lots of ties. A natural solution
is to count the metrics ties also in denominator to
avoid the problem. We will denote this variant as
WMT14:
</bodyText>
<equation confidence="0.8726364">
Metric
WMT14 &lt; = &gt;
1 0 -1
X X X
-1 0 1
</equation>
<bodyText confidence="0.999822142857143">
The WMT14 variant does not allow for gaming
the scoring like the WMT13 variant does. Com-
pared to WMT12 method, WMT14 does not pe-
nalize ties.
We were also considering to get human ties in-
volved. The most natural variant would be the fol-
lowing variant denoted as HTIES:
</bodyText>
<equation confidence="0.8968795">
Metric
HTIES
&lt;
&gt;
</equation>
<bodyText confidence="0.999888">
Unfortunately this method allows for gaming the
scoring as well. The least risky choice for metrics
in hard cases would be to assign a tie because it
cannot worsen the Kendall’s τ and there is quite a
high chance that the human rank is also a tie. Met-
rics could be therefore tuned to predict ties often
but such metrics are not very useful. For example,
the simplistic metric which assigns the same score
to all candidates (and therefore all pairs would be
tied by the metric) would get the score equal to
the proportion of ties in all human comparisons. It
would become one of the best performing metrics
in WMT13 even though it is not informative at all.
We have decided to use WMT14 variant as the
main evaluation measure this year, however, we
are also reporting average scores computed by
other variants.
</bodyText>
<subsectionHeader confidence="0.998164">
4.3 Kendall’s τ results
</subsectionHeader>
<bodyText confidence="0.999972545454546">
The final Kendall’s τ results are shown in Table 4
for directions into English and in Table 5 for di-
rections out of English. Each row in the tables
contains correlations of a metric in given direc-
tions. The metrics are sorted by average corre-
lation across translation directions. The highest
correlation in each column is in bold. The ta-
bles also contain average Kendall’s τ computed by
other variants including the variant WMT13 used
last year. Metrics which did not compute scores in
all directions are at the bottom of the tables. The
</bodyText>
<figure confidence="0.736476304347826">
Metric
WMT13
&lt;
&lt; = &gt;
1 X -1
X X X
&gt;
Human
-1 X 1
&lt;
Human
&gt;
τ =
(3)
|Sh,m|
&lt;
Human
&gt;
Human
&lt; = &gt;
1 0 -1
0 1 0
-1 0 1
</figure>
<page confidence="0.949039">
298
299
</page>
<figure confidence="0.8376838125">
WMT12 WMT13 HTIES
.235 ±.011 .289 ±.011 .256 ±.009
.314 ± .011 .320 ± .011 .272 ±.009
.217 ±.011 .285 ±.011 .252 ±.008
.246 ± .010 .273 ±.011 .257 ± .008
.300 ± .010 .306 ±.011 .256 ± .008
.235 ± .010 2.273 ±.010 2.257 ± .008
.248 ± .010 .271 ±.011 .256 ± .008
.292 ±.011 2.308 ±.011 2.259 ± .008
2.232 ± .011 .280 ±.011 .246 ±.009
Averages of other variants of Kendall’s T
.269 ±.011 .303 ±.011 .266 ±.009
2.249 ± .010 .272 ±.010 .256 ± .008
33350 54660 28120 55900 28960
.261 ±.012 .202 ±.009 .234 ± .013 .297 ± .009 .391 ± .012 .277 ± .011
en-fr en-de en-hi en-cs en-ru Avg
.264 ±.012 .227 ±.009 .286 ±.012 .302 ± .009 .397 ± .013 .295 ± .011
.279 ±.011 .234 ±.008 n/a .282 ± .009 .425 ± .013 .305 ± .011
METEOR
APAC
BEER
SENTBLEU
UPC-IPA
AMBER
Direction
REDSENT
REDSYSSENT
UPC-STOUT
BLEU NRC
REDCOMBSYSSENT
Extracted-pairs
REDCOMBSENT
</figure>
<figureCaption confidence="0.779772666666667">
Table 4: Segment-level Kendall’s T correlations of automatic evaluation metrics and the official WMT human judgements when translating into English. The
last three columns contain average Kendall’s T computed by other variants. The symbol “T&apos; indicates where the averages of other variants are out of sequence
compared to the WMT14 variant.
</figureCaption>
<table confidence="0.834473333333333">
.283 ±.011 .313 ±.011 2.273 ± .008
.292 ±.012 .268 ±.009 .250 ± .013 .344 ± .009 .440 ± .013 .319 ± .011
.280 ±.012 .238 ±.009 .264 ± .012 .318 ± .009 .427 ± .012 .306 ± .011
</table>
<tableCaption confidence="0.509334">
Table 5: Segment-level Kendall’s T correlations of automatic evaluation metrics and the official WMT human judgements when translating out of English. The
last three columns contain average Kendall’s T computed by other variants. The symbol “Y&apos; indicates where the averages of other variants are out of sequence
compared to the WMT14 variant.
</tableCaption>
<table confidence="0.983780073170732">
WMT12 WMT13 HTIES
Averages of other variants of Kendall’s T
.358±.013 .363±.013 2.318±.011
.346±.013 .360±.013 .317±.011
.346±.013 .359±.013 .316±.010
−.996±.001 2.676±.256 2.211±.005
.234±.014 .234±.014 .184±.011
.267±.014 .303±.014 .271±.011
.302±.013 .321±.014 2.286±.011
.320±.014 2.342±.014 2.304±.011
.336 ± .013 .339 ± .013 .294 ±.011
.334 ± .013 .349 ± .013 .308 ±.010
.335 ± .013 .350 ± .013 .309 ±.010
.386 ± .013 .386 ±.013 .306 ±.010
.341±.013 .359±.013 2.317±.010
2.340 ±.014 .343 ±.014 .300 ±.011
2.332 ±.013 .332 ±.013 .263 ±.011
.243 ± .014 .290 ± .014 .261 ±.011
2.323±.013 .341±.013 .302±.011
.258 ± .014 .293 ± .014 .264 ±.011
fr-en de-en hi-en cs-en ru-en Avg
.417±.013 .337±.014 .438±.013 .284 ± .016 .333 ± .011 .362 ± .013
.406±.012 .338±.014 .417±.013 .284 ± .015 .336 ± .011 .356 ± .013
.408±.012 .338±.014 .416±.013 .282 ± .014 .336 ± .011 .356 ± .013
26090 25260 20900 21130 34460
.433 ±.012 .380 ±.013 .434 ± .013 .328 ± .015 .355 ± .011 .386 ± .013
.406 ±.012 .334 ±.014 .420 ± .013 .282 ± .015 .329 ± .010 .354 ± .013
.005 ±.001 .001 ±.000 .000 ± .000 .002 ± .001 .001 ± .000 .002 ± .001
.311±.014 .224±.015 .238±.013 .187 ± .016 .209 ± .011 .234 ± .014
.364±.012 .271±.014 .288±.014 .198 ± .016 .276 ± .011 .279 ± .013
.378±.013 .271±.014 .300±.013 .213 ± .016 .263 ± .011 .285 ± .013
.382 ±.013 .272 ±.014 .322 ± .014 .226 ± .016 .269 ± .011 .294 ± .013
.367±.013 .313±.014 .362±.013 .246 ± .016 .294 ± .011 .316 ± .013
.395 ±.013 .334 ±.014 .362 ± .013 .264 ± .016 .305 ± .011 .332 ± .013
.407±.013 .315±.014 .384±.013 .263 ± .015 .312 ± .011 .336 ± .013
.399±.013 .321±.015 .386±.014 .263 ± .015 .315 ± .011 .337 ± .014
.403±.012 .345±.014 .352±.014 .275 ± .015 .317 ± .011 .338 ± .013
.412±.012 .340±.014 .368±.014 .274 ± .015 .316 ± .011 .342 ± .013
.403 ±.012 .336 ±.014 .383 ± .014 .283 ± .015 .323 ± .011 .345 ± .013
.404 ±.012 .338±.014 .386±.014 .283 ± .015 .321 ± .010 .346 ± .013
Direction
</table>
<figure confidence="0.961591384615385">
APAC
BEER
Extracted-pairs
SENTBLEU
BLEU NRC
AMBER
VERTA-EQ
VERTA-W
UPC-STOUT
UPC-IPA
REDSENT
REDSYSSENT
METEOR
DISCOTK-PARTY-TUNED
DISCOTK-LIGHT-KOOL
DISCOTK-LIGHT
DISCOTK-PARTY
REDCOMBSYSSENT
REDCOMBSENT
.253 ±.012 .210 ±.008 .203 ± .012 .292 ± .009 .388 ± .013 .269 ± .011
.291 ±.012 .244 ±.009 n/a n/a n/a .267 ± .010
.264 ±.012 .227 ±.009 n/a .298 ± .009 .426 ± .013 .304 ± .011
.256 ±.012 .191 ±.009 .227 ± .012 .290 ± .009 .381 ± .013 .269 ± .011
.290 ±.012 .242 ±.009 n/a n/a n/a .266 ± .010
.293 ±.012 .242 ±.009 n/a n/a n/a .267 ± .010
.290 ±.012 .239 ±.008 n/a n/a n/a .264 ± .010
</figure>
<bodyText confidence="0.999079363636364">
possible values of T range between -1 (a metric al-
ways predicted a different order than humans did)
and 1 (a metric always predicted the same order as
humans). Metrics with a higher T are better.
We also computed empirical confidence inter-
vals of Kendall’s T using bootstrap resampling.
We varied the “golden truth” by sampling from
human judgments. We have generated 1000 new
sets and report the average of the upper and lower
2.5 % empirical bound, which corresponds to the
95 % confidence interval.
In directions into English (Table 4), the
strongest correlated segment-level metric on av-
erage is DISCOTK-PARTY-TUNED followed by
BEER. Unlike the system level correlation, the
results are much more stable here. DISCOTK-
PARTY-TUNED has the highest correlation in 4 of
5 language directions. Generally, the ranking of
metrics is almost the same in each direction.
The only two metrics which also participated
in last year metrics task are METEOR and SENT-
BLEU. In both years, METEOR performed quite
well unlike SENTBLEU which was outperformed
by most of the metrics.
The metric DISCOTK-LIGHT-KOOL is worth
mentioning. It is deliberately designed to assign
the same score for all systems for most of the
segments. It obtained scores very close to zero
(i.e. totally uninformative) in WMT14 variant. In
WMT13 thought it reached the highest score.
In directions out of English (Table 5), the met-
ric with highest correlation on average across all
directions is BEER, followed by METEOR.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9929155">
In this paper, we summarized the results of the
WMT14 Metrics Shared Task, which assesses the
quality of various automatic machine translation
metrics. As in previous years, human judgements
collected in WMT14 serve as the golden truth and
we check how well the metrics predict the judge-
ments at the level of individual sentences as well
as at the level of the whole test set (system-level).
This year, neither the system-level nor the
segment-level scores are directly comparable to
the previous years. The system-level scores are af-
fected by the change of the underlying interpreta-
tion of the collected judgements in the main trans-
lation task evaluation as well as our choice of Pear-
son coefficient instead of Spearman’s rank corre-
lation. The segment-level scores are affected by
the different handling of ties this year. Despite
somewhat sacrificing the year-to-year comparabil-
ity, we believe all changes are towards a fairer
evaluation and thus better in the long term.
As in previous years, segment-level correlations
are much lower than system-level ones, reaching
at most Kendall’s T of 0.45 for the best performing
metric in its best language pair. So there is quite
some research work to be done. We are happy
to see that many new metrics emerged this year,
which also underlines the importance of the Met-
rics Shared Task.
</bodyText>
<sectionHeader confidence="0.993979" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998614">
This work was supported by the grant FP7-
ICT-2011-7-288487 (MosesCore) of the European
Union. We are grateful to Jacob Devlin and also
Preslav Nakov for pointing out the issue of reward-
ing ties and for further discussion.
</bodyText>
<sectionHeader confidence="0.997927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992905285714286">
Baranˇcikov´a, P. (2014). Parmesan: Improving
Meteor by More Fine-grained Paraphrasing. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, USA. As-
sociation for Computational Linguistics.
Bojar, O., Buck, C., Federmann, C., Haddow, B.,
Koehn, P., Leveling, J., Monz, C., Pecina, P.,
Post, M., Saint-Amand, H., Soricut, R., Specia,
L., and Tamchyna, A. (2014). Findings of the
2014 workshop on statistical machine transla-
tion. In Proceedings of the Ninth Workshop on
Statistical Machine Translation.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada. Association for Compu-
tational Linguistics.
Chen, B. and Cherry, C. (2014). A System-
atic Comparison of Smoothing Techniques for
Sentence-Level BLEU. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
Comelles, E. and Atserias, J. (2014). VERTa par-
ticipation in the WMT14 Metrics Task. In Pro-
ceedings of the Ninth Workshop on Statistical
</reference>
<page confidence="0.972149">
300
</page>
<reference confidence="0.999895510638298">
Machine Translation, Baltimore, USA. Associ-
ation for Computational Linguistics.
Denkowski, M. and Lavie, A. (2014). Meteor Uni-
versal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, USA. Association for
Computational Linguistics.
Doddington, G. (2002). Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research, HLT ’02, pages
138–145, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Echizen’ya, H. (2014). Application of Prize based
on Sentence Length in Chunk-based Automatic
Evaluation of Machine Translation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Gautam, S. and Bhattacharyya, P. (2014). LAY-
ERED: Description of Metric for Machine
Translation Evaluation in WMT14 Metrics
Task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
USA. Association for Computational Linguis-
tics.
Gonz`alez, M., Barr´on-Cede˜no, A., and M`arquez,
L. (2014). IPA and STOUT: Leveraging Lin-
guistic and Source-based Features for Machine
Translation Evaluation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
Guzman, F., Joty, S., M`arquez, L., and Nakov, P.
(2014). DiscoTK: Using Discourse Structure
for Machine Translation Evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical
Machine Translation, Baltimore, USA. Associ-
ation for Computational Linguistics.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween european languages. In Proceedings on
the Workshop on Statistical Machine Transla-
tion, pages 102–121, New York City. Associa-
tion for Computational Linguistics.
Leusch, G., Ueffing, N., and Ney, H. (2006).
CDER: Efficient MT Evaluation Using Block
Movements. In In Proceedings of EACL, pages
241–248.
Libovick´y, J. and Pecina, P. (2014). Tolerant
BLEU: a Submission to the WMT14 Metrics
Task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
USA. Association for Computational Linguis-
tics.
Mach´aˇcek, M. and Bojar, O. (2013). Results of the
WMT13 Metrics Shared Task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 45–51, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Mahmoudi, A., Faili, H., Dehghan, M., and
Maleki, J. (2013). ELEXR: Automatic Evalu-
ation of Machine Translation Using Lexical Re-
lationships. In Castro, F., Gelbukh, A., and
Gonz´alez, M., editors, Advances in Artificial In-
telligence and Its Applications, volume 8265 of
Lecture Notes in Computer Science, pages 394–
405. Springer Berlin Heidelberg.
Papineni, K., Roukos, S., Ward, T., and jing Zhu,
W. (2002). BLEU: a method for automatic eval-
uation of machine translation. pages 311–318.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of translation
edit rate with targeted human annotation. In In
Proceedings of Association for Machine Trans-
lation in the Americas, pages 223–231.
Stanojevic, M. and Sima’an, K. (2014). BEER:
A Smooth Sentence Level Evaluation Metric
with Rich Ingredients. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
Vazquez-Alvarez, Y. and Huckvale, M. (2002).
The reliability of the itu-t p.85 standard for
the evaluation of text-to-speech systems. In
Hansen, J. H. L. and Pellom, B. L., editors, IN-
TERSPEECH. ISCA.
Wu, X. and Yu, H. (2014). RED, The DCU Sub-
mission of Metrics Tasks. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.998917">
301
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.600679">
<title confidence="0.952405">Results of the WMT14 Metrics Shared Task</title>
<author confidence="0.743528">Mach´aˇcek</author>
<affiliation confidence="0.8507655">Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<abstract confidence="0.997672222222222">This paper presents the results of the WMT14 Metrics Shared Task. We asked participants of this task to score the outputs of the MT systems involved in WMT14 Shared Translation Task. We collected scores of 23 metrics from 12 research groups. In addition to that we computed scores of 6 standard metrics (BLEU, NIST, WER, PER, TER and CDER) as baselines. The collected scores were evaluated in terms of system level correlation (how well each metric’s scores correlate with WMT14 official manual ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Baranˇcikov´a</author>
</authors>
<title>Parmesan: Improving Meteor by More Fine-grained Paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<marker>Baranˇcikov´a, 2014</marker>
<rawString>Baranˇcikov´a, P. (2014). Parmesan: Improving Meteor by More Fine-grained Paraphrasing. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>C Buck</author>
<author>C Federmann</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>J Leveling</author>
<author>C Monz</author>
<author>P Pecina</author>
<author>M Post</author>
<author>H Saint-Amand</author>
<author>R Soricut</author>
<author>L Specia</author>
<author>A Tamchyna</author>
</authors>
<title>Findings of the 2014 workshop on statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1317" citStr="Bojar et al. (2014)" startWordPosition="206" endWordPosition="210"> scores correlate with WMT14 official manual ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence). 1 Introduction Automatic machine translation metrics play a very important role in the development of MT systems and their evaluation. There are many different metrics of diverse nature and one would like to assess their quality. For this reason, the Metrics Shared Task is held annually at the Workshop of Statistical Machine Translation1, starting with Koehn and Monz (2006) and following up to Bojar et al. (2014). In this task, we asked metrics developers to score the outputs of WMT14 Shared Translation Task (Bojar et al., 2014). We have collected the computed metrics’ scores and use them to evaluate quality of the metrics. The systems’ outputs, human judgements and evaluated metrics are described in Section 2. The quality of the metrics in terms of system level correlation is reported in Section 3. Segment level correlation with a detailed discussion and a slight 1http://www.statmt.org/wmt13 change in the calculation compared to the previous year is reported in Section 4. 2 Data We used the translati</context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, Tamchyna, 2014</marker>
<rawString>Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. (2014). Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="15938" citStr="Callison-Burch et al. (2012)" startWordPosition="2760" endWordPosition="2763">/a n/a .962 ± .009 n/a .962 ± .009 .940 ±.021 n/a .969 ± .008 .921 ± .013 .943 ± .014 .940 ±.021 n/a .938 ± .011 .919 ± .013 .933 ± .015 .940 ±.021 n/a n/a n/a .940 ± .021 .941 ±.021 n/a n/a n/a .941 ± .021 UPC-IPA PARMESAN WER TER TBLEU APAC BLEU METEOR CDER NIST REDSYS ELEXR REDSYSSENT UPC-STOUT Correlation coefficient Considered Systems Table 3: System-level correlations of automatic evaluation metrics and the official WMT human scores when translating out of English. The symbol “o” indicates where the Spearman’s p average is out of sequence compared to the main Pearson average. 297 Tasks (Callison-Burch et al. (2012) and earlier), comparisons with human ties were considered as discordant. To easily see which pairs are counted as concordant and which as discordant, we have developed the following tabular notation. This is for example the WMT12 method: Metric WMT12 &lt; = &gt; 1 -1 -1 X X X -1 -1 1 Given such a matrix Ch,m where h, m E {&lt;, = , &gt;}3 and a metric we compute the Kendall’s τ the following way: We insert each extracted human pairwise comparison into exactly one of the nine sets Sh,m according to human and metric ranks. For example the set S&lt;,&gt; contains all comparisons where the left-hand system was ran</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. (2012). Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chen</author>
<author>C Cherry</author>
</authors>
<title>A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<contexts>
<context position="4170" citStr="Chen and Cherry, 2014" startWordPosition="650" endWordPosition="653">ges 293–301, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Metric Participant APAC Hokkai-Gakuen University (Echizen’ya, 2014) BEER ILLC – University of Amsterdam (Stanojevic and Sima’an, 2014) RED-* Dublin City University (Wu and Yu, 2014) DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014) ELEXR University of Tehran (Mahmoudi et al., 2013) LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014) METEOR Carnegie Mellon University (Denkowski and Lavie, 2014) AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metrics as baselines: • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13</context>
</contexts>
<marker>Chen, Cherry, 2014</marker>
<rawString>Chen, B. and Cherry, C. (2014). A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Comelles</author>
<author>J Atserias</author>
</authors>
<title>VERTa participation in the WMT14 Metrics Task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<contexts>
<context position="4445" citStr="Comelles and Atserias, 2014" startWordPosition="685" endWordPosition="688">(Wu and Yu, 2014) DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014) ELEXR University of Tehran (Mahmoudi et al., 2013) LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014) METEOR Carnegie Mellon University (Denkowski and Lavie, 2014) AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metrics as baselines: • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl2 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´aˇcek and Bojar, 2013). • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER a</context>
</contexts>
<marker>Comelles, Atserias, 2014</marker>
<rawString>Comelles, E. and Atserias, J. (2014). VERTa participation in the WMT14 Metrics Task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Denkowski</author>
<author>A Lavie</author>
</authors>
<title>Meteor Universal: Language Specific Translation Evaluation for Any Target Language.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<contexts>
<context position="4094" citStr="Denkowski and Lavie, 2014" startWordPosition="639" endWordPosition="642">ave 293 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293–301, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Metric Participant APAC Hokkai-Gakuen University (Echizen’ya, 2014) BEER ILLC – University of Amsterdam (Stanojevic and Sima’an, 2014) RED-* Dublin City University (Wu and Yu, 2014) DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014) ELEXR University of Tehran (Mahmoudi et al., 2013) LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014) METEOR Carnegie Mellon University (Denkowski and Lavie, 2014) AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metrics as baselines: • Mteval. The metrics BLEU (Papineni et al.,</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Denkowski, M. and Lavie, A. (2014). Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research, HLT ’02,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4728" citStr="Doddington, 2002" startWordPosition="736" endWordPosition="738">National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metrics as baselines: • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl2 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´aˇcek and Bojar, 2013). • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses toolkit. We have normalized all metrics’ scores such that better translations get highe</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>Doddington, G. (2002). Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, HLT ’02, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Echizen’ya</author>
</authors>
<title>Application of Prize based on Sentence Length in Chunk-based Automatic Evaluation of Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<marker>Echizen’ya, 2014</marker>
<rawString>Echizen’ya, H. (2014). Application of Prize based on Sentence Length in Chunk-based Automatic Evaluation of Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gautam</author>
<author>P Bhattacharyya</author>
</authors>
<title>LAYERED: Description of Metric for Machine Translation Evaluation in WMT14 Metrics Task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<contexts>
<context position="4032" citStr="Gautam and Bhattacharyya, 2014" startWordPosition="631" endWordPosition="634">ipants of WMT14 Shared Metrics Task, along with their metrics. We have 293 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293–301, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Metric Participant APAC Hokkai-Gakuen University (Echizen’ya, 2014) BEER ILLC – University of Amsterdam (Stanojevic and Sima’an, 2014) RED-* Dublin City University (Wu and Yu, 2014) DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014) ELEXR University of Tehran (Mahmoudi et al., 2013) LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014) METEOR Carnegie Mellon University (Denkowski and Lavie, 2014) AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metr</context>
</contexts>
<marker>Gautam, Bhattacharyya, 2014</marker>
<rawString>Gautam, S. and Bhattacharyya, P. (2014). LAYERED: Description of Metric for Machine Translation Evaluation in WMT14 Metrics Task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gonz`alez</author>
<author>A Barr´on-Cede˜no</author>
<author>L M`arquez</author>
</authors>
<title>IPA and STOUT: Leveraging Linguistic and Source-based Features for Machine Translation Evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<marker>Gonz`alez, Barr´on-Cede˜no, M`arquez, 2014</marker>
<rawString>Gonz`alez, M., Barr´on-Cede˜no, A., and M`arquez, L. (2014). IPA and STOUT: Leveraging Linguistic and Source-based Features for Machine Translation Evaluation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Guzman</author>
<author>S Joty</author>
<author>L M`arquez</author>
<author>P Nakov</author>
</authors>
<title>DiscoTK: Using Discourse Structure for Machine Translation Evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<marker>Guzman, Joty, M`arquez, Nakov, 2014</marker>
<rawString>Guzman, F., Joty, S., M`arquez, L., and Nakov, P. (2014). DiscoTK: Using Discourse Structure for Machine Translation Evaluation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<institution>City. Association for Computational Linguistics.</institution>
<location>New York</location>
<contexts>
<context position="1277" citStr="Koehn and Monz (2006)" startWordPosition="198" endWordPosition="201"> level correlation (how well each metric’s scores correlate with WMT14 official manual ranking of systems) and in terms of segment level correlation (how often a metric agrees with humans in comparing two translations of a particular sentence). 1 Introduction Automatic machine translation metrics play a very important role in the development of MT systems and their evaluation. There are many different metrics of diverse nature and one would like to assess their quality. For this reason, the Metrics Shared Task is held annually at the Workshop of Statistical Machine Translation1, starting with Koehn and Monz (2006) and following up to Bojar et al. (2014). In this task, we asked metrics developers to score the outputs of WMT14 Shared Translation Task (Bojar et al., 2014). We have collected the computed metrics’ scores and use them to evaluate quality of the metrics. The systems’ outputs, human judgements and evaluated metrics are described in Section 2. The quality of the metrics in terms of system level correlation is reported in Section 3. Segment level correlation with a detailed discussion and a slight 1http://www.statmt.org/wmt13 change in the calculation compared to the previous year is reported in</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Koehn, P. and Monz, C. (2006). Manual and automatic evaluation of machine translation between european languages. In Proceedings on the Workshop on Statistical Machine Translation, pages 102–121, New York City. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>CDER: Efficient MT Evaluation Using Block Movements. In</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>241--248</pages>
<contexts>
<context position="5074" citStr="Leusch et al., 2006" startWordPosition="790" endWordPosition="793"> Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metrics as baselines: • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl2 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´aˇcek and Bojar, 2013). • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses toolkit. We have normalized all metrics’ scores such that better translations get higher scores. 3 System-Level Metric Analysis While the Spearman’s ρ correlation coefficient was used as the main measure of system-level metrics’ quality in the past, we have decided to use Pearson correlation coefficient as the main measure this year. At the end of this section we give reasons for this change. We use the following formula to compu</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Leusch, G., Ueffing, N., and Ney, H. (2006). CDER: Efficient MT Evaluation Using Block Movements. In In Proceedings of EACL, pages 241–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Libovick´y</author>
<author>P Pecina</author>
</authors>
<title>Tolerant BLEU: a Submission to the WMT14 Metrics Task.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<marker>Libovick´y, Pecina, 2014</marker>
<rawString>Libovick´y, J. and Pecina, P. (2014). Tolerant BLEU: a Submission to the WMT14 Metrics Task. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mach´aˇcek</author>
<author>O Bojar</author>
</authors>
<title>Results of the WMT13 Metrics Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<marker>Mach´aˇcek, Bojar, 2013</marker>
<rawString>Mach´aˇcek, M. and Bojar, O. (2013). Results of the WMT13 Metrics Shared Task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mahmoudi</author>
<author>H Faili</author>
<author>M Dehghan</author>
<author>J Maleki</author>
</authors>
<title>ELEXR: Automatic Evaluation of Machine Translation Using Lexical Relationships.</title>
<date>2013</date>
<booktitle>Advances in Artificial Intelligence and Its Applications,</booktitle>
<volume>8265</volume>
<pages>394--405</pages>
<editor>In Castro, F., Gelbukh, A., and Gonz´alez, M., editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="3952" citStr="Mahmoudi et al., 2013" startWordPosition="621" endWordPosition="624">e. 2.2 Participants of the Metrics Shared Task Table 1 lists the participants of WMT14 Shared Metrics Task, along with their metrics. We have 293 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293–301, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Metric Participant APAC Hokkai-Gakuen University (Echizen’ya, 2014) BEER ILLC – University of Amsterdam (Stanojevic and Sima’an, 2014) RED-* Dublin City University (Wu and Yu, 2014) DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014) ELEXR University of Tehran (Mahmoudi et al., 2013) LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014) METEOR Carnegie Mellon University (Denkowski and Lavie, 2014) AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups</context>
</contexts>
<marker>Mahmoudi, Faili, Dehghan, Maleki, 2013</marker>
<rawString>Mahmoudi, A., Faili, H., Dehghan, M., and Maleki, J. (2013). ELEXR: Automatic Evaluation of Machine Translation Using Lexical Relationships. In Castro, F., Gelbukh, A., and Gonz´alez, M., editors, Advances in Artificial Intelligence and Its Applications, volume 8265 of Lecture Notes in Computer Science, pages 394– 405. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>jing Zhu</author>
<author>W</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>311--318</pages>
<contexts>
<context position="4700" citStr="Papineni et al., 2002" startWordPosition="730" endWordPosition="733">and Lavie, 2014) AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metrics as baselines: • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl2 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´aˇcek and Bojar, 2013). • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses toolkit. We have normalized all metrics’ scores such that b</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, W, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and jing Zhu, W. (2002). BLEU: a method for automatic evaluation of machine translation. pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation. In</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="5033" citStr="Snover et al., 2006" startWordPosition="782" endWordPosition="785">na (Comelles and Atserias, 2014) Table 1: Participants of WMT14 Metrics Shared Task collected 23 metrics from a total of 12 research groups. In addition to that we have computed the following two groups of standard metrics as baselines: • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl2 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´aˇcek and Bojar, 2013). • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were computed using the Moses scorer which is used in Moses model optimization. To tokenize the sentences we used the standard tokenizer script as available in Moses toolkit. We have normalized all metrics’ scores such that better translations get higher scores. 3 System-Level Metric Analysis While the Spearman’s ρ correlation coefficient was used as the main measure of system-level metrics’ quality in the past, we have decided to use Pearson correlation coefficient as the main measure this year. At the end of this section we give reasons for this chan</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Snover, M., Dorr, B., Schwartz, R., Micciulla, L., and Makhoul, J. (2006). A study of translation edit rate with targeted human annotation. In In Proceedings of Association for Machine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stanojevic</author>
<author>K Sima’an</author>
</authors>
<title>BEER: A Smooth Sentence Level Evaluation Metric with Rich Ingredients.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<marker>Stanojevic, Sima’an, 2014</marker>
<rawString>Stanojevic, M. and Sima’an, K. (2014). BEER: A Smooth Sentence Level Evaluation Metric with Rich Ingredients. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Vazquez-Alvarez</author>
<author>M Huckvale</author>
</authors>
<title>The reliability of the itu-t p.85 standard for the evaluation of text-to-speech systems.</title>
<date>2002</date>
<editor>In Hansen, J. H. L. and Pellom, B. L., editors, INTERSPEECH.</editor>
<publisher>ISCA.</publisher>
<contexts>
<context position="10859" citStr="Vazquez-Alvarez and Huckvale (2002)" startWordPosition="1744" endWordPosition="1747">reover, the general agreement to adopt Pearson instead of Spearman’s correlation coefficient was already apparent during the WMT12 workshop. This change just did not get through for WMT13. 4 Segment-Level Metric Analysis We measure the quality of metrics’ segment-level scores using Kendall’s T rank correlation coefficient. In this type of evaluation, a metric is expected to predict the result of the manual pairwise comparison of two systems. Note that the golden truth is obtained from a compact annotation of five systems at once, while an experiment with text-tospeech evaluation techniques by Vazquez-Alvarez and Huckvale (2002) suggests that a genuine pairwise comparison is likely to lead to more stable results. In the past, slightly different variations of Kendall’s T computation were used in the Metrics Tasks. Also some of the participants have noticed a problem with ties in the WMT13 method. Therefore, we discuss several possible variants in detail in this paper. 4.1 Notation for Kendall’s T computation The basic formula for Kendall’s T is: |Concordant |− |Discordant| T = ( 2) |Concordant |+ |Discordant| where Concordant is the set of all human comparisons for which a given metric suggests the same order and Disc</context>
</contexts>
<marker>Vazquez-Alvarez, Huckvale, 2002</marker>
<rawString>Vazquez-Alvarez, Y. and Huckvale, M. (2002). The reliability of the itu-t p.85 standard for the evaluation of text-to-speech systems. In Hansen, J. H. L. and Pellom, B. L., editors, INTERSPEECH. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wu</author>
<author>H Yu</author>
</authors>
<title>RED, The DCU Submission of Metrics Tasks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA.</location>
<contexts>
<context position="3834" citStr="Wu and Yu, 2014" startWordPosition="604" endWordPosition="607">for details on how this score is computed. You can also find inter- and intra-annotator agreement estimates there. 2.2 Participants of the Metrics Shared Task Table 1 lists the participants of WMT14 Shared Metrics Task, along with their metrics. We have 293 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293–301, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Metric Participant APAC Hokkai-Gakuen University (Echizen’ya, 2014) BEER ILLC – University of Amsterdam (Stanojevic and Sima’an, 2014) RED-* Dublin City University (Wu and Yu, 2014) DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014) ELEXR University of Tehran (Mahmoudi et al., 2013) LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014) METEOR Carnegie Mellon University (Denkowski and Lavie, 2014) AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014) PARMESAN Charles University in Prague (Baranˇcikov´a, 2014) TBLEU Charles University in Prague (Libovick´y and Pecina, 2014) UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014) VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atse</context>
</contexts>
<marker>Wu, Yu, 2014</marker>
<rawString>Wu, X. and Yu, H. (2014). RED, The DCU Submission of Metrics Tasks. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
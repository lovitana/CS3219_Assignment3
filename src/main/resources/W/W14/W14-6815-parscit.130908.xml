<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001966">
<title confidence="0.999222">
Word Segmenter for Chinese Micro-blogging Text Segmentation
 Report for CIPS-SIGHAN’2014 Bakeoff
</title>
<author confidence="0.996152">
Lu Xiang Xiaoqing Li Yu Zhou
</author>
<affiliation confidence="0.986418">
Institute of Automation Chinese Academy of Sciences, Beijing, China
</affiliation>
<email confidence="0.996794">
{lu.xiang, xqli, yzhou}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866153846154">
This paper presents our system for the CIPS-
SIGHAN-2014 bakeoff task of Chinese word
segmentation. This system adopts a character-
based joint approach, which combines a charac-
ter-based generative model and a character-based
discriminative model. To further improve the
performance in cross-domain, an external dic-
tionary is employed. In addition, pre-processing
and post-processing rules are utilized to further
improve the performance. The final performance
on the test corpus shows that our system achieves
comparable results with other state-of-the-art
systems.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943673913044">
Because Chinese text is written without natural
delimiters, word segmentation is a prerequisite
and fundamental task in Chinese natural lan-
guage processing. And many approaches have
been proposed for this task. Among these meth-
ods, the character-based tagging approach (Xue,
2003) has become the prevailing technique for
Chinese word segmentation (CWS) due to its
good performance. In recent years, within the
framework of character-based, much efforts
(Tseng et al., 2005; Zhang et al., 2006; Jiang et
al., 2008) have been made to further improve
word segmentation’s performance.
The character-based joint model (Wang et al.,
2010, Wang et al., 2012) achieves a good bal-
ance between in-vocabulary (IV) words recogni-
tion and out-of-vocabulary (OOV) words identi-
fication. So, in this evaluation task, following
their work we adopt the character-based joint
model as our basic system, which combines a
character-based discriminative model and a char-
acter-based generative model. The generative
module holds a robust performance on IV words,
while the discriminative module can handle the
extra features easily and enhance the OOV words
segmentation.
Because the 2014 SIGHAN bakeoff task of
Chinese Word Segmentation is an opened evalu-
ation task and no training set is provided, the
OOV problem will be more serious. Although
the discriminative module can handle some cases
of OOV, the performance is less preferable if no
technique is utilized. So to further improve the
performance of the basic system and minimize
the OOV, we employ an external dictionary con-
taining a large set of unknown words from dif-
ferent domains. Another notable problem is the
Microblog text segmentation because Microblog
has become a new Internet literary which is dif-
ferent from the genres of common text. To make
our system more robust on Microblog text, we
propose several simple but novel pre-processing
and post-processing approaches in our system.
The final results show that our system per-
forms well on test set and achieves comparable
segmentation results with other participants.
</bodyText>
<sectionHeader confidence="0.995637" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.997574">
2.1 Character-Based Joint Model
</subsectionHeader>
<bodyText confidence="0.9882015">
The character-based joint model in our system
consists of two basic components:
➢ The character-based discriminative model.
➢ The character-based generative model.
The character-based discriminative model
(Xue, 2003) is based on a Maximum Entropy
(ME) framework (Ratnaparkhi, 1998) and can be
formulated as follows:
</bodyText>
<equation confidence="0.996603">
P t1n |c1n k+2F1 P tk  |tk _ 1, ck_2
n
(1)
k=
1
</equation>
<bodyText confidence="0.9999042">
Where is a member of {B, M, E, S}, in which
B, M and E indicate the Beginning, Middle and
End of character in its associated word respec-
tively, and S denotes that it’s a Single-character
word. For example, the word “北京市 (Beijing
</bodyText>
<page confidence="0.933948">
96
</page>
<note confidence="0.694983">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 96–100,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.969381166666667">
City)” will be assigned with the corresponding
tags as: “北 /B (North) 京 /M (Capital) 市 /E
(City)”.
This discriminative model can incorporate ex-
tra features easily and the Maximum Entropy
Modeling Toolkit1 given by Zhang Le is used to
implement the module. In our experiments, this
model is trained with Gaussian prior 1.0 and 600
iterations.
The character-based generative module is a
character-tag-pair-based trigram model (Wang et
al., 2009) and can be expressed as below:
</bodyText>
<equation confidence="0.988518846153846">
P([c,tj�`)�I1P([C,tj; �[�,tj�-2) k  1
Score t
          
 log P c t c t
,  |,
k k k  2

  
   
 P t t c k 2
1 log  |,  
k k  1 k  2
(2)
</equation>
<bodyText confidence="0.998884307692308">
SRI Language Modeling Toolkit 2 (Stolcke,
2002) is used to train the generative trigram
model with modified Kneser-Ney smoothing
(Chen and Goodman, 1998) in our experiments.
The character-based joint model combines the
above discriminative module and the generative
module with log-linear interpolation as follows:
Where the parameter is the
weight for the generative model and can be ob-
tained from the development set. will
be directly used to search for the best sequence.
We set an empirical value 0.4 to as there is no
development-set for various domains.
</bodyText>
<subsectionHeader confidence="0.962455">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.999957">
The feature templates used in the character-based
discriminative model are listed below:
</bodyText>
<equation confidence="0.998765">
((77a,�) C„ (n = —2,-1,0,1,2);
l&amp;quot;) C,, Cn+1(n = —2, —1, 0,1) ,
(C) CA;
(d) T(C2)T(C1)T(Co)T(C1)T(C2)
</equation>
<bodyText confidence="0.993116">
In the above templates, C,, represents a Chi-
</bodyText>
<listItem confidence="0.622508333333333">
nese character and the index n indicates the posi-
tion. For example, when we consider the third
character “奥” in the sequence
template (a) results in the features as following:
C2 =北, C1=京, CO =A, 1=运, 2=会, and
template (b) generates the features as: C_2C 1=北
</listItem>
<equation confidence="0.527262">
C 1C0 =京 A, CoC =奥运, =运会, and
</equation>
<footnote confidence="0.9912725">
1http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html
2 http://www.speech.sri.com/projects/srilm/
</footnote>
<bodyText confidence="0.977420916666667">
template (c) gives the feature =京运.
Template (d) is the feature of character type
and five type classes are defined: dates (“年”,
“月”, “日”, the Chinese character for “year”,
“month” and “day” respectively) represents class
0; foreign alphabets represent class 1; Arabic and
Chinese numbers represent class 2; punctuation
represents class 3 and other characters represent
class 4. For example, when considering the char-
acter “,” in the sequence “八月,阿Q”, the
feature will
be set to “20341”.
</bodyText>
<subsectionHeader confidence="0.998609">
2.3 External Dictionary
</subsectionHeader>
<bodyText confidence="0.999792470588235">
OOV words is a main problem faced by a Chi-
nese word segmenter and it will lead to lower
accuracy if the sentence to be segmented con-
tains many OOV words. To address the problem
of OOV words, we use an external dictionary
containing a large set of predefined words. We
following the method presented in Low et al.
(2005) to use the dictionary. In this method,
some sequence of neighboring characters around
Co will be looked up in a dictionary using max-
imum match strategy. And the longest matching
word W will be chosen. Let be the boundary
tag of in W, L the number of characters in W,
and be the character immediately fol-
lowing (preceding) C0 in the sentence. We then
add the following features derived from the dic-
tionary:
</bodyText>
<equation confidence="0.687512">
(e)Lto
(f)C,,to(n=-1,0,1)
</equation>
<bodyText confidence="0.994864857142857">
For example, consider the sentence “北京奥运
会...”. When processing the current characterCo
“京”, we will try to match the following candi-
dates “京”, “北京”, “京奥”, “北京奥”, “京奥运”,
“北京奥运” and “京奥运会” against existing
word in the external dictionary. Assuming that
both “京奥” and “京奥运” are found in the dic-
tionary, then the longest matching word “京奥运”
will be chosen. And the value of W, , L,C-1
and are “京奥运”, B, 3, “北” and “奥” re-
spectively.
In this work, we collect dictionaries from the
Internet, including the title of Wikipedia3, the
title of Hudong Baike4, Sogou word bank5 and
</bodyText>
<footnote confidence="0.9858115">
3 http://zh.wikipedia.org
4 http://www.baike.com/
</footnote>
<figure confidence="0.340467">
(3)
</figure>
<page confidence="0.99477">
97
</page>
<bodyText confidence="0.990050333333333">
some other internet dictionaries. Finally, we ob-
tain a dictionary containing 5,893,038 words in
our system.
</bodyText>
<subsectionHeader confidence="0.979884">
2.4 Restrictions in Constructing Lattice
</subsectionHeader>
<bodyText confidence="0.9607591875">
When considering a character in the sequence,
we take the type information of both the previous
and the next character into consideration and use
some restrictions to obtain a better tag lattice
(Wang et al., 2010). The restrictions are listed as
follows:
➢ If the previous, the current and the next
characters are all English or numbers, we
would fix the current tag to be “M”;
➢ If the previous and the next characters are
both English or numbers, while the current
character is a connective symbol such as “-”,
“/”, “_”, “\” etc., we would also fix the cur-
rent tag to be “M”;
➢ Otherwise, all four tags {B, E, M, S} would
be given to the current character.
</bodyText>
<sectionHeader confidence="0.996986" genericHeader="method">
3 Rule-based Adaptation
</sectionHeader>
<bodyText confidence="0.999977416666667">
The state-of-the-art Chinese word segmentation
systems can achieve a quite high performance on
well-formed text, while the performance of Mi-
croblog text segmentation is not satisfying due to
the specificity of Microblog text. For example,
there are lots of emotion symbols, URLs, abbre-
viations, consecutive and identical punctuations
and special characters in Microblog text. In order
to make our system more robust on segmenting
Microblog data, we propose some heuristic pre-
processing and post-processing rules to avoid
some segmentation errors.
</bodyText>
<subsectionHeader confidence="0.999695">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999732733333333">
As mentioned above, the Microblog texts contain
much noise like special format words and char-
acters. And such kind of noise will affect the
segmentation performance. In order to remove
these noise, we will pre-process the text before
segmentation.
Since URL, email and consecutive punctua-
tions should be treated as one word and these
content types can be easily recognized using the
regex expressions, we first replace all these con-
tent to special characters before segmentation,
and then restore all the special characters to the
original characters after the segmentation. Table
1 shows the content type we will process in the
pre-processing stage.
</bodyText>
<footnote confidence="0.656451">
5 http://pinyin.sogou.com/dict/
</footnote>
<tableCaption confidence="0.99065">
Table 1: Content type of pre-processing
</tableCaption>
<table confidence="0.9830342">
Type Example
URL http://t.cn/RPdBAPV
Email hanhuahr@126.com
Consecutive punctuations 。。。
!!!!
</table>
<subsectionHeader confidence="0.999229">
3.2 Post-processing
</subsectionHeader>
<bodyText confidence="0.999421">
We use some heuristic rules to further post-
process the results generated by the segmenter
and the rules are described below:
</bodyText>
<listItem confidence="0.995173777777778">
1) Numeral and Quantifier: In our results,
some numerals and quantifiers such as
“两个” and “三张” are segmented as one
unit. But in fact, the numeral and quanti-
fier should be segmented into two words
except some few words like “一个”. So
we use a simple rule to split these cases in
which the previous word is a numeral and
the next word is a quantifier.
2) Continuous mimetic words: There are
many continuous mimetic words in Mi-
croblog, such as “哈哈哈哈哈”, “呵呵
呵” . This kind of words should be treated
as one unit. But our system splits each
character into one word. Hence, we apply
a rule to group the continuous mimetic
words together.
3) Emoticons: some consecutive punctua-
</listItem>
<bodyText confidence="0.929268428571429">
tions like “:-)” represent an emoticon and
have some certain meanings. These emot-
icons should be grouped together. We
have collected a list of emoticons from
the web. For any consecutive punctua-
tions, we join them together as a single
word if they appear in the emoticon list.
</bodyText>
<sectionHeader confidence="0.998401" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99783">
4.1 Data sets
</subsectionHeader>
<bodyText confidence="0.999964285714286">
Since the Chinese word segmentation task focus-
es on the performance of multi-domain, we use
five datasets as our test data. Four of the five da-
tasets are the test data of SIGHAN10 closed
track and the rest one is the 500 Microblog mes-
sages released by SIGHAN12. Hence, our test
data covers 5 domains: Literature (Testing-A,
containing 671 sentences), Computer (Testing-B,
containing 1,330 sentences), Medicine (Testing-
C, containing 1,309 sentences), Finance (Test-
ing-D, containing 561 sentences) and Microblog
(Testing-E, containing 500 sentences). The train-
ing data of our segmenter consists of two parts:
one is the Peking University Corpora (PKU)
</bodyText>
<page confidence="0.989505">
98
</page>
<bodyText confidence="0.840293333333333">
from January to June and the other is manually
annotated Microblog data which contains nearly
7000 sentences.
</bodyText>
<subsectionHeader confidence="0.995245">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999935">
We first evaluate our approach on the five test
datasets using different strategies. The results are
shown in Table 2 and the evaluation criterion is
F-score. The strategies we used are:
</bodyText>
<listItem confidence="0.998255857142857">
• Joint: represents the result of our model
without dictionary.
• +Dic: represents the result of our model us-
ing the external dictionary.
• +Rule: represents the result of our model
using the external dictionary and the pre-
processing and post-processing rules.
</listItem>
<tableCaption confidence="0.998602">
Table 2: Evaluation results with different strategies
</tableCaption>
<table confidence="0.999225833333333">
Joint +Dic +Rule
Testing-A 0.9590 0.9622 0.9628
Testing-B 0.9589 0.9630 0.9634
Testing-C 0.9522 0.9557 0.9557
Testing-D 0.9670 0.9686 0.9696
Testing-E 0.9338 0.9381 0.9412
</table>
<bodyText confidence="0.999691066666667">
As Table 2 shows, our joint model performs
well on all the five datasets even though the do-
main of the training data which is mainly com-
posed of news data is different from the test sets.
This shows that our character-based joint model
is very robust and can achieve a good balance
between in-vocabulary (IV) words recognition
and OOV words identification
After the external dictionary added, the per-
formance increased a lot, which shows the exter-
nal dictionary is very useful and can help allevi-
ate the OOV problem efficiently. Finally, we
adopt the pre-processing and post-processing
rules in our system, the performance can be fur-
ther improved on all testing set except Testing-C.
</bodyText>
<tableCaption confidence="0.998032">
Table 3: Final Result of the Test Set
</tableCaption>
<table confidence="0.993692">
P R F
Final Test 0.9592 0.9566 0.9578
</table>
<bodyText confidence="0.999540142857143">
Since the final test data will be multi-domain,
we add all the five datasets to the training data
and retrain the segmentation model. Then we
apply the retrained model to the final test data
(containing 1,665 sentences) and the perfor-
mance is shown in Table 3. Table 3 shows that
our system can achieve an F-score of 0.9578.
</bodyText>
<sectionHeader confidence="0.997699" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999903714285714">
Our system is based on a character-based joint
model, which combines a generative module and
a discriminative module. In addition, we employ
an external dictionary and propose several pre-
processing and post-processing rules to further
improve the performance. Our system achieves
comparable performance with other participants.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99754675">
We would like to thank Mr. Zhilin Zhang for his
great help in implementation and experiments of
our system. This work is supported by the Hi-
Tech Research and Development Program (863)
of China under grant No. 2012AA011101 and
High New Technology Research and Develop-
ment Program of Xinjiang Uyghur Autonomous
Region under grant No. 201312103.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998907575757576">
Stanley F. Chen and Joshua Goodman, 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University Center for Research in Computing
Technology.
Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu,
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging.
In Proceedings of ACL, pages 897-904.
Adwait Ratnaparkhi, 1998. Maximum entropy models
for natural language ambiguity resolution. Univer-
sity of Pennsylvania.
Andreas Stolcke, 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 311-318.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Dan-
iel Jurafsky and Christopher Manning, 2005. A
Conditional Random Field Word Segmenter for
Sighan Bakeoff 2005. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 168-171.
Kun Wang, Chengqing Zong and Keh-Yih Su, 2009.
Which is more suitable for Chinese word segmen-
tation, the generative model or the discriminative
one? In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computa-
tion (PACLIC23), pages 827-834.
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010.
A Character-Based Joint Model for CIPS-SIGHAN
Word Segmentation Bakeoff 2010. Proceedings of
CIPS-SIGHAN Joint Conference on Chinese Lan-
guage Processing (CLP2010), pages 245-248.
</reference>
<page confidence="0.978642">
99
</page>
<reference confidence="0.99953884">
Kun Wang, Chengqing Zong and Keh-Yih Su, 2010.
A Character-Based Joint Model for Chinese Word
Segmentation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics
(COLING), Beijing, China, August 23-27, 2010.
Pages 1173-1181.
Kun Wang, Chengqing Zong and Keh-Yih Su, 2012.
Integrating generative and discriminative character
based models for Chinese word segmentation.
ACM Transactions on Asian Language Information
Processing (TALIP).
Low, Jin Kiat et al., 2005. A Maximum Entropy Ap-
proach to Chinese Word Segmentation. Proceed-
ings of the Fourth SIGHAN Workshop on Chinese
Language Processing, pages 161-164.
Nianwen Xue, 2003. Chinese Word Segmentation as
Character Tagging. Computational Linguistics and
Chinese Language Processing, 8 (1). pages 29-48.
Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita,
2006. Subword-based Tagging for Confidence de-
pendent Chinese Word Segmentation. In Proceed-
ings of the COLING/ACL, pages 961-968.
Xiaojin Zhu, 2006. Semi-supervised learning litera-
ture survey. Technical Report 1530, Computer Sci-
ences, University of Wisconsin-Madison.
</reference>
<page confidence="0.990784">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.899219">
<title confidence="0.989102">Word Segmenter for Chinese Micro-blogging Text Segmentation for Bakeoff</title>
<author confidence="0.99994">Lu Xiang Xiaoqing Li Yu Zhou</author>
<affiliation confidence="0.999877">Institute of Automation Chinese Academy of Sciences, Beijing,</affiliation>
<email confidence="0.957317">lu.xiang@nlpr.ia.ac.cn</email>
<email confidence="0.957317">xqli@nlpr.ia.ac.cn</email>
<email confidence="0.957317">yzhou@nlpr.ia.ac.cn</email>
<abstract confidence="0.997139857142857">This paper presents our system for the CIPS- SIGHAN-2014 bakeoff task of Chinese word segmentation. This system adopts a characterbased joint approach, which combines a character-based generative model and a character-based discriminative model. To further improve the performance in cross-domain, an external dictionary is employed. In addition, pre-processing and post-processing rules are utilized to further improve the performance. The final performance on the test corpus shows that our system achieves comparable results with other state-of-the-art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<contexts>
<context position="4456" citStr="Chen and Goodman, 1998" startWordPosition="715" endWordPosition="718">the Maximum Entropy Modeling Toolkit1 given by Zhang Le is used to implement the module. In our experiments, this model is trained with Gaussian prior 1.0 and 600 iterations. The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below: P([c,tj�`)�I1P([C,tj; �[�,tj�-2) k  1 Score t             log P c t c t , |, k k k  2          P t t c k 2 1 log |,   k k  1 k  2 (2) SRI Language Modeling Toolkit 2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) in our experiments. The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows: Where the parameter is the weight for the generative model and can be obtained from the development set. will be directly used to search for the best sequence. We set an empirical value 0.4 to as there is no development-set for various domains. 2.2 Features The feature templates used in the character-based discriminative model are listed below: ((77a,�) C„ (n = —2,-1,0,1,2); l&amp;quot;) C,, Cn+1(n = —2, —1, 0,1) , (C) CA; (d) T(C2)T(C1)T(Co)T</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman, 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Lu</author>
</authors>
<title>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>897--904</pages>
<contexts>
<context position="1333" citStr="Jiang et al., 2008" startWordPosition="186" endWordPosition="189">pus shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction Because Chinese text is written without natural delimiters, word segmentation is a prerequisite and fundamental task in Chinese natural language processing. And many approaches have been proposed for this task. Among these methods, the character-based tagging approach (Xue, 2003) has become the prevailing technique for Chinese word segmentation (CWS) due to its good performance. In recent years, within the framework of character-based, much efforts (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008) have been made to further improve word segmentation’s performance. The character-based joint model (Wang et al., 2010, Wang et al., 2012) achieves a good balance between in-vocabulary (IV) words recognition and out-of-vocabulary (OOV) words identification. So, in this evaluation task, following their work we adopt the character-based joint model as our basic system, which combines a character-based discriminative model and a character-based generative model. The generative module holds a robust performance on IV words, while the discriminative module can handle the extra features easily and e</context>
</contexts>
<marker>Jiang, Huang, Liu, Lu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu and Yajuan Lu, 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings of ACL, pages 897-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum entropy models for natural language ambiguity resolution.</title>
<date>1998</date>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3219" citStr="Ratnaparkhi, 1998" startWordPosition="476" endWordPosition="477">. To make our system more robust on Microblog text, we propose several simple but novel pre-processing and post-processing approaches in our system. The final results show that our system performs well on test set and achieves comparable segmentation results with other participants. 2 System Description 2.1 Character-Based Joint Model The character-based joint model in our system consists of two basic components: ➢ The character-based discriminative model. ➢ The character-based generative model. The character-based discriminative model (Xue, 2003) is based on a Maximum Entropy (ME) framework (Ratnaparkhi, 1998) and can be formulated as follows: P t1n |c1n k+2F1 P tk |tk _ 1, ck_2 n (1) k= 1 Where is a member of {B, M, E, S}, in which B, M and E indicate the Beginning, Middle and End of character in its associated word respectively, and S denotes that it’s a Single-character word. For example, the word “北京市 (Beijing 96 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 96–100, Wuhan, China, 20-21 October 2014 City)” will be assigned with the corresponding tags as: “北 /B (North) 京 /M (Capital) 市 /E (City)”. This discriminative model can incorporate extra featur</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi, 1998. Maximum entropy models for natural language ambiguity resolution. University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4350" citStr="Stolcke, 2002" startWordPosition="701" endWordPosition="702">京 /M (Capital) 市 /E (City)”. This discriminative model can incorporate extra features easily and the Maximum Entropy Modeling Toolkit1 given by Zhang Le is used to implement the module. In our experiments, this model is trained with Gaussian prior 1.0 and 600 iterations. The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below: P([c,tj�`)�I1P([C,tj; �[�,tj�-2) k  1 Score t             log P c t c t , |, k k k  2          P t t c k 2 1 log |,   k k  1 k  2 (2) SRI Language Modeling Toolkit 2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) in our experiments. The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows: Where the parameter is the weight for the generative model and can be obtained from the development set. will be directly used to search for the best sequence. We set an empirical value 0.4 to as there is no development-set for various domains. 2.2 Features The feature templates used in the character-based discriminative model are</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke, 2002. SRILM-an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A Conditional Random Field Word Segmenter for Sighan Bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="1292" citStr="Tseng et al., 2005" startWordPosition="178" endWordPosition="181">e. The final performance on the test corpus shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction Because Chinese text is written without natural delimiters, word segmentation is a prerequisite and fundamental task in Chinese natural language processing. And many approaches have been proposed for this task. Among these methods, the character-based tagging approach (Xue, 2003) has become the prevailing technique for Chinese word segmentation (CWS) due to its good performance. In recent years, within the framework of character-based, much efforts (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008) have been made to further improve word segmentation’s performance. The character-based joint model (Wang et al., 2010, Wang et al., 2012) achieves a good balance between in-vocabulary (IV) words recognition and out-of-vocabulary (OOV) words identification. So, in this evaluation task, following their work we adopt the character-based joint model as our basic system, which combines a character-based discriminative model and a character-based generative model. The generative module holds a robust performance on IV words, while the discriminative module c</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky and Christopher Manning, 2005. A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 168-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>Which is more suitable for Chinese word segmentation, the generative model or the discriminative one?</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC23),</booktitle>
<pages>827--834</pages>
<contexts>
<context position="4109" citStr="Wang et al., 2009" startWordPosition="626" endWordPosition="629">d. For example, the word “北京市 (Beijing 96 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 96–100, Wuhan, China, 20-21 October 2014 City)” will be assigned with the corresponding tags as: “北 /B (North) 京 /M (Capital) 市 /E (City)”. This discriminative model can incorporate extra features easily and the Maximum Entropy Modeling Toolkit1 given by Zhang Le is used to implement the module. In our experiments, this model is trained with Gaussian prior 1.0 and 600 iterations. The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below: P([c,tj�`)�I1P([C,tj; �[�,tj�-2) k  1 Score t             log P c t c t , |, k k k  2          P t t c k 2 1 log |,   k k  1 k  2 (2) SRI Language Modeling Toolkit 2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) in our experiments. The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows: Where the parameter is the weight for the generative model and can be obtained from the dev</context>
</contexts>
<marker>Wang, Zong, Su, 2009</marker>
<rawString>Kun Wang, Chengqing Zong and Keh-Yih Su, 2009. Which is more suitable for Chinese word segmentation, the generative model or the discriminative one? In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC23), pages 827-834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
</authors>
<title>Chengqing Zong and Keh-Yih Su,</title>
<date>2010</date>
<booktitle>Proceedings of CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP2010),</booktitle>
<pages>245--248</pages>
<marker>Wang, 2010</marker>
<rawString>Kun Wang, Chengqing Zong and Keh-Yih Su, 2010. A Character-Based Joint Model for CIPS-SIGHAN Word Segmentation Bakeoff 2010. Proceedings of CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP2010), pages 245-248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
</authors>
<title>Chengqing Zong and Keh-Yih Su,</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1173--1181</pages>
<location>Beijing, China,</location>
<marker>Wang, 2010</marker>
<rawString>Kun Wang, Chengqing Zong and Keh-Yih Su, 2010. A Character-Based Joint Model for Chinese Word Segmentation. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), Beijing, China, August 23-27, 2010. Pages 1173-1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
</authors>
<title>Chengqing Zong and Keh-Yih Su,</title>
<date>2012</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP).</journal>
<marker>Wang, 2012</marker>
<rawString>Kun Wang, Chengqing Zong and Keh-Yih Su, 2012. Integrating generative and discriminative character based models for Chinese word segmentation. ACM Transactions on Asian Language Information Processing (TALIP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
</authors>
<title>A Maximum Entropy Approach to Chinese Word Segmentation.</title>
<date>2005</date>
<booktitle>Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>161--164</pages>
<marker>Low, 2005</marker>
<rawString>Low, Jin Kiat et al., 2005. A Maximum Entropy Approach to Chinese Word Segmentation. Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<pages>29--48</pages>
<contexts>
<context position="1100" citStr="Xue, 2003" startWordPosition="151" endWordPosition="152">urther improve the performance in cross-domain, an external dictionary is employed. In addition, pre-processing and post-processing rules are utilized to further improve the performance. The final performance on the test corpus shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction Because Chinese text is written without natural delimiters, word segmentation is a prerequisite and fundamental task in Chinese natural language processing. And many approaches have been proposed for this task. Among these methods, the character-based tagging approach (Xue, 2003) has become the prevailing technique for Chinese word segmentation (CWS) due to its good performance. In recent years, within the framework of character-based, much efforts (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008) have been made to further improve word segmentation’s performance. The character-based joint model (Wang et al., 2010, Wang et al., 2012) achieves a good balance between in-vocabulary (IV) words recognition and out-of-vocabulary (OOV) words identification. So, in this evaluation task, following their work we adopt the character-based joint model as our basic syste</context>
<context position="3154" citStr="Xue, 2003" startWordPosition="466" endWordPosition="467">iterary which is different from the genres of common text. To make our system more robust on Microblog text, we propose several simple but novel pre-processing and post-processing approaches in our system. The final results show that our system performs well on test set and achieves comparable segmentation results with other participants. 2 System Description 2.1 Character-Based Joint Model The character-based joint model in our system consists of two basic components: ➢ The character-based discriminative model. ➢ The character-based generative model. The character-based discriminative model (Xue, 2003) is based on a Maximum Entropy (ME) framework (Ratnaparkhi, 1998) and can be formulated as follows: P t1n |c1n k+2F1 P tk |tk _ 1, ck_2 n (1) k= 1 Where is a member of {B, M, E, S}, in which B, M and E indicate the Beginning, Middle and End of character in its associated word respectively, and S denotes that it’s a Single-character word. For example, the word “北京市 (Beijing 96 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 96–100, Wuhan, China, 20-21 October 2014 City)” will be assigned with the corresponding tags as: “北 /B (North) 京 /M (Capital) 市 /</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue, 2003. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing, 8 (1). pages 29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
</authors>
<title>Genichiro Kikui and Eiichiro Sumita,</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL,</booktitle>
<pages>961--968</pages>
<marker>Zhang, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita, 2006. Subword-based Tagging for Confidence dependent Chinese Word Segmentation. In Proceedings of the COLING/ACL, pages 961-968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2006</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<marker>Zhu, 2006</marker>
<rawString>Xiaojin Zhu, 2006. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021803">
<title confidence="0.99157">
Finding your “inner-annotator”: An experiment in annotator
independence for rating discourse coherence quality in essays
</title>
<author confidence="0.93812">
Jill Burstein Swapna Somasundaran Martin Chodorow
</author>
<affiliation confidence="0.906085">
Educational Testing Service Educational Testing Service Hunter College, CUNY
</affiliation>
<address confidence="0.933145">
666 Rosedale Road 666 Rosedale Road 695 Park Avenue
Princeton, NJ 08541 Princeton, NJ 08541 New York, NY
</address>
<email confidence="0.997336">
jburstein@ets.org ssomasundaran@ets.org martin.chodorow@hunter.cuny.edu
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995901">
An experimental annotation method is described, showing promise for a subjective labeling task –
discourse coherence quality of essays. Annotators developed personal protocols, reducing front-end
resources: protocol development and annotator training. Substantial inter-annotator agreement was
achieved for a 4-point scale. Correlational analyses revealed how unique linguistic phenomena were
considered in annotation. Systems trained with the annotator data demonstrated utility of the data.
</bodyText>
<sectionHeader confidence="0.959031" genericHeader="keywords">
1 Introduction1
</sectionHeader>
<bodyText confidence="0.999490923076923">
Systems designed to evaluate discourse coherence quality often use supervised methods, relying on
human annotation that requires significant front-end resources (time and cost) for protocol
development and annotator training (Burstein et al., 2013). Crowd-sourcing (e.g., Amazon
Mechanical Turk) has been used to collect annotation judgments more efficiently than traditional
means for tasks requiring little domain expertise (Beigman Klebanov et al., 2013; Louis &amp; Nenkova,
2013). However, proprietary data (test-taker essays) may preclude crowd-sourcing use. In the U.S.,
the need for automated writing evaluation systems to score proprietary test-taker data is likely to
increase when Common Core2 assessments are administered to school-age students beginning in 2015
(Shermis, in press), increasing the need for data annotation. This paper describes an experimental
method for capturing discourse coherence quality judgments for test-taker essays. Annotators
developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard
front-end resources. The paper presents related work (Section 2), the experimental annotation (Section
3), system evaluations (Section 4), and conclusions (Section 5).
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999878">
Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein &amp;
Wolska, 2003; Reidsma &amp; op den Akker, 2008; Burstein et al., 2013). Front-end annotation activities
may require significant resources (protocol development and annotator training) (Miltsakaki and
Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013). Burstein et al (2013)
reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading
research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text
coherence is highly personal , relying on a variety of features, including adherence to standard writing
conventions (e.g., grammar), and patterns of rhetorical structure and vocabulary usage. They describe
an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low))
applied by 2 annotators to label 1,500 test-taker essays from 6 task types (Table 1). Protocol
development took several weeks, and offered extensive descriptions of the 3-point scale, including
illustrative test-taker responses; rigorous annotator training was also conducted. Burstein et al, 2013
collapsing the 3-point scale to a 2-point scale (i.e., high (3), low (1,2)). Results for a binary discourse
coherence quality system (high and low coherence) for essays achieved only borderline modest
</bodyText>
<footnote confidence="0.999794666666667">
1 This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2 See http://www.corestandards.org/.
</footnote>
<page confidence="0.967809">
48
</page>
<table confidence="0.553068333333333">
LAW VIII - The 8th Linguistic Annotation Workshop, pages 48–53,
Dublin, Ireland, August 23-24 2014.
Essay-Writing Item Type Test-Taker Population
1. K-12 expository Students3, ages 11-16
2. Expository NNES-Univ
3. Source-based, integrated (reading and listening) NNES-Univ
4. Expository Graduate school applicants
5. Critical argument Graduate school applicants
6. Professional licensing, content/expository Certification for a business-related profession
</table>
<tableCaption confidence="0.9681855">
Table 1. Six item types &amp; populations in the experimental annotation task. NNES-Univ = non-native
English speakers, university applicants
</tableCaption>
<bodyText confidence="0.99946375">
performance (κ=0.41)4. Outcomes reported in Burstein et al are consistent with discussions that text
coherence is a complex and individual process (Graesser et al, 2004; Van den Broek, 2012), motivating
our experimental method. In contrast to training annotators to follow an annotation scheme pre-
determined by others, annotators devised their own scoring protocols, capturing their independent
impressions – finding their “inner-annotator.” The practical outcomes of success of the method would
be reduced front-end resources in terms of time required to (a) develop the annotation protocol and (b)
train annotators. As a practical end-goal, another success criterion would be to achieve inter-annotator
agreement such that classifiers could be trained, yielding substantial annotator-system agreement.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="method">
3 Experimental Annotation Study
</sectionHeader>
<subsectionHeader confidence="0.949452">
Annotation scoring protocols from 2 annotators for coherence quality are evaluated and described.
3.1 Human Annotators
</subsectionHeader>
<bodyText confidence="0.999988666666667">
Two high school English teachers (employed by a company specializing in annotation) performed the
annotation. Annotators never met each other, did not know about each other’s activities, and only
communicated about the annotation with a facilitator from the company.
</bodyText>
<subsectionHeader confidence="0.997605">
3.2 Data
</subsectionHeader>
<bodyText confidence="0.999972857142857">
A random sample of 250 essays for 6 different item types (n=1500) and test-taker populations (Table 1)
was selected. The sample was selected across 20 different prompts (test questions) for each item type in
order to ensure topic generalizability in the resulting systems. Forty essays were randomly selected for a
small pilot study; the remaining data (1460 essays) were used for the full annotation study. For the full
study, 20% of the essays (n=292) had been randomly selected for double annotation to measure inter-
annotator agreement; the remaining 1168 essays were evenly divided, and each annotator labeled half
(n=584 per annotator). Each annotator labeled a total of 876 essays across the 6 task types.
</bodyText>
<subsectionHeader confidence="0.991108">
3.3 Experimental Method Description
</subsectionHeader>
<bodyText confidence="0.999988285714286">
A one-week pilot study was conducted. To provide some initial grounding, annotators received a 1-page
task description that offered a high-level explanation of “coherence” describing the end-points of a
potential protocol. (This description was written in about an hour.) It indicated that high coherence is
associated with an essay that can be easily understood, and low coherence is associated with an
incomprehensible essay. Each annotator developed her own protocol: for each score point she wrote
descriptive text illustrating a set of defining characteristics for each score point of coherence quality
(e.g., “The writer’s point is difficult to understand.”). Annotator 1 (A1) developed a 4-point scale;
</bodyText>
<footnote confidence="0.971452333333333">
3 Note that this task type was administered in an instructional setting; all other tasks were completed in high-stakes
assessment settings.
4 Kappa was not reported in the paper, but was accessed through personal communication.
</footnote>
<page confidence="0.988549">
49
</page>
<table confidence="0.999977272727273">
Feature Type A1 (r) A2 (r)
Grammar errors (e.g., subject verb agreement) 0.42 0.35
Word usage errors (e.g., determiner errors) 0.46 0.44
Mechanics errors (e.g., spelling, punctuation) 0.58 0.52
EGT -- best 3 features (out of 112 features): F1, F2, F3 F1. -0.30 F1. -0.14
F2. -0.28 F2. -0.15
F3. 0.27 F3. 0.11
RST features-- best 3 features (out of 100 features): F1, F2, F3 F1. -0.27 F1. -0.19
F2. 0.15 F2. 0.08
F3. 0.19 F3. 0.06
LDSP 0.19 0.06
</table>
<tableCaption confidence="0.992477">
Table 2. Pearson r between annotator discourse coherence scores and features. All correlations are
significant at p &lt; .0001, except for A2’s long-distance sentence-pair similarity at p &lt; .05.
</tableCaption>
<bodyText confidence="0.973665578947369">
Annotator 2 (A2) developed a 5-point scale. Because the two scales were different, κ could not be used
to measure agreement, so a Spearman rank-order correlation (rS) was used, yielding a promising value
(rS=0.82). Annotator protocols were completed at the end of the pilot study.
A full experiment was conducted. Each annotator used her protocol to assign a coherence quality
score to each essay. Annotators assigned a score and wrote brief comments as explanation (drawing from
the protocol). Comments provided a score supplement that could be used to support analyses beyond
quantitative measures (Reidsma &amp; Carletta, 2008). The data were annotated in 12 batches (by task)
composed of 75 essays (50 unique; 25 for double annotation). A Spearman rank-order correlation was
computed on the double-scored essays for completed batches. If the correlation fell below 0.70 (which
was infrequent), one of the authors reviewed the annotator scores and comments to look for
inconsistencies. Agreement was re-computed when annotator revisions were completed to ensure inter-
rater agreement of 0.70. Annotations were completed over approximately 4 weeks to accommodate
annotator schedules. While a time log was not strictly maintained, we estimate the total time for
communication to resolve inconsistency issues was about 4-6 hours. One author communicated score-
comment inconsistencies (e.g., high score with critical comments) to the company’s facilitator (through a
brief e-mail); the facilitator then relayed the inconsistency information to the annotator(s). The author’s
data review and communication e-mail took no longer than 45 minutes for the few rounds where
agreement fell below 0.70. Communication between the facilitator and the annotator(s) involved a brief
discussion, essentially reviewing the points made in the e-mail.
</bodyText>
<subsectionHeader confidence="0.646236">
3.4 Results: Inter-annotator agreement
</subsectionHeader>
<bodyText confidence="0.9998616">
Using the Spearman rank-order correlation, inter-rater agreement on the double-annotated data was
rS=0.71. In order to calculate Kappa statistic, A2’s 5-point scale assignments were then mapped to a 4-
point scale by collapsing the two lowest categories (1,2) into one (1), since there were very few cases of
1’s; this is consistent with low frequencies of very low-scoring essays. Using quadratic weighted kappa
(QWK), post-mapping indicated substantial agreement between the two annotators (κ=0.61).
</bodyText>
<subsectionHeader confidence="0.969808">
3.5 Correlational Analysis: Which Linguistic Features Did Annotators Consider?
</subsectionHeader>
<bodyText confidence="0.9997918">
A1 and A2 wrote brief comments explaining their coherence scores. Comments were shorthand notation
drawn from their protocols (e.g., There are significant grammatical errors...thoughts do not connect.).
Both annotators included descriptions such as “word patterns,” “logical sequencing,” and “clarity of
ideas”; however, A2 appeared to have more comments related to grammar and spelling. Burstein et al.,
(2013) describe the following features in their binary classification system: (1) grammar, word usage,
and mechanics errors (GUM), (2) rhetorical parse tree features (Marcu, 2000) (RST), (3) entity-grid
transition probabilities to capture local “topic distribution” (Barzilay &amp; Lapata, 2008) (EGT), and (4) a
long-distance sentence pair similarity measure using latent semantic analysis (Foltz, 1998) to capture
“long distance, topical distribution. (LDSP). Annotated data from this study were processed with the
Burstein et al (2013) system to extract the features above in (1) – (4). To quantify the observed
</bodyText>
<page confidence="0.990583">
50
</page>
<bodyText confidence="0.999857555555556">
differences in the annotators’ comments and potential effects for system score assignment (Section 4),
we computed Pearson (r) correlations between the system features (on our annotated data set), and the
discourse coherence scores of A1 and A2 (using the 4-point scale mapping for A2). There are 112 entity-
transition probability features and 100 Rhetorical Structure Theory (RST) features. In Table 2, the
correlations of the three best predictors from the EGT and RST sets, and the GUM features and the
LDSP feature are shown. Correlations in Table 2 are significantly correlated between the feature sets and
annotator coherence scores. However, we observed that the EGT, RST, and LDSP feature correlation
values for A2 are notably smaller than A1’s. This suggests that A2 may have had a strong reliance on
GUM features, or that the system feature set did not capture all linguistic phenomena that A2 considered.
</bodyText>
<sectionHeader confidence="0.9936" genericHeader="method">
4 System Evaluation5
</sectionHeader>
<bodyText confidence="0.9992466">
To evaluate the utility of the annotated data, two evaluations were conducted: one built classifiers with
all system features (Sys_All), and a second with the GUM features (Sys_GUM). Using 10-fold cross-
validation with a gradient boosting regression learner, four classifiers were trained to predict coherence
quality ratings on a 4-point scale, using the respective annotator data sets: A1 and A2 Sys_All, and A1
and A2 Sys_GUM systems.
</bodyText>
<subsectionHeader confidence="0.599874">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.999768142857143">
Sys-All trained with A1 data consistently outperformed Sys-All trained with A2 data. Results are
reported for averages across the 10-folds, and showed substantial system-human agreement for A1 (x =
0.68) and modest system-human agreement for A2 (x = 0.55). When Sys_GUM was trained with A1
data, system-human agreement dropped to a modest range (x = 0.60); when Sys_GUM was trained with
A2 data, however, human agreement was essentially unchanged, staying in the modest agreement range
(x = 0.50). Consistent with the correlational analysis, this finding suggests that A2 has strong reliance
on GUM features, or the system may have been less successful in capturing A2 features beyond GUM.
</bodyText>
<sectionHeader confidence="0.983078" genericHeader="conclusions">
5 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999818608695652">
Our experimental annotation method significantly reduced front-end resources for protocol development
and annotator training. Analyses reflect one genre: essays from standardized assessments. Minimal time
was required from the authors or the facilitator (about two hours) for protocol development; the
annotators developed personal protocols over a week during the pilot; in Burstein et al (2013), this
process was report to take about one month. Approximately 4-6 hours of additional discussion from one
author and the facilitator was required during the task; Burstein et al (2013) required two researchers and
two annotators participated in several 4-hour training sessions, totaling about 64-80 hours of person-time
across the 4 participants (personal communication). In addition to its efficiency, the experimental
method was successful per criteria in Section 2. The method captures annotators’ subjective judgments
about coherence quality, yielding substantial inter-annotator agreement (x=0.61) across a 4-point scale.
Second, classifiers trained with annotator data showed that the systems showed substantial and modest
agreement (A1 and A2, respectively) – demonstrating annotation utility, especially for A1. Correlational
analyses were used to analyze effects of features that annotators may have considered in making their
decisions. Comment patterns and results from the correlation analysis suggested that A2’s decisions
were either based on narrower considerations (GUM errors), or not captured by our feature set.
The experimental task facilitated the successful collection of subjective coherence judgments with
substantial inter-annotator agreement on test-taker essays. Consistent with conclusions from Reidsma &amp;
Carletta (2008), outcomes show that quantitative measures of inter-annotator agreement should not be
used exclusively. Descriptive comments were useful for monitoring during annotation, interpreting
annotator considerations and system evaluations during and after annotation, and informing system
development. In the future, we would explore strategies to evaluate intra-annotator reliability (Beigman-
Klebanov, Beigman, &amp; Diermeier, 2008) which may have contributed to lower system performance
with A2 data.
</bodyText>
<footnote confidence="0.815739">
5 Many thanks to Binod Gywali for engineering support.
</footnote>
<page confidence="0.998363">
51
</page>
<sectionHeader confidence="0.990223" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999721365853659">
Beata Beigman-Klebanov, Nitin Madnani,, and Jill Burstein. 2013. Using Pivot-Based Paraphrasing and
Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data, Transactions of the Association for
Computational Linguistics, Vol.1: 99-110.
Beata Beigman-Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing Disagreements. In
Proceedings of the workshop on Human Judgments in Computational Linguistics, Manchester: 2-7.
Jill Burstein, Joel Tetreault and Martin Chodorow. 2013. Holistic Annotation of Discourse Coherence Quality
in Noisy Essay Writing. In the Special issue of Dialogue and Discourse on: Beyond semantics: the challenges
of annotating pragmatic and discourse phenomena Eds. S. Dipper, H. Zinsmeister, and B. Webber.
Discourse &amp; Dialogue 42, 34-52.
Jill Burstein and Magdalena Wolska..2003. Toward Evaluation of Writing Style: Overly Repetitious Word Use.
In Proceedings of the 11th Conference of the European Chapter of the Association for Computational
Linguistics. Budapest, Hungary.
Jacob Cohen. 1960. &amp;quot;A coefficient of agreement for nominal scales&amp;quot;. Educational and Psychological
Measurement 20 1: 37–46.
Joseph Fleiss and Jacob Cohen 1973. &amp;quot;The equivalence of weighted kappa and the intraclass correlation
coefficient as measures of reliability&amp;quot; in Educational and Psychological Measurement, Vol. 33:613–619.
Peter Foltz, Walter Kintsch, &amp; Thomas Landuaer. 1998. Textual coherence using latent semantic analysis.
Discourse Processes, 252&amp;3: 285–307.
Arthur Graesser, Danielle McNamara, Max Louwerse. and Zhiqiang Cai, Z. 2004. Coh-metrix: Analysis of text
on cohesion and language. Behavior Research Methods, Instruments, &amp; Computers, 36(2), 193-202.
Derrick Higgins, Jill Burstein, Daniel Marcu &amp;. Claudia Gentile. 2004. Evaluating Multiple Aspects of
Coherence in Student Essays. In Proceedings of 4th Annual Meeting of the Human Language Technology and
North American Association for Computation Linguistics:185–192, Boston, MA
J. Richard Landis,. &amp; G. Koch. 1977. &amp;quot;The measurement of observer agreement for categorical
data&amp;quot;. Biometrics 33 1: 159–174.
Annie Louis and Ani Nenkova. 2013. A Text Quality Corpus for Science Journalism. In the Special Issue
of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse
phenomena Eds. S. Dipper, H. Zinsmeister, and B. Webber, 42: 87-117.
Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In
Proceedings of the Language Resources and Evaluation Conference, Athens, Greece.
Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. Cambridge, MA: The
MIT Press.
Dennis Reidsma, and Rieks op den Akker. 2008. Exploiting `Subjective&apos; Annotations. In Proceedings of the
Workshop on Human Judgments in Computational Linguistics, Coling 2008, 23 August 2008, Manchester,
UK.
Dennis Reidsma, and Jean Carletta. 2008. Reliability measurements without limits. Computational Linguistics,
343: 319-336.
Mark Shermis. to appear. State-of-the-art automated essay scoring: Competition, results, and future directions
from a United States demonstration. Assessing Writing.
Y. Wang, M. Harrington, and P. White. 2012. Detecting Breakdowns in Local Coherence in the Writing of
Chinese English Speakers. The Journal of Computer Assisted Learning. 28: 396–410.
</reference>
<page confidence="0.980259">
52
</page>
<reference confidence="0.739280333333333">
Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing
cognitive processes and outcomes. In: Sabatini, J.P., Albro, E.R., O&apos;Reilly, T. (Eds.), Measuring up:
Advances in how we assess reading ability., pp. 39-58. Lanham: Rowman &amp; Littlefield Education.
</reference>
<page confidence="0.999157">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000992">
<title confidence="0.966152">An experiment in independence for rating discourse coherence quality in essays</title>
<author confidence="0.999744">Jill Burstein Swapna Somasundaran Martin Chodorow</author>
<affiliation confidence="0.9455">Educational Testing Service Educational Testing Service Hunter College, CUNY</affiliation>
<address confidence="0.994789">666 Rosedale Road 666 Rosedale Road 695 Park Avenue Princeton, NJ 08541 Princeton, NJ 08541 New York, NY</address>
<email confidence="0.985909">jburstein@ets.orgssomasundaran@ets.orgmartin.chodorow@hunter.cuny.edu</email>
<abstract confidence="0.984788333333333">experimental annotation method is described, showing promise for a subjective labeling task discourse coherence quality of essays. Annotators developed personal protocols, reducing front-end resources: protocol development and annotator training. Substantial inter-annotator agreement was achieved for a 4-point scale. Correlational analyses revealed how unique linguistic phenomena were considered in annotation. Systems trained with the annotator data demonstrated utility of the data. Systems designed to evaluate discourse coherence quality often use supervised methods, relying on human annotation that requires significant front-end resources (time and cost) for protocol development and annotator training (Burstein et al., 2013). Crowd-sourcing (e.g., Amazon Mechanical Turk) has been used to collect annotation judgments more efficiently than traditional means for tasks requiring little domain expertise (Beigman Klebanov et al., 2013; Louis &amp; Nenkova, 2013). However, proprietary data (test-taker essays) may preclude crowd-sourcing use. In the U.S., the need for automated writing evaluation systems to score proprietary test-taker data is likely to when Common assessments are administered to school-age students beginning in 2015 (Shermis, in press), increasing the need for data annotation. This paper describes an experimental method for capturing discourse coherence quality judgments for test-taker essays. Annotators developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 3), system evaluations (Section 4), and conclusions (Section 5). 2 Related Work Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein &amp; Wolska, 2003; Reidsma &amp; op den Akker, 2008; Burstein et al., 2013). Front-end annotation activities may require significant resources (protocol development and annotator training) (Miltsakaki and Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013). Burstein et al (2013) reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text is personal relying on a of including adherence to standard writing conventions (e.g., grammar), and patterns of rhetorical structure and vocabulary usage. They describe annotation protocol that uses a 3-point coherence quality scale (3 2 and 1 applied by 2 annotators to label 1,500 test-taker essays from 6 task types (Table 1). Protocol development took several weeks, and offered extensive descriptions of the 3-point scale, including illustrative test-taker responses; rigorous annotator training was also conducted. Burstein et al, 2013 the 3-point scale to a 2-point scale (i.e., Results for a quality system for essays achieved only modest work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/</abstract>
<web confidence="0.662243">http://www.corestandards.org/.</web>
<note confidence="0.593802666666667">48 VIII - The 8th Linguistic Annotation pages Dublin, Ireland, August 23-24 2014.</note>
<title confidence="0.557227">Essay-Writing Item Type Test-Taker Population</title>
<abstract confidence="0.946534467625899">1. K-12 expository ages 11-16 2. Expository NNES-Univ 3. Source-based, integrated (reading and listening) NNES-Univ 4. Expository Graduate school applicants 5. Critical argument Graduate school applicants 6. Professional licensing, content/expository Certification for a business-related profession Table 1. Six item types &amp; populations in the experimental annotation task. NNES-Univ = non-native English speakers, university applicants Outcomes reported in Burstein et al are consistent with discussions that text coherence is a complex and individual process (Graesser et al, 2004; Van den Broek, 2012), motivating our experimental method. In contrast to training annotators to follow an annotation scheme predetermined by others, annotators devised their own scoring protocols, capturing their independent practical outcomes of success of the method would be reduced front-end resources in terms of time required to (a) develop the annotation protocol and (b) train annotators. As a practical end-goal, another success criterion would be to achieve inter-annotator such that classifiers could be trained, yielding agreement. 3 Experimental Annotation Study Annotation scoring protocols from 2 annotators for coherence quality are evaluated and described. 3.1 Human Annotators Two high school English teachers (employed by a company specializing in annotation) performed the Annotators never met each other, not know about each other’s and only communicated about the annotation with a facilitator from the company. 3.2 Data random sample of 250 essays for 6 different item types and test-taker populations (Table 1) was selected. The sample was selected across 20 different prompts (test questions) for each item type in order to ensure topic generalizability in the resulting systems. Forty essays were randomly selected for a study;the remaining data (1460 essays) were used for the annotation study.For the full 20% of the essays had been randomly selected for double annotation to measure interannotator agreement; the remaining 1168 essays were evenly divided, and each annotator labeled half per annotator). Each annotator labeled a total of 876 essays across the 6 task types. 3.3 Experimental Method Description one-week study conducted. To provide some initial grounding, annotators received a 1-page description that offered a explanation of “coherence” the end-points of a potential protocol. (This description was written in about an hour.) It indicated that high coherence is with an essay that can be and low coherence is associated with an Each annotator developed her own protocol: for each score point she wrote descriptive text illustrating a set of defining characteristics for each score point of coherence quality writer’s is difficult to 1 (A1) developed a 4-point scale; that this task type was administered in an instructional setting; all other tasks were completed in high-stakes assessment settings. was not reported in the paper, but was accessed through personal communication. 49 Feature Type Grammar errors (e.g., subject verb agreement) 0.42 0.35 Word usage errors (e.g., determiner errors) 0.46 0.44 Mechanics errors (e.g., spelling, punctuation) 0.58 0.52 EGT -best 3 features (out of 112 features): F1, F2, F3 F1. -0.30 F1. -0.14 F2. -0.28 F2. -0.15 F3. 0.27 F3. 0.11 RST features-best 3 features (out of 100 features): F1, F2, F3 F1. -0.27 F1. -0.19 F2. 0.15 F2. 0.08 F3. 0.19 F3. 0.06 LDSP 0.19 0.06 2. Pearson annotator discourse coherence scores and features. All correlations are at .0001, except for A2’s sentence-pair similarity at .05. 2 (A2) developed a 5-point scale. Because the two scales were different, not be used measure agreement, so a Spearman rank-order correlation was used, yielding a promising value Annotator protocols were completed at the end of the pilot study. experimentwas conducted. Each annotator used her protocol to assign a coherence quality score to each essay. Annotators assigned a score and wrote brief comments as explanation (drawing from the protocol). Comments provided a score supplement that could be used to support analyses beyond quantitative measures (Reidsma &amp; Carletta, 2008). The data were annotated in 12 batches (by task) composed of 75 essays (50 unique; 25 for double annotation). A Spearman rank-order correlation was computed on the double-scored essays for completed batches. If the correlation fell below 0.70 (which was infrequent), one of the authors reviewed the annotator scores and comments to look for inconsistencies. Agreement was re-computed when annotator revisions were completed to ensure interrater agreement of 0.70. Annotations were completed over approximately 4 weeks to accommodate annotator schedules. While a time log was not strictly maintained, we estimate the total time for to resolve inconsistency issues was about 4-6 hours. One author communicated scoreinconsistencies high score with critical comments) to the (through a e-mail); the facilitator then relayed the inconsistency information to the annotator(s). author’s data review and communication e-mail took no longer than 45 minutes for the few rounds where agreement fell below 0.70. Communication between the facilitator and the annotator(s) involved a brief discussion, essentially reviewing the points made in the e-mail. 3.4 Results: Inter-annotator agreement Using the Spearman rank-order correlation, inter-rater agreement on the double-annotated data was In order to calculate Kappa statistic, scale assignments were then mapped to a 4point scale by collapsing the two lowest categories (1,2) into one (1), since there were very few cases of this is consistent with low frequencies of very low-scoring essays. Using quadratic weighted kappa post-mapping indicated between the two annotators 3.5 Correlational Analysis: Which Linguistic Features Did Annotators Consider? A1 and A2 wrote brief comments explaining their coherence scores. Comments were shorthand notation from their protocols (e.g., are significant grammatical errors...thoughts do not annotators included descriptions such as and of however, appeared to have more comments related to grammar and spelling. Burstein et al., (2013) describe the following features in their binary classification system: (1) grammar, word usage, and mechanics errors (GUM), (2) rhetorical parse tree features (Marcu, 2000) (RST), (3) entity-grid probabilities to capture local “topic distribution” (Barzilay &amp; Lapata, 2008) and (4) a long-distance sentence pair similarity measure using latent semantic analysis (Foltz, 1998) to capture distance, topical distribution. Annotated data from this study were processed with the et al (2013) system to extract the features above in (1) To quantify the observed 50 in the annotators’ comments and effects for system score assignment (Section 4), computed Pearson correlations between the system features (on our annotated data set), and the discourse coherence scores of A1 and A2 (using the 4-point scale mapping for A2). There are 112 entitytransition probability features and 100 Rhetorical Structure Theory (RST) features. In Table 2, the of the best predictorsfrom the EGT and RST sets, and the GUM features and the LDSP feature are shown. Correlations in Table 2 are significantly correlated between the feature sets and annotator coherence scores. However, we observed that the EGT, RST, and LDSP feature correlation A2 are notably smaller than A1’s. This suggests that A2 have had a strong reliance on GUM features, or that the system feature set did not capture all linguistic phenomena that A2 considered. System To evaluate the utility of the annotated data, two evaluations were conducted: one built classifiers with all system features (Sys_All), and a second with the GUM features (Sys_GUM). Using 10-fold crossvalidation with a gradient boosting regression learner, four classifiers were trained to predict coherence quality ratings on a 4-point scale, using the respective annotator data sets: A1 and A2 Sys_All, and A1 and A2 Sys_GUM systems. 4.1 Results Sys-All trained with A1 data consistently outperformed Sys-All trained with A2 data. Results are for averages across the 10-folds, and showed agreement for A1 (x = and agreement for A2 (x = When Sys_GUM was trained with A1 system-human agreement dropped to a = when Sys_GUM was trained with data, however, human agreement was essentially unchanged, staying in the range 0.50). Consistent with the correlational analysis, this finding suggests that A2 has strong reliance on GUM features, or the system may have been less successful in capturing A2 features beyond GUM. 5 Discussion and Conclusions Our experimental annotation method significantly reduced front-end resources for protocol development and annotator training. Analyses reflect one genre: essays from standardized assessments. Minimal time was required from the authors or the facilitator (about two hours) for protocol development; the annotators developed personal protocols over a week during the pilot; in Burstein et al (2013), this process was report to take about one month. Approximately 4-6 hours of additional discussion from one author and the facilitator was required during the task; Burstein et al (2013) required two researchers and two annotators participated in several 4-hour training sessions, totaling about 64-80 hours of person-time across the 4 participants (personal communication). In addition to its efficiency, the experimental was criteria in Section 2. The method captures judgments coherence quality, yielding agreement across a 4-point scale. classifiers trained with annotator data showed that the systems showed (A1 and A2, respectively) annotation utility, especially for A1. Correlational analyses were used to analyze effects of features that annotators may have considered in making their Comment patterns and results from the correlation analysis suggested that were either based on narrower considerations (GUM errors), or not captured by our feature set. The experimental task facilitated the successful collection of subjective coherence judgments with substantial inter-annotator agreement on test-taker essays. Consistent with conclusions from Reidsma &amp; Carletta (2008), outcomes show that quantitative measures of inter-annotator agreement should not be exclusively. Descriptive comments were useful for monitoring interpreting considerations and system evaluations and informing system development. In the future, we would explore strategies to evaluate intra-annotator reliability (Beigman- Klebanov, Beigman, &amp; Diermeier, 2008) which may have contributed to lower system performance with A2 data.</abstract>
<note confidence="0.91468575">thanks to Binod Gywali for engineering support. 51 References Beata Beigman-Klebanov, Nitin Madnani,, and Jill Burstein. 2013. Using Pivot-Based Paraphrasing and Profiles to Improve a Subjectivity Lexicon for Essay Data, of the Association for Vol.1: 99-110. Beata Beigman-Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing Disagreements. In Proceedings of the workshop on Human Judgments in Computational Linguistics, Manchester: 2-7. Jill Burstein, Joel Tetreault and Martin Chodorow. 2013. Holistic Annotation of Discourse Coherence Quality Noisy Essay Writing. In the issue of Dialogue and Discourse on: Beyond semantics: the challenges annotating pragmatic and discourse phenomena S. Dipper, H. Zinsmeister, and B. Webber. Discourse &amp; Dialogue 42, 34-52. Jill Burstein and Magdalena Wolska..2003. Toward Evaluation of Writing Style: Overly Repetitious Word Use. of the 11th Conference of the European Chapter of the Association for Computational Budapest, Hungary. Jacob Cohen. 1960. &amp;quot;A coefficient of agreement for nominal scales&amp;quot;. Educational and Psychological 20 1: Joseph Fleiss and Jacob Cohen 1973. &amp;quot;The equivalence of weighted kappa and the intraclass correlation as measures of reliability&amp;quot; in and Psychological Vol. Peter Foltz, Walter Kintsch, &amp; Thomas Landuaer. 1998. Textual coherence using latent semantic analysis. 252&amp;3: Arthur Graesser, Danielle McNamara, Max Louwerse. and Zhiqiang Cai, Z. 2004. Coh-metrix: Analysis of text cohesion and language. Research Methods, Instruments, &amp; Computers, 193-202. Derrick Higgins, Jill Burstein, Daniel Marcu &amp;. Claudia Gentile. 2004. Evaluating Multiple Aspects of in Student Essays. In of 4th Annual Meeting of the Human Language Technology and American Association for Computation Boston, MA J. Richard Landis,. &amp; G. Koch. 1977. &amp;quot;The measurement of observer agreement for categorical Louis and Ani Nenkova. 2013. A Text Quality Corpus for Science Journalism. In the Issue of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse S. Dipper, H. Zinsmeister, and B. Webber, 42: 87-117. Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In of the Language Resources and Evaluation Athens, Greece. Marcu. 2000. Theory and Practice of Discourse Parsing and Summarization. MA: The MIT Press. Reidsma, and Rieks op den Akker. 2008. Exploiting `Subjective&apos; Annotations. In of the on Human Judgments in Computational Coling 2008, 23 August 2008, Manchester, UK. Reidsma, and Jean Carletta. 2008. Reliability measurements without limits. 343: 319-336. Shermis. to appear.State-of-the-art automated essay scoring: Competition, results, and future directions a United States demonstration. Y. Wang, M. Harrington, and P. White. 2012. Detecting Breakdowns in Local Coherence in the Writing of English Speakers. The of Computer Assisted 28: 52 Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing processes and outcomes. In: Sabatini, J.P., Albro, E.R., O&apos;Reilly, T. (Eds.), up: in how we assess reading pp. 39-58. Lanham: Rowman &amp; Littlefield Education. 53</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beata Beigman-Klebanov</author>
<author>Nitin Madnani</author>
</authors>
<title>Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data,</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<pages>99--110</pages>
<marker>Beigman-Klebanov, Madnani, 2013</marker>
<rawString>Beata Beigman-Klebanov, Nitin Madnani,, and Jill Burstein. 2013. Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data, Transactions of the Association for Computational Linguistics, Vol.1: 99-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman-Klebanov</author>
<author>Eyal Beigman</author>
<author>Daniel Diermeier</author>
</authors>
<title>Analyzing Disagreements.</title>
<date>2008</date>
<booktitle>In Proceedings of the workshop on Human Judgments in Computational Linguistics,</booktitle>
<pages>2--7</pages>
<location>Manchester:</location>
<marker>Beigman-Klebanov, Beigman, Diermeier, 2008</marker>
<rawString>Beata Beigman-Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing Disagreements. In Proceedings of the workshop on Human Judgments in Computational Linguistics, Manchester: 2-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Holistic Annotation of Discourse Coherence Quality in Noisy Essay Writing. In the Special issue of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse</title>
<date>2013</date>
<journal>Discourse &amp; Dialogue</journal>
<volume>42</volume>
<pages>34--52</pages>
<contexts>
<context position="1191" citStr="Burstein et al., 2013" startWordPosition="145" endWordPosition="148"> Annotators developed personal protocols, reducing front-end resources: protocol development and annotator training. Substantial inter-annotator agreement was achieved for a 4-point scale. Correlational analyses revealed how unique linguistic phenomena were considered in annotation. Systems trained with the annotator data demonstrated utility of the data. 1 Introduction1 Systems designed to evaluate discourse coherence quality often use supervised methods, relying on human annotation that requires significant front-end resources (time and cost) for protocol development and annotator training (Burstein et al., 2013). Crowd-sourcing (e.g., Amazon Mechanical Turk) has been used to collect annotation judgments more efficiently than traditional means for tasks requiring little domain expertise (Beigman Klebanov et al., 2013; Louis &amp; Nenkova, 2013). However, proprietary data (test-taker essays) may preclude crowd-sourcing use. In the U.S., the need for automated writing evaluation systems to score proprietary test-taker data is likely to increase when Common Core2 assessments are administered to school-age students beginning in 2015 (Shermis, in press), increasing the need for data annotation. This paper desc</context>
<context position="2566" citStr="Burstein et al., 2013" startWordPosition="332" endWordPosition="335">g their intuitions about essay coherence, thus reducing standard front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 3), system evaluations (Section 4), and conclusions (Section 5). 2 Related Work Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein &amp; Wolska, 2003; Reidsma &amp; op den Akker, 2008; Burstein et al., 2013). Front-end annotation activities may require significant resources (protocol development and annotator training) (Miltsakaki and Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013). Burstein et al (2013) reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text coherence is highly personal , relying on a variety of features, including adherence to standard writing conventions (e.g., grammar), and patterns of rhetorical structure and vocabulary usage. They describe an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) applied by 2 annotators to label 1,500 test-taker essays from 6 task</context>
<context position="10862" citStr="Burstein et al., (2013)" startWordPosition="1542" endWordPosition="1545">ring essays. Using quadratic weighted kappa (QWK), post-mapping indicated substantial agreement between the two annotators (κ=0.61). 3.5 Correlational Analysis: Which Linguistic Features Did Annotators Consider? A1 and A2 wrote brief comments explaining their coherence scores. Comments were shorthand notation drawn from their protocols (e.g., There are significant grammatical errors...thoughts do not connect.). Both annotators included descriptions such as “word patterns,” “logical sequencing,” and “clarity of ideas”; however, A2 appeared to have more comments related to grammar and spelling. Burstein et al., (2013) describe the following features in their binary classification system: (1) grammar, word usage, and mechanics errors (GUM), (2) rhetorical parse tree features (Marcu, 2000) (RST), (3) entity-grid transition probabilities to capture local “topic distribution” (Barzilay &amp; Lapata, 2008) (EGT), and (4) a long-distance sentence pair similarity measure using latent semantic analysis (Foltz, 1998) to capture “long distance, topical distribution. (LDSP). Annotated data from this study were processed with the Burstein et al (2013) system to extract the features above in (1) – (4). To quantify the obse</context>
<context position="13970" citStr="Burstein et al (2013)" startWordPosition="2014" endWordPosition="2017">0). Consistent with the correlational analysis, this finding suggests that A2 has strong reliance on GUM features, or the system may have been less successful in capturing A2 features beyond GUM. 5 Discussion and Conclusions Our experimental annotation method significantly reduced front-end resources for protocol development and annotator training. Analyses reflect one genre: essays from standardized assessments. Minimal time was required from the authors or the facilitator (about two hours) for protocol development; the annotators developed personal protocols over a week during the pilot; in Burstein et al (2013), this process was report to take about one month. Approximately 4-6 hours of additional discussion from one author and the facilitator was required during the task; Burstein et al (2013) required two researchers and two annotators participated in several 4-hour training sessions, totaling about 64-80 hours of person-time across the 4 participants (personal communication). In addition to its efficiency, the experimental method was successful per criteria in Section 2. The method captures annotators’ subjective judgments about coherence quality, yielding substantial inter-annotator agreement (x</context>
</contexts>
<marker>Burstein, Tetreault, Chodorow, 2013</marker>
<rawString>Jill Burstein, Joel Tetreault and Martin Chodorow. 2013. Holistic Annotation of Discourse Coherence Quality in Noisy Essay Writing. In the Special issue of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse phenomena Eds. S. Dipper, H. Zinsmeister, and B. Webber. Discourse &amp; Dialogue 42, 34-52.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jill Burstein</author>
<author>Magdalena Wolska 2003</author>
</authors>
<title>Toward Evaluation of Writing Style: Overly Repetitious Word Use.</title>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<location>Budapest, Hungary.</location>
<marker>Burstein, 2003, </marker>
<rawString>Jill Burstein and Magdalena Wolska..2003. Toward Evaluation of Writing Style: Overly Repetitious Word Use. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales&amp;quot;.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement</journal>
<volume>20</volume>
<pages>37--46</pages>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. &amp;quot;A coefficient of agreement for nominal scales&amp;quot;. Educational and Psychological Measurement 20 1: 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Fleiss</author>
<author>Jacob Cohen</author>
</authors>
<title>The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability&amp;quot;</title>
<date>1973</date>
<booktitle>in Educational and Psychological Measurement,</booktitle>
<volume>33</volume>
<marker>Fleiss, Cohen, 1973</marker>
<rawString>Joseph Fleiss and Jacob Cohen 1973. &amp;quot;The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability&amp;quot; in Educational and Psychological Measurement, Vol. 33:613–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landuaer</author>
</authors>
<title>Textual coherence using latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>252</volume>
<pages>285--307</pages>
<marker>Foltz, Kintsch, Landuaer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, &amp; Thomas Landuaer. 1998. Textual coherence using latent semantic analysis. Discourse Processes, 252&amp;3: 285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiqiang Cai</author>
<author>Z</author>
</authors>
<title>Coh-metrix: Analysis of text on cohesion and language.</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>36</volume>
<issue>2</issue>
<pages>193--202</pages>
<marker>Cai, Z, 2004</marker>
<rawString>Arthur Graesser, Danielle McNamara, Max Louwerse. and Zhiqiang Cai, Z. 2004. Coh-metrix: Analysis of text on cohesion and language. Behavior Research Methods, Instruments, &amp; Computers, 36(2), 193-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jill Burstein</author>
<author>Daniel Marcu &amp; Claudia Gentile</author>
</authors>
<title>Evaluating Multiple Aspects of Coherence in Student Essays.</title>
<date>2004</date>
<booktitle>In Proceedings of 4th Annual Meeting of the Human Language Technology and North American Association for Computation Linguistics:185–192,</booktitle>
<location>Boston, MA</location>
<contexts>
<context position="2523" citStr="Higgins, et al., 2004" startWordPosition="324" endWordPosition="327">ors developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 3), system evaluations (Section 4), and conclusions (Section 5). 2 Related Work Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein &amp; Wolska, 2003; Reidsma &amp; op den Akker, 2008; Burstein et al., 2013). Front-end annotation activities may require significant resources (protocol development and annotator training) (Miltsakaki and Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013). Burstein et al (2013) reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text coherence is highly personal , relying on a variety of features, including adherence to standard writing conventions (e.g., grammar), and patterns of rhetorical structure and vocabulary usage. They describe an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) applied by 2 annotators t</context>
</contexts>
<marker>Higgins, Burstein, Gentile, 2004</marker>
<rawString>Derrick Higgins, Jill Burstein, Daniel Marcu &amp;. Claudia Gentile. 2004. Evaluating Multiple Aspects of Coherence in Student Essays. In Proceedings of 4th Annual Meeting of the Human Language Technology and North American Association for Computation Linguistics:185–192, Boston, MA</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
</authors>
<title>The measurement of observer agreement for categorical data&amp;quot;.</title>
<date>1977</date>
<journal>Biometrics</journal>
<volume>33</volume>
<pages>159--174</pages>
<marker>Landis, 1977</marker>
<rawString>J. Richard Landis,. &amp; G. Koch. 1977. &amp;quot;The measurement of observer agreement for categorical data&amp;quot;. Biometrics 33 1: 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>A Text Quality Corpus for Science Journalism. In the Special Issue of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse</title>
<date>2013</date>
<volume>42</volume>
<pages>87--117</pages>
<contexts>
<context position="1423" citStr="Louis &amp; Nenkova, 2013" startWordPosition="177" endWordPosition="180">linguistic phenomena were considered in annotation. Systems trained with the annotator data demonstrated utility of the data. 1 Introduction1 Systems designed to evaluate discourse coherence quality often use supervised methods, relying on human annotation that requires significant front-end resources (time and cost) for protocol development and annotator training (Burstein et al., 2013). Crowd-sourcing (e.g., Amazon Mechanical Turk) has been used to collect annotation judgments more efficiently than traditional means for tasks requiring little domain expertise (Beigman Klebanov et al., 2013; Louis &amp; Nenkova, 2013). However, proprietary data (test-taker essays) may preclude crowd-sourcing use. In the U.S., the need for automated writing evaluation systems to score proprietary test-taker data is likely to increase when Common Core2 assessments are administered to school-age students beginning in 2015 (Shermis, in press), increasing the need for data annotation. This paper describes an experimental method for capturing discourse coherence quality judgments for test-taker essays. Annotators developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard front-end reso</context>
</contexts>
<marker>Louis, Nenkova, 2013</marker>
<rawString>Annie Louis and Ani Nenkova. 2013. A Text Quality Corpus for Science Journalism. In the Special Issue of Dialogue and Discourse on: Beyond semantics: the challenges of annotating pragmatic and discourse phenomena Eds. S. Dipper, H. Zinsmeister, and B. Webber, 42: 87-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Karen Kukich</author>
</authors>
<title>Automated evaluation of coherence in student essays.</title>
<date>2000</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2500" citStr="Miltsakaki and Kukich, 2000" startWordPosition="320" endWordPosition="323">or test-taker essays. Annotators developed personal protocols reflecting their intuitions about essay coherence, thus reducing standard front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 3), system evaluations (Section 4), and conclusions (Section 5). 2 Related Work Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein &amp; Wolska, 2003; Reidsma &amp; op den Akker, 2008; Burstein et al., 2013). Front-end annotation activities may require significant resources (protocol development and annotator training) (Miltsakaki and Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013). Burstein et al (2013) reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text coherence is highly personal , relying on a variety of features, including adherence to standard writing conventions (e.g., grammar), and patterns of rhetorical structure and vocabulary usage. They describe an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) ap</context>
</contexts>
<marker>Miltsakaki, Kukich, 2000</marker>
<rawString>Eleni Miltsakaki and Karen Kukich. 2000. Automated evaluation of coherence in student essays. In Proceedings of the Language Resources and Evaluation Conference, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="11035" citStr="Marcu, 2000" startWordPosition="1568" endWordPosition="1569">es Did Annotators Consider? A1 and A2 wrote brief comments explaining their coherence scores. Comments were shorthand notation drawn from their protocols (e.g., There are significant grammatical errors...thoughts do not connect.). Both annotators included descriptions such as “word patterns,” “logical sequencing,” and “clarity of ideas”; however, A2 appeared to have more comments related to grammar and spelling. Burstein et al., (2013) describe the following features in their binary classification system: (1) grammar, word usage, and mechanics errors (GUM), (2) rhetorical parse tree features (Marcu, 2000) (RST), (3) entity-grid transition probabilities to capture local “topic distribution” (Barzilay &amp; Lapata, 2008) (EGT), and (4) a long-distance sentence pair similarity measure using latent semantic analysis (Foltz, 1998) to capture “long distance, topical distribution. (LDSP). Annotated data from this study were processed with the Burstein et al (2013) system to extract the features above in (1) – (4). To quantify the observed 50 differences in the annotators’ comments and potential effects for system score assignment (Section 4), we computed Pearson (r) correlations between the system featur</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Rieks op den Akker</author>
</authors>
<title>Exploiting `Subjective&apos; Annotations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Human Judgments in Computational Linguistics, Coling</booktitle>
<location>Manchester, UK.</location>
<marker>Reidsma, den Akker, 2008</marker>
<rawString>Dennis Reidsma, and Rieks op den Akker. 2008. Exploiting `Subjective&apos; Annotations. In Proceedings of the Workshop on Human Judgments in Computational Linguistics, Coling 2008, 23 August 2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability measurements without limits.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>343</volume>
<pages>319--336</pages>
<contexts>
<context position="8628" citStr="Reidsma &amp; Carletta, 2008" startWordPosition="1224" endWordPosition="1227">nnotator 2 (A2) developed a 5-point scale. Because the two scales were different, κ could not be used to measure agreement, so a Spearman rank-order correlation (rS) was used, yielding a promising value (rS=0.82). Annotator protocols were completed at the end of the pilot study. A full experiment was conducted. Each annotator used her protocol to assign a coherence quality score to each essay. Annotators assigned a score and wrote brief comments as explanation (drawing from the protocol). Comments provided a score supplement that could be used to support analyses beyond quantitative measures (Reidsma &amp; Carletta, 2008). The data were annotated in 12 batches (by task) composed of 75 essays (50 unique; 25 for double annotation). A Spearman rank-order correlation was computed on the double-scored essays for completed batches. If the correlation fell below 0.70 (which was infrequent), one of the authors reviewed the annotator scores and comments to look for inconsistencies. Agreement was re-computed when annotator revisions were completed to ensure interrater agreement of 0.70. Annotations were completed over approximately 4 weeks to accommodate annotator schedules. While a time log was not strictly maintained,</context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma, and Jean Carletta. 2008. Reliability measurements without limits. Computational Linguistics, 343: 319-336.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Mark Shermis</author>
</authors>
<title>to appear. State-of-the-art automated essay scoring: Competition, results, and future directions from a United States demonstration. Assessing Writing.</title>
<marker>Shermis, </marker>
<rawString>Mark Shermis. to appear. State-of-the-art automated essay scoring: Competition, results, and future directions from a United States demonstration. Assessing Writing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>M Harrington</author>
<author>P White</author>
</authors>
<title>Detecting Breakdowns in Local Coherence in the Writing of Chinese English Speakers.</title>
<date>2012</date>
<journal>The Journal of Computer Assisted Learning.</journal>
<volume>28</volume>
<pages>396--410</pages>
<contexts>
<context position="2542" citStr="Wang et al., 2012" startWordPosition="328" endWordPosition="331">protocols reflecting their intuitions about essay coherence, thus reducing standard front-end resources. The paper presents related work (Section 2), the experimental annotation (Section 3), system evaluations (Section 4), and conclusions (Section 5). 2 Related Work Even after extensive training, subjective tasks may yield low inter-annotator agreement (Burstein &amp; Wolska, 2003; Reidsma &amp; op den Akker, 2008; Burstein et al., 2013). Front-end annotation activities may require significant resources (protocol development and annotator training) (Miltsakaki and Kukich, 2000; Higgins, et al., 2004; Wang et al., 2012; Burstein et al., 2013). Burstein et al (2013) reviewed coherence features as discussed in cognitive psychology (Graesser et al., 2004), reading research (Van den Broek, 2012), and computational linguistics, and concluded that evaluating text coherence is highly personal , relying on a variety of features, including adherence to standard writing conventions (e.g., grammar), and patterns of rhetorical structure and vocabulary usage. They describe an annotation protocol that uses a 3-point coherence quality scale (3 (high), 2 (somewhat,) and 1 (low)) applied by 2 annotators to label 1,500 test-</context>
</contexts>
<marker>Wang, Harrington, White, 2012</marker>
<rawString>Y. Wang, M. Harrington, and P. White. 2012. Detecting Breakdowns in Local Coherence in the Writing of Chinese English Speakers. The Journal of Computer Assisted Learning. 28: 396–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Van den Broek</author>
</authors>
<title>Individual and developmental differences in reading comprehension: Assessing cognitive processes and outcomes.</title>
<date>2012</date>
<pages>39--58</pages>
<editor>O&apos;Reilly, T. (Eds.),</editor>
<publisher>Littlefield Education.</publisher>
<location>In: Sabatini, J.P., Albro, E.R.,</location>
<marker>Van den Broek, 2012</marker>
<rawString>Paul Van den Broek. 2012. Individual and developmental differences in reading comprehension: Assessing cognitive processes and outcomes. In: Sabatini, J.P., Albro, E.R., O&apos;Reilly, T. (Eds.), Measuring up: Advances in how we assess reading ability., pp. 39-58. Lanham: Rowman &amp; Littlefield Education.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
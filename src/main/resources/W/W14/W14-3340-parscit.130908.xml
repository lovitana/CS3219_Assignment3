<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000237">
<bodyText confidence="0.560447">
FBK-UPV-UEdin participation in the WMT14 Quality Estimation
shared-task
</bodyText>
<note confidence="0.949733571428572">
Jos´e G. C. de Souza* Jes´us Gonz´alez-Rubio* Christian Buck*
University of Trento PRHLT Group University of Edinburgh
Fondazione Bruno Kessler U. Polit`ecnica de Val`encia School of Informatics
Trento, Italy Valencia, Spain Edinburgh, Scotland, UK
desouza@fbk.eu jegonzalez@prhlt.upv.es cbuck@lantis.de
Marco Turchi, Matteo Negri
Fondazione Bruno Kessler
</note>
<email confidence="0.812387">
turchi,negri@fbk.eu
</email>
<sectionHeader confidence="0.995621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999815222222222">
This paper describes the joint submission
of Fondazione Bruno Kessler, Universitat
Polit`ecnica de Val`encia and University of
Edinburgh to the Quality Estimation tasks
of the Workshop on Statistical Machine
Translation 2014. We present our submis-
sions for Task 1.2, 1.3 and 2. Our systems
ranked first for Task 1.2 and for the Binary
and Level1 settings in Task 2.
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997996272727273">
Quality Estimation (QE) for Machine Translation
(MT) is the task of evaluating the quality of the
output of an MT system without reference transla-
tions. Within the WMT 2014 QE Shared Task four
evaluation tasks were proposed, covering both
word and sentence level QE. In this work we de-
scribe the Fondazione Bruno Kessler (FBK), Uni-
versitat Polit`ecnica de Val`encia (UPV) and Uni-
versity of Edinburgh (UEdin) approach and sys-
tem setup for the shared task.
We developed models for two sentence-level
tasks: Task 1.2, scoring for post-editing effort,
and Task 1.3, predicting post-editing time, and
for all word-level variants of Task 2, binary and
multiclass classification. As opposed to previous
editions of the shared task, this year the partici-
pants were not supplied with the MT system that
was used to produce the translation. Furthermore
no system-internal features were provided. Thus,
while the trained models are tuned to detect the
errors of a specific system the features have to be
generated independently (black-box).
</bodyText>
<sectionHeader confidence="0.986798" genericHeader="method">
2 Sentence Level QE
</sectionHeader>
<bodyText confidence="0.991706444444445">
We submitted runs to two sentence-level tasks:
Task 1.2 and Task 1.3. The first task aims at
∗Contributed equally to this work.
predicting the Human mediated Translation Edit
Rate (HTER) (Snover et al., 2006) between a sug-
gestion generated by a machine translation sys-
tem and its manually post-edited version. The
data set contains 1,104 English-Spanish sentence
pairs post-edited by one translator (896 for train-
ing and 208 for test). The second task requires
to predict the time, in milliseconds, that was re-
quired to post edit a translation given by a ma-
chine translation system. Participants are provided
with 858 English-Spanish sentence pairs, source
and suggestion, along with their respective post-
edited sentence and post-editing time in seconds
(650 data points for training and 208 for test). We
participated in the scoring mode of both tasks.
</bodyText>
<sectionHeader confidence="0.599677" genericHeader="method">
2.1 Features
</sectionHeader>
<bodyText confidence="0.999936909090909">
For our sentence-level submissions we compute
features using different resources that do not use
the MT system internals. We use the same set of
features for both Task 1.2 and 1.3.
QuEst Black-box features (quest79). We ex-
tract 79 black-box features that capture the com-
plexity, fluency and adequacy aspects of the QE
problem. These features are extracted using the
implementation provided by the QuEst framework
(Specia et al., 2013). Among them are the 17 base-
line features provided by the task organizers.
The complexity features are computed on the
source sentence and indicate the complexity of
translating the segment. Examples of these fea-
tures are the language model (LM) probabilities
of the source sentence computed in a corpus of the
source language, different surface counts like the
number of punctuation marks and the number of
tokens in the source sentence, among others.
The fluency features are computed over the
translation generated by the MT system and in-
dicate how fluent the translation is in the target
</bodyText>
<page confidence="0.980208">
322
</page>
<bodyText confidence="0.972604765957447">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 322–328,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
language. One example would again be the LM
probability of the translation given by a LM model
trained on a corpus of the target language. Another
example is the average number of occurrences of
the target word within the target segment.
The third aspect covered by the QuEst features
is the adequacy of the translation with respect to
the source sentence, i.e., how the meaning of the
source is preserved in the translation. Examples of
features are the ratio of nouns, verbs and adjectives
in the source and in the translation. For a more
detailed description of the features in this group
please refer to (Specia et al., 2013).
Word alignment (wla). Following our last
year’s submission (de Souza et al., 2013a) we ex-
plore information about word alignments to ex-
tract quantitative (amount and distribution of the
alignments) and qualitative features (importance
of the aligned terms). Our assumption is that
features that explore what is aligned can bring
improvements to tasks where sentence-level se-
mantic relations need to be identified. We train
the word alignment models with the MGIZA++
toolkit (Gao and Vogel, 2008) implementation of
the IBM models (Brown et al., 1993). The models
are built on the concatenation of Europarl, News
Commentary, and MultiUN parallel corpora made
available in the QE shared task of 2013, compris-
ing about 12.8 million sentence pairs. A more de-
tailed description of the 89 features extracted can
be found in (de Souza et al., 2013a; de Souza et
al., 2013b).
Word Posterior Probabilities (wpp). Using an
external SMT system we produce 100k-best lists
from which we derive Word Posterior Probabili-
ties as detailed in Subsection 3.1.
We use the geometric mean of these probabili-
ties to derive a sentence-level score.
Because the system that we use to produce the
N-best list is not the same that generated the sug-
gestions some suggested words never appear in the
N-best list and thus receive zero probability. To
overcome this issue we first clip the WPPs to a
minimum probability. Using a small sample of the
data to estimate this number we arrive at:
</bodyText>
<equation confidence="0.862493">
log(p)min = −2.
</equation>
<bodyText confidence="0.9981551">
N-best diversity (div). Using the same 100k-
best list as above we extract a number of measures
that grasp the spatial distribution of hypotheses in
the search space as described in (de Souza et al.,
2013a).
Word Prediction (wpred). We introduce the
use of the predictions provided by the word-level
QE system described in Section 3 to leverage in-
formation for the sentence-level tasks. We com-
bine the binary word-level predictions in different
ways, with the objective of measuring the fluency
of the translation in a more fine-grained way. We
target a quantitative aspect of the words by com-
puting ratios of OK or BAD predictions. Further-
more, we also explore a qualitative aspect by cal-
culating ratios of different classes of words given
by their part-of-speech tags, indicating the qual-
ity of distinct meaningful regions that compose the
translation sentence. In total, we compute 18 fea-
tures:
</bodyText>
<listItem confidence="0.921804307692308">
• number of OK predictions divided by the no.
of words in the translation sentence (1 fea-
ture);
• number of OK function/content words predic-
tions divided by the no. of function/content
words in the translation (2 features);
• number of OK nouns, verbs, proper-nouns,
adjective, pronouns predictions divided by
the total nouns, verbs, proper-nouns, adjec-
tive, pronouns (5 features);
• size of the longest sequence of OK/BAD word
predictions divided by the total number of
OK/BAD predictions in the translation (2 fea-
tures);
• number of OK predicted n-grams divided by
the total number of n-grams in the transla-
tion. We vary n from 2 to 5 (4 features);
• number of words predicted as OK in the
first/second half of the translation divided by
the total number of words in the first/second
half of the translation (2 features).
• number of words predicted as OK in the
first/second quarter of the translation di-
vided by the total number of words in the
first/second quarter of the translation (2 fea-
tures).
</listItem>
<bodyText confidence="0.999619">
For some instances of the sentence-level tasks
we were not able to produce word-level predic-
tions due to an incomplete overlap between the
word-level and sentence-level tasks datasets. For
such data points we use the median of the feature
column for Task 1.2 and the mean for Task 1.3.
</bodyText>
<page confidence="0.9972">
323
</page>
<table confidence="0.879896916666667">
Method Features
SVR baseline
ET baseline
ET quest79 + wla + wpp
ET quest79 + wla + wpp + div2
ET quest79 + wla + wpp + div + wpred1
Train T1.2 Train T1.3 Test T1.2 Test T1.3
16.90 16864 15.23 21490
16.25 17888 17.73 19400
15.62 17474 14.44 18658
15.57 17471 14.38 18693
15.05 16392 12.89 17477
</table>
<tableCaption confidence="0.84701325">
Table 1: Training and test results for Task 1.2 and 1.3. Scores are the MAE on a development set
randomly sampled from the training data (20%). Baseline features were provided by the shared task
organizers. We used Support Vector Machines (SVM) regression to train the baseline models (first row).
Submissions are marked with 1 and 2 for primary and secondary, respectively.
</tableCaption>
<subsectionHeader confidence="0.998411">
2.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999992861111111">
We build the sentence-level models for both tasks
(T1.2 and T1.3) with the features described in Sec-
tion 2.1 using one learning algorithm: extremely
randomized trees (ET) (Geurts et al., 2006). ET is
an ensemble of randomized trees in which each
decision tree can be parameterized differently.
When a tree is built, the node splitting step is done
at random by picking the best split among a ran-
dom subset of the input features. All the trees
are grown on the whole training set and the re-
sults of the individual trees are combined by aver-
aging their predictions. The models produced by
this method demonstrated to be robust to a large
number of input features. For our experiments and
submissions we used the ET implementation in-
cluded in the Scikit-learn library (Pedregosa et al.,
2011).
During training we evaluate the models on a
development set. The development set was ob-
tained by randomly sampling 20% of the training
data. The remaining 80% were used for training.
The training process was carried out by optimiz-
ing the ET hyper-parameters with 100 iterations
of random search optimization (Bergstra and Ben-
gio, 2012) set to minimize the mean absolute er-
ror (MAE)1 on 10-fold cross-validation over the
training data. The ET hyper-parameters optimized
are: the number of decision trees in the ensemble,
the maximum number of features to consider when
looking for the best split, the maximum depth of
the trees used in the ensembles, the minimal num-
ber of samples required to split a node of the tree,
and the minimum number of samples in newly cre-
ated leaves. For the final submissions we run the
random search with 1000 iterations over the whole
training dataset.
</bodyText>
<equation confidence="0.987058">
�N i=1 |H(si)−V (si)|
1Given by MAE = , where H(si) is
N
</equation>
<bodyText confidence="0.9314555">
the hypothesis score for the entry si and V (si) is the gold
standard value for si in a dataset with N entries.
</bodyText>
<subsectionHeader confidence="0.936014">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.99998517948718">
We train models on different combinations of fea-
ture groups (described in Section 2.1). Experi-
ments results are summarized in Table 1. We have
results with baseline features for both SVR and the
ET models. For Task 1.2, adding features from dif-
ferent groups leads to increasing improvements.
The combination of the quest79, wla and wpp
groups outperforms the SVR baseline for Task 1.2
but not for Task 1.3. However, when compared
to the ET model trained with the baseline fea-
tures, it is possible to observe improvements with
this group of features. In addition, adding the
div group on top of the previous three leads to
marginal improvements for both tasks. The best
feature combination is given when adding the fea-
tures based on the word-level predictions, config-
uring the combination of all the feature groups to-
gether (a total of 221 features). For both tasks
this is our primary submission. The contrastive
run for both tasks is the best feature group com-
bination without the word-prediction-based fea-
tures, quest79, wla, wpp and div for Task 1.2 and
quest79, wla, wpp for Task 1.3.
Results on the test set can be found in the two
last columns of Table 1 and are in line with what
we found in the training phase. The rows that do
not correspond to the official submissions and that
are reported on the test set are experiments done
after the evaluation phase. For both tasks the im-
provements increase as we add features on top of
the baseline feature set and the best performance
is reached when using the word prediction fea-
tures with all the other features. The SVR base-
lines performance are the official numbers pro-
vided by the organizers. For Task 1.2 our primary
submission achieves a MAE score lower than the
score achieved during the training phase, show-
ing that the model is robust. For Task 1.3, how-
ever, we do not observe such trend. Even though
</bodyText>
<page confidence="0.995233">
324
</page>
<bodyText confidence="0.999777">
the primary submission for this task consistently
improves over the other feature combinations, it
does not outperform the score obtained during the
training phase. This might be explained due to
the difference in the distribution between train-
ing and test labels. In Task 1.2 the two distri-
butions are more similar than in Task 1.3, which
presents slightly different distributions between
training and test data.
</bodyText>
<sectionHeader confidence="0.999242" genericHeader="method">
3 Word-Level QE
</sectionHeader>
<bodyText confidence="0.983766315789474">
Task 2 is the word-level quality estimation of auto-
matically translated news sentences without given
reference translations. Participants are required to
produce a label for each word in one or more of
the following settings:
Binary classification: a OK/BAD label, where
BAD indicates the need for editing the word.
Level1 classification: OK, Accuracy, or
Fluency label specifying a coarser level of
errors for each word, or OK for words with
no error.
Multi-Class classification: one of the 20 error la-
bels described in the shared-task description
or OK for words with no error.
We submit word-level quality estimations for
the English-Spanish translation direction. The cor-
pus contains 1957 training sentences for a total of
47411 Spanish words, and 382 test sentences for a
total of 9613 words.
</bodyText>
<subsectionHeader confidence="0.894955">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.984778869565217">
Word Posterior Probabilities (WPP) In order
to generate an approximation of the decoder’s
search space as well as an N-best list of possi-
ble translations we re-translate the source using
the system that is available for the 2013 WMT QE
Shared Task (Bojar et al., 2013).
Certainly, there is a mismatch between the orig-
inal system and the one that we used but, since our
system was trained using the same news domain
as the QE data, we assume that both face similar
ambiguous words or possible reorderings. Using
this system we generate a 100k-best list which is
the foundation of several features.
We extract a set of word-level features based on
posterior probabilities computed over N-best lists
as proposed by previous works (Blatz et al., 2004;
Ueffing and Ney, 2007; Sanchis et al., 2007).
Consider a target word ei belonging to a transla-
tion e = e1 ... ei ... ejej generated from a source
sentence f. Let N(f) be the list of N-best trans-
lations for f. We compute features as the nor-
malized sum of probabilities of those translations
S(ei) C N(f) that “contain” word ei:
</bodyText>
<equation confidence="0.996691333333333">
1
Ee,,EN(f) P(e&apos;&apos;  |f) �S( P(e&apos;  |f) (1)
e ES(ei)
</equation>
<bodyText confidence="0.9998772">
where P(e|f) is the probability translation e given
source sentence f according to the SMT model.
We follow (Zens and Ney, 2006) and extract
three different WPP features depending on the
criteria chosen to compute S(ei):
</bodyText>
<equation confidence="0.946921">
S(ei) = {e&apos; E N(f)  |a=Le(e&apos;, e)∧e&apos;ai = ei}
S(ei) contain those translations e&apos; for which the
word Levenshtein-aligned (Levenshtein, 1966) to
position i in e is equal to ei.
S(ei) = {e&apos; E N(f)  |e&apos;i = ei}
</equation>
<bodyText confidence="0.984499566666667">
A second option is to select those translations
e&apos; that contain the word ei at position i.
S(ei) = {e&apos; E N(f)  |li&apos; : e&apos;i, = ei}
As third option, we select those translations e&apos;
that contain the word ei, disregarding its position.
Confusion Networks (CN) We use the same N-
best list used to compute the WPP features in the
previous section to compute features based on the
graph topology of confusion networks (Luong et
al., 2014). First, we Levenshtein-align all trans-
lations in the N-best list using e as skeleton, and
merge all of them into a confusion network. In this
network, each word-edge is labelled with the pos-
terior probability of the word. The output edges of
each node define different confusion sets of words,
each word belonging to one single confusion set.
Each complete path passing through all nodes in
the network represents one sentence in the N-best
list, and must contain exactly one link from each
confusion set. Looking to the confusion set which
the hypothesis word belongs to, we extract four
different features: maximum and minimum proba-
bility in the set (2 features), number of alternatives
in the set (1 feature) and entropy of the alternatives
in the set (1 feature).
Language Models (LM) As language model
features we produced n-gram length/backoff be-
haviour and conditional probabilities for every
word in the sentence. We employed both an inter-
polated LM taken from the MT system discussed
</bodyText>
<page confidence="0.998111">
325
</page>
<bodyText confidence="0.98473729787234">
in Section 3 as well as a very large LM which we
built on 62 billion tokens of monolingual data ex-
tracted from Common Crawl, a public web crawl.
While generally following the procedure of Buck
et al. (2014) we apply an additional lowercasing
step before training the model.
Word Lexicons (WL) We compute two dif-
ferent features based on statistical word lexi-
cons (Blatz et al., 2004):
Avg. probability: f�+1 ��f 0 P(ei  |fj)
Max. probability: max0≤j≤|f |P(ei I fj)
where P(e I f) is a probabilistic lexicon, and f0 is
the source “NULL” word (Brown et al., 1993).
POS tags (POS) We extract the part-of-speech
(POS) tags for both source and translation sen-
tences using TreeTagger (Schmid, 1994). We use
the actual POS tag of the target word as a feature.
Specifically, we represent it as a one-hot indicator
vector where all values are equal to zero except
the one representing the current tag of the word,
which is set to one. Regarding the source POS
tags, we first compute the lexical probability of
each target word given each source word. Then,
we compute two different feature vectors for each
target word. On the one hand, we use an indica-
tor vector to represent the POS tag of the maxi-
mum probability source word. On the other hand,
we sum up the indicator vectors for all the source
words each one weighted by the lexical probability
of the corresponding word. As a result, we obtain
a vector that represents the probability distribution
of source POS tags for each target word. Addi-
tionally, we extract a binary feature that indicates
whether the word is a stop word or not.2
Stacking (S) Finally, we also exploit the diverse
granularity of the word labels. The word classes
for the Level1 and Multi-class conditions are fine
grained versions of the Binary annotation, i.e. the
OK examples are the same for all cases.
We re-use our binary predictions as an addi-
tional feature for the finer-grained classes. How-
ever, due to time constrains, we were not able to
run the proper nested cross-validation but used a
model trained on all available data, which there-
fore over-fits on the training data. Cross-validation
results using the stacking approach are thus very
optimistic.
</bodyText>
<footnote confidence="0.941965">
2https://code.google.com/p/stop-words/
</footnote>
<subsectionHeader confidence="0.99778">
3.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.999990625">
We use bidirectional long short-term memory
recurrent neural networks (BLSTM-RNNs) as
implemented in the RNNLib package (Graves,
2008). Recurrent neural networks are a connec-
tionist model containing a self-connected hidden
layer. The recurrent connection provides informa-
tion of previous inputs, hence, the network can
benefit from past contextual information. Long
short-term memory is an advanced RNN archi-
tecture that allows context information over long
periods of time. Finally, BLSTM-RNNs com-
bine bidirectional recurrent neural networks and
the long short-term memory architecture allowing
forward and backward context information. Us-
ing such context modelling classifier we can avoid
the use of context-based features that have been
shown to lead to only slight improvements in QE
accuracy (Gonz´alez-Rubio et al., 2013).
As a secondary binary model we train a CRF.
Our choice of implementation is Pocket CRF3
which, while currently unmaintained, implements
continuous valued features. We use a history of
size 2 for all features and perform 10-fold cross-
validation, training on 9 folds each time.
</bodyText>
<subsectionHeader confidence="0.997934">
3.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999966130434782">
The free parameters of the BLSTM-RNNs are op-
timized by 10-fold cross-validation on the train-
ing set. Each cross-validation experiment con-
sider eight folds for training, one held-out fold
for development, and a final held-out fold for test-
ing. We estimate the neural network with the eight
training folds using the prediction performance in
the validation fold as stopping criterion. The re-
sult of each complete cross-validation experiment
is the average of the results for the predictions of
the ten held-out test folds. Additionally, to avoid
noise due to the random initialization of the net-
work, we repeat each cross-validation experiment
ten times and average the results. Once the opti-
mal values of the free parameters are established,
we estimate a new BLSTM-RNN using the full
training corpus and we use it as the final model
to predict the class labels of the test words.
Since our objective is to detect words that need
to be edited, we use the weighted averaged F1
score over the different class labels that denote an
error as our main performance metric (wF1err).
We also report the weighted averaged F1 scores
</bodyText>
<footnote confidence="0.969206">
3http://pocket-crf-1.sourceforge.net/
</footnote>
<page confidence="0.992891">
326
</page>
<table confidence="0.897551666666667">
Binary Level1 MultiClass
Method Features wF1err wF1all wF1err wF1all wF1err wF1all
BLSTM-RNNs LM+WPP+CN+WL 35.9 63.0 23.7 59.4 10.7 55.5
+POS 38.51 62.7 26.71 59.5 12.71 55.5
+Stacking — 82.92 93.9 64.72 88.0
CRF LM+WPP+CN+WL+POS 39.52 62.4 0 — — — —
Table 2: Cross-validation results for the different setups tested for Task 2. Our two submissions are
marked as (1) and (2) respectively.
over all the classes (wF1all).
</table>
<sectionHeader confidence="0.397555" genericHeader="evaluation">
3.4 Results
</sectionHeader>
<bodyText confidence="0.999338421052632">
Table 2 presents the wF1err and wF1all scores
for different sets of features. Our initial experi-
ment includes language model (LM), word poste-
rior probability (WPP), confusion network (CN),
and word lexicon (WL) features for a total of 11
features. We extend this basic feature set with the
indicator features based on POS tags for a total of
163 features. We further extend the feature vectors
by adding the stacking feature in a total of 164 fea-
tures.
Analyzing the results we observe that prediction
accuracy is quite low. Our hypothesis is that this is
due to the skewed class distribution. Even for the
binary classification scenario (the most balanced
of the three conditions), OK labels account for two
thirds of the samples. This effect worsens with in-
creasing number of error classes and the resulting
sparsity of observations. As a result, the system
tends to classify all samples as OK which leads to
the low F1 scores presented in Table 2.
We can observe that the use of POS tags indica-
tor features clearly improved the prediction accu-
racy of the systems in the three conditions. This
setup is our primary submission for the three con-
ditions of task 2.
In addition, we observe that the use of the stack-
ing feature provides a considerable improvement
in prediction accuracy for Level1 and MultiClass.
As discussed above the cross-validation results for
the stacking features are very optimistic. Test pre-
dictions using this setup are our contrastive sub-
mission for Level1 and MultiClass conditions.
Results achieved on the official test set can be
found in Table 3. Much in line with our cross-
validation results the stacking-features prove help-
ful, albeit by a much lower margin. For the bi-
nary task the RNN model strongly outperforms the
CRF.
</bodyText>
<table confidence="0.99960375">
Setup Binary Level1 MultiClass
BLSTM-RNN 48.7 37.2 17.1
+ Stacking — 38.5 23.1
CRF 42.6 — —
</table>
<tableCaption confidence="0.980878">
Table 3: Test results for Task 2. Numbers are
weighted averaged F1 scores (%) for all but the
OK class.
</tableCaption>
<sectionHeader confidence="0.999084" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999993">
This paper describes the approaches and system
setups of FBK, UPV and UEdin in the WMT14
Quality Estimation shared-task. In the sentence-
level QE tasks 1.2 (predicting post-edition effort)
and 1.3 (predicting post-editing time, in ms) we
explored different features and predicted with a
supervised tree-based ensemble learning method.
We were able to improve our results by explor-
ing features based on the word-level predictions
made by the system developed for Task 2. Our best
system for Task 1.2 ranked first among all partici-
pants.
In the word-level QE task (Task 2), we explored
different sets of features using a BLSTM-RNN as
our classification model. Cross-validation results
show that POS indicator features, despite sparse,
were able to improve the results of the baseline
features. Also, the use of the stacking feature pro-
vided a big leap in prediction accuracy. With this
model, we ranked first in the Binary and Level1
settings of Task 2 in the evaluation campaign.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998336">
This work was supported by the MateCat and Cas-
macat projects, which are funded by the EC un-
der the 7th Framework Programme. The authors
would like to thank Francisco ´Alvaro Mu˜noz for
providing the RNN classification software.
</bodyText>
<page confidence="0.996888">
327
</page>
<sectionHeader confidence="0.996243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99924835483871">
James Bergstra and Yoshua Bengio. 2012. Random
Search for Hyper-Parameter Optimization. Journal
of Machine Learning Research, 13:281–305.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence es-
timation for machine translation. In Proceedings of
the international conference on Computational Lin-
guistics, pages 315–321.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19:263–311.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram Counts and Language Models from
the Common Crawl. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
Jos´e G. C. de Souza, Christian Buck, Marco Turchi,
and Matteo Negri. 2013a. FBK-UEdin participation
to the WMT13 Quality Estimation shared-task. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 352–358.
Jos´e G. C. de Souza, Miquel Espl´a-Gomis, Marco
Turchi, and Matteo Negri. 2013b. Exploiting qual-
itative information from automatic word alignment
for cross-lingual nlp tasks. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 771–776.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49–57.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3–42.
Jes´us Gonz´alez-Rubio, Jos´e R. Navarro-Cerdan, and
Francisco Casacuberta. 2013. Partial least squares
for word confidence estimation in machine transla-
tion. In 6th Iberian Conference on Pattern Recog-
nition and Image Analysis, (IbPRIA) LNCS 7887,
pages 500–508. Springer.
Alex Graves. 2008. Rnnlib: A recurrent neural
network library for sequence learning problems.
http://sourceforge.net/projects/
rnnl/.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707–710.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014. Word confidence estimation and
its integration in sentence quality estimation for ma-
chine translation. In Knowledge and Systems Engi-
neering, volume 244, pages 85–98. Springer.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn : Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Alberto Sanchis, Alfons Juan, and Enrique Vidal.
2007. Estimation of confidence measures for ma-
chine translation. In Proceedings of the Machine
Translation Summit XI, pages 407–412.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44–49.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Association for Machine Translation in
the Americas.
Lucia Specia, Kashif Shah, Jos´e G. C. de Souza, and
Trevor Cohn. 2013. QuEst–a translation quality es-
timation framework. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics, pages 79–84.
Nicola Ueffing and Hermann Ney. 2007. Word-
level confidence estimation for machine translation.
Computational Linguistics, 33:9–40.
Richard Zens and Hermann Ney. 2006. N-gram poste-
rior probabilities for statistical machine translation.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 72–77.
</reference>
<page confidence="0.998334">
328
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.154417">
<title confidence="0.9287375">FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task</title>
<author confidence="0.989293">G C de_PRHLT University of Edinburgh School of Informatics Edinburgh</author>
<author confidence="0.989293">UK cbucklantis de Scotland</author>
<affiliation confidence="0.636763">University of U. Polit`ecnica de Fondazione Bruno Valencia,</affiliation>
<address confidence="0.62169">Trento, jegonzalez@prhlt.upv.es</address>
<author confidence="0.922955">desouzafbk eu Marco Turchi</author>
<author confidence="0.922955">Matteo Fondazione Bruno</author>
<email confidence="0.991366">turchi,negri@fbk.eu</email>
<abstract confidence="0.9704825">This paper describes the joint submission of Fondazione Bruno Kessler, Universitat Polit`ecnica de Val`encia and University of Edinburgh to the Quality Estimation tasks of the Workshop on Statistical Machine Translation 2014. We present our submissions for Task 1.2, 1.3 and 2. Our systems ranked first for Task 1.2 and for the Binary and Level1 settings in Task 2.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Bergstra</author>
<author>Yoshua Bengio</author>
</authors>
<title>Random Search for Hyper-Parameter Optimization.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>13--281</pages>
<contexts>
<context position="10074" citStr="Bergstra and Bengio, 2012" startWordPosition="1643" endWordPosition="1647">e individual trees are combined by averaging their predictions. The models produced by this method demonstrated to be robust to a large number of input features. For our experiments and submissions we used the ET implementation included in the Scikit-learn library (Pedregosa et al., 2011). During training we evaluate the models on a development set. The development set was obtained by randomly sampling 20% of the training data. The remaining 80% were used for training. The training process was carried out by optimizing the ET hyper-parameters with 100 iterations of random search optimization (Bergstra and Bengio, 2012) set to minimize the mean absolute error (MAE)1 on 10-fold cross-validation over the training data. The ET hyper-parameters optimized are: the number of decision trees in the ensemble, the maximum number of features to consider when looking for the best split, the maximum depth of the trees used in the ensembles, the minimal number of samples required to split a node of the tree, and the minimum number of samples in newly created leaves. For the final submissions we run the random search with 1000 iterations over the whole training dataset. �N i=1 |H(si)−V (si)| 1Given by MAE = , where H(si) i</context>
</contexts>
<marker>Bergstra, Bengio, 2012</marker>
<rawString>James Bergstra and Yoshua Bengio. 2012. Random Search for Hyper-Parameter Optimization. Journal of Machine Learning Research, 13:281–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the international conference on Computational Linguistics,</booktitle>
<pages>315--321</pages>
<contexts>
<context position="14655" citStr="Blatz et al., 2004" startWordPosition="2428" endWordPosition="2431"> of possible translations we re-translate the source using the system that is available for the 2013 WMT QE Shared Task (Bojar et al., 2013). Certainly, there is a mismatch between the original system and the one that we used but, since our system was trained using the same news domain as the QE data, we assume that both face similar ambiguous words or possible reorderings. Using this system we generate a 100k-best list which is the foundation of several features. We extract a set of word-level features based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Consider a target word ei belonging to a translation e = e1 ... ei ... ejej generated from a source sentence f. Let N(f) be the list of N-best translations for f. We compute features as the normalized sum of probabilities of those translations S(ei) C N(f) that “contain” word ei: 1 Ee,,EN(f) P(e&apos;&apos; |f) �S( P(e&apos; |f) (1) e ES(ei) where P(e|f) is the probability translation e given source sentence f according to the SMT model. We follow (Zens and Ney, 2006) and extract three different WPP features depending on the criteria chosen to compute S(ei): S(</context>
<context position="17264" citStr="Blatz et al., 2004" startWordPosition="2885" endWordPosition="2888">ature). Language Models (LM) As language model features we produced n-gram length/backoff behaviour and conditional probabilities for every word in the sentence. We employed both an interpolated LM taken from the MT system discussed 325 in Section 3 as well as a very large LM which we built on 62 billion tokens of monolingual data extracted from Common Crawl, a public web crawl. While generally following the procedure of Buck et al. (2014) we apply an additional lowercasing step before training the model. Word Lexicons (WL) We compute two different features based on statistical word lexicons (Blatz et al., 2004): Avg. probability: f�+1 ��f 0 P(ei |fj) Max. probability: max0≤j≤|f |P(ei I fj) where P(e I f) is a probabilistic lexicon, and f0 is the source “NULL” word (Brown et al., 1993). POS tags (POS) We extract the part-of-speech (POS) tags for both source and translation sentences using TreeTagger (Schmid, 1994). We use the actual POS tag of the target word as a feature. Specifically, we represent it as a one-hot indicator vector where all values are equal to zero except the one representing the current tag of the word, which is set to one. Regarding the source POS tags, we first compute the lexica</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2004. Confidence estimation for machine translation. In Proceedings of the international conference on Computational Linguistics, pages 315–321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<contexts>
<context position="14177" citStr="Bojar et al., 2013" startWordPosition="2347" endWordPosition="2350">lass classification: one of the 20 error labels described in the shared-task description or OK for words with no error. We submit word-level quality estimations for the English-Spanish translation direction. The corpus contains 1957 training sentences for a total of 47411 Spanish words, and 382 test sentences for a total of 9613 words. 3.1 Features Word Posterior Probabilities (WPP) In order to generate an approximation of the decoder’s search space as well as an N-best list of possible translations we re-translate the source using the system that is available for the 2013 WMT QE Shared Task (Bojar et al., 2013). Certainly, there is a mismatch between the original system and the one that we used but, since our system was trained using the same news domain as the QE data, we assume that both face similar ambiguous words or possible reorderings. Using this system we generate a 100k-best list which is the foundation of several features. We extract a set of word-level features based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Consider a target word ei belonging to a translation e = e1 ... ei ... eje</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5141" citStr="Brown et al., 1993" startWordPosition="802" endWordPosition="805">n of the features in this group please refer to (Specia et al., 2013). Word alignment (wla). Following our last year’s submission (de Souza et al., 2013a) we explore information about word alignments to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms). Our assumption is that features that explore what is aligned can bring improvements to tasks where sentence-level semantic relations need to be identified. We train the word alignment models with the MGIZA++ toolkit (Gao and Vogel, 2008) implementation of the IBM models (Brown et al., 1993). The models are built on the concatenation of Europarl, News Commentary, and MultiUN parallel corpora made available in the QE shared task of 2013, comprising about 12.8 million sentence pairs. A more detailed description of the 89 features extracted can be found in (de Souza et al., 2013a; de Souza et al., 2013b). Word Posterior Probabilities (wpp). Using an external SMT system we produce 100k-best lists from which we derive Word Posterior Probabilities as detailed in Subsection 3.1. We use the geometric mean of these probabilities to derive a sentence-level score. Because the system that we</context>
<context position="17441" citStr="Brown et al., 1993" startWordPosition="2917" endWordPosition="2920">h an interpolated LM taken from the MT system discussed 325 in Section 3 as well as a very large LM which we built on 62 billion tokens of monolingual data extracted from Common Crawl, a public web crawl. While generally following the procedure of Buck et al. (2014) we apply an additional lowercasing step before training the model. Word Lexicons (WL) We compute two different features based on statistical word lexicons (Blatz et al., 2004): Avg. probability: f�+1 ��f 0 P(ei |fj) Max. probability: max0≤j≤|f |P(ei I fj) where P(e I f) is a probabilistic lexicon, and f0 is the source “NULL” word (Brown et al., 1993). POS tags (POS) We extract the part-of-speech (POS) tags for both source and translation sentences using TreeTagger (Schmid, 1994). We use the actual POS tag of the target word as a feature. Specifically, we represent it as a one-hot indicator vector where all values are equal to zero except the one representing the current tag of the word, which is set to one. Regarding the source POS tags, we first compute the lexical probability of each target word given each source word. Then, we compute two different feature vectors for each target word. On the one hand, we use an indicator vector to rep</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Buck</author>
<author>Kenneth Heafield</author>
<author>Bas van Ooyen</author>
</authors>
<title>N-gram Counts and Language Models from the Common Crawl.</title>
<date>2014</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference.</booktitle>
<marker>Buck, Heafield, van Ooyen, 2014</marker>
<rawString>Christian Buck, Kenneth Heafield, and Bas van Ooyen. 2014. N-gram Counts and Language Models from the Common Crawl. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Christian Buck</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>FBK-UEdin participation to the WMT13 Quality Estimation shared-task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>352--358</pages>
<marker>de Souza, Buck, Turchi, Negri, 2013</marker>
<rawString>Jos´e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013a. FBK-UEdin participation to the WMT13 Quality Estimation shared-task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Miquel Espl´a-Gomis</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>Exploiting qualitative information from automatic word alignment for cross-lingual nlp tasks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>771--776</pages>
<marker>de Souza, Espl´a-Gomis, Turchi, Negri, 2013</marker>
<rawString>Jos´e G. C. de Souza, Miquel Espl´a-Gomis, Marco Turchi, and Matteo Negri. 2013b. Exploiting qualitative information from automatic word alignment for cross-lingual nlp tasks. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 771–776.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="5087" citStr="Gao and Vogel, 2008" startWordPosition="793" endWordPosition="796"> and in the translation. For a more detailed description of the features in this group please refer to (Specia et al., 2013). Word alignment (wla). Following our last year’s submission (de Souza et al., 2013a) we explore information about word alignments to extract quantitative (amount and distribution of the alignments) and qualitative features (importance of the aligned terms). Our assumption is that features that explore what is aligned can bring improvements to tasks where sentence-level semantic relations need to be identified. We train the word alignment models with the MGIZA++ toolkit (Gao and Vogel, 2008) implementation of the IBM models (Brown et al., 1993). The models are built on the concatenation of Europarl, News Commentary, and MultiUN parallel corpora made available in the QE shared task of 2013, comprising about 12.8 million sentence pairs. A more detailed description of the 89 features extracted can be found in (de Souza et al., 2013a; de Souza et al., 2013b). Word Posterior Probabilities (wpp). Using an external SMT system we produce 100k-best lists from which we derive Word Posterior Probabilities as detailed in Subsection 3.1. We use the geometric mean of these probabilities to der</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Geurts</author>
<author>Damien Ernst</author>
<author>Louis Wehenkel</author>
</authors>
<title>Extremely randomized trees.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>63--1</pages>
<contexts>
<context position="9140" citStr="Geurts et al., 2006" startWordPosition="1484" endWordPosition="1487">18693 15.05 16392 12.89 17477 Table 1: Training and test results for Task 1.2 and 1.3. Scores are the MAE on a development set randomly sampled from the training data (20%). Baseline features were provided by the shared task organizers. We used Support Vector Machines (SVM) regression to train the baseline models (first row). Submissions are marked with 1 and 2 for primary and secondary, respectively. 2.2 Experimental Setup We build the sentence-level models for both tasks (T1.2 and T1.3) with the features described in Section 2.1 using one learning algorithm: extremely randomized trees (ET) (Geurts et al., 2006). ET is an ensemble of randomized trees in which each decision tree can be parameterized differently. When a tree is built, the node splitting step is done at random by picking the best split among a random subset of the input features. All the trees are grown on the whole training set and the results of the individual trees are combined by averaging their predictions. The models produced by this method demonstrated to be robust to a large number of input features. For our experiments and submissions we used the ET implementation included in the Scikit-learn library (Pedregosa et al., 2011). D</context>
</contexts>
<marker>Geurts, Ernst, Wehenkel, 2006</marker>
<rawString>Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine Learning, 63(1):3–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Jos´e R Navarro-Cerdan</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Partial least squares for word confidence estimation in machine translation.</title>
<date>2013</date>
<booktitle>In 6th Iberian Conference on Pattern Recognition and Image Analysis, (IbPRIA) LNCS 7887,</booktitle>
<pages>500--508</pages>
<publisher>Springer.</publisher>
<marker>Gonz´alez-Rubio, Navarro-Cerdan, Casacuberta, 2013</marker>
<rawString>Jes´us Gonz´alez-Rubio, Jos´e R. Navarro-Cerdan, and Francisco Casacuberta. 2013. Partial least squares for word confidence estimation in machine translation. In 6th Iberian Conference on Pattern Recognition and Image Analysis, (IbPRIA) LNCS 7887, pages 500–508. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Rnnlib: A recurrent neural network library for sequence learning problems.</title>
<date>2008</date>
<note>http://sourceforge.net/projects/ rnnl/.</note>
<contexts>
<context position="19251" citStr="Graves, 2008" startWordPosition="3217" endWordPosition="3218">Binary annotation, i.e. the OK examples are the same for all cases. We re-use our binary predictions as an additional feature for the finer-grained classes. However, due to time constrains, we were not able to run the proper nested cross-validation but used a model trained on all available data, which therefore over-fits on the training data. Cross-validation results using the stacking approach are thus very optimistic. 2https://code.google.com/p/stop-words/ 3.2 Classifiers We use bidirectional long short-term memory recurrent neural networks (BLSTM-RNNs) as implemented in the RNNLib package (Graves, 2008). Recurrent neural networks are a connectionist model containing a self-connected hidden layer. The recurrent connection provides information of previous inputs, hence, the network can benefit from past contextual information. Long short-term memory is an advanced RNN architecture that allows context information over long periods of time. Finally, BLSTM-RNNs combine bidirectional recurrent neural networks and the long short-term memory architecture allowing forward and backward context information. Using such context modelling classifier we can avoid the use of context-based features that have</context>
</contexts>
<marker>Graves, 2008</marker>
<rawString>Alex Graves. 2008. Rnnlib: A recurrent neural network library for sequence learning problems. http://sourceforge.net/projects/ rnnl/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="15390" citStr="Levenshtein, 1966" startWordPosition="2559" endWordPosition="2560">. ejej generated from a source sentence f. Let N(f) be the list of N-best translations for f. We compute features as the normalized sum of probabilities of those translations S(ei) C N(f) that “contain” word ei: 1 Ee,,EN(f) P(e&apos;&apos; |f) �S( P(e&apos; |f) (1) e ES(ei) where P(e|f) is the probability translation e given source sentence f according to the SMT model. We follow (Zens and Ney, 2006) and extract three different WPP features depending on the criteria chosen to compute S(ei): S(ei) = {e&apos; E N(f) |a=Le(e&apos;, e)∧e&apos;ai = ei} S(ei) contain those translations e&apos; for which the word Levenshtein-aligned (Levenshtein, 1966) to position i in e is equal to ei. S(ei) = {e&apos; E N(f) |e&apos;i = ei} A second option is to select those translations e&apos; that contain the word ei at position i. S(ei) = {e&apos; E N(f) |li&apos; : e&apos;i, = ei} As third option, we select those translations e&apos; that contain the word ei, disregarding its position. Confusion Networks (CN) We use the same Nbest list used to compute the WPP features in the previous section to compute features based on the graph topology of confusion networks (Luong et al., 2014). First, we Levenshtein-align all translations in the N-best list using e as skeleton, and merge all of th</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc-Quang Luong</author>
<author>Laurent Besacier</author>
<author>Benjamin Lecouteux</author>
</authors>
<title>Word confidence estimation and its integration in sentence quality estimation for machine translation.</title>
<date>2014</date>
<booktitle>In Knowledge and Systems Engineering,</booktitle>
<volume>244</volume>
<pages>85--98</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="15884" citStr="Luong et al., 2014" startWordPosition="2652" endWordPosition="2655">E N(f) |a=Le(e&apos;, e)∧e&apos;ai = ei} S(ei) contain those translations e&apos; for which the word Levenshtein-aligned (Levenshtein, 1966) to position i in e is equal to ei. S(ei) = {e&apos; E N(f) |e&apos;i = ei} A second option is to select those translations e&apos; that contain the word ei at position i. S(ei) = {e&apos; E N(f) |li&apos; : e&apos;i, = ei} As third option, we select those translations e&apos; that contain the word ei, disregarding its position. Confusion Networks (CN) We use the same Nbest list used to compute the WPP features in the previous section to compute features based on the graph topology of confusion networks (Luong et al., 2014). First, we Levenshtein-align all translations in the N-best list using e as skeleton, and merge all of them into a confusion network. In this network, each word-edge is labelled with the posterior probability of the word. The output edges of each node define different confusion sets of words, each word belonging to one single confusion set. Each complete path passing through all nodes in the network represents one sentence in the N-best list, and must contain exactly one link from each confusion set. Looking to the confusion set which the hypothesis word belongs to, we extract four different </context>
</contexts>
<marker>Luong, Besacier, Lecouteux, 2014</marker>
<rawString>Ngoc-Quang Luong, Laurent Besacier, and Benjamin Lecouteux. 2014. Word confidence estimation and its integration in sentence quality estimation for machine translation. In Knowledge and Systems Engineering, volume 244, pages 85–98. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn : Machine Learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="9737" citStr="Pedregosa et al., 2011" startWordPosition="1589" endWordPosition="1592">(ET) (Geurts et al., 2006). ET is an ensemble of randomized trees in which each decision tree can be parameterized differently. When a tree is built, the node splitting step is done at random by picking the best split among a random subset of the input features. All the trees are grown on the whole training set and the results of the individual trees are combined by averaging their predictions. The models produced by this method demonstrated to be robust to a large number of input features. For our experiments and submissions we used the ET implementation included in the Scikit-learn library (Pedregosa et al., 2011). During training we evaluate the models on a development set. The development set was obtained by randomly sampling 20% of the training data. The remaining 80% were used for training. The training process was carried out by optimizing the ET hyper-parameters with 100 iterations of random search optimization (Bergstra and Bengio, 2012) set to minimize the mean absolute error (MAE)1 on 10-fold cross-validation over the training data. The ET hyper-parameters optimized are: the number of decision trees in the ensemble, the maximum number of features to consider when looking for the best split, th</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn : Machine Learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Sanchis</author>
<author>Alfons Juan</author>
<author>Enrique Vidal</author>
</authors>
<title>Estimation of confidence measures for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Machine Translation Summit XI,</booktitle>
<pages>407--412</pages>
<contexts>
<context position="14701" citStr="Sanchis et al., 2007" startWordPosition="2436" endWordPosition="2439">he source using the system that is available for the 2013 WMT QE Shared Task (Bojar et al., 2013). Certainly, there is a mismatch between the original system and the one that we used but, since our system was trained using the same news domain as the QE data, we assume that both face similar ambiguous words or possible reorderings. Using this system we generate a 100k-best list which is the foundation of several features. We extract a set of word-level features based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Consider a target word ei belonging to a translation e = e1 ... ei ... ejej generated from a source sentence f. Let N(f) be the list of N-best translations for f. We compute features as the normalized sum of probabilities of those translations S(ei) C N(f) that “contain” word ei: 1 Ee,,EN(f) P(e&apos;&apos; |f) �S( P(e&apos; |f) (1) e ES(ei) where P(e|f) is the probability translation e given source sentence f according to the SMT model. We follow (Zens and Ney, 2006) and extract three different WPP features depending on the criteria chosen to compute S(ei): S(ei) = {e&apos; E N(f) |a=Le(e&apos;, e)∧e&apos;ai = ei} S(ei)</context>
</contexts>
<marker>Sanchis, Juan, Vidal, 2007</marker>
<rawString>Alberto Sanchis, Alfons Juan, and Enrique Vidal. 2007. Estimation of confidence measures for machine translation. In Proceedings of the Machine Translation Summit XI, pages 407–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<volume>12</volume>
<pages>44--49</pages>
<contexts>
<context position="17572" citStr="Schmid, 1994" startWordPosition="2939" endWordPosition="2940">f monolingual data extracted from Common Crawl, a public web crawl. While generally following the procedure of Buck et al. (2014) we apply an additional lowercasing step before training the model. Word Lexicons (WL) We compute two different features based on statistical word lexicons (Blatz et al., 2004): Avg. probability: f�+1 ��f 0 P(ei |fj) Max. probability: max0≤j≤|f |P(ei I fj) where P(e I f) is a probabilistic lexicon, and f0 is the source “NULL” word (Brown et al., 1993). POS tags (POS) We extract the part-of-speech (POS) tags for both source and translation sentences using TreeTagger (Schmid, 1994). We use the actual POS tag of the target word as a feature. Specifically, we represent it as a one-hot indicator vector where all values are equal to zero except the one representing the current tag of the word, which is set to one. Regarding the source POS tags, we first compute the lexical probability of each target word given each source word. Then, we compute two different feature vectors for each target word. On the one hand, we use an indicator vector to represent the POS tag of the maximum probability source word. On the other hand, we sum up the indicator vectors for all the source wo</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, volume 12, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas.</title>
<date>2006</date>
<contexts>
<context position="2096" citStr="Snover et al., 2006" startWordPosition="313" endWordPosition="316">ary and multiclass classification. As opposed to previous editions of the shared task, this year the participants were not supplied with the MT system that was used to produce the translation. Furthermore no system-internal features were provided. Thus, while the trained models are tuned to detect the errors of a specific system the features have to be generated independently (black-box). 2 Sentence Level QE We submitted runs to two sentence-level tasks: Task 1.2 and Task 1.3. The first task aims at ∗Contributed equally to this work. predicting the Human mediated Translation Edit Rate (HTER) (Snover et al., 2006) between a suggestion generated by a machine translation system and its manually post-edited version. The data set contains 1,104 English-Spanish sentence pairs post-edited by one translator (896 for training and 208 for test). The second task requires to predict the time, in milliseconds, that was required to post edit a translation given by a machine translation system. Participants are provided with 858 English-Spanish sentence pairs, source and suggestion, along with their respective postedited sentence and post-editing time in seconds (650 data points for training and 208 for test). We pa</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jos´e G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst–a translation quality estimation framework.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>79--84</pages>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jos´e G. C. de Souza, and Trevor Cohn. 2013. QuEst–a translation quality estimation framework. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Wordlevel confidence estimation for machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--9</pages>
<contexts>
<context position="14678" citStr="Ueffing and Ney, 2007" startWordPosition="2432" endWordPosition="2435">tions we re-translate the source using the system that is available for the 2013 WMT QE Shared Task (Bojar et al., 2013). Certainly, there is a mismatch between the original system and the one that we used but, since our system was trained using the same news domain as the QE data, we assume that both face similar ambiguous words or possible reorderings. Using this system we generate a 100k-best list which is the foundation of several features. We extract a set of word-level features based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Consider a target word ei belonging to a translation e = e1 ... ei ... ejej generated from a source sentence f. Let N(f) be the list of N-best translations for f. We compute features as the normalized sum of probabilities of those translations S(ei) C N(f) that “contain” word ei: 1 Ee,,EN(f) P(e&apos;&apos; |f) �S( P(e&apos; |f) (1) e ES(ei) where P(e|f) is the probability translation e given source sentence f according to the SMT model. We follow (Zens and Ney, 2006) and extract three different WPP features depending on the criteria chosen to compute S(ei): S(ei) = {e&apos; E N(f) |a=Le(</context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. Wordlevel confidence estimation for machine translation. Computational Linguistics, 33:9–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>N-gram posterior probabilities for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>72--77</pages>
<contexts>
<context position="15160" citStr="Zens and Ney, 2006" startWordPosition="2522" endWordPosition="2525">ures based on posterior probabilities computed over N-best lists as proposed by previous works (Blatz et al., 2004; Ueffing and Ney, 2007; Sanchis et al., 2007). Consider a target word ei belonging to a translation e = e1 ... ei ... ejej generated from a source sentence f. Let N(f) be the list of N-best translations for f. We compute features as the normalized sum of probabilities of those translations S(ei) C N(f) that “contain” word ei: 1 Ee,,EN(f) P(e&apos;&apos; |f) �S( P(e&apos; |f) (1) e ES(ei) where P(e|f) is the probability translation e given source sentence f according to the SMT model. We follow (Zens and Ney, 2006) and extract three different WPP features depending on the criteria chosen to compute S(ei): S(ei) = {e&apos; E N(f) |a=Le(e&apos;, e)∧e&apos;ai = ei} S(ei) contain those translations e&apos; for which the word Levenshtein-aligned (Levenshtein, 1966) to position i in e is equal to ei. S(ei) = {e&apos; E N(f) |e&apos;i = ei} A second option is to select those translations e&apos; that contain the word ei at position i. S(ei) = {e&apos; E N(f) |li&apos; : e&apos;i, = ei} As third option, we select those translations e&apos; that contain the word ei, disregarding its position. Confusion Networks (CN) We use the same Nbest list used to compute the WPP</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. N-gram posterior probabilities for statistical machine translation. In Proceedings of the Workshop on Statistical Machine Translation, pages 72–77.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
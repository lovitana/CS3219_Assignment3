<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001019">
<title confidence="0.9981235">
Addressing Class Imbalance for Improved Recognition of Implicit
Discourse Relations
</title>
<author confidence="0.993351">
Junyi Jessy Li
</author>
<affiliation confidence="0.997592">
University of Pennsylvania
</affiliation>
<email confidence="0.990918">
ljunyi@seas.upenn.edu
</email>
<author confidence="0.98728">
Ani Nenkova
</author>
<affiliation confidence="0.996237">
University of Pennsylvania
</affiliation>
<email confidence="0.995395">
nenkova@seas.upenn.edu
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999315047619048">
In this paper we address the problem of
skewed class distribution in implicit dis-
course relation recognition. We examine
the performance of classifiers for both bi-
nary classification predicting if a particu-
lar relation holds or not and for multi-class
prediction. We review prior work to point
out that the problem has been addressed
differently for the binary and multi-class
problems. We demonstrate that adopting
a unified approach can significantly im-
prove the performance of multi-class pre-
diction. We also propose an approach that
makes better use of the full annotations
in the training set when downsampling is
used. We report significant absolute im-
provements in performance in multi-class
prediction, as well as significant improve-
ment of binary classifiers for detecting the
presence of implicit Temporal, Compari-
son and Contingency relations.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938912280702">
Discourse relations holding between adjacent sen-
tences in text play an essential role in establishing
local coherence and contribute to the semantic in-
terpretation of the text. For example, the causal re-
lationship is helpful for textual entailment or ques-
tion answering while restatement and exemplifica-
tion are important for automatic summarization.
Predicting the type of implicit relations, which
are not signaled by any of the common explicit
discourse connectives such as because, however,
has proven to be a most challenging task in dis-
course analysis. The Penn Discourse Treebank
(PDTB) (Prasad et al., 2008) provided valuable
annotations of implicit relations. Most research to
date has focused on developing and refining lex-
ical and linguistically rich features for the task
(Pitler et al., 2009; Lin et al., 2009; Park and
Cardie, 2012). Mostly ignored remains the prob-
lem of addressing the highly skewed distribution
of implicit discourse relations. Only about 35% of
pairs of adjacent sentences in the PDTB are con-
nected by three of the four top level discourse re-
lation: 5% participate in Temporal relation, 10%
in Comparison (contrast) and 20% in Contingency
(causal) relations. The remaining pairs are con-
nected by the catch-all Expansion relation (40%)
or by some other linguistic devices (24%). Finer
grained relations of interest to particular applica-
tions account for increasingly smaller percentage
of the PDTB data.
Class imbalance is particularly problematic for
training a binary classifier to distinguish one rela-
tion from the rest. As we will show later, it also
impacts the performance of multi-class prediction
in which each pair of sentences is labeled with one
of the five possible relations.
All prior work has resorted to downsampling
the training data for binary classifiers to distin-
guish a particular relation and use the full train-
ing set for multi-class prediction. In this pa-
per we compare several methods for address-
ing the skewed class distribution during training:
downsampling, upsampling and computing fea-
ture weights and performing feature selection on
the unaltered full training data. A major motiva-
tion for our work is to establish if any of the alter-
natives to downsampling would prove beneficial,
because in downsampling most of the expensively
annotated data is not used in the model. In addi-
tion, we seek to align the treatment of data imbal-
ance for the binary and multi-class tasks. We show
that downsampling in general leads to the best pre-
diction accuracy but that the alternative models
provide complementary information and signifi-
cant improvement can be obtained by combining
both types of models. We also report significant
improvement of multi-class prediction accuracy,
</bodyText>
<page confidence="0.965453">
142
</page>
<note confidence="0.741943">
Proceedings of the SIGDIAL 2014 Conference, pages 142–150,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9427815">
achieved by using the alternative binary classifiers
to perform the task.
</bodyText>
<sectionHeader confidence="0.867026" genericHeader="method">
2 The Penn Discourse Treebank
</sectionHeader>
<bodyText confidence="0.998958064935065">
In the PDTB, discourse relations are viewed as a
predicate with two arguments. The predicate is
the relation, the arguments correspond to the min-
imum spans of text whose interpretations are the
abstract objects between which the relation holds.
Consider the following example of a contrast rela-
tion. The italic and bold fonts mark the arguments
of the relation.
Commonwealth Edison said the ruling could force it to slash
its 1989 earnings by $1.55 a share. [Implicit = BY COM-
PARISON] For 1988, Commonwealth Edison reported
earnings of $737.5 million, or $3.01 a share.
For explicit relations, the predicate is marked by
a discourse connective that occurs in the text, i.e.
because, however, for example.
Implicit relations are marked between adjacent
sentences in the same paragraph. They are inferred
by the reader but are not lexically marked. Alter-
native lexicalizations (AltLex) are the ones where
there is a phrase in the sentence implying the rela-
tion but the phrase itself was not one of the explicit
discourse connectives. There are 16,224 and 624
examples of implicit and AltLex relations, respec-
tively, in the PDTB.
The sense of discourse relations in the PDTB
is organized in a three-tiered hierarchy. The four
top level relations are: Temporal (the two argu-
ments are related temporally), Comparison (con-
trast), Contingency (causal) and Expansion (one
argument is the expansion of the other and contin-
ues the context) (Miltsakaki et al., 2008). These
are the classes we focus on in our work.
Finally, 5,210 pairs of adjacent sentences were
marked as related by an entity relation (EntRel),
by virtue of the repetition of the same entity or
topic. EntRels were marked only if no other rela-
tion could be identified and they are not considered
a discourse relation, rather an alternative discourse
phenomena related to entity coherence (Grosz et
al., 1995). There are 254 pairs of sentences where
no discourse relation was identified (NoRel).
Pitler et al. (2008) has shown that performance
as high as 93% in accuracy can be easily achieved
for the explicit relations, because the connective it-
self is a highly informative feature. Efforts in iden-
tifying the argument spans have also yielded high
accuracies (Lin et al., 2014; Elwell and Baldridge,
2008; Ghosh et al., 2011).
However, in the absence of a connective, recog-
nizing non-explicit relations, which includes im-
plicit relations, alternative lexicalizations, entity
relation and no relation present, has proven to be a
real challenge. Prior work on supervised implicit
discourse recognition studied a wide range of fea-
tures including lexical, syntactic, verb classes, se-
mantic groups via General Inquirer and polarity
(Pitler et al., 2009; Lin et al., 2009). Park and
Cardie (2012) studied the combination of features
and achieved better performance with a different
combination for each individual relation. Meth-
ods for improving the sparsity of lexical represen-
tations have been proposed (Hernault et al., 2010;
Biran and McKeown, 2013), as well as web-driven
approaches which reduce the problem to explicit
relation recognition (Hong et al., 2012).
Remarkably, no prior work has discussed the
highly skewed class distribution of discourse re-
lation types. The tacitly adopted solution has been
to downsample the negative examples for one-vs-
all binary classification aimed at discovering if a
particular relation holds and keeping the full train-
ing set for multi-class prediction.
To highlight the problem, in Table 1 we show
the distribution of implicit relation classes in the
entire PDTB. In our work, we aim to develop clas-
sifiers to identify the four top-level relations listed
in the table1.
</bodyText>
<table confidence="0.9989534">
# of samples Percentage
Temporal 1038 4.3%
Comparison 2550 11.3%
Contingency 4532 20%
Expansion 9082 40%
</table>
<tableCaption confidence="0.9506825">
Table 1: Distribution of implicit relations in the
PDTB.
</tableCaption>
<sectionHeader confidence="0.991806" genericHeader="method">
3 Experimental settings
</sectionHeader>
<bodyText confidence="0.992344909090909">
In our experiments, we used all non-explicit in-
stances in the PDTB sections 2-19 for training and
those in sections 20-24 for testing. Like most stud-
ies, we kept sections 0-1 as development set. In
order to ensure we have a large enough test set to
properly perform tests for statistical significance
over F scores and balanced accuracies, we did not
follow previous work (Lin et al., 2014; Park and
Cardie, 2012) that used only section 23 or sec-
tions 23-24 for testing. Also, the traditional rule
of thumb is to split the available data into training
</bodyText>
<footnote confidence="0.992093">
1The rest of the data are EntRel/NoRel.
</footnote>
<page confidence="0.998898">
143
</page>
<bodyText confidence="0.999972617021277">
and testing sets with 80%/20% ratio. Our choice
ensures that this is the case for all of the relations.
The only features that we use in our experiments
are production rules. We exclude features that oc-
cur fewer than five times in the training set. Pro-
duction rules are the state-of-the-art representation
for discourse relation recognition. This represen-
tation leads to only slightly lower results than a
system including a much larger variety of features
in the first end-to-end PDTB style discourse parser
(Lin et al., 2014) .
The production rule representation is based on
the constituency parse of the arguments and in-
cludes both syntactic and lexical information. A
production rule is the parent with an left-to-right
ordered list of all of its children in the parse tree
(for example, S→NP VP). All non-terminal nodes
are included as a parent, from the sentence head
to the part-of-speech of a terminal. Thus words
that occur in each sentence augmented with their
part of speech are part of the representation (for
example, NN→company), along with more gen-
eral structures of the sentence corresponding to
production rules with only non-terminals on the
right-hand side.
There are three features corresponding to a pro-
duction rule, tracking if the rule occurs in the parse
of first argument of the relation, in the second, or
in both.
Adopting this representation allows us to fo-
cus on the issue of class imbalance and how
the choices of tackling this problem affect even-
tual prediction performance. Our findings are
representation-independent and will most likely
extend to other representations.
We train and evaluate a binary classifier with
linear kernel using SVMLight2 (Joachims, 1999)
for each of the four top level classes of relations:
Temporal, Comparison, Contingency and Expan-
sion. We used SVM-Multiclass3 for standard mul-
tiway classification. We also develop and evaluate
two approaches for multiway classification for the
four classes plus the additional class of entity rela-
tion and no relation.
Due to the uneven distribution of classes, we use
precision, recall and f-measure to measure binary
prediction performance. For multiway classifica-
</bodyText>
<footnote confidence="0.9982875">
2http://svmlight.joachims.org/
3http://svmlight.joachims.org/svm multiclass.html
</footnote>
<bodyText confidence="0.889411">
tion, we use the balanced accuracy (BAC):
</bodyText>
<equation confidence="0.994266">
1
BAC = k
</equation>
<bodyText confidence="0.999986681818182">
where k is the number of relations to predict, ci is
the number of instances of relation i that are cor-
rectly predicted, ni is the total number of instances
of relation i.
Balanced accuracy (or averaged accuracy) has
a more intuitive interpretation than F-measure. It
is not dominated by the majority class as much as
standard accuracy is. For example for two classes,
in a dataset where one class makes up 90% of the
data, predicting the majority class has accuracy of
90% but balanced accuracy of 45%.
In testing, we keep the original distribution in-
tact and make predictions for all pairs of adjacent
sentences in the same paragraph that do not have
an explicit discourse relation 4. In order to per-
form tests for statistical significance over F scores,
precision, recall and balanced accuracies, we ran-
domly partitioned the testing data into 10 groups.
We kept the data distribution in each group as
close as possible to the overall testing set. To com-
pare the performance of two different systems, a
paired t-test is performed over these 10 groups.
</bodyText>
<sectionHeader confidence="0.942193" genericHeader="method">
4 Why downsampling?
</sectionHeader>
<bodyText confidence="0.994356869565218">
Binary classification As mentioned in the pre-
vious sections, in all prior work of supervised im-
plicit relation classification, the technique to cope
with highly skewed distribution for binary classi-
fication is to downsample the negative training in-
stances so that the sizes of positive and negative
classes are equal. The reason for doing so is that
the classifier can achieve high accuracy just by ig-
noring the small class, learning nothing and aways
predicting the larger class. We illustrate this ef-
fect in Table 2. Without downsampling, the only
reasonable F measure is achieved for Expansion
where the smaller class accounts for 40% of the
data. Note that with downsampling, the recogni-
tion of Expansion is also improved considerably.
Multiway classification In prior work multiway
classifiers are trained on all available training data.
As we just saw, however, this approach leads
4Note the contrast with prior work where in some cases
EntRels are part of Expansion, or in some cases the perfor-
mance of methods is evaluated only on pairs of sentences
where a discourse relation holds, excluding EntRels, NoRels
or AltLexs.
</bodyText>
<equation confidence="0.993693333333333">
k
ci , (1)
i=1 ni
</equation>
<page confidence="0.983606">
144
</page>
<bodyText confidence="0.915969805555556">
All data Downsample
Temp. 0 (nan/0.0) 15.52 (8.8/65.4)
Comp. 2.17 (71.4/1.1) 27.65 (17.3/69.2)
Cont. 0.96 (100.0/0.5) 47.14 (34.5/74.5)
Exp. 44.27 (54.9/37.1) 55.42 (49.3/63.3)
Table 2: F measure (precision/recall) of binary
classification: including all of the data vs down-
sampling.
to poor results in identifying the core Temporal,
Comparison and Contingency discourse relations.
We propose an alternative approach to multi-class
prediction, based on binary one-against-all classi-
fiers for each of the four discourse relations, in-
cluding Expansion, trained using downsampling.
The intuition is that an instance of adjacent sen-
tences Si is assigned to a discourse relation Rj
if the binary classifier for Rj recognizes Si as a
positive instance with confidence higher than that
of the classifiers for other relations. If none of
the binary classifiers recognizes the instance as a
positive example, the instance is assigned to class
EntRel/NoRel. This approach modifies the way
multi-class classifiers are normally constructed by
including downsampling and having special treat-
ment of the EntRel/NoRel class.
Specifically, we first use the four binary classi-
fiers Cj for each relation j to get the confidence pj
of instance i belonging to class j. We approximate
the confidence by the distance to the hyperplane
separating the two classes, which SVMLight pro-
vides. If at least one pj is greater than zero, assign
instance i the class k where the classifier confi-
dence is the highest. If none of the pj’s is greater
than zero, assign i to be the EntRel/NoRel class.
We show balanced accuracies of these two mul-
tiway classification methods in Table 3.
</bodyText>
<subsectionHeader confidence="0.670012">
Multiway SVM One-Against-All
</subsectionHeader>
<bodyText confidence="0.643699">
5-way 32.58 37.15
</bodyText>
<tableCaption confidence="0.6483115">
Table 3: Balanced accuracies for SVM-Multiclass
and one-against-all 5-way classification.
</tableCaption>
<bodyText confidence="0.9998705">
The one-against-all approach leads to 5% abso-
lute improvement in performance. A t-test anal-
ysis confirms that the difference is significant at
p &lt; 0.05. Note that the improvement comes en-
tirely from acknowledging that skewed class dis-
tribution poses a problem for the task and by ad-
dressing the problem in the same way for binary
and multi-class prediction.
</bodyText>
<sectionHeader confidence="0.834186" genericHeader="method">
5 Using more data
</sectionHeader>
<bodyText confidence="0.99987547368421">
Although downsampling gives much better per-
formance than simply including all of the origi-
nal data, it still appears to be an undesirable so-
lution because in essence it throws away much of
the annotated data. This means that for the small-
est relations, as much as 90% of the data will
not be used. Feature selection and feature val-
ues are computed only based on this much smaller
dataset and do not properly reflect the information
about discourse relations encoded in the PDTB. In
this section we first discuss some of the widely
used methods for handling skewed data distribu-
tion, that is, weighted cost and upsampling. First,
we show that with highly skewed distributions, the
two methods result in almost identical classifiers.
Then we introduce a method for feature selection
and shaping which computes feature weights on
the full dataset and thus captures much of the in-
formation lost in downsampling.
</bodyText>
<subsectionHeader confidence="0.993768">
5.1 Weighted cost and upsampling
</subsectionHeader>
<bodyText confidence="0.998375241379311">
A number of methods have been developed for
the skewed distribution problem (Morik et al.,
1999; Veropoulos et al., 1999; Akbani et al., 2004;
Batista et al., 2004; Chawla et al., 2002). Here we
highlight weighted cost and random upsampling,
which are known to work well and widely used.
The idea behind weighted cost (Morik et al.,
1999; Veropoulos et al., 1999) is to use weights
to adjust the penalties for false positives and false
negatives in the objective function. As in Morik
et al. (1999), we specify the cost factor to be the
ratio of the size of the negative class vs. that of the
positive class.
In the case of upsampling, instead of ran-
domly downsampling negative instances, positive
instances are randomly upsampled. In our exper-
iments we randomly replicate positive instances
with replacement until the numbers of positive and
negative instances are equal to each other.
The binary and multiway classification results
for these two methods are shown in Table 4 and
Table 5. For binary classification, we can see sig-
nificantly higher F score for the smallest Temporal
class. Weighted cost is also able to achieve signif-
icantly better F-score for Expansion. For Compar-
ison and Contingency, the F-scores are similar to
that of plain downsampling. The balanced accura-
cies of multi-class classification with either meth-
ods are lower, or significantly lower in the case of
</bodyText>
<page confidence="0.995406">
145
</page>
<bodyText confidence="0.96629275">
weighted cost, than using downsampling in one-
against-all manner.
mation about the relative importance of features
gleaned from the entire training data.
</bodyText>
<table confidence="0.9907426">
Upsample WeightCost
Temp. 20.35* (16.8/25.9) 20.61* (16.9/26.3)
Comp. 28.11 (20.6/44.5) 28.38 (19.9/49.6)
Cont. 46.46 (37.4/61.3) 46.36 (34.6/70.1)
Exp. 54.93 (50.3/60.5) 57.43* (43.9/83.1)
</table>
<tableCaption confidence="0.9629385">
Table 4: F-measure (precision/recall) of binary
classification: upsampling vs. weighted cost.
</tableCaption>
<bodyText confidence="0.998442375">
For Temporal and Comparison relations listed
in Table 4, we noticed an interesting similarity
between the F and precision values of upsam-
pling and weighted cost. To quantify this simi-
larity, we calculated the Q-statistic (Kuncheva and
Whitaker, 2003) between the two classifiers. The
Q-statistic is a measurement of classifier agree-
ment raging between -1 and 1, defined as:
</bodyText>
<equation confidence="0.999315333333333">
N11N00 − N01N10
Qw,u = (2)
N11N00 + N01N10
</equation>
<bodyText confidence="0.999964857142857">
Where w denotes the system using weighted cost,
u denotes the upsampling system. N11 means both
systems make a correct prediction, N00 means
both systems are incorrect, N10 means w is incor-
rect but u is correct, and N01 means w is correct
but u is incorrect.
We have the following Q statistics: Tempo-
ral: 0.999, Comparison: 0.9938, Contingency:
0.9746, Expansion: 0.7762. These are good in-
dicators that for highly skewed relations, the two
methods give classifiers that behave almost identi-
cally on the test data. In the discussions that fol-
low, we discuss only weighted cost to avoid redun-
dancy.
</bodyText>
<subsectionHeader confidence="0.999819">
5.2 Feature selection and shaping
</subsectionHeader>
<bodyText confidence="0.999960666666666">
While weighted cost or upsampling can give bet-
ter performance over downsampling for some rela-
tions, their disadvantages towards multi-class clas-
sification and the obvious favor towards the major-
ity class give rise to the following question: is it
possible to inform the classifier of the information
encoded in the annotation of all of the data while
still using downsampling to handle the skewed
class distribution? Our proposal is feature value
augmentation. Here we introduce a relational ma-
trix in which we calculate augmented feature val-
ues via feature shaping. We first compute the val-
ues of features on the entire training set, then use
the downsampled set for training with these val-
ues. In this way we pass on to the classifiers infor-
</bodyText>
<subsectionHeader confidence="0.677174">
5.2.1 Feature shaping
</subsectionHeader>
<bodyText confidence="0.999923130434783">
The idea of feature shaping was introduced in the
context of improving the performance of linear
SVMs (Forman et al., 2009). In linear SVMs
the prediction is based on a linear combination of
weightxfeature values. The sign of weight indi-
cates the preference for a class (positive or nega-
tive), the value of the feature should correspond to
how strongly it indicates that class. Thus, features
that are strongly discriminative should have high
values so that they can contribute more to the final
class decision. Here we augment feature values
for a relation according to the following criteria:
1. Features are considered “good” if they strongly
indicate the presence of the relation; 2. Features
are considered “good” if they strongly indicate the
absence of the relation; 3. features are considered
“bad” if their presence give no information about
either the presence or the absence of the relation.
To capture this information, we first construct a
relation matrix M with each entry Mij defined as
the conditional probability of relation Rj given the
feature Fi computed as the maximum likelihood
estimate from the full training set:
</bodyText>
<equation confidence="0.947617">
Mij = P(Rj|Fi)
</equation>
<bodyText confidence="0.999957181818182">
Each column of the relation matrix captures the
predictive power of each feature to a certain re-
lation. A feature with value Mij higher than the
column mean indicates that it is predictive for the
presence of relation j, while a feature with Mij
lower than the mean is predictive for its absence;
the strength of such indication depends on how far
away Mij is from the mean: the further away it is,
the more valuable this feature should be for rela-
tion j. With this idea we give the following aug-
mented value for each feature:
</bodyText>
<equation confidence="0.99158525">
� (3)
Mij, if Mij &gt; µj.
M� ij =
µj + (µj − Mij), if Mij &lt; µj.
</equation>
<bodyText confidence="0.999950142857143">
where µj is the mean of the jth column corre-
sponding to the jth relation.
Given a feature Fi, very small and very high
probabilities of a certain relation j, i.e., P(Rj|Fi),
are both useful information. However, in linear
SVMs, lower values of a feature would mean that
it contributes less to the decision of the class. By
</bodyText>
<page confidence="0.997816">
146
</page>
<bodyText confidence="0.999991166666667">
feature shaping, we allow features that strongly in-
dicate the absence of a class to influence the deci-
sion and rely on the classifier to identify the nega-
tive association and reflect it by assigning a nega-
tive weight to these features.
When constructing the relation matrix, we used
the top four relation classes along with an En-
tRel/NoRel class. We computed the matrix before
downsampling to preserve the natural data distri-
bution and features that strongly indicate the ab-
sence of a class, then downsample the negative
data just like the previous downsampling setting.
</bodyText>
<subsectionHeader confidence="0.764142">
5.2.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.99998595">
The relation matrix also provides information for
feature selection using a binomial test for signifi-
cance, B(n, N, p), which gives the probability of
observing a feature n times in N instances of a
relation if the probability of any feature occurring
with the relation is p. For each relation, we use the
binomial test to pick the features that occur signif-
icantly more or less often than expected with the
relation. In the binomial test, p is set to be equal to
the probability of that relation in the PDTB train-
ing set. We select only the features which result in
a low p-value for the binomial test for at least some
relation. We used 9-fold cross validation on the
training data to pick the best p-values for each re-
lation individually; all best p-values were between
0.1 and 0.2.
Result listing Table 5 and Table 6 show the mul-
tiway and binary classification performance using
feature shaping and feature selection. We also
show the precision and recall for binary classifiers.
</bodyText>
<table confidence="0.99008525">
Multiway SVM One-Against-All
AllData 32.58 NA
Downsample NA 37.15
Upsample NA 36.63
Weighted Cost NA 34.23
Selection 32.52 38.42*
Shaping NA 38.81**
Shape+Sel NA 39.13**
</table>
<tableCaption confidence="0.980571">
Table 5: Balanced accuracy for multiway
</tableCaption>
<bodyText confidence="0.9991215">
SVM and one-against-all for 5-way classification.
One asterisk (*) means significantly better than
weighted cost and upsampling, and two means sig-
nificantly better than downsampling, at p &lt; 0.05.
For multi-way classification, performing feature
shaping leads to significant improvements over
downsampling, upsampling and weighted cost.
The binomial method for feature selection that
relies on the full training data distribution has a
similar effect. Combined feature shaping and se-
lection leads to 2% absolute improvement in dis-
course relation recognition. For binary classifica-
tion, though, the improvement is significant only
for Temporal.
</bodyText>
<subsectionHeader confidence="0.7566715">
6 Classifier analysis and combination
6.1 Discussion of precision and recall
</subsectionHeader>
<bodyText confidence="0.999991976190476">
A careful examination of Tables 5 and 6 leads
to some intriguing observations. For the most
skewed relations, if we consider not only the F
measure, but also the precision and recall, there
is an interesting difference between the systems.
While downsampling has the lowest precision, it
gives the highest recall. The case for weighted cost
is another story. For highly skewed relations such
as Temporal and Comparison, it gives the highest
precision and the lowest recall; but as the data set
balances out in downsampling, the classifier shifts
towards high recall and low precision.
We can also rank the three feature augmentation
techniques in terms of how much they reflect dis-
tributional information in the training data. Fea-
ture selection reflects the training data least among
the three, because it uses information from all of
the data to select the features, but the feature val-
ues are still either 1 or 0. Feature shaping engages
more data because the value of a feature encodes
its relative “effectiveness” for a relation. We can
see that feature selection gives slightly higher pre-
cision than just downsampling; feature shaping,
on the other hand, gives precision and recall val-
ues between these two. This is most obvious in
smaller relations, i.e. Temporal and Comparison.
To see if this trend is statistically significant, we
did a paired t-test over the precision and recall for
each system and each relation. For the Temporal
relation, all systems that use more data have sig-
nificantly higher (p &lt; 0.05) precision than that
for downsampling. For Comparison, the changes
in precision are either significant or tend towards
significance for three methods: feature shaping
(p &lt; 0.1), feature shaping+election (p &lt; 0.1)
and weighted cost (p &lt; 0.05). For Contingency,
feature shaping gives an improvement in precision
that tends toward significance (p &lt; 0.1). The
drops in recall using feature shaping or weighted
cost for the above three relations are significant
(p &lt; 0.05). For the Expansion relation, being the
largest class with 40% positive data, changes in
</bodyText>
<page confidence="0.994969">
147
</page>
<table confidence="0.7423298">
Downsample WeightCost Selection Shaping Shape+Sel
Temp. 15.52 (8.8/65.4) 20.61* (16.9/26.3) 18.47* (10.7/65.9) 20.37* (12.6/53.2) 21.30* (13.7/47.8)
Comp. 27.65 (17.3/69.2) 28.38 (19.9/49.6) 26.98 (17.4/60.1) 27.79 (18.3/58.2) 26.92 (18.7/48.2)
Cont. 47.14 (34.5/74.5) 46.36 (34.6/70.1) 47.45 (34.7/75.2) 47.62 (35.4/72.9) 46.93 (35.2/70.5)
Exp. 55.42 (49.3/63.3) 57.43* (43.9/83.1) 55.52 (49.3/63.5) 55.13 (49.3/62.5) 54.90 (49.2/62.1)
</table>
<tableCaption confidence="0.7735605">
Table 6: F score (precision/recall) of classifiers with feature augmentation. Asterisk(*) means F score or
BAC is significantly greater than plain downsampling at p &lt; 0.05.
</tableCaption>
<bodyText confidence="0.9970116">
precision and recall with downsampling systems
are not significant; yet weighted cost shifted to-
wards predicting more of the positive instances,
i.e., giving a significantly higher recall by trading
with a significantly lower precision (p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.999945">
6.2 Discussion of classifier similarity
</subsectionHeader>
<bodyText confidence="0.9999975">
To better understand the differences of classi-
fier behaviors under the weighted cost and each
downsampling technique (plain downsampling,
feature selection, feature shaping, feature shap-
ing+selection), in Table 7 we show the percentage
of test instances that the weighted cost system and
each downsample system agree or do not agree. In
particular, we study the following situations:
</bodyText>
<listItem confidence="0.9967966">
1. The downsample system predicts correctly
but the weighted cost system does not (“D+C-”);
2. The weighted cost system predicts correctly
but the downsample system does not (“D-C+”);
3. Both systems are correct (“D+C+”).
</listItem>
<bodyText confidence="0.999835333333333">
At a glance of the Q statistic, it seems that the
systems are not behaving very differently. How-
ever, as only the sum of disagreements is reflected
in the Q statistic, we look more closely at where
the systems do not agree in each situation. If we
focus on the rarer Temporal and Comparison re-
lations, first note that in the plain downsampling
vs. weighted cost, the percentage of test instances
in the “D+C-” column is much smaller than that
in the “D-C+” column. This aligns with the above
observation that plain downsampling gives much
lower precision for these relations than weighted
cost. Now, as more data is engaged from first
using feature selection, then using feature shap-
ing, then using both, the percentage of instances
where both systems predict correctly increase. At
the same time, there is a drop in the percentage of
test instances in the “D-C+” column. This trend is
also a reflection of the observation that as more
data is engaged, the precision got higher as the
recall drops lower. As the data gets more evenly
distributed, this phenomenon fades away. The ta-
ble also reveals a subtle difference between fea-
ture shaping and feature selection. Compared to
</bodyText>
<table confidence="0.999762681818182">
D+C- D-C+ D+C+ Q
(%) (%) (%) Stat
Temporal
Downsamp 2.56 28.27 61.47 0.73
Selection 2.91 22.04 67.71 0.77
Shaping 2.61 13.36 76.39 0.89
Sel+Shape 2.83 10.42 79.32 0.90
Comparison
Downsamp 5.74 18.24 53.76 0.84
Selection 7.72 16.14 55.85 0.80
Shaping 6.14 11.95 60.04 0.89
Sel+Shape 9.69 10.99 61.01 0.83
Contingency
Downsamp 6.88 7.89 58.74 0.93
Selection 8.01 8.92 57.70 0.91
Shaping 7.07 6.73 59.90 0.94
Sel+Shape 8.68 8.13 58.49 0.91
Expansion
Downsamp 16.39 8.23 44.66 0.82
Selection 17.87 9.71 43.18 0.76
Shaping 16.64 8.45 44.44 0.81
Sel+Shape 18.36 10.30 42.59 0.73
</table>
<tableCaption confidence="0.993106">
Table 7: Q statistics and agreements (in percent-
</tableCaption>
<bodyText confidence="0.995000333333333">
ages) of each downsampling system vs. weighted
cost. “D” denotes the respective downsample sys-
tem in the left most column; “C” denotes the
weighted cost system. A “+” means that a system
makes a correct prediction; a “-” means a system
makes an incorrect prediction.
downsampling, feature selection introduces an in-
crease in the column “D+C-” (i.e. the weighted
cost system makes a mistake but the downsample
system is correct). Feature shaping, on the other
hand, do not necessarily increase this new kind of
difference between classifiers.
</bodyText>
<subsectionHeader confidence="0.992941">
6.3 Classifier combination
</subsectionHeader>
<bodyText confidence="0.9997561">
Our classifier comparisons revealed that for highly
skewed distributions, there are consistent differ-
ences in the performance of classifiers obtained by
using the training data in different ways. It stands
to reason that a combination of these classifiers
with different strengths will result in an overall im-
proved classifier. This idea is explored here.
Suppose on a sample i, the downsampling clas-
sifier predicts the target class with confidence pid,
and the weighted cost classifier predicts the target
</bodyText>
<page confidence="0.994735">
148
</page>
<bodyText confidence="0.9996772">
class with confidence pic. Here again we approx-
imate the confidence of the class by the distance
from the hyperplane dividing the two classes. We
weight the two predictions and get a new predic-
tion confidence by:
</bodyText>
<equation confidence="0.753018">
I_ αdpid + αupic . (4)
A αd + αc
</equation>
<bodyText confidence="0.999987842105263">
where the αs are parameters we want to encode
how much we trust each classifier. To get these
values, we train the classifiers and get the accura-
cies from each of them on the development set.
Since we are using linear SVMs in our experi-
ments, we mark the sample as positive if pi &gt; 0,
and negative otherwise.
The results for the combination are shown in Ta-
ble 8. We include the original performances of the
classifiers by themselves for reference.
F measure For Temporal, the combined classi-
fier performs better than the original classifiers.
We see significant (p &lt; 0.05) improvements over
the corresponding downsampling system and the
weighted cost system. If feature shaping is in-
volved in the combination, it is also having bet-
ter performance that tend toward significance (p &lt;
0.1) over the weighted cost classifier. For Compar-
ison, the benefits of a combined system is also ob-
vious for feature shaping and/or selection. Feature
shaping combined with weighted cost gives sig-
nificantly (p &lt; 0.05) better performance than ei-
ther of them individually, and feature selection and
shaping+selection combined with weighted cost is
better than themselves alone. For Contingency,
though weighted cost do not give better results, the
improvement tends toward significance (p &lt; 0.1)
when combined with plain downsampling. For Ex-
pansion where weighted cost gives the lowest pre-
cision, combination with other classifiers do not
give significant improvements over F scores.
Precision and recall We can also compare the
precision and recall for each system before and af-
ter combination. In all but one cases for Temporal
and Comparison, we observe significantly higher
precision and much lower recall after the combi-
nation. The case for Expansion is just the opposite
as expected.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.8791345">
In this paper, we studied the effect of the use of an-
notated data for binary and multiway classification
</bodyText>
<table confidence="0.999353346153846">
Original Combined
Classifier Classifier
Temporal
WeightCost 20.61 (16.9/26.3)
Downsamp 15.52 (8.8/65.4) 21.78* (14.9/40.5)
Selection 18.47 (10.7/65.9) 22.99* (15.8/42.0)
Shaping 20.37 (12.6/53.2) 23.88* (17.5/37.6)
Sel+Shape 21.30 (13.7/47.8) 23.72* (17.7/36.1)
Comparison
WeightCost 28.38 (19.9/49.6)
Downsamp 27.65 (17.3/69.2) 28.72 (19.3/56.4)
Selection 26.98 (17.4/60.1) 29.25∗ (20.1/54.0)
Shaping 27.79 (18.3/58.2) 29.89*+ (20.5/54.9)
Sel+Shape 26.92 (18.7/48.2) 29.83* (21.3/50.0)
Contingency
WeightCost 46.36 (34.6/70.1)
Downsamp 47.14 (34.5/74.5) 48.38+ (35.9/74.4)
Selection 47.45 (34.7/75.2) 47.76+ (35.5/72.9)
Shaping 47.62 (35.4/72.9) 48.16+ (36.0/72.9)
Sel+Shape 46.93 (35.2/70.5) 47.37 (35.6/70.7)
Expansion
WeightCost 57.43 (43.9/83.1)
Downsamp 55.42 (49.3/63.3) 56.61* (46.4/72.7)
Selection 55.52 (49.3/63.5) 57.10* (46.5/73.0)
Shaping 55.13 (49.3/62.5) 56.74* (46.4/73.0)
Sel+Shape 54.90 (49.2/62.1) 57.06* (46.4/74.0)
</table>
<tableCaption confidence="0.99344">
Table 8: Classifier combination results for binary
</tableCaption>
<bodyText confidence="0.991195285714286">
classification. An asterisk(*) means significantly
better than the corresponding downsampling sys-
tem at, and a plus(+) means significantly better
than weighted cost, at p &lt; 0.05. Improvements
that tend toward significance (p &lt; 0.1) are not
shown here but are discussed in the text.
in supervised implicit discourse relation recogni-
tion. The starting point of our work was to estab-
lish the effectiveness of downsampling negative
examples, which was practiced but not experimen-
tally investigated in prior work. We also evalu-
ated alternative solutions to the skewed data prob-
lem, as downsampling throws away most of the
data. We examined the effect of upsampling and
weighted cost. In addition, we introduced the rela-
tion matrix to give more emphasis on informative
features through augmenting the feature value via
feature shaping. We found that as we summarize
more detailed information about the data in the full
training set, performance for multiway classifica-
tion gets better. We also observed through preci-
sion and recall that there are fundamental differ-
ences between downsampling and weighted cost,
and this difference can be beneficially exploited
by combining the two classifiers. We showed that
our way of doing such combination gives signifi-
cantly higher performance results for binary clas-
sification in the case of rarer relations.
</bodyText>
<page confidence="0.998696">
149
</page>
<sectionHeader confidence="0.995851" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999442641509434">
Rehan Akbani, Stephen Kwek, and Nathalie Japkow-
icz. 2004. Applying support vector machines to
imbalanced datasets. In Machine Learning: ECML
2004, pages 39–50.
Gustavo E. A. P. A. Batista, Ronaldo C. Prati, and
Maria Carolina Monard. 2004. A study of the
behavior of several methods for balancing machine
learning training data. ACM SIGKDD Explorations
Newsletter - Special issue on learning from imbal-
anced datasets, 6(1):20–29, June.
Or Biran and Kathleen McKeown. 2013. Aggregated
word pair features for implicit discourse relation dis-
ambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL): Short Papers, pages 69–73.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. 2002. SMOTE:
Synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 16(1):321–
357, June.
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In IEEE International Conference on Se-
mantic Computing (IEEE-ICSC), pages 198 –205.
George Forman, Martin Scholz, and Shyamsundar Ra-
jaram. 2009. Feature shaping for linear SVM classi-
fiers. In Proceedings of the 15th ACM International
Conference on Knowledge Discovery and Data Min-
ing (KDD), pages 299–308.
Sucheta Ghosh, Richard Johansson, Giuseppe Ric-
cardi, and Sara Tonelli. 2011. Shallow discourse
parsing with conditional random fields. In Pro-
ceedings of the 5th International Joint Conference
on Natural Language Processing (IJCNLP), pages
1071–1079.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21:203–225.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010. A semi-supervised approach to im-
prove classification of infrequent discourse relations
using feature vector extension. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 399–409.
Yu Hong, Xiaopei Zhou, Tingting Che, Jianmin Yao,
Qiaoming Zhu, and Guodong Zhou. 2012. Cross-
argument inference for implicit discourse relation
recognition. In Proceedings of the 21st ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 295–304.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods, pages 169–184.
Ludmila I. Kuncheva and Christopher J. Whitaker.
2003. Measures of diversity in classifier ensembles
and their relationship with the ensemble accuracy.
Machine Learning, 51(2):181–207, May.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 343–351.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20:151–184, 4.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind Joshi. 2008. Sense annotation in the Penn
Discourse Treebank. In Proceedings of the 9th
International Conference on Computational Lin-
guistics and Intelligent Text Processing (CICLing),
pages 275–286.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning
with a knowledge-based approach - a case study in
intensive care monitoring. In Proceedings of the Six-
teenth International Conference on Machine Learn-
ing (ICML), pages 268–277.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue (SIGDIAL), pages 108–112.
Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani
Nenkova, Alan Lee, and Aravind Joshi. 2008. Eas-
ily identifiable discourse relations. In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING): Companion volume:
Posters, pages 87–90.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP (ACL-IJCNLP),
pages 683–691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC).
Konstantinos Veropoulos, Colin Campbell, and Nello
Cristianini. 1999. Controlling the sensitivity of sup-
port vector machines. In Proceedings of the Inter-
national Joint Conference on Artificial Intelligence
(IJCAI), volume 1999, pages 55–60.
</reference>
<page confidence="0.998239">
150
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595471">
<title confidence="0.9984455">Addressing Class Imbalance for Improved Recognition of Discourse Relations</title>
<author confidence="0.995335">Junyi Jessy</author>
<affiliation confidence="0.998786">University of</affiliation>
<email confidence="0.998241">ljunyi@seas.upenn.edu</email>
<author confidence="0.791403">Ani</author>
<affiliation confidence="0.997523">University of</affiliation>
<email confidence="0.997364">nenkova@seas.upenn.edu</email>
<abstract confidence="0.9879565">In this paper we address the problem of skewed class distribution in implicit discourse relation recognition. We examine the performance of classifiers for both binary classification predicting if a particular relation holds or not and for multi-class prediction. We review prior work to point out that the problem has been addressed differently for the binary and multi-class problems. We demonstrate that adopting a unified approach can significantly improve the performance of multi-class prediction. We also propose an approach that makes better use of the full annotations in the training set when downsampling is used. We report significant absolute improvements in performance in multi-class prediction, as well as significant improvement of binary classifiers for detecting the presence of implicit Temporal, Comparison and Contingency relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rehan Akbani</author>
<author>Stephen Kwek</author>
<author>Nathalie Japkowicz</author>
</authors>
<title>Applying support vector machines to imbalanced datasets.</title>
<date>2004</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>39--50</pages>
<contexts>
<context position="16290" citStr="Akbani et al., 2004" startWordPosition="2594" endWordPosition="2597">ed in the PDTB. In this section we first discuss some of the widely used methods for handling skewed data distribution, that is, weighted cost and upsampling. First, we show that with highly skewed distributions, the two methods result in almost identical classifiers. Then we introduce a method for feature selection and shaping which computes feature weights on the full dataset and thus captures much of the information lost in downsampling. 5.1 Weighted cost and upsampling A number of methods have been developed for the skewed distribution problem (Morik et al., 1999; Veropoulos et al., 1999; Akbani et al., 2004; Batista et al., 2004; Chawla et al., 2002). Here we highlight weighted cost and random upsampling, which are known to work well and widely used. The idea behind weighted cost (Morik et al., 1999; Veropoulos et al., 1999) is to use weights to adjust the penalties for false positives and false negatives in the objective function. As in Morik et al. (1999), we specify the cost factor to be the ratio of the size of the negative class vs. that of the positive class. In the case of upsampling, instead of randomly downsampling negative instances, positive instances are randomly upsampled. In our ex</context>
</contexts>
<marker>Akbani, Kwek, Japkowicz, 2004</marker>
<rawString>Rehan Akbani, Stephen Kwek, and Nathalie Japkowicz. 2004. Applying support vector machines to imbalanced datasets. In Machine Learning: ECML 2004, pages 39–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gustavo E A P A Batista</author>
<author>Ronaldo C Prati</author>
<author>Maria Carolina Monard</author>
</authors>
<title>A study of the behavior of several methods for balancing machine learning training data.</title>
<date>2004</date>
<journal>ACM SIGKDD Explorations</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="16312" citStr="Batista et al., 2004" startWordPosition="2598" endWordPosition="2601">is section we first discuss some of the widely used methods for handling skewed data distribution, that is, weighted cost and upsampling. First, we show that with highly skewed distributions, the two methods result in almost identical classifiers. Then we introduce a method for feature selection and shaping which computes feature weights on the full dataset and thus captures much of the information lost in downsampling. 5.1 Weighted cost and upsampling A number of methods have been developed for the skewed distribution problem (Morik et al., 1999; Veropoulos et al., 1999; Akbani et al., 2004; Batista et al., 2004; Chawla et al., 2002). Here we highlight weighted cost and random upsampling, which are known to work well and widely used. The idea behind weighted cost (Morik et al., 1999; Veropoulos et al., 1999) is to use weights to adjust the penalties for false positives and false negatives in the objective function. As in Morik et al. (1999), we specify the cost factor to be the ratio of the size of the negative class vs. that of the positive class. In the case of upsampling, instead of randomly downsampling negative instances, positive instances are randomly upsampled. In our experiments we randomly </context>
</contexts>
<marker>Batista, Prati, Monard, 2004</marker>
<rawString>Gustavo E. A. P. A. Batista, Ronaldo C. Prati, and Maria Carolina Monard. 2004. A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD Explorations Newsletter - Special issue on learning from imbalanced datasets, 6(1):20–29, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aggregated word pair features for implicit discourse relation disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL): Short Papers,</booktitle>
<pages>69--73</pages>
<contexts>
<context position="7078" citStr="Biran and McKeown, 2013" startWordPosition="1101" endWordPosition="1104">icit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at discovering if a particular relation holds and keeping the full training set for multi-class prediction. To highlight the problem, in Table 1 we show the distribution of implicit relation classes in the entire PDTB. In our work, we aim to develop classifiers to</context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL): Short Papers, pages 69–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitesh V Chawla</author>
<author>Kevin W Bowyer</author>
<author>Lawrence O Hall</author>
<author>W Philip Kegelmeyer</author>
</authors>
<title>SMOTE: Synthetic minority over-sampling technique.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>357</pages>
<contexts>
<context position="16334" citStr="Chawla et al., 2002" startWordPosition="2602" endWordPosition="2605">scuss some of the widely used methods for handling skewed data distribution, that is, weighted cost and upsampling. First, we show that with highly skewed distributions, the two methods result in almost identical classifiers. Then we introduce a method for feature selection and shaping which computes feature weights on the full dataset and thus captures much of the information lost in downsampling. 5.1 Weighted cost and upsampling A number of methods have been developed for the skewed distribution problem (Morik et al., 1999; Veropoulos et al., 1999; Akbani et al., 2004; Batista et al., 2004; Chawla et al., 2002). Here we highlight weighted cost and random upsampling, which are known to work well and widely used. The idea behind weighted cost (Morik et al., 1999; Veropoulos et al., 1999) is to use weights to adjust the penalties for false positives and false negatives in the objective function. As in Morik et al. (1999), we specify the cost factor to be the ratio of the size of the negative class vs. that of the positive class. In the case of upsampling, instead of randomly downsampling negative instances, positive instances are randomly upsampled. In our experiments we randomly replicate positive ins</context>
</contexts>
<marker>Chawla, Bowyer, Hall, Kegelmeyer, 2002</marker>
<rawString>Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. SMOTE: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16(1):321– 357, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Elwell</author>
<author>J Baldridge</author>
</authors>
<title>Discourse connective argument identification with connective specific rankers.</title>
<date>2008</date>
<booktitle>In IEEE International Conference on Semantic Computing (IEEE-ICSC),</booktitle>
<pages>198--205</pages>
<contexts>
<context position="6335" citStr="Elwell and Baldridge, 2008" startWordPosition="989" endWordPosition="992"> same entity or topic. EntRels were marked only if no other relation could be identified and they are not considered a discourse relation, rather an alternative discourse phenomena related to entity coherence (Grosz et al., 1995). There are 254 pairs of sentences where no discourse relation was identified (NoRel). Pitler et al. (2008) has shown that performance as high as 93% in accuracy can be easily achieved for the explicit relations, because the connective itself is a highly informative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individ</context>
</contexts>
<marker>Elwell, Baldridge, 2008</marker>
<rawString>R. Elwell and J. Baldridge. 2008. Discourse connective argument identification with connective specific rankers. In IEEE International Conference on Semantic Computing (IEEE-ICSC), pages 198 –205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
<author>Martin Scholz</author>
<author>Shyamsundar Rajaram</author>
</authors>
<title>Feature shaping for linear SVM classifiers.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>299--308</pages>
<contexts>
<context position="19909" citStr="Forman et al., 2009" startWordPosition="3178" endWordPosition="3181">e classifier of the information encoded in the annotation of all of the data while still using downsampling to handle the skewed class distribution? Our proposal is feature value augmentation. Here we introduce a relational matrix in which we calculate augmented feature values via feature shaping. We first compute the values of features on the entire training set, then use the downsampled set for training with these values. In this way we pass on to the classifiers infor5.2.1 Feature shaping The idea of feature shaping was introduced in the context of improving the performance of linear SVMs (Forman et al., 2009). In linear SVMs the prediction is based on a linear combination of weightxfeature values. The sign of weight indicates the preference for a class (positive or negative), the value of the feature should correspond to how strongly it indicates that class. Thus, features that are strongly discriminative should have high values so that they can contribute more to the final class decision. Here we augment feature values for a relation according to the following criteria: 1. Features are considered “good” if they strongly indicate the presence of the relation; 2. Features are considered “good” if t</context>
</contexts>
<marker>Forman, Scholz, Rajaram, 2009</marker>
<rawString>George Forman, Martin Scholz, and Shyamsundar Rajaram. 2009. Feature shaping for linear SVM classifiers. In Proceedings of the 15th ACM International Conference on Knowledge Discovery and Data Mining (KDD), pages 299–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Giuseppe Riccardi</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow discourse parsing with conditional random fields.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP),</booktitle>
<pages>1071--1079</pages>
<contexts>
<context position="6356" citStr="Ghosh et al., 2011" startWordPosition="993" endWordPosition="996">ls were marked only if no other relation could be identified and they are not considered a discourse relation, rather an alternative discourse phenomena related to entity coherence (Grosz et al., 1995). There are 254 pairs of sentences where no discourse relation was identified (NoRel). Pitler et al. (2008) has shown that performance as high as 93% in accuracy can be easily achieved for the explicit relations, because the connective itself is a highly informative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods</context>
</contexts>
<marker>Ghosh, Johansson, Riccardi, Tonelli, 2011</marker>
<rawString>Sucheta Ghosh, Richard Johansson, Giuseppe Riccardi, and Sara Tonelli. 2011. Shallow discourse parsing with conditional random fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1071–1079.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Scott Weinstein</author>
<author>Aravind K Joshi</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--203</pages>
<contexts>
<context position="5938" citStr="Grosz et al., 1995" startWordPosition="924" endWordPosition="927">: Temporal (the two arguments are related temporally), Comparison (contrast), Contingency (causal) and Expansion (one argument is the expansion of the other and continues the context) (Miltsakaki et al., 2008). These are the classes we focus on in our work. Finally, 5,210 pairs of adjacent sentences were marked as related by an entity relation (EntRel), by virtue of the repetition of the same entity or topic. EntRels were marked only if no other relation could be identified and they are not considered a discourse relation, rather an alternative discourse phenomena related to entity coherence (Grosz et al., 1995). There are 254 pairs of sentences where no discourse relation was identified (NoRel). Pitler et al. (2008) has shown that performance as high as 93% in accuracy can be easily achieved for the explicit relations, because the connective itself is a highly informative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present</context>
</contexts>
<marker>Grosz, Weinstein, Joshi, 1995</marker>
<rawString>Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21:203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>399--409</pages>
<contexts>
<context position="7052" citStr="Hernault et al., 2010" startWordPosition="1097" endWordPosition="1100">ns, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at discovering if a particular relation holds and keeping the full training set for multi-class prediction. To highlight the problem, in Table 1 we show the distribution of implicit relation classes in the entire PDTB. In our work, we aim</context>
</contexts>
<marker>Hernault, Bollegala, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka. 2010. A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 399–409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hong</author>
<author>Xiaopei Zhou</author>
<author>Tingting Che</author>
<author>Jianmin Yao</author>
<author>Qiaoming Zhu</author>
<author>Guodong Zhou</author>
</authors>
<title>Crossargument inference for implicit discourse relation recognition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>295--304</pages>
<contexts>
<context position="7190" citStr="Hong et al., 2012" startWordPosition="1118" endWordPosition="1121">e. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at discovering if a particular relation holds and keeping the full training set for multi-class prediction. To highlight the problem, in Table 1 we show the distribution of implicit relation classes in the entire PDTB. In our work, we aim to develop classifiers to identify the four top-level relations listed in the table1. # of samples Percentage Temporal 1038 4.3% Comparis</context>
</contexts>
<marker>Hong, Zhou, Che, Yao, Zhu, Zhou, 2012</marker>
<rawString>Yu Hong, Xiaopei Zhou, Tingting Che, Jianmin Yao, Qiaoming Zhu, and Guodong Zhou. 2012. Crossargument inference for implicit discourse relation recognition. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM), pages 295–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>In Advances in kernel methods,</booktitle>
<pages>169--184</pages>
<contexts>
<context position="10218" citStr="Joachims, 1999" startWordPosition="1616" endWordPosition="1617">the sentence corresponding to production rules with only non-terminals on the right-hand side. There are three features corresponding to a production rule, tracking if the rule occurs in the parse of first argument of the relation, in the second, or in both. Adopting this representation allows us to focus on the issue of class imbalance and how the choices of tackling this problem affect eventual prediction performance. Our findings are representation-independent and will most likely extend to other representations. We train and evaluate a binary classifier with linear kernel using SVMLight2 (Joachims, 1999) for each of the four top level classes of relations: Temporal, Comparison, Contingency and Expansion. We used SVM-Multiclass3 for standard multiway classification. We also develop and evaluate two approaches for multiway classification for the four classes plus the additional class of entity relation and no relation. Due to the uneven distribution of classes, we use precision, recall and f-measure to measure binary prediction performance. For multiway classifica2http://svmlight.joachims.org/ 3http://svmlight.joachims.org/svm multiclass.html tion, we use the balanced accuracy (BAC): 1 BAC = k </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale support vector machine learning practical. In Advances in kernel methods, pages 169–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludmila I Kuncheva</author>
<author>Christopher J Whitaker</author>
</authors>
<title>Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy.</title>
<date>2003</date>
<booktitle>Machine Learning,</booktitle>
<volume>51</volume>
<issue>2</issue>
<contexts>
<context position="18223" citStr="Kuncheva and Whitaker, 2003" startWordPosition="2895" endWordPosition="2898">nner. mation about the relative importance of features gleaned from the entire training data. Upsample WeightCost Temp. 20.35* (16.8/25.9) 20.61* (16.9/26.3) Comp. 28.11 (20.6/44.5) 28.38 (19.9/49.6) Cont. 46.46 (37.4/61.3) 46.36 (34.6/70.1) Exp. 54.93 (50.3/60.5) 57.43* (43.9/83.1) Table 4: F-measure (precision/recall) of binary classification: upsampling vs. weighted cost. For Temporal and Comparison relations listed in Table 4, we noticed an interesting similarity between the F and precision values of upsampling and weighted cost. To quantify this similarity, we calculated the Q-statistic (Kuncheva and Whitaker, 2003) between the two classifiers. The Q-statistic is a measurement of classifier agreement raging between -1 and 1, defined as: N11N00 − N01N10 Qw,u = (2) N11N00 + N01N10 Where w denotes the system using weighted cost, u denotes the upsampling system. N11 means both systems make a correct prediction, N00 means both systems are incorrect, N10 means w is incorrect but u is correct, and N01 means w is correct but u is incorrect. We have the following Q statistics: Temporal: 0.999, Comparison: 0.9938, Contingency: 0.9746, Expansion: 0.7762. These are good indicators that for highly skewed relations, t</context>
</contexts>
<marker>Kuncheva, Whitaker, 2003</marker>
<rawString>Ludmila I. Kuncheva and Christopher J. Whitaker. 2003. Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Machine Learning, 51(2):181–207, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>343--351</pages>
<contexts>
<context position="1911" citStr="Lin et al., 2009" startWordPosition="281" endWordPosition="284">ionship is helpful for textual entailment or question answering while restatement and exemplification are important for automatic summarization. Predicting the type of implicit relations, which are not signaled by any of the common explicit discourse connectives such as because, however, has proven to be a most challenging task in discourse analysis. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provided valuable annotations of implicit relations. Most research to date has focused on developing and refining lexical and linguistically rich features for the task (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012). Mostly ignored remains the problem of addressing the highly skewed distribution of implicit discourse relations. Only about 35% of pairs of adjacent sentences in the PDTB are connected by three of the four top level discourse relation: 5% participate in Temporal relation, 10% in Comparison (contrast) and 20% in Contingency (causal) relations. The remaining pairs are connected by the catch-all Expansion relation (40%) or by some other linguistic devices (24%). Finer grained relations of interest to particular applications account for increasingly smaller percentage of </context>
<context position="6797" citStr="Lin et al., 2009" startWordPosition="1059" endWordPosition="1062"> is a highly informative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary clas</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-styled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<pages>4</pages>
<contexts>
<context position="6307" citStr="Lin et al., 2014" startWordPosition="985" endWordPosition="988"> repetition of the same entity or topic. EntRels were marked only if no other relation could be identified and they are not considered a discourse relation, rather an alternative discourse phenomena related to entity coherence (Grosz et al., 1995). There are 254 pairs of sentences where no discourse relation was identified (NoRel). Pitler et al. (2008) has shown that performance as high as 93% in accuracy can be easily achieved for the explicit relations, because the connective itself is a highly informative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different </context>
<context position="8314" citStr="Lin et al., 2014" startWordPosition="1302" endWordPosition="1305">op-level relations listed in the table1. # of samples Percentage Temporal 1038 4.3% Comparison 2550 11.3% Contingency 4532 20% Expansion 9082 40% Table 1: Distribution of implicit relations in the PDTB. 3 Experimental settings In our experiments, we used all non-explicit instances in the PDTB sections 2-19 for training and those in sections 20-24 for testing. Like most studies, we kept sections 0-1 as development set. In order to ensure we have a large enough test set to properly perform tests for statistical significance over F scores and balanced accuracies, we did not follow previous work (Lin et al., 2014; Park and Cardie, 2012) that used only section 23 or sections 23-24 for testing. Also, the traditional rule of thumb is to split the available data into training 1The rest of the data are EntRel/NoRel. 143 and testing sets with 80%/20% ratio. Our choice ensures that this is the case for all of the relations. The only features that we use in our experiments are production rules. We exclude features that occur fewer than five times in the training set. Production rules are the state-of-the-art representation for discourse relation recognition. This representation leads to only slightly lower re</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural Language Engineering, 20:151–184, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
</authors>
<title>Sense annotation in the Penn Discourse Treebank.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing),</booktitle>
<pages>275--286</pages>
<contexts>
<context position="5528" citStr="Miltsakaki et al., 2008" startWordPosition="855" endWordPosition="858">xically marked. Alternative lexicalizations (AltLex) are the ones where there is a phrase in the sentence implying the relation but the phrase itself was not one of the explicit discourse connectives. There are 16,224 and 624 examples of implicit and AltLex relations, respectively, in the PDTB. The sense of discourse relations in the PDTB is organized in a three-tiered hierarchy. The four top level relations are: Temporal (the two arguments are related temporally), Comparison (contrast), Contingency (causal) and Expansion (one argument is the expansion of the other and continues the context) (Miltsakaki et al., 2008). These are the classes we focus on in our work. Finally, 5,210 pairs of adjacent sentences were marked as related by an entity relation (EntRel), by virtue of the repetition of the same entity or topic. EntRels were marked only if no other relation could be identified and they are not considered a discourse relation, rather an alternative discourse phenomena related to entity coherence (Grosz et al., 1995). There are 254 pairs of sentences where no discourse relation was identified (NoRel). Pitler et al. (2008) has shown that performance as high as 93% in accuracy can be easily achieved for t</context>
</contexts>
<marker>Miltsakaki, Robaldo, Lee, Joshi, 2008</marker>
<rawString>Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Aravind Joshi. 2008. Sense annotation in the Penn Discourse Treebank. In Proceedings of the 9th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing), pages 275–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Morik</author>
<author>Peter Brockhausen</author>
<author>Thorsten Joachims</author>
</authors>
<title>Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Machine Learning (ICML),</booktitle>
<pages>268--277</pages>
<contexts>
<context position="16244" citStr="Morik et al., 1999" startWordPosition="2586" endWordPosition="2589">e information about discourse relations encoded in the PDTB. In this section we first discuss some of the widely used methods for handling skewed data distribution, that is, weighted cost and upsampling. First, we show that with highly skewed distributions, the two methods result in almost identical classifiers. Then we introduce a method for feature selection and shaping which computes feature weights on the full dataset and thus captures much of the information lost in downsampling. 5.1 Weighted cost and upsampling A number of methods have been developed for the skewed distribution problem (Morik et al., 1999; Veropoulos et al., 1999; Akbani et al., 2004; Batista et al., 2004; Chawla et al., 2002). Here we highlight weighted cost and random upsampling, which are known to work well and widely used. The idea behind weighted cost (Morik et al., 1999; Veropoulos et al., 1999) is to use weights to adjust the penalties for false positives and false negatives in the objective function. As in Morik et al. (1999), we specify the cost factor to be the ratio of the size of the negative class vs. that of the positive class. In the case of upsampling, instead of randomly downsampling negative instances, positi</context>
</contexts>
<marker>Morik, Brockhausen, Joachims, 1999</marker>
<rawString>Katharina Morik, Peter Brockhausen, and Thorsten Joachims. 1999. Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring. In Proceedings of the Sixteenth International Conference on Machine Learning (ICML), pages 268–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Improving implicit discourse relation recognition through feature set optimization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),</booktitle>
<pages>108--112</pages>
<contexts>
<context position="1935" citStr="Park and Cardie, 2012" startWordPosition="285" endWordPosition="288"> for textual entailment or question answering while restatement and exemplification are important for automatic summarization. Predicting the type of implicit relations, which are not signaled by any of the common explicit discourse connectives such as because, however, has proven to be a most challenging task in discourse analysis. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provided valuable annotations of implicit relations. Most research to date has focused on developing and refining lexical and linguistically rich features for the task (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012). Mostly ignored remains the problem of addressing the highly skewed distribution of implicit discourse relations. Only about 35% of pairs of adjacent sentences in the PDTB are connected by three of the four top level discourse relation: 5% participate in Temporal relation, 10% in Comparison (contrast) and 20% in Contingency (causal) relations. The remaining pairs are connected by the catch-all Expansion relation (40%) or by some other linguistic devices (24%). Finer grained relations of interest to particular applications account for increasingly smaller percentage of the PDTB data. Class imb</context>
<context position="6821" citStr="Park and Cardie (2012)" startWordPosition="1063" endWordPosition="1066">ative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for one-vsall binary classification aimed at disc</context>
<context position="8338" citStr="Park and Cardie, 2012" startWordPosition="1306" endWordPosition="1309"> listed in the table1. # of samples Percentage Temporal 1038 4.3% Comparison 2550 11.3% Contingency 4532 20% Expansion 9082 40% Table 1: Distribution of implicit relations in the PDTB. 3 Experimental settings In our experiments, we used all non-explicit instances in the PDTB sections 2-19 for training and those in sections 20-24 for testing. Like most studies, we kept sections 0-1 as development set. In order to ensure we have a large enough test set to properly perform tests for statistical significance over F scores and balanced accuracies, we did not follow previous work (Lin et al., 2014; Park and Cardie, 2012) that used only section 23 or sections 23-24 for testing. Also, the traditional rule of thumb is to split the available data into training 1The rest of the data are EntRel/NoRel. 143 and testing sets with 80%/20% ratio. Our choice ensures that this is the case for all of the relations. The only features that we use in our experiments are production rules. We exclude features that occur fewer than five times in the training set. Production rules are the state-of-the-art representation for discourse relation recognition. This representation leads to only slightly lower results than a system incl</context>
</contexts>
<marker>Park, Cardie, 2012</marker>
<rawString>Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 108–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Mridhula Raghupathy</author>
<author>Hena Mehta</author>
<author>Ani Nenkova</author>
<author>Alan Lee</author>
<author>Aravind Joshi</author>
</authors>
<title>Easily identifiable discourse relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING): Companion volume: Posters,</booktitle>
<pages>87--90</pages>
<contexts>
<context position="6045" citStr="Pitler et al. (2008)" startWordPosition="941" endWordPosition="944">ansion (one argument is the expansion of the other and continues the context) (Miltsakaki et al., 2008). These are the classes we focus on in our work. Finally, 5,210 pairs of adjacent sentences were marked as related by an entity relation (EntRel), by virtue of the repetition of the same entity or topic. EntRels were marked only if no other relation could be identified and they are not considered a discourse relation, rather an alternative discourse phenomena related to entity coherence (Grosz et al., 1995). There are 254 pairs of sentences where no discourse relation was identified (NoRel). Pitler et al. (2008) has shown that performance as high as 93% in accuracy can be easily achieved for the explicit relations, because the connective itself is a highly informative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, and Aravind Joshi. 2008. Easily identifiable discourse relations. In Proceedings of the International Conference on Computational Linguistics (COLING): Companion volume: Posters, pages 87–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP),</booktitle>
<pages>683--691</pages>
<contexts>
<context position="1893" citStr="Pitler et al., 2009" startWordPosition="277" endWordPosition="280">ple, the causal relationship is helpful for textual entailment or question answering while restatement and exemplification are important for automatic summarization. Predicting the type of implicit relations, which are not signaled by any of the common explicit discourse connectives such as because, however, has proven to be a most challenging task in discourse analysis. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provided valuable annotations of implicit relations. Most research to date has focused on developing and refining lexical and linguistically rich features for the task (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012). Mostly ignored remains the problem of addressing the highly skewed distribution of implicit discourse relations. Only about 35% of pairs of adjacent sentences in the PDTB are connected by three of the four top level discourse relation: 5% participate in Temporal relation, 10% in Comparison (contrast) and 20% in Contingency (causal) relations. The remaining pairs are connected by the catch-all Expansion relation (40%) or by some other linguistic devices (24%). Finer grained relations of interest to particular applications account for increasingly smal</context>
<context position="6778" citStr="Pitler et al., 2009" startWordPosition="1055" endWordPosition="1058">the connective itself is a highly informative feature. Efforts in identifying the argument spans have also yielded high accuracies (Lin et al., 2014; Elwell and Baldridge, 2008; Ghosh et al., 2011). However, in the absence of a connective, recognizing non-explicit relations, which includes implicit relations, alternative lexicalizations, entity relation and no relation present, has proven to be a real challenge. Prior work on supervised implicit discourse recognition studied a wide range of features including lexical, syntactic, verb classes, semantic groups via General Inquirer and polarity (Pitler et al., 2009; Lin et al., 2009). Park and Cardie (2012) studied the combination of features and achieved better performance with a different combination for each individual relation. Methods for improving the sparsity of lexical representations have been proposed (Hernault et al., 2010; Biran and McKeown, 2013), as well as web-driven approaches which reduce the problem to explicit relation recognition (Hong et al., 2012). Remarkably, no prior work has discussed the highly skewed class distribution of discourse relation types. The tacitly adopted solution has been to downsample the negative examples for on</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP), pages 683–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="1704" citStr="Prasad et al., 2008" startWordPosition="248" endWordPosition="251">tion Discourse relations holding between adjacent sentences in text play an essential role in establishing local coherence and contribute to the semantic interpretation of the text. For example, the causal relationship is helpful for textual entailment or question answering while restatement and exemplification are important for automatic summarization. Predicting the type of implicit relations, which are not signaled by any of the common explicit discourse connectives such as because, however, has proven to be a most challenging task in discourse analysis. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provided valuable annotations of implicit relations. Most research to date has focused on developing and refining lexical and linguistically rich features for the task (Pitler et al., 2009; Lin et al., 2009; Park and Cardie, 2012). Mostly ignored remains the problem of addressing the highly skewed distribution of implicit discourse relations. Only about 35% of pairs of adjacent sentences in the PDTB are connected by three of the four top level discourse relation: 5% participate in Temporal relation, 10% in Comparison (contrast) and 20% in Contingency (causal) relations. The remaining pairs ar</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantinos Veropoulos</author>
<author>Colin Campbell</author>
<author>Nello Cristianini</author>
</authors>
<title>Controlling the sensitivity of support vector machines.</title>
<date>1999</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<volume>volume</volume>
<pages>55--60</pages>
<contexts>
<context position="16269" citStr="Veropoulos et al., 1999" startWordPosition="2590" endWordPosition="2593">discourse relations encoded in the PDTB. In this section we first discuss some of the widely used methods for handling skewed data distribution, that is, weighted cost and upsampling. First, we show that with highly skewed distributions, the two methods result in almost identical classifiers. Then we introduce a method for feature selection and shaping which computes feature weights on the full dataset and thus captures much of the information lost in downsampling. 5.1 Weighted cost and upsampling A number of methods have been developed for the skewed distribution problem (Morik et al., 1999; Veropoulos et al., 1999; Akbani et al., 2004; Batista et al., 2004; Chawla et al., 2002). Here we highlight weighted cost and random upsampling, which are known to work well and widely used. The idea behind weighted cost (Morik et al., 1999; Veropoulos et al., 1999) is to use weights to adjust the penalties for false positives and false negatives in the objective function. As in Morik et al. (1999), we specify the cost factor to be the ratio of the size of the negative class vs. that of the positive class. In the case of upsampling, instead of randomly downsampling negative instances, positive instances are randomly</context>
</contexts>
<marker>Veropoulos, Campbell, Cristianini, 1999</marker>
<rawString>Konstantinos Veropoulos, Colin Campbell, and Nello Cristianini. 1999. Controlling the sensitivity of support vector machines. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), volume 1999, pages 55–60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>